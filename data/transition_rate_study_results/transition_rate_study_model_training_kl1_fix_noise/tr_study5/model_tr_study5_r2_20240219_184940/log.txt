Args:
Namespace(name='model_tr_study5', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2', training_data='data/transition_rate_studies/tr_study5/tr_study5_training/r2', validation_data='data/transition_rate_studies/tr_study5/tr_study5_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 347601803

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.256350754856967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.256350754856967 | validation: 10.368196069873056]
	TIME [epoch: 80.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.028470757070513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.028470757070513 | validation: 10.273423891882894]
	TIME [epoch: 9.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.790079807690038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.790079807690038 | validation: 9.87048395910465]
	TIME [epoch: 9.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.619606326158898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.619606326158898 | validation: 10.154081344060396]
	TIME [epoch: 9.81 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.215710337546913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.215710337546913 | validation: 8.896872402389482]
	TIME [epoch: 9.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.902009273531682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.902009273531682 | validation: 8.353758267823368]
	TIME [epoch: 9.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.724061536730522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.724061536730522 | validation: 8.41349158996834]
	TIME [epoch: 9.81 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.68389739264487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.68389739264487 | validation: 8.864348482928255]
	TIME [epoch: 9.81 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.502500483222283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.502500483222283 | validation: 9.297408439883004]
	TIME [epoch: 9.81 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.840787028854844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.840787028854844 | validation: 8.788072742262436]
	TIME [epoch: 9.82 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.647603468247956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.647603468247956 | validation: 11.55414624279802]
	TIME [epoch: 9.79 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.67968704278723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.67968704278723 | validation: 8.855130751422232]
	TIME [epoch: 9.78 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.050507224986251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.050507224986251 | validation: 8.564494910416455]
	TIME [epoch: 9.78 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.663231747879857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.663231747879857 | validation: 8.582920892331359]
	TIME [epoch: 9.81 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.10932991523525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.10932991523525 | validation: 8.856921481986555]
	TIME [epoch: 9.78 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.784844171065728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.784844171065728 | validation: 8.643220980992535]
	TIME [epoch: 9.79 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.49835975041234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.49835975041234 | validation: 8.92742377776638]
	TIME [epoch: 9.8 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.778590548704203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.778590548704203 | validation: 10.740180099074328]
	TIME [epoch: 9.8 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.219309298389017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.219309298389017 | validation: 8.354114871425175]
	TIME [epoch: 9.8 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.264640431705589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.264640431705589 | validation: 8.3839051504787]
	TIME [epoch: 9.8 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.204881815437105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.204881815437105 | validation: 7.937658157589166]
	TIME [epoch: 9.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.337903547203194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.337903547203194 | validation: 11.400900645911788]
	TIME [epoch: 9.77 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.497092909737454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.497092909737454 | validation: 8.347418677594968]
	TIME [epoch: 9.77 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.679032618298157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.679032618298157 | validation: 7.5727199077854435]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.089025071956543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.089025071956543 | validation: 7.485045989119251]
	TIME [epoch: 9.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.758440328760905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.758440328760905 | validation: 7.2133770633600545]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.674694787477231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.674694787477231 | validation: 7.437499141404253]
	TIME [epoch: 9.77 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.562256481755672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.562256481755672 | validation: 7.1556461586783335]
	TIME [epoch: 9.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.032790752580898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.032790752580898 | validation: 7.018128298644842]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.866058758274842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.866058758274842 | validation: 6.342082529144713]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.330640334570484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.330640334570484 | validation: 6.900593478281121]
	TIME [epoch: 9.79 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.26657349630076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.26657349630076 | validation: 6.103641335258896]
	TIME [epoch: 9.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.916316730798885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.916316730798885 | validation: 6.324603729863542]
	TIME [epoch: 9.8 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.988851519544136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.988851519544136 | validation: 5.979201773653852]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7302297480308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7302297480308 | validation: 6.169276085185766]
	TIME [epoch: 9.78 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.765056067196889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.765056067196889 | validation: 6.139367586863364]
	TIME [epoch: 9.79 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7966703812258675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7966703812258675 | validation: 6.278075063658406]
	TIME [epoch: 9.77 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.610720834182401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.610720834182401 | validation: 6.129027850099531]
	TIME [epoch: 9.76 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.880549950369873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.880549950369873 | validation: 6.327734642717014]
	TIME [epoch: 9.78 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.588536039472802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.588536039472802 | validation: 6.25353511105513]
	TIME [epoch: 9.78 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.740048035108174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.740048035108174 | validation: 6.278654188656579]
	TIME [epoch: 9.76 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.598460451244659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.598460451244659 | validation: 6.676590720911993]
	TIME [epoch: 9.78 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0055386556211054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0055386556211054 | validation: 6.806965856197849]
	TIME [epoch: 9.78 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.891888965185869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.891888965185869 | validation: 6.113448052785748]
	TIME [epoch: 9.76 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.667369226737894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.667369226737894 | validation: 6.307852851261294]
	TIME [epoch: 9.76 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.641057709551662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.641057709551662 | validation: 6.177673534914803]
	TIME [epoch: 9.77 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.678931612671002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.678931612671002 | validation: 7.7820286738974485]
	TIME [epoch: 9.77 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.395361202518438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.395361202518438 | validation: 6.556676746018807]
	TIME [epoch: 9.76 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0152243695782195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0152243695782195 | validation: 6.687631801685684]
	TIME [epoch: 9.76 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.005235101717528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.005235101717528 | validation: 6.389721735312255]
	TIME [epoch: 9.78 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.845658408214732		[learning rate: 0.0099811]
	Learning Rate: 0.00998112
	LOSS [training: 5.845658408214732 | validation: 6.24096846754535]
	TIME [epoch: 9.77 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.806304073114773		[learning rate: 0.0099576]
	Learning Rate: 0.00995758
	LOSS [training: 5.806304073114773 | validation: 6.236589212589436]
	TIME [epoch: 9.77 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6449245344973225		[learning rate: 0.0099341]
	Learning Rate: 0.00993409
	LOSS [training: 5.6449245344973225 | validation: 6.192366056559299]
	TIME [epoch: 9.77 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.710447667176271		[learning rate: 0.0099107]
	Learning Rate: 0.00991066
	LOSS [training: 5.710447667176271 | validation: 6.104778447271679]
	TIME [epoch: 9.8 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.723730601084755		[learning rate: 0.0098873]
	Learning Rate: 0.00988728
	LOSS [training: 5.723730601084755 | validation: 6.008904525797753]
	TIME [epoch: 9.77 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5146256209360365		[learning rate: 0.009864]
	Learning Rate: 0.00986396
	LOSS [training: 5.5146256209360365 | validation: 6.088966247935219]
	TIME [epoch: 9.78 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.05794573583543		[learning rate: 0.0098407]
	Learning Rate: 0.00984069
	LOSS [training: 6.05794573583543 | validation: 6.173980696618117]
	TIME [epoch: 9.77 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8728320568343255		[learning rate: 0.0098175]
	Learning Rate: 0.00981748
	LOSS [training: 5.8728320568343255 | validation: 6.132779623791889]
	TIME [epoch: 9.79 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.808274081625633		[learning rate: 0.0097943]
	Learning Rate: 0.00979432
	LOSS [training: 5.808274081625633 | validation: 6.047427658021238]
	TIME [epoch: 9.78 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.706855898173157		[learning rate: 0.0097712]
	Learning Rate: 0.00977122
	LOSS [training: 5.706855898173157 | validation: 6.017800781563405]
	TIME [epoch: 9.76 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7483706757266475		[learning rate: 0.0097482]
	Learning Rate: 0.00974817
	LOSS [training: 5.7483706757266475 | validation: 6.124324969022939]
	TIME [epoch: 9.78 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581716649986362		[learning rate: 0.0097252]
	Learning Rate: 0.00972517
	LOSS [training: 5.581716649986362 | validation: 6.733304131811567]
	TIME [epoch: 9.79 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.828112988192767		[learning rate: 0.0097022]
	Learning Rate: 0.00970223
	LOSS [training: 5.828112988192767 | validation: 5.899240586055621]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.383927213748046		[learning rate: 0.0096793]
	Learning Rate: 0.00967935
	LOSS [training: 5.383927213748046 | validation: 5.987722018547068]
	TIME [epoch: 9.77 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.355929822203548		[learning rate: 0.0096565]
	Learning Rate: 0.00965652
	LOSS [training: 5.355929822203548 | validation: 6.570333690319569]
	TIME [epoch: 9.77 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.541809623149429		[learning rate: 0.0096337]
	Learning Rate: 0.00963374
	LOSS [training: 5.541809623149429 | validation: 6.1572454747992245]
	TIME [epoch: 9.76 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.478739975924356		[learning rate: 0.009611]
	Learning Rate: 0.00961101
	LOSS [training: 5.478739975924356 | validation: 6.5372923799728415]
	TIME [epoch: 9.75 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.747597799639978		[learning rate: 0.0095883]
	Learning Rate: 0.00958834
	LOSS [training: 5.747597799639978 | validation: 6.130388828396688]
	TIME [epoch: 9.76 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.511699290513229		[learning rate: 0.0095657]
	Learning Rate: 0.00956573
	LOSS [training: 5.511699290513229 | validation: 6.0564247386978325]
	TIME [epoch: 9.79 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6802474822740985		[learning rate: 0.0095432]
	Learning Rate: 0.00954316
	LOSS [training: 5.6802474822740985 | validation: 5.952559176898908]
	TIME [epoch: 9.75 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.203539450123713		[learning rate: 0.0095207]
	Learning Rate: 0.00952065
	LOSS [training: 6.203539450123713 | validation: 7.94914264800806]
	TIME [epoch: 9.77 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.35296900446012		[learning rate: 0.0094982]
	Learning Rate: 0.00949819
	LOSS [training: 6.35296900446012 | validation: 6.357674743796273]
	TIME [epoch: 9.78 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8234219500626905		[learning rate: 0.0094758]
	Learning Rate: 0.00947579
	LOSS [training: 5.8234219500626905 | validation: 5.951580200812575]
	TIME [epoch: 9.77 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.445257112941998		[learning rate: 0.0094534]
	Learning Rate: 0.00945344
	LOSS [training: 5.445257112941998 | validation: 6.128052913119973]
	TIME [epoch: 9.75 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.87317315338427		[learning rate: 0.0094311]
	Learning Rate: 0.00943114
	LOSS [training: 5.87317315338427 | validation: 6.308103138497574]
	TIME [epoch: 9.76 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.912486837073561		[learning rate: 0.0094089]
	Learning Rate: 0.00940889
	LOSS [training: 5.912486837073561 | validation: 6.171062828744072]
	TIME [epoch: 9.78 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.128017642718379		[learning rate: 0.0093867]
	Learning Rate: 0.0093867
	LOSS [training: 6.128017642718379 | validation: 7.331477432043832]
	TIME [epoch: 9.76 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.306696799251701		[learning rate: 0.0093646]
	Learning Rate: 0.00936456
	LOSS [training: 6.306696799251701 | validation: 7.104960155571858]
	TIME [epoch: 9.77 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.205691022432777		[learning rate: 0.0093425]
	Learning Rate: 0.00934247
	LOSS [training: 6.205691022432777 | validation: 6.265425607346864]
	TIME [epoch: 9.76 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6725535172977		[learning rate: 0.0093204]
	Learning Rate: 0.00932043
	LOSS [training: 5.6725535172977 | validation: 6.487987378983924]
	TIME [epoch: 9.79 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.602014391138963		[learning rate: 0.0092984]
	Learning Rate: 0.00929844
	LOSS [training: 5.602014391138963 | validation: 5.991585679699231]
	TIME [epoch: 9.76 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.633267796945648		[learning rate: 0.0092765]
	Learning Rate: 0.00927651
	LOSS [training: 5.633267796945648 | validation: 6.06741320215682]
	TIME [epoch: 9.77 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581797840310241		[learning rate: 0.0092546]
	Learning Rate: 0.00925463
	LOSS [training: 5.581797840310241 | validation: 5.883151268978857]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.365801298889433		[learning rate: 0.0092328]
	Learning Rate: 0.0092328
	LOSS [training: 5.365801298889433 | validation: 6.162629819784249]
	TIME [epoch: 9.79 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.488730890417128		[learning rate: 0.009211]
	Learning Rate: 0.00921102
	LOSS [training: 5.488730890417128 | validation: 6.3096781336820555]
	TIME [epoch: 9.77 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.957540426678859		[learning rate: 0.0091893]
	Learning Rate: 0.00918929
	LOSS [training: 5.957540426678859 | validation: 6.435975878929882]
	TIME [epoch: 9.78 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.675316828076447		[learning rate: 0.0091676]
	Learning Rate: 0.00916762
	LOSS [training: 5.675316828076447 | validation: 7.333317620171369]
	TIME [epoch: 9.79 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.944989763745118		[learning rate: 0.009146]
	Learning Rate: 0.00914599
	LOSS [training: 5.944989763745118 | validation: 5.767773195988432]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.39941211458549		[learning rate: 0.0091244]
	Learning Rate: 0.00912442
	LOSS [training: 5.39941211458549 | validation: 6.112485822220601]
	TIME [epoch: 9.78 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.539819788026097		[learning rate: 0.0091029]
	Learning Rate: 0.00910289
	LOSS [training: 5.539819788026097 | validation: 5.974940619606847]
	TIME [epoch: 9.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.031464038393528		[learning rate: 0.0090814]
	Learning Rate: 0.00908142
	LOSS [training: 6.031464038393528 | validation: 6.730184311479923]
	TIME [epoch: 9.81 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.107358836143968		[learning rate: 0.00906]
	Learning Rate: 0.00906
	LOSS [training: 6.107358836143968 | validation: 6.121649619926729]
	TIME [epoch: 9.77 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.218984259129039		[learning rate: 0.0090386]
	Learning Rate: 0.00903863
	LOSS [training: 6.218984259129039 | validation: 6.900041311523662]
	TIME [epoch: 9.79 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.142488412014268		[learning rate: 0.0090173]
	Learning Rate: 0.00901731
	LOSS [training: 6.142488412014268 | validation: 6.745392375044755]
	TIME [epoch: 9.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.040384881238989		[learning rate: 0.008996]
	Learning Rate: 0.00899604
	LOSS [training: 6.040384881238989 | validation: 6.315772237124308]
	TIME [epoch: 9.82 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8592525582050445		[learning rate: 0.0089748]
	Learning Rate: 0.00897482
	LOSS [training: 5.8592525582050445 | validation: 6.409410218430907]
	TIME [epoch: 9.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.894522059844695		[learning rate: 0.0089536]
	Learning Rate: 0.00895365
	LOSS [training: 5.894522059844695 | validation: 6.569755713651528]
	TIME [epoch: 9.79 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.5477670473989225		[learning rate: 0.0089325]
	Learning Rate: 0.00893253
	LOSS [training: 6.5477670473989225 | validation: 7.433113668729418]
	TIME [epoch: 9.81 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.279804977731595		[learning rate: 0.0089115]
	Learning Rate: 0.00891146
	LOSS [training: 6.279804977731595 | validation: 7.119465458122379]
	TIME [epoch: 9.79 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.112440850457405		[learning rate: 0.0088904]
	Learning Rate: 0.00889044
	LOSS [training: 6.112440850457405 | validation: 6.0740611643935125]
	TIME [epoch: 9.78 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.731551281610251		[learning rate: 0.0088695]
	Learning Rate: 0.00886946
	LOSS [training: 5.731551281610251 | validation: 6.17587887168343]
	TIME [epoch: 9.77 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.842184681744388		[learning rate: 0.0088485]
	Learning Rate: 0.00884854
	LOSS [training: 5.842184681744388 | validation: 6.49146964503693]
	TIME [epoch: 9.8 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.953831022422212		[learning rate: 0.0088277]
	Learning Rate: 0.00882767
	LOSS [training: 5.953831022422212 | validation: 6.396290798425289]
	TIME [epoch: 9.76 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.890670485572191		[learning rate: 0.0088068]
	Learning Rate: 0.00880685
	LOSS [training: 5.890670485572191 | validation: 6.114627958339948]
	TIME [epoch: 9.76 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.988297314423147		[learning rate: 0.0087861]
	Learning Rate: 0.00878607
	LOSS [training: 5.988297314423147 | validation: 6.9844356045000415]
	TIME [epoch: 9.76 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.743866902311747		[learning rate: 0.0087653]
	Learning Rate: 0.00876535
	LOSS [training: 5.743866902311747 | validation: 5.735951080155392]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.829422127198159		[learning rate: 0.0087447]
	Learning Rate: 0.00874467
	LOSS [training: 5.829422127198159 | validation: 6.270888545194167]
	TIME [epoch: 9.78 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.059589795639012		[learning rate: 0.008724]
	Learning Rate: 0.00872405
	LOSS [training: 6.059589795639012 | validation: 6.483332505022458]
	TIME [epoch: 9.77 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0610637910476965		[learning rate: 0.0087035]
	Learning Rate: 0.00870347
	LOSS [training: 6.0610637910476965 | validation: 6.741220109569347]
	TIME [epoch: 9.8 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.928835738610076		[learning rate: 0.0086829]
	Learning Rate: 0.00868294
	LOSS [training: 5.928835738610076 | validation: 6.061175245095568]
	TIME [epoch: 9.77 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.40499168472887		[learning rate: 0.0086625]
	Learning Rate: 0.00866246
	LOSS [training: 5.40499168472887 | validation: 5.945267827530829]
	TIME [epoch: 9.78 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.591711945161347		[learning rate: 0.008642]
	Learning Rate: 0.00864202
	LOSS [training: 5.591711945161347 | validation: 5.676035185361955]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.509454810454582		[learning rate: 0.0086216]
	Learning Rate: 0.00862164
	LOSS [training: 5.509454810454582 | validation: 5.891351782571035]
	TIME [epoch: 9.81 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.998361789114738		[learning rate: 0.0086013]
	Learning Rate: 0.0086013
	LOSS [training: 5.998361789114738 | validation: 6.321915804731551]
	TIME [epoch: 9.77 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.903478208159109		[learning rate: 0.008581]
	Learning Rate: 0.00858101
	LOSS [training: 5.903478208159109 | validation: 6.249559347634276]
	TIME [epoch: 9.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.777998083495083		[learning rate: 0.0085608]
	Learning Rate: 0.00856077
	LOSS [training: 5.777998083495083 | validation: 6.610241090786121]
	TIME [epoch: 9.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.221961280442389		[learning rate: 0.0085406]
	Learning Rate: 0.00854058
	LOSS [training: 6.221961280442389 | validation: 6.9080478055040055]
	TIME [epoch: 9.77 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.5312287760926795		[learning rate: 0.0085204]
	Learning Rate: 0.00852043
	LOSS [training: 7.5312287760926795 | validation: 7.758289744524125]
	TIME [epoch: 9.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.121720033696766		[learning rate: 0.0085003]
	Learning Rate: 0.00850033
	LOSS [training: 8.121720033696766 | validation: 7.666375108920074]
	TIME [epoch: 9.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.038483381909442		[learning rate: 0.0084803]
	Learning Rate: 0.00848028
	LOSS [training: 8.038483381909442 | validation: 7.2216080192430105]
	TIME [epoch: 9.78 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.970399022711679		[learning rate: 0.0084603]
	Learning Rate: 0.00846028
	LOSS [training: 6.970399022711679 | validation: 6.330489207674429]
	TIME [epoch: 9.78 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.986827186541759		[learning rate: 0.0084403]
	Learning Rate: 0.00844032
	LOSS [training: 5.986827186541759 | validation: 6.306334740820298]
	TIME [epoch: 9.78 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.8786507173923574		[learning rate: 0.0084204]
	Learning Rate: 0.00842041
	LOSS [training: 5.8786507173923574 | validation: 5.991199018304086]
	TIME [epoch: 9.78 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.666477897471557		[learning rate: 0.0084005]
	Learning Rate: 0.00840055
	LOSS [training: 5.666477897471557 | validation: 5.839716576247575]
	TIME [epoch: 9.81 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6873185519643075		[learning rate: 0.0083807]
	Learning Rate: 0.00838073
	LOSS [training: 5.6873185519643075 | validation: 5.949667623500388]
	TIME [epoch: 9.76 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.879698113689651		[learning rate: 0.008361]
	Learning Rate: 0.00836096
	LOSS [training: 5.879698113689651 | validation: 5.867217386716199]
	TIME [epoch: 9.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.94058477415291		[learning rate: 0.0083412]
	Learning Rate: 0.00834124
	LOSS [training: 5.94058477415291 | validation: 5.944487905492588]
	TIME [epoch: 9.77 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.166048884954382		[learning rate: 0.0083216]
	Learning Rate: 0.00832157
	LOSS [training: 6.166048884954382 | validation: 5.983486212749325]
	TIME [epoch: 9.79 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.733570259180534		[learning rate: 0.0083019]
	Learning Rate: 0.00830194
	LOSS [training: 5.733570259180534 | validation: 6.382203397285705]
	TIME [epoch: 9.76 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.604361439670033		[learning rate: 0.0082824]
	Learning Rate: 0.00828236
	LOSS [training: 5.604361439670033 | validation: 5.565030653870813]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.303312221419242		[learning rate: 0.0082628]
	Learning Rate: 0.00826282
	LOSS [training: 5.303312221419242 | validation: 5.5501619119190195]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.471804430396794		[learning rate: 0.0082433]
	Learning Rate: 0.00824333
	LOSS [training: 5.471804430396794 | validation: 6.464281195359757]
	TIME [epoch: 9.76 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.124336560947243		[learning rate: 0.0082239]
	Learning Rate: 0.00822388
	LOSS [training: 6.124336560947243 | validation: 6.714438360359207]
	TIME [epoch: 9.76 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.199846342795961		[learning rate: 0.0082045]
	Learning Rate: 0.00820448
	LOSS [training: 6.199846342795961 | validation: 6.416427760885898]
	TIME [epoch: 9.76 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.910087509106391		[learning rate: 0.0081851]
	Learning Rate: 0.00818513
	LOSS [training: 5.910087509106391 | validation: 6.152238201097541]
	TIME [epoch: 9.78 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.380394380247766		[learning rate: 0.0081658]
	Learning Rate: 0.00816582
	LOSS [training: 5.380394380247766 | validation: 5.713569300403092]
	TIME [epoch: 9.76 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.372713950109004		[learning rate: 0.0081466]
	Learning Rate: 0.00814656
	LOSS [training: 5.372713950109004 | validation: 5.813719351671887]
	TIME [epoch: 9.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.306922223538068		[learning rate: 0.0081273]
	Learning Rate: 0.00812735
	LOSS [training: 5.306922223538068 | validation: 5.766300690092105]
	TIME [epoch: 9.76 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193837106217741		[learning rate: 0.0081082]
	Learning Rate: 0.00810817
	LOSS [training: 5.193837106217741 | validation: 5.560082013446789]
	TIME [epoch: 9.78 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.305875620286357		[learning rate: 0.008089]
	Learning Rate: 0.00808905
	LOSS [training: 5.305875620286357 | validation: 5.699067410814348]
	TIME [epoch: 9.75 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.177796347378124		[learning rate: 0.00807]
	Learning Rate: 0.00806997
	LOSS [training: 6.177796347378124 | validation: 6.717343792208887]
	TIME [epoch: 9.76 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.968367516901178		[learning rate: 0.0080509]
	Learning Rate: 0.00805093
	LOSS [training: 5.968367516901178 | validation: 7.038059622868648]
	TIME [epoch: 9.78 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.205314397027779		[learning rate: 0.0080319]
	Learning Rate: 0.00803194
	LOSS [training: 6.205314397027779 | validation: 6.81915516511049]
	TIME [epoch: 9.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.161440763261348		[learning rate: 0.008013]
	Learning Rate: 0.00801299
	LOSS [training: 6.161440763261348 | validation: 6.738964116508417]
	TIME [epoch: 9.76 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.167415368149294		[learning rate: 0.0079941]
	Learning Rate: 0.00799409
	LOSS [training: 6.167415368149294 | validation: 7.014419548815556]
	TIME [epoch: 9.76 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.203795203180417		[learning rate: 0.0079752]
	Learning Rate: 0.00797524
	LOSS [training: 6.203795203180417 | validation: 6.557654840042366]
	TIME [epoch: 9.78 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9341425315926415		[learning rate: 0.0079564]
	Learning Rate: 0.00795642
	LOSS [training: 5.9341425315926415 | validation: 6.463376685047663]
	TIME [epoch: 9.76 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.134194922081971		[learning rate: 0.0079377]
	Learning Rate: 0.00793766
	LOSS [training: 6.134194922081971 | validation: 7.060093316080606]
	TIME [epoch: 9.76 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.098896110019196		[learning rate: 0.0079189]
	Learning Rate: 0.00791893
	LOSS [training: 6.098896110019196 | validation: 6.3260967521872455]
	TIME [epoch: 9.76 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.950203344007501		[learning rate: 0.0079003]
	Learning Rate: 0.00790025
	LOSS [training: 5.950203344007501 | validation: 6.606281743550412]
	TIME [epoch: 9.79 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.34851430745711		[learning rate: 0.0078816]
	Learning Rate: 0.00788162
	LOSS [training: 6.34851430745711 | validation: 7.068487985557722]
	TIME [epoch: 9.76 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.024351735836011		[learning rate: 0.007863]
	Learning Rate: 0.00786303
	LOSS [training: 7.024351735836011 | validation: 6.732759346803127]
	TIME [epoch: 9.77 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.112380279298293		[learning rate: 0.0078445]
	Learning Rate: 0.00784448
	LOSS [training: 6.112380279298293 | validation: 6.704074612315074]
	TIME [epoch: 9.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.853138030639754		[learning rate: 0.007826]
	Learning Rate: 0.00782597
	LOSS [training: 5.853138030639754 | validation: 6.069638957158129]
	TIME [epoch: 9.78 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.757877583211803		[learning rate: 0.0078075]
	Learning Rate: 0.00780751
	LOSS [training: 5.757877583211803 | validation: 6.549959447754039]
	TIME [epoch: 9.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.168929771733152		[learning rate: 0.0077891]
	Learning Rate: 0.0077891
	LOSS [training: 6.168929771733152 | validation: 6.626302511615805]
	TIME [epoch: 9.76 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.92826028567003		[learning rate: 0.0077707]
	Learning Rate: 0.00777072
	LOSS [training: 5.92826028567003 | validation: 6.709444229540832]
	TIME [epoch: 9.78 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.189155076184378		[learning rate: 0.0077524]
	Learning Rate: 0.00775239
	LOSS [training: 6.189155076184378 | validation: 7.019821308510118]
	TIME [epoch: 9.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.067534228302248		[learning rate: 0.0077341]
	Learning Rate: 0.00773411
	LOSS [training: 6.067534228302248 | validation: 6.752601586899687]
	TIME [epoch: 9.77 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.284089063931136		[learning rate: 0.0077159]
	Learning Rate: 0.00771586
	LOSS [training: 6.284089063931136 | validation: 6.338353514361679]
	TIME [epoch: 9.76 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.862449471760315		[learning rate: 0.0076977]
	Learning Rate: 0.00769766
	LOSS [training: 5.862449471760315 | validation: 6.952276461269929]
	TIME [epoch: 9.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.992199052212545		[learning rate: 0.0076795]
	Learning Rate: 0.00767951
	LOSS [training: 5.992199052212545 | validation: 6.47413347294646]
	TIME [epoch: 9.77 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.998560717614042		[learning rate: 0.0076614]
	Learning Rate: 0.00766139
	LOSS [training: 5.998560717614042 | validation: 6.005479868376515]
	TIME [epoch: 9.76 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.425014335358114		[learning rate: 0.0076433]
	Learning Rate: 0.00764332
	LOSS [training: 6.425014335358114 | validation: 7.246236796303557]
	TIME [epoch: 9.76 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.3913807795580775		[learning rate: 0.0076253]
	Learning Rate: 0.00762529
	LOSS [training: 6.3913807795580775 | validation: 7.185756232972903]
	TIME [epoch: 9.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.136042779240799		[learning rate: 0.0076073]
	Learning Rate: 0.0076073
	LOSS [training: 6.136042779240799 | validation: 6.078680065454596]
	TIME [epoch: 9.76 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.987197821448534		[learning rate: 0.0075894]
	Learning Rate: 0.00758936
	LOSS [training: 5.987197821448534 | validation: 6.556583614945397]
	TIME [epoch: 9.76 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.165138549147589		[learning rate: 0.0075715]
	Learning Rate: 0.00757146
	LOSS [training: 6.165138549147589 | validation: 7.490485850331882]
	TIME [epoch: 9.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.419005321816425		[learning rate: 0.0075536]
	Learning Rate: 0.0075536
	LOSS [training: 6.419005321816425 | validation: 6.715306129127853]
	TIME [epoch: 9.76 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.883574834763701		[learning rate: 0.0075358]
	Learning Rate: 0.00753578
	LOSS [training: 5.883574834763701 | validation: 6.586139085809889]
	TIME [epoch: 9.75 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.965485037214255		[learning rate: 0.007518]
	Learning Rate: 0.007518
	LOSS [training: 5.965485037214255 | validation: 6.6672337246681685]
	TIME [epoch: 9.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9735335116870445		[learning rate: 0.0075003]
	Learning Rate: 0.00750027
	LOSS [training: 5.9735335116870445 | validation: 6.629749651264437]
	TIME [epoch: 9.78 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.253032615470047		[learning rate: 0.0074826]
	Learning Rate: 0.00748258
	LOSS [training: 6.253032615470047 | validation: 8.048752412847598]
	TIME [epoch: 9.76 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.102652669959088		[learning rate: 0.0074649]
	Learning Rate: 0.00746493
	LOSS [training: 8.102652669959088 | validation: 7.323946589341404]
	TIME [epoch: 9.76 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.25178374858393		[learning rate: 0.0074473]
	Learning Rate: 0.00744732
	LOSS [training: 7.25178374858393 | validation: 6.489133873694363]
	TIME [epoch: 9.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.125918035125159		[learning rate: 0.0074298]
	Learning Rate: 0.00742975
	LOSS [training: 6.125918035125159 | validation: 5.995675412360592]
	TIME [epoch: 9.79 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9314912438814975		[learning rate: 0.0074122]
	Learning Rate: 0.00741223
	LOSS [training: 5.9314912438814975 | validation: 5.976389880504457]
	TIME [epoch: 9.76 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.832139090294461		[learning rate: 0.0073947]
	Learning Rate: 0.00739474
	LOSS [training: 5.832139090294461 | validation: 6.924720058291079]
	TIME [epoch: 9.76 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.114469378513093		[learning rate: 0.0073773]
	Learning Rate: 0.0073773
	LOSS [training: 6.114469378513093 | validation: 6.83534302232446]
	TIME [epoch: 9.77 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.069395869955976		[learning rate: 0.0073599]
	Learning Rate: 0.0073599
	LOSS [training: 6.069395869955976 | validation: 6.536224936510666]
	TIME [epoch: 9.77 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.760314816363358		[learning rate: 0.0073425]
	Learning Rate: 0.00734254
	LOSS [training: 5.760314816363358 | validation: 6.823673498725068]
	TIME [epoch: 9.75 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.07455505311548		[learning rate: 0.0073252]
	Learning Rate: 0.00732522
	LOSS [training: 6.07455505311548 | validation: 6.938313761932605]
	TIME [epoch: 9.75 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.158501995835094		[learning rate: 0.0073079]
	Learning Rate: 0.00730794
	LOSS [training: 6.158501995835094 | validation: 6.807278784280932]
	TIME [epoch: 9.79 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0661579436792		[learning rate: 0.0072907]
	Learning Rate: 0.0072907
	LOSS [training: 6.0661579436792 | validation: 7.015186347641702]
	TIME [epoch: 9.76 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1539720256724255		[learning rate: 0.0072735]
	Learning Rate: 0.0072735
	LOSS [training: 6.1539720256724255 | validation: 6.6783538352950895]
	TIME [epoch: 9.76 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.010554840228842		[learning rate: 0.0072563]
	Learning Rate: 0.00725635
	LOSS [training: 6.010554840228842 | validation: 6.676541072960027]
	TIME [epoch: 9.76 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.001415845078199		[learning rate: 0.0072392]
	Learning Rate: 0.00723923
	LOSS [training: 6.001415845078199 | validation: 6.858407568986881]
	TIME [epoch: 9.78 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.950725475773135		[learning rate: 0.0072222]
	Learning Rate: 0.00722215
	LOSS [training: 5.950725475773135 | validation: 6.803551185508118]
	TIME [epoch: 9.76 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.948671622692357		[learning rate: 0.0072051]
	Learning Rate: 0.00720512
	LOSS [training: 5.948671622692357 | validation: 6.78534449155898]
	TIME [epoch: 9.76 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0516407201185105		[learning rate: 0.0071881]
	Learning Rate: 0.00718812
	LOSS [training: 6.0516407201185105 | validation: 6.732377181065549]
	TIME [epoch: 9.76 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.76244709566406		[learning rate: 0.0071712]
	Learning Rate: 0.00717117
	LOSS [training: 7.76244709566406 | validation: 7.969323677490434]
	TIME [epoch: 9.78 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.261331971066728		[learning rate: 0.0071542]
	Learning Rate: 0.00715425
	LOSS [training: 8.261331971066728 | validation: 8.114524829774455]
	TIME [epoch: 9.76 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.100605625019663		[learning rate: 0.0071374]
	Learning Rate: 0.00713737
	LOSS [training: 8.100605625019663 | validation: 7.77222695851531]
	TIME [epoch: 9.76 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.677464354655773		[learning rate: 0.0071205]
	Learning Rate: 0.00712054
	LOSS [training: 7.677464354655773 | validation: 7.378331074656797]
	TIME [epoch: 9.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.779638801721762		[learning rate: 0.0071037]
	Learning Rate: 0.00710374
	LOSS [training: 6.779638801721762 | validation: 6.383436087982554]
	TIME [epoch: 9.76 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.792918503073565		[learning rate: 0.007087]
	Learning Rate: 0.00708698
	LOSS [training: 5.792918503073565 | validation: 6.104423466082338]
	TIME [epoch: 9.75 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.752016074889558		[learning rate: 0.0070703]
	Learning Rate: 0.00707027
	LOSS [training: 5.752016074889558 | validation: 6.168639632812395]
	TIME [epoch: 9.76 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.554412965746178		[learning rate: 0.0070536]
	Learning Rate: 0.00705359
	LOSS [training: 6.554412965746178 | validation: 7.024179952928109]
	TIME [epoch: 9.78 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.221166756408454		[learning rate: 0.007037]
	Learning Rate: 0.00703695
	LOSS [training: 6.221166756408454 | validation: 7.015967909729825]
	TIME [epoch: 9.75 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.123814287990477		[learning rate: 0.0070204]
	Learning Rate: 0.00702035
	LOSS [training: 6.123814287990477 | validation: 6.6730905245465735]
	TIME [epoch: 9.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.435510278165924		[learning rate: 0.0070038]
	Learning Rate: 0.00700379
	LOSS [training: 6.435510278165924 | validation: 6.9818189176250325]
	TIME [epoch: 9.76 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.216225226315607		[learning rate: 0.0069873]
	Learning Rate: 0.00698727
	LOSS [training: 6.216225226315607 | validation: 6.971074409903755]
	TIME [epoch: 9.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.116882364201335		[learning rate: 0.0069708]
	Learning Rate: 0.00697079
	LOSS [training: 6.116882364201335 | validation: 6.841775863253299]
	TIME [epoch: 9.75 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.08198754244291		[learning rate: 0.0069543]
	Learning Rate: 0.00695435
	LOSS [training: 6.08198754244291 | validation: 6.895702311838529]
	TIME [epoch: 9.75 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1454937449944085		[learning rate: 0.0069379]
	Learning Rate: 0.00693794
	LOSS [training: 6.1454937449944085 | validation: 6.765407988119667]
	TIME [epoch: 9.76 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.0493348449884135		[learning rate: 0.0069216]
	Learning Rate: 0.00692158
	LOSS [training: 6.0493348449884135 | validation: 6.658638898935408]
	TIME [epoch: 9.76 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.9677182774502056		[learning rate: 0.0069053]
	Learning Rate: 0.00690525
	LOSS [training: 5.9677182774502056 | validation: 6.63246393933721]
	TIME [epoch: 9.76 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.007843991267388		[learning rate: 0.006889]
	Learning Rate: 0.00688896
	LOSS [training: 6.007843991267388 | validation: 6.6745409426985525]
	TIME [epoch: 9.76 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.015999605462078		[learning rate: 0.0068727]
	Learning Rate: 0.00687271
	LOSS [training: 7.015999605462078 | validation: 7.111559564183927]
	TIME [epoch: 9.78 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.339773478698815		[learning rate: 0.0068565]
	Learning Rate: 0.0068565
	LOSS [training: 6.339773478698815 | validation: 6.967969098726059]
	TIME [epoch: 9.76 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.178621557634463		[learning rate: 0.0068403]
	Learning Rate: 0.00684033
	LOSS [training: 6.178621557634463 | validation: 6.027798639719463]
	TIME [epoch: 9.75 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.4399156355423886		[learning rate: 0.0068242]
	Learning Rate: 0.00682419
	LOSS [training: 6.4399156355423886 | validation: 8.474086476752149]
	TIME [epoch: 9.75 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.138381471031641		[learning rate: 0.0068081]
	Learning Rate: 0.0068081
	LOSS [training: 7.138381471031641 | validation: 6.23497655270365]
	TIME [epoch: 9.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.429512333109654		[learning rate: 0.006792]
	Learning Rate: 0.00679204
	LOSS [training: 6.429512333109654 | validation: 6.290820620033847]
	TIME [epoch: 9.76 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.932181134627796		[learning rate: 0.006776]
	Learning Rate: 0.00677601
	LOSS [training: 5.932181134627796 | validation: 7.509203051177269]
	TIME [epoch: 9.76 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.099990299898485		[learning rate: 0.00676]
	Learning Rate: 0.00676003
	LOSS [training: 7.099990299898485 | validation: 7.031353163831106]
	TIME [epoch: 9.76 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.156134194595398		[learning rate: 0.0067441]
	Learning Rate: 0.00674409
	LOSS [training: 6.156134194595398 | validation: 6.173103905498079]
	TIME [epoch: 9.77 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.091706737427481		[learning rate: 0.0067282]
	Learning Rate: 0.00672818
	LOSS [training: 6.091706737427481 | validation: 7.062791782792126]
	TIME [epoch: 9.76 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.34245153875158		[learning rate: 0.0067123]
	Learning Rate: 0.00671231
	LOSS [training: 6.34245153875158 | validation: 7.034618903177407]
	TIME [epoch: 9.75 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.234917437591339		[learning rate: 0.0066965]
	Learning Rate: 0.00669647
	LOSS [training: 6.234917437591339 | validation: 6.796163382614484]
	TIME [epoch: 9.77 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.826528459449364		[learning rate: 0.0066807]
	Learning Rate: 0.00668068
	LOSS [training: 5.826528459449364 | validation: 6.220327234136733]
	TIME [epoch: 9.76 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.551407783042135		[learning rate: 0.0066649]
	Learning Rate: 0.00666492
	LOSS [training: 5.551407783042135 | validation: 5.314685126846952]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234005068178351		[learning rate: 0.0066492]
	Learning Rate: 0.0066492
	LOSS [training: 5.234005068178351 | validation: 5.268375613740296]
	TIME [epoch: 9.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.966796240004406		[learning rate: 0.0066335]
	Learning Rate: 0.00663351
	LOSS [training: 4.966796240004406 | validation: 5.722996551840819]
	TIME [epoch: 9.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.864736596057599		[learning rate: 0.0066179]
	Learning Rate: 0.00661787
	LOSS [training: 4.864736596057599 | validation: 5.296170785040081]
	TIME [epoch: 9.79 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8778090082735766		[learning rate: 0.0066023]
	Learning Rate: 0.00660226
	LOSS [training: 4.8778090082735766 | validation: 5.0856090281932325]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study5/model_tr_study5_r2_20240219_184940/states/model_tr_study5_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781472016366338		[learning rate: 0.0065867]
	Learning Rate: 0.00658668
	LOSS [training: 4.781472016366338 | validation: 5.321239363424055]
	TIME [epoch: 9.78 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.867887743214856		[learning rate: 0.0065711]
	Learning Rate: 0.00657114
	LOSS [training: 6.867887743214856 | validation: 8.294669861761978]
	TIME [epoch: 9.76 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.367078499815836		[learning rate: 0.0065556]
	Learning Rate: 0.00655564
	LOSS [training: 9.367078499815836 | validation: 9.835137845605425]
	TIME [epoch: 9.75 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.287482334016543		[learning rate: 0.0065402]
	Learning Rate: 0.00654018
	LOSS [training: 7.287482334016543 | validation: 6.7962053822036115]
	TIME [epoch: 9.75 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.220468668650017		[learning rate: 0.0065248]
	Learning Rate: 0.00652475
	LOSS [training: 6.220468668650017 | validation: 6.75167277720122]
	TIME [epoch: 9.78 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.263429149384898		[learning rate: 0.0065094]
	Learning Rate: 0.00650936
	LOSS [training: 7.263429149384898 | validation: 7.47645402447488]
	TIME [epoch: 9.76 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.693059653007651		[learning rate: 0.006494]
	Learning Rate: 0.00649401
	LOSS [training: 7.693059653007651 | validation: 7.1806325126111075]
	TIME [epoch: 9.76 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.542022885235795		[learning rate: 0.0064787]
	Learning Rate: 0.00647869
	LOSS [training: 6.542022885235795 | validation: 6.48165320748255]
	TIME [epoch: 9.75 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.2048428268856455		[learning rate: 0.0064634]
	Learning Rate: 0.00646341
	LOSS [training: 6.2048428268856455 | validation: 6.351312659673986]
	TIME [epoch: 9.77 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.979236829938255		[learning rate: 0.0064482]
	Learning Rate: 0.00644816
	LOSS [training: 5.979236829938255 | validation: 7.50353651564604]
	TIME [epoch: 9.76 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.952672774994907		[learning rate: 0.006433]
	Learning Rate: 0.00643295
	LOSS [training: 6.952672774994907 | validation: 6.097392046787434]
	TIME [epoch: 9.76 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.636103299900927		[learning rate: 0.0064178]
	Learning Rate: 0.00641778
	LOSS [training: 5.636103299900927 | validation: 6.156536841378793]
	TIME [epoch: 9.77 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.616633376594166		[learning rate: 0.0064026]
	Learning Rate: 0.00640264
	LOSS [training: 5.616633376594166 | validation: 6.05064289384844]
	TIME [epoch: 9.78 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.651630100675767		[learning rate: 0.0063875]
	Learning Rate: 0.00638754
	LOSS [training: 5.651630100675767 | validation: 6.1442294466407406]
	TIME [epoch: 9.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.686149827167169		[learning rate: 0.0063725]
	Learning Rate: 0.00637247
	LOSS [training: 5.686149827167169 | validation: 6.142181868473915]
	TIME [epoch: 9.75 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.657868777438341		[learning rate: 0.0063574]
	Learning Rate: 0.00635744
	LOSS [training: 5.657868777438341 | validation: 6.088417463616338]
	TIME [epoch: 9.78 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.707907883937472		[learning rate: 0.0063424]
	Learning Rate: 0.00634244
	LOSS [training: 5.707907883937472 | validation: 6.128517750504265]
	TIME [epoch: 9.76 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.715947638089555		[learning rate: 0.0063275]
	Learning Rate: 0.00632748
	LOSS [training: 5.715947638089555 | validation: 6.163493939089997]
	TIME [epoch: 9.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.680562201812516		[learning rate: 0.0063126]
	Learning Rate: 0.00631255
	LOSS [training: 5.680562201812516 | validation: 6.19980205380326]
	TIME [epoch: 9.76 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.789940497700637		[learning rate: 0.0062977]
	Learning Rate: 0.00629766
	LOSS [training: 5.789940497700637 | validation: 6.06384650232513]
	TIME [epoch: 9.78 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.821279886362165		[learning rate: 0.0062828]
	Learning Rate: 0.00628281
	LOSS [training: 5.821279886362165 | validation: 6.205757945073729]
	TIME [epoch: 9.77 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.667215767341296		[learning rate: 0.006268]
	Learning Rate: 0.00626799
	LOSS [training: 5.667215767341296 | validation: 6.140656091850097]
	TIME [epoch: 9.76 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.673569454229051		[learning rate: 0.0062532]
	Learning Rate: 0.0062532
	LOSS [training: 5.673569454229051 | validation: 6.073880852060499]
	TIME [epoch: 9.77 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.627661727632045		[learning rate: 0.0062385]
	Learning Rate: 0.00623845
	LOSS [training: 5.627661727632045 | validation: 6.116117567775307]
	TIME [epoch: 9.78 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.60435133667268		[learning rate: 0.0062237]
	Learning Rate: 0.00622374
	LOSS [training: 5.60435133667268 | validation: 6.239956701851861]
	TIME [epoch: 9.76 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.688135991277248		[learning rate: 0.0062091]
	Learning Rate: 0.00620906
	LOSS [training: 5.688135991277248 | validation: 6.200356170986772]
	TIME [epoch: 9.76 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.651038696131534		[learning rate: 0.0061944]
	Learning Rate: 0.00619441
	LOSS [training: 5.651038696131534 | validation: 6.105218755007411]
	TIME [epoch: 9.79 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.599008993144176		[learning rate: 0.0061798]
	Learning Rate: 0.0061798
	LOSS [training: 5.599008993144176 | validation: 6.015126525232645]
	TIME [epoch: 9.76 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.646334782676545		[learning rate: 0.0061652]
	Learning Rate: 0.00616522
	LOSS [training: 5.646334782676545 | validation: 6.092518793637092]
	TIME [epoch: 9.76 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.686517946542941		[learning rate: 0.0061507]
	Learning Rate: 0.00615068
	LOSS [training: 5.686517946542941 | validation: 6.081237779108339]
	TIME [epoch: 9.77 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.761484862708729		[learning rate: 0.0061362]
	Learning Rate: 0.00613617
	LOSS [training: 5.761484862708729 | validation: 6.041575701918146]
	TIME [epoch: 9.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.641655179834303		[learning rate: 0.0061217]
	Learning Rate: 0.0061217
	LOSS [training: 5.641655179834303 | validation: 6.233436926198306]
	TIME [epoch: 9.76 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.645742355922765		[learning rate: 0.0061073]
	Learning Rate: 0.00610726
	LOSS [training: 5.645742355922765 | validation: 6.033697588805703]
	TIME [epoch: 9.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.534257083535166		[learning rate: 0.0060929]
	Learning Rate: 0.00609285
	LOSS [training: 5.534257083535166 | validation: 5.957476509760787]
	TIME [epoch: 9.76 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.505146008179732		[learning rate: 0.0060785]
	Learning Rate: 0.00607848
	LOSS [training: 5.505146008179732 | validation: 5.968774483198363]
	TIME [epoch: 9.79 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581340952210643		[learning rate: 0.0060641]
	Learning Rate: 0.00606414
	LOSS [training: 5.581340952210643 | validation: 6.027884176791954]
	TIME [epoch: 9.77 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.469968020459682		[learning rate: 0.0060498]
	Learning Rate: 0.00604984
	LOSS [training: 5.469968020459682 | validation: 5.969589259820877]
	TIME [epoch: 9.77 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.439111947836367		[learning rate: 0.0060356]
	Learning Rate: 0.00603556
	LOSS [training: 5.439111947836367 | validation: 5.980875715154549]
	TIME [epoch: 9.78 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.429166827087377		[learning rate: 0.0060213]
	Learning Rate: 0.00602133
	LOSS [training: 5.429166827087377 | validation: 5.883337130811161]
	TIME [epoch: 9.79 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4104053315397		[learning rate: 0.0060071]
	Learning Rate: 0.00600712
	LOSS [training: 5.4104053315397 | validation: 6.096372966499769]
	TIME [epoch: 9.78 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.44755221986765		[learning rate: 0.005993]
	Learning Rate: 0.00599296
	LOSS [training: 5.44755221986765 | validation: 5.891988399018258]
	TIME [epoch: 9.81 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.481226091541715		[learning rate: 0.0059788]
	Learning Rate: 0.00597882
	LOSS [training: 5.481226091541715 | validation: 5.8811809368279055]
	TIME [epoch: 9.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.420904257797955		[learning rate: 0.0059647]
	Learning Rate: 0.00596472
	LOSS [training: 5.420904257797955 | validation: 5.869408528659541]
	TIME [epoch: 9.78 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.413014001704468		[learning rate: 0.0059506]
	Learning Rate: 0.00595065
	LOSS [training: 5.413014001704468 | validation: 5.929394313945587]
	TIME [epoch: 9.77 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.436092384719611		[learning rate: 0.0059366]
	Learning Rate: 0.00593661
	LOSS [training: 5.436092384719611 | validation: 5.865430783441257]
	TIME [epoch: 9.76 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.42595536912707		[learning rate: 0.0059226]
	Learning Rate: 0.00592261
	LOSS [training: 5.42595536912707 | validation: 5.911341951964041]
	TIME [epoch: 9.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.430532309036587		[learning rate: 0.0059086]
	Learning Rate: 0.00590863
	LOSS [training: 5.430532309036587 | validation: 6.002145338804303]
	TIME [epoch: 9.76 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.411413571989793		[learning rate: 0.0058947]
	Learning Rate: 0.0058947
	LOSS [training: 5.411413571989793 | validation: 6.090265612172483]
	TIME [epoch: 9.77 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.55260764502644		[learning rate: 0.0058808]
	Learning Rate: 0.00588079
	LOSS [training: 5.55260764502644 | validation: 5.858272294239518]
	TIME [epoch: 9.78 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.484249906198859		[learning rate: 0.0058669]
	Learning Rate: 0.00586692
	LOSS [training: 5.484249906198859 | validation: 6.167922380988346]
	TIME [epoch: 9.79 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.478758556832416		[learning rate: 0.0058531]
	Learning Rate: 0.00585308
	LOSS [training: 5.478758556832416 | validation: 5.870747979644453]
	TIME [epoch: 9.78 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.395727396906534		[learning rate: 0.0058393]
	Learning Rate: 0.00583928
	LOSS [training: 5.395727396906534 | validation: 5.8774320684256125]
	TIME [epoch: 9.75 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.401269516968572		[learning rate: 0.0058255]
	Learning Rate: 0.0058255
	LOSS [training: 5.401269516968572 | validation: 5.910877779539078]
	TIME [epoch: 9.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.409070355264016		[learning rate: 0.0058118]
	Learning Rate: 0.00581176
	LOSS [training: 5.409070355264016 | validation: 5.935566313569272]
	TIME [epoch: 9.76 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.474374337313863		[learning rate: 0.0057981]
	Learning Rate: 0.00579805
	LOSS [training: 5.474374337313863 | validation: 5.899095698785007]
	TIME [epoch: 9.76 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.468567565975354		[learning rate: 0.0057844]
	Learning Rate: 0.00578438
	LOSS [training: 5.468567565975354 | validation: 5.911736081639726]
	TIME [epoch: 9.77 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.439866425022937		[learning rate: 0.0057707]
	Learning Rate: 0.00577073
	LOSS [training: 5.439866425022937 | validation: 5.851008567294919]
	TIME [epoch: 9.78 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.399626546063105		[learning rate: 0.0057571]
	Learning Rate: 0.00575712
	LOSS [training: 5.399626546063105 | validation: 5.956344674559862]
	TIME [epoch: 9.78 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.471943725547205		[learning rate: 0.0057435]
	Learning Rate: 0.00574354
	LOSS [training: 5.471943725547205 | validation: 5.825819450747721]
	TIME [epoch: 9.77 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.430979532082862		[learning rate: 0.00573]
	Learning Rate: 0.00572999
	LOSS [training: 5.430979532082862 | validation: 5.883466390105622]
	TIME [epoch: 9.77 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.418836301536393		[learning rate: 0.0057165]
	Learning Rate: 0.00571647
	LOSS [training: 5.418836301536393 | validation: 6.1224459362672805]
	TIME [epoch: 9.78 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.594284198624365		[learning rate: 0.005703]
	Learning Rate: 0.00570299
	LOSS [training: 5.594284198624365 | validation: 5.969485647231391]
	TIME [epoch: 9.76 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.576197536044981		[learning rate: 0.0056895]
	Learning Rate: 0.00568954
	LOSS [training: 5.576197536044981 | validation: 5.909543902097084]
	TIME [epoch: 9.76 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.486085178874401		[learning rate: 0.0056761]
	Learning Rate: 0.00567612
	LOSS [training: 5.486085178874401 | validation: 5.926898338135609]
	TIME [epoch: 9.79 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.524859115104478		[learning rate: 0.0056627]
	Learning Rate: 0.00566273
	LOSS [training: 5.524859115104478 | validation: 5.8542699526738895]
	TIME [epoch: 9.75 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.481277387088832		[learning rate: 0.0056494]
	Learning Rate: 0.00564937
	LOSS [training: 5.481277387088832 | validation: 5.904549469772412]
	TIME [epoch: 9.77 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.44433558146874		[learning rate: 0.005636]
	Learning Rate: 0.00563604
	LOSS [training: 5.44433558146874 | validation: 5.923384178096435]
	TIME [epoch: 9.77 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.427530663285009		[learning rate: 0.0056227]
	Learning Rate: 0.00562275
	LOSS [training: 5.427530663285009 | validation: 6.109798225134244]
	TIME [epoch: 9.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.518529747022043		[learning rate: 0.0056095]
	Learning Rate: 0.00560949
	LOSS [training: 5.518529747022043 | validation: 5.8520250884468465]
	TIME [epoch: 9.76 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4682855555873005		[learning rate: 0.0055963]
	Learning Rate: 0.00559625
	LOSS [training: 5.4682855555873005 | validation: 5.968756831781497]
	TIME [epoch: 9.77 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4103477953721795		[learning rate: 0.0055831]
	Learning Rate: 0.00558305
	LOSS [training: 5.4103477953721795 | validation: 6.098466887033063]
	TIME [epoch: 9.77 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.475906501989151		[learning rate: 0.0055699]
	Learning Rate: 0.00556988
	LOSS [training: 5.475906501989151 | validation: 5.871235985649732]
	TIME [epoch: 9.79 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.461440470947346		[learning rate: 0.0055567]
	Learning Rate: 0.00555674
	LOSS [training: 5.461440470947346 | validation: 5.96030119070599]
	TIME [epoch: 9.77 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.532146619346339		[learning rate: 0.0055436]
	Learning Rate: 0.00554364
	LOSS [training: 5.532146619346339 | validation: 6.253380639061283]
	TIME [epoch: 9.77 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.764739789885468		[learning rate: 0.0055306]
	Learning Rate: 0.00553056
	LOSS [training: 5.764739789885468 | validation: 6.20448862040888]
	TIME [epoch: 9.77 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.693342675737918		[learning rate: 0.0055175]
	Learning Rate: 0.00551752
	LOSS [training: 5.693342675737918 | validation: 6.038174387955251]
	TIME [epoch: 9.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.731784686356329		[learning rate: 0.0055045]
	Learning Rate: 0.0055045
	LOSS [training: 5.731784686356329 | validation: 6.681509743608249]
	TIME [epoch: 9.77 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.830864290704364		[learning rate: 0.0054915]
	Learning Rate: 0.00549152
	LOSS [training: 5.830864290704364 | validation: 5.9901155426934]
	TIME [epoch: 9.77 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.544972852075144		[learning rate: 0.0054786]
	Learning Rate: 0.00547856
	LOSS [training: 5.544972852075144 | validation: 5.971531355777638]
	TIME [epoch: 9.79 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.512842000881304		[learning rate: 0.0054656]
	Learning Rate: 0.00546564
	LOSS [training: 5.512842000881304 | validation: 6.065115856365601]
	TIME [epoch: 9.77 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.545674635059697		[learning rate: 0.0054527]
	Learning Rate: 0.00545275
	LOSS [training: 5.545674635059697 | validation: 5.91674133895106]
	TIME [epoch: 9.77 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.567743663166494		[learning rate: 0.0054399]
	Learning Rate: 0.00543988
	LOSS [training: 5.567743663166494 | validation: 6.064449779307495]
	TIME [epoch: 9.76 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.548860541228089		[learning rate: 0.0054271]
	Learning Rate: 0.00542705
	LOSS [training: 5.548860541228089 | validation: 5.897200448046808]
	TIME [epoch: 9.81 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.568215710859071		[learning rate: 0.0054143]
	Learning Rate: 0.00541425
	LOSS [training: 5.568215710859071 | validation: 5.889881641373222]
	TIME [epoch: 9.79 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.404749321070791		[learning rate: 0.0054015]
	Learning Rate: 0.00540148
	LOSS [training: 5.404749321070791 | validation: 5.8154010527098965]
	TIME [epoch: 9.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.720284360223334		[learning rate: 0.0053887]
	Learning Rate: 0.00538874
	LOSS [training: 5.720284360223334 | validation: 7.1204624793900635]
	TIME [epoch: 9.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.12397871239484		[learning rate: 0.005376]
	Learning Rate: 0.00537603
	LOSS [training: 6.12397871239484 | validation: 6.005616165267449]
	TIME [epoch: 9.8 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4734160240476255		[learning rate: 0.0053633]
	Learning Rate: 0.00536335
	LOSS [training: 5.4734160240476255 | validation: 5.952478660126043]
	TIME [epoch: 9.77 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.425706049401069		[learning rate: 0.0053507]
	Learning Rate: 0.00535069
	LOSS [training: 5.425706049401069 | validation: 5.947759832133222]
	TIME [epoch: 9.76 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5158704595807535		[learning rate: 0.0053381]
	Learning Rate: 0.00533807
	LOSS [training: 5.5158704595807535 | validation: 5.876998121295128]
	TIME [epoch: 9.78 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.437210649362173		[learning rate: 0.0053255]
	Learning Rate: 0.00532548
	LOSS [training: 5.437210649362173 | validation: 5.85084723219681]
	TIME [epoch: 9.76 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.273193170823976		[learning rate: 0.0053129]
	Learning Rate: 0.00531292
	LOSS [training: 5.273193170823976 | validation: 5.7998369919141854]
	TIME [epoch: 9.76 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.262275344727799		[learning rate: 0.0053004]
	Learning Rate: 0.00530039
	LOSS [training: 5.262275344727799 | validation: 5.881151995988808]
	TIME [epoch: 9.76 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.287458291059567		[learning rate: 0.0052879]
	Learning Rate: 0.00528789
	LOSS [training: 5.287458291059567 | validation: 5.81851607595118]
	TIME [epoch: 9.78 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.301479379497184		[learning rate: 0.0052754]
	Learning Rate: 0.00527541
	LOSS [training: 5.301479379497184 | validation: 5.754452119702662]
	TIME [epoch: 9.76 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2763062495245086		[learning rate: 0.005263]
	Learning Rate: 0.00526297
	LOSS [training: 5.2763062495245086 | validation: 5.889085074800894]
	TIME [epoch: 9.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.344973622658615		[learning rate: 0.0052506]
	Learning Rate: 0.00525055
	LOSS [training: 5.344973622658615 | validation: 5.7786852093622745]
	TIME [epoch: 9.77 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.288640109617164		[learning rate: 0.0052382]
	Learning Rate: 0.00523817
	LOSS [training: 5.288640109617164 | validation: 5.838794003598694]
	TIME [epoch: 9.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272128213603488		[learning rate: 0.0052258]
	Learning Rate: 0.00522581
	LOSS [training: 5.272128213603488 | validation: 5.79407211428216]
	TIME [epoch: 9.76 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.349472385072765		[learning rate: 0.0052135]
	Learning Rate: 0.00521349
	LOSS [training: 5.349472385072765 | validation: 5.860839238804547]
	TIME [epoch: 9.77 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.274521009497936		[learning rate: 0.0052012]
	Learning Rate: 0.00520119
	LOSS [training: 5.274521009497936 | validation: 5.78046872307874]
	TIME [epoch: 9.79 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.272565264071558		[learning rate: 0.0051889]
	Learning Rate: 0.00518892
	LOSS [training: 5.272565264071558 | validation: 5.7776970557806475]
	TIME [epoch: 9.77 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.420747441031713		[learning rate: 0.0051767]
	Learning Rate: 0.00517668
	LOSS [training: 5.420747441031713 | validation: 5.814397569023251]
	TIME [epoch: 9.77 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.254199792145857		[learning rate: 0.0051645]
	Learning Rate: 0.00516447
	LOSS [training: 5.254199792145857 | validation: 5.78038159552166]
	TIME [epoch: 9.77 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.310278564717727		[learning rate: 0.0051523]
	Learning Rate: 0.00515229
	LOSS [training: 5.310278564717727 | validation: 5.830384295263586]
	TIME [epoch: 9.81 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30733555190876		[learning rate: 0.0051401]
	Learning Rate: 0.00514013
	LOSS [training: 5.30733555190876 | validation: 5.743379030896095]
	TIME [epoch: 9.77 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.312328215834594		[learning rate: 0.005128]
	Learning Rate: 0.00512801
	LOSS [training: 5.312328215834594 | validation: 5.793694826712274]
	TIME [epoch: 9.77 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.275928613700672		[learning rate: 0.0051159]
	Learning Rate: 0.00511591
	LOSS [training: 5.275928613700672 | validation: 5.763926940579043]
	TIME [epoch: 9.78 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.401766397913427		[learning rate: 0.0051038]
	Learning Rate: 0.00510384
	LOSS [training: 5.401766397913427 | validation: 5.891129748996953]
	TIME [epoch: 9.79 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.293698048040733		[learning rate: 0.0050918]
	Learning Rate: 0.00509181
	LOSS [training: 5.293698048040733 | validation: 5.834717554450305]
	TIME [epoch: 9.76 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.274261256175732		[learning rate: 0.0050798]
	Learning Rate: 0.00507979
	LOSS [training: 5.274261256175732 | validation: 6.000029346888423]
	TIME [epoch: 9.77 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.314371644526483		[learning rate: 0.0050678]
	Learning Rate: 0.00506781
	LOSS [training: 5.314371644526483 | validation: 5.764230612640097]
	TIME [epoch: 9.79 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.244245907334964		[learning rate: 0.0050559]
	Learning Rate: 0.00505586
	LOSS [training: 5.244245907334964 | validation: 5.755549697335666]
	TIME [epoch: 9.78 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.270136960076707		[learning rate: 0.0050439]
	Learning Rate: 0.00504393
	LOSS [training: 5.270136960076707 | validation: 5.742932214987956]
	TIME [epoch: 9.78 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3199592997598995		[learning rate: 0.005032]
	Learning Rate: 0.00503203
	LOSS [training: 5.3199592997598995 | validation: 5.795546982115389]
	TIME [epoch: 9.77 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.353990482762194		[learning rate: 0.0050202]
	Learning Rate: 0.00502016
	LOSS [training: 5.353990482762194 | validation: 5.771930925737604]
	TIME [epoch: 9.79 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.327637960354592		[learning rate: 0.0050083]
	Learning Rate: 0.00500832
	LOSS [training: 5.327637960354592 | validation: 5.731644419833194]
	TIME [epoch: 9.77 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.28647897892562		[learning rate: 0.0049965]
	Learning Rate: 0.00499651
	LOSS [training: 5.28647897892562 | validation: 5.702304676956163]
	TIME [epoch: 9.78 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.306935510276453		[learning rate: 0.0049847]
	Learning Rate: 0.00498472
	LOSS [training: 5.306935510276453 | validation: 5.709610431865525]
	TIME [epoch: 9.77 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.326894950946598		[learning rate: 0.004973]
	Learning Rate: 0.00497296
	LOSS [training: 5.326894950946598 | validation: 5.759648652953494]
	TIME [epoch: 9.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.35114739898018		[learning rate: 0.0049612]
	Learning Rate: 0.00496123
	LOSS [training: 5.35114739898018 | validation: 5.847733186092763]
	TIME [epoch: 9.77 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3024956251909545		[learning rate: 0.0049495]
	Learning Rate: 0.00494953
	LOSS [training: 5.3024956251909545 | validation: 5.726861847275372]
	TIME [epoch: 9.77 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.286283150676359		[learning rate: 0.0049379]
	Learning Rate: 0.00493786
	LOSS [training: 5.286283150676359 | validation: 5.744928652888966]
	TIME [epoch: 9.78 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.339324716575151		[learning rate: 0.0049262]
	Learning Rate: 0.00492621
	LOSS [training: 5.339324716575151 | validation: 6.145574053573519]
	TIME [epoch: 9.79 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.358761885152369		[learning rate: 0.0049146]
	Learning Rate: 0.00491459
	LOSS [training: 5.358761885152369 | validation: 5.808566650129028]
	TIME [epoch: 9.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.296544005749527		[learning rate: 0.004903]
	Learning Rate: 0.004903
	LOSS [training: 5.296544005749527 | validation: 5.718696814405814]
	TIME [epoch: 9.78 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.249682588110259		[learning rate: 0.0048914]
	Learning Rate: 0.00489143
	LOSS [training: 5.249682588110259 | validation: 5.807368366364987]
	TIME [epoch: 9.81 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.283629704816585		[learning rate: 0.0048799]
	Learning Rate: 0.00487989
	LOSS [training: 5.283629704816585 | validation: 5.835290497930146]
	TIME [epoch: 9.78 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3747328293137375		[learning rate: 0.0048684]
	Learning Rate: 0.00486838
	LOSS [training: 5.3747328293137375 | validation: 5.95609484939954]
	TIME [epoch: 9.79 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.342037906012476		[learning rate: 0.0048569]
	Learning Rate: 0.0048569
	LOSS [training: 5.342037906012476 | validation: 5.7125740321312435]
	TIME [epoch: 9.77 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207613370597305		[learning rate: 0.0048454]
	Learning Rate: 0.00484544
	LOSS [training: 5.207613370597305 | validation: 5.682364533272347]
	TIME [epoch: 9.83 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.199480425494075		[learning rate: 0.004834]
	Learning Rate: 0.00483401
	LOSS [training: 5.199480425494075 | validation: 5.680687197792094]
	TIME [epoch: 9.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.250686667379545		[learning rate: 0.0048226]
	Learning Rate: 0.00482261
	LOSS [training: 5.250686667379545 | validation: 5.7207983394491695]
	TIME [epoch: 9.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197589894881579		[learning rate: 0.0048112]
	Learning Rate: 0.00481123
	LOSS [training: 5.197589894881579 | validation: 5.68951622164097]
	TIME [epoch: 9.81 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.313762348581084		[learning rate: 0.0047999]
	Learning Rate: 0.00479988
	LOSS [training: 5.313762348581084 | validation: 5.809833812294137]
	TIME [epoch: 9.82 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2236828440870084		[learning rate: 0.0047886]
	Learning Rate: 0.00478856
	LOSS [training: 5.2236828440870084 | validation: 5.907169779300193]
	TIME [epoch: 9.77 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.269913817144064		[learning rate: 0.0047773]
	Learning Rate: 0.00477727
	LOSS [training: 5.269913817144064 | validation: 5.748801122469463]
	TIME [epoch: 9.77 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.23357904744659		[learning rate: 0.004766]
	Learning Rate: 0.004766
	LOSS [training: 5.23357904744659 | validation: 6.844204279172721]
	TIME [epoch: 9.83 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.545019189801954		[learning rate: 0.0047548]
	Learning Rate: 0.00475476
	LOSS [training: 5.545019189801954 | validation: 5.680171372847731]
	TIME [epoch: 9.79 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.365440494199118		[learning rate: 0.0047435]
	Learning Rate: 0.00474354
	LOSS [training: 5.365440494199118 | validation: 5.706804271304358]
	TIME [epoch: 9.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1944358038993075		[learning rate: 0.0047324]
	Learning Rate: 0.00473235
	LOSS [training: 5.1944358038993075 | validation: 5.840444185822886]
	TIME [epoch: 9.82 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223989472899544		[learning rate: 0.0047212]
	Learning Rate: 0.00472119
	LOSS [training: 5.223989472899544 | validation: 5.677342908685421]
	TIME [epoch: 9.84 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.193546429173535		[learning rate: 0.0047101]
	Learning Rate: 0.00471005
	LOSS [training: 5.193546429173535 | validation: 5.7368064149555105]
	TIME [epoch: 9.77 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2375500356890345		[learning rate: 0.0046989]
	Learning Rate: 0.00469894
	LOSS [training: 5.2375500356890345 | validation: 5.7130470777897315]
	TIME [epoch: 9.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.258362679643858		[learning rate: 0.0046879]
	Learning Rate: 0.00468786
	LOSS [training: 5.258362679643858 | validation: 5.81006569238958]
	TIME [epoch: 9.82 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.260517824115649		[learning rate: 0.0046768]
	Learning Rate: 0.0046768
	LOSS [training: 5.260517824115649 | validation: 5.854567229017391]
	TIME [epoch: 9.81 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.222246157181496		[learning rate: 0.0046658]
	Learning Rate: 0.00466577
	LOSS [training: 5.222246157181496 | validation: 5.801297800111144]
	TIME [epoch: 9.79 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.265597474234256		[learning rate: 0.0046548]
	Learning Rate: 0.00465476
	LOSS [training: 5.265597474234256 | validation: 5.6419642205714515]
	TIME [epoch: 9.79 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.253756437863499		[learning rate: 0.0046438]
	Learning Rate: 0.00464378
	LOSS [training: 5.253756437863499 | validation: 5.6756937023728575]
	TIME [epoch: 9.83 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.255738400607905		[learning rate: 0.0046328]
	Learning Rate: 0.00463283
	LOSS [training: 5.255738400607905 | validation: 5.740565179177877]
	TIME [epoch: 9.78 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.377654965964707		[learning rate: 0.0046219]
	Learning Rate: 0.0046219
	LOSS [training: 5.377654965964707 | validation: 5.639117355491634]
	TIME [epoch: 9.81 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.230392110933985		[learning rate: 0.004611]
	Learning Rate: 0.004611
	LOSS [training: 5.230392110933985 | validation: 5.719522191920555]
	TIME [epoch: 9.79 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.197349744489847		[learning rate: 0.0046001]
	Learning Rate: 0.00460012
	LOSS [training: 5.197349744489847 | validation: 5.708851337327344]
	TIME [epoch: 9.82 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.219325558700364		[learning rate: 0.0045893]
	Learning Rate: 0.00458927
	LOSS [training: 5.219325558700364 | validation: 5.618065756153339]
	TIME [epoch: 9.79 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.184254817458822		[learning rate: 0.0045784]
	Learning Rate: 0.00457844
	LOSS [training: 5.184254817458822 | validation: 5.638623771018704]
	TIME [epoch: 9.78 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2241116607905145		[learning rate: 0.0045676]
	Learning Rate: 0.00456765
	LOSS [training: 5.2241116607905145 | validation: 5.624990868987825]
	TIME [epoch: 9.81 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1346376448844975		[learning rate: 0.0045569]
	Learning Rate: 0.00455687
	LOSS [training: 5.1346376448844975 | validation: 5.617886039298301]
	TIME [epoch: 9.82 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.228643113254279		[learning rate: 0.0045461]
	Learning Rate: 0.00454612
	LOSS [training: 5.228643113254279 | validation: 5.6102420232943455]
	TIME [epoch: 9.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.143551458739094		[learning rate: 0.0045354]
	Learning Rate: 0.0045354
	LOSS [training: 5.143551458739094 | validation: 5.932604403811528]
	TIME [epoch: 9.77 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196223295336887		[learning rate: 0.0045247]
	Learning Rate: 0.0045247
	LOSS [training: 5.196223295336887 | validation: 5.659341201050288]
	TIME [epoch: 9.82 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156814957274686		[learning rate: 0.004514]
	Learning Rate: 0.00451403
	LOSS [training: 5.156814957274686 | validation: 5.668478933254227]
	TIME [epoch: 9.82 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.124797329428755		[learning rate: 0.0045034]
	Learning Rate: 0.00450338
	LOSS [training: 5.124797329428755 | validation: 5.666070555844954]
	TIME [epoch: 9.79 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.168290239010952		[learning rate: 0.0044928]
	Learning Rate: 0.00449276
	LOSS [training: 5.168290239010952 | validation: 5.652859642110144]
	TIME [epoch: 9.81 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.15217061756304		[learning rate: 0.0044822]
	Learning Rate: 0.00448216
	LOSS [training: 5.15217061756304 | validation: 5.613673576949262]
	TIME [epoch: 9.82 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.180809822421439		[learning rate: 0.0044716]
	Learning Rate: 0.00447159
	LOSS [training: 5.180809822421439 | validation: 5.769160894100044]
	TIME [epoch: 9.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.148957220812105		[learning rate: 0.004461]
	Learning Rate: 0.00446104
	LOSS [training: 5.148957220812105 | validation: 5.621580579827244]
	TIME [epoch: 9.81 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.13634702413908		[learning rate: 0.0044505]
	Learning Rate: 0.00445051
	LOSS [training: 5.13634702413908 | validation: 5.698055404606959]
	TIME [epoch: 9.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192155499453532		[learning rate: 0.00444]
	Learning Rate: 0.00444002
	LOSS [training: 5.192155499453532 | validation: 5.585515587143576]
	TIME [epoch: 9.83 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.136086476237129		[learning rate: 0.0044295]
	Learning Rate: 0.00442954
	LOSS [training: 5.136086476237129 | validation: 5.612885437238003]
	TIME [epoch: 9.81 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.14827259397256		[learning rate: 0.0044191]
	Learning Rate: 0.00441909
	LOSS [training: 5.14827259397256 | validation: 5.648690970275005]
	TIME [epoch: 9.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.253187742512502		[learning rate: 0.0044087]
	Learning Rate: 0.00440867
	LOSS [training: 5.253187742512502 | validation: 5.643371712575215]
	TIME [epoch: 9.83 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170537442522269		[learning rate: 0.0043983]
	Learning Rate: 0.00439827
	LOSS [training: 5.170537442522269 | validation: 5.65589847386846]
	TIME [epoch: 9.83 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.128970948701747		[learning rate: 0.0043879]
	Learning Rate: 0.0043879
	LOSS [training: 5.128970948701747 | validation: 5.6490543986433295]
	TIME [epoch: 9.81 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.223710655977837		[learning rate: 0.0043775]
	Learning Rate: 0.00437755
	LOSS [training: 5.223710655977837 | validation: 5.577236893325622]
	TIME [epoch: 9.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.116899083556705		[learning rate: 0.0043672]
	Learning Rate: 0.00436722
	LOSS [training: 5.116899083556705 | validation: 5.631360848082568]
	TIME [epoch: 9.83 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.141609343265786		[learning rate: 0.0043569]
	Learning Rate: 0.00435692
	LOSS [training: 5.141609343265786 | validation: 5.547828452328743]
	TIME [epoch: 9.81 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.171420380754923		[learning rate: 0.0043466]
	Learning Rate: 0.00434664
	LOSS [training: 5.171420380754923 | validation: 5.658179566431913]
	TIME [epoch: 9.81 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.233308289971193		[learning rate: 0.0043364]
	Learning Rate: 0.00433639
	LOSS [training: 5.233308289971193 | validation: 5.667312785864699]
	TIME [epoch: 9.81 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.147003047967845		[learning rate: 0.0043262]
	Learning Rate: 0.00432616
	LOSS [training: 5.147003047967845 | validation: 5.727379145741259]
	TIME [epoch: 9.84 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.131816589062689		[learning rate: 0.004316]
	Learning Rate: 0.00431596
	LOSS [training: 5.131816589062689 | validation: 5.597004118224324]
	TIME [epoch: 9.82 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0962543852063105		[learning rate: 0.0043058]
	Learning Rate: 0.00430577
	LOSS [training: 5.0962543852063105 | validation: 5.585741511913741]
	TIME [epoch: 9.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.6133200094127265		[learning rate: 0.0042956]
	Learning Rate: 0.00429562
	LOSS [training: 5.6133200094127265 | validation: 5.866250350324482]
	TIME [epoch: 9.82 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.436586215450956		[learning rate: 0.0042855]
	Learning Rate: 0.00428549
	LOSS [training: 5.436586215450956 | validation: 5.666348618444695]
	TIME [epoch: 9.85 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.234720409617898		[learning rate: 0.0042754]
	Learning Rate: 0.00427538
	LOSS [training: 5.234720409617898 | validation: 5.5952332695669895]
	TIME [epoch: 9.81 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156457838464544		[learning rate: 0.0042653]
	Learning Rate: 0.00426529
	LOSS [training: 5.156457838464544 | validation: 5.774162940532985]
	TIME [epoch: 9.81 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.218981658677279		[learning rate: 0.0042552]
	Learning Rate: 0.00425523
	LOSS [training: 5.218981658677279 | validation: 5.5922529340887674]
	TIME [epoch: 9.83 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.178489221430881		[learning rate: 0.0042452]
	Learning Rate: 0.00424519
	LOSS [training: 5.178489221430881 | validation: 5.63445258009116]
	TIME [epoch: 9.83 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.222510257132234		[learning rate: 0.0042352]
	Learning Rate: 0.00423518
	LOSS [training: 5.222510257132234 | validation: 5.625943474971202]
	TIME [epoch: 9.81 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152996359851093		[learning rate: 0.0042252]
	Learning Rate: 0.00422519
	LOSS [training: 5.152996359851093 | validation: 5.5644999074031265]
	TIME [epoch: 9.81 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.117021122196395		[learning rate: 0.0042152]
	Learning Rate: 0.00421522
	LOSS [training: 5.117021122196395 | validation: 5.606891886850156]
	TIME [epoch: 9.82 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.060948182968487		[learning rate: 0.0042053]
	Learning Rate: 0.00420528
	LOSS [training: 5.060948182968487 | validation: 5.626266915298595]
	TIME [epoch: 9.81 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.170305276518912		[learning rate: 0.0041954]
	Learning Rate: 0.00419536
	LOSS [training: 5.170305276518912 | validation: 5.571211081556919]
	TIME [epoch: 9.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.056513661637529		[learning rate: 0.0041855]
	Learning Rate: 0.00418546
	LOSS [training: 5.056513661637529 | validation: 5.635775643829698]
	TIME [epoch: 9.82 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.209551666873331		[learning rate: 0.0041756]
	Learning Rate: 0.00417559
	LOSS [training: 5.209551666873331 | validation: 5.489028466915511]
	TIME [epoch: 9.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.120553968572982		[learning rate: 0.0041657]
	Learning Rate: 0.00416574
	LOSS [training: 5.120553968572982 | validation: 5.739392119189894]
	TIME [epoch: 9.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.236799242769237		[learning rate: 0.0041559]
	Learning Rate: 0.00415592
	LOSS [training: 5.236799242769237 | validation: 5.5423055397359455]
	TIME [epoch: 9.8 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.118719231569476		[learning rate: 0.0041461]
	Learning Rate: 0.00414611
	LOSS [training: 5.118719231569476 | validation: 5.57365598513747]
	TIME [epoch: 9.82 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.164211684754404		[learning rate: 0.0041363]
	Learning Rate: 0.00413633
	LOSS [training: 5.164211684754404 | validation: 5.672518610319024]
	TIME [epoch: 9.83 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.383683835430456		[learning rate: 0.0041266]
	Learning Rate: 0.00412657
	LOSS [training: 5.383683835430456 | validation: 6.1055565472336415]
	TIME [epoch: 9.82 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.376029630987605		[learning rate: 0.0041168]
	Learning Rate: 0.00411684
	LOSS [training: 5.376029630987605 | validation: 5.645458873480207]
	TIME [epoch: 9.79 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.290712225001342		[learning rate: 0.0041071]
	Learning Rate: 0.00410713
	LOSS [training: 5.290712225001342 | validation: 5.673083217195065]
	TIME [epoch: 9.84 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.199115483629267		[learning rate: 0.0040974]
	Learning Rate: 0.00409744
	LOSS [training: 5.199115483629267 | validation: 5.600408299755153]
	TIME [epoch: 9.82 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0886615943918		[learning rate: 0.0040878]
	Learning Rate: 0.00408778
	LOSS [training: 5.0886615943918 | validation: 5.788077762797002]
	TIME [epoch: 9.79 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.099323515724068		[learning rate: 0.0040781]
	Learning Rate: 0.00407813
	LOSS [training: 5.099323515724068 | validation: 5.619259808475098]
	TIME [epoch: 9.81 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.08070573039411		[learning rate: 0.0040685]
	Learning Rate: 0.00406851
	LOSS [training: 5.08070573039411 | validation: 5.5176879759783075]
	TIME [epoch: 9.83 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16023312159005		[learning rate: 0.0040589]
	Learning Rate: 0.00405892
	LOSS [training: 5.16023312159005 | validation: 6.260159957535852]
	TIME [epoch: 9.81 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5720069122119895		[learning rate: 0.0040493]
	Learning Rate: 0.00404934
	LOSS [training: 5.5720069122119895 | validation: 5.560501853644068]
	TIME [epoch: 9.81 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.16745629974902		[learning rate: 0.0040398]
	Learning Rate: 0.00403979
	LOSS [training: 5.16745629974902 | validation: 5.699258477234876]
	TIME [epoch: 9.84 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.088753978836703		[learning rate: 0.0040303]
	Learning Rate: 0.00403026
	LOSS [training: 5.088753978836703 | validation: 5.588475333052094]
	TIME [epoch: 9.82 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.052392007990344		[learning rate: 0.0040208]
	Learning Rate: 0.00402076
	LOSS [training: 5.052392007990344 | validation: 5.673677416877614]
	TIME [epoch: 9.82 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.06484078535037		[learning rate: 0.0040113]
	Learning Rate: 0.00401127
	LOSS [training: 5.06484078535037 | validation: 5.563556315740442]
	TIME [epoch: 9.82 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.063698604531055		[learning rate: 0.0040018]
	Learning Rate: 0.00400181
	LOSS [training: 5.063698604531055 | validation: 5.5909465041776425]
	TIME [epoch: 9.84 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.072429271843872		[learning rate: 0.0039924]
	Learning Rate: 0.00399237
	LOSS [training: 5.072429271843872 | validation: 5.589307415058904]
	TIME [epoch: 9.81 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.125937245765223		[learning rate: 0.003983]
	Learning Rate: 0.00398295
	LOSS [training: 5.125937245765223 | validation: 5.5632571889748945]
	TIME [epoch: 9.81 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0607613744735005		[learning rate: 0.0039736]
	Learning Rate: 0.00397356
	LOSS [training: 5.0607613744735005 | validation: 5.501314453329294]
	TIME [epoch: 9.82 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0774447458420955		[learning rate: 0.0039642]
	Learning Rate: 0.00396418
	LOSS [training: 5.0774447458420955 | validation: 5.577699107016721]
	TIME [epoch: 9.84 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.008999617334414		[learning rate: 0.0039548]
	Learning Rate: 0.00395483
	LOSS [training: 5.008999617334414 | validation: 5.566375986556838]
	TIME [epoch: 9.82 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.049066297427629		[learning rate: 0.0039455]
	Learning Rate: 0.0039455
	LOSS [training: 5.049066297427629 | validation: 5.570563430692277]
	TIME [epoch: 9.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.192806321309081		[learning rate: 0.0039362]
	Learning Rate: 0.0039362
	LOSS [training: 5.192806321309081 | validation: 5.5507823894480195]
	TIME [epoch: 9.83 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.150038619321624		[learning rate: 0.0039269]
	Learning Rate: 0.00392691
	LOSS [training: 5.150038619321624 | validation: 5.651385887216434]
	TIME [epoch: 9.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0613352017276		[learning rate: 0.0039176]
	Learning Rate: 0.00391765
	LOSS [training: 5.0613352017276 | validation: 5.489061560121891]
	TIME [epoch: 9.81 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.059031363654175		[learning rate: 0.0039084]
	Learning Rate: 0.00390841
	LOSS [training: 5.059031363654175 | validation: 5.5195702369414645]
	TIME [epoch: 9.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183472501459972		[learning rate: 0.0038992]
	Learning Rate: 0.00389919
	LOSS [training: 5.183472501459972 | validation: 5.542803335747023]
	TIME [epoch: 9.84 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.054211873875119		[learning rate: 0.00389]
	Learning Rate: 0.00388999
	LOSS [training: 5.054211873875119 | validation: 5.570206883795889]
	TIME [epoch: 9.82 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.052385485859394		[learning rate: 0.0038808]
	Learning Rate: 0.00388082
	LOSS [training: 5.052385485859394 | validation: 5.622395236301259]
	TIME [epoch: 9.83 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.109230095190699		[learning rate: 0.0038717]
	Learning Rate: 0.00387166
	LOSS [training: 5.109230095190699 | validation: 5.502320745481143]
	TIME [epoch: 9.82 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0461738003460805		[learning rate: 0.0038625]
	Learning Rate: 0.00386253
	LOSS [training: 5.0461738003460805 | validation: 5.499409674456302]
	TIME [epoch: 9.83 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.030788863676056		[learning rate: 0.0038534]
	Learning Rate: 0.00385342
	LOSS [training: 5.030788863676056 | validation: 5.70952228613988]
	TIME [epoch: 9.81 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.066499813733486		[learning rate: 0.0038443]
	Learning Rate: 0.00384433
	LOSS [training: 5.066499813733486 | validation: 5.588807292304073]
	TIME [epoch: 9.77 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0495129062485375		[learning rate: 0.0038353]
	Learning Rate: 0.00383526
	LOSS [training: 5.0495129062485375 | validation: 5.481600703727545]
	TIME [epoch: 9.83 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.033470661285149		[learning rate: 0.0038262]
	Learning Rate: 0.00382621
	LOSS [training: 5.033470661285149 | validation: 5.485832526132363]
	TIME [epoch: 9.8 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.026462590442489		[learning rate: 0.0038172]
	Learning Rate: 0.00381719
	LOSS [training: 5.026462590442489 | validation: 5.540041561033814]
	TIME [epoch: 9.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.029726912761753		[learning rate: 0.0038082]
	Learning Rate: 0.00380818
	LOSS [training: 5.029726912761753 | validation: 5.535220359761642]
	TIME [epoch: 9.81 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085820155620976		[learning rate: 0.0037992]
	Learning Rate: 0.0037992
	LOSS [training: 5.085820155620976 | validation: 5.510250723828535]
	TIME [epoch: 9.82 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1242392523398355		[learning rate: 0.0037902]
	Learning Rate: 0.00379024
	LOSS [training: 5.1242392523398355 | validation: 5.522449836929081]
	TIME [epoch: 9.81 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.068969387723791		[learning rate: 0.0037813]
	Learning Rate: 0.0037813
	LOSS [training: 5.068969387723791 | validation: 5.488624841676822]
	TIME [epoch: 9.82 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.009963260532773		[learning rate: 0.0037724]
	Learning Rate: 0.00377238
	LOSS [training: 5.009963260532773 | validation: 5.475207407326565]
	TIME [epoch: 9.82 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1675442812078		[learning rate: 0.0037635]
	Learning Rate: 0.00376348
	LOSS [training: 5.1675442812078 | validation: 5.6578376531287295]
	TIME [epoch: 9.85 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.054733716792164		[learning rate: 0.0037546]
	Learning Rate: 0.0037546
	LOSS [training: 5.054733716792164 | validation: 5.484688512130366]
	TIME [epoch: 9.82 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.002802208978145		[learning rate: 0.0037457]
	Learning Rate: 0.00374575
	LOSS [training: 5.002802208978145 | validation: 5.496318377123098]
	TIME [epoch: 9.82 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.118119564356389		[learning rate: 0.0037369]
	Learning Rate: 0.00373691
	LOSS [training: 5.118119564356389 | validation: 5.578121636211013]
	TIME [epoch: 9.82 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1325618902667705		[learning rate: 0.0037281]
	Learning Rate: 0.0037281
	LOSS [training: 5.1325618902667705 | validation: 5.573048384452758]
	TIME [epoch: 9.82 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.032500304852932		[learning rate: 0.0037193]
	Learning Rate: 0.0037193
	LOSS [training: 5.032500304852932 | validation: 5.446886618244909]
	TIME [epoch: 9.83 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.033588382687889		[learning rate: 0.0037105]
	Learning Rate: 0.00371053
	LOSS [training: 5.033588382687889 | validation: 5.57657850016845]
	TIME [epoch: 9.83 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.051516989647982		[learning rate: 0.0037018]
	Learning Rate: 0.00370178
	LOSS [training: 5.051516989647982 | validation: 5.4596844182800535]
	TIME [epoch: 9.83 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.99146364348978		[learning rate: 0.003693]
	Learning Rate: 0.00369304
	LOSS [training: 4.99146364348978 | validation: 5.709377940991139]
	TIME [epoch: 9.81 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.152285191722357		[learning rate: 0.0036843]
	Learning Rate: 0.00368433
	LOSS [training: 5.152285191722357 | validation: 5.499866097560051]
	TIME [epoch: 9.82 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0075139016126915		[learning rate: 0.0036756]
	Learning Rate: 0.00367564
	LOSS [training: 5.0075139016126915 | validation: 5.576354280942378]
	TIME [epoch: 9.82 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.994784598279917		[learning rate: 0.003667]
	Learning Rate: 0.00366697
	LOSS [training: 4.994784598279917 | validation: 5.767774418641072]
	TIME [epoch: 9.84 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.067544475058989		[learning rate: 0.0036583]
	Learning Rate: 0.00365832
	LOSS [training: 5.067544475058989 | validation: 5.5517320204358445]
	TIME [epoch: 9.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0354903636752555		[learning rate: 0.0036497]
	Learning Rate: 0.00364969
	LOSS [training: 5.0354903636752555 | validation: 5.506874424270095]
	TIME [epoch: 9.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.988369804173901		[learning rate: 0.0036411]
	Learning Rate: 0.00364108
	LOSS [training: 4.988369804173901 | validation: 5.463327211708084]
	TIME [epoch: 9.78 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.985592465433044		[learning rate: 0.0036325]
	Learning Rate: 0.0036325
	LOSS [training: 4.985592465433044 | validation: 5.509019318957464]
	TIME [epoch: 9.76 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0152342774492595		[learning rate: 0.0036239]
	Learning Rate: 0.00362393
	LOSS [training: 5.0152342774492595 | validation: 5.4927575150328725]
	TIME [epoch: 9.76 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.00566476604794		[learning rate: 0.0036154]
	Learning Rate: 0.00361538
	LOSS [training: 5.00566476604794 | validation: 5.747087309796566]
	TIME [epoch: 9.76 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.113625009903076		[learning rate: 0.0036069]
	Learning Rate: 0.00360685
	LOSS [training: 5.113625009903076 | validation: 5.513828557053557]
	TIME [epoch: 9.78 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.071868739716831		[learning rate: 0.0035983]
	Learning Rate: 0.00359834
	LOSS [training: 5.071868739716831 | validation: 5.668444910951355]
	TIME [epoch: 9.77 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.050575361795104		[learning rate: 0.0035899]
	Learning Rate: 0.00358986
	LOSS [training: 5.050575361795104 | validation: 5.484650146493687]
	TIME [epoch: 9.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.992599462790947		[learning rate: 0.0035814]
	Learning Rate: 0.00358139
	LOSS [training: 4.992599462790947 | validation: 5.434610465353711]
	TIME [epoch: 9.77 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.979276102347702		[learning rate: 0.0035729]
	Learning Rate: 0.00357294
	LOSS [training: 4.979276102347702 | validation: 5.497766946006445]
	TIME [epoch: 9.78 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.998876962037168		[learning rate: 0.0035645]
	Learning Rate: 0.00356451
	LOSS [training: 4.998876962037168 | validation: 5.505783372592672]
	TIME [epoch: 9.76 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.036457632472123		[learning rate: 0.0035561]
	Learning Rate: 0.0035561
	LOSS [training: 5.036457632472123 | validation: 5.502471333916372]
	TIME [epoch: 9.75 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0465967507774225		[learning rate: 0.0035477]
	Learning Rate: 0.00354771
	LOSS [training: 5.0465967507774225 | validation: 5.509649185633803]
	TIME [epoch: 9.77 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207898579266265		[learning rate: 0.0035393]
	Learning Rate: 0.00353935
	LOSS [training: 5.207898579266265 | validation: 5.580855204796462]
	TIME [epoch: 9.77 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085073213923559		[learning rate: 0.003531]
	Learning Rate: 0.003531
	LOSS [training: 5.085073213923559 | validation: 5.42119036331906]
	TIME [epoch: 9.76 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.024116009906635		[learning rate: 0.0035227]
	Learning Rate: 0.00352267
	LOSS [training: 5.024116009906635 | validation: 5.497209091817377]
	TIME [epoch: 9.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.017623358549838		[learning rate: 0.0035144]
	Learning Rate: 0.00351436
	LOSS [training: 5.017623358549838 | validation: 5.4647232897553115]
	TIME [epoch: 9.78 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0664532962329805		[learning rate: 0.0035061]
	Learning Rate: 0.00350607
	LOSS [training: 5.0664532962329805 | validation: 5.45143114789578]
	TIME [epoch: 9.77 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.032891445544313		[learning rate: 0.0034978]
	Learning Rate: 0.0034978
	LOSS [training: 5.032891445544313 | validation: 5.499809220597845]
	TIME [epoch: 9.76 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.054435846145457		[learning rate: 0.0034895]
	Learning Rate: 0.00348955
	LOSS [training: 5.054435846145457 | validation: 5.462562479706746]
	TIME [epoch: 9.75 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.913933204233404		[learning rate: 0.0034813]
	Learning Rate: 0.00348132
	LOSS [training: 4.913933204233404 | validation: 5.377935688099119]
	TIME [epoch: 9.78 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.065093535510667		[learning rate: 0.0034731]
	Learning Rate: 0.00347311
	LOSS [training: 5.065093535510667 | validation: 5.498226727803581]
	TIME [epoch: 9.76 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.976035754553626		[learning rate: 0.0034649]
	Learning Rate: 0.00346491
	LOSS [training: 4.976035754553626 | validation: 5.548270256383315]
	TIME [epoch: 9.76 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.953169540705674		[learning rate: 0.0034567]
	Learning Rate: 0.00345674
	LOSS [training: 4.953169540705674 | validation: 5.515696465564745]
	TIME [epoch: 9.76 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.994526999543547		[learning rate: 0.0034486]
	Learning Rate: 0.00344859
	LOSS [training: 4.994526999543547 | validation: 5.412423402595957]
	TIME [epoch: 9.78 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9442112529186755		[learning rate: 0.0034405]
	Learning Rate: 0.00344045
	LOSS [training: 4.9442112529186755 | validation: 5.632534096848251]
	TIME [epoch: 9.76 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.03663581543327		[learning rate: 0.0034323]
	Learning Rate: 0.00343234
	LOSS [training: 5.03663581543327 | validation: 5.903248735826294]
	TIME [epoch: 9.76 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.06792324027429		[learning rate: 0.0034242]
	Learning Rate: 0.00342424
	LOSS [training: 5.06792324027429 | validation: 5.573937638314842]
	TIME [epoch: 9.77 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.979015131646699		[learning rate: 0.0034162]
	Learning Rate: 0.00341616
	LOSS [training: 4.979015131646699 | validation: 5.4185654030258465]
	TIME [epoch: 9.77 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.969542805298943		[learning rate: 0.0034081]
	Learning Rate: 0.0034081
	LOSS [training: 4.969542805298943 | validation: 5.40599511887091]
	TIME [epoch: 9.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.920561776453583		[learning rate: 0.0034001]
	Learning Rate: 0.00340006
	LOSS [training: 4.920561776453583 | validation: 5.490688425828041]
	TIME [epoch: 9.76 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.980844222674923		[learning rate: 0.003392]
	Learning Rate: 0.00339204
	LOSS [training: 4.980844222674923 | validation: 5.444295842331837]
	TIME [epoch: 9.78 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.969323625820299		[learning rate: 0.003384]
	Learning Rate: 0.00338404
	LOSS [training: 4.969323625820299 | validation: 5.439065548344721]
	TIME [epoch: 9.76 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.981342722381025		[learning rate: 0.0033761]
	Learning Rate: 0.00337606
	LOSS [training: 4.981342722381025 | validation: 5.607262047857533]
	TIME [epoch: 9.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.97305096877484		[learning rate: 0.0033681]
	Learning Rate: 0.0033681
	LOSS [training: 4.97305096877484 | validation: 5.483346807118123]
	TIME [epoch: 9.77 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.930110789057833		[learning rate: 0.0033602]
	Learning Rate: 0.00336015
	LOSS [training: 4.930110789057833 | validation: 5.497040722787014]
	TIME [epoch: 9.78 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.971712812352756		[learning rate: 0.0033522]
	Learning Rate: 0.00335223
	LOSS [training: 4.971712812352756 | validation: 5.442253243547155]
	TIME [epoch: 9.76 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.97910217712045		[learning rate: 0.0033443]
	Learning Rate: 0.00334432
	LOSS [training: 4.97910217712045 | validation: 6.138712422358947]
	TIME [epoch: 9.76 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.173740422096678		[learning rate: 0.0033364]
	Learning Rate: 0.00333643
	LOSS [training: 5.173740422096678 | validation: 5.42553215122271]
	TIME [epoch: 9.78 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.935548517430615		[learning rate: 0.0033286]
	Learning Rate: 0.00332856
	LOSS [training: 4.935548517430615 | validation: 5.432733659245186]
	TIME [epoch: 9.78 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.944854082544654		[learning rate: 0.0033207]
	Learning Rate: 0.00332071
	LOSS [training: 4.944854082544654 | validation: 5.49918976187568]
	TIME [epoch: 9.77 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0062031061489645		[learning rate: 0.0033129]
	Learning Rate: 0.00331288
	LOSS [training: 5.0062031061489645 | validation: 5.473284364212293]
	TIME [epoch: 9.76 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.929023937861819		[learning rate: 0.0033051]
	Learning Rate: 0.00330506
	LOSS [training: 4.929023937861819 | validation: 5.485469414027546]
	TIME [epoch: 9.79 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.90048532714295		[learning rate: 0.0032973]
	Learning Rate: 0.00329727
	LOSS [training: 4.90048532714295 | validation: 5.444929590132546]
	TIME [epoch: 9.77 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.976870098421971		[learning rate: 0.0032895]
	Learning Rate: 0.00328949
	LOSS [training: 4.976870098421971 | validation: 5.437665292679795]
	TIME [epoch: 9.76 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.038291172525367		[learning rate: 0.0032817]
	Learning Rate: 0.00328173
	LOSS [training: 5.038291172525367 | validation: 5.423713542028287]
	TIME [epoch: 9.76 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.905678973110892		[learning rate: 0.003274]
	Learning Rate: 0.00327399
	LOSS [training: 4.905678973110892 | validation: 5.476156786364911]
	TIME [epoch: 9.79 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.941002168353415		[learning rate: 0.0032663]
	Learning Rate: 0.00326626
	LOSS [training: 4.941002168353415 | validation: 5.5185518009119185]
	TIME [epoch: 9.76 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.906114583394578		[learning rate: 0.0032586]
	Learning Rate: 0.00325856
	LOSS [training: 4.906114583394578 | validation: 5.378066420560146]
	TIME [epoch: 9.77 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897410335838572		[learning rate: 0.0032509]
	Learning Rate: 0.00325087
	LOSS [training: 4.897410335838572 | validation: 5.429752330603127]
	TIME [epoch: 9.76 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925029467829406		[learning rate: 0.0032432]
	Learning Rate: 0.0032432
	LOSS [training: 4.925029467829406 | validation: 5.372108026803814]
	TIME [epoch: 9.78 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.907123876447924		[learning rate: 0.0032356]
	Learning Rate: 0.00323555
	LOSS [training: 4.907123876447924 | validation: 5.3958955462378535]
	TIME [epoch: 9.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9630801545428085		[learning rate: 0.0032279]
	Learning Rate: 0.00322792
	LOSS [training: 4.9630801545428085 | validation: 5.570966212745143]
	TIME [epoch: 9.76 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.956455807267712		[learning rate: 0.0032203]
	Learning Rate: 0.00322031
	LOSS [training: 4.956455807267712 | validation: 5.443352394940055]
	TIME [epoch: 9.77 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.950371039206872		[learning rate: 0.0032127]
	Learning Rate: 0.00321271
	LOSS [training: 4.950371039206872 | validation: 5.4902786847087475]
	TIME [epoch: 9.77 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9667616839465465		[learning rate: 0.0032051]
	Learning Rate: 0.00320513
	LOSS [training: 4.9667616839465465 | validation: 5.516796234959741]
	TIME [epoch: 9.76 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.250428230796251		[learning rate: 0.0031976]
	Learning Rate: 0.00319757
	LOSS [training: 5.250428230796251 | validation: 5.505211522032681]
	TIME [epoch: 9.76 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.942884073957934		[learning rate: 0.00319]
	Learning Rate: 0.00319003
	LOSS [training: 4.942884073957934 | validation: 5.45795370485093]
	TIME [epoch: 9.79 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.895495784438465		[learning rate: 0.0031825]
	Learning Rate: 0.00318251
	LOSS [training: 4.895495784438465 | validation: 5.387637692158864]
	TIME [epoch: 9.76 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.99335092764209		[learning rate: 0.003175]
	Learning Rate: 0.003175
	LOSS [training: 4.99335092764209 | validation: 5.627538233443016]
	TIME [epoch: 9.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.020573599359877		[learning rate: 0.0031675]
	Learning Rate: 0.00316751
	LOSS [training: 5.020573599359877 | validation: 5.41344597233664]
	TIME [epoch: 9.76 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9071253665743555		[learning rate: 0.00316]
	Learning Rate: 0.00316004
	LOSS [training: 4.9071253665743555 | validation: 5.393756308439698]
	TIME [epoch: 9.78 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.094989262327781		[learning rate: 0.0031526]
	Learning Rate: 0.00315258
	LOSS [training: 5.094989262327781 | validation: 5.52934648743209]
	TIME [epoch: 9.76 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925603984128072		[learning rate: 0.0031451]
	Learning Rate: 0.00314515
	LOSS [training: 4.925603984128072 | validation: 5.401873706310125]
	TIME [epoch: 9.76 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.894916061023214		[learning rate: 0.0031377]
	Learning Rate: 0.00313773
	LOSS [training: 4.894916061023214 | validation: 5.3951415588445375]
	TIME [epoch: 9.78 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.996484313369013		[learning rate: 0.0031303]
	Learning Rate: 0.00313033
	LOSS [training: 4.996484313369013 | validation: 5.4532001247643]
	TIME [epoch: 9.77 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.95518434205596		[learning rate: 0.0031229]
	Learning Rate: 0.00312294
	LOSS [training: 4.95518434205596 | validation: 5.479665123112091]
	TIME [epoch: 9.76 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9827968773100695		[learning rate: 0.0031156]
	Learning Rate: 0.00311558
	LOSS [training: 4.9827968773100695 | validation: 5.498931848744463]
	TIME [epoch: 9.76 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0126934260878		[learning rate: 0.0031082]
	Learning Rate: 0.00310823
	LOSS [training: 5.0126934260878 | validation: 5.430898952582729]
	TIME [epoch: 9.79 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.915718818032926		[learning rate: 0.0031009]
	Learning Rate: 0.0031009
	LOSS [training: 4.915718818032926 | validation: 5.453314252351337]
	TIME [epoch: 9.76 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926078476391597		[learning rate: 0.0030936]
	Learning Rate: 0.00309358
	LOSS [training: 4.926078476391597 | validation: 5.617663615251965]
	TIME [epoch: 9.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.943718822209381		[learning rate: 0.0030863]
	Learning Rate: 0.00308628
	LOSS [training: 4.943718822209381 | validation: 5.393569402126745]
	TIME [epoch: 9.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.906707119124495		[learning rate: 0.003079]
	Learning Rate: 0.003079
	LOSS [training: 4.906707119124495 | validation: 5.384443021157738]
	TIME [epoch: 9.79 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885646910929248		[learning rate: 0.0030717]
	Learning Rate: 0.00307174
	LOSS [training: 4.885646910929248 | validation: 5.398074115024294]
	TIME [epoch: 9.76 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.896396351941598		[learning rate: 0.0030645]
	Learning Rate: 0.0030645
	LOSS [training: 4.896396351941598 | validation: 5.456305348401629]
	TIME [epoch: 9.77 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897456329941725		[learning rate: 0.0030573]
	Learning Rate: 0.00305727
	LOSS [training: 4.897456329941725 | validation: 5.6087276230124345]
	TIME [epoch: 9.77 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.936969707862972		[learning rate: 0.0030501]
	Learning Rate: 0.00305006
	LOSS [training: 4.936969707862972 | validation: 5.418273265060268]
	TIME [epoch: 9.77 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8493555531754975		[learning rate: 0.0030429]
	Learning Rate: 0.00304286
	LOSS [training: 4.8493555531754975 | validation: 5.5067695925205955]
	TIME [epoch: 9.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.981121789058714		[learning rate: 0.0030357]
	Learning Rate: 0.00303568
	LOSS [training: 4.981121789058714 | validation: 5.465473584608921]
	TIME [epoch: 9.76 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902238280337141		[learning rate: 0.0030285]
	Learning Rate: 0.00302852
	LOSS [training: 4.902238280337141 | validation: 5.538289155913443]
	TIME [epoch: 9.77 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.965684345640815		[learning rate: 0.0030214]
	Learning Rate: 0.00302138
	LOSS [training: 4.965684345640815 | validation: 5.395616694568038]
	TIME [epoch: 9.76 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.907693736433354		[learning rate: 0.0030143]
	Learning Rate: 0.00301425
	LOSS [training: 4.907693736433354 | validation: 5.383911363439214]
	TIME [epoch: 9.76 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.904627028295073		[learning rate: 0.0030071]
	Learning Rate: 0.00300714
	LOSS [training: 4.904627028295073 | validation: 5.527770686349043]
	TIME [epoch: 9.77 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.915774742604104		[learning rate: 0.003]
	Learning Rate: 0.00300005
	LOSS [training: 4.915774742604104 | validation: 5.34725769444001]
	TIME [epoch: 9.79 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.87881026317255		[learning rate: 0.002993]
	Learning Rate: 0.00299297
	LOSS [training: 4.87881026317255 | validation: 5.458545560197942]
	TIME [epoch: 9.76 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9036847908237124		[learning rate: 0.0029859]
	Learning Rate: 0.00298591
	LOSS [training: 4.9036847908237124 | validation: 5.5120280734326865]
	TIME [epoch: 9.76 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.931156743029229		[learning rate: 0.0029789]
	Learning Rate: 0.00297887
	LOSS [training: 4.931156743029229 | validation: 5.354566752250548]
	TIME [epoch: 9.77 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8964252130852675		[learning rate: 0.0029718]
	Learning Rate: 0.00297184
	LOSS [training: 4.8964252130852675 | validation: 5.563133924553498]
	TIME [epoch: 9.77 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.016630839715313		[learning rate: 0.0029648]
	Learning Rate: 0.00296483
	LOSS [training: 5.016630839715313 | validation: 5.409842898776593]
	TIME [epoch: 9.76 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.86861930322451		[learning rate: 0.0029578]
	Learning Rate: 0.00295784
	LOSS [training: 4.86861930322451 | validation: 5.453135688707896]
	TIME [epoch: 9.76 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.880902198318777		[learning rate: 0.0029509]
	Learning Rate: 0.00295086
	LOSS [training: 4.880902198318777 | validation: 5.406763281575829]
	TIME [epoch: 9.78 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8695957618476715		[learning rate: 0.0029439]
	Learning Rate: 0.0029439
	LOSS [training: 4.8695957618476715 | validation: 5.410551139894606]
	TIME [epoch: 9.78 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9257663779445116		[learning rate: 0.002937]
	Learning Rate: 0.00293696
	LOSS [training: 4.9257663779445116 | validation: 5.371531460972205]
	TIME [epoch: 9.76 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.856284641269459		[learning rate: 0.00293]
	Learning Rate: 0.00293003
	LOSS [training: 4.856284641269459 | validation: 5.421177926323664]
	TIME [epoch: 9.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.856347111660346		[learning rate: 0.0029231]
	Learning Rate: 0.00292312
	LOSS [training: 4.856347111660346 | validation: 5.469081819456659]
	TIME [epoch: 9.79 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.892743530340072		[learning rate: 0.0029162]
	Learning Rate: 0.00291622
	LOSS [training: 4.892743530340072 | validation: 5.403265509167367]
	TIME [epoch: 9.77 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.876166832789278		[learning rate: 0.0029093]
	Learning Rate: 0.00290934
	LOSS [training: 4.876166832789278 | validation: 5.387027207767148]
	TIME [epoch: 9.77 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.93568202598222		[learning rate: 0.0029025]
	Learning Rate: 0.00290248
	LOSS [training: 4.93568202598222 | validation: 5.474604962223955]
	TIME [epoch: 9.78 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.898544973032981		[learning rate: 0.0028956]
	Learning Rate: 0.00289563
	LOSS [training: 4.898544973032981 | validation: 5.403119337617498]
	TIME [epoch: 9.78 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922278318345143		[learning rate: 0.0028888]
	Learning Rate: 0.0028888
	LOSS [training: 4.922278318345143 | validation: 5.374708603292856]
	TIME [epoch: 9.77 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.889962260862786		[learning rate: 0.002882]
	Learning Rate: 0.00288199
	LOSS [training: 4.889962260862786 | validation: 5.438455511877667]
	TIME [epoch: 9.79 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.882480611083832		[learning rate: 0.0028752]
	Learning Rate: 0.00287519
	LOSS [training: 4.882480611083832 | validation: 5.488176912843508]
	TIME [epoch: 9.79 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0275536099242		[learning rate: 0.0028684]
	Learning Rate: 0.00286841
	LOSS [training: 5.0275536099242 | validation: 5.426066615687217]
	TIME [epoch: 9.76 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9163245528888755		[learning rate: 0.0028616]
	Learning Rate: 0.00286164
	LOSS [training: 4.9163245528888755 | validation: 5.371525733718947]
	TIME [epoch: 9.76 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.88354987270419		[learning rate: 0.0028549]
	Learning Rate: 0.00285489
	LOSS [training: 4.88354987270419 | validation: 5.582931204762126]
	TIME [epoch: 9.78 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.890948622266797		[learning rate: 0.0028482]
	Learning Rate: 0.00284816
	LOSS [training: 4.890948622266797 | validation: 5.394621090131201]
	TIME [epoch: 9.78 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.906193587694643		[learning rate: 0.0028414]
	Learning Rate: 0.00284144
	LOSS [training: 4.906193587694643 | validation: 5.3726698587617445]
	TIME [epoch: 9.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8808593578959005		[learning rate: 0.0028347]
	Learning Rate: 0.00283474
	LOSS [training: 4.8808593578959005 | validation: 5.3363085585623455]
	TIME [epoch: 9.77 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926995340807735		[learning rate: 0.0028281]
	Learning Rate: 0.00282805
	LOSS [training: 4.926995340807735 | validation: 5.389950621274125]
	TIME [epoch: 9.76 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176220036130369		[learning rate: 0.0028214]
	Learning Rate: 0.00282138
	LOSS [training: 5.176220036130369 | validation: 5.4505482476902465]
	TIME [epoch: 9.8 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.889528361494335		[learning rate: 0.0028147]
	Learning Rate: 0.00281472
	LOSS [training: 4.889528361494335 | validation: 5.468735119809802]
	TIME [epoch: 9.77 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8760383180908935		[learning rate: 0.0028081]
	Learning Rate: 0.00280808
	LOSS [training: 4.8760383180908935 | validation: 5.403026432021286]
	TIME [epoch: 9.77 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926147834663343		[learning rate: 0.0028015]
	Learning Rate: 0.00280146
	LOSS [training: 4.926147834663343 | validation: 5.365390026290178]
	TIME [epoch: 9.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.866858039291749		[learning rate: 0.0027949]
	Learning Rate: 0.00279485
	LOSS [training: 4.866858039291749 | validation: 5.416228972649387]
	TIME [epoch: 9.78 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.875742520583574		[learning rate: 0.0027883]
	Learning Rate: 0.00278826
	LOSS [training: 4.875742520583574 | validation: 5.36316715539254]
	TIME [epoch: 9.77 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862309047888923		[learning rate: 0.0027817]
	Learning Rate: 0.00278168
	LOSS [training: 4.862309047888923 | validation: 5.363221326792646]
	TIME [epoch: 9.76 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.840471017040594		[learning rate: 0.0027751]
	Learning Rate: 0.00277512
	LOSS [training: 4.840471017040594 | validation: 5.377548780071988]
	TIME [epoch: 9.77 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908353193437748		[learning rate: 0.0027686]
	Learning Rate: 0.00276858
	LOSS [training: 4.908353193437748 | validation: 5.439103781657457]
	TIME [epoch: 9.76 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.881745737609921		[learning rate: 0.002762]
	Learning Rate: 0.00276205
	LOSS [training: 4.881745737609921 | validation: 5.353300605324143]
	TIME [epoch: 9.77 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9271206869215405		[learning rate: 0.0027555]
	Learning Rate: 0.00275553
	LOSS [training: 4.9271206869215405 | validation: 5.415537348558585]
	TIME [epoch: 9.75 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.896936707959055		[learning rate: 0.002749]
	Learning Rate: 0.00274903
	LOSS [training: 4.896936707959055 | validation: 5.355120109452957]
	TIME [epoch: 9.78 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.923690224133779		[learning rate: 0.0027425]
	Learning Rate: 0.00274255
	LOSS [training: 4.923690224133779 | validation: 5.414250248619596]
	TIME [epoch: 9.75 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8680161771670045		[learning rate: 0.0027361]
	Learning Rate: 0.00273608
	LOSS [training: 4.8680161771670045 | validation: 5.482282199239752]
	TIME [epoch: 9.77 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.880911118000313		[learning rate: 0.0027296]
	Learning Rate: 0.00272962
	LOSS [training: 4.880911118000313 | validation: 5.453776944719817]
	TIME [epoch: 9.78 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.967695591323716		[learning rate: 0.0027232]
	Learning Rate: 0.00272318
	LOSS [training: 4.967695591323716 | validation: 5.350350960638868]
	TIME [epoch: 9.79 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.853285276175066		[learning rate: 0.0027168]
	Learning Rate: 0.00271676
	LOSS [training: 4.853285276175066 | validation: 5.3786972154264]
	TIME [epoch: 9.76 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893308723080709		[learning rate: 0.0027104]
	Learning Rate: 0.00271035
	LOSS [training: 4.893308723080709 | validation: 5.349869488330169]
	TIME [epoch: 9.77 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.859639741048547		[learning rate: 0.002704]
	Learning Rate: 0.00270396
	LOSS [training: 4.859639741048547 | validation: 5.521019942737446]
	TIME [epoch: 9.78 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.02217081603611		[learning rate: 0.0026976]
	Learning Rate: 0.00269758
	LOSS [training: 5.02217081603611 | validation: 5.496907577996525]
	TIME [epoch: 9.76 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912120369906111		[learning rate: 0.0026912]
	Learning Rate: 0.00269122
	LOSS [training: 4.912120369906111 | validation: 5.375783886233714]
	TIME [epoch: 9.76 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8341482609914355		[learning rate: 0.0026849]
	Learning Rate: 0.00268487
	LOSS [training: 4.8341482609914355 | validation: 5.382225437702211]
	TIME [epoch: 9.76 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846251277065649		[learning rate: 0.0026785]
	Learning Rate: 0.00267854
	LOSS [training: 4.846251277065649 | validation: 5.360948579032287]
	TIME [epoch: 9.79 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.932907692386989		[learning rate: 0.0026722]
	Learning Rate: 0.00267222
	LOSS [training: 4.932907692386989 | validation: 5.412018034338785]
	TIME [epoch: 9.77 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.917616056551845		[learning rate: 0.0026659]
	Learning Rate: 0.00266591
	LOSS [training: 4.917616056551845 | validation: 5.368816079167135]
	TIME [epoch: 9.76 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.887693769790881		[learning rate: 0.0026596]
	Learning Rate: 0.00265963
	LOSS [training: 4.887693769790881 | validation: 5.5082059574655]
	TIME [epoch: 9.76 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.87412991877285		[learning rate: 0.0026534]
	Learning Rate: 0.00265335
	LOSS [training: 4.87412991877285 | validation: 5.338778426522248]
	TIME [epoch: 9.78 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.853734358017808		[learning rate: 0.0026471]
	Learning Rate: 0.00264709
	LOSS [training: 4.853734358017808 | validation: 5.399661886215999]
	TIME [epoch: 9.76 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9292866846584396		[learning rate: 0.0026408]
	Learning Rate: 0.00264085
	LOSS [training: 4.9292866846584396 | validation: 5.560999543263328]
	TIME [epoch: 9.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959800155532095		[learning rate: 0.0026346]
	Learning Rate: 0.00263462
	LOSS [training: 4.959800155532095 | validation: 5.363059938891213]
	TIME [epoch: 9.78 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8520298521873375		[learning rate: 0.0026284]
	Learning Rate: 0.00262841
	LOSS [training: 4.8520298521873375 | validation: 5.423785337052897]
	TIME [epoch: 9.76 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9333538874005445		[learning rate: 0.0026222]
	Learning Rate: 0.00262221
	LOSS [training: 4.9333538874005445 | validation: 5.404585999997587]
	TIME [epoch: 9.75 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.901734797212939		[learning rate: 0.002616]
	Learning Rate: 0.00261602
	LOSS [training: 4.901734797212939 | validation: 5.323183678966184]
	TIME [epoch: 9.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.977462039567994		[learning rate: 0.0026098]
	Learning Rate: 0.00260985
	LOSS [training: 4.977462039567994 | validation: 5.397044027772681]
	TIME [epoch: 9.78 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.864327211390533		[learning rate: 0.0026037]
	Learning Rate: 0.00260369
	LOSS [training: 4.864327211390533 | validation: 5.356370267816019]
	TIME [epoch: 9.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9186536841630355		[learning rate: 0.0025976]
	Learning Rate: 0.00259755
	LOSS [training: 4.9186536841630355 | validation: 5.342761192821825]
	TIME [epoch: 9.76 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846124649809239		[learning rate: 0.0025914]
	Learning Rate: 0.00259142
	LOSS [training: 4.846124649809239 | validation: 5.34994862277275]
	TIME [epoch: 9.76 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.842361862403539		[learning rate: 0.0025853]
	Learning Rate: 0.00258531
	LOSS [training: 4.842361862403539 | validation: 5.38310639410258]
	TIME [epoch: 9.78 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8410358142813275		[learning rate: 0.0025792]
	Learning Rate: 0.00257921
	LOSS [training: 4.8410358142813275 | validation: 5.345468502599915]
	TIME [epoch: 9.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8777500629548225		[learning rate: 0.0025731]
	Learning Rate: 0.00257313
	LOSS [training: 4.8777500629548225 | validation: 5.349836306401782]
	TIME [epoch: 9.76 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.919826426139537		[learning rate: 0.0025671]
	Learning Rate: 0.00256706
	LOSS [training: 4.919826426139537 | validation: 5.434982542278131]
	TIME [epoch: 9.77 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.883155301265193		[learning rate: 0.002561]
	Learning Rate: 0.002561
	LOSS [training: 4.883155301265193 | validation: 5.362584342210144]
	TIME [epoch: 9.76 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846156281784292		[learning rate: 0.002555]
	Learning Rate: 0.00255496
	LOSS [training: 4.846156281784292 | validation: 5.405032998029928]
	TIME [epoch: 9.75 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.860267569004927		[learning rate: 0.0025489]
	Learning Rate: 0.00254894
	LOSS [training: 4.860267569004927 | validation: 5.360580740788264]
	TIME [epoch: 9.75 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.907250973406714		[learning rate: 0.0025429]
	Learning Rate: 0.00254292
	LOSS [training: 4.907250973406714 | validation: 5.349310173073073]
	TIME [epoch: 9.77 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.85510717718803		[learning rate: 0.0025369]
	Learning Rate: 0.00253693
	LOSS [training: 4.85510717718803 | validation: 5.328057718099451]
	TIME [epoch: 9.77 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.866741447026563		[learning rate: 0.0025309]
	Learning Rate: 0.00253094
	LOSS [training: 4.866741447026563 | validation: 5.809688423806423]
	TIME [epoch: 9.76 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925445616334713		[learning rate: 0.002525]
	Learning Rate: 0.00252497
	LOSS [training: 4.925445616334713 | validation: 5.415534788946356]
	TIME [epoch: 9.76 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873591403243156		[learning rate: 0.002519]
	Learning Rate: 0.00251901
	LOSS [training: 4.873591403243156 | validation: 5.388972655819522]
	TIME [epoch: 9.78 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.844754163828905		[learning rate: 0.0025131]
	Learning Rate: 0.00251307
	LOSS [training: 4.844754163828905 | validation: 5.309547245683889]
	TIME [epoch: 9.76 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.83673529663653		[learning rate: 0.0025071]
	Learning Rate: 0.00250715
	LOSS [training: 4.83673529663653 | validation: 5.2928940018803035]
	TIME [epoch: 9.75 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.84189518525372		[learning rate: 0.0025012]
	Learning Rate: 0.00250123
	LOSS [training: 4.84189518525372 | validation: 5.432883635043845]
	TIME [epoch: 9.76 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.847456476300069		[learning rate: 0.0024953]
	Learning Rate: 0.00249533
	LOSS [training: 4.847456476300069 | validation: 5.384994515387071]
	TIME [epoch: 9.78 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.911234342439238		[learning rate: 0.0024894]
	Learning Rate: 0.00248945
	LOSS [training: 4.911234342439238 | validation: 5.42200117003188]
	TIME [epoch: 9.77 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8717830050037785		[learning rate: 0.0024836]
	Learning Rate: 0.00248357
	LOSS [training: 4.8717830050037785 | validation: 5.592559280383559]
	TIME [epoch: 9.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.929843820920219		[learning rate: 0.0024777]
	Learning Rate: 0.00247771
	LOSS [training: 4.929843820920219 | validation: 5.313725378168838]
	TIME [epoch: 9.79 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.965025731925621		[learning rate: 0.0024719]
	Learning Rate: 0.00247187
	LOSS [training: 4.965025731925621 | validation: 5.410303722089338]
	TIME [epoch: 9.75 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.900296220151116		[learning rate: 0.002466]
	Learning Rate: 0.00246604
	LOSS [training: 4.900296220151116 | validation: 5.429049269821029]
	TIME [epoch: 9.77 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.815797732877314		[learning rate: 0.0024602]
	Learning Rate: 0.00246022
	LOSS [training: 4.815797732877314 | validation: 5.462439841321225]
	TIME [epoch: 9.78 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8761129588355585		[learning rate: 0.0024544]
	Learning Rate: 0.00245442
	LOSS [training: 4.8761129588355585 | validation: 5.477153352491157]
	TIME [epoch: 9.79 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.830942053264374		[learning rate: 0.0024486]
	Learning Rate: 0.00244863
	LOSS [training: 4.830942053264374 | validation: 5.559092145173063]
	TIME [epoch: 9.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.901831415866443		[learning rate: 0.0024429]
	Learning Rate: 0.00244285
	LOSS [training: 4.901831415866443 | validation: 5.525834730304139]
	TIME [epoch: 9.78 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893086635491441		[learning rate: 0.0024371]
	Learning Rate: 0.00243709
	LOSS [training: 4.893086635491441 | validation: 5.330340114994741]
	TIME [epoch: 9.79 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.860769889048223		[learning rate: 0.0024313]
	Learning Rate: 0.00243134
	LOSS [training: 4.860769889048223 | validation: 5.379715790697817]
	TIME [epoch: 9.79 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9430110079992895		[learning rate: 0.0024256]
	Learning Rate: 0.00242561
	LOSS [training: 4.9430110079992895 | validation: 5.325278336131892]
	TIME [epoch: 9.77 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.830007470870219		[learning rate: 0.0024199]
	Learning Rate: 0.00241989
	LOSS [training: 4.830007470870219 | validation: 5.436986564536235]
	TIME [epoch: 9.78 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8529261457542425		[learning rate: 0.0024142]
	Learning Rate: 0.00241418
	LOSS [training: 4.8529261457542425 | validation: 5.3369690860705035]
	TIME [epoch: 9.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798151168476529		[learning rate: 0.0024085]
	Learning Rate: 0.00240848
	LOSS [training: 4.798151168476529 | validation: 5.385300242024644]
	TIME [epoch: 9.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8889590501195945		[learning rate: 0.0024028]
	Learning Rate: 0.0024028
	LOSS [training: 4.8889590501195945 | validation: 5.4445997607075185]
	TIME [epoch: 9.8 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8496128299804635		[learning rate: 0.0023971]
	Learning Rate: 0.00239713
	LOSS [training: 4.8496128299804635 | validation: 5.384113664728211]
	TIME [epoch: 9.78 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1390414592834		[learning rate: 0.0023915]
	Learning Rate: 0.00239148
	LOSS [training: 5.1390414592834 | validation: 5.388706434576907]
	TIME [epoch: 9.81 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.833961688897905		[learning rate: 0.0023858]
	Learning Rate: 0.00238584
	LOSS [training: 4.833961688897905 | validation: 5.437170702411868]
	TIME [epoch: 9.76 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.847113334842115		[learning rate: 0.0023802]
	Learning Rate: 0.00238021
	LOSS [training: 4.847113334842115 | validation: 5.332160223002911]
	TIME [epoch: 9.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.837781674269463		[learning rate: 0.0023746]
	Learning Rate: 0.0023746
	LOSS [training: 4.837781674269463 | validation: 5.3891639123733555]
	TIME [epoch: 9.76 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.834808874396717		[learning rate: 0.002369]
	Learning Rate: 0.00236899
	LOSS [training: 4.834808874396717 | validation: 5.463020951673764]
	TIME [epoch: 9.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8685391647380545		[learning rate: 0.0023634]
	Learning Rate: 0.00236341
	LOSS [training: 4.8685391647380545 | validation: 5.445392858807942]
	TIME [epoch: 9.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.945902670141977		[learning rate: 0.0023578]
	Learning Rate: 0.00235783
	LOSS [training: 4.945902670141977 | validation: 5.387754565512899]
	TIME [epoch: 9.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839435387356578		[learning rate: 0.0023523]
	Learning Rate: 0.00235227
	LOSS [training: 4.839435387356578 | validation: 5.327216608161591]
	TIME [epoch: 9.77 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.818762035587488		[learning rate: 0.0023467]
	Learning Rate: 0.00234672
	LOSS [training: 4.818762035587488 | validation: 5.459724918156325]
	TIME [epoch: 9.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.085982407156893		[learning rate: 0.0023412]
	Learning Rate: 0.00234119
	LOSS [training: 5.085982407156893 | validation: 5.422526977495192]
	TIME [epoch: 9.79 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.851702802609445		[learning rate: 0.0023357]
	Learning Rate: 0.00233566
	LOSS [training: 4.851702802609445 | validation: 5.354483487668102]
	TIME [epoch: 9.79 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8716045457933985		[learning rate: 0.0023302]
	Learning Rate: 0.00233015
	LOSS [training: 4.8716045457933985 | validation: 5.424936745376857]
	TIME [epoch: 9.8 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893021533369788		[learning rate: 0.0023247]
	Learning Rate: 0.00232466
	LOSS [training: 4.893021533369788 | validation: 5.471514368821367]
	TIME [epoch: 9.78 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.940514132949464		[learning rate: 0.0023192]
	Learning Rate: 0.00231917
	LOSS [training: 4.940514132949464 | validation: 5.4828348171934795]
	TIME [epoch: 9.81 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.0319986910912275		[learning rate: 0.0023137]
	Learning Rate: 0.0023137
	LOSS [training: 5.0319986910912275 | validation: 5.492202467530339]
	TIME [epoch: 9.77 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903931389995291		[learning rate: 0.0023082]
	Learning Rate: 0.00230825
	LOSS [training: 4.903931389995291 | validation: 5.454018696748055]
	TIME [epoch: 9.81 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902917194233023		[learning rate: 0.0023028]
	Learning Rate: 0.0023028
	LOSS [training: 4.902917194233023 | validation: 5.304593418469273]
	TIME [epoch: 9.78 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854965806011709		[learning rate: 0.0022974]
	Learning Rate: 0.00229737
	LOSS [training: 4.854965806011709 | validation: 5.297720788076963]
	TIME [epoch: 9.8 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8433274445105585		[learning rate: 0.0022919]
	Learning Rate: 0.00229195
	LOSS [training: 4.8433274445105585 | validation: 5.364526785945668]
	TIME [epoch: 9.81 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.880096380122351		[learning rate: 0.0022865]
	Learning Rate: 0.00228654
	LOSS [training: 4.880096380122351 | validation: 5.317406944971278]
	TIME [epoch: 9.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922830243514379		[learning rate: 0.0022811]
	Learning Rate: 0.00228115
	LOSS [training: 4.922830243514379 | validation: 5.36445288549145]
	TIME [epoch: 9.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.837514156535474		[learning rate: 0.0022758]
	Learning Rate: 0.00227577
	LOSS [training: 4.837514156535474 | validation: 5.399003458565448]
	TIME [epoch: 9.79 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.947987250576854		[learning rate: 0.0022704]
	Learning Rate: 0.0022704
	LOSS [training: 4.947987250576854 | validation: 5.510612443489861]
	TIME [epoch: 9.83 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.027068211240018		[learning rate: 0.002265]
	Learning Rate: 0.00226505
	LOSS [training: 5.027068211240018 | validation: 5.339549030162866]
	TIME [epoch: 9.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8656846789530395		[learning rate: 0.0022597]
	Learning Rate: 0.0022597
	LOSS [training: 4.8656846789530395 | validation: 5.348368954928771]
	TIME [epoch: 9.8 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.835670238531682		[learning rate: 0.0022544]
	Learning Rate: 0.00225437
	LOSS [training: 4.835670238531682 | validation: 5.427401257721029]
	TIME [epoch: 9.78 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.909382671578226		[learning rate: 0.0022491]
	Learning Rate: 0.00224905
	LOSS [training: 4.909382671578226 | validation: 5.40079577828406]
	TIME [epoch: 9.82 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820335489205015		[learning rate: 0.0022437]
	Learning Rate: 0.00224375
	LOSS [training: 4.820335489205015 | validation: 5.434802470227565]
	TIME [epoch: 9.8 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.874292556791235		[learning rate: 0.0022385]
	Learning Rate: 0.00223846
	LOSS [training: 4.874292556791235 | validation: 5.338455341280368]
	TIME [epoch: 9.81 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8716805973456605		[learning rate: 0.0022332]
	Learning Rate: 0.00223318
	LOSS [training: 4.8716805973456605 | validation: 5.334266189537368]
	TIME [epoch: 9.79 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.856160003147837		[learning rate: 0.0022279]
	Learning Rate: 0.00222791
	LOSS [training: 4.856160003147837 | validation: 5.347496810229361]
	TIME [epoch: 9.81 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.913370973617885		[learning rate: 0.0022227]
	Learning Rate: 0.00222265
	LOSS [training: 4.913370973617885 | validation: 5.376656017177391]
	TIME [epoch: 9.78 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806738446263646		[learning rate: 0.0022174]
	Learning Rate: 0.00221741
	LOSS [training: 4.806738446263646 | validation: 5.390225328292022]
	TIME [epoch: 9.78 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780876131457931		[learning rate: 0.0022122]
	Learning Rate: 0.00221218
	LOSS [training: 4.780876131457931 | validation: 5.403970503407426]
	TIME [epoch: 9.8 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.851741688761173		[learning rate: 0.002207]
	Learning Rate: 0.00220696
	LOSS [training: 4.851741688761173 | validation: 5.580569117490332]
	TIME [epoch: 9.79 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.93960199717622		[learning rate: 0.0022018]
	Learning Rate: 0.00220176
	LOSS [training: 4.93960199717622 | validation: 5.3343153180125205]
	TIME [epoch: 9.78 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80140234304051		[learning rate: 0.0021966]
	Learning Rate: 0.00219656
	LOSS [training: 4.80140234304051 | validation: 5.320630656207489]
	TIME [epoch: 9.78 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824093679649762		[learning rate: 0.0021914]
	Learning Rate: 0.00219138
	LOSS [training: 4.824093679649762 | validation: 5.287894837191496]
	TIME [epoch: 9.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.833378751456512		[learning rate: 0.0021862]
	Learning Rate: 0.00218621
	LOSS [training: 4.833378751456512 | validation: 5.388830049575522]
	TIME [epoch: 9.78 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800008191331882		[learning rate: 0.0021811]
	Learning Rate: 0.00218106
	LOSS [training: 4.800008191331882 | validation: 5.3937702703720065]
	TIME [epoch: 9.8 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.814862189998428		[learning rate: 0.0021759]
	Learning Rate: 0.00217591
	LOSS [training: 4.814862189998428 | validation: 5.302559978550632]
	TIME [epoch: 9.8 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786664397451025		[learning rate: 0.0021708]
	Learning Rate: 0.00217078
	LOSS [training: 4.786664397451025 | validation: 5.341429177596633]
	TIME [epoch: 9.82 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8457406161507635		[learning rate: 0.0021657]
	Learning Rate: 0.00216566
	LOSS [training: 4.8457406161507635 | validation: 5.336191125001676]
	TIME [epoch: 9.79 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836172128952475		[learning rate: 0.0021605]
	Learning Rate: 0.00216055
	LOSS [training: 4.836172128952475 | validation: 5.330261948892307]
	TIME [epoch: 9.8 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.82204175824949		[learning rate: 0.0021555]
	Learning Rate: 0.00215545
	LOSS [training: 4.82204175824949 | validation: 5.438352184951202]
	TIME [epoch: 9.81 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854596393367136		[learning rate: 0.0021504]
	Learning Rate: 0.00215037
	LOSS [training: 4.854596393367136 | validation: 5.418173943559723]
	TIME [epoch: 9.8 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.845509704763814		[learning rate: 0.0021453]
	Learning Rate: 0.0021453
	LOSS [training: 4.845509704763814 | validation: 5.54230542164341]
	TIME [epoch: 9.77 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.874838145544902		[learning rate: 0.0021402]
	Learning Rate: 0.00214024
	LOSS [training: 4.874838145544902 | validation: 5.367726639466066]
	TIME [epoch: 9.81 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7957132084560925		[learning rate: 0.0021352]
	Learning Rate: 0.00213519
	LOSS [training: 4.7957132084560925 | validation: 5.310218379231765]
	TIME [epoch: 9.79 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785390015816137		[learning rate: 0.0021302]
	Learning Rate: 0.00213015
	LOSS [training: 4.785390015816137 | validation: 5.38073070522423]
	TIME [epoch: 9.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925082425271274		[learning rate: 0.0021251]
	Learning Rate: 0.00212513
	LOSS [training: 4.925082425271274 | validation: 5.305010648180977]
	TIME [epoch: 9.79 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854044699882918		[learning rate: 0.0021201]
	Learning Rate: 0.00212011
	LOSS [training: 4.854044699882918 | validation: 5.620364797237577]
	TIME [epoch: 9.82 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.918233856763488		[learning rate: 0.0021151]
	Learning Rate: 0.00211511
	LOSS [training: 4.918233856763488 | validation: 5.371583895784119]
	TIME [epoch: 9.8 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.831977311403675		[learning rate: 0.0021101]
	Learning Rate: 0.00211012
	LOSS [training: 4.831977311403675 | validation: 5.377140958223223]
	TIME [epoch: 9.81 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.850905543068976		[learning rate: 0.0021051]
	Learning Rate: 0.00210515
	LOSS [training: 4.850905543068976 | validation: 5.4074225287525115]
	TIME [epoch: 9.79 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9759113551176695		[learning rate: 0.0021002]
	Learning Rate: 0.00210018
	LOSS [training: 4.9759113551176695 | validation: 5.346028495970984]
	TIME [epoch: 9.81 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.859104985283514		[learning rate: 0.0020952]
	Learning Rate: 0.00209523
	LOSS [training: 4.859104985283514 | validation: 5.320821015593629]
	TIME [epoch: 9.78 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.823663324984205		[learning rate: 0.0020903]
	Learning Rate: 0.00209028
	LOSS [training: 4.823663324984205 | validation: 5.357650749840998]
	TIME [epoch: 9.81 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824849269605634		[learning rate: 0.0020854]
	Learning Rate: 0.00208535
	LOSS [training: 4.824849269605634 | validation: 5.3435108257821655]
	TIME [epoch: 9.78 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813256989987634		[learning rate: 0.0020804]
	Learning Rate: 0.00208043
	LOSS [training: 4.813256989987634 | validation: 5.3849136936245]
	TIME [epoch: 9.82 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872183673762988		[learning rate: 0.0020755]
	Learning Rate: 0.00207553
	LOSS [training: 4.872183673762988 | validation: 5.359946102102752]
	TIME [epoch: 9.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863585790663111		[learning rate: 0.0020706]
	Learning Rate: 0.00207063
	LOSS [training: 4.863585790663111 | validation: 5.439789430388749]
	TIME [epoch: 9.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836950665719206		[learning rate: 0.0020657]
	Learning Rate: 0.00206575
	LOSS [training: 4.836950665719206 | validation: 5.309941250585689]
	TIME [epoch: 9.8 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.888016511996876		[learning rate: 0.0020609]
	Learning Rate: 0.00206087
	LOSS [training: 4.888016511996876 | validation: 5.317527721433184]
	TIME [epoch: 9.81 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872913050018312		[learning rate: 0.002056]
	Learning Rate: 0.00205601
	LOSS [training: 4.872913050018312 | validation: 5.675548754342694]
	TIME [epoch: 9.81 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.962631460717481		[learning rate: 0.0020512]
	Learning Rate: 0.00205116
	LOSS [training: 4.962631460717481 | validation: 5.479434803980786]
	TIME [epoch: 9.81 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.884743614925251		[learning rate: 0.0020463]
	Learning Rate: 0.00204632
	LOSS [training: 4.884743614925251 | validation: 5.43623399464008]
	TIME [epoch: 9.83 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.894705674917357		[learning rate: 0.0020415]
	Learning Rate: 0.0020415
	LOSS [training: 4.894705674917357 | validation: 5.357200052071307]
	TIME [epoch: 9.82 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893615158978031		[learning rate: 0.0020367]
	Learning Rate: 0.00203668
	LOSS [training: 4.893615158978031 | validation: 5.38971593121533]
	TIME [epoch: 9.8 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.904589415566672		[learning rate: 0.0020319]
	Learning Rate: 0.00203188
	LOSS [training: 4.904589415566672 | validation: 5.390014964180241]
	TIME [epoch: 9.81 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.891212472099011		[learning rate: 0.0020271]
	Learning Rate: 0.00202708
	LOSS [training: 4.891212472099011 | validation: 5.396554984760038]
	TIME [epoch: 9.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.918275323868206		[learning rate: 0.0020223]
	Learning Rate: 0.0020223
	LOSS [training: 4.918275323868206 | validation: 5.411170859729538]
	TIME [epoch: 9.78 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.899133199778015		[learning rate: 0.0020175]
	Learning Rate: 0.00201753
	LOSS [training: 4.899133199778015 | validation: 5.331302102552838]
	TIME [epoch: 9.8 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.909878275669522		[learning rate: 0.0020128]
	Learning Rate: 0.00201277
	LOSS [training: 4.909878275669522 | validation: 5.387762744389513]
	TIME [epoch: 9.81 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.871346495748673		[learning rate: 0.002008]
	Learning Rate: 0.00200803
	LOSS [training: 4.871346495748673 | validation: 5.414124332962199]
	TIME [epoch: 9.81 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9141836519535635		[learning rate: 0.0020033]
	Learning Rate: 0.00200329
	LOSS [training: 4.9141836519535635 | validation: 5.34218522119608]
	TIME [epoch: 9.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867048069999104		[learning rate: 0.0019986]
	Learning Rate: 0.00199856
	LOSS [training: 4.867048069999104 | validation: 5.362557112827065]
	TIME [epoch: 9.79 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820524592421697		[learning rate: 0.0019938]
	Learning Rate: 0.00199385
	LOSS [training: 4.820524592421697 | validation: 5.557227473707669]
	TIME [epoch: 9.83 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903607724007002		[learning rate: 0.0019891]
	Learning Rate: 0.00198915
	LOSS [training: 4.903607724007002 | validation: 5.34302950564354]
	TIME [epoch: 9.83 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925335138225772		[learning rate: 0.0019845]
	Learning Rate: 0.00198445
	LOSS [training: 4.925335138225772 | validation: 5.407903812657168]
	TIME [epoch: 9.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.843731854031458		[learning rate: 0.0019798]
	Learning Rate: 0.00197977
	LOSS [training: 4.843731854031458 | validation: 5.3531392979623185]
	TIME [epoch: 9.8 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.866442119469541		[learning rate: 0.0019751]
	Learning Rate: 0.0019751
	LOSS [training: 4.866442119469541 | validation: 5.619534028361762]
	TIME [epoch: 9.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.964565866821886		[learning rate: 0.0019704]
	Learning Rate: 0.00197044
	LOSS [training: 4.964565866821886 | validation: 5.625374616637159]
	TIME [epoch: 9.81 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867499806102379		[learning rate: 0.0019658]
	Learning Rate: 0.0019658
	LOSS [training: 4.867499806102379 | validation: 5.327694528514305]
	TIME [epoch: 9.81 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804912423714508		[learning rate: 0.0019612]
	Learning Rate: 0.00196116
	LOSS [training: 4.804912423714508 | validation: 5.333957671786818]
	TIME [epoch: 9.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870169615343849		[learning rate: 0.0019565]
	Learning Rate: 0.00195653
	LOSS [training: 4.870169615343849 | validation: 5.3377588187005705]
	TIME [epoch: 9.83 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8922326855530605		[learning rate: 0.0019519]
	Learning Rate: 0.00195192
	LOSS [training: 4.8922326855530605 | validation: 5.368239104083655]
	TIME [epoch: 9.81 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908528302688254		[learning rate: 0.0019473]
	Learning Rate: 0.00194731
	LOSS [training: 4.908528302688254 | validation: 5.3570306703335335]
	TIME [epoch: 9.8 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839909933107276		[learning rate: 0.0019427]
	Learning Rate: 0.00194272
	LOSS [training: 4.839909933107276 | validation: 5.383091545620466]
	TIME [epoch: 9.81 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873161181780652		[learning rate: 0.0019381]
	Learning Rate: 0.00193814
	LOSS [training: 4.873161181780652 | validation: 5.3263649462346505]
	TIME [epoch: 9.83 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.919426011681563		[learning rate: 0.0019336]
	Learning Rate: 0.00193357
	LOSS [training: 4.919426011681563 | validation: 5.38353054559423]
	TIME [epoch: 9.82 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.940937210917158		[learning rate: 0.001929]
	Learning Rate: 0.001929
	LOSS [training: 4.940937210917158 | validation: 5.343469906469415]
	TIME [epoch: 9.81 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836083790989705		[learning rate: 0.0019245]
	Learning Rate: 0.00192445
	LOSS [training: 4.836083790989705 | validation: 5.342874713500273]
	TIME [epoch: 9.82 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862521426944637		[learning rate: 0.0019199]
	Learning Rate: 0.00191992
	LOSS [training: 4.862521426944637 | validation: 5.518381221054055]
	TIME [epoch: 9.83 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893717195245056		[learning rate: 0.0019154]
	Learning Rate: 0.00191539
	LOSS [training: 4.893717195245056 | validation: 5.330408985012995]
	TIME [epoch: 9.79 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.971996404151065		[learning rate: 0.0019109]
	Learning Rate: 0.00191087
	LOSS [training: 4.971996404151065 | validation: 5.380772295731739]
	TIME [epoch: 9.8 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872705516047936		[learning rate: 0.0019064]
	Learning Rate: 0.00190636
	LOSS [training: 4.872705516047936 | validation: 5.4571186393675895]
	TIME [epoch: 9.83 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.946509709308219		[learning rate: 0.0019019]
	Learning Rate: 0.00190186
	LOSS [training: 4.946509709308219 | validation: 5.3958219545405255]
	TIME [epoch: 9.78 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.874866155418189		[learning rate: 0.0018974]
	Learning Rate: 0.00189738
	LOSS [training: 4.874866155418189 | validation: 5.307477949417815]
	TIME [epoch: 9.78 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836556473806079		[learning rate: 0.0018929]
	Learning Rate: 0.0018929
	LOSS [training: 4.836556473806079 | validation: 5.372709001848562]
	TIME [epoch: 9.79 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8460451191185		[learning rate: 0.0018884]
	Learning Rate: 0.00188844
	LOSS [training: 4.8460451191185 | validation: 5.324932352977937]
	TIME [epoch: 9.82 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.911435078746304		[learning rate: 0.001884]
	Learning Rate: 0.00188398
	LOSS [training: 4.911435078746304 | validation: 5.311862852698368]
	TIME [epoch: 9.81 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.848418640870567		[learning rate: 0.0018795]
	Learning Rate: 0.00187954
	LOSS [training: 4.848418640870567 | validation: 5.344731373489075]
	TIME [epoch: 9.82 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.844200256834778		[learning rate: 0.0018751]
	Learning Rate: 0.00187511
	LOSS [training: 4.844200256834778 | validation: 5.306017680883499]
	TIME [epoch: 9.82 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846074046906375		[learning rate: 0.0018707]
	Learning Rate: 0.00187068
	LOSS [training: 4.846074046906375 | validation: 5.317427435697054]
	TIME [epoch: 9.78 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824510795231876		[learning rate: 0.0018663]
	Learning Rate: 0.00186627
	LOSS [training: 4.824510795231876 | validation: 5.320518073671651]
	TIME [epoch: 9.81 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.886132208315412		[learning rate: 0.0018619]
	Learning Rate: 0.00186187
	LOSS [training: 4.886132208315412 | validation: 5.377214609611233]
	TIME [epoch: 9.8 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.868550730660002		[learning rate: 0.0018575]
	Learning Rate: 0.00185748
	LOSS [training: 4.868550730660002 | validation: 5.4039809114827]
	TIME [epoch: 9.78 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.864274062537001		[learning rate: 0.0018531]
	Learning Rate: 0.00185309
	LOSS [training: 4.864274062537001 | validation: 5.380345854267489]
	TIME [epoch: 9.8 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863565395601621		[learning rate: 0.0018487]
	Learning Rate: 0.00184872
	LOSS [training: 4.863565395601621 | validation: 5.332333518944125]
	TIME [epoch: 9.79 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.909291476600687		[learning rate: 0.0018444]
	Learning Rate: 0.00184436
	LOSS [training: 4.909291476600687 | validation: 5.518226290148477]
	TIME [epoch: 9.78 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.931227356315879		[learning rate: 0.00184]
	Learning Rate: 0.00184001
	LOSS [training: 4.931227356315879 | validation: 5.414219075873894]
	TIME [epoch: 9.81 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.97856060508809		[learning rate: 0.0018357]
	Learning Rate: 0.00183567
	LOSS [training: 4.97856060508809 | validation: 5.3761132056975915]
	TIME [epoch: 9.76 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862272711517387		[learning rate: 0.0018313]
	Learning Rate: 0.00183134
	LOSS [training: 4.862272711517387 | validation: 5.3505373629848805]
	TIME [epoch: 9.77 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.907011152867307		[learning rate: 0.001827]
	Learning Rate: 0.00182702
	LOSS [training: 4.907011152867307 | validation: 5.377591067557481]
	TIME [epoch: 9.79 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.876895117368343		[learning rate: 0.0018227]
	Learning Rate: 0.00182271
	LOSS [training: 4.876895117368343 | validation: 5.3082472432279255]
	TIME [epoch: 9.77 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8587740413354235		[learning rate: 0.0018184]
	Learning Rate: 0.00181841
	LOSS [training: 4.8587740413354235 | validation: 5.3061713569457405]
	TIME [epoch: 9.77 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863918019972771		[learning rate: 0.0018141]
	Learning Rate: 0.00181412
	LOSS [training: 4.863918019972771 | validation: 5.371779320187411]
	TIME [epoch: 9.77 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.904168892968538		[learning rate: 0.0018098]
	Learning Rate: 0.00180984
	LOSS [training: 4.904168892968538 | validation: 5.330591799844158]
	TIME [epoch: 9.81 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.845184957544246		[learning rate: 0.0018056]
	Learning Rate: 0.00180557
	LOSS [training: 4.845184957544246 | validation: 5.4181648829375435]
	TIME [epoch: 9.78 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.895052190553718		[learning rate: 0.0018013]
	Learning Rate: 0.00180132
	LOSS [training: 4.895052190553718 | validation: 5.399847670234654]
	TIME [epoch: 9.77 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9507912025525345		[learning rate: 0.0017971]
	Learning Rate: 0.00179707
	LOSS [training: 4.9507912025525345 | validation: 5.49266466880997]
	TIME [epoch: 9.76 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.887490681340379		[learning rate: 0.0017928]
	Learning Rate: 0.00179283
	LOSS [training: 4.887490681340379 | validation: 5.36258295542538]
	TIME [epoch: 9.79 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.849321404365023		[learning rate: 0.0017886]
	Learning Rate: 0.0017886
	LOSS [training: 4.849321404365023 | validation: 5.3202731057675114]
	TIME [epoch: 9.78 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.904349667344841		[learning rate: 0.0017844]
	Learning Rate: 0.00178438
	LOSS [training: 4.904349667344841 | validation: 5.4639777197269375]
	TIME [epoch: 9.77 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862059231930161		[learning rate: 0.0017802]
	Learning Rate: 0.00178017
	LOSS [training: 4.862059231930161 | validation: 5.322898953589632]
	TIME [epoch: 9.78 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.864457051436514		[learning rate: 0.001776]
	Learning Rate: 0.00177597
	LOSS [training: 4.864457051436514 | validation: 5.4522935592888215]
	TIME [epoch: 9.76 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9209724153006515		[learning rate: 0.0017718]
	Learning Rate: 0.00177178
	LOSS [training: 4.9209724153006515 | validation: 5.467211181629185]
	TIME [epoch: 9.75 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832225342643919		[learning rate: 0.0017676]
	Learning Rate: 0.0017676
	LOSS [training: 4.832225342643919 | validation: 5.613380790911488]
	TIME [epoch: 9.79 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.888017071902716		[learning rate: 0.0017634]
	Learning Rate: 0.00176343
	LOSS [training: 4.888017071902716 | validation: 5.346022528088784]
	TIME [epoch: 9.78 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820143419576807		[learning rate: 0.0017593]
	Learning Rate: 0.00175927
	LOSS [training: 4.820143419576807 | validation: 5.288955349251913]
	TIME [epoch: 9.77 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839274067439521		[learning rate: 0.0017551]
	Learning Rate: 0.00175512
	LOSS [training: 4.839274067439521 | validation: 5.358601943942828]
	TIME [epoch: 9.77 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.86903125473704		[learning rate: 0.001751]
	Learning Rate: 0.00175098
	LOSS [training: 4.86903125473704 | validation: 5.341216370292313]
	TIME [epoch: 9.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.84945577695953		[learning rate: 0.0017469]
	Learning Rate: 0.00174685
	LOSS [training: 4.84945577695953 | validation: 5.323947036891444]
	TIME [epoch: 9.78 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872719768397078		[learning rate: 0.0017427]
	Learning Rate: 0.00174273
	LOSS [training: 4.872719768397078 | validation: 5.343555019384805]
	TIME [epoch: 9.75 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.881550772820598		[learning rate: 0.0017386]
	Learning Rate: 0.00173862
	LOSS [training: 4.881550772820598 | validation: 5.431257644405014]
	TIME [epoch: 9.76 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.917688136156086		[learning rate: 0.0017345]
	Learning Rate: 0.00173452
	LOSS [training: 4.917688136156086 | validation: 5.320147826462317]
	TIME [epoch: 9.76 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.852803586214266		[learning rate: 0.0017304]
	Learning Rate: 0.00173043
	LOSS [training: 4.852803586214266 | validation: 5.403784782792634]
	TIME [epoch: 9.77 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867661740383871		[learning rate: 0.0017263]
	Learning Rate: 0.00172635
	LOSS [training: 4.867661740383871 | validation: 5.429802995933539]
	TIME [epoch: 9.75 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.890906778083406		[learning rate: 0.0017223]
	Learning Rate: 0.00172228
	LOSS [training: 4.890906778083406 | validation: 5.444597931203917]
	TIME [epoch: 9.78 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812099249602662		[learning rate: 0.0017182]
	Learning Rate: 0.00171821
	LOSS [training: 4.812099249602662 | validation: 5.38626978797304]
	TIME [epoch: 9.78 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908406698946711		[learning rate: 0.0017142]
	Learning Rate: 0.00171416
	LOSS [training: 4.908406698946711 | validation: 5.294374676966724]
	TIME [epoch: 9.76 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.83482749699448		[learning rate: 0.0017101]
	Learning Rate: 0.00171012
	LOSS [training: 4.83482749699448 | validation: 5.318505029484725]
	TIME [epoch: 9.75 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8342476477896525		[learning rate: 0.0017061]
	Learning Rate: 0.00170608
	LOSS [training: 4.8342476477896525 | validation: 5.331691330145422]
	TIME [epoch: 9.76 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8306775307575505		[learning rate: 0.0017021]
	Learning Rate: 0.00170206
	LOSS [training: 4.8306775307575505 | validation: 5.3058972558344815]
	TIME [epoch: 9.78 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.845966895642883		[learning rate: 0.001698]
	Learning Rate: 0.00169804
	LOSS [training: 4.845966895642883 | validation: 5.617925682436073]
	TIME [epoch: 9.77 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.969227142310045		[learning rate: 0.001694]
	Learning Rate: 0.00169404
	LOSS [training: 4.969227142310045 | validation: 5.334840635864282]
	TIME [epoch: 9.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.159076774881134		[learning rate: 0.00169]
	Learning Rate: 0.00169004
	LOSS [training: 5.159076774881134 | validation: 5.351187275219363]
	TIME [epoch: 9.77 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.833530075234187		[learning rate: 0.0016861]
	Learning Rate: 0.00168606
	LOSS [training: 4.833530075234187 | validation: 5.381180138010445]
	TIME [epoch: 9.77 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922229505468405		[learning rate: 0.0016821]
	Learning Rate: 0.00168208
	LOSS [training: 4.922229505468405 | validation: 5.532254893119472]
	TIME [epoch: 9.75 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.888282919434991		[learning rate: 0.0016781]
	Learning Rate: 0.00167811
	LOSS [training: 4.888282919434991 | validation: 5.335222421063528]
	TIME [epoch: 9.76 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832603657021899		[learning rate: 0.0016742]
	Learning Rate: 0.00167415
	LOSS [training: 4.832603657021899 | validation: 5.4750855975004225]
	TIME [epoch: 9.77 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.950811594599791		[learning rate: 0.0016702]
	Learning Rate: 0.0016702
	LOSS [training: 4.950811594599791 | validation: 5.492975951573121]
	TIME [epoch: 9.75 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.935405512859459		[learning rate: 0.0016663]
	Learning Rate: 0.00166626
	LOSS [training: 4.935405512859459 | validation: 5.488709127643706]
	TIME [epoch: 9.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870918107243332		[learning rate: 0.0016623]
	Learning Rate: 0.00166233
	LOSS [training: 4.870918107243332 | validation: 5.327831287992342]
	TIME [epoch: 9.76 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872143581675062		[learning rate: 0.0016584]
	Learning Rate: 0.00165841
	LOSS [training: 4.872143581675062 | validation: 5.348265901764051]
	TIME [epoch: 9.78 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903593796335235		[learning rate: 0.0016545]
	Learning Rate: 0.0016545
	LOSS [training: 4.903593796335235 | validation: 5.410755491495445]
	TIME [epoch: 9.75 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.938134549167114		[learning rate: 0.0016506]
	Learning Rate: 0.0016506
	LOSS [training: 4.938134549167114 | validation: 5.4564861672327405]
	TIME [epoch: 9.75 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926122805793779		[learning rate: 0.0016467]
	Learning Rate: 0.0016467
	LOSS [training: 4.926122805793779 | validation: 5.340905876539828]
	TIME [epoch: 9.75 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.896460700392847		[learning rate: 0.0016428]
	Learning Rate: 0.00164282
	LOSS [training: 4.896460700392847 | validation: 5.376036026742736]
	TIME [epoch: 9.77 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.881329690817784		[learning rate: 0.0016389]
	Learning Rate: 0.00163894
	LOSS [training: 4.881329690817784 | validation: 5.341749302474109]
	TIME [epoch: 9.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9300718773860925		[learning rate: 0.0016351]
	Learning Rate: 0.00163508
	LOSS [training: 4.9300718773860925 | validation: 5.338750532972102]
	TIME [epoch: 9.76 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.960543313437702		[learning rate: 0.0016312]
	Learning Rate: 0.00163122
	LOSS [training: 4.960543313437702 | validation: 5.3405236913260605]
	TIME [epoch: 9.77 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.986091972433631		[learning rate: 0.0016274]
	Learning Rate: 0.00162737
	LOSS [training: 4.986091972433631 | validation: 5.306341431903312]
	TIME [epoch: 9.76 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.842776079878528		[learning rate: 0.0016235]
	Learning Rate: 0.00162353
	LOSS [training: 4.842776079878528 | validation: 5.3200848785317465]
	TIME [epoch: 9.75 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.911632303083615		[learning rate: 0.0016197]
	Learning Rate: 0.0016197
	LOSS [training: 4.911632303083615 | validation: 5.330973693672463]
	TIME [epoch: 9.76 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.919420251154641		[learning rate: 0.0016159]
	Learning Rate: 0.00161588
	LOSS [training: 4.919420251154641 | validation: 5.41044857753144]
	TIME [epoch: 9.78 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.923675551402333		[learning rate: 0.0016121]
	Learning Rate: 0.00161207
	LOSS [training: 4.923675551402333 | validation: 5.394746743584488]
	TIME [epoch: 9.76 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8915570196193		[learning rate: 0.0016083]
	Learning Rate: 0.00160827
	LOSS [training: 4.8915570196193 | validation: 5.313087862856092]
	TIME [epoch: 9.75 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897008470243267		[learning rate: 0.0016045]
	Learning Rate: 0.00160448
	LOSS [training: 4.897008470243267 | validation: 5.292343780205442]
	TIME [epoch: 9.76 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.952659910649993		[learning rate: 0.0016007]
	Learning Rate: 0.00160069
	LOSS [training: 4.952659910649993 | validation: 5.406660049639572]
	TIME [epoch: 9.77 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.844842327554911		[learning rate: 0.0015969]
	Learning Rate: 0.00159692
	LOSS [training: 4.844842327554911 | validation: 5.4679419552976745]
	TIME [epoch: 9.76 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8782163453647485		[learning rate: 0.0015931]
	Learning Rate: 0.00159315
	LOSS [training: 4.8782163453647485 | validation: 5.37814126365822]
	TIME [epoch: 9.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863944162997809		[learning rate: 0.0015894]
	Learning Rate: 0.00158939
	LOSS [training: 4.863944162997809 | validation: 5.35797635849585]
	TIME [epoch: 9.77 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.842526023018972		[learning rate: 0.0015856]
	Learning Rate: 0.00158564
	LOSS [training: 4.842526023018972 | validation: 5.3511368869377245]
	TIME [epoch: 9.76 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.844311585266316		[learning rate: 0.0015819]
	Learning Rate: 0.0015819
	LOSS [training: 4.844311585266316 | validation: 5.38087486461481]
	TIME [epoch: 9.76 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.881971275972475		[learning rate: 0.0015782]
	Learning Rate: 0.00157817
	LOSS [training: 4.881971275972475 | validation: 5.356547261056062]
	TIME [epoch: 9.76 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8441584444333765		[learning rate: 0.0015744]
	Learning Rate: 0.00157445
	LOSS [training: 4.8441584444333765 | validation: 5.379672111898071]
	TIME [epoch: 9.78 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9198269073743015		[learning rate: 0.0015707]
	Learning Rate: 0.00157073
	LOSS [training: 4.9198269073743015 | validation: 5.500951751139926]
	TIME [epoch: 9.76 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.924778101774065		[learning rate: 0.001567]
	Learning Rate: 0.00156703
	LOSS [training: 4.924778101774065 | validation: 5.327848208870613]
	TIME [epoch: 9.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839214425975479		[learning rate: 0.0015633]
	Learning Rate: 0.00156333
	LOSS [training: 4.839214425975479 | validation: 5.321490272440131]
	TIME [epoch: 9.76 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.848959607005876		[learning rate: 0.0015596]
	Learning Rate: 0.00155964
	LOSS [training: 4.848959607005876 | validation: 5.335450167549787]
	TIME [epoch: 9.78 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.861450531046229		[learning rate: 0.001556]
	Learning Rate: 0.00155597
	LOSS [training: 4.861450531046229 | validation: 5.316855061841181]
	TIME [epoch: 9.76 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.860146149445262		[learning rate: 0.0015523]
	Learning Rate: 0.0015523
	LOSS [training: 4.860146149445262 | validation: 5.361119113144063]
	TIME [epoch: 9.76 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846482550485889		[learning rate: 0.0015486]
	Learning Rate: 0.00154863
	LOSS [training: 4.846482550485889 | validation: 5.449033615131775]
	TIME [epoch: 9.78 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872979436742491		[learning rate: 0.001545]
	Learning Rate: 0.00154498
	LOSS [training: 4.872979436742491 | validation: 5.325792389976484]
	TIME [epoch: 9.78 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836289555897399		[learning rate: 0.0015413]
	Learning Rate: 0.00154134
	LOSS [training: 4.836289555897399 | validation: 5.317334679210294]
	TIME [epoch: 9.76 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.866615527069773		[learning rate: 0.0015377]
	Learning Rate: 0.0015377
	LOSS [training: 4.866615527069773 | validation: 5.344914808188028]
	TIME [epoch: 9.76 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863558537250805		[learning rate: 0.0015341]
	Learning Rate: 0.00153407
	LOSS [training: 4.863558537250805 | validation: 5.314216029665421]
	TIME [epoch: 9.77 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824032024899472		[learning rate: 0.0015305]
	Learning Rate: 0.00153045
	LOSS [training: 4.824032024899472 | validation: 5.377199504000611]
	TIME [epoch: 9.76 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.920422298904304		[learning rate: 0.0015268]
	Learning Rate: 0.00152684
	LOSS [training: 4.920422298904304 | validation: 5.3874608140194145]
	TIME [epoch: 9.75 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.875145595726032		[learning rate: 0.0015232]
	Learning Rate: 0.00152324
	LOSS [training: 4.875145595726032 | validation: 5.313453359727496]
	TIME [epoch: 9.75 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.826774154286484		[learning rate: 0.0015196]
	Learning Rate: 0.00151965
	LOSS [training: 4.826774154286484 | validation: 5.3418850749837565]
	TIME [epoch: 9.77 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.87862815353602		[learning rate: 0.0015161]
	Learning Rate: 0.00151607
	LOSS [training: 4.87862815353602 | validation: 5.402817144137503]
	TIME [epoch: 9.75 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.884814522642768		[learning rate: 0.0015125]
	Learning Rate: 0.00151249
	LOSS [training: 4.884814522642768 | validation: 5.38619261067561]
	TIME [epoch: 9.75 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.899140803588592		[learning rate: 0.0015089]
	Learning Rate: 0.00150892
	LOSS [training: 4.899140803588592 | validation: 5.4065118546165625]
	TIME [epoch: 9.76 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.947534236130075		[learning rate: 0.0015054]
	Learning Rate: 0.00150536
	LOSS [training: 4.947534236130075 | validation: 5.5465396366419775]
	TIME [epoch: 9.77 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.909007118276971		[learning rate: 0.0015018]
	Learning Rate: 0.00150181
	LOSS [training: 4.909007118276971 | validation: 5.421169078590278]
	TIME [epoch: 9.76 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9685779237155305		[learning rate: 0.0014983]
	Learning Rate: 0.00149827
	LOSS [training: 4.9685779237155305 | validation: 5.367806603153358]
	TIME [epoch: 9.76 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925768824064276		[learning rate: 0.0014947]
	Learning Rate: 0.00149473
	LOSS [training: 4.925768824064276 | validation: 5.3340213124534435]
	TIME [epoch: 9.77 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8921361012609434		[learning rate: 0.0014912]
	Learning Rate: 0.00149121
	LOSS [training: 4.8921361012609434 | validation: 5.438521752121276]
	TIME [epoch: 9.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897354372989457		[learning rate: 0.0014877]
	Learning Rate: 0.00148769
	LOSS [training: 4.897354372989457 | validation: 5.33925966594552]
	TIME [epoch: 9.75 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.916833286459787		[learning rate: 0.0014842]
	Learning Rate: 0.00148418
	LOSS [training: 4.916833286459787 | validation: 5.54556536263774]
	TIME [epoch: 9.75 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.979911678325793		[learning rate: 0.0014807]
	Learning Rate: 0.00148068
	LOSS [training: 4.979911678325793 | validation: 5.367542906832593]
	TIME [epoch: 9.78 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.936715603655301		[learning rate: 0.0014772]
	Learning Rate: 0.00147719
	LOSS [training: 4.936715603655301 | validation: 5.3659878982006175]
	TIME [epoch: 9.75 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.910095199491961		[learning rate: 0.0014737]
	Learning Rate: 0.0014737
	LOSS [training: 4.910095199491961 | validation: 5.4269417552111365]
	TIME [epoch: 9.76 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.921214580497891		[learning rate: 0.0014702]
	Learning Rate: 0.00147023
	LOSS [training: 4.921214580497891 | validation: 5.35587254503558]
	TIME [epoch: 9.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.880518180144695		[learning rate: 0.0014668]
	Learning Rate: 0.00146676
	LOSS [training: 4.880518180144695 | validation: 5.325582994511238]
	TIME [epoch: 9.78 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.869119262803778		[learning rate: 0.0014633]
	Learning Rate: 0.0014633
	LOSS [training: 4.869119262803778 | validation: 5.379677400902035]
	TIME [epoch: 9.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.87439899931592		[learning rate: 0.0014598]
	Learning Rate: 0.00145985
	LOSS [training: 4.87439899931592 | validation: 5.322058793966028]
	TIME [epoch: 9.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.869274850597469		[learning rate: 0.0014564]
	Learning Rate: 0.0014564
	LOSS [training: 4.869274850597469 | validation: 5.333859904435398]
	TIME [epoch: 9.77 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8849241274974755		[learning rate: 0.001453]
	Learning Rate: 0.00145297
	LOSS [training: 4.8849241274974755 | validation: 5.437559667695325]
	TIME [epoch: 9.77 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.860860781769285		[learning rate: 0.0014495]
	Learning Rate: 0.00144954
	LOSS [training: 4.860860781769285 | validation: 5.339212586724791]
	TIME [epoch: 9.77 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9044192784597005		[learning rate: 0.0014461]
	Learning Rate: 0.00144612
	LOSS [training: 4.9044192784597005 | validation: 5.3422973815431405]
	TIME [epoch: 9.75 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8873068129022945		[learning rate: 0.0014427]
	Learning Rate: 0.00144271
	LOSS [training: 4.8873068129022945 | validation: 5.344378700802393]
	TIME [epoch: 9.78 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.90751122068524		[learning rate: 0.0014393]
	Learning Rate: 0.00143931
	LOSS [training: 4.90751122068524 | validation: 5.328962091784083]
	TIME [epoch: 9.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.964111244779565		[learning rate: 0.0014359]
	Learning Rate: 0.00143591
	LOSS [training: 4.964111244779565 | validation: 5.342710546515405]
	TIME [epoch: 9.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.993724362484563		[learning rate: 0.0014325]
	Learning Rate: 0.00143253
	LOSS [training: 4.993724362484563 | validation: 5.383724484674503]
	TIME [epoch: 9.75 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.999364324991812		[learning rate: 0.0014291]
	Learning Rate: 0.00142915
	LOSS [training: 4.999364324991812 | validation: 5.482586952833187]
	TIME [epoch: 9.77 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.02222126801023		[learning rate: 0.0014258]
	Learning Rate: 0.00142578
	LOSS [training: 5.02222126801023 | validation: 5.354085676549333]
	TIME [epoch: 9.75 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908563337304601		[learning rate: 0.0014224]
	Learning Rate: 0.00142241
	LOSS [training: 4.908563337304601 | validation: 5.4216734147188985]
	TIME [epoch: 9.75 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.493748251781631		[learning rate: 0.0014191]
	Learning Rate: 0.00141906
	LOSS [training: 5.493748251781631 | validation: 5.365927517105058]
	TIME [epoch: 9.76 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893500323849243		[learning rate: 0.0014157]
	Learning Rate: 0.00141571
	LOSS [training: 4.893500323849243 | validation: 5.354342303749613]
	TIME [epoch: 9.77 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.859408361955666		[learning rate: 0.0014124]
	Learning Rate: 0.00141237
	LOSS [training: 4.859408361955666 | validation: 5.373928761805509]
	TIME [epoch: 9.75 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885783886346105		[learning rate: 0.001409]
	Learning Rate: 0.00140904
	LOSS [training: 4.885783886346105 | validation: 5.321570621464807]
	TIME [epoch: 9.75 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.884961027715878		[learning rate: 0.0014057]
	Learning Rate: 0.00140572
	LOSS [training: 4.884961027715878 | validation: 5.355730313995111]
	TIME [epoch: 9.77 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.879429436787733		[learning rate: 0.0014024]
	Learning Rate: 0.0014024
	LOSS [training: 4.879429436787733 | validation: 5.404020526159595]
	TIME [epoch: 9.76 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912117958401516		[learning rate: 0.0013991]
	Learning Rate: 0.00139909
	LOSS [training: 4.912117958401516 | validation: 5.347337691670235]
	TIME [epoch: 9.76 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.93952254426176		[learning rate: 0.0013958]
	Learning Rate: 0.00139579
	LOSS [training: 4.93952254426176 | validation: 5.31902539138596]
	TIME [epoch: 9.75 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.914099083788027		[learning rate: 0.0013925]
	Learning Rate: 0.0013925
	LOSS [training: 4.914099083788027 | validation: 5.348946023721105]
	TIME [epoch: 9.79 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922537231578684		[learning rate: 0.0013892]
	Learning Rate: 0.00138921
	LOSS [training: 4.922537231578684 | validation: 5.3692140456519315]
	TIME [epoch: 9.75 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.932000054855923		[learning rate: 0.0013859]
	Learning Rate: 0.00138594
	LOSS [training: 4.932000054855923 | validation: 5.368183727849996]
	TIME [epoch: 9.76 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903680726876949		[learning rate: 0.0013827]
	Learning Rate: 0.00138267
	LOSS [training: 4.903680726876949 | validation: 5.3707395195399386]
	TIME [epoch: 9.76 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912203489643217		[learning rate: 0.0013794]
	Learning Rate: 0.00137941
	LOSS [training: 4.912203489643217 | validation: 5.438541813249254]
	TIME [epoch: 9.77 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.946109356112265		[learning rate: 0.0013762]
	Learning Rate: 0.00137615
	LOSS [training: 4.946109356112265 | validation: 5.338879079377587]
	TIME [epoch: 9.75 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9347096122689935		[learning rate: 0.0013729]
	Learning Rate: 0.00137291
	LOSS [training: 4.9347096122689935 | validation: 5.333053262683209]
	TIME [epoch: 9.76 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926016825528768		[learning rate: 0.0013697]
	Learning Rate: 0.00136967
	LOSS [training: 4.926016825528768 | validation: 5.341191313331044]
	TIME [epoch: 9.78 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.947370886742417		[learning rate: 0.0013664]
	Learning Rate: 0.00136644
	LOSS [training: 4.947370886742417 | validation: 5.351126947124057]
	TIME [epoch: 9.76 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.872003364122729		[learning rate: 0.0013632]
	Learning Rate: 0.00136321
	LOSS [training: 4.872003364122729 | validation: 5.356944787044617]
	TIME [epoch: 9.75 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9473862136443625		[learning rate: 0.00136]
	Learning Rate: 0.00136
	LOSS [training: 4.9473862136443625 | validation: 5.362503513470117]
	TIME [epoch: 9.75 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9997615294575395		[learning rate: 0.0013568]
	Learning Rate: 0.00135679
	LOSS [training: 4.9997615294575395 | validation: 5.403945758218843]
	TIME [epoch: 9.77 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.988864417934273		[learning rate: 0.0013536]
	Learning Rate: 0.00135359
	LOSS [training: 4.988864417934273 | validation: 5.366231011245609]
	TIME [epoch: 9.75 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959579773122996		[learning rate: 0.0013504]
	Learning Rate: 0.0013504
	LOSS [training: 4.959579773122996 | validation: 5.32798390514755]
	TIME [epoch: 9.75 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.954927845310835		[learning rate: 0.0013472]
	Learning Rate: 0.00134721
	LOSS [training: 4.954927845310835 | validation: 5.382537770115741]
	TIME [epoch: 9.75 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.93139954075229		[learning rate: 0.001344]
	Learning Rate: 0.00134403
	LOSS [training: 4.93139954075229 | validation: 5.481010884622221]
	TIME [epoch: 9.77 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.989263292014444		[learning rate: 0.0013409]
	Learning Rate: 0.00134086
	LOSS [training: 4.989263292014444 | validation: 5.606804374498052]
	TIME [epoch: 9.75 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.049832588057056		[learning rate: 0.0013377]
	Learning Rate: 0.0013377
	LOSS [training: 5.049832588057056 | validation: 5.467585081838552]
	TIME [epoch: 9.75 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8889833397141045		[learning rate: 0.0013345]
	Learning Rate: 0.00133455
	LOSS [training: 4.8889833397141045 | validation: 5.345012818006806]
	TIME [epoch: 9.77 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.88577569498287		[learning rate: 0.0013314]
	Learning Rate: 0.0013314
	LOSS [training: 4.88577569498287 | validation: 5.323204909101735]
	TIME [epoch: 9.76 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.976616453460036		[learning rate: 0.0013283]
	Learning Rate: 0.00132826
	LOSS [training: 4.976616453460036 | validation: 5.3676042451434]
	TIME [epoch: 9.77 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.920225962810386		[learning rate: 0.0013251]
	Learning Rate: 0.00132512
	LOSS [training: 4.920225962810386 | validation: 5.445682959299427]
	TIME [epoch: 9.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.905818899809096		[learning rate: 0.001322]
	Learning Rate: 0.001322
	LOSS [training: 4.905818899809096 | validation: 5.335177259720478]
	TIME [epoch: 9.77 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.923490366483521		[learning rate: 0.0013189]
	Learning Rate: 0.00131888
	LOSS [training: 4.923490366483521 | validation: 5.412726449415084]
	TIME [epoch: 9.75 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.92507701035875		[learning rate: 0.0013158]
	Learning Rate: 0.00131577
	LOSS [training: 4.92507701035875 | validation: 5.299817021487463]
	TIME [epoch: 9.75 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.91586093620958		[learning rate: 0.0013127]
	Learning Rate: 0.00131266
	LOSS [training: 4.91586093620958 | validation: 5.358595875025985]
	TIME [epoch: 9.75 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.884652271651887		[learning rate: 0.0013096]
	Learning Rate: 0.00130957
	LOSS [training: 4.884652271651887 | validation: 5.4668574131413745]
	TIME [epoch: 9.77 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9434312470247725		[learning rate: 0.0013065]
	Learning Rate: 0.00130648
	LOSS [training: 4.9434312470247725 | validation: 5.3953668006696445]
	TIME [epoch: 9.75 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.934650221181137		[learning rate: 0.0013034]
	Learning Rate: 0.0013034
	LOSS [training: 4.934650221181137 | validation: 5.341979663150296]
	TIME [epoch: 9.75 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.865348051118849		[learning rate: 0.0013003]
	Learning Rate: 0.00130032
	LOSS [training: 4.865348051118849 | validation: 5.400972633390669]
	TIME [epoch: 9.77 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.921654509971811		[learning rate: 0.0012973]
	Learning Rate: 0.00129726
	LOSS [training: 4.921654509971811 | validation: 5.481181573351519]
	TIME [epoch: 9.77 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.01795547230321		[learning rate: 0.0012942]
	Learning Rate: 0.0012942
	LOSS [training: 5.01795547230321 | validation: 5.333978136195262]
	TIME [epoch: 9.75 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926354818457559		[learning rate: 0.0012911]
	Learning Rate: 0.00129114
	LOSS [training: 4.926354818457559 | validation: 5.398336509239246]
	TIME [epoch: 9.75 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.949277948074498		[learning rate: 0.0012881]
	Learning Rate: 0.0012881
	LOSS [training: 4.949277948074498 | validation: 5.469123524115803]
	TIME [epoch: 9.77 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.935116770408909		[learning rate: 0.0012851]
	Learning Rate: 0.00128506
	LOSS [training: 4.935116770408909 | validation: 5.355234361744528]
	TIME [epoch: 9.76 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.987092392231975		[learning rate: 0.001282]
	Learning Rate: 0.00128203
	LOSS [training: 4.987092392231975 | validation: 5.382468419689028]
	TIME [epoch: 9.76 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959341656903383		[learning rate: 0.001279]
	Learning Rate: 0.001279
	LOSS [training: 4.959341656903383 | validation: 5.442769356782736]
	TIME [epoch: 9.76 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9660871026125415		[learning rate: 0.001276]
	Learning Rate: 0.00127599
	LOSS [training: 4.9660871026125415 | validation: 5.354090618772948]
	TIME [epoch: 9.78 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.899096664836353		[learning rate: 0.001273]
	Learning Rate: 0.00127298
	LOSS [training: 4.899096664836353 | validation: 5.3257729295587914]
	TIME [epoch: 9.75 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902257624943358		[learning rate: 0.00127]
	Learning Rate: 0.00126997
	LOSS [training: 4.902257624943358 | validation: 5.444866516894401]
	TIME [epoch: 9.75 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.910010100770998		[learning rate: 0.001267]
	Learning Rate: 0.00126698
	LOSS [training: 4.910010100770998 | validation: 5.392183799707066]
	TIME [epoch: 9.76 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863502407203181		[learning rate: 0.001264]
	Learning Rate: 0.00126399
	LOSS [training: 4.863502407203181 | validation: 5.337038103352088]
	TIME [epoch: 9.77 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870173175869422		[learning rate: 0.001261]
	Learning Rate: 0.00126101
	LOSS [training: 4.870173175869422 | validation: 5.337806154060242]
	TIME [epoch: 9.76 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862008697625272		[learning rate: 0.001258]
	Learning Rate: 0.00125803
	LOSS [training: 4.862008697625272 | validation: 5.358914950600898]
	TIME [epoch: 9.76 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922377145545238		[learning rate: 0.0012551]
	Learning Rate: 0.00125507
	LOSS [training: 4.922377145545238 | validation: 5.390522611344217]
	TIME [epoch: 9.78 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.900665079989187		[learning rate: 0.0012521]
	Learning Rate: 0.00125211
	LOSS [training: 4.900665079989187 | validation: 5.346392763661441]
	TIME [epoch: 9.76 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.90604138110559		[learning rate: 0.0012492]
	Learning Rate: 0.00124915
	LOSS [training: 4.90604138110559 | validation: 5.32237729499959]
	TIME [epoch: 9.76 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.887194902105352		[learning rate: 0.0012462]
	Learning Rate: 0.00124621
	LOSS [training: 4.887194902105352 | validation: 5.405448986276278]
	TIME [epoch: 9.76 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.946963519862178		[learning rate: 0.0012433]
	Learning Rate: 0.00124327
	LOSS [training: 4.946963519862178 | validation: 5.336290309275974]
	TIME [epoch: 9.78 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9410678422464125		[learning rate: 0.0012403]
	Learning Rate: 0.00124033
	LOSS [training: 4.9410678422464125 | validation: 5.36701879102331]
	TIME [epoch: 9.78 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.910288877432026		[learning rate: 0.0012374]
	Learning Rate: 0.00123741
	LOSS [training: 4.910288877432026 | validation: 5.35147585159688]
	TIME [epoch: 9.76 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.917978130759616		[learning rate: 0.0012345]
	Learning Rate: 0.00123449
	LOSS [training: 4.917978130759616 | validation: 5.339314831633902]
	TIME [epoch: 9.75 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893124982935718		[learning rate: 0.0012316]
	Learning Rate: 0.00123158
	LOSS [training: 4.893124982935718 | validation: 5.352359138014822]
	TIME [epoch: 9.77 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873552365150036		[learning rate: 0.0012287]
	Learning Rate: 0.00122867
	LOSS [training: 4.873552365150036 | validation: 5.382031404014824]
	TIME [epoch: 9.77 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8923289177590785		[learning rate: 0.0012258]
	Learning Rate: 0.00122577
	LOSS [training: 4.8923289177590785 | validation: 5.346065777084404]
	TIME [epoch: 9.76 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.89953683481029		[learning rate: 0.0012229]
	Learning Rate: 0.00122288
	LOSS [training: 4.89953683481029 | validation: 5.31827595352702]
	TIME [epoch: 9.78 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.87389741149654		[learning rate: 0.00122]
	Learning Rate: 0.00122
	LOSS [training: 4.87389741149654 | validation: 5.315507682384614]
	TIME [epoch: 9.77 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.886943208951895		[learning rate: 0.0012171]
	Learning Rate: 0.00121712
	LOSS [training: 4.886943208951895 | validation: 5.327607385868804]
	TIME [epoch: 9.76 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8746418782532555		[learning rate: 0.0012142]
	Learning Rate: 0.00121425
	LOSS [training: 4.8746418782532555 | validation: 5.345375854450682]
	TIME [epoch: 9.77 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885326997674692		[learning rate: 0.0012114]
	Learning Rate: 0.00121138
	LOSS [training: 4.885326997674692 | validation: 5.38678070593584]
	TIME [epoch: 9.77 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.865634807118148		[learning rate: 0.0012085]
	Learning Rate: 0.00120853
	LOSS [training: 4.865634807118148 | validation: 5.368142120684538]
	TIME [epoch: 9.77 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.857798863280581		[learning rate: 0.0012057]
	Learning Rate: 0.00120568
	LOSS [training: 4.857798863280581 | validation: 5.400641537287064]
	TIME [epoch: 9.76 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.845436320084365		[learning rate: 0.0012028]
	Learning Rate: 0.00120283
	LOSS [training: 4.845436320084365 | validation: 5.327459572006271]
	TIME [epoch: 9.77 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.855287839635655		[learning rate: 0.0012]
	Learning Rate: 0.0012
	LOSS [training: 4.855287839635655 | validation: 5.342063549984378]
	TIME [epoch: 9.78 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.984742340570458		[learning rate: 0.0011972]
	Learning Rate: 0.00119716
	LOSS [training: 4.984742340570458 | validation: 5.426961748927209]
	TIME [epoch: 9.77 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.955396965473868		[learning rate: 0.0011943]
	Learning Rate: 0.00119434
	LOSS [training: 4.955396965473868 | validation: 5.398881239302443]
	TIME [epoch: 9.77 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.020230264206356		[learning rate: 0.0011915]
	Learning Rate: 0.00119152
	LOSS [training: 5.020230264206356 | validation: 5.473722537218186]
	TIME [epoch: 9.77 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.044057376644012		[learning rate: 0.0011887]
	Learning Rate: 0.00118871
	LOSS [training: 5.044057376644012 | validation: 5.371434057075578]
	TIME [epoch: 9.79 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.007559817959859		[learning rate: 0.0011859]
	Learning Rate: 0.00118591
	LOSS [training: 5.007559817959859 | validation: 5.463314347129917]
	TIME [epoch: 9.76 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.960090851870359		[learning rate: 0.0011831]
	Learning Rate: 0.00118311
	LOSS [training: 4.960090851870359 | validation: 5.386866800126172]
	TIME [epoch: 9.77 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959473743718453		[learning rate: 0.0011803]
	Learning Rate: 0.00118032
	LOSS [training: 4.959473743718453 | validation: 5.3628437144920635]
	TIME [epoch: 9.78 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.968838454778653		[learning rate: 0.0011775]
	Learning Rate: 0.00117754
	LOSS [training: 4.968838454778653 | validation: 5.3458658648743915]
	TIME [epoch: 9.77 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.004576270967353		[learning rate: 0.0011748]
	Learning Rate: 0.00117476
	LOSS [training: 5.004576270967353 | validation: 5.34063615310354]
	TIME [epoch: 9.77 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.941301933588288		[learning rate: 0.001172]
	Learning Rate: 0.00117199
	LOSS [training: 4.941301933588288 | validation: 5.35994813327997]
	TIME [epoch: 9.76 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.949081305854725		[learning rate: 0.0011692]
	Learning Rate: 0.00116922
	LOSS [training: 4.949081305854725 | validation: 5.387511920990526]
	TIME [epoch: 9.79 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.975476153568172		[learning rate: 0.0011665]
	Learning Rate: 0.00116646
	LOSS [training: 4.975476153568172 | validation: 5.403652231929546]
	TIME [epoch: 9.77 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.977921394261406		[learning rate: 0.0011637]
	Learning Rate: 0.00116371
	LOSS [training: 4.977921394261406 | validation: 5.344735380777418]
	TIME [epoch: 9.77 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.874291844927616		[learning rate: 0.001161]
	Learning Rate: 0.00116097
	LOSS [training: 4.874291844927616 | validation: 5.336409386992184]
	TIME [epoch: 9.77 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8878531964840155		[learning rate: 0.0011582]
	Learning Rate: 0.00115823
	LOSS [training: 4.8878531964840155 | validation: 5.324972373316618]
	TIME [epoch: 9.78 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.888775727375954		[learning rate: 0.0011555]
	Learning Rate: 0.0011555
	LOSS [training: 4.888775727375954 | validation: 5.413242746280876]
	TIME [epoch: 9.77 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.989747348982659		[learning rate: 0.0011528]
	Learning Rate: 0.00115277
	LOSS [training: 4.989747348982659 | validation: 5.310286719875369]
	TIME [epoch: 9.77 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.900855580615745		[learning rate: 0.0011501]
	Learning Rate: 0.00115005
	LOSS [training: 4.900855580615745 | validation: 5.3198558473361235]
	TIME [epoch: 9.78 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912523184283302		[learning rate: 0.0011473]
	Learning Rate: 0.00114734
	LOSS [training: 4.912523184283302 | validation: 5.32999871366999]
	TIME [epoch: 9.77 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.916068480170786		[learning rate: 0.0011446]
	Learning Rate: 0.00114463
	LOSS [training: 4.916068480170786 | validation: 5.34505061847374]
	TIME [epoch: 9.76 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9021943271196635		[learning rate: 0.0011419]
	Learning Rate: 0.00114193
	LOSS [training: 4.9021943271196635 | validation: 5.536412304087849]
	TIME [epoch: 9.76 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.023397511333971		[learning rate: 0.0011392]
	Learning Rate: 0.00113924
	LOSS [training: 5.023397511333971 | validation: 5.356261797001203]
	TIME [epoch: 9.78 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.89247859553129		[learning rate: 0.0011366]
	Learning Rate: 0.00113655
	LOSS [training: 4.89247859553129 | validation: 5.334270517282374]
	TIME [epoch: 9.77 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.841618995377346		[learning rate: 0.0011339]
	Learning Rate: 0.00113387
	LOSS [training: 4.841618995377346 | validation: 5.318441426029255]
	TIME [epoch: 9.76 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897245378267682		[learning rate: 0.0011312]
	Learning Rate: 0.0011312
	LOSS [training: 4.897245378267682 | validation: 5.376061280700276]
	TIME [epoch: 9.76 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8886827882156485		[learning rate: 0.0011285]
	Learning Rate: 0.00112853
	LOSS [training: 4.8886827882156485 | validation: 5.3704273576374595]
	TIME [epoch: 9.77 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.868567300132213		[learning rate: 0.0011259]
	Learning Rate: 0.00112587
	LOSS [training: 4.868567300132213 | validation: 5.353821422300623]
	TIME [epoch: 9.77 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8686376490140315		[learning rate: 0.0011232]
	Learning Rate: 0.00112321
	LOSS [training: 4.8686376490140315 | validation: 5.342068568605367]
	TIME [epoch: 9.76 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.898212076269029		[learning rate: 0.0011206]
	Learning Rate: 0.00112056
	LOSS [training: 4.898212076269029 | validation: 5.313669831570572]
	TIME [epoch: 9.78 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.905039702138717		[learning rate: 0.0011179]
	Learning Rate: 0.00111792
	LOSS [training: 4.905039702138717 | validation: 5.3003396328350645]
	TIME [epoch: 9.76 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.840974341278238		[learning rate: 0.0011153]
	Learning Rate: 0.00111528
	LOSS [training: 4.840974341278238 | validation: 5.35596621745691]
	TIME [epoch: 9.76 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8935406484197035		[learning rate: 0.0011127]
	Learning Rate: 0.00111265
	LOSS [training: 4.8935406484197035 | validation: 5.316382057808457]
	TIME [epoch: 9.76 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902517858554424		[learning rate: 0.00111]
	Learning Rate: 0.00111003
	LOSS [training: 4.902517858554424 | validation: 5.395631771204791]
	TIME [epoch: 9.79 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.906301997737825		[learning rate: 0.0011074]
	Learning Rate: 0.00110741
	LOSS [training: 4.906301997737825 | validation: 5.320655949446169]
	TIME [epoch: 9.76 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.857398704394244		[learning rate: 0.0011048]
	Learning Rate: 0.0011048
	LOSS [training: 4.857398704394244 | validation: 5.323023201475365]
	TIME [epoch: 9.76 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863290752895525		[learning rate: 0.0011022]
	Learning Rate: 0.00110219
	LOSS [training: 4.863290752895525 | validation: 5.372073779939583]
	TIME [epoch: 9.76 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.896499425796906		[learning rate: 0.0010996]
	Learning Rate: 0.00109959
	LOSS [training: 4.896499425796906 | validation: 5.3655740638282134]
	TIME [epoch: 9.78 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867097513561538		[learning rate: 0.001097]
	Learning Rate: 0.001097
	LOSS [training: 4.867097513561538 | validation: 5.347677343840394]
	TIME [epoch: 9.76 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.890230161195423		[learning rate: 0.0010944]
	Learning Rate: 0.00109441
	LOSS [training: 4.890230161195423 | validation: 5.322853889643889]
	TIME [epoch: 9.76 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8916318110018455		[learning rate: 0.0010918]
	Learning Rate: 0.00109183
	LOSS [training: 4.8916318110018455 | validation: 5.38990637446028]
	TIME [epoch: 9.77 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.98716966278989		[learning rate: 0.0010893]
	Learning Rate: 0.00108925
	LOSS [training: 4.98716966278989 | validation: 5.354951124247162]
	TIME [epoch: 9.77 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.93974348999547		[learning rate: 0.0010867]
	Learning Rate: 0.00108668
	LOSS [training: 4.93974348999547 | validation: 5.358842278291023]
	TIME [epoch: 9.76 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922169562264245		[learning rate: 0.0010841]
	Learning Rate: 0.00108412
	LOSS [training: 4.922169562264245 | validation: 5.348936949615125]
	TIME [epoch: 9.77 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.984326361684553		[learning rate: 0.0010816]
	Learning Rate: 0.00108156
	LOSS [training: 4.984326361684553 | validation: 5.372829873863589]
	TIME [epoch: 9.78 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.991130459769501		[learning rate: 0.001079]
	Learning Rate: 0.00107901
	LOSS [training: 4.991130459769501 | validation: 5.37415851735773]
	TIME [epoch: 9.76 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.031394088434317		[learning rate: 0.0010765]
	Learning Rate: 0.00107647
	LOSS [training: 5.031394088434317 | validation: 5.349586380743843]
	TIME [epoch: 9.76 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.944820164191667		[learning rate: 0.0010739]
	Learning Rate: 0.00107393
	LOSS [training: 4.944820164191667 | validation: 5.338074864546274]
	TIME [epoch: 9.77 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.015649424727217		[learning rate: 0.0010714]
	Learning Rate: 0.00107139
	LOSS [training: 5.015649424727217 | validation: 5.3748807039390405]
	TIME [epoch: 9.79 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.939248072838362		[learning rate: 0.0010689]
	Learning Rate: 0.00106887
	LOSS [training: 4.939248072838362 | validation: 5.464975245853822]
	TIME [epoch: 9.76 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8959222739580825		[learning rate: 0.0010663]
	Learning Rate: 0.00106634
	LOSS [training: 4.8959222739580825 | validation: 5.349984952434666]
	TIME [epoch: 9.77 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.931855342958511		[learning rate: 0.0010638]
	Learning Rate: 0.00106383
	LOSS [training: 4.931855342958511 | validation: 5.361500313030223]
	TIME [epoch: 9.76 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.918898834771598		[learning rate: 0.0010613]
	Learning Rate: 0.00106132
	LOSS [training: 4.918898834771598 | validation: 5.3435231922840964]
	TIME [epoch: 9.77 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8932121647309215		[learning rate: 0.0010588]
	Learning Rate: 0.00105882
	LOSS [training: 4.8932121647309215 | validation: 5.367121863407539]
	TIME [epoch: 9.76 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873526286833992		[learning rate: 0.0010563]
	Learning Rate: 0.00105632
	LOSS [training: 4.873526286833992 | validation: 5.331206302992622]
	TIME [epoch: 9.77 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.892195007526282		[learning rate: 0.0010538]
	Learning Rate: 0.00105383
	LOSS [training: 4.892195007526282 | validation: 5.3343342513939955]
	TIME [epoch: 9.77 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.982968671575596		[learning rate: 0.0010513]
	Learning Rate: 0.00105134
	LOSS [training: 4.982968671575596 | validation: 5.3054366006522375]
	TIME [epoch: 9.76 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.874562174825693		[learning rate: 0.0010489]
	Learning Rate: 0.00104886
	LOSS [training: 4.874562174825693 | validation: 5.3609928209604165]
	TIME [epoch: 9.77 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.891656971068345		[learning rate: 0.0010464]
	Learning Rate: 0.00104639
	LOSS [training: 4.891656971068345 | validation: 5.410665815998193]
	TIME [epoch: 9.77 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.918079606742941		[learning rate: 0.0010439]
	Learning Rate: 0.00104392
	LOSS [training: 4.918079606742941 | validation: 5.321952051554251]
	TIME [epoch: 9.78 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.942651972921892		[learning rate: 0.0010415]
	Learning Rate: 0.00104146
	LOSS [training: 4.942651972921892 | validation: 5.373795765206053]
	TIME [epoch: 9.77 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.88914818886696		[learning rate: 0.001039]
	Learning Rate: 0.001039
	LOSS [training: 4.88914818886696 | validation: 5.335483152961429]
	TIME [epoch: 9.76 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.882275400537853		[learning rate: 0.0010365]
	Learning Rate: 0.00103655
	LOSS [training: 4.882275400537853 | validation: 5.317121395108955]
	TIME [epoch: 9.77 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8575667120868165		[learning rate: 0.0010341]
	Learning Rate: 0.0010341
	LOSS [training: 4.8575667120868165 | validation: 5.352789871349256]
	TIME [epoch: 9.78 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.876020611143925		[learning rate: 0.0010317]
	Learning Rate: 0.00103166
	LOSS [training: 4.876020611143925 | validation: 5.329764094780586]
	TIME [epoch: 9.76 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839811524133546		[learning rate: 0.0010292]
	Learning Rate: 0.00102923
	LOSS [training: 4.839811524133546 | validation: 5.4045091939789485]
	TIME [epoch: 9.77 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.942133104014916		[learning rate: 0.0010268]
	Learning Rate: 0.0010268
	LOSS [training: 4.942133104014916 | validation: 5.320885810214056]
	TIME [epoch: 9.77 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.884420254324337		[learning rate: 0.0010244]
	Learning Rate: 0.00102438
	LOSS [training: 4.884420254324337 | validation: 5.346686005928698]
	TIME [epoch: 9.78 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.899201821305681		[learning rate: 0.001022]
	Learning Rate: 0.00102196
	LOSS [training: 4.899201821305681 | validation: 5.3418272792834545]
	TIME [epoch: 9.76 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.882674907901258		[learning rate: 0.0010196]
	Learning Rate: 0.00101955
	LOSS [training: 4.882674907901258 | validation: 5.333457232656363]
	TIME [epoch: 9.77 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8871202854724105		[learning rate: 0.0010171]
	Learning Rate: 0.00101715
	LOSS [training: 4.8871202854724105 | validation: 5.331531820156414]
	TIME [epoch: 9.79 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.941799845826973		[learning rate: 0.0010147]
	Learning Rate: 0.00101475
	LOSS [training: 4.941799845826973 | validation: 5.444597882446096]
	TIME [epoch: 9.77 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9533673278092705		[learning rate: 0.0010124]
	Learning Rate: 0.00101236
	LOSS [training: 4.9533673278092705 | validation: 5.371871011345227]
	TIME [epoch: 9.77 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922998384782095		[learning rate: 0.00101]
	Learning Rate: 0.00100997
	LOSS [training: 4.922998384782095 | validation: 5.403324813490191]
	TIME [epoch: 9.77 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.976066384420932		[learning rate: 0.0010076]
	Learning Rate: 0.00100759
	LOSS [training: 4.976066384420932 | validation: 5.411932317361838]
	TIME [epoch: 9.78 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.891444239074223		[learning rate: 0.0010052]
	Learning Rate: 0.00100521
	LOSS [training: 4.891444239074223 | validation: 5.374757560941637]
	TIME [epoch: 9.77 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.970177184491625		[learning rate: 0.0010028]
	Learning Rate: 0.00100284
	LOSS [training: 4.970177184491625 | validation: 5.4182627482462316]
	TIME [epoch: 9.76 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.075612133609116		[learning rate: 0.0010005]
	Learning Rate: 0.00100047
	LOSS [training: 5.075612133609116 | validation: 5.3532516254060925]
	TIME [epoch: 9.78 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.975458169978978		[learning rate: 0.00099811]
	Learning Rate: 0.000998112
	LOSS [training: 4.975458169978978 | validation: 5.358154796178616]
	TIME [epoch: 9.77 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.964560126105894		[learning rate: 0.00099576]
	Learning Rate: 0.000995758
	LOSS [training: 4.964560126105894 | validation: 5.505817506496104]
	TIME [epoch: 9.77 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9781124169388535		[learning rate: 0.00099341]
	Learning Rate: 0.000993409
	LOSS [training: 4.9781124169388535 | validation: 5.332727227548689]
	TIME [epoch: 9.77 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.938017180315235		[learning rate: 0.00099107]
	Learning Rate: 0.000991066
	LOSS [training: 4.938017180315235 | validation: 5.353241988309702]
	TIME [epoch: 9.79 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.950575905131703		[learning rate: 0.00098873]
	Learning Rate: 0.000988728
	LOSS [training: 4.950575905131703 | validation: 5.398796150102403]
	TIME [epoch: 9.77 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.957788635901963		[learning rate: 0.0009864]
	Learning Rate: 0.000986396
	LOSS [training: 4.957788635901963 | validation: 5.373239844913489]
	TIME [epoch: 9.77 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.901369352423587		[learning rate: 0.00098407]
	Learning Rate: 0.000984069
	LOSS [training: 4.901369352423587 | validation: 5.364452827890664]
	TIME [epoch: 9.77 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.893334291459395		[learning rate: 0.00098175]
	Learning Rate: 0.000981748
	LOSS [training: 4.893334291459395 | validation: 5.322901106520352]
	TIME [epoch: 9.79 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9276859875041925		[learning rate: 0.00097943]
	Learning Rate: 0.000979432
	LOSS [training: 4.9276859875041925 | validation: 5.432124372567292]
	TIME [epoch: 9.77 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959900914653717		[learning rate: 0.00097712]
	Learning Rate: 0.000977122
	LOSS [training: 4.959900914653717 | validation: 5.368671699045685]
	TIME [epoch: 9.77 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.984345019506943		[learning rate: 0.00097482]
	Learning Rate: 0.000974817
	LOSS [training: 4.984345019506943 | validation: 5.453934561139051]
	TIME [epoch: 9.78 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.995148267989821		[learning rate: 0.00097252]
	Learning Rate: 0.000972517
	LOSS [training: 4.995148267989821 | validation: 5.418363409880815]
	TIME [epoch: 9.78 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.958550138719726		[learning rate: 0.00097022]
	Learning Rate: 0.000970224
	LOSS [training: 4.958550138719726 | validation: 5.390399621130509]
	TIME [epoch: 9.77 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.947017486480695		[learning rate: 0.00096793]
	Learning Rate: 0.000967935
	LOSS [training: 4.947017486480695 | validation: 5.350316314514319]
	TIME [epoch: 9.77 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.932037695404414		[learning rate: 0.00096565]
	Learning Rate: 0.000965652
	LOSS [training: 4.932037695404414 | validation: 5.358778030245877]
	TIME [epoch: 9.79 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.961662800197007		[learning rate: 0.00096337]
	Learning Rate: 0.000963374
	LOSS [training: 4.961662800197007 | validation: 5.3899490629567035]
	TIME [epoch: 9.77 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.936212451357632		[learning rate: 0.0009611]
	Learning Rate: 0.000961101
	LOSS [training: 4.936212451357632 | validation: 5.351547566496361]
	TIME [epoch: 9.77 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9080556214871125		[learning rate: 0.00095883]
	Learning Rate: 0.000958834
	LOSS [training: 4.9080556214871125 | validation: 5.351327092437794]
	TIME [epoch: 9.77 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.89345397482002		[learning rate: 0.00095657]
	Learning Rate: 0.000956572
	LOSS [training: 4.89345397482002 | validation: 5.3609646172695715]
	TIME [epoch: 9.79 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.892998291666068		[learning rate: 0.00095432]
	Learning Rate: 0.000954316
	LOSS [training: 4.892998291666068 | validation: 5.351809838956021]
	TIME [epoch: 9.77 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.877660124009959		[learning rate: 0.00095207]
	Learning Rate: 0.000952065
	LOSS [training: 4.877660124009959 | validation: 5.376781719797869]
	TIME [epoch: 9.77 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.939482533251288		[learning rate: 0.00094982]
	Learning Rate: 0.000949819
	LOSS [training: 4.939482533251288 | validation: 5.355243388779189]
	TIME [epoch: 9.77 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.929771725129969		[learning rate: 0.00094758]
	Learning Rate: 0.000947579
	LOSS [training: 4.929771725129969 | validation: 5.339189776511737]
	TIME [epoch: 9.77 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.942833729060234		[learning rate: 0.00094534]
	Learning Rate: 0.000945344
	LOSS [training: 4.942833729060234 | validation: 5.375060948110977]
	TIME [epoch: 9.77 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9167899051847845		[learning rate: 0.00094311]
	Learning Rate: 0.000943114
	LOSS [training: 4.9167899051847845 | validation: 5.420186937177116]
	TIME [epoch: 9.77 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.946856549272608		[learning rate: 0.00094089]
	Learning Rate: 0.000940889
	LOSS [training: 4.946856549272608 | validation: 5.420000685872533]
	TIME [epoch: 9.78 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9837994269083055		[learning rate: 0.00093867]
	Learning Rate: 0.00093867
	LOSS [training: 4.9837994269083055 | validation: 5.405301083532733]
	TIME [epoch: 9.77 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.962626926774265		[learning rate: 0.00093646]
	Learning Rate: 0.000936456
	LOSS [training: 4.962626926774265 | validation: 5.37714091005301]
	TIME [epoch: 9.77 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.953846664536114		[learning rate: 0.00093425]
	Learning Rate: 0.000934246
	LOSS [training: 4.953846664536114 | validation: 5.33745738259715]
	TIME [epoch: 9.77 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.859000740750045		[learning rate: 0.00093204]
	Learning Rate: 0.000932043
	LOSS [training: 4.859000740750045 | validation: 5.309287447804426]
	TIME [epoch: 9.8 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.891987809936381		[learning rate: 0.00092984]
	Learning Rate: 0.000929844
	LOSS [training: 4.891987809936381 | validation: 5.328082482268472]
	TIME [epoch: 9.75 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.921950196954282		[learning rate: 0.00092765]
	Learning Rate: 0.000927651
	LOSS [training: 4.921950196954282 | validation: 5.3616755913851994]
	TIME [epoch: 9.76 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873515635051949		[learning rate: 0.00092546]
	Learning Rate: 0.000925463
	LOSS [training: 4.873515635051949 | validation: 5.348468986359899]
	TIME [epoch: 9.77 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863269310674225		[learning rate: 0.00092328]
	Learning Rate: 0.00092328
	LOSS [training: 4.863269310674225 | validation: 5.306847530295632]
	TIME [epoch: 9.78 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.835234588893436		[learning rate: 0.0009211]
	Learning Rate: 0.000921102
	LOSS [training: 4.835234588893436 | validation: 5.314132222845536]
	TIME [epoch: 9.77 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8629961603047125		[learning rate: 0.00091893]
	Learning Rate: 0.000918929
	LOSS [training: 4.8629961603047125 | validation: 5.329881142717396]
	TIME [epoch: 9.76 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.835869200121363		[learning rate: 0.00091676]
	Learning Rate: 0.000916762
	LOSS [training: 4.835869200121363 | validation: 5.311340275849914]
	TIME [epoch: 9.78 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8686447599356315		[learning rate: 0.0009146]
	Learning Rate: 0.000914599
	LOSS [training: 4.8686447599356315 | validation: 5.332106340116572]
	TIME [epoch: 9.77 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.926619524161336		[learning rate: 0.00091244]
	Learning Rate: 0.000912442
	LOSS [training: 4.926619524161336 | validation: 5.5256100704383355]
	TIME [epoch: 9.77 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8318864849800365		[learning rate: 0.00091029]
	Learning Rate: 0.000910289
	LOSS [training: 4.8318864849800365 | validation: 5.3141056258000985]
	TIME [epoch: 9.77 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789453240158517		[learning rate: 0.00090814]
	Learning Rate: 0.000908142
	LOSS [training: 4.789453240158517 | validation: 5.323407675434931]
	TIME [epoch: 9.79 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8314767540911445		[learning rate: 0.000906]
	Learning Rate: 0.000906
	LOSS [training: 4.8314767540911445 | validation: 5.42139548374586]
	TIME [epoch: 9.77 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.863649081962885		[learning rate: 0.00090386]
	Learning Rate: 0.000903863
	LOSS [training: 4.863649081962885 | validation: 5.3489484048858955]
	TIME [epoch: 9.77 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8295719436547175		[learning rate: 0.00090173]
	Learning Rate: 0.000901731
	LOSS [training: 4.8295719436547175 | validation: 5.3110847782046555]
	TIME [epoch: 9.77 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8187969923427465		[learning rate: 0.0008996]
	Learning Rate: 0.000899604
	LOSS [training: 4.8187969923427465 | validation: 5.370103884500484]
	TIME [epoch: 9.78 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.811022920651835		[learning rate: 0.00089748]
	Learning Rate: 0.000897482
	LOSS [training: 4.811022920651835 | validation: 5.357296496637235]
	TIME [epoch: 9.76 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.827258841053611		[learning rate: 0.00089536]
	Learning Rate: 0.000895365
	LOSS [training: 4.827258841053611 | validation: 5.369380373620543]
	TIME [epoch: 9.77 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.834057769787747		[learning rate: 0.00089325]
	Learning Rate: 0.000893253
	LOSS [training: 4.834057769787747 | validation: 5.34796272150143]
	TIME [epoch: 9.78 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.838908248391444		[learning rate: 0.00089115]
	Learning Rate: 0.000891146
	LOSS [training: 4.838908248391444 | validation: 5.310522143440978]
	TIME [epoch: 9.78 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.848807083710601		[learning rate: 0.00088904]
	Learning Rate: 0.000889044
	LOSS [training: 4.848807083710601 | validation: 5.30764232603844]
	TIME [epoch: 9.76 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.850782981756261		[learning rate: 0.00088695]
	Learning Rate: 0.000886946
	LOSS [training: 4.850782981756261 | validation: 5.391939346610204]
	TIME [epoch: 9.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.886978260688268		[learning rate: 0.00088485]
	Learning Rate: 0.000884854
	LOSS [training: 4.886978260688268 | validation: 5.324868419020986]
	TIME [epoch: 9.79 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.84911925113898		[learning rate: 0.00088277]
	Learning Rate: 0.000882767
	LOSS [training: 4.84911925113898 | validation: 5.322046406011209]
	TIME [epoch: 9.77 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8599461103291866		[learning rate: 0.00088068]
	Learning Rate: 0.000880685
	LOSS [training: 4.8599461103291866 | validation: 5.31354934794472]
	TIME [epoch: 9.77 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.856399969795936		[learning rate: 0.00087861]
	Learning Rate: 0.000878607
	LOSS [training: 4.856399969795936 | validation: 5.346446019022931]
	TIME [epoch: 9.76 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846501296987211		[learning rate: 0.00087653]
	Learning Rate: 0.000876535
	LOSS [training: 4.846501296987211 | validation: 5.327910053254002]
	TIME [epoch: 9.78 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.857509048467325		[learning rate: 0.00087447]
	Learning Rate: 0.000874467
	LOSS [training: 4.857509048467325 | validation: 5.304300070804607]
	TIME [epoch: 9.76 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897739516262646		[learning rate: 0.0008724]
	Learning Rate: 0.000872405
	LOSS [training: 4.897739516262646 | validation: 5.296355446231167]
	TIME [epoch: 9.77 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824598682802605		[learning rate: 0.00087035]
	Learning Rate: 0.000870346
	LOSS [training: 4.824598682802605 | validation: 5.323535165525007]
	TIME [epoch: 9.77 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873847137087356		[learning rate: 0.00086829]
	Learning Rate: 0.000868294
	LOSS [training: 4.873847137087356 | validation: 5.348903543829166]
	TIME [epoch: 9.77 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.842028953009528		[learning rate: 0.00086625]
	Learning Rate: 0.000866246
	LOSS [training: 4.842028953009528 | validation: 5.3907993452685865]
	TIME [epoch: 9.76 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8088688799939545		[learning rate: 0.0008642]
	Learning Rate: 0.000864202
	LOSS [training: 4.8088688799939545 | validation: 5.3635427408620835]
	TIME [epoch: 9.76 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.825073208676696		[learning rate: 0.00086216]
	Learning Rate: 0.000862164
	LOSS [training: 4.825073208676696 | validation: 5.352990515441177]
	TIME [epoch: 9.78 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.827937776566463		[learning rate: 0.00086013]
	Learning Rate: 0.00086013
	LOSS [training: 4.827937776566463 | validation: 5.300680147198521]
	TIME [epoch: 9.77 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.834688639161142		[learning rate: 0.0008581]
	Learning Rate: 0.000858101
	LOSS [training: 4.834688639161142 | validation: 5.331947023894461]
	TIME [epoch: 9.77 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820771563522877		[learning rate: 0.00085608]
	Learning Rate: 0.000856077
	LOSS [training: 4.820771563522877 | validation: 5.359983304156721]
	TIME [epoch: 9.76 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.831760136099088		[learning rate: 0.00085406]
	Learning Rate: 0.000854058
	LOSS [training: 4.831760136099088 | validation: 5.28214197022915]
	TIME [epoch: 9.79 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785182281932827		[learning rate: 0.00085204]
	Learning Rate: 0.000852043
	LOSS [training: 4.785182281932827 | validation: 5.316243770279558]
	TIME [epoch: 9.77 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781789741002366		[learning rate: 0.00085003]
	Learning Rate: 0.000850033
	LOSS [training: 4.781789741002366 | validation: 5.291527932880233]
	TIME [epoch: 9.77 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824581738503421		[learning rate: 0.00084803]
	Learning Rate: 0.000848028
	LOSS [training: 4.824581738503421 | validation: 5.3041506217861505]
	TIME [epoch: 9.78 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783186320671805		[learning rate: 0.00084603]
	Learning Rate: 0.000846028
	LOSS [training: 4.783186320671805 | validation: 5.413140276853603]
	TIME [epoch: 9.79 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793813343978862		[learning rate: 0.00084403]
	Learning Rate: 0.000844032
	LOSS [training: 4.793813343978862 | validation: 5.293323773304788]
	TIME [epoch: 9.78 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7643976232213605		[learning rate: 0.00084204]
	Learning Rate: 0.000842041
	LOSS [training: 4.7643976232213605 | validation: 5.319708973219926]
	TIME [epoch: 9.77 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.77344295521142		[learning rate: 0.00084005]
	Learning Rate: 0.000840055
	LOSS [training: 4.77344295521142 | validation: 5.285783283632555]
	TIME [epoch: 9.79 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807002781559421		[learning rate: 0.00083807]
	Learning Rate: 0.000838073
	LOSS [training: 4.807002781559421 | validation: 5.324506982369793]
	TIME [epoch: 9.77 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.81581261957071		[learning rate: 0.0008361]
	Learning Rate: 0.000836096
	LOSS [training: 4.81581261957071 | validation: 5.586282647965085]
	TIME [epoch: 9.77 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854874288504989		[learning rate: 0.00083412]
	Learning Rate: 0.000834124
	LOSS [training: 4.854874288504989 | validation: 5.324972543972567]
	TIME [epoch: 9.77 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.833826923277142		[learning rate: 0.00083216]
	Learning Rate: 0.000832157
	LOSS [training: 4.833826923277142 | validation: 5.328229824562945]
	TIME [epoch: 9.8 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854071362839159		[learning rate: 0.00083019]
	Learning Rate: 0.000830194
	LOSS [training: 4.854071362839159 | validation: 5.366139162320888]
	TIME [epoch: 9.77 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.803396953760995		[learning rate: 0.00082824]
	Learning Rate: 0.000828236
	LOSS [training: 4.803396953760995 | validation: 5.392800043667495]
	TIME [epoch: 9.77 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.831236896041686		[learning rate: 0.00082628]
	Learning Rate: 0.000826282
	LOSS [training: 4.831236896041686 | validation: 5.324463807464106]
	TIME [epoch: 9.77 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799120762712265		[learning rate: 0.00082433]
	Learning Rate: 0.000824333
	LOSS [training: 4.799120762712265 | validation: 5.325803300810003]
	TIME [epoch: 9.79 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.81446665366327		[learning rate: 0.00082239]
	Learning Rate: 0.000822388
	LOSS [training: 4.81446665366327 | validation: 5.394894687039152]
	TIME [epoch: 9.77 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.847142373427529		[learning rate: 0.00082045]
	Learning Rate: 0.000820448
	LOSS [training: 4.847142373427529 | validation: 5.386491778216796]
	TIME [epoch: 9.77 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.842622222446387		[learning rate: 0.00081851]
	Learning Rate: 0.000818513
	LOSS [training: 4.842622222446387 | validation: 5.3735589149318095]
	TIME [epoch: 9.79 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.830302673628603		[learning rate: 0.00081658]
	Learning Rate: 0.000816582
	LOSS [training: 4.830302673628603 | validation: 5.3377372815203445]
	TIME [epoch: 9.78 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.855562463911943		[learning rate: 0.00081466]
	Learning Rate: 0.000814656
	LOSS [training: 4.855562463911943 | validation: 5.378749808291534]
	TIME [epoch: 9.77 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.818124617823898		[learning rate: 0.00081273]
	Learning Rate: 0.000812734
	LOSS [training: 4.818124617823898 | validation: 5.347320430426911]
	TIME [epoch: 9.77 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793529146129547		[learning rate: 0.00081082]
	Learning Rate: 0.000810817
	LOSS [training: 4.793529146129547 | validation: 5.296755720030435]
	TIME [epoch: 9.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813327358973781		[learning rate: 0.0008089]
	Learning Rate: 0.000808905
	LOSS [training: 4.813327358973781 | validation: 5.297756491762632]
	TIME [epoch: 9.76 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.888611421146892		[learning rate: 0.000807]
	Learning Rate: 0.000806997
	LOSS [training: 4.888611421146892 | validation: 5.295570085806956]
	TIME [epoch: 9.77 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800604429770232		[learning rate: 0.00080509]
	Learning Rate: 0.000805093
	LOSS [training: 4.800604429770232 | validation: 5.307244721361635]
	TIME [epoch: 9.77 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80468662320679		[learning rate: 0.00080319]
	Learning Rate: 0.000803194
	LOSS [training: 4.80468662320679 | validation: 5.345056208786989]
	TIME [epoch: 9.78 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781521956209948		[learning rate: 0.0008013]
	Learning Rate: 0.000801299
	LOSS [training: 4.781521956209948 | validation: 5.356203461813082]
	TIME [epoch: 9.77 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774558751305105		[learning rate: 0.00079941]
	Learning Rate: 0.000799409
	LOSS [training: 4.774558751305105 | validation: 5.329951312427097]
	TIME [epoch: 9.78 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80627265324908		[learning rate: 0.00079752]
	Learning Rate: 0.000797524
	LOSS [training: 4.80627265324908 | validation: 5.398442906422819]
	TIME [epoch: 9.78 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795709114479081		[learning rate: 0.00079564]
	Learning Rate: 0.000795642
	LOSS [training: 4.795709114479081 | validation: 5.271487845339736]
	TIME [epoch: 9.77 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781062016008578		[learning rate: 0.00079377]
	Learning Rate: 0.000793766
	LOSS [training: 4.781062016008578 | validation: 5.324901614518089]
	TIME [epoch: 9.77 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76720302344595		[learning rate: 0.00079189]
	Learning Rate: 0.000791893
	LOSS [training: 4.76720302344595 | validation: 5.328300156076227]
	TIME [epoch: 9.77 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793435800743732		[learning rate: 0.00079003]
	Learning Rate: 0.000790025
	LOSS [training: 4.793435800743732 | validation: 5.485560341811546]
	TIME [epoch: 9.79 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778115030511222		[learning rate: 0.00078816]
	Learning Rate: 0.000788162
	LOSS [training: 4.778115030511222 | validation: 5.331377636573161]
	TIME [epoch: 9.77 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765014153773437		[learning rate: 0.0007863]
	Learning Rate: 0.000786302
	LOSS [training: 4.765014153773437 | validation: 5.354909755093225]
	TIME [epoch: 9.77 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764897065630828		[learning rate: 0.00078445]
	Learning Rate: 0.000784448
	LOSS [training: 4.764897065630828 | validation: 5.457347235192969]
	TIME [epoch: 9.77 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771297673446361		[learning rate: 0.0007826]
	Learning Rate: 0.000782598
	LOSS [training: 4.771297673446361 | validation: 5.295297091779262]
	TIME [epoch: 9.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773035209110897		[learning rate: 0.00078075]
	Learning Rate: 0.000780751
	LOSS [training: 4.773035209110897 | validation: 5.2875759166100815]
	TIME [epoch: 9.77 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771398427150307		[learning rate: 0.00077891]
	Learning Rate: 0.00077891
	LOSS [training: 4.771398427150307 | validation: 5.276891299228819]
	TIME [epoch: 9.77 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765977779059093		[learning rate: 0.00077707]
	Learning Rate: 0.000777073
	LOSS [training: 4.765977779059093 | validation: 5.309995190226588]
	TIME [epoch: 9.77 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.753731758771808		[learning rate: 0.00077524]
	Learning Rate: 0.000775239
	LOSS [training: 4.753731758771808 | validation: 5.298624914971506]
	TIME [epoch: 9.79 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.75069004797788		[learning rate: 0.00077341]
	Learning Rate: 0.000773411
	LOSS [training: 4.75069004797788 | validation: 5.300954362275397]
	TIME [epoch: 9.78 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780727816056316		[learning rate: 0.00077159]
	Learning Rate: 0.000771586
	LOSS [training: 4.780727816056316 | validation: 5.505858234125933]
	TIME [epoch: 9.77 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778451124451755		[learning rate: 0.00076977]
	Learning Rate: 0.000769766
	LOSS [training: 4.778451124451755 | validation: 5.320018575155956]
	TIME [epoch: 9.78 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758126363478688		[learning rate: 0.00076795]
	Learning Rate: 0.000767951
	LOSS [training: 4.758126363478688 | validation: 5.29531325317707]
	TIME [epoch: 9.77 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8009728888459025		[learning rate: 0.00076614]
	Learning Rate: 0.000766139
	LOSS [training: 4.8009728888459025 | validation: 5.320379039275681]
	TIME [epoch: 9.77 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76651894976653		[learning rate: 0.00076433]
	Learning Rate: 0.000764332
	LOSS [training: 4.76651894976653 | validation: 5.297225826217816]
	TIME [epoch: 9.77 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7506554862548995		[learning rate: 0.00076253]
	Learning Rate: 0.000762529
	LOSS [training: 4.7506554862548995 | validation: 5.3411120350356285]
	TIME [epoch: 9.78 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.733586686972414		[learning rate: 0.00076073]
	Learning Rate: 0.00076073
	LOSS [training: 4.733586686972414 | validation: 5.301094460422985]
	TIME [epoch: 9.77 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.718550206150162		[learning rate: 0.00075894]
	Learning Rate: 0.000758936
	LOSS [training: 4.718550206150162 | validation: 5.424011331412804]
	TIME [epoch: 9.77 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.753293669778377		[learning rate: 0.00075715]
	Learning Rate: 0.000757146
	LOSS [training: 4.753293669778377 | validation: 5.269350248648932]
	TIME [epoch: 9.77 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.75663138191461		[learning rate: 0.00075536]
	Learning Rate: 0.00075536
	LOSS [training: 4.75663138191461 | validation: 5.38742122162958]
	TIME [epoch: 9.79 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784220873771004		[learning rate: 0.00075358]
	Learning Rate: 0.000753578
	LOSS [training: 4.784220873771004 | validation: 5.447647746003264]
	TIME [epoch: 9.77 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.749069285408807		[learning rate: 0.0007518]
	Learning Rate: 0.0007518
	LOSS [training: 4.749069285408807 | validation: 5.303679017485151]
	TIME [epoch: 9.77 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.759018275622557		[learning rate: 0.00075003]
	Learning Rate: 0.000750027
	LOSS [training: 4.759018275622557 | validation: 5.261659066198014]
	TIME [epoch: 9.8 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.715486818778032		[learning rate: 0.00074826]
	Learning Rate: 0.000748258
	LOSS [training: 4.715486818778032 | validation: 5.3050385748482345]
	TIME [epoch: 9.77 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771522620575487		[learning rate: 0.00074649]
	Learning Rate: 0.000746493
	LOSS [training: 4.771522620575487 | validation: 5.3376125177663445]
	TIME [epoch: 9.77 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767004329266084		[learning rate: 0.00074473]
	Learning Rate: 0.000744732
	LOSS [training: 4.767004329266084 | validation: 5.344973401210468]
	TIME [epoch: 9.77 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7532797746731035		[learning rate: 0.00074298]
	Learning Rate: 0.000742975
	LOSS [training: 4.7532797746731035 | validation: 5.349031297688788]
	TIME [epoch: 9.79 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.741978327178767		[learning rate: 0.00074122]
	Learning Rate: 0.000741223
	LOSS [training: 4.741978327178767 | validation: 5.336930945098446]
	TIME [epoch: 9.77 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787675316105019		[learning rate: 0.00073947]
	Learning Rate: 0.000739474
	LOSS [training: 4.787675316105019 | validation: 5.3266898433381975]
	TIME [epoch: 9.77 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.74930275179273		[learning rate: 0.00073773]
	Learning Rate: 0.00073773
	LOSS [training: 4.74930275179273 | validation: 5.346761871524247]
	TIME [epoch: 9.77 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.814956161405786		[learning rate: 0.00073599]
	Learning Rate: 0.00073599
	LOSS [training: 4.814956161405786 | validation: 5.429533884440682]
	TIME [epoch: 9.78 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.747307409066876		[learning rate: 0.00073425]
	Learning Rate: 0.000734254
	LOSS [training: 4.747307409066876 | validation: 5.285048256208373]
	TIME [epoch: 9.79 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.732784703171978		[learning rate: 0.00073252]
	Learning Rate: 0.000732522
	LOSS [training: 4.732784703171978 | validation: 5.411176667089923]
	TIME [epoch: 9.77 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7415720813910385		[learning rate: 0.00073079]
	Learning Rate: 0.000730794
	LOSS [training: 4.7415720813910385 | validation: 5.286274100832919]
	TIME [epoch: 9.79 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.730095770105063		[learning rate: 0.00072907]
	Learning Rate: 0.00072907
	LOSS [training: 4.730095770105063 | validation: 5.4397960791633935]
	TIME [epoch: 9.77 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.755489700180648		[learning rate: 0.00072735]
	Learning Rate: 0.00072735
	LOSS [training: 4.755489700180648 | validation: 5.3164985620585155]
	TIME [epoch: 9.77 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79422713710853		[learning rate: 0.00072563]
	Learning Rate: 0.000725634
	LOSS [training: 4.79422713710853 | validation: 5.33745521907559]
	TIME [epoch: 9.77 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790779540074572		[learning rate: 0.00072392]
	Learning Rate: 0.000723923
	LOSS [training: 4.790779540074572 | validation: 5.35321581902572]
	TIME [epoch: 9.79 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787736373727529		[learning rate: 0.00072222]
	Learning Rate: 0.000722215
	LOSS [training: 4.787736373727529 | validation: 5.323423433124517]
	TIME [epoch: 9.77 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804350579913927		[learning rate: 0.00072051]
	Learning Rate: 0.000720512
	LOSS [training: 4.804350579913927 | validation: 5.358608717834354]
	TIME [epoch: 9.77 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.849431271267424		[learning rate: 0.00071881]
	Learning Rate: 0.000718812
	LOSS [training: 4.849431271267424 | validation: 5.378915135912704]
	TIME [epoch: 9.77 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.949513273183768		[learning rate: 0.00071712]
	Learning Rate: 0.000717117
	LOSS [training: 4.949513273183768 | validation: 5.548077260455023]
	TIME [epoch: 9.79 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922014377919244		[learning rate: 0.00071542]
	Learning Rate: 0.000715425
	LOSS [training: 4.922014377919244 | validation: 5.394067058642229]
	TIME [epoch: 9.79 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.859171077347526		[learning rate: 0.00071374]
	Learning Rate: 0.000713738
	LOSS [training: 4.859171077347526 | validation: 5.488911349990628]
	TIME [epoch: 9.77 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.876301414236599		[learning rate: 0.00071205]
	Learning Rate: 0.000712054
	LOSS [training: 4.876301414236599 | validation: 5.447756468149786]
	TIME [epoch: 9.78 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.817162602180664		[learning rate: 0.00071037]
	Learning Rate: 0.000710374
	LOSS [training: 4.817162602180664 | validation: 5.402461193178611]
	TIME [epoch: 9.78 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867780333651202		[learning rate: 0.0007087]
	Learning Rate: 0.000708698
	LOSS [training: 4.867780333651202 | validation: 5.666337209749818]
	TIME [epoch: 9.77 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.930703288383867		[learning rate: 0.00070703]
	Learning Rate: 0.000707027
	LOSS [training: 4.930703288383867 | validation: 5.412005865988626]
	TIME [epoch: 9.76 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.982271830518402		[learning rate: 0.00070536]
	Learning Rate: 0.000705359
	LOSS [training: 4.982271830518402 | validation: 5.531128114943677]
	TIME [epoch: 9.79 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.881042599531102		[learning rate: 0.0007037]
	Learning Rate: 0.000703695
	LOSS [training: 4.881042599531102 | validation: 5.364384809132004]
	TIME [epoch: 9.77 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.866785956916802		[learning rate: 0.00070204]
	Learning Rate: 0.000702035
	LOSS [training: 4.866785956916802 | validation: 5.415684484546359]
	TIME [epoch: 9.76 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.85606674102673		[learning rate: 0.00070038]
	Learning Rate: 0.000700379
	LOSS [training: 4.85606674102673 | validation: 5.3948028798751215]
	TIME [epoch: 9.77 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.845307047266962		[learning rate: 0.00069873]
	Learning Rate: 0.000698727
	LOSS [training: 4.845307047266962 | validation: 5.496125706903]
	TIME [epoch: 9.8 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.918388152308393		[learning rate: 0.00069708]
	Learning Rate: 0.000697079
	LOSS [training: 4.918388152308393 | validation: 5.414281729792415]
	TIME [epoch: 9.77 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870749276395577		[learning rate: 0.00069543]
	Learning Rate: 0.000695435
	LOSS [training: 4.870749276395577 | validation: 5.3988528565440035]
	TIME [epoch: 9.76 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.86307016067717		[learning rate: 0.00069379]
	Learning Rate: 0.000693794
	LOSS [training: 4.86307016067717 | validation: 5.384883387109654]
	TIME [epoch: 9.77 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867542852996931		[learning rate: 0.00069216]
	Learning Rate: 0.000692158
	LOSS [training: 4.867542852996931 | validation: 5.444974421271415]
	TIME [epoch: 9.79 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.923490575078145		[learning rate: 0.00069053]
	Learning Rate: 0.000690525
	LOSS [training: 4.923490575078145 | validation: 5.439224553367137]
	TIME [epoch: 9.77 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.815583314088407		[learning rate: 0.0006889]
	Learning Rate: 0.000688896
	LOSS [training: 4.815583314088407 | validation: 5.293607325030759]
	TIME [epoch: 9.77 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.738848715068746		[learning rate: 0.00068727]
	Learning Rate: 0.000687271
	LOSS [training: 4.738848715068746 | validation: 5.311744017302531]
	TIME [epoch: 9.79 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.72081962177701		[learning rate: 0.00068565]
	Learning Rate: 0.00068565
	LOSS [training: 4.72081962177701 | validation: 5.4744645694311895]
	TIME [epoch: 9.77 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.870821098569004		[learning rate: 0.00068403]
	Learning Rate: 0.000684033
	LOSS [training: 4.870821098569004 | validation: 5.3411883945098415]
	TIME [epoch: 9.77 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8118291134572715		[learning rate: 0.00068242]
	Learning Rate: 0.000682419
	LOSS [training: 4.8118291134572715 | validation: 5.334198664583609]
	TIME [epoch: 9.8 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769644162568755		[learning rate: 0.00068081]
	Learning Rate: 0.00068081
	LOSS [training: 4.769644162568755 | validation: 5.436782517292477]
	TIME [epoch: 9.8 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78480637743211		[learning rate: 0.0006792]
	Learning Rate: 0.000679204
	LOSS [training: 4.78480637743211 | validation: 5.309992472008092]
	TIME [epoch: 9.79 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.759638562739974		[learning rate: 0.0006776]
	Learning Rate: 0.000677601
	LOSS [training: 4.759638562739974 | validation: 5.330527120289543]
	TIME [epoch: 9.76 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76126518425443		[learning rate: 0.000676]
	Learning Rate: 0.000676003
	LOSS [training: 4.76126518425443 | validation: 5.37977764530717]
	TIME [epoch: 9.79 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799081093811905		[learning rate: 0.00067441]
	Learning Rate: 0.000674409
	LOSS [training: 4.799081093811905 | validation: 5.364334911164628]
	TIME [epoch: 9.78 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832100116379964		[learning rate: 0.00067282]
	Learning Rate: 0.000672818
	LOSS [training: 4.832100116379964 | validation: 5.370289255384128]
	TIME [epoch: 9.77 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.856113004963028		[learning rate: 0.00067123]
	Learning Rate: 0.000671231
	LOSS [training: 4.856113004963028 | validation: 5.382045950043219]
	TIME [epoch: 9.77 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8490061643849245		[learning rate: 0.00066965]
	Learning Rate: 0.000669647
	LOSS [training: 4.8490061643849245 | validation: 5.557843371980367]
	TIME [epoch: 9.78 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3913488272308765		[learning rate: 0.00066807]
	Learning Rate: 0.000668068
	LOSS [training: 5.3913488272308765 | validation: 6.158098896316526]
	TIME [epoch: 9.77 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.412411349314344		[learning rate: 0.00066649]
	Learning Rate: 0.000666492
	LOSS [training: 5.412411349314344 | validation: 6.125802757910999]
	TIME [epoch: 9.77 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.576770138711388		[learning rate: 0.00066492]
	Learning Rate: 0.00066492
	LOSS [training: 5.576770138711388 | validation: 6.265270025999172]
	TIME [epoch: 9.77 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581350715215121		[learning rate: 0.00066335]
	Learning Rate: 0.000663351
	LOSS [training: 5.581350715215121 | validation: 6.387727388944191]
	TIME [epoch: 9.79 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.579941138979457		[learning rate: 0.00066179]
	Learning Rate: 0.000661786
	LOSS [training: 5.579941138979457 | validation: 6.255826882046918]
	TIME [epoch: 9.77 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.487796116354657		[learning rate: 0.00066023]
	Learning Rate: 0.000660225
	LOSS [training: 5.487796116354657 | validation: 6.107326472934851]
	TIME [epoch: 9.77 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.413658458205606		[learning rate: 0.00065867]
	Learning Rate: 0.000658668
	LOSS [training: 5.413658458205606 | validation: 6.131223894805796]
	TIME [epoch: 9.77 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.372150836270727		[learning rate: 0.00065711]
	Learning Rate: 0.000657114
	LOSS [training: 5.372150836270727 | validation: 6.241553086182721]
	TIME [epoch: 9.79 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.311735241851278		[learning rate: 0.00065556]
	Learning Rate: 0.000655564
	LOSS [training: 5.311735241851278 | validation: 5.769182760305771]
	TIME [epoch: 9.78 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.26740983381063		[learning rate: 0.00065402]
	Learning Rate: 0.000654018
	LOSS [training: 5.26740983381063 | validation: 5.8581033492441525]
	TIME [epoch: 9.78 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.165862696294485		[learning rate: 0.00065248]
	Learning Rate: 0.000652475
	LOSS [training: 5.165862696294485 | validation: 5.61599956510933]
	TIME [epoch: 9.79 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.103654089048302		[learning rate: 0.00065094]
	Learning Rate: 0.000650936
	LOSS [training: 5.103654089048302 | validation: 5.6150401051628975]
	TIME [epoch: 9.78 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.038557765606815		[learning rate: 0.0006494]
	Learning Rate: 0.000649401
	LOSS [training: 5.038557765606815 | validation: 5.487108810998887]
	TIME [epoch: 9.78 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9779470869094204		[learning rate: 0.00064787]
	Learning Rate: 0.000647869
	LOSS [training: 4.9779470869094204 | validation: 5.503460877603718]
	TIME [epoch: 9.77 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.978898125107699		[learning rate: 0.00064634]
	Learning Rate: 0.000646341
	LOSS [training: 4.978898125107699 | validation: 5.4480511044348665]
	TIME [epoch: 9.79 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.033198046454196		[learning rate: 0.00064482]
	Learning Rate: 0.000644816
	LOSS [training: 5.033198046454196 | validation: 5.481254033200166]
	TIME [epoch: 9.77 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9374344753029416		[learning rate: 0.0006433]
	Learning Rate: 0.000643295
	LOSS [training: 4.9374344753029416 | validation: 5.440992806138135]
	TIME [epoch: 9.79 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.907919032757642		[learning rate: 0.00064178]
	Learning Rate: 0.000641778
	LOSS [training: 4.907919032757642 | validation: 5.568529159048913]
	TIME [epoch: 9.77 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.012082270734536		[learning rate: 0.00064026]
	Learning Rate: 0.000640264
	LOSS [training: 5.012082270734536 | validation: 5.413397318332488]
	TIME [epoch: 9.8 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9111268382010325		[learning rate: 0.00063875]
	Learning Rate: 0.000638754
	LOSS [training: 4.9111268382010325 | validation: 5.443792888613789]
	TIME [epoch: 9.78 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.903848081058922		[learning rate: 0.00063725]
	Learning Rate: 0.000637247
	LOSS [training: 4.903848081058922 | validation: 5.457171745946876]
	TIME [epoch: 9.77 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922338271609369		[learning rate: 0.00063574]
	Learning Rate: 0.000635744
	LOSS [training: 4.922338271609369 | validation: 5.416139825752689]
	TIME [epoch: 9.77 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.853166693821052		[learning rate: 0.00063424]
	Learning Rate: 0.000634244
	LOSS [training: 4.853166693821052 | validation: 5.346464042933343]
	TIME [epoch: 9.78 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.855160492092542		[learning rate: 0.00063275]
	Learning Rate: 0.000632748
	LOSS [training: 4.855160492092542 | validation: 5.410914463029119]
	TIME [epoch: 9.77 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.851610633913429		[learning rate: 0.00063126]
	Learning Rate: 0.000631255
	LOSS [training: 4.851610633913429 | validation: 5.367565398836771]
	TIME [epoch: 9.77 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867936076429961		[learning rate: 0.00062977]
	Learning Rate: 0.000629766
	LOSS [training: 4.867936076429961 | validation: 5.399622441541157]
	TIME [epoch: 9.79 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832314811187599		[learning rate: 0.00062828]
	Learning Rate: 0.000628281
	LOSS [training: 4.832314811187599 | validation: 5.34115660362782]
	TIME [epoch: 9.76 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.816406730049202		[learning rate: 0.0006268]
	Learning Rate: 0.000626799
	LOSS [training: 4.816406730049202 | validation: 5.432799718166077]
	TIME [epoch: 9.77 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.99359610108026		[learning rate: 0.00062532]
	Learning Rate: 0.00062532
	LOSS [training: 4.99359610108026 | validation: 5.43693723637334]
	TIME [epoch: 9.76 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908322465079517		[learning rate: 0.00062385]
	Learning Rate: 0.000623845
	LOSS [training: 4.908322465079517 | validation: 5.451319007630897]
	TIME [epoch: 9.78 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.927532447972064		[learning rate: 0.00062237]
	Learning Rate: 0.000622374
	LOSS [training: 4.927532447972064 | validation: 5.43506266649641]
	TIME [epoch: 9.76 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.908701476097747		[learning rate: 0.00062091]
	Learning Rate: 0.000620906
	LOSS [training: 4.908701476097747 | validation: 5.389159297490683]
	TIME [epoch: 9.77 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.877787563364502		[learning rate: 0.00061944]
	Learning Rate: 0.000619441
	LOSS [training: 4.877787563364502 | validation: 5.427758857172182]
	TIME [epoch: 9.77 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.873475819579885		[learning rate: 0.00061798]
	Learning Rate: 0.00061798
	LOSS [training: 4.873475819579885 | validation: 5.406792618149488]
	TIME [epoch: 9.78 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.821534712938153		[learning rate: 0.00061652]
	Learning Rate: 0.000616522
	LOSS [training: 4.821534712938153 | validation: 5.355763576806708]
	TIME [epoch: 9.77 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820403251760776		[learning rate: 0.00061507]
	Learning Rate: 0.000615068
	LOSS [training: 4.820403251760776 | validation: 5.412074347066977]
	TIME [epoch: 9.77 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804765679160654		[learning rate: 0.00061362]
	Learning Rate: 0.000613617
	LOSS [training: 4.804765679160654 | validation: 5.34343985186829]
	TIME [epoch: 9.78 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.85306943010202		[learning rate: 0.00061217]
	Learning Rate: 0.00061217
	LOSS [training: 4.85306943010202 | validation: 5.322724974191951]
	TIME [epoch: 9.77 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862000934755606		[learning rate: 0.00061073]
	Learning Rate: 0.000610726
	LOSS [training: 4.862000934755606 | validation: 5.66509976960107]
	TIME [epoch: 9.76 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.867247360987339		[learning rate: 0.00060929]
	Learning Rate: 0.000609285
	LOSS [training: 4.867247360987339 | validation: 5.325759825437591]
	TIME [epoch: 9.77 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.803549944364964		[learning rate: 0.00060785]
	Learning Rate: 0.000607848
	LOSS [training: 4.803549944364964 | validation: 5.417302421811406]
	TIME [epoch: 9.78 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.817932903792122		[learning rate: 0.00060641]
	Learning Rate: 0.000606414
	LOSS [training: 4.817932903792122 | validation: 5.330322374493638]
	TIME [epoch: 9.77 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.829614542151562		[learning rate: 0.00060498]
	Learning Rate: 0.000604983
	LOSS [training: 4.829614542151562 | validation: 5.317758442023244]
	TIME [epoch: 9.76 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.761753417086129		[learning rate: 0.00060356]
	Learning Rate: 0.000603557
	LOSS [training: 4.761753417086129 | validation: 5.316288269915332]
	TIME [epoch: 9.77 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776964397633039		[learning rate: 0.00060213]
	Learning Rate: 0.000602133
	LOSS [training: 4.776964397633039 | validation: 5.3526485892299025]
	TIME [epoch: 9.79 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802089026658427		[learning rate: 0.00060071]
	Learning Rate: 0.000600712
	LOSS [training: 4.802089026658427 | validation: 5.329230186083785]
	TIME [epoch: 9.77 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.759911395449881		[learning rate: 0.0005993]
	Learning Rate: 0.000599296
	LOSS [training: 4.759911395449881 | validation: 5.307786774305805]
	TIME [epoch: 9.77 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7676072702639		[learning rate: 0.00059788]
	Learning Rate: 0.000597882
	LOSS [training: 4.7676072702639 | validation: 5.383252071971746]
	TIME [epoch: 9.78 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797803846991619		[learning rate: 0.00059647]
	Learning Rate: 0.000596471
	LOSS [training: 4.797803846991619 | validation: 5.321926299439297]
	TIME [epoch: 9.77 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767104910882926		[learning rate: 0.00059506]
	Learning Rate: 0.000595065
	LOSS [training: 4.767104910882926 | validation: 5.322225454138898]
	TIME [epoch: 9.76 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804903980845673		[learning rate: 0.00059366]
	Learning Rate: 0.000593661
	LOSS [training: 4.804903980845673 | validation: 5.3131693794517565]
	TIME [epoch: 9.77 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783362691957512		[learning rate: 0.00059226]
	Learning Rate: 0.000592261
	LOSS [training: 4.783362691957512 | validation: 5.310703408994332]
	TIME [epoch: 9.79 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.754466328709098		[learning rate: 0.00059086]
	Learning Rate: 0.000590863
	LOSS [training: 4.754466328709098 | validation: 5.430042785247008]
	TIME [epoch: 9.77 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.919133970522402		[learning rate: 0.00058947]
	Learning Rate: 0.00058947
	LOSS [training: 4.919133970522402 | validation: 5.440749630159107]
	TIME [epoch: 9.77 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.901103214286112		[learning rate: 0.00058808]
	Learning Rate: 0.000588079
	LOSS [training: 4.901103214286112 | validation: 5.422675903566878]
	TIME [epoch: 9.77 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.875527259818653		[learning rate: 0.00058669]
	Learning Rate: 0.000586692
	LOSS [training: 4.875527259818653 | validation: 5.362499287808737]
	TIME [epoch: 9.78 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.848568224292915		[learning rate: 0.00058531]
	Learning Rate: 0.000585308
	LOSS [training: 4.848568224292915 | validation: 5.401663742581288]
	TIME [epoch: 9.76 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.851130792244785		[learning rate: 0.00058393]
	Learning Rate: 0.000583927
	LOSS [training: 4.851130792244785 | validation: 5.377573722502316]
	TIME [epoch: 9.77 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.834850906338265		[learning rate: 0.00058255]
	Learning Rate: 0.00058255
	LOSS [training: 4.834850906338265 | validation: 5.359691809222801]
	TIME [epoch: 9.78 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.828039392922889		[learning rate: 0.00058118]
	Learning Rate: 0.000581176
	LOSS [training: 4.828039392922889 | validation: 5.377651780041951]
	TIME [epoch: 9.77 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8407687893915785		[learning rate: 0.00057981]
	Learning Rate: 0.000579805
	LOSS [training: 4.8407687893915785 | validation: 5.472276146662007]
	TIME [epoch: 9.76 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8604353699586245		[learning rate: 0.00057844]
	Learning Rate: 0.000578437
	LOSS [training: 4.8604353699586245 | validation: 5.3344129789030035]
	TIME [epoch: 9.76 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792316702933422		[learning rate: 0.00057707]
	Learning Rate: 0.000577073
	LOSS [training: 4.792316702933422 | validation: 5.314023980561701]
	TIME [epoch: 9.79 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795453920692874		[learning rate: 0.00057571]
	Learning Rate: 0.000575712
	LOSS [training: 4.795453920692874 | validation: 5.312029856255199]
	TIME [epoch: 9.77 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791160606970949		[learning rate: 0.00057435]
	Learning Rate: 0.000574354
	LOSS [training: 4.791160606970949 | validation: 5.309496327656335]
	TIME [epoch: 9.77 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773596774966897		[learning rate: 0.000573]
	Learning Rate: 0.000572999
	LOSS [training: 4.773596774966897 | validation: 5.305909012685843]
	TIME [epoch: 9.78 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770202156630084		[learning rate: 0.00057165]
	Learning Rate: 0.000571647
	LOSS [training: 4.770202156630084 | validation: 5.319096821764184]
	TIME [epoch: 9.79 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.761628357921323		[learning rate: 0.0005703]
	Learning Rate: 0.000570299
	LOSS [training: 4.761628357921323 | validation: 5.295074436515178]
	TIME [epoch: 9.75 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.746982673747181		[learning rate: 0.00056895]
	Learning Rate: 0.000568954
	LOSS [training: 4.746982673747181 | validation: 5.3116495179166945]
	TIME [epoch: 9.77 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.753314993071397		[learning rate: 0.00056761]
	Learning Rate: 0.000567612
	LOSS [training: 4.753314993071397 | validation: 5.3434690368550894]
	TIME [epoch: 9.78 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773587287860421		[learning rate: 0.00056627]
	Learning Rate: 0.000566273
	LOSS [training: 4.773587287860421 | validation: 5.30306223983707]
	TIME [epoch: 9.77 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.746331553018305		[learning rate: 0.00056494]
	Learning Rate: 0.000564937
	LOSS [training: 4.746331553018305 | validation: 5.295704718538226]
	TIME [epoch: 9.77 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760228674099411		[learning rate: 0.0005636]
	Learning Rate: 0.000563604
	LOSS [training: 4.760228674099411 | validation: 5.308225038387887]
	TIME [epoch: 9.77 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.753741345257348		[learning rate: 0.00056227]
	Learning Rate: 0.000562275
	LOSS [training: 4.753741345257348 | validation: 5.303043121420525]
	TIME [epoch: 9.79 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.75231370732583		[learning rate: 0.00056095]
	Learning Rate: 0.000560949
	LOSS [training: 4.75231370732583 | validation: 5.307846144469689]
	TIME [epoch: 9.77 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781534094523606		[learning rate: 0.00055963]
	Learning Rate: 0.000559625
	LOSS [training: 4.781534094523606 | validation: 5.438272914110767]
	TIME [epoch: 9.77 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769391822353937		[learning rate: 0.00055831]
	Learning Rate: 0.000558305
	LOSS [training: 4.769391822353937 | validation: 5.335529194686658]
	TIME [epoch: 9.78 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.853566588990466		[learning rate: 0.00055699]
	Learning Rate: 0.000556988
	LOSS [training: 4.853566588990466 | validation: 5.386668327067563]
	TIME [epoch: 9.78 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7543588166419735		[learning rate: 0.00055567]
	Learning Rate: 0.000555674
	LOSS [training: 4.7543588166419735 | validation: 5.292562737210637]
	TIME [epoch: 9.76 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.750347193845025		[learning rate: 0.00055436]
	Learning Rate: 0.000554364
	LOSS [training: 4.750347193845025 | validation: 5.346614537137227]
	TIME [epoch: 9.76 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.749777534708142		[learning rate: 0.00055306]
	Learning Rate: 0.000553056
	LOSS [training: 4.749777534708142 | validation: 5.279728471757702]
	TIME [epoch: 9.76 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.719534056961654		[learning rate: 0.00055175]
	Learning Rate: 0.000551752
	LOSS [training: 4.719534056961654 | validation: 5.286172816333799]
	TIME [epoch: 9.78 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.71913229638775		[learning rate: 0.00055045]
	Learning Rate: 0.00055045
	LOSS [training: 4.71913229638775 | validation: 5.318729273281805]
	TIME [epoch: 9.76 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763685170628195		[learning rate: 0.00054915]
	Learning Rate: 0.000549152
	LOSS [training: 4.763685170628195 | validation: 5.302617111909243]
	TIME [epoch: 9.75 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.748024732520714		[learning rate: 0.00054786]
	Learning Rate: 0.000547856
	LOSS [training: 4.748024732520714 | validation: 5.316611399957352]
	TIME [epoch: 9.78 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.722814645992931		[learning rate: 0.00054656]
	Learning Rate: 0.000546564
	LOSS [training: 4.722814645992931 | validation: 5.296481558894848]
	TIME [epoch: 9.77 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7358983290327		[learning rate: 0.00054527]
	Learning Rate: 0.000545275
	LOSS [training: 4.7358983290327 | validation: 5.283942832296187]
	TIME [epoch: 9.77 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.739002147607501		[learning rate: 0.00054399]
	Learning Rate: 0.000543988
	LOSS [training: 4.739002147607501 | validation: 5.304681062454442]
	TIME [epoch: 9.76 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.735597520805651		[learning rate: 0.00054271]
	Learning Rate: 0.000542705
	LOSS [training: 4.735597520805651 | validation: 5.284849279015993]
	TIME [epoch: 9.78 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7305390245166965		[learning rate: 0.00054143]
	Learning Rate: 0.000541425
	LOSS [training: 4.7305390245166965 | validation: 5.350742902044288]
	TIME [epoch: 9.76 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.749100949843181		[learning rate: 0.00054015]
	Learning Rate: 0.000540148
	LOSS [training: 4.749100949843181 | validation: 5.366088913964362]
	TIME [epoch: 9.75 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.743203624003379		[learning rate: 0.00053887]
	Learning Rate: 0.000538874
	LOSS [training: 4.743203624003379 | validation: 5.31305182144878]
	TIME [epoch: 9.76 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.745546291269229		[learning rate: 0.0005376]
	Learning Rate: 0.000537603
	LOSS [training: 4.745546291269229 | validation: 5.36457763079969]
	TIME [epoch: 9.78 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7556379053086015		[learning rate: 0.00053633]
	Learning Rate: 0.000536335
	LOSS [training: 4.7556379053086015 | validation: 5.299391723996286]
	TIME [epoch: 9.76 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.735286427069627		[learning rate: 0.00053507]
	Learning Rate: 0.00053507
	LOSS [training: 4.735286427069627 | validation: 5.297644528159526]
	TIME [epoch: 9.76 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.729724510717752		[learning rate: 0.00053381]
	Learning Rate: 0.000533807
	LOSS [training: 4.729724510717752 | validation: 5.270927895140078]
	TIME [epoch: 9.78 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.716828835563108		[learning rate: 0.00053255]
	Learning Rate: 0.000532548
	LOSS [training: 4.716828835563108 | validation: 5.297375460016329]
	TIME [epoch: 9.76 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.74957530090338		[learning rate: 0.00053129]
	Learning Rate: 0.000531292
	LOSS [training: 4.74957530090338 | validation: 5.32087768269751]
	TIME [epoch: 9.75 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766774320460175		[learning rate: 0.00053004]
	Learning Rate: 0.000530039
	LOSS [training: 4.766774320460175 | validation: 5.316377085482639]
	TIME [epoch: 9.76 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791824778864649		[learning rate: 0.00052879]
	Learning Rate: 0.000528789
	LOSS [training: 4.791824778864649 | validation: 5.794713468204978]
	TIME [epoch: 9.78 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.935131966546888		[learning rate: 0.00052754]
	Learning Rate: 0.000527541
	LOSS [training: 4.935131966546888 | validation: 5.290268910183813]
	TIME [epoch: 9.76 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.722458629630459		[learning rate: 0.0005263]
	Learning Rate: 0.000526297
	LOSS [training: 4.722458629630459 | validation: 5.323561405255439]
	TIME [epoch: 9.76 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.732565856056296		[learning rate: 0.00052506]
	Learning Rate: 0.000525055
	LOSS [training: 4.732565856056296 | validation: 5.303277817394255]
	TIME [epoch: 9.76 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7304052610705085		[learning rate: 0.00052382]
	Learning Rate: 0.000523817
	LOSS [training: 4.7304052610705085 | validation: 5.301496016698946]
	TIME [epoch: 9.77 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7225966798361245		[learning rate: 0.00052258]
	Learning Rate: 0.000522581
	LOSS [training: 4.7225966798361245 | validation: 5.332386822566768]
	TIME [epoch: 9.76 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.726223996202092		[learning rate: 0.00052135]
	Learning Rate: 0.000521348
	LOSS [training: 4.726223996202092 | validation: 5.30204836065517]
	TIME [epoch: 9.76 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7235577609014525		[learning rate: 0.00052012]
	Learning Rate: 0.000520119
	LOSS [training: 4.7235577609014525 | validation: 5.302850853048056]
	TIME [epoch: 9.78 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.728350043514257		[learning rate: 0.00051889]
	Learning Rate: 0.000518892
	LOSS [training: 4.728350043514257 | validation: 5.359516349527613]
	TIME [epoch: 9.77 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.753808345403742		[learning rate: 0.00051767]
	Learning Rate: 0.000517668
	LOSS [training: 4.753808345403742 | validation: 5.417421028637622]
	TIME [epoch: 9.76 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.815876570958309		[learning rate: 0.00051645]
	Learning Rate: 0.000516447
	LOSS [training: 4.815876570958309 | validation: 5.324934536278097]
	TIME [epoch: 9.77 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.755444328187904		[learning rate: 0.00051523]
	Learning Rate: 0.000515229
	LOSS [training: 4.755444328187904 | validation: 5.364459395100603]
	TIME [epoch: 9.78 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763041422669788		[learning rate: 0.00051401]
	Learning Rate: 0.000514013
	LOSS [training: 4.763041422669788 | validation: 5.307747987934615]
	TIME [epoch: 9.76 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7751946126920135		[learning rate: 0.0005128]
	Learning Rate: 0.000512801
	LOSS [training: 4.7751946126920135 | validation: 5.37355114225759]
	TIME [epoch: 9.76 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.745952776891692		[learning rate: 0.00051159]
	Learning Rate: 0.000511591
	LOSS [training: 4.745952776891692 | validation: 5.314304185799695]
	TIME [epoch: 9.77 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.745315537799655		[learning rate: 0.00051038]
	Learning Rate: 0.000510384
	LOSS [training: 4.745315537799655 | validation: 5.2968643404135625]
	TIME [epoch: 9.79 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760476281911677		[learning rate: 0.00050918]
	Learning Rate: 0.00050918
	LOSS [training: 4.760476281911677 | validation: 5.3194799387949265]
	TIME [epoch: 9.76 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781179813034394		[learning rate: 0.00050798]
	Learning Rate: 0.000507979
	LOSS [training: 4.781179813034394 | validation: 5.318960104635107]
	TIME [epoch: 9.76 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80097849401656		[learning rate: 0.00050678]
	Learning Rate: 0.000506781
	LOSS [training: 4.80097849401656 | validation: 5.430802022313226]
	TIME [epoch: 9.76 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798959812881173		[learning rate: 0.00050559]
	Learning Rate: 0.000505586
	LOSS [training: 4.798959812881173 | validation: 5.314049620444857]
	TIME [epoch: 9.76 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784044192584937		[learning rate: 0.00050439]
	Learning Rate: 0.000504393
	LOSS [training: 4.784044192584937 | validation: 5.3294852558523305]
	TIME [epoch: 9.76 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78007392884974		[learning rate: 0.0005032]
	Learning Rate: 0.000503203
	LOSS [training: 4.78007392884974 | validation: 5.317343968721487]
	TIME [epoch: 9.76 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790082119176918		[learning rate: 0.00050202]
	Learning Rate: 0.000502016
	LOSS [training: 4.790082119176918 | validation: 5.3604629716583005]
	TIME [epoch: 9.79 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779450103585834		[learning rate: 0.00050083]
	Learning Rate: 0.000500832
	LOSS [training: 4.779450103585834 | validation: 5.303603340273295]
	TIME [epoch: 9.76 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783190829247281		[learning rate: 0.00049965]
	Learning Rate: 0.000499651
	LOSS [training: 4.783190829247281 | validation: 5.320974804976944]
	TIME [epoch: 9.76 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788357092961416		[learning rate: 0.00049847]
	Learning Rate: 0.000498472
	LOSS [training: 4.788357092961416 | validation: 5.335012040982405]
	TIME [epoch: 9.75 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792050074529598		[learning rate: 0.0004973]
	Learning Rate: 0.000497296
	LOSS [training: 4.792050074529598 | validation: 5.325355919147236]
	TIME [epoch: 9.78 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766697749316377		[learning rate: 0.00049612]
	Learning Rate: 0.000496123
	LOSS [training: 4.766697749316377 | validation: 5.358841736255564]
	TIME [epoch: 9.77 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.825248730456261		[learning rate: 0.00049495]
	Learning Rate: 0.000494953
	LOSS [training: 4.825248730456261 | validation: 5.4014586070350505]
	TIME [epoch: 9.77 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765532684332517		[learning rate: 0.00049379]
	Learning Rate: 0.000493786
	LOSS [training: 4.765532684332517 | validation: 5.314793092008656]
	TIME [epoch: 9.77 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.755832164698455		[learning rate: 0.00049262]
	Learning Rate: 0.000492621
	LOSS [training: 4.755832164698455 | validation: 5.320973155937268]
	TIME [epoch: 9.78 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781821330499068		[learning rate: 0.00049146]
	Learning Rate: 0.000491459
	LOSS [training: 4.781821330499068 | validation: 5.294194404374142]
	TIME [epoch: 9.77 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7668163495903695		[learning rate: 0.0004903]
	Learning Rate: 0.0004903
	LOSS [training: 4.7668163495903695 | validation: 5.343289504588487]
	TIME [epoch: 9.76 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7853614234397295		[learning rate: 0.00048914]
	Learning Rate: 0.000489143
	LOSS [training: 4.7853614234397295 | validation: 5.3129785220395265]
	TIME [epoch: 9.78 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778774412140974		[learning rate: 0.00048799]
	Learning Rate: 0.000487989
	LOSS [training: 4.778774412140974 | validation: 5.327365183610403]
	TIME [epoch: 9.77 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790759407972186		[learning rate: 0.00048684]
	Learning Rate: 0.000486838
	LOSS [training: 4.790759407972186 | validation: 5.333547819916644]
	TIME [epoch: 9.76 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784913188112837		[learning rate: 0.00048569]
	Learning Rate: 0.00048569
	LOSS [training: 4.784913188112837 | validation: 5.370390357704726]
	TIME [epoch: 9.77 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7841633803574775		[learning rate: 0.00048454]
	Learning Rate: 0.000484544
	LOSS [training: 4.7841633803574775 | validation: 5.338823947327097]
	TIME [epoch: 9.78 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.825672380860377		[learning rate: 0.0004834]
	Learning Rate: 0.000483401
	LOSS [training: 4.825672380860377 | validation: 5.319430087591895]
	TIME [epoch: 9.77 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813645231838758		[learning rate: 0.00048226]
	Learning Rate: 0.000482261
	LOSS [training: 4.813645231838758 | validation: 5.326410368412886]
	TIME [epoch: 9.76 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776045626540709		[learning rate: 0.00048112]
	Learning Rate: 0.000481123
	LOSS [training: 4.776045626540709 | validation: 5.40386385404028]
	TIME [epoch: 9.77 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8398377110835336		[learning rate: 0.00047999]
	Learning Rate: 0.000479988
	LOSS [training: 4.8398377110835336 | validation: 5.316326338056694]
	TIME [epoch: 9.78 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.82723892623708		[learning rate: 0.00047886]
	Learning Rate: 0.000478856
	LOSS [training: 4.82723892623708 | validation: 5.411719037327055]
	TIME [epoch: 9.76 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778739846926348		[learning rate: 0.00047773]
	Learning Rate: 0.000477727
	LOSS [training: 4.778739846926348 | validation: 5.304341624264688]
	TIME [epoch: 9.76 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76416620396279		[learning rate: 0.0004766]
	Learning Rate: 0.0004766
	LOSS [training: 4.76416620396279 | validation: 5.31746332757373]
	TIME [epoch: 9.78 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772945987427244		[learning rate: 0.00047548]
	Learning Rate: 0.000475476
	LOSS [training: 4.772945987427244 | validation: 5.337893515461485]
	TIME [epoch: 9.77 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760608301954365		[learning rate: 0.00047435]
	Learning Rate: 0.000474354
	LOSS [training: 4.760608301954365 | validation: 5.311732507785347]
	TIME [epoch: 9.76 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780901816252159		[learning rate: 0.00047324]
	Learning Rate: 0.000473235
	LOSS [training: 4.780901816252159 | validation: 5.368166990732923]
	TIME [epoch: 9.76 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801095700144943		[learning rate: 0.00047212]
	Learning Rate: 0.000472119
	LOSS [training: 4.801095700144943 | validation: 5.332847753009976]
	TIME [epoch: 9.78 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795395535072618		[learning rate: 0.00047101]
	Learning Rate: 0.000471005
	LOSS [training: 4.795395535072618 | validation: 5.350502029512065]
	TIME [epoch: 9.76 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.829142879557375		[learning rate: 0.00046989]
	Learning Rate: 0.000469894
	LOSS [training: 4.829142879557375 | validation: 5.3605028369558925]
	TIME [epoch: 9.76 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801054268281851		[learning rate: 0.00046879]
	Learning Rate: 0.000468786
	LOSS [training: 4.801054268281851 | validation: 5.318026061748867]
	TIME [epoch: 9.76 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774996468248871		[learning rate: 0.00046768]
	Learning Rate: 0.00046768
	LOSS [training: 4.774996468248871 | validation: 5.365148405121386]
	TIME [epoch: 9.78 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.77694423933818		[learning rate: 0.00046658]
	Learning Rate: 0.000466577
	LOSS [training: 4.77694423933818 | validation: 5.312254183236457]
	TIME [epoch: 9.77 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777797907946395		[learning rate: 0.00046548]
	Learning Rate: 0.000465476
	LOSS [training: 4.777797907946395 | validation: 5.295190201977359]
	TIME [epoch: 9.76 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763317951324065		[learning rate: 0.00046438]
	Learning Rate: 0.000464378
	LOSS [training: 4.763317951324065 | validation: 5.29251477669289]
	TIME [epoch: 9.77 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.720986113788605		[learning rate: 0.00046328]
	Learning Rate: 0.000463283
	LOSS [training: 4.720986113788605 | validation: 5.3100586988645855]
	TIME [epoch: 9.76 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.750704508929205		[learning rate: 0.00046219]
	Learning Rate: 0.00046219
	LOSS [training: 4.750704508929205 | validation: 5.299137915338276]
	TIME [epoch: 9.76 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.738449143843575		[learning rate: 0.0004611]
	Learning Rate: 0.0004611
	LOSS [training: 4.738449143843575 | validation: 5.32723351370777]
	TIME [epoch: 9.76 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.736415498162576		[learning rate: 0.00046001]
	Learning Rate: 0.000460012
	LOSS [training: 4.736415498162576 | validation: 5.335966588068682]
	TIME [epoch: 9.79 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7867121449925705		[learning rate: 0.00045893]
	Learning Rate: 0.000458927
	LOSS [training: 4.7867121449925705 | validation: 5.330086532728973]
	TIME [epoch: 9.77 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.730774904971321		[learning rate: 0.00045784]
	Learning Rate: 0.000457844
	LOSS [training: 4.730774904971321 | validation: 5.281150999595577]
	TIME [epoch: 9.77 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.741850197925845		[learning rate: 0.00045676]
	Learning Rate: 0.000456764
	LOSS [training: 4.741850197925845 | validation: 5.336621966480171]
	TIME [epoch: 9.76 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.748939173313494		[learning rate: 0.00045569]
	Learning Rate: 0.000455687
	LOSS [training: 4.748939173313494 | validation: 5.316349575034185]
	TIME [epoch: 9.79 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.75448726578019		[learning rate: 0.00045461]
	Learning Rate: 0.000454612
	LOSS [training: 4.75448726578019 | validation: 5.384823157868562]
	TIME [epoch: 9.76 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.761569718161392		[learning rate: 0.00045354]
	Learning Rate: 0.00045354
	LOSS [training: 4.761569718161392 | validation: 5.326662968071274]
	TIME [epoch: 9.76 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774997013079072		[learning rate: 0.00045247]
	Learning Rate: 0.00045247
	LOSS [training: 4.774997013079072 | validation: 5.352376282247583]
	TIME [epoch: 9.77 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765068145549134		[learning rate: 0.0004514]
	Learning Rate: 0.000451403
	LOSS [training: 4.765068145549134 | validation: 5.33710487319597]
	TIME [epoch: 9.77 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776964995472703		[learning rate: 0.00045034]
	Learning Rate: 0.000450338
	LOSS [training: 4.776964995472703 | validation: 5.398943456212642]
	TIME [epoch: 9.76 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.88309650449151		[learning rate: 0.00044928]
	Learning Rate: 0.000449276
	LOSS [training: 4.88309650449151 | validation: 5.545623877687383]
	TIME [epoch: 9.76 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.966864932621571		[learning rate: 0.00044822]
	Learning Rate: 0.000448216
	LOSS [training: 4.966864932621571 | validation: 5.414734082905381]
	TIME [epoch: 9.78 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.925307224364458		[learning rate: 0.00044716]
	Learning Rate: 0.000447159
	LOSS [training: 4.925307224364458 | validation: 5.416299925899748]
	TIME [epoch: 9.77 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.914846205630502		[learning rate: 0.0004461]
	Learning Rate: 0.000446104
	LOSS [training: 4.914846205630502 | validation: 5.3957769702320135]
	TIME [epoch: 9.77 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.881752599415817		[learning rate: 0.00044505]
	Learning Rate: 0.000445051
	LOSS [training: 4.881752599415817 | validation: 5.49756250368089]
	TIME [epoch: 9.77 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8853178761160665		[learning rate: 0.000444]
	Learning Rate: 0.000444002
	LOSS [training: 4.8853178761160665 | validation: 5.371554153483799]
	TIME [epoch: 9.78 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.89864314679681		[learning rate: 0.00044295]
	Learning Rate: 0.000442954
	LOSS [training: 4.89864314679681 | validation: 5.464312338092225]
	TIME [epoch: 9.76 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.887069914247176		[learning rate: 0.00044191]
	Learning Rate: 0.000441909
	LOSS [training: 4.887069914247176 | validation: 5.387640352917891]
	TIME [epoch: 9.76 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.87193970319554		[learning rate: 0.00044087]
	Learning Rate: 0.000440867
	LOSS [training: 4.87193970319554 | validation: 5.3904165676590035]
	TIME [epoch: 9.76 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.844164649519116		[learning rate: 0.00043983]
	Learning Rate: 0.000439827
	LOSS [training: 4.844164649519116 | validation: 5.344152032514406]
	TIME [epoch: 9.79 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912340400091478		[learning rate: 0.00043879]
	Learning Rate: 0.00043879
	LOSS [training: 4.912340400091478 | validation: 5.482807818029971]
	TIME [epoch: 9.76 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.849626078059437		[learning rate: 0.00043775]
	Learning Rate: 0.000437755
	LOSS [training: 4.849626078059437 | validation: 5.396767051222066]
	TIME [epoch: 9.77 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.862091002550397		[learning rate: 0.00043672]
	Learning Rate: 0.000436722
	LOSS [training: 4.862091002550397 | validation: 5.454791499634007]
	TIME [epoch: 9.77 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.855706401320917		[learning rate: 0.00043569]
	Learning Rate: 0.000435692
	LOSS [training: 4.855706401320917 | validation: 5.353427376423182]
	TIME [epoch: 9.77 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.828066039224224		[learning rate: 0.00043466]
	Learning Rate: 0.000434664
	LOSS [training: 4.828066039224224 | validation: 5.3869540336347335]
	TIME [epoch: 9.77 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.848067238826079		[learning rate: 0.00043364]
	Learning Rate: 0.000433639
	LOSS [training: 4.848067238826079 | validation: 5.336860616705153]
	TIME [epoch: 9.76 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801730170451433		[learning rate: 0.00043262]
	Learning Rate: 0.000432616
	LOSS [training: 4.801730170451433 | validation: 5.345462203151375]
	TIME [epoch: 9.78 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.818199312537926		[learning rate: 0.0004316]
	Learning Rate: 0.000431595
	LOSS [training: 4.818199312537926 | validation: 5.325886851952881]
	TIME [epoch: 9.77 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797506622877572		[learning rate: 0.00043058]
	Learning Rate: 0.000430577
	LOSS [training: 4.797506622877572 | validation: 5.3676667943552205]
	TIME [epoch: 9.76 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796534669043664		[learning rate: 0.00042956]
	Learning Rate: 0.000429562
	LOSS [training: 4.796534669043664 | validation: 5.3281419273447685]
	TIME [epoch: 9.77 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.810086898248632		[learning rate: 0.00042855]
	Learning Rate: 0.000428548
	LOSS [training: 4.810086898248632 | validation: 5.351135248904011]
	TIME [epoch: 9.78 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.819374359117823		[learning rate: 0.00042754]
	Learning Rate: 0.000427538
	LOSS [training: 4.819374359117823 | validation: 5.367121907266751]
	TIME [epoch: 9.76 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78877693810671		[learning rate: 0.00042653]
	Learning Rate: 0.000426529
	LOSS [training: 4.78877693810671 | validation: 5.325396997729568]
	TIME [epoch: 9.77 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785717138633626		[learning rate: 0.00042552]
	Learning Rate: 0.000425523
	LOSS [training: 4.785717138633626 | validation: 5.337854074605842]
	TIME [epoch: 9.79 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773619363696624		[learning rate: 0.00042452]
	Learning Rate: 0.000424519
	LOSS [training: 4.773619363696624 | validation: 5.322374789884534]
	TIME [epoch: 9.77 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790067250987887		[learning rate: 0.00042352]
	Learning Rate: 0.000423518
	LOSS [training: 4.790067250987887 | validation: 5.342572108011329]
	TIME [epoch: 9.76 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779915294954327		[learning rate: 0.00042252]
	Learning Rate: 0.000422519
	LOSS [training: 4.779915294954327 | validation: 5.371699064113937]
	TIME [epoch: 9.77 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800837090796442		[learning rate: 0.00042152]
	Learning Rate: 0.000421522
	LOSS [training: 4.800837090796442 | validation: 5.351164249022013]
	TIME [epoch: 9.79 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8006959173354105		[learning rate: 0.00042053]
	Learning Rate: 0.000420528
	LOSS [training: 4.8006959173354105 | validation: 5.3454714648467645]
	TIME [epoch: 9.77 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790962874128498		[learning rate: 0.00041954]
	Learning Rate: 0.000419536
	LOSS [training: 4.790962874128498 | validation: 5.332112798400467]
	TIME [epoch: 9.77 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79243035029312		[learning rate: 0.00041855]
	Learning Rate: 0.000418546
	LOSS [training: 4.79243035029312 | validation: 5.325901360273033]
	TIME [epoch: 9.76 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786981133245908		[learning rate: 0.00041756]
	Learning Rate: 0.000417559
	LOSS [training: 4.786981133245908 | validation: 5.334897744738096]
	TIME [epoch: 9.78 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788620357416779		[learning rate: 0.00041657]
	Learning Rate: 0.000416574
	LOSS [training: 4.788620357416779 | validation: 5.365564574987948]
	TIME [epoch: 9.76 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801688703077461		[learning rate: 0.00041559]
	Learning Rate: 0.000415591
	LOSS [training: 4.801688703077461 | validation: 5.349361402087982]
	TIME [epoch: 9.77 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799610185147805		[learning rate: 0.00041461]
	Learning Rate: 0.000414611
	LOSS [training: 4.799610185147805 | validation: 5.337507078111887]
	TIME [epoch: 9.78 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783703268542725		[learning rate: 0.00041363]
	Learning Rate: 0.000413633
	LOSS [training: 4.783703268542725 | validation: 5.301976823075231]
	TIME [epoch: 9.78 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781571842794983		[learning rate: 0.00041266]
	Learning Rate: 0.000412657
	LOSS [training: 4.781571842794983 | validation: 5.329218186282798]
	TIME [epoch: 9.77 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766884932501706		[learning rate: 0.00041168]
	Learning Rate: 0.000411684
	LOSS [training: 4.766884932501706 | validation: 5.390377888491395]
	TIME [epoch: 9.76 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.815295908376255		[learning rate: 0.00041071]
	Learning Rate: 0.000410713
	LOSS [training: 4.815295908376255 | validation: 5.3059045330880075]
	TIME [epoch: 9.77 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.756330317493568		[learning rate: 0.00040974]
	Learning Rate: 0.000409744
	LOSS [training: 4.756330317493568 | validation: 5.349221863903477]
	TIME [epoch: 9.76 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778599834252562		[learning rate: 0.00040878]
	Learning Rate: 0.000408778
	LOSS [training: 4.778599834252562 | validation: 5.335306906727503]
	TIME [epoch: 9.76 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765609767687279		[learning rate: 0.00040781]
	Learning Rate: 0.000407813
	LOSS [training: 4.765609767687279 | validation: 5.328724718831142]
	TIME [epoch: 9.77 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791589794783419		[learning rate: 0.00040685]
	Learning Rate: 0.000406851
	LOSS [training: 4.791589794783419 | validation: 5.420771877711253]
	TIME [epoch: 9.78 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800568014118676		[learning rate: 0.00040589]
	Learning Rate: 0.000405892
	LOSS [training: 4.800568014118676 | validation: 5.46505174451993]
	TIME [epoch: 9.77 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79974018423043		[learning rate: 0.00040493]
	Learning Rate: 0.000404934
	LOSS [training: 4.79974018423043 | validation: 5.333362296034493]
	TIME [epoch: 9.76 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.762646122094945		[learning rate: 0.00040398]
	Learning Rate: 0.000403979
	LOSS [training: 4.762646122094945 | validation: 5.306524195662288]
	TIME [epoch: 9.78 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.733579877039928		[learning rate: 0.00040303]
	Learning Rate: 0.000403026
	LOSS [training: 4.733579877039928 | validation: 5.326760848101659]
	TIME [epoch: 9.77 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.754561846672478		[learning rate: 0.00040208]
	Learning Rate: 0.000402076
	LOSS [training: 4.754561846672478 | validation: 5.33459067843475]
	TIME [epoch: 9.77 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.747064669212891		[learning rate: 0.00040113]
	Learning Rate: 0.000401127
	LOSS [training: 4.747064669212891 | validation: 5.331433825571645]
	TIME [epoch: 9.76 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758209098894534		[learning rate: 0.00040018]
	Learning Rate: 0.000400181
	LOSS [training: 4.758209098894534 | validation: 5.339142784479813]
	TIME [epoch: 9.78 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7505723689154		[learning rate: 0.00039924]
	Learning Rate: 0.000399237
	LOSS [training: 4.7505723689154 | validation: 5.332774640460186]
	TIME [epoch: 9.77 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758257198102699		[learning rate: 0.0003983]
	Learning Rate: 0.000398295
	LOSS [training: 4.758257198102699 | validation: 5.41438296138347]
	TIME [epoch: 9.77 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764220663156909		[learning rate: 0.00039736]
	Learning Rate: 0.000397356
	LOSS [training: 4.764220663156909 | validation: 5.320016139898451]
	TIME [epoch: 9.77 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7577713864149125		[learning rate: 0.00039642]
	Learning Rate: 0.000396418
	LOSS [training: 4.7577713864149125 | validation: 5.319972006991357]
	TIME [epoch: 9.79 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758449166453445		[learning rate: 0.00039548]
	Learning Rate: 0.000395483
	LOSS [training: 4.758449166453445 | validation: 5.332720572487559]
	TIME [epoch: 9.77 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7598407130493126		[learning rate: 0.00039455]
	Learning Rate: 0.00039455
	LOSS [training: 4.7598407130493126 | validation: 5.3627695833767755]
	TIME [epoch: 9.76 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788432030770172		[learning rate: 0.00039362]
	Learning Rate: 0.00039362
	LOSS [training: 4.788432030770172 | validation: 5.374411480423978]
	TIME [epoch: 9.76 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7712072108660895		[learning rate: 0.00039269]
	Learning Rate: 0.000392691
	LOSS [training: 4.7712072108660895 | validation: 5.322643946049635]
	TIME [epoch: 9.77 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.745347371319171		[learning rate: 0.00039177]
	Learning Rate: 0.000391765
	LOSS [training: 4.745347371319171 | validation: 5.324288884193765]
	TIME [epoch: 9.76 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787325191745356		[learning rate: 0.00039084]
	Learning Rate: 0.000390841
	LOSS [training: 4.787325191745356 | validation: 5.4868351729642795]
	TIME [epoch: 9.76 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.842943315819592		[learning rate: 0.00038992]
	Learning Rate: 0.000389919
	LOSS [training: 4.842943315819592 | validation: 5.437462932002754]
	TIME [epoch: 9.78 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773586818335393		[learning rate: 0.000389]
	Learning Rate: 0.000388999
	LOSS [training: 4.773586818335393 | validation: 5.314333907090858]
	TIME [epoch: 9.77 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766218455425064		[learning rate: 0.00038808]
	Learning Rate: 0.000388082
	LOSS [training: 4.766218455425064 | validation: 5.336920030091415]
	TIME [epoch: 9.76 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7694015282155515		[learning rate: 0.00038717]
	Learning Rate: 0.000387166
	LOSS [training: 4.7694015282155515 | validation: 5.427273403478177]
	TIME [epoch: 9.77 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808529952699244		[learning rate: 0.00038625]
	Learning Rate: 0.000386253
	LOSS [training: 4.808529952699244 | validation: 5.307326701171026]
	TIME [epoch: 9.8 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7683707584627895		[learning rate: 0.00038534]
	Learning Rate: 0.000385342
	LOSS [training: 4.7683707584627895 | validation: 5.357830545108823]
	TIME [epoch: 9.76 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758388703646334		[learning rate: 0.00038443]
	Learning Rate: 0.000384433
	LOSS [training: 4.758388703646334 | validation: 5.31957289765463]
	TIME [epoch: 9.77 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773311979367778		[learning rate: 0.00038353]
	Learning Rate: 0.000383526
	LOSS [training: 4.773311979367778 | validation: 5.340556772741999]
	TIME [epoch: 9.77 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765860073595584		[learning rate: 0.00038262]
	Learning Rate: 0.000382621
	LOSS [training: 4.765860073595584 | validation: 5.3234110875749785]
	TIME [epoch: 9.78 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764269733586891		[learning rate: 0.00038172]
	Learning Rate: 0.000381719
	LOSS [training: 4.764269733586891 | validation: 5.300616620510489]
	TIME [epoch: 9.75 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.756385845663432		[learning rate: 0.00038082]
	Learning Rate: 0.000380818
	LOSS [training: 4.756385845663432 | validation: 5.331480165943771]
	TIME [epoch: 9.77 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768508317862063		[learning rate: 0.00037992]
	Learning Rate: 0.00037992
	LOSS [training: 4.768508317862063 | validation: 5.309022062137951]
	TIME [epoch: 9.78 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.757366564661602		[learning rate: 0.00037902]
	Learning Rate: 0.000379024
	LOSS [training: 4.757366564661602 | validation: 5.336347800252252]
	TIME [epoch: 9.77 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792164570069941		[learning rate: 0.00037813]
	Learning Rate: 0.00037813
	LOSS [training: 4.792164570069941 | validation: 5.340481035821506]
	TIME [epoch: 9.77 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767302419006716		[learning rate: 0.00037724]
	Learning Rate: 0.000377238
	LOSS [training: 4.767302419006716 | validation: 5.328706669056032]
	TIME [epoch: 9.76 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776229413821417		[learning rate: 0.00037635]
	Learning Rate: 0.000376348
	LOSS [training: 4.776229413821417 | validation: 5.335206759817362]
	TIME [epoch: 9.78 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.77712577795472		[learning rate: 0.00037546]
	Learning Rate: 0.00037546
	LOSS [training: 4.77712577795472 | validation: 5.315111440098213]
	TIME [epoch: 9.77 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775673128998224		[learning rate: 0.00037457]
	Learning Rate: 0.000374575
	LOSS [training: 4.775673128998224 | validation: 5.323007444236573]
	TIME [epoch: 9.77 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802301913330579		[learning rate: 0.00037369]
	Learning Rate: 0.000373691
	LOSS [training: 4.802301913330579 | validation: 5.419101430390671]
	TIME [epoch: 9.77 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798975364361222		[learning rate: 0.00037281]
	Learning Rate: 0.00037281
	LOSS [training: 4.798975364361222 | validation: 5.323315861725586]
	TIME [epoch: 9.78 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790219667381308		[learning rate: 0.00037193]
	Learning Rate: 0.00037193
	LOSS [training: 4.790219667381308 | validation: 5.34955451989614]
	TIME [epoch: 9.77 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.821062744781723		[learning rate: 0.00037105]
	Learning Rate: 0.000371053
	LOSS [training: 4.821062744781723 | validation: 5.385037417688451]
	TIME [epoch: 9.76 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784142628309271		[learning rate: 0.00037018]
	Learning Rate: 0.000370178
	LOSS [training: 4.784142628309271 | validation: 5.3313055857500755]
	TIME [epoch: 9.77 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.829707688783691		[learning rate: 0.0003693]
	Learning Rate: 0.000369305
	LOSS [training: 4.829707688783691 | validation: 5.350019352303071]
	TIME [epoch: 9.78 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78987722256493		[learning rate: 0.00036843]
	Learning Rate: 0.000368433
	LOSS [training: 4.78987722256493 | validation: 5.333398034838986]
	TIME [epoch: 9.77 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792888924149897		[learning rate: 0.00036756]
	Learning Rate: 0.000367564
	LOSS [training: 4.792888924149897 | validation: 5.403351608371371]
	TIME [epoch: 9.76 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798214566836606		[learning rate: 0.0003667]
	Learning Rate: 0.000366697
	LOSS [training: 4.798214566836606 | validation: 5.321071983790488]
	TIME [epoch: 9.78 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808016695697544		[learning rate: 0.00036583]
	Learning Rate: 0.000365832
	LOSS [training: 4.808016695697544 | validation: 5.34449926773125]
	TIME [epoch: 9.77 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.811288012479382		[learning rate: 0.00036497]
	Learning Rate: 0.000364969
	LOSS [training: 4.811288012479382 | validation: 5.362390051089753]
	TIME [epoch: 9.77 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785008513969215		[learning rate: 0.00036411]
	Learning Rate: 0.000364108
	LOSS [training: 4.785008513969215 | validation: 5.3299975642749144]
	TIME [epoch: 9.77 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807318301174105		[learning rate: 0.00036325]
	Learning Rate: 0.00036325
	LOSS [training: 4.807318301174105 | validation: 5.356178497135119]
	TIME [epoch: 9.81 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824594978894049		[learning rate: 0.00036239]
	Learning Rate: 0.000362393
	LOSS [training: 4.824594978894049 | validation: 5.353240336634477]
	TIME [epoch: 9.77 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836810414727738		[learning rate: 0.00036154]
	Learning Rate: 0.000361538
	LOSS [training: 4.836810414727738 | validation: 5.461365204337544]
	TIME [epoch: 9.77 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.843896963376552		[learning rate: 0.00036069]
	Learning Rate: 0.000360685
	LOSS [training: 4.843896963376552 | validation: 5.356220990132808]
	TIME [epoch: 9.78 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812336410729275		[learning rate: 0.00035983]
	Learning Rate: 0.000359834
	LOSS [training: 4.812336410729275 | validation: 5.334125464064973]
	TIME [epoch: 9.78 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799064761912792		[learning rate: 0.00035899]
	Learning Rate: 0.000358986
	LOSS [training: 4.799064761912792 | validation: 5.415909784527492]
	TIME [epoch: 9.77 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.840268272127622		[learning rate: 0.00035814]
	Learning Rate: 0.000358139
	LOSS [training: 4.840268272127622 | validation: 5.377545608613515]
	TIME [epoch: 9.77 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.835631129731		[learning rate: 0.00035729]
	Learning Rate: 0.000357294
	LOSS [training: 4.835631129731 | validation: 5.373091601541293]
	TIME [epoch: 9.79 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788661488618239		[learning rate: 0.00035645]
	Learning Rate: 0.000356451
	LOSS [training: 4.788661488618239 | validation: 5.37511059498602]
	TIME [epoch: 9.79 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839540077429247		[learning rate: 0.00035561]
	Learning Rate: 0.00035561
	LOSS [training: 4.839540077429247 | validation: 5.354589092615132]
	TIME [epoch: 9.77 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789507500030707		[learning rate: 0.00035477]
	Learning Rate: 0.000354771
	LOSS [training: 4.789507500030707 | validation: 5.351220047790262]
	TIME [epoch: 9.77 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794479847088151		[learning rate: 0.00035393]
	Learning Rate: 0.000353935
	LOSS [training: 4.794479847088151 | validation: 5.334740237010924]
	TIME [epoch: 9.81 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784713599077996		[learning rate: 0.0003531]
	Learning Rate: 0.0003531
	LOSS [training: 4.784713599077996 | validation: 5.315173707238341]
	TIME [epoch: 9.78 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7860209934471225		[learning rate: 0.00035227]
	Learning Rate: 0.000352267
	LOSS [training: 4.7860209934471225 | validation: 5.30259528830189]
	TIME [epoch: 9.77 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7808376231614576		[learning rate: 0.00035144]
	Learning Rate: 0.000351436
	LOSS [training: 4.7808376231614576 | validation: 5.339478688015074]
	TIME [epoch: 9.79 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787683174476052		[learning rate: 0.00035061]
	Learning Rate: 0.000350607
	LOSS [training: 4.787683174476052 | validation: 5.321590628514534]
	TIME [epoch: 9.78 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791246240425535		[learning rate: 0.00034978]
	Learning Rate: 0.00034978
	LOSS [training: 4.791246240425535 | validation: 5.3120802485032526]
	TIME [epoch: 9.78 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781927620893616		[learning rate: 0.00034895]
	Learning Rate: 0.000348955
	LOSS [training: 4.781927620893616 | validation: 5.323380907046101]
	TIME [epoch: 9.77 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78846785582802		[learning rate: 0.00034813]
	Learning Rate: 0.000348132
	LOSS [training: 4.78846785582802 | validation: 5.328434755721163]
	TIME [epoch: 9.8 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772623563096414		[learning rate: 0.00034731]
	Learning Rate: 0.000347311
	LOSS [training: 4.772623563096414 | validation: 5.337799492459156]
	TIME [epoch: 9.79 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78630373610844		[learning rate: 0.00034649]
	Learning Rate: 0.000346491
	LOSS [training: 4.78630373610844 | validation: 5.324966750512267]
	TIME [epoch: 9.77 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813853818798485		[learning rate: 0.00034567]
	Learning Rate: 0.000345674
	LOSS [training: 4.813853818798485 | validation: 5.333364006024746]
	TIME [epoch: 9.77 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783018900525391		[learning rate: 0.00034486]
	Learning Rate: 0.000344859
	LOSS [training: 4.783018900525391 | validation: 5.314643669261085]
	TIME [epoch: 9.79 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782246408136418		[learning rate: 0.00034405]
	Learning Rate: 0.000344045
	LOSS [training: 4.782246408136418 | validation: 5.375283599480373]
	TIME [epoch: 9.77 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.831041154237566		[learning rate: 0.00034323]
	Learning Rate: 0.000343233
	LOSS [training: 4.831041154237566 | validation: 5.328097510975054]
	TIME [epoch: 9.76 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764106515059187		[learning rate: 0.00034242]
	Learning Rate: 0.000342424
	LOSS [training: 4.764106515059187 | validation: 5.323294283776793]
	TIME [epoch: 9.77 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771447821793807		[learning rate: 0.00034162]
	Learning Rate: 0.000341616
	LOSS [training: 4.771447821793807 | validation: 5.318787148416338]
	TIME [epoch: 9.78 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78278212179058		[learning rate: 0.00034081]
	Learning Rate: 0.00034081
	LOSS [training: 4.78278212179058 | validation: 5.370180102581287]
	TIME [epoch: 9.77 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780187079468746		[learning rate: 0.00034001]
	Learning Rate: 0.000340006
	LOSS [training: 4.780187079468746 | validation: 5.339199777747602]
	TIME [epoch: 9.79 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780986940567791		[learning rate: 0.0003392]
	Learning Rate: 0.000339204
	LOSS [training: 4.780986940567791 | validation: 5.342881480160486]
	TIME [epoch: 9.78 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780075693291762		[learning rate: 0.0003384]
	Learning Rate: 0.000338404
	LOSS [training: 4.780075693291762 | validation: 5.360090426839893]
	TIME [epoch: 9.78 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806449565174974		[learning rate: 0.00033761]
	Learning Rate: 0.000337606
	LOSS [training: 4.806449565174974 | validation: 5.374250235178927]
	TIME [epoch: 9.77 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793764674065966		[learning rate: 0.00033681]
	Learning Rate: 0.00033681
	LOSS [training: 4.793764674065966 | validation: 5.306761142537213]
	TIME [epoch: 9.78 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763412380221255		[learning rate: 0.00033602]
	Learning Rate: 0.000336015
	LOSS [training: 4.763412380221255 | validation: 5.319973022059083]
	TIME [epoch: 9.79 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767546494055173		[learning rate: 0.00033522]
	Learning Rate: 0.000335223
	LOSS [training: 4.767546494055173 | validation: 5.322667544423439]
	TIME [epoch: 9.78 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7651775684676645		[learning rate: 0.00033443]
	Learning Rate: 0.000334432
	LOSS [training: 4.7651775684676645 | validation: 5.332825473817397]
	TIME [epoch: 9.8 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7730891306541805		[learning rate: 0.00033364]
	Learning Rate: 0.000333643
	LOSS [training: 4.7730891306541805 | validation: 5.324543875896672]
	TIME [epoch: 9.79 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790422694475182		[learning rate: 0.00033286]
	Learning Rate: 0.000332856
	LOSS [training: 4.790422694475182 | validation: 5.33969598807932]
	TIME [epoch: 9.79 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832410506771034		[learning rate: 0.00033207]
	Learning Rate: 0.000332071
	LOSS [training: 4.832410506771034 | validation: 5.438654182462511]
	TIME [epoch: 9.78 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791607842633915		[learning rate: 0.00033129]
	Learning Rate: 0.000331288
	LOSS [training: 4.791607842633915 | validation: 5.333988755735781]
	TIME [epoch: 9.77 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.805516735516473		[learning rate: 0.00033051]
	Learning Rate: 0.000330506
	LOSS [training: 4.805516735516473 | validation: 5.357171265632892]
	TIME [epoch: 9.78 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768165071571383		[learning rate: 0.00032973]
	Learning Rate: 0.000329726
	LOSS [training: 4.768165071571383 | validation: 5.301701259156379]
	TIME [epoch: 9.79 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.740970991363506		[learning rate: 0.00032895]
	Learning Rate: 0.000328949
	LOSS [training: 4.740970991363506 | validation: 5.35504529478231]
	TIME [epoch: 9.77 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.741331145526151		[learning rate: 0.00032817]
	Learning Rate: 0.000328173
	LOSS [training: 4.741331145526151 | validation: 5.309367826941691]
	TIME [epoch: 9.77 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.733574925405065		[learning rate: 0.0003274]
	Learning Rate: 0.000327399
	LOSS [training: 4.733574925405065 | validation: 5.295115095372844]
	TIME [epoch: 9.8 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.735196014041228		[learning rate: 0.00032663]
	Learning Rate: 0.000326626
	LOSS [training: 4.735196014041228 | validation: 5.3114978387143195]
	TIME [epoch: 9.77 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.74805733398578		[learning rate: 0.00032586]
	Learning Rate: 0.000325856
	LOSS [training: 4.74805733398578 | validation: 5.29121211502503]
	TIME [epoch: 9.78 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.744008450976101		[learning rate: 0.00032509]
	Learning Rate: 0.000325087
	LOSS [training: 4.744008450976101 | validation: 5.38775611443918]
	TIME [epoch: 9.78 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764206115873931		[learning rate: 0.00032432]
	Learning Rate: 0.00032432
	LOSS [training: 4.764206115873931 | validation: 5.307248131697829]
	TIME [epoch: 9.79 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.742772181666955		[learning rate: 0.00032356]
	Learning Rate: 0.000323555
	LOSS [training: 4.742772181666955 | validation: 5.294780668342009]
	TIME [epoch: 9.77 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.752725784799469		[learning rate: 0.00032279]
	Learning Rate: 0.000322792
	LOSS [training: 4.752725784799469 | validation: 5.304076264030461]
	TIME [epoch: 9.77 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772498397081501		[learning rate: 0.00032203]
	Learning Rate: 0.000322031
	LOSS [training: 4.772498397081501 | validation: 5.331599561956714]
	TIME [epoch: 9.79 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769251830662438		[learning rate: 0.00032127]
	Learning Rate: 0.000321271
	LOSS [training: 4.769251830662438 | validation: 5.327370392530845]
	TIME [epoch: 9.78 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776446937990255		[learning rate: 0.00032051]
	Learning Rate: 0.000320513
	LOSS [training: 4.776446937990255 | validation: 5.318134858240358]
	TIME [epoch: 9.78 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7538007183824105		[learning rate: 0.00031976]
	Learning Rate: 0.000319757
	LOSS [training: 4.7538007183824105 | validation: 5.297685805207999]
	TIME [epoch: 9.77 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.750667508025083		[learning rate: 0.000319]
	Learning Rate: 0.000319003
	LOSS [training: 4.750667508025083 | validation: 5.304770915504104]
	TIME [epoch: 9.79 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784814932990228		[learning rate: 0.00031825]
	Learning Rate: 0.000318251
	LOSS [training: 4.784814932990228 | validation: 5.343864969829216]
	TIME [epoch: 9.78 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760537986282939		[learning rate: 0.0003175]
	Learning Rate: 0.0003175
	LOSS [training: 4.760537986282939 | validation: 5.316765120509997]
	TIME [epoch: 9.78 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767401826657015		[learning rate: 0.00031675]
	Learning Rate: 0.000316751
	LOSS [training: 4.767401826657015 | validation: 5.33936671919765]
	TIME [epoch: 9.77 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758457179215326		[learning rate: 0.000316]
	Learning Rate: 0.000316004
	LOSS [training: 4.758457179215326 | validation: 5.375240400721777]
	TIME [epoch: 9.79 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778173767605464		[learning rate: 0.00031526]
	Learning Rate: 0.000315258
	LOSS [training: 4.778173767605464 | validation: 5.307292593973114]
	TIME [epoch: 9.77 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765709770918983		[learning rate: 0.00031451]
	Learning Rate: 0.000314515
	LOSS [training: 4.765709770918983 | validation: 5.31846562082008]
	TIME [epoch: 9.78 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764456618578113		[learning rate: 0.00031377]
	Learning Rate: 0.000313773
	LOSS [training: 4.764456618578113 | validation: 5.32184332406739]
	TIME [epoch: 9.79 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804342512800547		[learning rate: 0.00031303]
	Learning Rate: 0.000313033
	LOSS [training: 4.804342512800547 | validation: 5.320547256147295]
	TIME [epoch: 9.78 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789942679186813		[learning rate: 0.00031229]
	Learning Rate: 0.000312294
	LOSS [training: 4.789942679186813 | validation: 5.3520678718218475]
	TIME [epoch: 9.78 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806213447886814		[learning rate: 0.00031156]
	Learning Rate: 0.000311558
	LOSS [training: 4.806213447886814 | validation: 5.4058520769013505]
	TIME [epoch: 9.78 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824090615281834		[learning rate: 0.00031082]
	Learning Rate: 0.000310823
	LOSS [training: 4.824090615281834 | validation: 5.342775813999144]
	TIME [epoch: 9.8 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778795018146928		[learning rate: 0.00031009]
	Learning Rate: 0.00031009
	LOSS [training: 4.778795018146928 | validation: 5.339585960227043]
	TIME [epoch: 9.78 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7958054868608855		[learning rate: 0.00030936]
	Learning Rate: 0.000309358
	LOSS [training: 4.7958054868608855 | validation: 5.421110751836092]
	TIME [epoch: 9.78 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8279596228347526		[learning rate: 0.00030863]
	Learning Rate: 0.000308628
	LOSS [training: 4.8279596228347526 | validation: 5.350976586351866]
	TIME [epoch: 9.77 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799993276346422		[learning rate: 0.0003079]
	Learning Rate: 0.0003079
	LOSS [training: 4.799993276346422 | validation: 5.350882408196918]
	TIME [epoch: 9.8 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8257023997860795		[learning rate: 0.00030717]
	Learning Rate: 0.000307174
	LOSS [training: 4.8257023997860795 | validation: 5.325048016618421]
	TIME [epoch: 9.79 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768317739163042		[learning rate: 0.00030645]
	Learning Rate: 0.00030645
	LOSS [training: 4.768317739163042 | validation: 5.339126068855378]
	TIME [epoch: 9.77 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7925875964759665		[learning rate: 0.00030573]
	Learning Rate: 0.000305727
	LOSS [training: 4.7925875964759665 | validation: 5.420997747277534]
	TIME [epoch: 9.79 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8279757113083726		[learning rate: 0.00030501]
	Learning Rate: 0.000305005
	LOSS [training: 4.8279757113083726 | validation: 5.375959489865226]
	TIME [epoch: 9.79 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7786539468276565		[learning rate: 0.00030429]
	Learning Rate: 0.000304286
	LOSS [training: 4.7786539468276565 | validation: 5.367428130120886]
	TIME [epoch: 9.77 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813315799242734		[learning rate: 0.00030357]
	Learning Rate: 0.000303568
	LOSS [training: 4.813315799242734 | validation: 5.338300054774143]
	TIME [epoch: 9.78 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807162410174417		[learning rate: 0.00030285]
	Learning Rate: 0.000302852
	LOSS [training: 4.807162410174417 | validation: 5.371944941500338]
	TIME [epoch: 9.79 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8105220757956735		[learning rate: 0.00030214]
	Learning Rate: 0.000302138
	LOSS [training: 4.8105220757956735 | validation: 5.35555763463334]
	TIME [epoch: 9.79 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8028698767739275		[learning rate: 0.00030143]
	Learning Rate: 0.000301425
	LOSS [training: 4.8028698767739275 | validation: 5.344028342840435]
	TIME [epoch: 9.79 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799992092950882		[learning rate: 0.00030071]
	Learning Rate: 0.000300714
	LOSS [training: 4.799992092950882 | validation: 5.392464661840391]
	TIME [epoch: 9.79 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.849754025072896		[learning rate: 0.0003]
	Learning Rate: 0.000300005
	LOSS [training: 4.849754025072896 | validation: 5.57140497030253]
	TIME [epoch: 9.82 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.066799455483857		[learning rate: 0.0002993]
	Learning Rate: 0.000299297
	LOSS [training: 5.066799455483857 | validation: 5.482474560592448]
	TIME [epoch: 9.78 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.941701409020702		[learning rate: 0.00029859]
	Learning Rate: 0.000298591
	LOSS [training: 4.941701409020702 | validation: 5.439514186157488]
	TIME [epoch: 9.77 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.944165672570678		[learning rate: 0.00029789]
	Learning Rate: 0.000297887
	LOSS [training: 4.944165672570678 | validation: 5.481954883297424]
	TIME [epoch: 9.8 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.950397178379781		[learning rate: 0.00029718]
	Learning Rate: 0.000297184
	LOSS [training: 4.950397178379781 | validation: 5.411046193779062]
	TIME [epoch: 9.8 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.931982392066306		[learning rate: 0.00029648]
	Learning Rate: 0.000296483
	LOSS [training: 4.931982392066306 | validation: 5.391547866534011]
	TIME [epoch: 9.78 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.917346005031237		[learning rate: 0.00029578]
	Learning Rate: 0.000295784
	LOSS [training: 4.917346005031237 | validation: 5.411813068265201]
	TIME [epoch: 9.78 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.931680900683753		[learning rate: 0.00029509]
	Learning Rate: 0.000295086
	LOSS [training: 4.931680900683753 | validation: 5.477595404647206]
	TIME [epoch: 9.8 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.943760957654117		[learning rate: 0.00029439]
	Learning Rate: 0.00029439
	LOSS [training: 4.943760957654117 | validation: 5.495488672571958]
	TIME [epoch: 9.77 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9327070123991446		[learning rate: 0.0002937]
	Learning Rate: 0.000293696
	LOSS [training: 4.9327070123991446 | validation: 5.442463395789995]
	TIME [epoch: 9.79 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.947310152679218		[learning rate: 0.000293]
	Learning Rate: 0.000293003
	LOSS [training: 4.947310152679218 | validation: 5.411291524741589]
	TIME [epoch: 9.77 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.916712553629411		[learning rate: 0.00029231]
	Learning Rate: 0.000292312
	LOSS [training: 4.916712553629411 | validation: 5.427896204133269]
	TIME [epoch: 9.78 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.922359793388697		[learning rate: 0.00029162]
	Learning Rate: 0.000291622
	LOSS [training: 4.922359793388697 | validation: 5.41015910040132]
	TIME [epoch: 9.77 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.912728460145283		[learning rate: 0.00029093]
	Learning Rate: 0.000290934
	LOSS [training: 4.912728460145283 | validation: 5.412938559640207]
	TIME [epoch: 9.77 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.924261554367785		[learning rate: 0.00029025]
	Learning Rate: 0.000290248
	LOSS [training: 4.924261554367785 | validation: 5.517950942444201]
	TIME [epoch: 9.79 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.974082876858239		[learning rate: 0.00028956]
	Learning Rate: 0.000289563
	LOSS [training: 4.974082876858239 | validation: 5.424630916073941]
	TIME [epoch: 9.81 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.959710895518733		[learning rate: 0.00028888]
	Learning Rate: 0.00028888
	LOSS [training: 4.959710895518733 | validation: 5.48102372249856]
	TIME [epoch: 9.77 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.953272152912042		[learning rate: 0.0002882]
	Learning Rate: 0.000288199
	LOSS [training: 4.953272152912042 | validation: 5.418957268609509]
	TIME [epoch: 9.79 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.924873762856634		[learning rate: 0.00028752]
	Learning Rate: 0.000287519
	LOSS [training: 4.924873762856634 | validation: 5.400341042371897]
	TIME [epoch: 9.78 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.943309437744658		[learning rate: 0.00028684]
	Learning Rate: 0.000286841
	LOSS [training: 4.943309437744658 | validation: 5.4380939762775045]
	TIME [epoch: 9.77 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.948530840647699		[learning rate: 0.00028616]
	Learning Rate: 0.000286164
	LOSS [training: 4.948530840647699 | validation: 5.41803813446772]
	TIME [epoch: 9.78 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9327576816604415		[learning rate: 0.00028549]
	Learning Rate: 0.000285489
	LOSS [training: 4.9327576816604415 | validation: 5.417842267616599]
	TIME [epoch: 9.78 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.891950783852782		[learning rate: 0.00028482]
	Learning Rate: 0.000284816
	LOSS [training: 4.891950783852782 | validation: 5.39245451851227]
	TIME [epoch: 9.8 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.928512453690129		[learning rate: 0.00028414]
	Learning Rate: 0.000284144
	LOSS [training: 4.928512453690129 | validation: 5.418747448368817]
	TIME [epoch: 9.76 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.900995840309616		[learning rate: 0.00028347]
	Learning Rate: 0.000283474
	LOSS [training: 4.900995840309616 | validation: 5.401230436331978]
	TIME [epoch: 9.78 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.876236283501887		[learning rate: 0.00028281]
	Learning Rate: 0.000282805
	LOSS [training: 4.876236283501887 | validation: 5.367534757390882]
	TIME [epoch: 9.77 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.869496855536047		[learning rate: 0.00028214]
	Learning Rate: 0.000282138
	LOSS [training: 4.869496855536047 | validation: 5.427727650757129]
	TIME [epoch: 9.8 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8725666626657205		[learning rate: 0.00028147]
	Learning Rate: 0.000281472
	LOSS [training: 4.8725666626657205 | validation: 5.385492827339354]
	TIME [epoch: 9.77 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8865684558508775		[learning rate: 0.00028081]
	Learning Rate: 0.000280808
	LOSS [training: 4.8865684558508775 | validation: 5.349316635385692]
	TIME [epoch: 9.78 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.827034399192368		[learning rate: 0.00028015]
	Learning Rate: 0.000280146
	LOSS [training: 4.827034399192368 | validation: 5.336382437758188]
	TIME [epoch: 9.79 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808098179573481		[learning rate: 0.00027949]
	Learning Rate: 0.000279485
	LOSS [training: 4.808098179573481 | validation: 5.375490575484175]
	TIME [epoch: 9.78 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.847675396680239		[learning rate: 0.00027883]
	Learning Rate: 0.000278826
	LOSS [training: 4.847675396680239 | validation: 5.356276560967469]
	TIME [epoch: 9.77 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.809548168148142		[learning rate: 0.00027817]
	Learning Rate: 0.000278168
	LOSS [training: 4.809548168148142 | validation: 5.322244761324166]
	TIME [epoch: 9.78 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.809047675258602		[learning rate: 0.00027751]
	Learning Rate: 0.000277512
	LOSS [training: 4.809047675258602 | validation: 5.322800516239734]
	TIME [epoch: 9.79 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7880106734559345		[learning rate: 0.00027686]
	Learning Rate: 0.000276858
	LOSS [training: 4.7880106734559345 | validation: 5.346307352413046]
	TIME [epoch: 9.76 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787339845975854		[learning rate: 0.0002762]
	Learning Rate: 0.000276204
	LOSS [training: 4.787339845975854 | validation: 5.315971922431569]
	TIME [epoch: 9.76 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782770235723815		[learning rate: 0.00027555]
	Learning Rate: 0.000275553
	LOSS [training: 4.782770235723815 | validation: 5.341687967381891]
	TIME [epoch: 9.77 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8054873636632705		[learning rate: 0.0002749]
	Learning Rate: 0.000274903
	LOSS [training: 4.8054873636632705 | validation: 5.308714780711198]
	TIME [epoch: 9.79 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787777359652237		[learning rate: 0.00027425]
	Learning Rate: 0.000274255
	LOSS [training: 4.787777359652237 | validation: 5.322337510416134]
	TIME [epoch: 9.76 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78466312180011		[learning rate: 0.00027361]
	Learning Rate: 0.000273608
	LOSS [training: 4.78466312180011 | validation: 5.333333656336728]
	TIME [epoch: 9.78 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781697176304122		[learning rate: 0.00027296]
	Learning Rate: 0.000272962
	LOSS [training: 4.781697176304122 | validation: 5.316408993821434]
	TIME [epoch: 9.78 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773600230571461		[learning rate: 0.00027232]
	Learning Rate: 0.000272318
	LOSS [training: 4.773600230571461 | validation: 5.3442077481587225]
	TIME [epoch: 9.77 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.811775006554697		[learning rate: 0.00027168]
	Learning Rate: 0.000271676
	LOSS [training: 4.811775006554697 | validation: 5.371535167344239]
	TIME [epoch: 9.77 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793425269462036		[learning rate: 0.00027104]
	Learning Rate: 0.000271035
	LOSS [training: 4.793425269462036 | validation: 5.3567155710907235]
	TIME [epoch: 9.78 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7933389679981655		[learning rate: 0.0002704]
	Learning Rate: 0.000270396
	LOSS [training: 4.7933389679981655 | validation: 5.323555420773397]
	TIME [epoch: 9.79 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79394023877234		[learning rate: 0.00026976]
	Learning Rate: 0.000269758
	LOSS [training: 4.79394023877234 | validation: 5.361195182394375]
	TIME [epoch: 9.78 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7879707278472585		[learning rate: 0.00026912]
	Learning Rate: 0.000269122
	LOSS [training: 4.7879707278472585 | validation: 5.312188857549619]
	TIME [epoch: 9.78 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789478730164202		[learning rate: 0.00026849]
	Learning Rate: 0.000268487
	LOSS [training: 4.789478730164202 | validation: 5.354081710257146]
	TIME [epoch: 9.78 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791539947519496		[learning rate: 0.00026785]
	Learning Rate: 0.000267854
	LOSS [training: 4.791539947519496 | validation: 5.324615927705052]
	TIME [epoch: 9.8 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804642640675832		[learning rate: 0.00026722]
	Learning Rate: 0.000267222
	LOSS [training: 4.804642640675832 | validation: 5.340702420092039]
	TIME [epoch: 9.77 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812278174680882		[learning rate: 0.00026659]
	Learning Rate: 0.000266591
	LOSS [training: 4.812278174680882 | validation: 5.347702484969191]
	TIME [epoch: 9.77 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791664814119147		[learning rate: 0.00026596]
	Learning Rate: 0.000265963
	LOSS [training: 4.791664814119147 | validation: 5.334014210001296]
	TIME [epoch: 9.78 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804061174616989		[learning rate: 0.00026534]
	Learning Rate: 0.000265335
	LOSS [training: 4.804061174616989 | validation: 5.332210302430349]
	TIME [epoch: 9.8 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8082945299285615		[learning rate: 0.00026471]
	Learning Rate: 0.000264709
	LOSS [training: 4.8082945299285615 | validation: 5.334373134743405]
	TIME [epoch: 9.78 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791831698782717		[learning rate: 0.00026408]
	Learning Rate: 0.000264085
	LOSS [training: 4.791831698782717 | validation: 5.3226690283945]
	TIME [epoch: 9.8 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790180669020841		[learning rate: 0.00026346]
	Learning Rate: 0.000263462
	LOSS [training: 4.790180669020841 | validation: 5.3371120302843655]
	TIME [epoch: 9.8 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800236854071178		[learning rate: 0.00026284]
	Learning Rate: 0.00026284
	LOSS [training: 4.800236854071178 | validation: 5.345714243716148]
	TIME [epoch: 9.77 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798010001581055		[learning rate: 0.00026222]
	Learning Rate: 0.00026222
	LOSS [training: 4.798010001581055 | validation: 5.341094631037065]
	TIME [epoch: 9.77 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813754699259754		[learning rate: 0.0002616]
	Learning Rate: 0.000261602
	LOSS [training: 4.813754699259754 | validation: 5.320610209602604]
	TIME [epoch: 9.78 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785358356297179		[learning rate: 0.00026098]
	Learning Rate: 0.000260985
	LOSS [training: 4.785358356297179 | validation: 5.344058995183701]
	TIME [epoch: 9.8 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795830959681995		[learning rate: 0.00026037]
	Learning Rate: 0.000260369
	LOSS [training: 4.795830959681995 | validation: 5.319528356452543]
	TIME [epoch: 9.77 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782859308235775		[learning rate: 0.00025976]
	Learning Rate: 0.000259755
	LOSS [training: 4.782859308235775 | validation: 5.337973187208008]
	TIME [epoch: 9.8 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808817105458449		[learning rate: 0.00025914]
	Learning Rate: 0.000259142
	LOSS [training: 4.808817105458449 | validation: 5.332486456488851]
	TIME [epoch: 9.78 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804109432293034		[learning rate: 0.00025853]
	Learning Rate: 0.000258531
	LOSS [training: 4.804109432293034 | validation: 5.360130474983483]
	TIME [epoch: 9.79 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801509356995018		[learning rate: 0.00025792]
	Learning Rate: 0.000257921
	LOSS [training: 4.801509356995018 | validation: 5.340602005785297]
	TIME [epoch: 9.78 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7876473354201865		[learning rate: 0.00025731]
	Learning Rate: 0.000257313
	LOSS [training: 4.7876473354201865 | validation: 5.334118814745554]
	TIME [epoch: 9.78 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804913883953146		[learning rate: 0.00025671]
	Learning Rate: 0.000256706
	LOSS [training: 4.804913883953146 | validation: 5.333887183364702]
	TIME [epoch: 9.8 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798292884651202		[learning rate: 0.0002561]
	Learning Rate: 0.0002561
	LOSS [training: 4.798292884651202 | validation: 5.3491010854403545]
	TIME [epoch: 9.77 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806103076062437		[learning rate: 0.0002555]
	Learning Rate: 0.000255496
	LOSS [training: 4.806103076062437 | validation: 5.345483461692968]
	TIME [epoch: 9.78 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.817607973234149		[learning rate: 0.00025489]
	Learning Rate: 0.000254894
	LOSS [training: 4.817607973234149 | validation: 5.334214699697823]
	TIME [epoch: 9.78 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793796050509177		[learning rate: 0.00025429]
	Learning Rate: 0.000254292
	LOSS [training: 4.793796050509177 | validation: 5.36413497444127]
	TIME [epoch: 9.8 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793892775865148		[learning rate: 0.00025369]
	Learning Rate: 0.000253693
	LOSS [training: 4.793892775865148 | validation: 5.34410023583585]
	TIME [epoch: 9.79 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778323680304885		[learning rate: 0.00025309]
	Learning Rate: 0.000253094
	LOSS [training: 4.778323680304885 | validation: 5.343512722063103]
	TIME [epoch: 9.77 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802108531435614		[learning rate: 0.0002525]
	Learning Rate: 0.000252497
	LOSS [training: 4.802108531435614 | validation: 5.396058347250716]
	TIME [epoch: 9.79 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.830226378494934		[learning rate: 0.0002519]
	Learning Rate: 0.000251902
	LOSS [training: 4.830226378494934 | validation: 5.356228308155964]
	TIME [epoch: 9.79 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.861766115046034		[learning rate: 0.00025131]
	Learning Rate: 0.000251307
	LOSS [training: 4.861766115046034 | validation: 5.316584996510927]
	TIME [epoch: 9.77 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788145285720837		[learning rate: 0.00025071]
	Learning Rate: 0.000250714
	LOSS [training: 4.788145285720837 | validation: 5.349532632027387]
	TIME [epoch: 9.77 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8276377433384505		[learning rate: 0.00025012]
	Learning Rate: 0.000250123
	LOSS [training: 4.8276377433384505 | validation: 5.346798123503118]
	TIME [epoch: 9.8 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801003899017293		[learning rate: 0.00024953]
	Learning Rate: 0.000249533
	LOSS [training: 4.801003899017293 | validation: 5.334478403397543]
	TIME [epoch: 9.77 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800113927440663		[learning rate: 0.00024894]
	Learning Rate: 0.000248945
	LOSS [training: 4.800113927440663 | validation: 5.329578953889643]
	TIME [epoch: 9.78 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.805024361941564		[learning rate: 0.00024836]
	Learning Rate: 0.000248357
	LOSS [training: 4.805024361941564 | validation: 5.355176683500865]
	TIME [epoch: 9.78 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.827194500622872		[learning rate: 0.00024777]
	Learning Rate: 0.000247771
	LOSS [training: 4.827194500622872 | validation: 5.349344817559397]
	TIME [epoch: 9.79 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.819470597039571		[learning rate: 0.00024719]
	Learning Rate: 0.000247187
	LOSS [training: 4.819470597039571 | validation: 5.348007864992505]
	TIME [epoch: 9.78 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.824572140132058		[learning rate: 0.0002466]
	Learning Rate: 0.000246604
	LOSS [training: 4.824572140132058 | validation: 5.359197344964072]
	TIME [epoch: 9.77 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.825853231663571		[learning rate: 0.00024602]
	Learning Rate: 0.000246022
	LOSS [training: 4.825853231663571 | validation: 5.355679802884627]
	TIME [epoch: 9.77 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812969842828883		[learning rate: 0.00024544]
	Learning Rate: 0.000245442
	LOSS [training: 4.812969842828883 | validation: 5.361203392448983]
	TIME [epoch: 9.79 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.839936771252497		[learning rate: 0.00024486]
	Learning Rate: 0.000244863
	LOSS [training: 4.839936771252497 | validation: 5.347241315169814]
	TIME [epoch: 9.77 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812599677213794		[learning rate: 0.00024429]
	Learning Rate: 0.000244285
	LOSS [training: 4.812599677213794 | validation: 5.362240210525142]
	TIME [epoch: 9.79 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.836890248309323		[learning rate: 0.00024371]
	Learning Rate: 0.000243709
	LOSS [training: 4.836890248309323 | validation: 5.366983262925665]
	TIME [epoch: 9.81 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.837794773975321		[learning rate: 0.00024313]
	Learning Rate: 0.000243134
	LOSS [training: 4.837794773975321 | validation: 5.368131487121641]
	TIME [epoch: 9.77 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8378674051360075		[learning rate: 0.00024256]
	Learning Rate: 0.000242561
	LOSS [training: 4.8378674051360075 | validation: 5.3517228868232865]
	TIME [epoch: 9.78 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806489563581496		[learning rate: 0.00024199]
	Learning Rate: 0.000241989
	LOSS [training: 4.806489563581496 | validation: 5.363160653763461]
	TIME [epoch: 9.78 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.819871371571906		[learning rate: 0.00024142]
	Learning Rate: 0.000241418
	LOSS [training: 4.819871371571906 | validation: 5.358458288681586]
	TIME [epoch: 9.79 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820469058021722		[learning rate: 0.00024085]
	Learning Rate: 0.000240848
	LOSS [training: 4.820469058021722 | validation: 5.348796507555762]
	TIME [epoch: 9.77 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.814745597078111		[learning rate: 0.00024028]
	Learning Rate: 0.00024028
	LOSS [training: 4.814745597078111 | validation: 5.367348099749092]
	TIME [epoch: 9.78 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.826416709643828		[learning rate: 0.00023971]
	Learning Rate: 0.000239713
	LOSS [training: 4.826416709643828 | validation: 5.344940561174657]
	TIME [epoch: 9.76 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801703775047099		[learning rate: 0.00023915]
	Learning Rate: 0.000239148
	LOSS [training: 4.801703775047099 | validation: 5.328128039241813]
	TIME [epoch: 9.79 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801303844028831		[learning rate: 0.00023858]
	Learning Rate: 0.000238584
	LOSS [training: 4.801303844028831 | validation: 5.337069592442126]
	TIME [epoch: 9.77 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795605580732034		[learning rate: 0.00023802]
	Learning Rate: 0.000238021
	LOSS [training: 4.795605580732034 | validation: 5.336368270287485]
	TIME [epoch: 9.78 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.809499108311351		[learning rate: 0.00023746]
	Learning Rate: 0.00023746
	LOSS [training: 4.809499108311351 | validation: 5.316148022074952]
	TIME [epoch: 9.81 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78407557269623		[learning rate: 0.0002369]
	Learning Rate: 0.000236899
	LOSS [training: 4.78407557269623 | validation: 5.313484812312538]
	TIME [epoch: 9.78 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777438067529699		[learning rate: 0.00023634]
	Learning Rate: 0.000236341
	LOSS [training: 4.777438067529699 | validation: 5.3317440692682405]
	TIME [epoch: 9.78 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779293127560366		[learning rate: 0.00023578]
	Learning Rate: 0.000235783
	LOSS [training: 4.779293127560366 | validation: 5.331913050982195]
	TIME [epoch: 9.77 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779797386253606		[learning rate: 0.00023523]
	Learning Rate: 0.000235227
	LOSS [training: 4.779797386253606 | validation: 5.401640799762041]
	TIME [epoch: 9.8 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792430376199584		[learning rate: 0.00023467]
	Learning Rate: 0.000234672
	LOSS [training: 4.792430376199584 | validation: 5.320399116493329]
	TIME [epoch: 9.78 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778585109627345		[learning rate: 0.00023412]
	Learning Rate: 0.000234119
	LOSS [training: 4.778585109627345 | validation: 5.320289596032283]
	TIME [epoch: 9.79 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7884611224100535		[learning rate: 0.00023357]
	Learning Rate: 0.000233566
	LOSS [training: 4.7884611224100535 | validation: 5.385159167848372]
	TIME [epoch: 9.79 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807962179872386		[learning rate: 0.00023302]
	Learning Rate: 0.000233015
	LOSS [training: 4.807962179872386 | validation: 5.3387570936888755]
	TIME [epoch: 9.79 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798894253743264		[learning rate: 0.00023247]
	Learning Rate: 0.000232466
	LOSS [training: 4.798894253743264 | validation: 5.316283518261141]
	TIME [epoch: 9.78 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7840713942626865		[learning rate: 0.00023192]
	Learning Rate: 0.000231917
	LOSS [training: 4.7840713942626865 | validation: 5.31826618515817]
	TIME [epoch: 9.77 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795975110311487		[learning rate: 0.00023137]
	Learning Rate: 0.00023137
	LOSS [training: 4.795975110311487 | validation: 5.354343371230607]
	TIME [epoch: 9.79 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804658803602663		[learning rate: 0.00023082]
	Learning Rate: 0.000230825
	LOSS [training: 4.804658803602663 | validation: 5.329066032230453]
	TIME [epoch: 9.78 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780031661289215		[learning rate: 0.00023028]
	Learning Rate: 0.00023028
	LOSS [training: 4.780031661289215 | validation: 5.302563768992082]
	TIME [epoch: 9.77 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782916747201443		[learning rate: 0.00022974]
	Learning Rate: 0.000229737
	LOSS [training: 4.782916747201443 | validation: 5.318613294620656]
	TIME [epoch: 9.77 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786033812860554		[learning rate: 0.00022919]
	Learning Rate: 0.000229195
	LOSS [training: 4.786033812860554 | validation: 5.326823986886914]
	TIME [epoch: 9.8 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775894696456618		[learning rate: 0.00022865]
	Learning Rate: 0.000228654
	LOSS [training: 4.775894696456618 | validation: 5.388147670817944]
	TIME [epoch: 9.77 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.819722195028069		[learning rate: 0.00022811]
	Learning Rate: 0.000228115
	LOSS [training: 4.819722195028069 | validation: 5.324347775001514]
	TIME [epoch: 9.77 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793724084435941		[learning rate: 0.00022758]
	Learning Rate: 0.000227577
	LOSS [training: 4.793724084435941 | validation: 5.323721824949508]
	TIME [epoch: 9.79 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796179605991982		[learning rate: 0.00022704]
	Learning Rate: 0.00022704
	LOSS [training: 4.796179605991982 | validation: 5.313531221771084]
	TIME [epoch: 9.79 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779957416573632		[learning rate: 0.0002265]
	Learning Rate: 0.000226505
	LOSS [training: 4.779957416573632 | validation: 5.315290484958083]
	TIME [epoch: 9.79 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775762597947468		[learning rate: 0.00022597]
	Learning Rate: 0.00022597
	LOSS [training: 4.775762597947468 | validation: 5.331744990130014]
	TIME [epoch: 9.77 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785563388631674		[learning rate: 0.00022544]
	Learning Rate: 0.000225437
	LOSS [training: 4.785563388631674 | validation: 5.345133419950212]
	TIME [epoch: 9.79 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78081080968452		[learning rate: 0.00022491]
	Learning Rate: 0.000224905
	LOSS [training: 4.78081080968452 | validation: 5.3241813757989735]
	TIME [epoch: 9.78 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771684331495253		[learning rate: 0.00022437]
	Learning Rate: 0.000224375
	LOSS [training: 4.771684331495253 | validation: 5.365158967782963]
	TIME [epoch: 9.77 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787510049663779		[learning rate: 0.00022385]
	Learning Rate: 0.000223846
	LOSS [training: 4.787510049663779 | validation: 5.344685033426417]
	TIME [epoch: 9.78 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7725581360197085		[learning rate: 0.00022332]
	Learning Rate: 0.000223318
	LOSS [training: 4.7725581360197085 | validation: 5.329367793872112]
	TIME [epoch: 9.78 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8087266319752455		[learning rate: 0.00022279]
	Learning Rate: 0.000222791
	LOSS [training: 4.8087266319752455 | validation: 5.386698647686878]
	TIME [epoch: 9.77 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782448635822536		[learning rate: 0.00022227]
	Learning Rate: 0.000222265
	LOSS [training: 4.782448635822536 | validation: 5.322453328825394]
	TIME [epoch: 9.78 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766626773719068		[learning rate: 0.00022174]
	Learning Rate: 0.000221741
	LOSS [training: 4.766626773719068 | validation: 5.320675043511618]
	TIME [epoch: 9.77 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802138259644		[learning rate: 0.00022122]
	Learning Rate: 0.000221218
	LOSS [training: 4.802138259644 | validation: 5.3251447876372335]
	TIME [epoch: 9.8 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778734777554537		[learning rate: 0.0002207]
	Learning Rate: 0.000220696
	LOSS [training: 4.778734777554537 | validation: 5.339948909996679]
	TIME [epoch: 9.79 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772853114311218		[learning rate: 0.00022018]
	Learning Rate: 0.000220176
	LOSS [training: 4.772853114311218 | validation: 5.3226402062344995]
	TIME [epoch: 9.79 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769953884234794		[learning rate: 0.00021966]
	Learning Rate: 0.000219656
	LOSS [training: 4.769953884234794 | validation: 5.311045216425947]
	TIME [epoch: 9.78 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770344878287664		[learning rate: 0.00021914]
	Learning Rate: 0.000219138
	LOSS [training: 4.770344878287664 | validation: 5.325936212228903]
	TIME [epoch: 9.78 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.759209358891921		[learning rate: 0.00021862]
	Learning Rate: 0.000218621
	LOSS [training: 4.759209358891921 | validation: 5.325382443611003]
	TIME [epoch: 9.77 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760747457101063		[learning rate: 0.00021811]
	Learning Rate: 0.000218106
	LOSS [training: 4.760747457101063 | validation: 5.322020694718146]
	TIME [epoch: 9.77 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765773551357252		[learning rate: 0.00021759]
	Learning Rate: 0.000217591
	LOSS [training: 4.765773551357252 | validation: 5.307464501698629]
	TIME [epoch: 9.79 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779242402036691		[learning rate: 0.00021708]
	Learning Rate: 0.000217078
	LOSS [training: 4.779242402036691 | validation: 5.326537268194896]
	TIME [epoch: 9.76 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788245384228435		[learning rate: 0.00021657]
	Learning Rate: 0.000216566
	LOSS [training: 4.788245384228435 | validation: 5.33329438574272]
	TIME [epoch: 9.77 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775187722769355		[learning rate: 0.00021605]
	Learning Rate: 0.000216055
	LOSS [training: 4.775187722769355 | validation: 5.328586477641977]
	TIME [epoch: 9.76 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796426119952306		[learning rate: 0.00021555]
	Learning Rate: 0.000215545
	LOSS [training: 4.796426119952306 | validation: 5.332759476278027]
	TIME [epoch: 9.81 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777359568363918		[learning rate: 0.00021504]
	Learning Rate: 0.000215037
	LOSS [training: 4.777359568363918 | validation: 5.3181864050160605]
	TIME [epoch: 9.77 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788332941398104		[learning rate: 0.00021453]
	Learning Rate: 0.00021453
	LOSS [training: 4.788332941398104 | validation: 5.323337305486193]
	TIME [epoch: 9.78 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771738981203111		[learning rate: 0.00021402]
	Learning Rate: 0.000214024
	LOSS [training: 4.771738981203111 | validation: 5.309887973213385]
	TIME [epoch: 9.8 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770771804108695		[learning rate: 0.00021352]
	Learning Rate: 0.000213519
	LOSS [training: 4.770771804108695 | validation: 5.318705096134813]
	TIME [epoch: 9.8 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.816512508378869		[learning rate: 0.00021302]
	Learning Rate: 0.000213015
	LOSS [training: 4.816512508378869 | validation: 5.386521082610513]
	TIME [epoch: 9.8 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.81780985538934		[learning rate: 0.00021251]
	Learning Rate: 0.000212513
	LOSS [training: 4.81780985538934 | validation: 5.378551540482717]
	TIME [epoch: 9.77 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801830826107658		[learning rate: 0.00021201]
	Learning Rate: 0.000212011
	LOSS [training: 4.801830826107658 | validation: 5.341444295075207]
	TIME [epoch: 9.8 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788346134029726		[learning rate: 0.00021151]
	Learning Rate: 0.000211511
	LOSS [training: 4.788346134029726 | validation: 5.33084025760443]
	TIME [epoch: 9.79 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801726339184135		[learning rate: 0.00021101]
	Learning Rate: 0.000211012
	LOSS [training: 4.801726339184135 | validation: 5.3333059091074055]
	TIME [epoch: 9.78 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786275074873986		[learning rate: 0.00021051]
	Learning Rate: 0.000210514
	LOSS [training: 4.786275074873986 | validation: 5.329630890859521]
	TIME [epoch: 9.78 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799963537985103		[learning rate: 0.00021002]
	Learning Rate: 0.000210018
	LOSS [training: 4.799963537985103 | validation: 5.323921282327235]
	TIME [epoch: 9.79 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785683684877424		[learning rate: 0.00020952]
	Learning Rate: 0.000209523
	LOSS [training: 4.785683684877424 | validation: 5.332247424174884]
	TIME [epoch: 9.78 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795784138764342		[learning rate: 0.00020903]
	Learning Rate: 0.000209028
	LOSS [training: 4.795784138764342 | validation: 5.331488589528434]
	TIME [epoch: 9.77 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797082613359372		[learning rate: 0.00020854]
	Learning Rate: 0.000208535
	LOSS [training: 4.797082613359372 | validation: 5.354647068782005]
	TIME [epoch: 9.78 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792200341423798		[learning rate: 0.00020804]
	Learning Rate: 0.000208043
	LOSS [training: 4.792200341423798 | validation: 5.381163387281515]
	TIME [epoch: 9.8 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802505800582255		[learning rate: 0.00020755]
	Learning Rate: 0.000207553
	LOSS [training: 4.802505800582255 | validation: 5.339070800103149]
	TIME [epoch: 9.8 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808537881585619		[learning rate: 0.00020706]
	Learning Rate: 0.000207063
	LOSS [training: 4.808537881585619 | validation: 5.3182317338740654]
	TIME [epoch: 9.76 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777880493642387		[learning rate: 0.00020657]
	Learning Rate: 0.000206575
	LOSS [training: 4.777880493642387 | validation: 5.333097956048307]
	TIME [epoch: 9.78 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7900654143761425		[learning rate: 0.00020609]
	Learning Rate: 0.000206087
	LOSS [training: 4.7900654143761425 | validation: 5.3384668029821185]
	TIME [epoch: 9.8 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783780369765957		[learning rate: 0.0002056]
	Learning Rate: 0.000205601
	LOSS [training: 4.783780369765957 | validation: 5.333380300094645]
	TIME [epoch: 9.79 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782732553642282		[learning rate: 0.00020512]
	Learning Rate: 0.000205116
	LOSS [training: 4.782732553642282 | validation: 5.324363038516813]
	TIME [epoch: 9.79 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785694329400667		[learning rate: 0.00020463]
	Learning Rate: 0.000204632
	LOSS [training: 4.785694329400667 | validation: 5.318451186848307]
	TIME [epoch: 9.8 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784902544109013		[learning rate: 0.00020415]
	Learning Rate: 0.00020415
	LOSS [training: 4.784902544109013 | validation: 5.3330238999918755]
	TIME [epoch: 9.8 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796513993380435		[learning rate: 0.00020367]
	Learning Rate: 0.000203668
	LOSS [training: 4.796513993380435 | validation: 5.351961049136628]
	TIME [epoch: 9.77 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789361494132118		[learning rate: 0.00020319]
	Learning Rate: 0.000203188
	LOSS [training: 4.789361494132118 | validation: 5.307421209413292]
	TIME [epoch: 9.81 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777489463963238		[learning rate: 0.00020271]
	Learning Rate: 0.000202708
	LOSS [training: 4.777489463963238 | validation: 5.337723505836668]
	TIME [epoch: 9.8 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799274602836144		[learning rate: 0.00020223]
	Learning Rate: 0.00020223
	LOSS [training: 4.799274602836144 | validation: 5.348123859564604]
	TIME [epoch: 9.79 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789771417981922		[learning rate: 0.00020175]
	Learning Rate: 0.000201753
	LOSS [training: 4.789771417981922 | validation: 5.3293233130640205]
	TIME [epoch: 9.77 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7664315036980565		[learning rate: 0.00020128]
	Learning Rate: 0.000201277
	LOSS [training: 4.7664315036980565 | validation: 5.327778825742537]
	TIME [epoch: 9.79 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779837315405368		[learning rate: 0.0002008]
	Learning Rate: 0.000200803
	LOSS [training: 4.779837315405368 | validation: 5.3387232807261045]
	TIME [epoch: 9.78 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784337924294507		[learning rate: 0.00020033]
	Learning Rate: 0.000200329
	LOSS [training: 4.784337924294507 | validation: 5.308251521656723]
	TIME [epoch: 9.78 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78200428951706		[learning rate: 0.00019986]
	Learning Rate: 0.000199856
	LOSS [training: 4.78200428951706 | validation: 5.319413351526552]
	TIME [epoch: 9.78 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786962449521797		[learning rate: 0.00019938]
	Learning Rate: 0.000199385
	LOSS [training: 4.786962449521797 | validation: 5.32650181529836]
	TIME [epoch: 9.79 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8002291073671035		[learning rate: 0.00019891]
	Learning Rate: 0.000198915
	LOSS [training: 4.8002291073671035 | validation: 5.319395213709908]
	TIME [epoch: 9.78 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7793688083855645		[learning rate: 0.00019845]
	Learning Rate: 0.000198445
	LOSS [training: 4.7793688083855645 | validation: 5.338140414266945]
	TIME [epoch: 9.78 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786278045766461		[learning rate: 0.00019798]
	Learning Rate: 0.000197977
	LOSS [training: 4.786278045766461 | validation: 5.33273760251628]
	TIME [epoch: 9.77 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7794511645395445		[learning rate: 0.00019751]
	Learning Rate: 0.00019751
	LOSS [training: 4.7794511645395445 | validation: 5.348182364271504]
	TIME [epoch: 9.81 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784811136202704		[learning rate: 0.00019704]
	Learning Rate: 0.000197044
	LOSS [training: 4.784811136202704 | validation: 5.323796343248664]
	TIME [epoch: 9.77 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776648558682761		[learning rate: 0.00019658]
	Learning Rate: 0.00019658
	LOSS [training: 4.776648558682761 | validation: 5.321603634462856]
	TIME [epoch: 9.77 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7793697416039125		[learning rate: 0.00019612]
	Learning Rate: 0.000196116
	LOSS [training: 4.7793697416039125 | validation: 5.357588252550365]
	TIME [epoch: 9.8 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7890945551476465		[learning rate: 0.00019565]
	Learning Rate: 0.000195653
	LOSS [training: 4.7890945551476465 | validation: 5.3384969493266246]
	TIME [epoch: 9.8 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79256452380393		[learning rate: 0.00019519]
	Learning Rate: 0.000195192
	LOSS [training: 4.79256452380393 | validation: 5.368376169045564]
	TIME [epoch: 9.78 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792815152582348		[learning rate: 0.00019473]
	Learning Rate: 0.000194731
	LOSS [training: 4.792815152582348 | validation: 5.325820895118651]
	TIME [epoch: 9.78 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781328300964073		[learning rate: 0.00019427]
	Learning Rate: 0.000194272
	LOSS [training: 4.781328300964073 | validation: 5.318307571986207]
	TIME [epoch: 9.79 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.77738467190611		[learning rate: 0.00019381]
	Learning Rate: 0.000193814
	LOSS [training: 4.77738467190611 | validation: 5.3112229876516075]
	TIME [epoch: 9.79 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776970089354597		[learning rate: 0.00019336]
	Learning Rate: 0.000193357
	LOSS [training: 4.776970089354597 | validation: 5.331331455253362]
	TIME [epoch: 9.77 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7816855847953645		[learning rate: 0.0001929]
	Learning Rate: 0.0001929
	LOSS [training: 4.7816855847953645 | validation: 5.308074310011602]
	TIME [epoch: 9.77 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782015878992821		[learning rate: 0.00019245]
	Learning Rate: 0.000192445
	LOSS [training: 4.782015878992821 | validation: 5.3201123575872575]
	TIME [epoch: 9.79 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771079161212344		[learning rate: 0.00019199]
	Learning Rate: 0.000191992
	LOSS [training: 4.771079161212344 | validation: 5.304398768130214]
	TIME [epoch: 9.78 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7715376429023655		[learning rate: 0.00019154]
	Learning Rate: 0.000191539
	LOSS [training: 4.7715376429023655 | validation: 5.307488897666186]
	TIME [epoch: 9.78 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775353825528065		[learning rate: 0.00019109]
	Learning Rate: 0.000191087
	LOSS [training: 4.775353825528065 | validation: 5.341112380552698]
	TIME [epoch: 9.79 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7958639611432945		[learning rate: 0.00019064]
	Learning Rate: 0.000190636
	LOSS [training: 4.7958639611432945 | validation: 5.379362374113693]
	TIME [epoch: 9.79 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797640470077786		[learning rate: 0.00019019]
	Learning Rate: 0.000190186
	LOSS [training: 4.797640470077786 | validation: 5.332465953626707]
	TIME [epoch: 9.81 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767430097636086		[learning rate: 0.00018974]
	Learning Rate: 0.000189738
	LOSS [training: 4.767430097636086 | validation: 5.31765593371449]
	TIME [epoch: 9.78 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771362389411806		[learning rate: 0.00018929]
	Learning Rate: 0.00018929
	LOSS [training: 4.771362389411806 | validation: 5.313424520446579]
	TIME [epoch: 9.81 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7782431222446045		[learning rate: 0.00018884]
	Learning Rate: 0.000188844
	LOSS [training: 4.7782431222446045 | validation: 5.32123118586577]
	TIME [epoch: 9.78 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783590535570067		[learning rate: 0.0001884]
	Learning Rate: 0.000188398
	LOSS [training: 4.783590535570067 | validation: 5.313323631424007]
	TIME [epoch: 9.77 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777256583019792		[learning rate: 0.00018795]
	Learning Rate: 0.000187954
	LOSS [training: 4.777256583019792 | validation: 5.3117463738565345]
	TIME [epoch: 9.78 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79170786815436		[learning rate: 0.00018751]
	Learning Rate: 0.000187511
	LOSS [training: 4.79170786815436 | validation: 5.3398419897350085]
	TIME [epoch: 9.8 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778705618247207		[learning rate: 0.00018707]
	Learning Rate: 0.000187068
	LOSS [training: 4.778705618247207 | validation: 5.336661815563039]
	TIME [epoch: 9.8 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788193674545462		[learning rate: 0.00018663]
	Learning Rate: 0.000186627
	LOSS [training: 4.788193674545462 | validation: 5.376240509773509]
	TIME [epoch: 9.79 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.827633171854967		[learning rate: 0.00018619]
	Learning Rate: 0.000186187
	LOSS [training: 4.827633171854967 | validation: 5.377448831446365]
	TIME [epoch: 9.78 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7828164774225		[learning rate: 0.00018575]
	Learning Rate: 0.000185748
	LOSS [training: 4.7828164774225 | validation: 5.316083161763198]
	TIME [epoch: 9.8 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771258176323784		[learning rate: 0.00018531]
	Learning Rate: 0.000185309
	LOSS [training: 4.771258176323784 | validation: 5.335136826259872]
	TIME [epoch: 9.78 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7739021712459095		[learning rate: 0.00018487]
	Learning Rate: 0.000184872
	LOSS [training: 4.7739021712459095 | validation: 5.307405111787135]
	TIME [epoch: 9.8 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780760102900312		[learning rate: 0.00018444]
	Learning Rate: 0.000184436
	LOSS [training: 4.780760102900312 | validation: 5.30678457020276]
	TIME [epoch: 9.81 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7778484272054875		[learning rate: 0.000184]
	Learning Rate: 0.000184001
	LOSS [training: 4.7778484272054875 | validation: 5.333119341393812]
	TIME [epoch: 9.79 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807757843830717		[learning rate: 0.00018357]
	Learning Rate: 0.000183567
	LOSS [training: 4.807757843830717 | validation: 5.3490049477048185]
	TIME [epoch: 9.77 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795302117517208		[learning rate: 0.00018313]
	Learning Rate: 0.000183134
	LOSS [training: 4.795302117517208 | validation: 5.34480571166115]
	TIME [epoch: 9.78 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789858410346364		[learning rate: 0.0001827]
	Learning Rate: 0.000182702
	LOSS [training: 4.789858410346364 | validation: 5.342036419365488]
	TIME [epoch: 9.79 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.811941279163354		[learning rate: 0.00018227]
	Learning Rate: 0.000182271
	LOSS [training: 4.811941279163354 | validation: 5.378764285774266]
	TIME [epoch: 9.79 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7961495516872805		[learning rate: 0.00018184]
	Learning Rate: 0.000181841
	LOSS [training: 4.7961495516872805 | validation: 5.3180179862505135]
	TIME [epoch: 9.77 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774681152188056		[learning rate: 0.00018141]
	Learning Rate: 0.000181412
	LOSS [training: 4.774681152188056 | validation: 5.3152576247026015]
	TIME [epoch: 9.78 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771993067100927		[learning rate: 0.00018098]
	Learning Rate: 0.000180984
	LOSS [training: 4.771993067100927 | validation: 5.320028461264835]
	TIME [epoch: 9.82 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769836523383086		[learning rate: 0.00018056]
	Learning Rate: 0.000180557
	LOSS [training: 4.769836523383086 | validation: 5.332142843739055]
	TIME [epoch: 9.79 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7767361697368615		[learning rate: 0.00018013]
	Learning Rate: 0.000180132
	LOSS [training: 4.7767361697368615 | validation: 5.317090627304905]
	TIME [epoch: 9.78 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779284593449165		[learning rate: 0.00017971]
	Learning Rate: 0.000179707
	LOSS [training: 4.779284593449165 | validation: 5.345629715736429]
	TIME [epoch: 9.81 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785428539719225		[learning rate: 0.00017928]
	Learning Rate: 0.000179283
	LOSS [training: 4.785428539719225 | validation: 5.325013005405758]
	TIME [epoch: 9.8 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.77226415765496		[learning rate: 0.00017886]
	Learning Rate: 0.00017886
	LOSS [training: 4.77226415765496 | validation: 5.331132564477299]
	TIME [epoch: 9.77 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767628330688007		[learning rate: 0.00017844]
	Learning Rate: 0.000178438
	LOSS [training: 4.767628330688007 | validation: 5.306493080959902]
	TIME [epoch: 9.8 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760250084116594		[learning rate: 0.00017802]
	Learning Rate: 0.000178017
	LOSS [training: 4.760250084116594 | validation: 5.314749670727385]
	TIME [epoch: 9.8 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.757929208538374		[learning rate: 0.0001776]
	Learning Rate: 0.000177597
	LOSS [training: 4.757929208538374 | validation: 5.318139512338156]
	TIME [epoch: 9.78 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772573896016655		[learning rate: 0.00017718]
	Learning Rate: 0.000177178
	LOSS [training: 4.772573896016655 | validation: 5.336519696486149]
	TIME [epoch: 9.77 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.757953814347738		[learning rate: 0.00017676]
	Learning Rate: 0.00017676
	LOSS [training: 4.757953814347738 | validation: 5.30730066184932]
	TIME [epoch: 9.77 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758718160479839		[learning rate: 0.00017634]
	Learning Rate: 0.000176343
	LOSS [training: 4.758718160479839 | validation: 5.2969698274002734]
	TIME [epoch: 9.8 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.762383617029336		[learning rate: 0.00017593]
	Learning Rate: 0.000175927
	LOSS [training: 4.762383617029336 | validation: 5.316279776589091]
	TIME [epoch: 9.77 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771790163877697		[learning rate: 0.00017551]
	Learning Rate: 0.000175512
	LOSS [training: 4.771790163877697 | validation: 5.316742979869609]
	TIME [epoch: 9.81 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76757789565225		[learning rate: 0.0001751]
	Learning Rate: 0.000175098
	LOSS [training: 4.76757789565225 | validation: 5.309533061862521]
	TIME [epoch: 9.8 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7671525635064285		[learning rate: 0.00017469]
	Learning Rate: 0.000174685
	LOSS [training: 4.7671525635064285 | validation: 5.315172079385056]
	TIME [epoch: 9.79 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775395835610118		[learning rate: 0.00017427]
	Learning Rate: 0.000174273
	LOSS [training: 4.775395835610118 | validation: 5.31649966914786]
	TIME [epoch: 9.77 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767983735964293		[learning rate: 0.00017386]
	Learning Rate: 0.000173862
	LOSS [training: 4.767983735964293 | validation: 5.307078491500708]
	TIME [epoch: 9.77 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766846759573332		[learning rate: 0.00017345]
	Learning Rate: 0.000173452
	LOSS [training: 4.766846759573332 | validation: 5.326989328014616]
	TIME [epoch: 9.8 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778009100737013		[learning rate: 0.00017304]
	Learning Rate: 0.000173043
	LOSS [training: 4.778009100737013 | validation: 5.328901325436963]
	TIME [epoch: 9.78 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80097726485811		[learning rate: 0.00017263]
	Learning Rate: 0.000172635
	LOSS [training: 4.80097726485811 | validation: 5.361440390433199]
	TIME [epoch: 9.79 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795288036702077		[learning rate: 0.00017223]
	Learning Rate: 0.000172228
	LOSS [training: 4.795288036702077 | validation: 5.33490806184921]
	TIME [epoch: 9.77 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798320898299804		[learning rate: 0.00017182]
	Learning Rate: 0.000171821
	LOSS [training: 4.798320898299804 | validation: 5.312536209236119]
	TIME [epoch: 9.79 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777230569151132		[learning rate: 0.00017142]
	Learning Rate: 0.000171416
	LOSS [training: 4.777230569151132 | validation: 5.337917766273147]
	TIME [epoch: 9.78 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768371212471521		[learning rate: 0.00017101]
	Learning Rate: 0.000171012
	LOSS [training: 4.768371212471521 | validation: 5.323064937592036]
	TIME [epoch: 9.78 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773860305100557		[learning rate: 0.00017061]
	Learning Rate: 0.000170608
	LOSS [training: 4.773860305100557 | validation: 5.3232773055360125]
	TIME [epoch: 9.78 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774620689197863		[learning rate: 0.00017021]
	Learning Rate: 0.000170206
	LOSS [training: 4.774620689197863 | validation: 5.316607603653079]
	TIME [epoch: 9.78 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79085434840402		[learning rate: 0.0001698]
	Learning Rate: 0.000169804
	LOSS [training: 4.79085434840402 | validation: 5.315601967942687]
	TIME [epoch: 9.77 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.767216167356702		[learning rate: 0.0001694]
	Learning Rate: 0.000169404
	LOSS [training: 4.767216167356702 | validation: 5.301359442690625]
	TIME [epoch: 9.77 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7621073293437535		[learning rate: 0.000169]
	Learning Rate: 0.000169004
	LOSS [training: 4.7621073293437535 | validation: 5.321868063471084]
	TIME [epoch: 9.79 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796363720695367		[learning rate: 0.00016861]
	Learning Rate: 0.000168606
	LOSS [training: 4.796363720695367 | validation: 5.3506395232760315]
	TIME [epoch: 9.76 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.821991132460352		[learning rate: 0.00016821]
	Learning Rate: 0.000168208
	LOSS [training: 4.821991132460352 | validation: 5.328567074769558]
	TIME [epoch: 9.77 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7876097544304415		[learning rate: 0.00016781]
	Learning Rate: 0.000167811
	LOSS [training: 4.7876097544304415 | validation: 5.341495965113317]
	TIME [epoch: 9.78 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790370648364817		[learning rate: 0.00016742]
	Learning Rate: 0.000167415
	LOSS [training: 4.790370648364817 | validation: 5.313894806980215]
	TIME [epoch: 9.81 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774773666607494		[learning rate: 0.00016702]
	Learning Rate: 0.00016702
	LOSS [training: 4.774773666607494 | validation: 5.314392056185477]
	TIME [epoch: 9.78 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763822253754234		[learning rate: 0.00016663]
	Learning Rate: 0.000166626
	LOSS [training: 4.763822253754234 | validation: 5.317838430739722]
	TIME [epoch: 9.77 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.760145746950791		[learning rate: 0.00016623]
	Learning Rate: 0.000166233
	LOSS [training: 4.760145746950791 | validation: 5.300804638165119]
	TIME [epoch: 9.78 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766031901121051		[learning rate: 0.00016584]
	Learning Rate: 0.000165841
	LOSS [training: 4.766031901121051 | validation: 5.3255218708958605]
	TIME [epoch: 9.77 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772206224097678		[learning rate: 0.00016545]
	Learning Rate: 0.00016545
	LOSS [training: 4.772206224097678 | validation: 5.350563086757594]
	TIME [epoch: 9.78 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768258979315287		[learning rate: 0.00016506]
	Learning Rate: 0.00016506
	LOSS [training: 4.768258979315287 | validation: 5.30562417725626]
	TIME [epoch: 9.77 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774903091487852		[learning rate: 0.00016467]
	Learning Rate: 0.00016467
	LOSS [training: 4.774903091487852 | validation: 5.314937038776251]
	TIME [epoch: 9.79 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770451956750593		[learning rate: 0.00016428]
	Learning Rate: 0.000164282
	LOSS [training: 4.770451956750593 | validation: 5.308302537416272]
	TIME [epoch: 9.77 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770897368223535		[learning rate: 0.00016389]
	Learning Rate: 0.000163894
	LOSS [training: 4.770897368223535 | validation: 5.320420083558842]
	TIME [epoch: 9.78 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.801448695760451		[learning rate: 0.00016351]
	Learning Rate: 0.000163508
	LOSS [training: 4.801448695760451 | validation: 5.3493631867250295]
	TIME [epoch: 9.78 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813662284535735		[learning rate: 0.00016312]
	Learning Rate: 0.000163122
	LOSS [training: 4.813662284535735 | validation: 5.356774691114319]
	TIME [epoch: 9.8 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8118886790687		[learning rate: 0.00016274]
	Learning Rate: 0.000162737
	LOSS [training: 4.8118886790687 | validation: 5.3345216192524205]
	TIME [epoch: 9.77 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.832977510234627		[learning rate: 0.00016235]
	Learning Rate: 0.000162353
	LOSS [training: 4.832977510234627 | validation: 5.3486181821739756]
	TIME [epoch: 9.77 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797234122516359		[learning rate: 0.00016197]
	Learning Rate: 0.00016197
	LOSS [training: 4.797234122516359 | validation: 5.32223932474089]
	TIME [epoch: 9.77 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795857595580253		[learning rate: 0.00016159]
	Learning Rate: 0.000161588
	LOSS [training: 4.795857595580253 | validation: 5.316988376551857]
	TIME [epoch: 9.79 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790898943875627		[learning rate: 0.00016121]
	Learning Rate: 0.000161207
	LOSS [training: 4.790898943875627 | validation: 5.349703484605492]
	TIME [epoch: 9.78 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.798773066661333		[learning rate: 0.00016083]
	Learning Rate: 0.000160827
	LOSS [training: 4.798773066661333 | validation: 5.338479838231462]
	TIME [epoch: 9.78 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784766408402474		[learning rate: 0.00016045]
	Learning Rate: 0.000160448
	LOSS [training: 4.784766408402474 | validation: 5.341452715192229]
	TIME [epoch: 9.79 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796515613542108		[learning rate: 0.00016007]
	Learning Rate: 0.000160069
	LOSS [training: 4.796515613542108 | validation: 5.347580731075491]
	TIME [epoch: 9.79 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.810437453082387		[learning rate: 0.00015969]
	Learning Rate: 0.000159692
	LOSS [training: 4.810437453082387 | validation: 5.334380149428417]
	TIME [epoch: 9.77 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794383071092735		[learning rate: 0.00015931]
	Learning Rate: 0.000159315
	LOSS [training: 4.794383071092735 | validation: 5.34634876992414]
	TIME [epoch: 9.78 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790383649729613		[learning rate: 0.00015894]
	Learning Rate: 0.000158939
	LOSS [training: 4.790383649729613 | validation: 5.324626948611991]
	TIME [epoch: 9.79 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792946525497909		[learning rate: 0.00015856]
	Learning Rate: 0.000158564
	LOSS [training: 4.792946525497909 | validation: 5.327297561777223]
	TIME [epoch: 9.77 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80343284076443		[learning rate: 0.00015819]
	Learning Rate: 0.00015819
	LOSS [training: 4.80343284076443 | validation: 5.3378416906568775]
	TIME [epoch: 9.77 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812477443459948		[learning rate: 0.00015782]
	Learning Rate: 0.000157817
	LOSS [training: 4.812477443459948 | validation: 5.342192656582535]
	TIME [epoch: 9.78 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.828534111500859		[learning rate: 0.00015744]
	Learning Rate: 0.000157445
	LOSS [training: 4.828534111500859 | validation: 5.361358057441111]
	TIME [epoch: 9.79 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8393359319746665		[learning rate: 0.00015707]
	Learning Rate: 0.000157073
	LOSS [training: 4.8393359319746665 | validation: 5.410210791105053]
	TIME [epoch: 9.78 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.820923990823969		[learning rate: 0.0001567]
	Learning Rate: 0.000156703
	LOSS [training: 4.820923990823969 | validation: 5.358358656772238]
	TIME [epoch: 9.79 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8139197975323995		[learning rate: 0.00015633]
	Learning Rate: 0.000156333
	LOSS [training: 4.8139197975323995 | validation: 5.340600234755593]
	TIME [epoch: 9.8 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7860981377750935		[learning rate: 0.00015596]
	Learning Rate: 0.000155964
	LOSS [training: 4.7860981377750935 | validation: 5.312522925466241]
	TIME [epoch: 9.78 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785530878341254		[learning rate: 0.0001556]
	Learning Rate: 0.000155597
	LOSS [training: 4.785530878341254 | validation: 5.35371806890735]
	TIME [epoch: 9.77 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799821738140205		[learning rate: 0.00015523]
	Learning Rate: 0.00015523
	LOSS [training: 4.799821738140205 | validation: 5.329737639075646]
	TIME [epoch: 9.78 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802901628719278		[learning rate: 0.00015486]
	Learning Rate: 0.000154863
	LOSS [training: 4.802901628719278 | validation: 5.334238516388366]
	TIME [epoch: 9.8 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7897146918012705		[learning rate: 0.0001545]
	Learning Rate: 0.000154498
	LOSS [training: 4.7897146918012705 | validation: 5.335969548072815]
	TIME [epoch: 9.77 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787097931500544		[learning rate: 0.00015413]
	Learning Rate: 0.000154134
	LOSS [training: 4.787097931500544 | validation: 5.328838298594433]
	TIME [epoch: 9.79 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797243520270535		[learning rate: 0.00015377]
	Learning Rate: 0.00015377
	LOSS [training: 4.797243520270535 | validation: 5.332097642040246]
	TIME [epoch: 9.78 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78352967246639		[learning rate: 0.00015341]
	Learning Rate: 0.000153407
	LOSS [training: 4.78352967246639 | validation: 5.322914577112698]
	TIME [epoch: 9.8 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782701031808717		[learning rate: 0.00015305]
	Learning Rate: 0.000153045
	LOSS [training: 4.782701031808717 | validation: 5.358671764929656]
	TIME [epoch: 9.78 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.817746857403138		[learning rate: 0.00015268]
	Learning Rate: 0.000152684
	LOSS [training: 4.817746857403138 | validation: 5.370240310811807]
	TIME [epoch: 9.78 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813783387626127		[learning rate: 0.00015232]
	Learning Rate: 0.000152324
	LOSS [training: 4.813783387626127 | validation: 5.330013178829671]
	TIME [epoch: 9.8 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799404689730716		[learning rate: 0.00015196]
	Learning Rate: 0.000151965
	LOSS [training: 4.799404689730716 | validation: 5.334714847710261]
	TIME [epoch: 9.78 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80265017334861		[learning rate: 0.00015161]
	Learning Rate: 0.000151607
	LOSS [training: 4.80265017334861 | validation: 5.3316024847477586]
	TIME [epoch: 9.77 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807437772539414		[learning rate: 0.00015125]
	Learning Rate: 0.000151249
	LOSS [training: 4.807437772539414 | validation: 5.324745675447996]
	TIME [epoch: 9.77 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797198268036007		[learning rate: 0.00015089]
	Learning Rate: 0.000150892
	LOSS [training: 4.797198268036007 | validation: 5.339008333771278]
	TIME [epoch: 9.79 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797137594172293		[learning rate: 0.00015054]
	Learning Rate: 0.000150536
	LOSS [training: 4.797137594172293 | validation: 5.340002658905408]
	TIME [epoch: 9.78 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800928711149218		[learning rate: 0.00015018]
	Learning Rate: 0.000150181
	LOSS [training: 4.800928711149218 | validation: 5.329327172761343]
	TIME [epoch: 9.77 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797555579356823		[learning rate: 0.00014983]
	Learning Rate: 0.000149827
	LOSS [training: 4.797555579356823 | validation: 5.323597105489262]
	TIME [epoch: 9.78 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800599834981185		[learning rate: 0.00014947]
	Learning Rate: 0.000149473
	LOSS [training: 4.800599834981185 | validation: 5.3384118708419015]
	TIME [epoch: 9.79 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796711142471549		[learning rate: 0.00014912]
	Learning Rate: 0.000149121
	LOSS [training: 4.796711142471549 | validation: 5.326127181168115]
	TIME [epoch: 9.77 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.77240198770547		[learning rate: 0.00014877]
	Learning Rate: 0.000148769
	LOSS [training: 4.77240198770547 | validation: 5.312816515536147]
	TIME [epoch: 9.76 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782065496148925		[learning rate: 0.00014842]
	Learning Rate: 0.000148418
	LOSS [training: 4.782065496148925 | validation: 5.354781181160779]
	TIME [epoch: 9.78 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78614666009537		[learning rate: 0.00014807]
	Learning Rate: 0.000148068
	LOSS [training: 4.78614666009537 | validation: 5.33184933898419]
	TIME [epoch: 9.79 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776110585193159		[learning rate: 0.00014772]
	Learning Rate: 0.000147719
	LOSS [training: 4.776110585193159 | validation: 5.326574340889904]
	TIME [epoch: 9.77 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7856337400216375		[learning rate: 0.00014737]
	Learning Rate: 0.00014737
	LOSS [training: 4.7856337400216375 | validation: 5.338577234442801]
	TIME [epoch: 9.77 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777180618629672		[learning rate: 0.00014702]
	Learning Rate: 0.000147023
	LOSS [training: 4.777180618629672 | validation: 5.3270358524515125]
	TIME [epoch: 9.8 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772718194462859		[learning rate: 0.00014668]
	Learning Rate: 0.000146676
	LOSS [training: 4.772718194462859 | validation: 5.323743760146605]
	TIME [epoch: 9.78 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784553011444321		[learning rate: 0.00014633]
	Learning Rate: 0.00014633
	LOSS [training: 4.784553011444321 | validation: 5.3258903812351015]
	TIME [epoch: 9.78 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791053028474185		[learning rate: 0.00014598]
	Learning Rate: 0.000145985
	LOSS [training: 4.791053028474185 | validation: 5.3323655220862305]
	TIME [epoch: 9.76 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795698756405105		[learning rate: 0.00014564]
	Learning Rate: 0.00014564
	LOSS [training: 4.795698756405105 | validation: 5.314575498638761]
	TIME [epoch: 9.79 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788436379459367		[learning rate: 0.0001453]
	Learning Rate: 0.000145297
	LOSS [training: 4.788436379459367 | validation: 5.342793608379311]
	TIME [epoch: 9.76 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790372121125797		[learning rate: 0.00014495]
	Learning Rate: 0.000144954
	LOSS [training: 4.790372121125797 | validation: 5.3544814530475495]
	TIME [epoch: 9.78 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802046304377472		[learning rate: 0.00014461]
	Learning Rate: 0.000144612
	LOSS [training: 4.802046304377472 | validation: 5.34060779941231]
	TIME [epoch: 9.79 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.791496290473318		[learning rate: 0.00014427]
	Learning Rate: 0.000144271
	LOSS [training: 4.791496290473318 | validation: 5.346092523850477]
	TIME [epoch: 9.78 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795115793482407		[learning rate: 0.00014393]
	Learning Rate: 0.000143931
	LOSS [training: 4.795115793482407 | validation: 5.349881107887931]
	TIME [epoch: 9.77 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795130789883054		[learning rate: 0.00014359]
	Learning Rate: 0.000143591
	LOSS [training: 4.795130789883054 | validation: 5.333197253609543]
	TIME [epoch: 9.78 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786725595114321		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 4.786725595114321 | validation: 5.334755009322496]
	TIME [epoch: 9.79 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788005575729058		[learning rate: 0.00014291]
	Learning Rate: 0.000142915
	LOSS [training: 4.788005575729058 | validation: 5.321160213180118]
	TIME [epoch: 9.77 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794467942047583		[learning rate: 0.00014258]
	Learning Rate: 0.000142578
	LOSS [training: 4.794467942047583 | validation: 5.334228848952884]
	TIME [epoch: 9.77 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784770079327908		[learning rate: 0.00014224]
	Learning Rate: 0.000142241
	LOSS [training: 4.784770079327908 | validation: 5.317028894714215]
	TIME [epoch: 9.77 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788058939021573		[learning rate: 0.00014191]
	Learning Rate: 0.000141906
	LOSS [training: 4.788058939021573 | validation: 5.330705920286162]
	TIME [epoch: 9.79 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.835783755836298		[learning rate: 0.00014157]
	Learning Rate: 0.000141571
	LOSS [training: 4.835783755836298 | validation: 5.33234733297588]
	TIME [epoch: 9.78 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808647354568693		[learning rate: 0.00014124]
	Learning Rate: 0.000141237
	LOSS [training: 4.808647354568693 | validation: 5.3147013444774]
	TIME [epoch: 9.78 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7932762255270145		[learning rate: 0.0001409]
	Learning Rate: 0.000140904
	LOSS [training: 4.7932762255270145 | validation: 5.318448188956295]
	TIME [epoch: 9.78 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.802310933769853		[learning rate: 0.00014057]
	Learning Rate: 0.000140572
	LOSS [training: 4.802310933769853 | validation: 5.349838312288289]
	TIME [epoch: 9.79 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794485887974973		[learning rate: 0.00014024]
	Learning Rate: 0.00014024
	LOSS [training: 4.794485887974973 | validation: 5.334010526820783]
	TIME [epoch: 9.77 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786879764825779		[learning rate: 0.00013991]
	Learning Rate: 0.000139909
	LOSS [training: 4.786879764825779 | validation: 5.329854805977645]
	TIME [epoch: 9.77 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.79244787923291		[learning rate: 0.00013958]
	Learning Rate: 0.000139579
	LOSS [training: 4.79244787923291 | validation: 5.327995035511349]
	TIME [epoch: 9.78 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781832200464924		[learning rate: 0.00013925]
	Learning Rate: 0.00013925
	LOSS [training: 4.781832200464924 | validation: 5.359603919169565]
	TIME [epoch: 9.77 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779301148925361		[learning rate: 0.00013892]
	Learning Rate: 0.000138921
	LOSS [training: 4.779301148925361 | validation: 5.332319252782063]
	TIME [epoch: 9.77 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770305534631627		[learning rate: 0.00013859]
	Learning Rate: 0.000138594
	LOSS [training: 4.770305534631627 | validation: 5.344001820595769]
	TIME [epoch: 9.77 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7816788398954575		[learning rate: 0.00013827]
	Learning Rate: 0.000138267
	LOSS [training: 4.7816788398954575 | validation: 5.3053168680449145]
	TIME [epoch: 9.79 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774674607718287		[learning rate: 0.00013794]
	Learning Rate: 0.000137941
	LOSS [training: 4.774674607718287 | validation: 5.3121241334602205]
	TIME [epoch: 9.77 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773260089508034		[learning rate: 0.00013762]
	Learning Rate: 0.000137615
	LOSS [training: 4.773260089508034 | validation: 5.311775351840489]
	TIME [epoch: 9.76 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786161830351395		[learning rate: 0.00013729]
	Learning Rate: 0.000137291
	LOSS [training: 4.786161830351395 | validation: 5.339311544346754]
	TIME [epoch: 9.79 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772076844214038		[learning rate: 0.00013697]
	Learning Rate: 0.000136967
	LOSS [training: 4.772076844214038 | validation: 5.339001438081286]
	TIME [epoch: 9.8 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776204450886953		[learning rate: 0.00013664]
	Learning Rate: 0.000136644
	LOSS [training: 4.776204450886953 | validation: 5.325506027710261]
	TIME [epoch: 9.78 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78467302622891		[learning rate: 0.00013632]
	Learning Rate: 0.000136321
	LOSS [training: 4.78467302622891 | validation: 5.354460713787019]
	TIME [epoch: 9.77 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779537140733931		[learning rate: 0.000136]
	Learning Rate: 0.000136
	LOSS [training: 4.779537140733931 | validation: 5.321260175824848]
	TIME [epoch: 9.79 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772703293302017		[learning rate: 0.00013568]
	Learning Rate: 0.000135679
	LOSS [training: 4.772703293302017 | validation: 5.320736694408047]
	TIME [epoch: 9.78 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775516270471054		[learning rate: 0.00013536]
	Learning Rate: 0.000135359
	LOSS [training: 4.775516270471054 | validation: 5.317725326446216]
	TIME [epoch: 9.78 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.779309415345945		[learning rate: 0.00013504]
	Learning Rate: 0.00013504
	LOSS [training: 4.779309415345945 | validation: 5.310591681611486]
	TIME [epoch: 9.78 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770522265595348		[learning rate: 0.00013472]
	Learning Rate: 0.000134721
	LOSS [training: 4.770522265595348 | validation: 5.312732097249998]
	TIME [epoch: 9.79 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766523858314992		[learning rate: 0.0001344]
	Learning Rate: 0.000134403
	LOSS [training: 4.766523858314992 | validation: 5.32344021437661]
	TIME [epoch: 9.76 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784914057015861		[learning rate: 0.00013409]
	Learning Rate: 0.000134086
	LOSS [training: 4.784914057015861 | validation: 5.328043298795024]
	TIME [epoch: 9.77 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7742833876405895		[learning rate: 0.00013377]
	Learning Rate: 0.00013377
	LOSS [training: 4.7742833876405895 | validation: 5.324128494317799]
	TIME [epoch: 9.75 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775628298069233		[learning rate: 0.00013345]
	Learning Rate: 0.000133455
	LOSS [training: 4.775628298069233 | validation: 5.322449881534117]
	TIME [epoch: 9.79 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781479545848387		[learning rate: 0.00013314]
	Learning Rate: 0.00013314
	LOSS [training: 4.781479545848387 | validation: 5.315692274167868]
	TIME [epoch: 9.78 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780808556634694		[learning rate: 0.00013283]
	Learning Rate: 0.000132826
	LOSS [training: 4.780808556634694 | validation: 5.319149311058895]
	TIME [epoch: 9.77 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770933149012018		[learning rate: 0.00013251]
	Learning Rate: 0.000132512
	LOSS [training: 4.770933149012018 | validation: 5.322806751606225]
	TIME [epoch: 9.81 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773475533436172		[learning rate: 0.0001322]
	Learning Rate: 0.0001322
	LOSS [training: 4.773475533436172 | validation: 5.3522204758573]
	TIME [epoch: 9.79 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.815584804728905		[learning rate: 0.00013189]
	Learning Rate: 0.000131888
	LOSS [training: 4.815584804728905 | validation: 5.344725385948559]
	TIME [epoch: 9.77 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.783763562775362		[learning rate: 0.00013158]
	Learning Rate: 0.000131577
	LOSS [training: 4.783763562775362 | validation: 5.327097973798539]
	TIME [epoch: 9.77 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781991457545871		[learning rate: 0.00013127]
	Learning Rate: 0.000131266
	LOSS [training: 4.781991457545871 | validation: 5.308918301073976]
	TIME [epoch: 9.8 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773272899676685		[learning rate: 0.00013096]
	Learning Rate: 0.000130957
	LOSS [training: 4.773272899676685 | validation: 5.309935153593935]
	TIME [epoch: 9.79 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778685083293874		[learning rate: 0.00013065]
	Learning Rate: 0.000130648
	LOSS [training: 4.778685083293874 | validation: 5.32829949914576]
	TIME [epoch: 9.78 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7960505532096835		[learning rate: 0.00013034]
	Learning Rate: 0.00013034
	LOSS [training: 4.7960505532096835 | validation: 5.324416797091953]
	TIME [epoch: 9.79 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80898031135709		[learning rate: 0.00013003]
	Learning Rate: 0.000130032
	LOSS [training: 4.80898031135709 | validation: 5.350255994565303]
	TIME [epoch: 9.8 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7995027229953795		[learning rate: 0.00012973]
	Learning Rate: 0.000129726
	LOSS [training: 4.7995027229953795 | validation: 5.362882801841908]
	TIME [epoch: 9.77 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.810934466760725		[learning rate: 0.00012942]
	Learning Rate: 0.00012942
	LOSS [training: 4.810934466760725 | validation: 5.355504359179313]
	TIME [epoch: 9.78 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786113512766464		[learning rate: 0.00012911]
	Learning Rate: 0.000129114
	LOSS [training: 4.786113512766464 | validation: 5.321718844471244]
	TIME [epoch: 9.79 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787981149702722		[learning rate: 0.00012881]
	Learning Rate: 0.00012881
	LOSS [training: 4.787981149702722 | validation: 5.316634721862272]
	TIME [epoch: 9.79 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786265163058731		[learning rate: 0.00012851]
	Learning Rate: 0.000128506
	LOSS [training: 4.786265163058731 | validation: 5.326441670606069]
	TIME [epoch: 9.78 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7688572677071335		[learning rate: 0.0001282]
	Learning Rate: 0.000128203
	LOSS [training: 4.7688572677071335 | validation: 5.319231107039652]
	TIME [epoch: 9.77 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78817746311685		[learning rate: 0.0001279]
	Learning Rate: 0.0001279
	LOSS [training: 4.78817746311685 | validation: 5.3288909016676325]
	TIME [epoch: 9.8 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789623949067274		[learning rate: 0.0001276]
	Learning Rate: 0.000127599
	LOSS [training: 4.789623949067274 | validation: 5.329587719087479]
	TIME [epoch: 9.77 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790698499058285		[learning rate: 0.0001273]
	Learning Rate: 0.000127298
	LOSS [training: 4.790698499058285 | validation: 5.31194933987664]
	TIME [epoch: 9.78 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7808075336897735		[learning rate: 0.000127]
	Learning Rate: 0.000126997
	LOSS [training: 4.7808075336897735 | validation: 5.33295329608364]
	TIME [epoch: 9.78 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784481957101138		[learning rate: 0.0001267]
	Learning Rate: 0.000126698
	LOSS [training: 4.784481957101138 | validation: 5.315369552219293]
	TIME [epoch: 9.79 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7927679432116514		[learning rate: 0.0001264]
	Learning Rate: 0.000126399
	LOSS [training: 4.7927679432116514 | validation: 5.325413036175542]
	TIME [epoch: 9.78 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790121503742236		[learning rate: 0.0001261]
	Learning Rate: 0.000126101
	LOSS [training: 4.790121503742236 | validation: 5.333974062608201]
	TIME [epoch: 9.78 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.781458532914831		[learning rate: 0.0001258]
	Learning Rate: 0.000125803
	LOSS [training: 4.781458532914831 | validation: 5.317786145830296]
	TIME [epoch: 9.8 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787356863820526		[learning rate: 0.00012551]
	Learning Rate: 0.000125507
	LOSS [training: 4.787356863820526 | validation: 5.333723234858438]
	TIME [epoch: 9.78 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.794520332093969		[learning rate: 0.00012521]
	Learning Rate: 0.000125211
	LOSS [training: 4.794520332093969 | validation: 5.354137464442405]
	TIME [epoch: 9.78 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.833987650269522		[learning rate: 0.00012492]
	Learning Rate: 0.000124915
	LOSS [training: 4.833987650269522 | validation: 5.343548526204579]
	TIME [epoch: 9.78 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795436102284095		[learning rate: 0.00012462]
	Learning Rate: 0.000124621
	LOSS [training: 4.795436102284095 | validation: 5.3400915128209565]
	TIME [epoch: 9.8 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.788518077157783		[learning rate: 0.00012433]
	Learning Rate: 0.000124327
	LOSS [training: 4.788518077157783 | validation: 5.36440428451039]
	TIME [epoch: 9.78 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.823242151306765		[learning rate: 0.00012403]
	Learning Rate: 0.000124033
	LOSS [training: 4.823242151306765 | validation: 5.346859962603101]
	TIME [epoch: 9.78 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.789377322057172		[learning rate: 0.00012374]
	Learning Rate: 0.000123741
	LOSS [training: 4.789377322057172 | validation: 5.335180211240649]
	TIME [epoch: 9.8 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.799572606857116		[learning rate: 0.00012345]
	Learning Rate: 0.000123449
	LOSS [training: 4.799572606857116 | validation: 5.348791193043835]
	TIME [epoch: 9.79 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.810019172351775		[learning rate: 0.00012316]
	Learning Rate: 0.000123158
	LOSS [training: 4.810019172351775 | validation: 5.324539972715759]
	TIME [epoch: 9.78 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7868706548342335		[learning rate: 0.00012287]
	Learning Rate: 0.000122867
	LOSS [training: 4.7868706548342335 | validation: 5.353024485468018]
	TIME [epoch: 9.78 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7974974385721785		[learning rate: 0.00012258]
	Learning Rate: 0.000122577
	LOSS [training: 4.7974974385721785 | validation: 5.358984857185321]
	TIME [epoch: 9.8 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806354793348875		[learning rate: 0.00012229]
	Learning Rate: 0.000122288
	LOSS [training: 4.806354793348875 | validation: 5.366280985905157]
	TIME [epoch: 9.78 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.809470617346191		[learning rate: 0.000122]
	Learning Rate: 0.000122
	LOSS [training: 4.809470617346191 | validation: 5.3658758734429055]
	TIME [epoch: 9.78 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813340780656682		[learning rate: 0.00012171]
	Learning Rate: 0.000121712
	LOSS [training: 4.813340780656682 | validation: 5.323321351496179]
	TIME [epoch: 9.78 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787390501013484		[learning rate: 0.00012142]
	Learning Rate: 0.000121425
	LOSS [training: 4.787390501013484 | validation: 5.337798249090663]
	TIME [epoch: 9.78 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787440005535842		[learning rate: 0.00012114]
	Learning Rate: 0.000121138
	LOSS [training: 4.787440005535842 | validation: 5.325635712534406]
	TIME [epoch: 9.77 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776969699397478		[learning rate: 0.00012085]
	Learning Rate: 0.000120853
	LOSS [training: 4.776969699397478 | validation: 5.318468125994685]
	TIME [epoch: 9.79 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7894524707759505		[learning rate: 0.00012057]
	Learning Rate: 0.000120568
	LOSS [training: 4.7894524707759505 | validation: 5.3271230875217706]
	TIME [epoch: 9.76 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792283383375443		[learning rate: 0.00012028]
	Learning Rate: 0.000120283
	LOSS [training: 4.792283383375443 | validation: 5.330818341771667]
	TIME [epoch: 9.8 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792211038401618		[learning rate: 0.00012]
	Learning Rate: 0.00012
	LOSS [training: 4.792211038401618 | validation: 5.357715403046584]
	TIME [epoch: 9.79 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8008928212198825		[learning rate: 0.00011972]
	Learning Rate: 0.000119716
	LOSS [training: 4.8008928212198825 | validation: 5.330859566320169]
	TIME [epoch: 9.76 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787303439535004		[learning rate: 0.00011943]
	Learning Rate: 0.000119434
	LOSS [training: 4.787303439535004 | validation: 5.326490475946343]
	TIME [epoch: 9.78 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.808992412280817		[learning rate: 0.00011915]
	Learning Rate: 0.000119152
	LOSS [training: 4.808992412280817 | validation: 5.330101393511758]
	TIME [epoch: 9.78 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.800598830557854		[learning rate: 0.00011887]
	Learning Rate: 0.000118871
	LOSS [training: 4.800598830557854 | validation: 5.316563092411817]
	TIME [epoch: 9.78 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776082509391859		[learning rate: 0.00011859]
	Learning Rate: 0.000118591
	LOSS [training: 4.776082509391859 | validation: 5.319165670728828]
	TIME [epoch: 9.77 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782279422274442		[learning rate: 0.00011831]
	Learning Rate: 0.000118311
	LOSS [training: 4.782279422274442 | validation: 5.329660596194818]
	TIME [epoch: 9.81 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7904508960603		[learning rate: 0.00011803]
	Learning Rate: 0.000118032
	LOSS [training: 4.7904508960603 | validation: 5.321223810271272]
	TIME [epoch: 9.78 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777621519647697		[learning rate: 0.00011775]
	Learning Rate: 0.000117754
	LOSS [training: 4.777621519647697 | validation: 5.3394614279798995]
	TIME [epoch: 9.77 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7720382645781525		[learning rate: 0.00011748]
	Learning Rate: 0.000117476
	LOSS [training: 4.7720382645781525 | validation: 5.321096049042858]
	TIME [epoch: 9.78 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776356998331454		[learning rate: 0.0001172]
	Learning Rate: 0.000117199
	LOSS [training: 4.776356998331454 | validation: 5.331280115636623]
	TIME [epoch: 9.8 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772740627931979		[learning rate: 0.00011692]
	Learning Rate: 0.000116922
	LOSS [training: 4.772740627931979 | validation: 5.305011858909886]
	TIME [epoch: 9.78 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.782766079020003		[learning rate: 0.00011665]
	Learning Rate: 0.000116647
	LOSS [training: 4.782766079020003 | validation: 5.319744357770727]
	TIME [epoch: 9.78 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777997459394491		[learning rate: 0.00011637]
	Learning Rate: 0.000116371
	LOSS [training: 4.777997459394491 | validation: 5.327404088644884]
	TIME [epoch: 9.78 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778932554214339		[learning rate: 0.0001161]
	Learning Rate: 0.000116097
	LOSS [training: 4.778932554214339 | validation: 5.32806678672048]
	TIME [epoch: 9.79 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775505585872965		[learning rate: 0.00011582]
	Learning Rate: 0.000115823
	LOSS [training: 4.775505585872965 | validation: 5.328298459290639]
	TIME [epoch: 9.77 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.790173058338648		[learning rate: 0.00011555]
	Learning Rate: 0.00011555
	LOSS [training: 4.790173058338648 | validation: 5.33223807236115]
	TIME [epoch: 9.79 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.785702562840803		[learning rate: 0.00011528]
	Learning Rate: 0.000115277
	LOSS [training: 4.785702562840803 | validation: 5.33481955008288]
	TIME [epoch: 9.79 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778283193834221		[learning rate: 0.00011501]
	Learning Rate: 0.000115005
	LOSS [training: 4.778283193834221 | validation: 5.338196387934745]
	TIME [epoch: 9.78 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778527237719431		[learning rate: 0.00011473]
	Learning Rate: 0.000114734
	LOSS [training: 4.778527237719431 | validation: 5.348774213029151]
	TIME [epoch: 9.77 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.775787286206336		[learning rate: 0.00011446]
	Learning Rate: 0.000114463
	LOSS [training: 4.775787286206336 | validation: 5.317277649654943]
	TIME [epoch: 9.78 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769656642785412		[learning rate: 0.00011419]
	Learning Rate: 0.000114193
	LOSS [training: 4.769656642785412 | validation: 5.314014820118725]
	TIME [epoch: 9.79 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771718456389888		[learning rate: 0.00011392]
	Learning Rate: 0.000113924
	LOSS [training: 4.771718456389888 | validation: 5.316598685669312]
	TIME [epoch: 9.78 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769178351103458		[learning rate: 0.00011366]
	Learning Rate: 0.000113655
	LOSS [training: 4.769178351103458 | validation: 5.312388857308483]
	TIME [epoch: 9.78 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76689742380714		[learning rate: 0.00011339]
	Learning Rate: 0.000113387
	LOSS [training: 4.76689742380714 | validation: 5.312173488901155]
	TIME [epoch: 9.79 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772673010945451		[learning rate: 0.00011312]
	Learning Rate: 0.00011312
	LOSS [training: 4.772673010945451 | validation: 5.301103085475311]
	TIME [epoch: 9.8 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76575568171894		[learning rate: 0.00011285]
	Learning Rate: 0.000112853
	LOSS [training: 4.76575568171894 | validation: 5.313405973304295]
	TIME [epoch: 9.79 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765428162289632		[learning rate: 0.00011259]
	Learning Rate: 0.000112587
	LOSS [training: 4.765428162289632 | validation: 5.3030132294367345]
	TIME [epoch: 9.79 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773435023149477		[learning rate: 0.00011232]
	Learning Rate: 0.000112321
	LOSS [training: 4.773435023149477 | validation: 5.3240401005249]
	TIME [epoch: 9.8 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.787624386563401		[learning rate: 0.00011206]
	Learning Rate: 0.000112056
	LOSS [training: 4.787624386563401 | validation: 5.3125926042991685]
	TIME [epoch: 9.79 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.761617616408422		[learning rate: 0.00011179]
	Learning Rate: 0.000111792
	LOSS [training: 4.761617616408422 | validation: 5.300266840279625]
	TIME [epoch: 9.78 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7676035346261285		[learning rate: 0.00011153]
	Learning Rate: 0.000111528
	LOSS [training: 4.7676035346261285 | validation: 5.321424785925619]
	TIME [epoch: 9.78 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793228095114124		[learning rate: 0.00011127]
	Learning Rate: 0.000111265
	LOSS [training: 4.793228095114124 | validation: 5.356617846817135]
	TIME [epoch: 9.79 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.797392047914605		[learning rate: 0.000111]
	Learning Rate: 0.000111003
	LOSS [training: 4.797392047914605 | validation: 5.347010573166937]
	TIME [epoch: 9.77 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773514327970981		[learning rate: 0.00011074]
	Learning Rate: 0.000110741
	LOSS [training: 4.773514327970981 | validation: 5.335347904319657]
	TIME [epoch: 9.78 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766512758939494		[learning rate: 0.00011048]
	Learning Rate: 0.00011048
	LOSS [training: 4.766512758939494 | validation: 5.3041883159296095]
	TIME [epoch: 9.78 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768216800543181		[learning rate: 0.00011022]
	Learning Rate: 0.000110219
	LOSS [training: 4.768216800543181 | validation: 5.3379319328137695]
	TIME [epoch: 9.78 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769563852807117		[learning rate: 0.00010996]
	Learning Rate: 0.000109959
	LOSS [training: 4.769563852807117 | validation: 5.325791636735398]
	TIME [epoch: 9.77 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7801510306243555		[learning rate: 0.0001097]
	Learning Rate: 0.0001097
	LOSS [training: 4.7801510306243555 | validation: 5.301819497760919]
	TIME [epoch: 9.76 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777621300693395		[learning rate: 0.00010944]
	Learning Rate: 0.000109441
	LOSS [training: 4.777621300693395 | validation: 5.329950579955202]
	TIME [epoch: 9.78 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774181431605787		[learning rate: 0.00010918]
	Learning Rate: 0.000109183
	LOSS [training: 4.774181431605787 | validation: 5.310214066607016]
	TIME [epoch: 9.78 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768545625368527		[learning rate: 0.00010893]
	Learning Rate: 0.000108925
	LOSS [training: 4.768545625368527 | validation: 5.300137940145653]
	TIME [epoch: 9.77 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777714585488192		[learning rate: 0.00010867]
	Learning Rate: 0.000108668
	LOSS [training: 4.777714585488192 | validation: 5.311235250535977]
	TIME [epoch: 9.78 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763876739849845		[learning rate: 0.00010841]
	Learning Rate: 0.000108412
	LOSS [training: 4.763876739849845 | validation: 5.302117817900487]
	TIME [epoch: 9.79 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.795277834551809		[learning rate: 0.00010816]
	Learning Rate: 0.000108156
	LOSS [training: 4.795277834551809 | validation: 5.307321919129031]
	TIME [epoch: 9.78 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.776027834033075		[learning rate: 0.0001079]
	Learning Rate: 0.000107901
	LOSS [training: 4.776027834033075 | validation: 5.315589542530715]
	TIME [epoch: 9.78 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763043224842785		[learning rate: 0.00010765]
	Learning Rate: 0.000107647
	LOSS [training: 4.763043224842785 | validation: 5.32154302995471]
	TIME [epoch: 9.78 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.777938994733674		[learning rate: 0.00010739]
	Learning Rate: 0.000107393
	LOSS [training: 4.777938994733674 | validation: 5.332904180785366]
	TIME [epoch: 9.79 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.758341663253036		[learning rate: 0.00010714]
	Learning Rate: 0.000107139
	LOSS [training: 4.758341663253036 | validation: 5.305060820776585]
	TIME [epoch: 9.77 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764668291957102		[learning rate: 0.00010689]
	Learning Rate: 0.000106887
	LOSS [training: 4.764668291957102 | validation: 5.296889002303425]
	TIME [epoch: 9.78 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763015453338154		[learning rate: 0.00010663]
	Learning Rate: 0.000106634
	LOSS [training: 4.763015453338154 | validation: 5.305392427845929]
	TIME [epoch: 9.78 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7592696391848275		[learning rate: 0.00010638]
	Learning Rate: 0.000106383
	LOSS [training: 4.7592696391848275 | validation: 5.305293785012575]
	TIME [epoch: 9.78 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7656946704981005		[learning rate: 0.00010613]
	Learning Rate: 0.000106132
	LOSS [training: 4.7656946704981005 | validation: 5.314502933544419]
	TIME [epoch: 9.77 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.765023954465609		[learning rate: 0.00010588]
	Learning Rate: 0.000105882
	LOSS [training: 4.765023954465609 | validation: 5.311299070790551]
	TIME [epoch: 9.77 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.764731326827479		[learning rate: 0.00010563]
	Learning Rate: 0.000105632
	LOSS [training: 4.764731326827479 | validation: 5.3203703078585844]
	TIME [epoch: 9.8 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.770357721721302		[learning rate: 0.00010538]
	Learning Rate: 0.000105383
	LOSS [training: 4.770357721721302 | validation: 5.318181800028152]
	TIME [epoch: 9.77 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.780045432544766		[learning rate: 0.00010513]
	Learning Rate: 0.000105134
	LOSS [training: 4.780045432544766 | validation: 5.3165022725883]
	TIME [epoch: 9.78 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774028713137114		[learning rate: 0.00010489]
	Learning Rate: 0.000104886
	LOSS [training: 4.774028713137114 | validation: 5.3196020858917015]
	TIME [epoch: 9.77 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766221084777603		[learning rate: 0.00010464]
	Learning Rate: 0.000104639
	LOSS [training: 4.766221084777603 | validation: 5.3058432184669115]
	TIME [epoch: 9.8 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772242409932375		[learning rate: 0.00010439]
	Learning Rate: 0.000104392
	LOSS [training: 4.772242409932375 | validation: 5.3158295136355695]
	TIME [epoch: 9.76 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.773895424381701		[learning rate: 0.00010415]
	Learning Rate: 0.000104146
	LOSS [training: 4.773895424381701 | validation: 5.297539857921893]
	TIME [epoch: 9.77 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766814940005209		[learning rate: 0.0001039]
	Learning Rate: 0.0001039
	LOSS [training: 4.766814940005209 | validation: 5.3081830357526965]
	TIME [epoch: 9.78 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.772984255243161		[learning rate: 0.00010365]
	Learning Rate: 0.000103655
	LOSS [training: 4.772984255243161 | validation: 5.306224484723621]
	TIME [epoch: 9.78 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769489206959863		[learning rate: 0.00010341]
	Learning Rate: 0.00010341
	LOSS [training: 4.769489206959863 | validation: 5.309920930275414]
	TIME [epoch: 9.78 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766628950115315		[learning rate: 0.00010317]
	Learning Rate: 0.000103166
	LOSS [training: 4.766628950115315 | validation: 5.3276369251344295]
	TIME [epoch: 9.76 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7702349300241575		[learning rate: 0.00010292]
	Learning Rate: 0.000102923
	LOSS [training: 4.7702349300241575 | validation: 5.3044938932580985]
	TIME [epoch: 9.79 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768415337224877		[learning rate: 0.00010268]
	Learning Rate: 0.00010268
	LOSS [training: 4.768415337224877 | validation: 5.304028748627425]
	TIME [epoch: 9.78 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.769144603333797		[learning rate: 0.00010244]
	Learning Rate: 0.000102438
	LOSS [training: 4.769144603333797 | validation: 5.315802629435047]
	TIME [epoch: 9.77 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.778405579880091		[learning rate: 0.0001022]
	Learning Rate: 0.000102196
	LOSS [training: 4.778405579880091 | validation: 5.3128179202897625]
	TIME [epoch: 9.77 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.762819091150898		[learning rate: 0.00010196]
	Learning Rate: 0.000101955
	LOSS [training: 4.762819091150898 | validation: 5.304298211832065]
	TIME [epoch: 9.79 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.771775344728658		[learning rate: 0.00010171]
	Learning Rate: 0.000101715
	LOSS [training: 4.771775344728658 | validation: 5.297674810206131]
	TIME [epoch: 9.77 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.763291768417107		[learning rate: 0.00010147]
	Learning Rate: 0.000101475
	LOSS [training: 4.763291768417107 | validation: 5.308388250697715]
	TIME [epoch: 9.76 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.768093411548567		[learning rate: 0.00010124]
	Learning Rate: 0.000101236
	LOSS [training: 4.768093411548567 | validation: 5.3042708306775195]
	TIME [epoch: 9.79 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766261848606838		[learning rate: 0.000101]
	Learning Rate: 0.000100997
	LOSS [training: 4.766261848606838 | validation: 5.315446822089462]
	TIME [epoch: 9.79 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.774159389842478		[learning rate: 0.00010076]
	Learning Rate: 0.000100759
	LOSS [training: 4.774159389842478 | validation: 5.319285472306099]
	TIME [epoch: 9.78 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78170934401326		[learning rate: 0.00010052]
	Learning Rate: 0.000100521
	LOSS [training: 4.78170934401326 | validation: 5.342907610288757]
	TIME [epoch: 9.77 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7730460527589305		[learning rate: 0.00010028]
	Learning Rate: 0.000100284
	LOSS [training: 4.7730460527589305 | validation: 5.323863037691715]
	TIME [epoch: 9.8 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.766741192924897		[learning rate: 0.00010005]
	Learning Rate: 0.000100047
	LOSS [training: 4.766741192924897 | validation: 5.320514911638429]
	TIME [epoch: 9.79 sec]
Finished training in 19687.789 seconds.
