Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r0', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1711892175

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 10.75795709297912		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.846794485978581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.802375789478852 | validation: 7.938689968167685]
	TIME [epoch: 69.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 8.285228647327994		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.799173216994914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.042200932161453 | validation: 8.480823153331071]
	TIME [epoch: 8.82 sec]
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.947265834391994		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.583567198471123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.765416516431559 | validation: 6.322813066744468]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.2554170950158765		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.17178656277272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.213601828894298 | validation: 6.311018245937731]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.126366649490163		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.23433933031495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.180352989902556 | validation: 6.201614377944837]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.1584817228858615		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.189204669130683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.173843196008271 | validation: 6.236580770187128]
	TIME [epoch: 8.81 sec]
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.058218173216181		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.186660940791908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.122439557004043 | validation: 6.248904747053528]
	TIME [epoch: 8.8 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.0852939975704805		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.199783632819354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.142538815194917 | validation: 6.104052108076409]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.036670989342674		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.201552655083578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.119111822213126 | validation: 6.059162003462402]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.8812944634215585		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.082567117771919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.981930790596737 | validation: 5.953134837228489]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.738222221653222		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.641227994722032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.689725108187628 | validation: 4.612326977222162]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.483074048699819		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.209357133567434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3462155911336255 | validation: 4.108972164192536]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.165472309524685		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.2013248590793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.183398584301992 | validation: 4.224356414667739]
	TIME [epoch: 8.74 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.9975330191244027		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4083263979539637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.702929708539183 | validation: 2.374347300893899]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.371983114373786		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.088230713141545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2301069137576657 | validation: 1.9661984866438962]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.074583583430449		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8796928719938149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9771382277121319 | validation: 3.408628733948915]
	TIME [epoch: 8.76 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.087978393280942		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7509251323997348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9194517628403385 | validation: 1.656993537369626]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7379448986218933		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.624434690228301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6811897944250969 | validation: 1.7206250065930881]
	TIME [epoch: 8.73 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.786418886306209		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9927249625480314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8895719244271205 | validation: 1.4401581043699316]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5975880665633002		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5310845334573315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5643363000103157 | validation: 1.773124715775873]
	TIME [epoch: 8.75 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.52355114074079		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6159907751279015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5697709579343455 | validation: 1.4888086113564698]
	TIME [epoch: 8.77 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.715696080217003		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.818178776269529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7669374282432657 | validation: 1.5283711354465574]
	TIME [epoch: 8.74 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4205712404158704		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4012015550885215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.410886397752196 | validation: 1.3607729159562283]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.40251528796052		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.192535147690676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7975252178255978 | validation: 1.356531590935389]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_24.pth
	Model improved!!!
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4215348671955121		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3816473034298422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4015910853126772 | validation: 1.2514498087357135]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4069543782877223		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3459534444838477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.376453911385785 | validation: 1.5447678373110276]
	TIME [epoch: 8.76 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3636081903191843		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.371885461443389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3677468258812868 | validation: 1.1931988247167418]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_27.pth
	Model improved!!!
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4331153716699034		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3876461041097166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.41038073788981 | validation: 1.3652065386127508]
	TIME [epoch: 8.73 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4977559830860607		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4677114143730008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4827336987295308 | validation: 1.4356175033420056]
	TIME [epoch: 8.73 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3402761002362729		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3747499946238946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3575130474300836 | validation: 1.2599835734659446]
	TIME [epoch: 8.72 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3437004898643492		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.063887473417801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.203793981641075 | validation: 4.895028211185235]
	TIME [epoch: 8.75 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.6097276809002823		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.539832931688009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5747803062941452 | validation: 1.3691764450925583]
	TIME [epoch: 8.73 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3166884228553326		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.321165419901957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3189269213786452 | validation: 1.7122239817167784]
	TIME [epoch: 8.74 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3828522332490896		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2691273932590765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3259898132540833 | validation: 1.2215552297072554]
	TIME [epoch: 8.73 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.36626981720597		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.176713575588518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.271491696397244 | validation: 1.161747620795085]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_35.pth
	Model improved!!!
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.232664991862786		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3051445586623316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2689047752625586 | validation: 1.2349471154292573]
	TIME [epoch: 8.74 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2637990354087107		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.489949885557949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3768744604833296 | validation: 1.3825846298027544]
	TIME [epoch: 8.74 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.24241153191675		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1976883958601179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.220049963888434 | validation: 1.4923464601351817]
	TIME [epoch: 8.72 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2310135821251833		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4608376250946455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3459256036099148 | validation: 1.1488243736211998]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2420305546835402		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.308455334026391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2752429443549653 | validation: 1.2305412167943703]
	TIME [epoch: 8.73 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3608623533995978		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2333560283411702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.297109190870384 | validation: 1.1782964135742224]
	TIME [epoch: 8.73 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1776884857251626		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3678513813572342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2727699335411984 | validation: 0.958575391584517]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.404514124698605		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2062176622368708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3053658934677377 | validation: 1.0377331157711203]
	TIME [epoch: 8.73 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1335706280580884		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2120674966679448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1728190623630164 | validation: 1.394297985543969]
	TIME [epoch: 8.72 sec]
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.12469671418819		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.015411182249834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.070053948219012 | validation: 0.9012707704832091]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0402888115092928		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2423059933218767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1412974024155849 | validation: 1.2439013131062615]
	TIME [epoch: 8.74 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2491284298617704		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0203438928856288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1347361613736995 | validation: 0.7969507799858563]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9414682443551877		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0743180074463354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0078931259007615 | validation: 1.3981004783777533]
	TIME [epoch: 8.74 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1744707620448		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0768374827806952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1256541224127476 | validation: 0.8973371364634408]
	TIME [epoch: 8.72 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9584037923944193		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0136595186888493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9860316555416345 | validation: 0.954338152700392]
	TIME [epoch: 8.73 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9927753982760871		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 1.4398897219183424		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.2163325600972144 | validation: 1.118592744921513]
	TIME [epoch: 8.72 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0054775637675981		[learning rate: 0.00993]
		[batch 20/20] avg loss: 0.8483610870156906		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 0.9269193253916443 | validation: 0.8561255216330499]
	TIME [epoch: 8.75 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8729962109831257		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 1.081812810033563		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 0.9774045105083443 | validation: 0.7761651909581477]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_53.pth
	Model improved!!!
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8445503809783729		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 1.007457824594764		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 0.9260041027865684 | validation: 0.9415653927524112]
	TIME [epoch: 8.72 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9080731742030297		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 0.8835423153483589		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 0.8958077447756944 | validation: 0.6836686180708076]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_55.pth
	Model improved!!!
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9017724224748719		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 0.9857422122261162		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 0.9437573173504938 | validation: 0.9147192855602202]
	TIME [epoch: 8.76 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0605817068824994		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 0.8424238484008093		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 0.9515027776416545 | validation: 0.7837543417810613]
	TIME [epoch: 8.74 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9992291465003728		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 1.0621666284691287		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 1.0306978874847508 | validation: 1.2320866613034314]
	TIME [epoch: 8.72 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8772693044665297		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 0.8578339793471763		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 0.8675516419068533 | validation: 0.6349875803919436]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_59.pth
	Model improved!!!
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8494687868580181		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 0.849253300690668		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 0.849361043774343 | validation: 0.7309793434711349]
	TIME [epoch: 8.74 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8603510351728654		[learning rate: 0.009506]
		[batch 20/20] avg loss: 1.0362904376256128		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 0.9483207363992392 | validation: 2.3700714422079763]
	TIME [epoch: 8.73 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2662190179134405		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 0.8496871961994528		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 1.0579531070564465 | validation: 0.8725579256400406]
	TIME [epoch: 8.75 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9018373827720472		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 0.7457001363778566		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 0.8237687595749519 | validation: 0.8891660472355285]
	TIME [epoch: 8.73 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8927469175148183		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 0.7905358407362286		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 0.8416413791255234 | validation: 0.6685867861183431]
	TIME [epoch: 8.72 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8819131878195708		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 0.7562514140581891		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 0.8190823009388801 | validation: 1.3750074029778614]
	TIME [epoch: 8.72 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0360578941277927		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 0.793069911513843		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 0.9145639028208178 | validation: 1.216552966444477]
	TIME [epoch: 8.72 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8366753915309648		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 0.7664493066436535		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 0.8015623490873092 | validation: 1.2375909157181835]
	TIME [epoch: 8.75 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8334703499904261		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 0.8150314234960669		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 0.8242508867432464 | validation: 0.7575635544848197]
	TIME [epoch: 8.73 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7395244423275587		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 0.7884344274945657		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 0.7639794349110621 | validation: 0.7345128797953211]
	TIME [epoch: 8.72 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6787004715567728		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 0.6036540365840904		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.6411772540704316 | validation: 0.7104883280374481]
	TIME [epoch: 8.73 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9306366762161872		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 1.0048966506637134		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 0.9677666634399504 | validation: 0.669244144769743]
	TIME [epoch: 8.72 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6713700328365039		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 0.7990461154271606		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 0.7352080741318323 | validation: 1.5953133701769198]
	TIME [epoch: 8.73 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8813635324321822		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 0.5858523359205792		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 0.7336079341763807 | validation: 0.7738638503085288]
	TIME [epoch: 8.74 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6940940537356133		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 0.6293770128291406		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 0.661735533282377 | validation: 0.48829032630536423]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_74.pth
	Model improved!!!
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6072367669249255		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 0.5695896309742898		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 0.5884131989496076 | validation: 0.8672868148305621]
	TIME [epoch: 8.74 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7292951344252101		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 0.8701264011379479		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 0.7997107677815791 | validation: 0.800386904293513]
	TIME [epoch: 8.73 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.793004688629939		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 0.662226794488675		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 0.7276157415593072 | validation: 0.602778436268472]
	TIME [epoch: 8.74 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6694628340520874		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 0.5434235593821811		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 0.6064431967171342 | validation: 0.6731105774477275]
	TIME [epoch: 8.73 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5862878137645969		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 0.5439916869628596		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 0.5651397503637282 | validation: 0.5271999701555108]
	TIME [epoch: 8.73 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6386182831004736		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.6219659588452522		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 0.630292120972863 | validation: 0.4706814092387377]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_80.pth
	Model improved!!!
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9084988377628307		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 0.49792826182307515		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 0.7032135497929528 | validation: 0.42611165507360543]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_81.pth
	Model improved!!!
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6030598222565857		[learning rate: 0.008586]
		[batch 20/20] avg loss: 0.6140432819753365		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 0.6085515521159611 | validation: 0.8480847074294606]
	TIME [epoch: 8.74 sec]
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5681758290918741		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 0.432225612010736		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 0.500200720551305 | validation: 0.589048562724452]
	TIME [epoch: 8.75 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.619857783377487		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 0.47539538933097225		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 0.5476265863542296 | validation: 0.3878750911211917]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_84.pth
	Model improved!!!
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5652702540755865		[learning rate: 0.008462]
		[batch 20/20] avg loss: 0.7461855609851783		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 0.6557279075303823 | validation: 0.40694339078640596]
	TIME [epoch: 8.73 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5277813961243052		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 0.5334864898505752		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 0.53063394298744 | validation: 0.6252230658274995]
	TIME [epoch: 8.72 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5270867252283955		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 0.6243878877787481		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 0.5757373065035718 | validation: 1.0393294946353158]
	TIME [epoch: 8.74 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5331704664493082		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 0.6297443083279087		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 0.5814573873886084 | validation: 0.6848301452791441]
	TIME [epoch: 8.76 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5480015435431638		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 0.5431261447710906		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.5455638441571272 | validation: 0.45466074811081725]
	TIME [epoch: 8.74 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46437994343239397		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 0.5366501774506276		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 0.5005150604415108 | validation: 0.29055271875369326]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_90.pth
	Model improved!!!
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43053968890174		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 0.58450488294729		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 0.5075222859245149 | validation: 0.3233088939630174]
	TIME [epoch: 8.73 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5712603201831801		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 0.4730000262015702		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 0.5221301731923751 | validation: 0.3278174587401097]
	TIME [epoch: 8.72 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5076842232559462		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 0.4644474605490327		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 0.4860658419024896 | validation: 0.4067777700074696]
	TIME [epoch: 8.74 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45284328288983805		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 0.6927689451840916		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 0.5728061140369648 | validation: 0.45224118279038755]
	TIME [epoch: 8.72 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41302417167853883		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 0.5293717578983383		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 0.4711979647884387 | validation: 1.176725402338282]
	TIME [epoch: 8.71 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5594717016607685		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 0.42446061039076366		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 0.49196615602576604 | validation: 0.34086523334396746]
	TIME [epoch: 8.72 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6945851909903789		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 0.5261736741608346		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 0.6103794325756068 | validation: 0.45001684274191933]
	TIME [epoch: 8.72 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42204924306076846		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 0.42415868308480215		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 0.4231039630727852 | validation: 0.6619521040940248]
	TIME [epoch: 8.73 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5610500934342317		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.4586533021727147		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 0.5098516978034733 | validation: 0.3451519114349412]
	TIME [epoch: 8.73 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4418528779848952		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 0.3712307439764069		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 0.406541810980651 | validation: 0.371404058180499]
	TIME [epoch: 8.73 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48317001377334223		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 0.38551723344460964		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 0.4343436236089759 | validation: 0.4345261633994068]
	TIME [epoch: 8.74 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4194215141138664		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 0.40163036997790397		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 0.4105259420458852 | validation: 0.9150214332026789]
	TIME [epoch: 8.73 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5790137615037485		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 0.5069944492490573		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 0.543004105376403 | validation: 0.24289417991010448]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_103.pth
	Model improved!!!
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49149092614347517		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 0.3780917750746381		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 0.4347913506090566 | validation: 0.37603219619983785]
	TIME [epoch: 8.75 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.436873991034293		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 0.5146884296278839		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 0.4757812103310884 | validation: 0.3530626213455594]
	TIME [epoch: 8.72 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44454939068153215		[learning rate: 0.007643]
		[batch 20/20] avg loss: 0.5032848776101428		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 0.4739171341458374 | validation: 0.6147340216887138]
	TIME [epoch: 8.73 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48310301939941835		[learning rate: 0.007606]
		[batch 20/20] avg loss: 0.6030380485176586		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 0.5430705339585387 | validation: 0.7114795985763114]
	TIME [epoch: 8.73 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3636375102810754		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 0.617925127272595		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.4907813187768352 | validation: 0.3043430135741985]
	TIME [epoch: 8.74 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4376333230462697		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 0.38110765710865846		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 0.4093704900774641 | validation: 0.37193006094625225]
	TIME [epoch: 8.75 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48741742218222317		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 0.3815673403052194		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 0.4344923812437213 | validation: 0.23832957953563233]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_110.pth
	Model improved!!!
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4662019615701817		[learning rate: 0.00746]
		[batch 20/20] avg loss: 0.43598663546507443		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 0.451094298517628 | validation: 0.28650435593247614]
	TIME [epoch: 8.71 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38777941645694913		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 0.4022847650469659		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 0.39503209075195744 | validation: 0.3852804871246831]
	TIME [epoch: 8.7 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46384055433839155		[learning rate: 0.007388]
		[batch 20/20] avg loss: 0.36129394766468437		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 0.4125672510015379 | validation: 0.2921813164146442]
	TIME [epoch: 8.71 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38193137978644176		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 0.39337670385650036		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 0.3876540418214711 | validation: 0.2760780162234058]
	TIME [epoch: 8.74 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4399412780973687		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 0.43088597731959977		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 0.4354136277084842 | validation: 0.4019822397164579]
	TIME [epoch: 8.73 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40318193031252036		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 0.3663608409420986		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 0.3847713856273095 | validation: 0.3340655880857998]
	TIME [epoch: 8.73 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6135793372933792		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 0.5498762354336171		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 0.5817277863634981 | validation: 0.2793825044784413]
	TIME [epoch: 8.72 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5044326801288114		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.44477219723325573		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 0.4746024386810336 | validation: 0.8042156793267101]
	TIME [epoch: 8.72 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.506777397900025		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 0.41588852990800146		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 0.4613329639040131 | validation: 0.8587099581663109]
	TIME [epoch: 8.73 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44875668124831114		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 0.39881750071709277		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 0.4237870909827019 | validation: 0.48667287851343966]
	TIME [epoch: 8.72 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3336994045750798		[learning rate: 0.007107]
		[batch 20/20] avg loss: 0.434648447179511		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 0.38417392587729543 | validation: 0.538105592792358]
	TIME [epoch: 8.73 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5711563478889464		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 0.339970075259788		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 0.45556321157436724 | validation: 0.3523766959173883]
	TIME [epoch: 8.72 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3712690318008868		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 0.31758023781993483		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 0.3444246348104108 | validation: 0.4184189792062958]
	TIME [epoch: 8.72 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.369244373930728		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 0.5387442927134092		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 0.4539943333220685 | validation: 0.5225298636953727]
	TIME [epoch: 8.74 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30473972089900847		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 0.5202319646727458		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 0.41248584278587713 | validation: 0.5359468397977877]
	TIME [epoch: 8.73 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46953163772362927		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 0.38288826697561895		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 0.4262099523496241 | validation: 0.329679452117637]
	TIME [epoch: 8.71 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.339401900254314		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 0.3642764362660387		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.35183916826017636 | validation: 0.40928453646389074]
	TIME [epoch: 8.72 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4749503382041498		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 0.4505336081972645		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 0.4627419732007071 | validation: 0.3327674461484583]
	TIME [epoch: 8.74 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44528607844906415		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 0.38675578452331266		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 0.41602093148618835 | validation: 0.23098988372037715]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_129.pth
	Model improved!!!
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37227432291861084		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 0.5007234029666473		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 0.4364988629426291 | validation: 0.29274474693914915]
	TIME [epoch: 8.76 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3153897403335527		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 0.4190965667956813		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 0.36724315356461706 | validation: 0.2155930306603201]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_131.pth
	Model improved!!!
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4170107077263953		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 0.3412915473451677		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 0.37915112753578156 | validation: 0.6250790999106761]
	TIME [epoch: 8.73 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46242118907113827		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 0.34378303094605683		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 0.4031021100085975 | validation: 0.27777953873760164]
	TIME [epoch: 8.72 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4383405335094864		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 0.41106741962239945		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 0.42470397656594294 | validation: 0.4098246310042895]
	TIME [epoch: 8.73 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35604749871048913		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 0.33568995065856644		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 0.34586872468452773 | validation: 0.4574969392264946]
	TIME [epoch: 8.75 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34968213399193304		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 0.3493231414860499		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 0.34950263773899154 | validation: 0.3326510202061642]
	TIME [epoch: 8.74 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4163921684770705		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.3873001098087143		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 0.4018461391428924 | validation: 0.38205497018180523]
	TIME [epoch: 8.73 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4256700814032873		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 0.35798700448709064		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 0.391828542945189 | validation: 0.5781146589978146]
	TIME [epoch: 8.73 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4306380824393738		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 0.4408939517047717		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 0.43576601707207285 | validation: 0.37091972725035094]
	TIME [epoch: 8.72 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49239626371245493		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 0.34108422561430735		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 0.4167402446633811 | validation: 0.1691943141650651]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_140.pth
	Model improved!!!
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.300205919634163		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 0.39435446405908137		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 0.3472801918466222 | validation: 0.4725770380523903]
	TIME [epoch: 8.73 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4133879845731454		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 0.3366191555030783		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 0.37500357003811186 | validation: 0.5298548462831212]
	TIME [epoch: 8.73 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3436887193675307		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 0.3795707986686177		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 0.36162975901807426 | validation: 0.3608098562727332]
	TIME [epoch: 8.72 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28949796344609346		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 0.3873116780229975		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 0.33840482073454553 | validation: 0.4200189805760291]
	TIME [epoch: 8.72 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36226986322360255		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.3053031153600819		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 0.3337864892918422 | validation: 0.2169774855446388]
	TIME [epoch: 8.75 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44593449307068356		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.29975630863961866		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.37284540085515117 | validation: 0.2704886531279115]
	TIME [epoch: 8.72 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3746098761398172		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 0.3738749342266364		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 0.3742424051832268 | validation: 0.2620983566716351]
	TIME [epoch: 8.71 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30165310714262		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 0.411868749745291		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 0.3567609284439555 | validation: 0.5527750543443906]
	TIME [epoch: 8.72 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36684704419491787		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.41276965996731996		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 0.38980835208111897 | validation: 0.5698035771666108]
	TIME [epoch: 8.73 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42032620628436873		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.29940231320462746		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.359864259744498 | validation: 0.22186171317992487]
	TIME [epoch: 8.75 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3369699685609967		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 0.42842330669774376		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 0.38269663762937023 | validation: 0.32191879310604526]
	TIME [epoch: 8.72 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3797934688453168		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 0.43479364295644746		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 0.4072935559008822 | validation: 0.30551034876297745]
	TIME [epoch: 8.73 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28846444434924684		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 0.3990952546146409		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 0.3437798494819438 | validation: 0.38176822726644694]
	TIME [epoch: 8.72 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28691324309822935		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 0.3519188828292553		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 0.31941606296374225 | validation: 0.27594048517750747]
	TIME [epoch: 8.72 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3831555674761945		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 0.38787310902030664		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 0.3855143382482506 | validation: 0.22505825960522682]
	TIME [epoch: 8.73 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30323763061174314		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.3980945962856991		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 0.3506661134487211 | validation: 0.2637738171947246]
	TIME [epoch: 8.76 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37901493277717513		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 0.3614234110194904		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 0.3702191718983327 | validation: 0.3413548036388219]
	TIME [epoch: 8.75 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37179652724892226		[learning rate: 0.00594]
		[batch 20/20] avg loss: 0.5472935994530157		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 0.4595450633509691 | validation: 0.34355032413399367]
	TIME [epoch: 8.72 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.325770509202066		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 0.3731829684890257		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 0.34947673884554586 | validation: 0.232605037020229]
	TIME [epoch: 8.73 sec]
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30355976231191484		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 0.33303051811892515		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 0.31829514021542005 | validation: 0.36587978938148225]
	TIME [epoch: 8.72 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3171918893466199		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.30948419558826745		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.3133380424674436 | validation: 0.37640056267845384]
	TIME [epoch: 8.74 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42742431385180985		[learning rate: 0.005826]
		[batch 20/20] avg loss: 0.30746196885711347		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 0.36744314135446166 | validation: 0.2773155864861373]
	TIME [epoch: 8.72 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3865180878995626		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 0.3353536764315862		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 0.3609358821655743 | validation: 0.34275562285818195]
	TIME [epoch: 8.72 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5696323642052539		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 0.3168077545287697		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 0.4432200593670118 | validation: 0.21366623054418815]
	TIME [epoch: 8.73 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25581974598025903		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 0.3018784786855976		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.27884911233292836 | validation: 0.23512264832739688]
	TIME [epoch: 8.72 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3212707710129478		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 0.4126680814394872		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 0.3669694262262175 | validation: 0.35862936596540984]
	TIME [epoch: 8.76 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31950718920441296		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 0.31779434104304366		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 0.31865076512372836 | validation: 0.2723148854153258]
	TIME [epoch: 8.73 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25015502598525036		[learning rate: 0.005659]
		[batch 20/20] avg loss: 0.27720842704799015		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 0.2636817265166202 | validation: 0.22924150425772807]
	TIME [epoch: 8.74 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2824655001700611		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 0.355765121992488		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.3191153110812745 | validation: 0.31784177536157243]
	TIME [epoch: 8.74 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3741244529124691		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 0.47466552647381616		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 0.42439498969314265 | validation: 0.28805503690085155]
	TIME [epoch: 8.73 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37462079467320597		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 0.44743490925530766		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 0.41102785196425684 | validation: 0.34393261221273214]
	TIME [epoch: 8.76 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2755109616435766		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 0.31281954109411		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 0.2941652513688433 | validation: 0.4018404427861933]
	TIME [epoch: 8.73 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3995306277671259		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 0.4221777535488099		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 0.41085419065796785 | validation: 0.2687771009702939]
	TIME [epoch: 8.73 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29687875081215426		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 0.46819154780245914		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 0.3825351493073067 | validation: 0.3764880778867133]
	TIME [epoch: 8.75 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3135735842859232		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.3509094439051276		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.3322415140955254 | validation: 0.2602042066169323]
	TIME [epoch: 8.73 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3005573084362298		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.3500576257895962		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.3253074671129131 | validation: 0.28566842117306324]
	TIME [epoch: 8.75 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41905987443143466		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.28616299900706355		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.35261143671924916 | validation: 0.19903541021623808]
	TIME [epoch: 8.72 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22763957188113496		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 0.3545071049826899		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.29107333843191235 | validation: 0.2385759300069509]
	TIME [epoch: 8.73 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3690091941639793		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.35017678801921137		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 0.35959299109159537 | validation: 0.26259942488076454]
	TIME [epoch: 8.72 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.259391647343744		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 0.29704858674649887		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 0.27822011704512145 | validation: 0.3292181581063062]
	TIME [epoch: 8.73 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5459273597168414		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.3466804649941025		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.44630391235547184 | validation: 0.25343728988226094]
	TIME [epoch: 8.74 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3000186978269026		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 0.2757337412926417		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 0.28787621955977216 | validation: 0.4161489171750634]
	TIME [epoch: 8.73 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2741666182363781		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.3310970912121087		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.3026318547242434 | validation: 0.3021419532038141]
	TIME [epoch: 8.74 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4174028618032102		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 0.40999667033552073		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.41369976606936554 | validation: 0.22404375572211804]
	TIME [epoch: 8.73 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32685896250794627		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 0.34931979612451014		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 0.3380893793162283 | validation: 0.3185745026759904]
	TIME [epoch: 8.74 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33566772260557687		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 0.26816742875116845		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 0.30191757567837263 | validation: 0.323883964824378]
	TIME [epoch: 8.73 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26664406527900364		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.39757438026676584		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 0.33210922277288474 | validation: 0.2353574370856957]
	TIME [epoch: 8.76 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32676574847334827		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.2972657894227023		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 0.3120157689480253 | validation: 0.39361524242944096]
	TIME [epoch: 8.73 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36768356339111574		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.39000157034940686		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.37884256687026124 | validation: 0.2774834695815909]
	TIME [epoch: 8.72 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27658800695420804		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 0.3411111720702281		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 0.3088495895122181 | validation: 0.578012458643858]
	TIME [epoch: 8.73 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38491095365830574		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.5855978675923156		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.48525441062531066 | validation: 0.18427734048317596]
	TIME [epoch: 8.72 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2602332486924835		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 0.3007559865593443		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 0.2804946176259139 | validation: 0.3060664555248685]
	TIME [epoch: 8.74 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41612378010515805		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 0.354061882738849		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 0.38509283142200346 | validation: 0.2623801375165706]
	TIME [epoch: 8.73 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2629261051441494		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.25719898158652654		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 0.26006254336533796 | validation: 0.2090760964059612]
	TIME [epoch: 8.73 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2661428885309097		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.3403136379105647		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.30322826322073715 | validation: 0.19370486314773663]
	TIME [epoch: 8.73 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22013503174144136		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.3197170148110643		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.26992602327625276 | validation: 0.262755348884556]
	TIME [epoch: 8.74 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39824831852353787		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 0.3492784680001074		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 0.3737633932618226 | validation: 0.32703100590403267]
	TIME [epoch: 8.77 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37854663662045696		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.3184798751227225		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.34851325587158966 | validation: 0.21191151622577917]
	TIME [epoch: 8.74 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32961055709444226		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 0.3317335913217357		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.330672074208089 | validation: 0.4175630548892679]
	TIME [epoch: 8.74 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33678779615408055		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 0.36674623270060625		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 0.3517670144273434 | validation: 0.476398196699069]
	TIME [epoch: 8.74 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3557751015777176		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.2541296638543135		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.30495238271601555 | validation: 0.2550656660115622]
	TIME [epoch: 8.72 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3366818652282608		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 0.31834378935010654		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 0.3275128272891837 | validation: 0.2780222224208718]
	TIME [epoch: 8.75 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32463405897064934		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.3168647227922542		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.32074939088145177 | validation: 0.3790937058866738]
	TIME [epoch: 8.74 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39102625415957204		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.32109260219086533		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 0.3560594281752188 | validation: 0.41475961489129615]
	TIME [epoch: 8.73 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2950794143979998		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.3132073106714722		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.30414336253473606 | validation: 0.29005845280920656]
	TIME [epoch: 8.73 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24455422594538706		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.25424981676131886		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.24940202135335293 | validation: 0.3083111160500747]
	TIME [epoch: 8.73 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3307390233046665		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.29314888025580954		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.3119439517802381 | validation: 0.2583509542250031]
	TIME [epoch: 8.74 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29597184177696895		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 0.2834689567320889		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 0.28972039925452886 | validation: 0.23356044493663686]
	TIME [epoch: 8.74 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30234661974120847		[learning rate: 0.004639]
		[batch 20/20] avg loss: 0.30069239098849354		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 0.30151950536485095 | validation: 0.21984759278040186]
	TIME [epoch: 8.73 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27256332247560683		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 0.38131960061512665		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 0.3269414615453668 | validation: 0.1692780211557628]
	TIME [epoch: 8.73 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23951698485839729		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.303687837670619		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.2716024112645082 | validation: 0.2673546182129933]
	TIME [epoch: 8.73 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24863526349653547		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.3861965280508576		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 0.3174158957736965 | validation: 0.2887385547478722]
	TIME [epoch: 8.74 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29268861490508946		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.33369764308945027		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.31319312899726987 | validation: 0.3849269503895042]
	TIME [epoch: 8.74 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27126736656924466		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 0.2585261691427053		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 0.264896767855975 | validation: 0.39600886992563117]
	TIME [epoch: 8.74 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24443054121804067		[learning rate: 0.004506]
		[batch 20/20] avg loss: 0.26914998908944476		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 0.2567902651537427 | validation: 0.2309775207369657]
	TIME [epoch: 8.73 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2592827756454542		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.273473989213821		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.26637838242963763 | validation: 0.39914313219439]
	TIME [epoch: 8.73 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.281248813454964		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 0.24981928588412416		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 0.2655340496695441 | validation: 0.2557725653905018]
	TIME [epoch: 8.74 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30016670780481		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 0.220961070943338		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 0.260563889374074 | validation: 0.20253733634828097]
	TIME [epoch: 8.76 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2838419528940037		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 0.22803275409980356		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 0.25593735349690366 | validation: 0.19376131802250351]
	TIME [epoch: 8.73 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25206881630665234		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 0.30581716477123533		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 0.2789429905389439 | validation: 0.2296233751387545]
	TIME [epoch: 8.73 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.302737167862792		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 0.3998580041807461		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 0.351297586021769 | validation: 0.2063722464295412]
	TIME [epoch: 8.72 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29924171360662755		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 0.27302254809200693		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.2861321308493173 | validation: 0.2564788274857109]
	TIME [epoch: 8.73 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3208274940439992		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 0.39761702550704053		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 0.3592222597755198 | validation: 0.20008747785829895]
	TIME [epoch: 8.76 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643720907827153		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 0.3111776738090467		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 0.287774882295881 | validation: 0.1623052627131226]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_224.pth
	Model improved!!!
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2311543597647973		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 0.27762279756358244		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.25438857866418985 | validation: 0.24889646057568632]
	TIME [epoch: 8.74 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25142996420288044		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.2579956575076475		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.2547128108552639 | validation: 0.18308617101286634]
	TIME [epoch: 8.73 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27533234667322515		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 0.24459711577242974		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 0.2599647312228274 | validation: 0.23733116000086535]
	TIME [epoch: 8.73 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2441665636747845		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 0.34396600121072435		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 0.2940662824427544 | validation: 0.21894868304559303]
	TIME [epoch: 8.75 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2914748315084534		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 0.30173042151919394		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 0.2966026265138237 | validation: 0.21099392469471637]
	TIME [epoch: 8.72 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2747448571173333		[learning rate: 0.00419]
		[batch 20/20] avg loss: 0.227331247156134		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.2510380521367337 | validation: 0.24857249724223954]
	TIME [epoch: 8.72 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2551385103833592		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.21806267056732817		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.23660059047534365 | validation: 0.18290258963633388]
	TIME [epoch: 8.72 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3297936994805462		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.3163467480861782		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.3230702237833622 | validation: 0.28623066460530583]
	TIME [epoch: 8.72 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2634599814644054		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.26333606364822565		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.2633980225563156 | validation: 1.2651221761166043]
	TIME [epoch: 8.75 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5002853430985625		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.2581916441032705		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.37923849360091644 | validation: 0.22653909586214124]
	TIME [epoch: 8.71 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24647149975358537		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.33972873251256674		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 0.29310011613307607 | validation: 0.19293771309965294]
	TIME [epoch: 8.71 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22276383453253157		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.2217322887337391		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.2222480616331354 | validation: 0.25771372327338604]
	TIME [epoch: 8.72 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2821499189986066		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.2804851438637374		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.28131753143117205 | validation: 0.40954353126358545]
	TIME [epoch: 8.72 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26792707423269146		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 0.33189774576285164		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.2999124099977715 | validation: 0.20084271098737358]
	TIME [epoch: 8.74 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25464062615268357		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.3144136088465204		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.28452711749960197 | validation: 0.16523621711080108]
	TIME [epoch: 8.71 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2367564276835064		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.35377009791161584		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.29526326279756115 | validation: 0.3311686243875187]
	TIME [epoch: 8.71 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2963601783583269		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 0.4290038724603963		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.36268202540936156 | validation: 0.28132212331462425]
	TIME [epoch: 8.72 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2505985531570605		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.26229086649836364		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.256444709827712 | validation: 0.2297956697904162]
	TIME [epoch: 8.72 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29676974443566984		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.27129543883577834		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.2840325916357241 | validation: 0.32379216423184026]
	TIME [epoch: 8.73 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28437583061238675		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.2814851325501249		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.28293048158125583 | validation: 0.23011708170039086]
	TIME [epoch: 8.71 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31367201866025574		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.24657164075106613		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.2801218297056608 | validation: 0.22314275544609036]
	TIME [epoch: 8.72 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25667429847568723		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.23127668847198574		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.24397549347383646 | validation: 0.3473896489702018]
	TIME [epoch: 8.72 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2810092709803885		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.21448708108383102		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.24774817603210977 | validation: 0.20625753651320194]
	TIME [epoch: 8.75 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2644581796484983		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.21866016220311885		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 0.24155917092580856 | validation: 0.2289784685442392]
	TIME [epoch: 8.72 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24007179488159122		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.26906854374367695		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.2545701693126341 | validation: 0.2878165605243705]
	TIME [epoch: 8.71 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24371242912922458		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.30001827151568583		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.2718653503224552 | validation: 0.2816781092783787]
	TIME [epoch: 8.72 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23253782096392195		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.2540616941330087		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.2432997575484653 | validation: 0.3718940587596454]
	TIME [epoch: 8.72 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36385494350674247		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 0.3125038544183335		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.338179398962538 | validation: 0.16247550501350483]
	TIME [epoch: 8.76 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2254948209768699		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.25690808825705025		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.24120145461696002 | validation: 0.2308667365999544]
	TIME [epoch: 8.73 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3059590628265129		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.2925057767027327		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.2992324197646228 | validation: 0.1477446281275489]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_254.pth
	Model improved!!!
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2740138011880143		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.21684974875233873		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.24543177497017651 | validation: 0.2442196782406916]
	TIME [epoch: 8.74 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3376938654636818		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.2967118605817077		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.31720286302269474 | validation: 0.30210205475176016]
	TIME [epoch: 8.73 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2692593112329204		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.2616423979010464		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.2654508545669834 | validation: 0.33012454119435053]
	TIME [epoch: 8.75 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23248714987460278		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.3100997357483999		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.2712934428115013 | validation: 0.26834195702683444]
	TIME [epoch: 8.71 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23404332301866804		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.35820350788617566		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.2961234154524218 | validation: 0.2429794858633964]
	TIME [epoch: 8.72 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2582026141267792		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.21448024188069797		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.23634142800373864 | validation: 0.15259167126469425]
	TIME [epoch: 8.73 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36337786025775226		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.28943599698532424		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.3264069286215382 | validation: 0.26960858124445186]
	TIME [epoch: 8.73 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24019050518300372		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.23487433498974664		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.23753242008637518 | validation: 0.344655019342318]
	TIME [epoch: 8.74 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23827860826420966		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.25036763102263027		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.24432311964342 | validation: 0.24551369588004202]
	TIME [epoch: 8.71 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23318655550986195		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.22915025908214903		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.2311684072960055 | validation: 0.6074009676730735]
	TIME [epoch: 8.72 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2869980121038771		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 0.26654206040769113		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.2767700362557841 | validation: 0.25222596651114065]
	TIME [epoch: 8.73 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21585737136925034		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.22206756283382117		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.21896246710153572 | validation: 0.27630365839721305]
	TIME [epoch: 8.76 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.212075901467428		[learning rate: 0.003502]
		[batch 20/20] avg loss: 0.1908267084594183		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.20145130496342314 | validation: 0.2143435101946377]
	TIME [epoch: 8.74 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2192929377156425		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 0.2079820772835539		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.2136375074995982 | validation: 0.3456489419486032]
	TIME [epoch: 8.72 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2886851723122125		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.25422230458546746		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.27145373844883997 | validation: 0.21952697886118297]
	TIME [epoch: 8.72 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1974742930876206		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.27746506121228587		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.2374696771499532 | validation: 0.21007486069511744]
	TIME [epoch: 8.72 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22773589741375969		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.3018316161938144		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.264783756803787 | validation: 0.2223624347061317]
	TIME [epoch: 8.75 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23173110634933852		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.3623909886474462		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.29706104749839235 | validation: 0.20195028867499476]
	TIME [epoch: 8.73 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19329638820803374		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.2156824564096506		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.20448942230884218 | validation: 0.4221379061125668]
	TIME [epoch: 8.72 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2681915214491676		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.24898579413967434		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.25858865779442103 | validation: 0.14649955639992768]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_274.pth
	Model improved!!!
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20934027336216246		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.26520914211521107		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.23727470773868684 | validation: 0.4212457391464455]
	TIME [epoch: 8.75 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26666773451001896		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.22295153693437228		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.24480963572219566 | validation: 0.17424192560758783]
	TIME [epoch: 8.75 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23942056046854643		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.21318784421278192		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.22630420234066415 | validation: 0.22713982233040983]
	TIME [epoch: 8.73 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2777778756229654		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.2373086093366346		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.2575432424797999 | validation: 0.260487171970703]
	TIME [epoch: 8.73 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2099553655638083		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.23767260908317372		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.22381398732349106 | validation: 0.14102693668947097]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_279.pth
	Model improved!!!
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26047216653750555		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.20798718395624105		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.23422967524687333 | validation: 0.21008359414387692]
	TIME [epoch: 8.75 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20470429405845034		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.17144717745204652		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.1880757357552484 | validation: 0.15289366009510824]
	TIME [epoch: 8.76 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15361265612698202		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.20439034786667704		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.17900150199682957 | validation: 0.1868938504163555]
	TIME [epoch: 8.73 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2991334185642275		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.2349118821616114		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.2670226503629195 | validation: 0.25982269513420153]
	TIME [epoch: 8.74 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20153289233169408		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.1839331219814442		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.19273300715656916 | validation: 0.2610163128731181]
	TIME [epoch: 8.73 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2092376622593696		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 0.23060823018990165		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.21992294622463557 | validation: 0.18429226159633955]
	TIME [epoch: 8.74 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1983083797419058		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.2603188598747952		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.22931361980835047 | validation: 0.275417129747168]
	TIME [epoch: 8.75 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.272443563425819		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.1921117511098772		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.23227765726784808 | validation: 0.23324826530270004]
	TIME [epoch: 8.73 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27790632461268594		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.3131622980935295		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.29553431135310776 | validation: 0.21320288790729974]
	TIME [epoch: 8.74 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1981790093419247		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.23947030081555737		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.21882465507874102 | validation: 0.13625392542144535]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_289.pth
	Model improved!!!
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21635774816406048		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 0.23663774572326587		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.22649774694366315 | validation: 0.17334804689806]
	TIME [epoch: 8.75 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22170402686459584		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 0.21385476726684435		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.21777939706572008 | validation: 0.12391618674759106]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_291.pth
	Model improved!!!
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18008589615651627		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.18066626070780584		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.18037607843216105 | validation: 0.14024868097170268]
	TIME [epoch: 8.74 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19798418977508533		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.21090780897617467		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.20444599937563002 | validation: 0.312927750196487]
	TIME [epoch: 8.75 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20772400426938584		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 0.1733186533023999		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.19052132878589284 | validation: 0.1706967590898464]
	TIME [epoch: 8.75 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21898771631273167		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.20132386092390692		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.21015578861831927 | validation: 0.27232374245755364]
	TIME [epoch: 8.77 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.303464967860256		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.22870874279420272		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.26608685532722937 | validation: 0.23648499815601196]
	TIME [epoch: 8.74 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24828889101544158		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.2218809612238723		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.23508492611965695 | validation: 0.18677791275032787]
	TIME [epoch: 8.73 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21131258765735228		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.21044879103125208		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.21088068934430218 | validation: 0.28364396542034415]
	TIME [epoch: 8.74 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26058271765194185		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.2143948417247962		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.23748877968836904 | validation: 0.1949247886534371]
	TIME [epoch: 8.73 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2202349470292869		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.25745937833613936		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.23884716268271317 | validation: 0.19911784208440558]
	TIME [epoch: 8.76 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20541056997148005		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.21487583161560467		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.21014320079354235 | validation: 0.14305996992989037]
	TIME [epoch: 8.73 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22744003264255858		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.19270520237952812		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.2100726175110433 | validation: 0.19852372004025226]
	TIME [epoch: 8.73 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20125312731986042		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.2137733659796121		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.20751324664973625 | validation: 0.2988190276917026]
	TIME [epoch: 8.73 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21349945681077154		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.274939943788896		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.24421970029983378 | validation: 0.14343420186834704]
	TIME [epoch: 8.73 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16822880539034737		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.22690984214317916		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.19756932376676326 | validation: 0.26477836325577997]
	TIME [epoch: 8.75 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23206148748959107		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.19559728334870302		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.21382938541914703 | validation: 0.1766006551116032]
	TIME [epoch: 8.74 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2043280640301306		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.18621678894196506		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.19527242648604787 | validation: 0.16211299588697625]
	TIME [epoch: 8.74 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18128600055349012		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.2336050331300855		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.20744551684178783 | validation: 0.2659232523335815]
	TIME [epoch: 8.74 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2019482866244724		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.277239274775347		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.2395937806999097 | validation: 0.18063819690863556]
	TIME [epoch: 8.75 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22373202795677583		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.21541796285336218		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.21957499540506897 | validation: 0.4352370472982231]
	TIME [epoch: 8.74 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2193692307088977		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.15344650009074337		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.18640786539982052 | validation: 0.2100564290913597]
	TIME [epoch: 8.73 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2301298745360764		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.1875126978434351		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.20882128618975576 | validation: 0.24054621784277705]
	TIME [epoch: 8.76 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20320929053978234		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.17052356502674465		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.1868664277832635 | validation: 0.17937400281078095]
	TIME [epoch: 8.75 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18098280731040778		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.20763629247951174		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.19430954989495974 | validation: 0.22653996193155845]
	TIME [epoch: 8.75 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18362605879830082		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.16975196720308505		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.17668901300069292 | validation: 0.26781532643735856]
	TIME [epoch: 8.74 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20919016546390598		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.17288244010753215		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.19103630278571906 | validation: 0.2742260764413046]
	TIME [epoch: 8.72 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20986959598572516		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.22690786393755125		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.21838872996163822 | validation: 0.2068155926365597]
	TIME [epoch: 8.74 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21090219673139893		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.1841553788548421		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.1975287877931205 | validation: 0.18204436382305275]
	TIME [epoch: 8.73 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18424065729576075		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.2072114400887571		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.1957260486922589 | validation: 0.144686833075077]
	TIME [epoch: 8.76 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17802127253275568		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.24306427080017595		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.2105427716664658 | validation: 0.11012934305954597]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_320.pth
	Model improved!!!
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17137514989167563		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.16479017114491537		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.16808266051829548 | validation: 0.15621224701475686]
	TIME [epoch: 8.74 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19805455801097654		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.2057764891389254		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.20191552357495096 | validation: 0.17193435608728916]
	TIME [epoch: 8.74 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1608868273970797		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.2988690778099211		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.22987795260350047 | validation: 0.28499506289900445]
	TIME [epoch: 8.73 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16585508129763946		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.20619939259428693		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.18602723694596318 | validation: 0.14099630405159336]
	TIME [epoch: 8.75 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1636673671598484		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.1817234155307119		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.17269539134528014 | validation: 0.48305453894494876]
	TIME [epoch: 8.73 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22755680401415304		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.22710561895888243		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.22733121148651775 | validation: 0.17156037054707057]
	TIME [epoch: 8.72 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19291231987267304		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.17317317558207712		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.18304274772737503 | validation: 0.12631369651132526]
	TIME [epoch: 8.74 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20874160727246133		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.19802150649617145		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.20338155688431633 | validation: 0.19838313814478953]
	TIME [epoch: 8.74 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19116440498047482		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.19936356726064958		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.1952639861205622 | validation: 0.1767355751233512]
	TIME [epoch: 8.75 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20042754105549448		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.1951540222599913		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.19779078165774291 | validation: 0.5262421369264197]
	TIME [epoch: 8.72 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2071288698165507		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.1989414069858611		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.2030351384012059 | validation: 0.3386997813627422]
	TIME [epoch: 8.72 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20648919161723592		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.18943950135764728		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.19796434648744157 | validation: 0.2682845932604112]
	TIME [epoch: 8.73 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19099259814803002		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.1855957075612473		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.18829415285463866 | validation: 0.28623405865376217]
	TIME [epoch: 8.76 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22137883637579417		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.22038976107638		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.22088429872608706 | validation: 0.3216138654049704]
	TIME [epoch: 8.75 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1927289171266899		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.1997377990309594		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.19623335807882464 | validation: 0.14069168766204657]
	TIME [epoch: 8.74 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15578527795829705		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.2291666053030072		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.19247594163065213 | validation: 0.2878217242152033]
	TIME [epoch: 8.73 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21932238308834706		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.1687907568720397		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.19405656998019338 | validation: 0.23220437417562528]
	TIME [epoch: 8.73 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16855630854742507		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.16158708059317767		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.1650716945703014 | validation: 0.17318109798686698]
	TIME [epoch: 8.74 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.191073720827064		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.17346282957667267		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.18226827520186833 | validation: 0.16981607532942805]
	TIME [epoch: 8.73 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14601747756752187		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.19276424500293207		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.16939086128522698 | validation: 0.20717333783451242]
	TIME [epoch: 8.73 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18369246228440816		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.14667429903405993		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.16518338065923405 | validation: 0.1430182938401506]
	TIME [epoch: 8.73 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14747129952653254		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.19455417533921546		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.17101273743287404 | validation: 0.17940795445069752]
	TIME [epoch: 8.74 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19398509110932535		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.18580018741562826		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.18989263926247685 | validation: 0.1602236784747817]
	TIME [epoch: 8.74 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18496287634369005		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.23134634525952338		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.20815461080160672 | validation: 0.3115767073976992]
	TIME [epoch: 8.73 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18376436247146874		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.17805883020711175		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.18091159633929024 | validation: 0.2980190289119089]
	TIME [epoch: 8.73 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20448662798364564		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.211324922836646		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.20790577541014582 | validation: 0.18711714337140392]
	TIME [epoch: 8.73 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1840390434473807		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.19586809139781516		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.18995356742259795 | validation: 0.17898892270402134]
	TIME [epoch: 8.74 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17922554974939503		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.18611115056182204		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.18266835015560853 | validation: 0.16975565308753096]
	TIME [epoch: 8.75 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18352378367080494		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.18097453358988386		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.18224915863034438 | validation: 0.22107076895056588]
	TIME [epoch: 8.74 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1943785515473829		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.16527567666101062		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.1798271141041968 | validation: 0.22607449076182176]
	TIME [epoch: 8.72 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17817663465383818		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.17556016628549215		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.17686840046966512 | validation: 0.22271030876481046]
	TIME [epoch: 8.73 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21747271562036025		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.13081409154303297		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.1741434035816966 | validation: 0.24169999278047705]
	TIME [epoch: 8.75 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20417850638559565		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.15862755788457888		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.1814030321350873 | validation: 0.1419098385709529]
	TIME [epoch: 8.73 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14718180119478047		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.1937176464980897		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.17044972384643509 | validation: 0.1584204711913686]
	TIME [epoch: 8.73 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21704897800384632		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.17701648608232695		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.1970327320430866 | validation: 0.14498959894154193]
	TIME [epoch: 8.73 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1692300819861981		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.15239109541379556		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.16081058869999681 | validation: 0.22797684764352166]
	TIME [epoch: 8.73 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16390337879892786		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.2209787677839929		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.19244107329146037 | validation: 0.12873039896153238]
	TIME [epoch: 8.75 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.164188548447391		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.15225414980731136		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.15822134912735117 | validation: 0.1898341460541137]
	TIME [epoch: 8.73 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22777579926363165		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.17744969043867903		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.2026127448511553 | validation: 0.46797603188712766]
	TIME [epoch: 8.73 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2658067830487984		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.15918684877591288		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.21249681591235564 | validation: 0.15197091352706238]
	TIME [epoch: 8.74 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19345676692065644		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.16811500561099554		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.180785886265826 | validation: 0.2038853144456418]
	TIME [epoch: 8.75 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21868401492345288		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.16168510653415283		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.19018456072880283 | validation: 0.15787724184857088]
	TIME [epoch: 8.76 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14961616321917526		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.21624584989608078		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.182931006557628 | validation: 0.18256649762989208]
	TIME [epoch: 8.74 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17511385126946663		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.2860359742184757		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.2305749127439712 | validation: 0.1490606840097076]
	TIME [epoch: 8.73 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1924485355267653		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.18872867237061824		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.1905886039486918 | validation: 0.286652888582267]
	TIME [epoch: 8.73 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20688763624408546		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.13292040176615233		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.1699040190051189 | validation: 0.15410953786714832]
	TIME [epoch: 8.74 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19560991696754115		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.2023512534898301		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.19898058522868564 | validation: 0.19485868981234053]
	TIME [epoch: 8.75 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15409545450761392		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.16962251028422706		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.16185898239592053 | validation: 0.14426882383202563]
	TIME [epoch: 8.73 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15306409524828885		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.15789745015756473		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.15548077270292682 | validation: 0.12599173817945866]
	TIME [epoch: 8.72 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16018692611554458		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.15123195637238043		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.15570944124396252 | validation: 0.13690397017744405]
	TIME [epoch: 8.73 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15827580152094822		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.1843662578457001		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.17132102968332413 | validation: 0.20682229571083038]
	TIME [epoch: 8.75 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16691248385126867		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.13739072061321894		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.15215160223224383 | validation: 0.12007612127642696]
	TIME [epoch: 8.73 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1659059485450053		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.2208047297435852		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.19335533914429529 | validation: 0.208391823371609]
	TIME [epoch: 8.72 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17084861087760456		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.16768851343871527		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.1692685621581599 | validation: 0.13929406964697183]
	TIME [epoch: 8.73 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1330186609876505		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.1492564060159866		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.14113753350181854 | validation: 0.1675265076629739]
	TIME [epoch: 8.74 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15669271504004165		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.1440955912681192		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.1503941531540804 | validation: 0.21182959044198363]
	TIME [epoch: 8.75 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19376346530149471		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.1543881264733593		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.17407579588742703 | validation: 0.12130489537613509]
	TIME [epoch: 8.74 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14419551226351693		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.17152017334211683		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.15785784280281687 | validation: 0.1482956910991473]
	TIME [epoch: 8.73 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16663719094170196		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.18251255516216486		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.17457487305193337 | validation: 0.1351143511497575]
	TIME [epoch: 8.73 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1641352892987646		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.16693902896502907		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.16553715913189684 | validation: 0.18267426991687602]
	TIME [epoch: 8.74 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23123188923804552		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.15589291433469518		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.19356240178637035 | validation: 0.14116292728071678]
	TIME [epoch: 8.75 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16340603818760385		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.1426508193619137		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.15302842877475878 | validation: 0.14927703631169917]
	TIME [epoch: 8.73 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1327050310099767		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.1695472523915087		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.15112614170074265 | validation: 0.13285897165770047]
	TIME [epoch: 8.72 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20936612099682272		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.1734062975966963		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.19138620929675948 | validation: 0.15326659619432603]
	TIME [epoch: 8.72 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19790657402388737		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.16890848508498613		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.18340752955443676 | validation: 0.17034892231715604]
	TIME [epoch: 8.74 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.155284217216813		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.19421897783979225		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.17475159752830258 | validation: 0.17594573359010243]
	TIME [epoch: 8.74 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16081372147636316		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.19493337337279057		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.1778735474245769 | validation: 0.1766757948520133]
	TIME [epoch: 8.74 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18848861687635118		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.14906733819231113		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.16877797753433116 | validation: 0.202738999465353]
	TIME [epoch: 8.73 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14824471927563443		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.1532307891553582		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.15073775421549632 | validation: 0.10326290175373883]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_389.pth
	Model improved!!!
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1947301051370148		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.14610511466061535		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.17041760989881508 | validation: 0.12546638405929225]
	TIME [epoch: 8.76 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14778157185458551		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.1532455783187886		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.15051357508668703 | validation: 0.1600679508186909]
	TIME [epoch: 8.74 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15688242760771087		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.18238044958412716		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.16963143859591906 | validation: 0.22175902836978556]
	TIME [epoch: 8.73 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1789860053004272		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.1387901534410984		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.1588880793707628 | validation: 0.1181783506268634]
	TIME [epoch: 8.72 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17190971434286978		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.14987985392164568		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.16089478413225777 | validation: 0.1459688852559568]
	TIME [epoch: 8.73 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13948752113206922		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.14401085803836622		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.1417491895852177 | validation: 0.19156251702679378]
	TIME [epoch: 8.75 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15596735571802517		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.177972138160975		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.16696974693950012 | validation: 0.164414146483273]
	TIME [epoch: 8.73 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1327613901377252		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.18678117552471993		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.15977128283122258 | validation: 0.1364755667693775]
	TIME [epoch: 8.72 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13526713037632906		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.17764240873370096		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.15645476955501503 | validation: 0.17626130041899024]
	TIME [epoch: 8.71 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1326114216885747		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.14090525510317814		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.13675833839587642 | validation: 0.14364799718908022]
	TIME [epoch: 8.72 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17891187478757226		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.14488023172740389		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.16189605325748807 | validation: 0.11294941115298823]
	TIME [epoch: 8.74 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1470320290381968		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.17000625031068975		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.15851913967444325 | validation: 0.17199757637522775]
	TIME [epoch: 8.72 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16362626755340567		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.14557340942128447		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.15459983848734507 | validation: 0.18192860219669185]
	TIME [epoch: 8.73 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18868243147101002		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.13562352969161295		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.1621529805813115 | validation: 0.23690475932107596]
	TIME [epoch: 8.72 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15722843605779036		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.14072770672796833		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.14897807139287936 | validation: 0.15949765161722082]
	TIME [epoch: 8.73 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15492461045179268		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.16746199836625889		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.16119330440902577 | validation: 0.17778287900280257]
	TIME [epoch: 8.73 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1795681318383226		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.13697053963328215		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.15826933573580235 | validation: 0.15611298888402594]
	TIME [epoch: 8.72 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17390508835775276		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.15078082854740354		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.16234295845257812 | validation: 0.17734908503434954]
	TIME [epoch: 8.72 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14099792923455895		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.14432904367203359		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.14266348645329627 | validation: 0.1950763864747494]
	TIME [epoch: 8.74 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1389903416442188		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.1471718000403644		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.1430810708422916 | validation: 0.12249118807122983]
	TIME [epoch: 8.75 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1436225891990454		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.14029257947696477		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.14195758433800504 | validation: 0.1357234751655486]
	TIME [epoch: 8.73 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13569306078131396		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.14790164394660427		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.14179735236395913 | validation: 0.1637531096623038]
	TIME [epoch: 8.72 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14978392694093187		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.14252528087116179		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.14615460390604681 | validation: 0.21501925896211943]
	TIME [epoch: 8.72 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17543019992871298		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.17828440502603682		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.1768573024773749 | validation: 0.1332755847328017]
	TIME [epoch: 8.72 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1417653981265972		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.12658371361019174		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.13417455586839444 | validation: 0.1443226186258424]
	TIME [epoch: 8.75 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17665685050762953		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.15709318392529742		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.1668750172164635 | validation: 0.18846414078772047]
	TIME [epoch: 8.72 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14466032656893568		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.15248367508747795		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.14857200082820682 | validation: 0.13678489673920363]
	TIME [epoch: 8.73 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13781862100821907		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 0.19440603662126466		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 0.16611232881474186 | validation: 0.12725985595207354]
	TIME [epoch: 8.72 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19613384418454669		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 0.1581335654141171		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 0.17713370479933194 | validation: 0.1821979341665862]
	TIME [epoch: 8.72 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12284886676127704		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 0.12378034292062567		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 0.12331460484095133 | validation: 0.16346370417252087]
	TIME [epoch: 8.73 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1852403571248487		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.15345979355322575		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.1693500753390373 | validation: 0.14130590097921863]
	TIME [epoch: 8.72 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14616447576004118		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.14612089706178227		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.14614268641091174 | validation: 0.1499730649765568]
	TIME [epoch: 8.72 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12710476186553668		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.13310679535310882		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.13010577860932274 | validation: 0.16043937917977885]
	TIME [epoch: 8.71 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1380080939338683		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.1483568488071199		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.1431824713704941 | validation: 0.13728729809675774]
	TIME [epoch: 8.72 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12887907544061042		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.12338629809482213		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.1261326867677163 | validation: 0.1926689375347399]
	TIME [epoch: 8.73 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15032783419359624		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.155174470260952		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.15275115222727412 | validation: 0.11429721215732845]
	TIME [epoch: 8.72 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12404278324117271		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.13587972180581762		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.12996125252349516 | validation: 0.1456255848241552]
	TIME [epoch: 8.71 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16714610497462104		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.15625798278498287		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.16170204387980197 | validation: 0.11879826269723182]
	TIME [epoch: 8.71 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1464507488480699		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.13357710799985412		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.14001392842396204 | validation: 0.13249156008466967]
	TIME [epoch: 8.73 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13175072743016464		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.12907215276887948		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.13041144009952205 | validation: 0.14638759627866155]
	TIME [epoch: 8.73 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1414849772622974		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.1391148347772295		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.14029990601976344 | validation: 0.18819959208233117]
	TIME [epoch: 8.73 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13273053844716318		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.12446659288589965		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.12859856566653144 | validation: 0.12166239134006882]
	TIME [epoch: 8.73 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14772838800786367		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.15919866620799691		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.15346352710793026 | validation: 0.15806140934255294]
	TIME [epoch: 8.73 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13789207449627208		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.14884371648831957		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.1433678954922958 | validation: 0.12362559004439341]
	TIME [epoch: 8.74 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1354166775938412		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.16900903671868117		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.15221285715626118 | validation: 0.12963414235196125]
	TIME [epoch: 8.71 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15570828018203803		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.1620294006532182		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.1588688404176281 | validation: 0.11910486924149478]
	TIME [epoch: 8.72 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15023750726050136		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.13114564217943464		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.14069157471996802 | validation: 0.1867646728478037]
	TIME [epoch: 8.72 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15117052465917036		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.16038024673865678		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.15577538569891355 | validation: 0.1692028863314083]
	TIME [epoch: 8.72 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.128035641637392		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.13879231729140287		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.13341397946439745 | validation: 0.13748828706828312]
	TIME [epoch: 8.73 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12983002198629212		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.15887455608836304		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.14435228903732758 | validation: 0.11725103972963319]
	TIME [epoch: 8.72 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.124378976733406		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.14447052236599384		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.13442474954969993 | validation: 0.13771410196547712]
	TIME [epoch: 8.72 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14446095047117363		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.14335584708128019		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.14390839877622688 | validation: 0.13193411183925505]
	TIME [epoch: 8.72 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12714684692552855		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.14607477175710912		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.13661080934131883 | validation: 0.16426423898816858]
	TIME [epoch: 8.73 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.176378163747584		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.11531968164481325		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.1458489226961986 | validation: 0.15744487906492738]
	TIME [epoch: 8.75 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1523622504600091		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.19061875170874445		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.17149050108437675 | validation: 0.14132658844323306]
	TIME [epoch: 8.72 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11677110896710222		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.13323448940941923		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.12500279918826074 | validation: 0.1058866567630008]
	TIME [epoch: 8.72 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1452571285244547		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.14258255006265466		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.14391983929355465 | validation: 0.12911427909338896]
	TIME [epoch: 8.72 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13934429481856772		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.18260436512916284		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.16097432997386527 | validation: 0.1561608325874716]
	TIME [epoch: 8.74 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14228085482428454		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.13024269332240865		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.1362617740733466 | validation: 0.15614091982894399]
	TIME [epoch: 8.73 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14714842155934682		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.15266779380129375		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.14990810768032026 | validation: 0.13139831124013182]
	TIME [epoch: 8.72 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14891886603496512		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.12717785073537538		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.1380483583851702 | validation: 0.17768107793779966]
	TIME [epoch: 8.72 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13194320538497467		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.13074054979605343		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.13134187759051402 | validation: 0.14519572361194122]
	TIME [epoch: 8.72 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13427480490741953		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.1318481176689007		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.13306146128816013 | validation: 0.17978574700233865]
	TIME [epoch: 8.74 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14577798972910755		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.10855897566884194		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.12716848269897477 | validation: 0.09699980403964845]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_453.pth
	Model improved!!!
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13821464282487445		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.15348804915591296		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.14585134599039368 | validation: 0.09505462982834384]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_454.pth
	Model improved!!!
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12457580328848357		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.13846547941756704		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.1315206413530253 | validation: 0.1416444261536819]
	TIME [epoch: 8.71 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10907577482742983		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.1354454621893572		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.12226061850839354 | validation: 0.1336425737673457]
	TIME [epoch: 8.72 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13891359776541015		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.1238818273122367		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.13139771253882343 | validation: 0.12304932745556882]
	TIME [epoch: 8.74 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1272126690608112		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.14777777071744916		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.13749521988913022 | validation: 0.13584670222150622]
	TIME [epoch: 8.72 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14553158428420837		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.1478810449856022		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.1467063146349053 | validation: 0.10285810382866141]
	TIME [epoch: 8.72 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1111384880970376		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.11354426573656787		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.11234137691680272 | validation: 0.18816561274393903]
	TIME [epoch: 8.71 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16633977042838327		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.15942813593593164		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.16288395318215748 | validation: 0.1668307451158145]
	TIME [epoch: 8.72 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14064083040421657		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.15279374422360162		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.14671728731390912 | validation: 0.15101438801562042]
	TIME [epoch: 8.73 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14686394259656282		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.11988446560666685		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.13337420410161485 | validation: 0.15744909201716667]
	TIME [epoch: 8.72 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13804695566461034		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.15568346684846196		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.14686521125653612 | validation: 0.13088034254430153]
	TIME [epoch: 8.71 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1243204043369017		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.13000124453850326		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.12716082443770244 | validation: 0.1580050150685311]
	TIME [epoch: 8.71 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1347334092517802		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.13846268015808688		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.13659804470493356 | validation: 0.14676766302219196]
	TIME [epoch: 8.73 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13315767394272776		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.10201673817054902		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.11758720605663839 | validation: 0.10299348300679945]
	TIME [epoch: 8.72 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13593177192552622		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.12728867269256816		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.1316102223090472 | validation: 0.1438506557944085]
	TIME [epoch: 8.71 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14829223780225895		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.15121882295393957		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.14975553037809924 | validation: 0.1189860107603869]
	TIME [epoch: 8.71 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13397521915804175		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.10577587434140641		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.11987554674972412 | validation: 0.12489035386008683]
	TIME [epoch: 8.71 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1325610848475767		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.16280167046330873		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.1476813776554427 | validation: 0.16701768838924363]
	TIME [epoch: 8.74 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1441654704520357		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.15206600962610156		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.14811574003906863 | validation: 0.11843227099185161]
	TIME [epoch: 8.72 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11857274412535712		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.1208885922452961		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.11973066818532659 | validation: 0.14318939438359554]
	TIME [epoch: 8.72 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14435363163161524		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.12795425886586825		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.13615394524874175 | validation: 0.11989483992303901]
	TIME [epoch: 8.71 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12170797714743478		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.1293392378304972		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.125523607488966 | validation: 0.10080819081780093]
	TIME [epoch: 8.72 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1336546332521736		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.13118256702737874		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.1324186001397762 | validation: 0.12773197162620722]
	TIME [epoch: 8.73 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12100579564191742		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.11611160622694035		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.11855870093442888 | validation: 0.1417077049939427]
	TIME [epoch: 8.72 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13077440093066522		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.11782942610894107		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.12430191351980316 | validation: 0.14550760194028498]
	TIME [epoch: 8.72 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.132246042326559		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.14517867578365293		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.13871235905510598 | validation: 0.160433748240277]
	TIME [epoch: 8.71 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285923935865697		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.13526632497436375		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.13192935928046673 | validation: 0.11663475711267834]
	TIME [epoch: 8.71 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1375271119534422		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.13929918895677254		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.13841315045510735 | validation: 0.16110987448446198]
	TIME [epoch: 8.74 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11513835475451462		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.1099310176006365		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.11253468617757553 | validation: 0.1589650215334037]
	TIME [epoch: 8.71 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17120832868019137		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.12842714865200205		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.1498177386660967 | validation: 0.10405645101211095]
	TIME [epoch: 8.71 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12170769078206609		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.14103659597164614		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.13137214337685607 | validation: 0.10789111289364714]
	TIME [epoch: 8.72 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13224122676008157		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 0.11459758950940632		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 0.12341940813474397 | validation: 0.1525063252815782]
	TIME [epoch: 8.74 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1120495736869834		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 0.11796914739149307		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 0.11500936053923824 | validation: 0.15100907856528764]
	TIME [epoch: 8.74 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11158344557376079		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.1516674472534723		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.13162544641361656 | validation: 0.15830752416424715]
	TIME [epoch: 8.72 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13216368686631708		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.12884499548003986		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.13050434117317847 | validation: 0.2230665887697028]
	TIME [epoch: 8.72 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14227904517058837		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.13711993194987734		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.13969948856023287 | validation: 0.09632485955766988]
	TIME [epoch: 8.71 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11291548197132277		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.1223376296881565		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.1176265558297396 | validation: 0.12278948874485443]
	TIME [epoch: 8.75 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09657081511079725		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.09168648696050788		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.09412865103565257 | validation: 0.12167246517601049]
	TIME [epoch: 8.73 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11640274720308072		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 0.13092851623011367		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 0.1236656317165972 | validation: 0.13591959304924836]
	TIME [epoch: 8.72 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1238201594216916		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 0.10993543513845343		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 0.1168777972800725 | validation: 0.12211617998199685]
	TIME [epoch: 8.71 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15652822220220475		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 0.1386084230468673		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 0.14756832262453604 | validation: 0.1194491205753473]
	TIME [epoch: 8.71 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11136420029610908		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 0.112785760406547		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 0.11207498035132804 | validation: 0.11550078681349396]
	TIME [epoch: 8.74 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11295730484753248		[learning rate: 0.001154]
		[batch 20/20] avg loss: 0.12661820401291063		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 0.11978775443022154 | validation: 0.10926116030878513]
	TIME [epoch: 8.72 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1308303608566876		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 0.1672514894350145		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 0.14904092514585107 | validation: 0.18541794268056477]
	TIME [epoch: 8.71 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11866813064324247		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.10873505569946618		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 0.11370159317135434 | validation: 0.08870791875038728]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_498.pth
	Model improved!!!
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12219197737332463		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.12168222508859375		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.12193710123095917 | validation: 0.14356432645158645]
	TIME [epoch: 8.74 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13292945671154577		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.13281068436356394		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.13287007053755484 | validation: 0.14273003971202392]
	TIME [epoch: 8.75 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1256193365748987		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.1259542470868998		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.12578679183089927 | validation: 0.12221700161548987]
	TIME [epoch: 8.72 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12326032440891163		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.12664569181148802		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.12495300811019983 | validation: 0.10103294838507326]
	TIME [epoch: 8.72 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.118455962430893		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.1077297215626751		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.11309284199678404 | validation: 0.16752232166016]
	TIME [epoch: 8.72 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10517965290754401		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.12099792619963004		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.113088789553587 | validation: 0.1078744777287609]
	TIME [epoch: 8.73 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1081713158723829		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.10718491241984562		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.10767811414611425 | validation: 0.16514606829596903]
	TIME [epoch: 8.74 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13141069831678		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.09780248469173958		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.11460659150425978 | validation: 0.09840715331883845]
	TIME [epoch: 8.72 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10966407603408021		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.1216051896951285		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.11563463286460438 | validation: 0.13113083698956915]
	TIME [epoch: 8.72 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1113798316810406		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.11433991954784166		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.11285987561444114 | validation: 0.17447459213132352]
	TIME [epoch: 8.71 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14833732255949028		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 0.11341377833673785		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 0.13087555044811405 | validation: 0.11477685561029213]
	TIME [epoch: 8.74 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877086637652504		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.11741811916044687		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 0.11309449276848596 | validation: 0.10473580625017707]
	TIME [epoch: 8.73 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10635712895784566		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 0.11420069030181199		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 0.11027890962982882 | validation: 0.10769782678289994]
	TIME [epoch: 8.72 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11260999372568617		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 0.1246939988699995		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 0.11865199629784284 | validation: 0.1274827342367725]
	TIME [epoch: 8.72 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11424176858125656		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 0.12186753101388426		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 0.11805464979757044 | validation: 0.13079059306139934]
	TIME [epoch: 8.72 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12735751836810769		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.12084811552673957		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.12410281694742362 | validation: 0.12527227206949612]
	TIME [epoch: 8.74 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1268891425984544		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 0.102499069911836		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.11469410625514517 | validation: 0.1135094296271192]
	TIME [epoch: 8.72 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10315181433564465		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 0.11498386679915282		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 0.10906784056739874 | validation: 0.12879132461194764]
	TIME [epoch: 8.73 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13204610496130634		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.10736856591538686		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 0.1197073354383466 | validation: 0.10786195103527599]
	TIME [epoch: 8.72 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12116341371557544		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.11869158459272237		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.11992749915414887 | validation: 0.10449925903177323]
	TIME [epoch: 8.71 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12195204038664276		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 0.12703746348691747		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 0.12449475193678013 | validation: 0.12523637822111597]
	TIME [epoch: 8.74 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13426879481076429		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 0.13651674133724684		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 0.13539276807400558 | validation: 0.12831544401650147]
	TIME [epoch: 8.72 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12192109608874033		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 0.11044956590164139		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 0.11618533099519086 | validation: 0.10391511442275167]
	TIME [epoch: 8.72 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12225344787207167		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 0.11714292090649218		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 0.11969818438928195 | validation: 0.10519746898151686]
	TIME [epoch: 8.71 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11361891923821707		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 0.11003428914925481		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 0.11182660419373594 | validation: 0.10204154608091391]
	TIME [epoch: 8.73 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1176432451239922		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 0.10324666764457728		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 0.11044495638428475 | validation: 0.1262946964835438]
	TIME [epoch: 8.74 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12222838137083783		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 0.11488262048052425		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 0.11855550092568098 | validation: 0.08846231176696556]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_525.pth
	Model improved!!!
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10782382091132474		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 0.11552090418611445		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.11167236254871962 | validation: 0.14449740933494237]
	TIME [epoch: 8.73 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11213034906870674		[learning rate: 0.000993]
		[batch 20/20] avg loss: 0.11373133724129478		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 0.11293084315500075 | validation: 0.10794624621174911]
	TIME [epoch: 8.72 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11317374381005978		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 0.09429543226352853		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 0.10373458803679417 | validation: 0.08877285249384755]
	TIME [epoch: 8.73 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11002129586917202		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.1198118214925149		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 0.11491655868084345 | validation: 0.10732514436010444]
	TIME [epoch: 8.74 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12409795689754632		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 0.13213747622894115		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 0.12811771656324372 | validation: 0.102277995399735]
	TIME [epoch: 8.71 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1219809974050123		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 0.13987602550743056		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 0.13092851145622147 | validation: 0.15380870724458645]
	TIME [epoch: 8.7 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10602161686027756		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 0.11443081910194194		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 0.11022621798110974 | validation: 0.09194151486491473]
	TIME [epoch: 8.7 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09299284647403158		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 0.12989869583239658		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 0.11144577115321408 | validation: 0.15321420324158586]
	TIME [epoch: 8.74 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11410229840063912		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 0.10954735615621809		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 0.11182482727842862 | validation: 0.11173425168217238]
	TIME [epoch: 8.71 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10846675527443939		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 0.10051080688365141		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 0.10448878107904543 | validation: 0.11777035320347656]
	TIME [epoch: 8.71 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12294649448773005		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.13025971906451655		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 0.1266031067761233 | validation: 0.12096749520830673]
	TIME [epoch: 8.71 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11470832832798461		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.10044312912257931		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 0.10757572872528195 | validation: 0.12366377492086604]
	TIME [epoch: 8.71 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10501973998809586		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.10317163512364136		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.1040956875558686 | validation: 0.08629733595861246]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_538.pth
	Model improved!!!
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09467851032481685		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.11621819785098815		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.1054483540879025 | validation: 0.10837231949827261]
	TIME [epoch: 8.73 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11411002393002989		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.12895307297165512		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.12153154845084249 | validation: 0.13376309328681957]
	TIME [epoch: 8.73 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11462690536592528		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.11912577784427698		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.11687634160510114 | validation: 0.09308195679147094]
	TIME [epoch: 8.72 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11678995871459015		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.09541717068466499		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.10610356469962756 | validation: 0.11884085927181343]
	TIME [epoch: 8.72 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11401389032154838		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.09381253116987007		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.10391321074570921 | validation: 0.12095106680892918]
	TIME [epoch: 8.73 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1090378988962916		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.1169004818809444		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.11296919038861801 | validation: 0.13761475806022316]
	TIME [epoch: 8.71 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10628602845321049		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.11038007929828761		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.10833305387574906 | validation: 0.11849670416078624]
	TIME [epoch: 8.72 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11354151675452717		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.09647062436822006		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.1050060705613736 | validation: 0.0948106355642796]
	TIME [epoch: 8.72 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09962186468208296		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 0.1253260537025334		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.11247395919230821 | validation: 0.11026258616782814]
	TIME [epoch: 8.73 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10299253671375155		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 0.10897057160727339		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 0.10598155416051247 | validation: 0.09496145277775232]
	TIME [epoch: 8.73 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09648496832735422		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 0.09511172214291544		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 0.09579834523513482 | validation: 0.09914049626780147]
	TIME [epoch: 8.71 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10446884479521032		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.10705608347196541		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.10576246413358788 | validation: 0.10343871136058536]
	TIME [epoch: 8.71 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0880066634961499		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.09879172239516015		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.09339919294565502 | validation: 0.11308932115786945]
	TIME [epoch: 8.72 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10895203191665417		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.12535659431541252		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.11715431311603337 | validation: 0.11948371237010157]
	TIME [epoch: 8.74 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0971093089124053		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.10605567909769138		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.10158249400504835 | validation: 0.12295887049521691]
	TIME [epoch: 8.73 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10127322409000387		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.11412251748293327		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.10769787078646857 | validation: 0.13186397354530294]
	TIME [epoch: 8.71 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09306894787510729		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.10358647241949448		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.09832771014730089 | validation: 0.09528805466670846]
	TIME [epoch: 8.72 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10464928283698216		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.10212310533024642		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.10338619408361431 | validation: 0.12865815469871855]
	TIME [epoch: 8.71 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10449631801212034		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.09348534772917548		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.09899083287064793 | validation: 0.07978299183494895]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_557.pth
	Model improved!!!
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0941014180238741		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.0961900968828655		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.09514575745336978 | validation: 0.09539748399601515]
	TIME [epoch: 8.72 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10205630765224187		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.12512234838679404		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.11358932801951797 | validation: 0.10081196712971102]
	TIME [epoch: 8.72 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09858652969561067		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 0.10349858690010479		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 0.10104255829785776 | validation: 0.09721425640640799]
	TIME [epoch: 8.73 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09929300153795298		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 0.11983620653388001		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 0.1095646040359165 | validation: 0.111618250281732]
	TIME [epoch: 8.72 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960566543570232		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 0.1448590702365149		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 0.12045786229676905 | validation: 0.08396054968648692]
	TIME [epoch: 8.75 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11515346740448389		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 0.09047796904475473		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 0.10281571822461932 | validation: 0.08923786035526426]
	TIME [epoch: 8.71 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10138531944673854		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 0.11119526291970935		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.10629029118322397 | validation: 0.1436181948896983]
	TIME [epoch: 8.71 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11868079345729157		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 0.11740928638752952		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 0.11804503992241056 | validation: 0.10502134002016156]
	TIME [epoch: 8.71 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11677038124200786		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 0.10370958899908696		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 0.11023998512054742 | validation: 0.09738533107446334]
	TIME [epoch: 8.72 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10864726121109891		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 0.11420423649423876		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 0.11142574885266884 | validation: 0.1358673263653898]
	TIME [epoch: 8.75 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1174397903832944		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 0.09707113372484692		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 0.10725546205407063 | validation: 0.11104577006660653]
	TIME [epoch: 8.74 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10191804437238441		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 0.10437677087800121		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 0.10314740762519281 | validation: 0.09711850272117056]
	TIME [epoch: 8.73 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10288205015714798		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 0.09821893030900855		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 0.10055049023307827 | validation: 0.10725310054849838]
	TIME [epoch: 8.72 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12191647023545338		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 0.09011115198359722		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 0.1060138111095253 | validation: 0.09968273977025366]
	TIME [epoch: 8.74 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11061445775845684		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 0.10486767310202186		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 0.10774106543023934 | validation: 0.11566680490265899]
	TIME [epoch: 8.72 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14702774619324208		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 0.1115428598453537		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 0.12928530301929791 | validation: 0.09344698957637337]
	TIME [epoch: 8.71 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11308210509916383		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.09448642074820546		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 0.10378426292368466 | validation: 0.12283748860994874]
	TIME [epoch: 8.73 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11407458663502554		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 0.0911320004043316		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 0.10260329351967859 | validation: 0.09792220399399007]
	TIME [epoch: 8.72 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11526213896535883		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 0.09609945010029727		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 0.10568079453282808 | validation: 0.08861255440208961]
	TIME [epoch: 8.74 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0897940768612229		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 0.10230974412529512		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 0.09605191049325901 | validation: 0.09552501622508275]
	TIME [epoch: 8.72 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11291598173379065		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 0.09327334645311995		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 0.10309466409345532 | validation: 0.0913869280816796]
	TIME [epoch: 8.72 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11071472263054796		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 0.12255621614151038		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 0.11663546938602916 | validation: 0.13103847497892965]
	TIME [epoch: 8.72 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1110270895778924		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 0.09517609942243208		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 0.10310159450016225 | validation: 0.12700660050910184]
	TIME [epoch: 8.73 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10232857602066521		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 0.08597993011952405		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 0.09415425307009463 | validation: 0.09538615616720242]
	TIME [epoch: 8.75 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09359198635330002		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 0.10215773575579332		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 0.09787486105454665 | validation: 0.1176724934321661]
	TIME [epoch: 8.7 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13338683781556868		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 0.10114548635329526		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.11726616208443194 | validation: 0.1202235702138137]
	TIME [epoch: 8.72 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10583591682096538		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.11438313627066483		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 0.11010952654581511 | validation: 0.1380014485168492]
	TIME [epoch: 8.73 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10243835600523285		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.09748887487881809		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.09996361544202545 | validation: 0.09538413374487319]
	TIME [epoch: 8.73 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11180996772102936		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.0962889032711791		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.10404943549610421 | validation: 0.09250823200252503]
	TIME [epoch: 8.74 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09893463640915567		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.09615544518151468		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.09754504079533519 | validation: 0.0929241215096222]
	TIME [epoch: 8.72 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11202051533338189		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.12419588906838966		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.11810820220088576 | validation: 0.18115210982387484]
	TIME [epoch: 8.72 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13475972085642812		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.1065733043159343		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.12066651258618122 | validation: 0.12983409807928212]
	TIME [epoch: 8.71 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0986944136487747		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.09344321307722242		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.09606881336299856 | validation: 0.11101867436679021]
	TIME [epoch: 8.72 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09178272351116548		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.10271224053078724		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.09724748202097637 | validation: 0.11706491146338142]
	TIME [epoch: 8.74 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0930113444550041		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.08226407440958905		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.08763770943229657 | validation: 0.0782571480220906]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_592.pth
	Model improved!!!
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08922474062936724		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.0965947088239972		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.0929097247266822 | validation: 0.10501801765799659]
	TIME [epoch: 8.73 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09502211163388713		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.09908362831548187		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.09705286997468451 | validation: 0.09946689733067504]
	TIME [epoch: 8.72 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09734734464224226		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.08559264952233293		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.09146999708228759 | validation: 0.12018777336852608]
	TIME [epoch: 8.74 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09335942622268532		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.11171194058789424		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.10253568340528978 | validation: 0.10639120927696721]
	TIME [epoch: 8.73 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09198450642943441		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.1005035714226122		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.09624403892602332 | validation: 0.11757849226108211]
	TIME [epoch: 8.72 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10184968257386044		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.10380291296574377		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.1028262977698021 | validation: 0.09780813742805544]
	TIME [epoch: 8.72 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10047069792151489		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.1115762397758387		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.10602346884867678 | validation: 0.09515644602417478]
	TIME [epoch: 8.71 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09512397498997799		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.09942642160744164		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.09727519829870981 | validation: 0.08503946397761286]
	TIME [epoch: 8.74 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07871503851260966		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.10597410950952428		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.09234457401106698 | validation: 0.09140791821583863]
	TIME [epoch: 8.72 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09112073851299478		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.10274278558151273		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.09693176204725376 | validation: 0.09413616271143384]
	TIME [epoch: 8.71 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09353359665639487		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.11796178279831634		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.10574768972735563 | validation: 0.12684014101191118]
	TIME [epoch: 8.73 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09922300358326612		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.09740369774049701		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.09831335066188159 | validation: 0.08876397433049635]
	TIME [epoch: 8.72 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0999454738056745		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.09028728715460464		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.09511638048013958 | validation: 0.12260244315998922]
	TIME [epoch: 8.73 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09665947995583345		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.1135743556228546		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.10511691778934404 | validation: 0.11477351655190188]
	TIME [epoch: 8.71 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10733936573855267		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.09137820011884569		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.09935878292869917 | validation: 0.10644556642565864]
	TIME [epoch: 8.72 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09760608541365247		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.09443332601526083		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.09601970571445664 | validation: 0.09534074347948135]
	TIME [epoch: 8.72 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11020845985906098		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.1018973008050249		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.10605288033204292 | validation: 0.08760236043364489]
	TIME [epoch: 8.74 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0917534648724865		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.1239340344392968		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.10784374965589165 | validation: 0.09955850045565072]
	TIME [epoch: 8.74 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960407799396489		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.10032529341798802		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.09818303667881846 | validation: 0.08547758238714337]
	TIME [epoch: 8.71 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08814247295769037		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.09828151935361833		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.09321199615565434 | validation: 0.09690695934079752]
	TIME [epoch: 8.72 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09190291351655605		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.08746513052292172		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.08968402201973888 | validation: 0.12570963336255359]
	TIME [epoch: 8.71 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09450304519413363		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.09701487434226441		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.09575895976819901 | validation: 0.08451482062742344]
	TIME [epoch: 8.73 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09404511831829844		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.08301708838508523		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.08853110335169183 | validation: 0.07331779585185029]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_615.pth
	Model improved!!!
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09268511052800929		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.10206528364251403		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.09737519708526166 | validation: 0.15187890350858152]
	TIME [epoch: 8.72 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09785481617128229		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.092941296732378		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.09539805645183014 | validation: 0.10656335338979742]
	TIME [epoch: 8.73 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08651182860015064		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.09618622092474187		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.09134902476244625 | validation: 0.08548810413359192]
	TIME [epoch: 8.73 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.085017584001709		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.09432229130651891		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.08966993765411396 | validation: 0.08751988157315849]
	TIME [epoch: 8.74 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08881806941217463		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.09093648638722365		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.08987727789969915 | validation: 0.0992964756391424]
	TIME [epoch: 8.72 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09508841397231181		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.09372154225273839		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.09440497811252511 | validation: 0.09779958008120414]
	TIME [epoch: 8.71 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09582467062529079		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.1122391704071188		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.1040319205162048 | validation: 0.09330911361437214]
	TIME [epoch: 8.73 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10473487770969861		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.10106012595438849		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.10289750183204353 | validation: 0.12010701949870752]
	TIME [epoch: 8.72 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10716979583332802		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.08894046428943145		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.09805513006137972 | validation: 0.09637417294894371]
	TIME [epoch: 8.73 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10525930984509028		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.09151713444828433		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.09838822214668728 | validation: 0.09212115252654134]
	TIME [epoch: 8.72 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09192288824251603		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.08920404857721057		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.09056346840986329 | validation: 0.09503519179756581]
	TIME [epoch: 8.73 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0921619467829107		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.10102565645967627		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.09659380162129347 | validation: 0.09232159326296367]
	TIME [epoch: 8.72 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09116648054971693		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.09864280779921224		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.09490464417446459 | validation: 0.12365995204367641]
	TIME [epoch: 8.72 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09012092883097442		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.1000299330271972		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.09507543092908582 | validation: 0.10010915410450577]
	TIME [epoch: 8.75 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10261553738893259		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.09108585764304059		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.0968506975159866 | validation: 0.10237979534072092]
	TIME [epoch: 8.73 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08767698754314066		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.08565283316588326		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.08666491035451195 | validation: 0.10399179657421617]
	TIME [epoch: 8.73 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08939755902770286		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.09354469164835208		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.09147112533802744 | validation: 0.09770816822071107]
	TIME [epoch: 8.72 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10384030092972844		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.09140851547068485		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.09762440820020665 | validation: 0.09735425354374512]
	TIME [epoch: 8.74 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08483428011780703		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.08318426292598885		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.08400927152189794 | validation: 0.09301231673708904]
	TIME [epoch: 8.73 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10780947037309559		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.11487935328415537		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.11134441182862549 | validation: 0.11886106261945686]
	TIME [epoch: 8.72 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08632773773629672		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.09690604726481346		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.09161689250055507 | validation: 0.10087312319058751]
	TIME [epoch: 8.73 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08933348954809697		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.08826676175790352		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.08880012565300023 | validation: 0.08928160810504009]
	TIME [epoch: 8.73 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08684744684510504		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.09839213077198734		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.0926197888085462 | validation: 0.10444310468758511]
	TIME [epoch: 8.74 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08750563619253242		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.08964717784555841		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.08857640701904541 | validation: 0.1031693580385383]
	TIME [epoch: 8.72 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09204314474242525		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 0.08422274014678463		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.08813294244460494 | validation: 0.10732640427464482]
	TIME [epoch: 8.72 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08606291557401693		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 0.09135333570579585		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 0.08870812563990638 | validation: 0.08059530742682244]
	TIME [epoch: 8.72 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08860328582812357		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 0.08611942259470483		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 0.08736135421141421 | validation: 0.09617008681647089]
	TIME [epoch: 8.72 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08484145009926097		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 0.09635237867163882		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 0.0905969143854499 | validation: 0.09314690310864959]
	TIME [epoch: 8.75 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09168443474732917		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 0.10294470795876756		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 0.09731457135304836 | validation: 0.0845494451737178]
	TIME [epoch: 8.72 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09376853817191001		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 0.10166919374116465		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 0.09771886595653732 | validation: 0.10002054782085322]
	TIME [epoch: 8.71 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09583384754442836		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 0.09472665261023701		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 0.09528025007733268 | validation: 0.11459642231800153]
	TIME [epoch: 8.72 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1002831075072852		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 0.08569847925572822		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 0.0929907933815067 | validation: 0.12238152988010215]
	TIME [epoch: 8.72 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08797447250211457		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 0.10944611871936014		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 0.09871029561073734 | validation: 0.12950992891027052]
	TIME [epoch: 8.73 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08959199633661354		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 0.08539135655918265		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 0.08749167644789808 | validation: 0.08451292789934009]
	TIME [epoch: 8.72 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09419811340430079		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.09496765787680336		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 0.09458288564055209 | validation: 0.10388485263462734]
	TIME [epoch: 8.72 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09167912586689256		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 0.09075439319535437		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 0.09121675953112347 | validation: 0.09639873532124443]
	TIME [epoch: 8.73 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08097912258109272		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 0.08991533120792325		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 0.085447226894508 | validation: 0.09082678653957449]
	TIME [epoch: 8.73 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08074406218464272		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 0.094164300588686		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 0.08745418138666436 | validation: 0.0953946929200373]
	TIME [epoch: 8.73 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08885211445031166		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 0.08921759429583487		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 0.08903485437307328 | validation: 0.10349412572702853]
	TIME [epoch: 8.72 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0974803538112333		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 0.08315198704052279		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 0.09031617042587803 | validation: 0.08278606512417629]
	TIME [epoch: 8.72 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09252486902340577		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 0.1051565095942949		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 0.09884068930885034 | validation: 0.07983107417645466]
	TIME [epoch: 8.72 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07902651768128106		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 0.09230120203936719		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 0.08566385986032413 | validation: 0.10651969470865492]
	TIME [epoch: 8.75 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10351388754957769		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 0.10324377665858422		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 0.10337883210408094 | validation: 0.0900662746845077]
	TIME [epoch: 8.71 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09089760852520332		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 0.09322044080241723		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.09205902466381027 | validation: 0.10682153118902225]
	TIME [epoch: 8.71 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09889693023337147		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 0.10281346560829718		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 0.10085519792083433 | validation: 0.09230760399852835]
	TIME [epoch: 8.72 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09373790637167359		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 0.0992397966635836		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 0.09648885151762861 | validation: 0.10734548510912509]
	TIME [epoch: 8.72 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09194896741455168		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.07983969884462595		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 0.0858943331295888 | validation: 0.09054149489096742]
	TIME [epoch: 8.74 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08347957078679072		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 0.08922155192591316		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 0.08635056135635197 | validation: 0.12545875887710475]
	TIME [epoch: 8.73 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08475818697045892		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 0.08305911886207472		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 0.08390865291626681 | validation: 0.0810752760027906]
	TIME [epoch: 8.73 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.088163192083995		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 0.09479102652725235		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 0.09147710930562367 | validation: 0.0761096405186994]
	TIME [epoch: 8.73 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07690387191958317		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 0.07964357595597263		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 0.0782737239377779 | validation: 0.09931984456424418]
	TIME [epoch: 8.73 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0989308643609063		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 0.0921766696375472		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 0.09555376699922673 | validation: 0.09487733285507115]
	TIME [epoch: 8.73 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08963985198858648		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 0.08148114152797051		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 0.0855604967582785 | validation: 0.0791405854370163]
	TIME [epoch: 8.72 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08394716840970745		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.10959064872734078		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 0.09676890856852412 | validation: 0.12268706968019416]
	TIME [epoch: 8.71 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09337151971214719		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 0.11207999373654125		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 0.10272575672434421 | validation: 0.0793802785003072]
	TIME [epoch: 8.72 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07881487655738782		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 0.08468503426645031		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 0.08174995541191905 | validation: 0.09847788779217015]
	TIME [epoch: 8.73 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09229509728796871		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 0.07592953857754264		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 0.08411231793275567 | validation: 0.08262154442045976]
	TIME [epoch: 8.73 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08342338837501184		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 0.09982103274882619		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 0.09162221056191902 | validation: 0.08942460033328109]
	TIME [epoch: 8.72 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09753801923605912		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 0.07865234067778745		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 0.0880951799569233 | validation: 0.09481699670005718]
	TIME [epoch: 8.72 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08450504508680394		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 0.09963634446452085		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 0.0920706947756624 | validation: 0.10120639128221881]
	TIME [epoch: 8.72 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08682486921900581		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 0.10409805900886453		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 0.09546146411393516 | validation: 0.12250085773101708]
	TIME [epoch: 8.75 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10299605657480422		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 0.11064568936105268		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 0.10682087296792846 | validation: 0.09383546608662974]
	TIME [epoch: 8.73 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09055925871298547		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 0.08338159634465679		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.08697042752882113 | validation: 0.08973125151404308]
	TIME [epoch: 8.72 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09003141729108391		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 0.095765449984941		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 0.09289843363801246 | validation: 0.10853413167338996]
	TIME [epoch: 8.73 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09580130532570706		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 0.08318643731434883		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 0.08949387132002794 | validation: 0.08855485723374296]
	TIME [epoch: 8.73 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0817575935357912		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.08454186804235973		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 0.08314973078907548 | validation: 0.0931844692521007]
	TIME [epoch: 8.75 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09591283467596758		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 0.09124076748391283		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 0.09357680107994021 | validation: 0.08909554795655168]
	TIME [epoch: 8.72 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08816729228683105		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 0.0811524305398642		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 0.08465986141334762 | validation: 0.09250381698751114]
	TIME [epoch: 8.72 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08748364457658435		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 0.08259420262784022		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 0.08503892360221228 | validation: 0.08784777986387356]
	TIME [epoch: 8.73 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08964444876654518		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 0.08156066253295415		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 0.08560255564974967 | validation: 0.0886701843756102]
	TIME [epoch: 8.74 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08323662416009706		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 0.08814128642830601		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 0.08568895529420154 | validation: 0.08853415978502985]
	TIME [epoch: 8.74 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08256863549330555		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 0.10125297156365447		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 0.09191080352848002 | validation: 0.0850918729977417]
	TIME [epoch: 8.72 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08494835313285842		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.09400681474792412		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 0.08947758394039126 | validation: 0.107005906576515]
	TIME [epoch: 8.71 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0914218374596059		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 0.08178691847960207		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 0.08660437796960399 | validation: 0.09489970238268909]
	TIME [epoch: 8.72 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07766588285020659		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 0.08060760644994633		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 0.07913674465007646 | validation: 0.0846126084993961]
	TIME [epoch: 8.73 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08576099987171168		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 0.08728281951523059		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 0.08652190969347112 | validation: 0.11723873055512826]
	TIME [epoch: 8.74 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10552323988532733		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 0.08634170243703479		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 0.09593247116118106 | validation: 0.0922800728774177]
	TIME [epoch: 8.73 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08908734651851459		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 0.10445774406097579		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 0.09677254528974522 | validation: 0.098611183206921]
	TIME [epoch: 8.72 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09023887817076333		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 0.09991842421663097		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 0.09507865119369714 | validation: 0.09961136847831968]
	TIME [epoch: 8.72 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09751790844021246		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 0.08472505770752117		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 0.0911214830738668 | validation: 0.09789316564348062]
	TIME [epoch: 8.74 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09245579803973966		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 0.09936822295272678		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 0.09591201049623321 | validation: 0.09693065929805726]
	TIME [epoch: 8.72 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09022491491451187		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 0.08858658025333824		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.08940574758392507 | validation: 0.09211964603197867]
	TIME [epoch: 8.73 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08614664238528233		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 0.0964332083051894		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 0.09128992534523586 | validation: 0.09545277904325897]
	TIME [epoch: 8.72 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10749075372492904		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.08848841378910507		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 0.09798958375701705 | validation: 0.09275808026574178]
	TIME [epoch: 8.72 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0885272432214709		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 0.10957801530381477		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 0.09905262926264283 | validation: 0.08521485485408643]
	TIME [epoch: 8.74 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09541072499913138		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 0.08480201777887804		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 0.09010637138900471 | validation: 0.0924442145814192]
	TIME [epoch: 8.72 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08988071676494756		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 0.09055866682337935		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 0.09021969179416345 | validation: 0.10393436001798101]
	TIME [epoch: 8.72 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09171926514300763		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 0.09113532682195206		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 0.09142729598247985 | validation: 0.08839893248640837]
	TIME [epoch: 8.72 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09720993477595308		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 0.08933941199339063		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 0.09327467338467185 | validation: 0.08992797566389671]
	TIME [epoch: 8.73 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.085192656952091		[learning rate: 0.000419]
		[batch 20/20] avg loss: 0.09034227091966855		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 0.08776746393587977 | validation: 0.08796592641576989]
	TIME [epoch: 8.74 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09099134398725325		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 0.08051163008095227		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 0.08575148703410276 | validation: 0.10069207251630968]
	TIME [epoch: 8.73 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0871636876154091		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.08971512700392273		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 0.08843940730966592 | validation: 0.0799197873292272]
	TIME [epoch: 8.72 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09762390958817699		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.08774354179559443		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.09268372569188571 | validation: 0.0982874017539928]
	TIME [epoch: 8.72 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08230249821803509		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 0.09717623887808248		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 0.08973936854805879 | validation: 0.1096953858481331]
	TIME [epoch: 8.73 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09318872565530864		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.08734409961190491		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.09026641263360678 | validation: 0.11056244692716205]
	TIME [epoch: 8.73 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0955580635667256		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 0.08172266370942584		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 0.08864036363807572 | validation: 0.0903420079304395]
	TIME [epoch: 8.72 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08735720789785026		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 0.07969353162605201		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 0.08352536976195113 | validation: 0.11532076678946962]
	TIME [epoch: 8.72 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09749726598673647		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.10450647135906092		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.10100186867289869 | validation: 0.08677331117897794]
	TIME [epoch: 8.72 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08204087924721615		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 0.09188564326039919		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 0.08696326125380768 | validation: 0.09687412466035268]
	TIME [epoch: 8.74 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08363403469988007		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.07803326050380545		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 0.08083364760184276 | validation: 0.0949004233684291]
	TIME [epoch: 8.71 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09211391983207878		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 0.07800368364826701		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.08505880174017291 | validation: 0.08508639238528742]
	TIME [epoch: 8.72 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07613185378884711		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.08287609145501734		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 0.07950397262193222 | validation: 0.09833621933984778]
	TIME [epoch: 8.71 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08136579992188137		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 0.0804650152221894		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 0.08091540757203536 | validation: 0.08507671206920404]
	TIME [epoch: 8.72 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08526130175921692		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.08667756736280322		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 0.08596943456101007 | validation: 0.07676313313731374]
	TIME [epoch: 8.74 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08729483703189483		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 0.10354626492682803		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 0.09542055097936145 | validation: 0.10549552993835629]
	TIME [epoch: 8.72 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08177933310010244		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.07839288770284329		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 0.08008611040147284 | validation: 0.07532537220755586]
	TIME [epoch: 8.72 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07788837602100188		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.09340644279607382		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.08564740940853785 | validation: 0.08176541774179441]
	TIME [epoch: 8.71 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07248868223759447		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.08212419510675398		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.07730643867217422 | validation: 0.0805687533961431]
	TIME [epoch: 8.73 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08306763066685988		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 0.08892292025710241		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.08599527546198114 | validation: 0.09108926384284965]
	TIME [epoch: 8.74 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08232191274254134		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 0.07609458710072894		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 0.07920824992163514 | validation: 0.08587622043466987]
	TIME [epoch: 8.73 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08795016925865005		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.07993471000412011		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 0.0839424396313851 | validation: 0.09948853612587374]
	TIME [epoch: 8.73 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09405907522324154		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.08180872776801143		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.08793390149562649 | validation: 0.08086059864740215]
	TIME [epoch: 8.72 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08100724173949463		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.0736495455395947		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.07732839363954466 | validation: 0.10344538862294032]
	TIME [epoch: 8.73 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08900777724976715		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 0.0876346544067787		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 0.08832121582827292 | validation: 0.07716803000141338]
	TIME [epoch: 8.74 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09293546210647133		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.0903315811217846		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 0.09163352161412795 | validation: 0.09318371127280084]
	TIME [epoch: 8.72 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08043857957335968		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.08391123211818083		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.08217490584577025 | validation: 0.0998648808926568]
	TIME [epoch: 8.71 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08428606270439323		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.08316993483533029		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.08372799876986176 | validation: 0.08904823626406157]
	TIME [epoch: 8.72 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07457957937218766		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.08138171780574256		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.07798064858896511 | validation: 0.08691211389224267]
	TIME [epoch: 8.78 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09811336432673176		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.08059085820171442		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.08935211126422309 | validation: 0.07580845604721953]
	TIME [epoch: 8.72 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07914008840252132		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.09014194734049974		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.0846410178715105 | validation: 0.0873805953540134]
	TIME [epoch: 8.72 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08178477589501112		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.09082028109375836		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.08630252849438473 | validation: 0.08211954757854156]
	TIME [epoch: 8.71 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09090213413132457		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.08721008027242158		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.08905610720187304 | validation: 0.08122473177561566]
	TIME [epoch: 8.72 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08793844153491392		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.07726142420633589		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.08259993287062488 | validation: 0.07570021329381689]
	TIME [epoch: 8.73 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07954420921999086		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.07966827709286547		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.07960624315642816 | validation: 0.07839647738538055]
	TIME [epoch: 8.72 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10874792525282403		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 0.08209258561146907		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 0.09542025543214655 | validation: 0.08931639745871323]
	TIME [epoch: 8.72 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0762531419732636		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 0.08409859248210783		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 0.08017586722768572 | validation: 0.09293137707375534]
	TIME [epoch: 8.71 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08537743011377388		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 0.07383892071043459		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 0.07960817541210424 | validation: 0.1035371767928073]
	TIME [epoch: 8.72 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09568564774919729		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 0.07242368755958166		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 0.08405466765438949 | validation: 0.07883075476890186]
	TIME [epoch: 8.74 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08560484017528504		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 0.09064829758163634		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 0.08812656887846068 | validation: 0.09464070542620998]
	TIME [epoch: 8.72 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07385120879499638		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.08623977923668691		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 0.08004549401584163 | validation: 0.0754372950049645]
	TIME [epoch: 8.73 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08191010410985547		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 0.08918432242899003		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 0.08554721326942277 | validation: 0.09932548414211381]
	TIME [epoch: 8.73 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08404087680000213		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 0.0889154347450433		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 0.08647815577252274 | validation: 0.07223973291588835]
	TIME [epoch: 8.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_747.pth
	Model improved!!!
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06988946289741176		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 0.07971298997919136		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 0.07480122643830156 | validation: 0.10434266688184511]
	TIME [epoch: 8.72 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0774695632512164		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 0.07320171149090168		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 0.07533563737105904 | validation: 0.09608859291712313]
	TIME [epoch: 8.71 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08550048316836065		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 0.08221930547898368		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 0.08385989432367216 | validation: 0.0793072539563975]
	TIME [epoch: 8.71 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07101062284205993		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 0.07122156832723284		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 0.07111609558464639 | validation: 0.09193294118738445]
	TIME [epoch: 8.71 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07545340633051725		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 0.0854068373044298		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 0.08043012181747353 | validation: 0.10432249642828445]
	TIME [epoch: 8.73 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08164671379725699		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 0.07774646036235053		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 0.07969658707980375 | validation: 0.07649931667663812]
	TIME [epoch: 8.71 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07628209517248576		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 0.07712386128062154		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.07670297822655366 | validation: 0.08567626493715165]
	TIME [epoch: 8.71 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08469157593800483		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 0.08882128812602505		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 0.08675643203201495 | validation: 0.0773857792737788]
	TIME [epoch: 8.71 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07565089385076786		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 0.07572044835001127		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 0.07568567110038957 | validation: 0.08184212518267667]
	TIME [epoch: 8.72 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08549845728141243		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.08891084439118639		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 0.08720465083629941 | validation: 0.08153729770610187]
	TIME [epoch: 8.74 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07959268825915565		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 0.0842737956979556		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 0.08193324197855562 | validation: 0.09073480609566058]
	TIME [epoch: 8.71 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08532721159911297		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 0.10053547257007604		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 0.09293134208459451 | validation: 0.08342971254821897]
	TIME [epoch: 8.72 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0862206382472354		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 0.07625626389973789		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 0.08123845107348662 | validation: 0.07865502097965586]
	TIME [epoch: 8.72 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0772564069264466		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 0.07858835490953821		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 0.07792238091799239 | validation: 0.08693133686443674]
	TIME [epoch: 8.72 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08119514722877083		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 0.0834013817583306		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 0.08229826449355074 | validation: 0.0804907577752842]
	TIME [epoch: 8.74 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06993526134893133		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 0.08470627726536596		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 0.07732076930714862 | validation: 0.07317022781973591]
	TIME [epoch: 8.72 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07567584861307124		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.0767952708864817		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 0.07623555974977646 | validation: 0.08115304148451927]
	TIME [epoch: 8.72 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07990110301282369		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 0.07070449303875051		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 0.0753027980257871 | validation: 0.07260553024711283]
	TIME [epoch: 8.73 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07250568230897987		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 0.08008276105031037		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 0.07629422167964511 | validation: 0.0925832561005131]
	TIME [epoch: 8.72 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07788126907395297		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 0.077721857659218		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 0.07780156336658547 | validation: 0.07404491540983806]
	TIME [epoch: 8.74 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07979436790338854		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 0.07570994466037702		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 0.07775215628188277 | validation: 0.08651007216987207]
	TIME [epoch: 8.71 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08381275786984464		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.08368591587168148		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.08374933687076305 | validation: 0.07196300459577795]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_769.pth
	Model improved!!!
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07326615732175472		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 0.09008381907909273		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 0.08167498820042372 | validation: 0.0869162604341131]
	TIME [epoch: 8.72 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08000163507801794		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 0.07109595794381951		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 0.07554879651091871 | validation: 0.08368568718154126]
	TIME [epoch: 8.73 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.079923561569839		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 0.08076021396367397		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 0.0803418877667565 | validation: 0.08459576408218203]
	TIME [epoch: 8.72 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06444254880803259		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 0.09009036664906978		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.07726645772855119 | validation: 0.08662298697302946]
	TIME [epoch: 8.73 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08969043024754514		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 0.08614536672034785		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 0.0879178984839465 | validation: 0.09246156331307574]
	TIME [epoch: 8.73 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960548114396065		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 0.07238384044448914		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 0.08421932594204781 | validation: 0.0791177919977962]
	TIME [epoch: 8.73 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07249839740633719		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 0.0841062404236084		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 0.0783023189149728 | validation: 0.08673503054822554]
	TIME [epoch: 8.74 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08193794421858167		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 0.07559528714762376		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 0.07876661568310271 | validation: 0.08931681297006096]
	TIME [epoch: 8.72 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07987036215795869		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 0.07259452627894729		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 0.07623244421845297 | validation: 0.0783615825017637]
	TIME [epoch: 8.72 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0825344344897286		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 0.08290106749544321		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 0.0827177509925859 | validation: 0.08227320823173853]
	TIME [epoch: 8.72 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0839174231699002		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.09077724718138576		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 0.08734733517564298 | validation: 0.07101650235494379]
	TIME [epoch: 8.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_780.pth
	Model improved!!!
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06809326599195524		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.07070584067440103		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.06939955333317814 | validation: 0.09221983418241597]
	TIME [epoch: 8.74 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08140930404107319		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.07367768301465193		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.07754349352786255 | validation: 0.08406039037596735]
	TIME [epoch: 8.7 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08366675502655478		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.07420128324556222		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 0.07893401913605849 | validation: 0.07701234049982675]
	TIME [epoch: 8.69 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07018945744327301		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.0757984838274578		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.0729939706353654 | validation: 0.07417363276248695]
	TIME [epoch: 8.71 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08025984140405354		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 0.08448692522450384		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 0.0823733833142787 | validation: 0.08278963739629595]
	TIME [epoch: 8.73 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07447260591579333		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 0.07404284312905782		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 0.07425772452242559 | validation: 0.08380434693429467]
	TIME [epoch: 8.74 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08245558112221116		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.09533255756932205		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.08889406934576659 | validation: 0.08807746353274043]
	TIME [epoch: 8.72 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07596567841904896		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.07521771374367647		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.07559169608136271 | validation: 0.0982805582352136]
	TIME [epoch: 8.71 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08049158603008588		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.07451629879036434		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.07750394241022511 | validation: 0.08580919456171093]
	TIME [epoch: 8.71 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07759964795161405		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.08310689818711033		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.08035327306936219 | validation: 0.08557998432822216]
	TIME [epoch: 8.72 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08009092843770847		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.07254427329482867		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.07631760086626856 | validation: 0.07887243420390298]
	TIME [epoch: 8.72 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0708161440463024		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.07625042445672421		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.07353328425151331 | validation: 0.07419725061108154]
	TIME [epoch: 8.71 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08033514108245651		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.07683971515704241		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.07858742811974948 | validation: 0.07947650400843888]
	TIME [epoch: 8.71 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0783702539979266		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.07695594506720024		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.07766309953256345 | validation: 0.08628084259596445]
	TIME [epoch: 8.72 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07452917070783482		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.06841029973431491		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.07146973522107487 | validation: 0.07025452636417256]
	TIME [epoch: 8.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_795.pth
	Model improved!!!
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07082341069126015		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.07775227883282763		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.07428784476204389 | validation: 0.07195391495664774]
	TIME [epoch: 8.72 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0747929435202327		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.07839602952622224		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.07659448652322749 | validation: 0.08062611536015234]
	TIME [epoch: 8.72 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08061003700218097		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.07014907085527207		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.07537955392872653 | validation: 0.07912421440740225]
	TIME [epoch: 8.72 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08042900163999275		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.07816909125275907		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.0792990464463759 | validation: 0.075583927252547]
	TIME [epoch: 8.72 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07864428593093675		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.0784164441472259		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.07853036503908134 | validation: 0.07543007994670133]
	TIME [epoch: 8.75 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06728689189806987		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.07733587793387973		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.0723113849159748 | validation: 0.07386959011191462]
	TIME [epoch: 8.71 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07276533910125065		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.06790557785384121		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.07033545847754594 | validation: 0.08397520496820488]
	TIME [epoch: 8.72 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06935660045281876		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.07538761622424292		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.07237210833853085 | validation: 0.07662918168271485]
	TIME [epoch: 8.71 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07096984654654577		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.07659175007048316		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.07378079830851444 | validation: 0.08207353574457178]
	TIME [epoch: 8.73 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07502522595041874		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.07818791542403318		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.07660657068722597 | validation: 0.08187610331039273]
	TIME [epoch: 8.73 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08164752926292178		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.08563759588414925		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.08364256257353551 | validation: 0.08376704996008975]
	TIME [epoch: 8.71 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08221504641808991		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.07074147348177402		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.07647825994993196 | validation: 0.0731673746291872]
	TIME [epoch: 8.72 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06770669054084025		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.07521337238551092		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.07146003146317559 | validation: 0.08924176461221282]
	TIME [epoch: 8.73 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07825078591333032		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.07308982089876373		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.07567030340604702 | validation: 0.082411382346012]
	TIME [epoch: 8.74 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07698571057898451		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.07473574349348835		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.07586072703623642 | validation: 0.09018383483868328]
	TIME [epoch: 8.73 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07044620263557844		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.08057092180502225		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.07550856222030033 | validation: 0.08220591578133662]
	TIME [epoch: 8.71 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07422408904872309		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.07655714933175758		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.07539061919024034 | validation: 0.08157123187966486]
	TIME [epoch: 8.71 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07542492767487032		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.06880401525849247		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.07211447146668137 | validation: 0.07762747701245508]
	TIME [epoch: 8.72 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07691116191028993		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.0710751788333914		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.07399317037184068 | validation: 0.08487465991480987]
	TIME [epoch: 8.76 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07426843710868583		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.07903967642442834		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.07665405676655708 | validation: 0.07517120724289486]
	TIME [epoch: 8.73 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07518151351664092		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.08278331586950896		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.07898241469307493 | validation: 0.08616328499854288]
	TIME [epoch: 8.76 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07585827583085185		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.08577657331517152		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.0808174245730117 | validation: 0.07910061093680218]
	TIME [epoch: 8.72 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07086646971298373		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.07531115799839075		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.07308881385568723 | validation: 0.08576902720886562]
	TIME [epoch: 8.72 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08338276458542909		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.07537068278298956		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.07937672368420932 | validation: 0.07685416037094857]
	TIME [epoch: 8.74 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07639885064112603		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.07548293677587946		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.07594089370850274 | validation: 0.08099997575502663]
	TIME [epoch: 8.71 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.077939306110582		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.07240514939404089		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.07517222775231144 | validation: 0.0822946933373133]
	TIME [epoch: 8.72 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07993372204531804		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.07181582514590912		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.07587477359561355 | validation: 0.08710975388810306]
	TIME [epoch: 8.72 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07585389152531792		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.06767985980634658		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.07176687566583224 | validation: 0.0846275891080448]
	TIME [epoch: 8.71 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07639401951727863		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.07804129454197127		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.07721765702962494 | validation: 0.0740506075775929]
	TIME [epoch: 8.74 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07591791028761578		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.07219669652883129		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.07405730340822354 | validation: 0.070856699562501]
	TIME [epoch: 8.71 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08037889384108607		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 0.08763964182457105		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.08400926783282858 | validation: 0.08767226978208595]
	TIME [epoch: 8.73 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08636211183245218		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.07371439832125622		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.0800382550768542 | validation: 0.08606600647657167]
	TIME [epoch: 8.72 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07917001184810105		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.07510772288731074		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.07713886736770588 | validation: 0.07277150484347483]
	TIME [epoch: 8.74 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08195742638757014		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.08209467099599552		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.08202604869178284 | validation: 0.09090839025594755]
	TIME [epoch: 8.74 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0759053792087379		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.06813891668077021		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.07202214794475408 | validation: 0.07088268878935998]
	TIME [epoch: 8.72 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06274334817728394		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.08193782556062776		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.07234058686895585 | validation: 0.07511295297908452]
	TIME [epoch: 8.71 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07289393450226762		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.07102946606000594		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.07196170028113677 | validation: 0.07916173687109494]
	TIME [epoch: 8.72 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06524639813990171		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.07519647898433648		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.07022143856211908 | validation: 0.07942109397672353]
	TIME [epoch: 8.73 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08405235759043869		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.07179758276481626		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.07792497017762748 | validation: 0.07933507384164558]
	TIME [epoch: 8.72 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06897317089939269		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.07461645144319237		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.07179481117129252 | validation: 0.08251796694789779]
	TIME [epoch: 8.71 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07345225080016948		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.07370656859707099		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.07357940969862024 | validation: 0.07885020236901048]
	TIME [epoch: 8.72 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0734104471493086		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.06375320045269398		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.06858182380100128 | validation: 0.07796908427687702]
	TIME [epoch: 8.7 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07745566260659074		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.0825152182917916		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.07998544044919118 | validation: 0.0959873412329634]
	TIME [epoch: 8.74 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07658783602913259		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.07509141577696735		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.07583962590304996 | validation: 0.06918118535855546]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_839.pth
	Model improved!!!
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0796993676799177		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.065510485495438		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.07260492658767784 | validation: 0.07251788721683025]
	TIME [epoch: 8.71 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06875948571700292		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.06652637585545346		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.06764293078622818 | validation: 0.07788843376439414]
	TIME [epoch: 8.72 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.072737763421556		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.07639049268414015		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.07456412805284808 | validation: 0.07844405117347086]
	TIME [epoch: 8.72 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07314855917896002		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.08075274275769063		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.07695065096832532 | validation: 0.07577339065915122]
	TIME [epoch: 8.74 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0674350864438524		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.07675008048656094		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.07209258346520667 | validation: 0.06628963669968815]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_844.pth
	Model improved!!!
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07367204090936144		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.07595505604892039		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.07481354847914093 | validation: 0.08811640432016928]
	TIME [epoch: 8.72 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0818477810744181		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.08049559797302719		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.08117168952372265 | validation: 0.07694418753134205]
	TIME [epoch: 8.71 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0689843238057897		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.08289723386539262		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.07594077883559117 | validation: 0.07532245356210777]
	TIME [epoch: 8.72 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07308360801831311		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.07232998195406876		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.07270679498619094 | validation: 0.08910732425141142]
	TIME [epoch: 8.75 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08516902072731333		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.07564748687978558		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.08040825380354945 | validation: 0.07813524412692643]
	TIME [epoch: 8.72 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06750125951314226		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.07522387920440063		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.07136256935877142 | validation: 0.08626370571320739]
	TIME [epoch: 8.72 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0722223558367295		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.06999626718524354		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.07110931151098651 | validation: 0.07389179960933577]
	TIME [epoch: 8.71 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07388202701297254		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.07006700055754136		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.07197451378525696 | validation: 0.08700485535927677]
	TIME [epoch: 8.73 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07638335218106242		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.07546169545281152		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.07592252381693695 | validation: 0.06848921341503189]
	TIME [epoch: 8.72 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07380490881442889		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.07148736621584179		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.07264613751513535 | validation: 0.07679531069348368]
	TIME [epoch: 8.71 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07043185309738985		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.07137391302649093		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.07090288306194038 | validation: 0.0707986506640108]
	TIME [epoch: 8.73 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07381031706620203		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.06890152888509424		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.07135592297564813 | validation: 0.0815387370122079]
	TIME [epoch: 8.72 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07209543450435649		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.07740971098233931		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.07475257274334789 | validation: 0.07088830855478682]
	TIME [epoch: 8.75 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06447432942064871		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.07871404891803144		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.07159418916934007 | validation: 0.07693095559800694]
	TIME [epoch: 8.72 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07177277472685409		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.07572720756721843		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.07374999114703626 | validation: 0.08877130321089735]
	TIME [epoch: 8.72 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08556660735183315		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.06910877755002415		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.07733769245092863 | validation: 0.07631127952606509]
	TIME [epoch: 8.73 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07074020408786666		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.08039093150655155		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.0755655677972091 | validation: 0.0731297666377517]
	TIME [epoch: 8.72 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07729593732132098		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.07532991562879277		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.07631292647505687 | validation: 0.0752000299999109]
	TIME [epoch: 8.73 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06347189636321063		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.07364742806021471		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.06855966221171267 | validation: 0.07729346858069702]
	TIME [epoch: 8.71 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07271679457201766		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.07030473346309224		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.07151076401755496 | validation: 0.07194025817330288]
	TIME [epoch: 8.72 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06974958583927668		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.07375699157643398		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.07175328870785533 | validation: 0.07257006445449608]
	TIME [epoch: 8.72 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06850754922331567		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.06774006913189513		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.06812380917760538 | validation: 0.07221912612419634]
	TIME [epoch: 8.72 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0759384544685068		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.06526400226614791		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.07060122836732735 | validation: 0.07831329792848653]
	TIME [epoch: 8.73 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07750476226832757		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.08002031988981906		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.07876254107907332 | validation: 0.06991798669571761]
	TIME [epoch: 8.71 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06815309764171815		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.06808002765730395		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.06811656264951106 | validation: 0.0932002818873767]
	TIME [epoch: 8.72 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07057229460194318		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.07854753654394905		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.07455991557294611 | validation: 0.06636172732342835]
	TIME [epoch: 8.73 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06544338501804195		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.07378453462264235		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.06961395982034213 | validation: 0.07612278784690978]
	TIME [epoch: 8.73 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07178101442480514		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.06294507610164005		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.06736304526322259 | validation: 0.06940981639658167]
	TIME [epoch: 8.72 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07231913746601099		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.07486456083384192		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.07359184914992646 | validation: 0.07687188052020892]
	TIME [epoch: 8.71 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07312300886051663		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.07321649721119912		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.07316975303585785 | validation: 0.07968704774721758]
	TIME [epoch: 8.72 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07488957743909644		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.06531760428027117		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.07010359085968379 | validation: 0.06727898177876443]
	TIME [epoch: 8.72 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06950275319401603		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.06879163208530877		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.0691471926396624 | validation: 0.08406363550269674]
	TIME [epoch: 8.74 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0732213430281515		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.07228382638291996		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.07275258470553572 | validation: 0.07955681206184673]
	TIME [epoch: 8.72 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07370546382524933		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.07109868110959501		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.07240207246742218 | validation: 0.08755540316412383]
	TIME [epoch: 8.71 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07627255063104568		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.06212419097480719		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.06919837080292643 | validation: 0.0700026153945805]
	TIME [epoch: 8.72 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07294298365904597		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.0796881384976288		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.07631556107833738 | validation: 0.09195168360268959]
	TIME [epoch: 8.71 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07277813223440212		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.07142393509707277		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.07210103366573745 | validation: 0.08146401919172364]
	TIME [epoch: 8.74 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0666071599562271		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.07453261488866046		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.07056988742244379 | validation: 0.07432094364371357]
	TIME [epoch: 8.74 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07630722193310188		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.06584919785082463		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.07107820989196326 | validation: 0.07415027146083972]
	TIME [epoch: 8.72 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662618383644271		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.07512613154643041		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.07069398495542877 | validation: 0.08456029625719785]
	TIME [epoch: 8.72 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07459620660214875		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.07538007291559175		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.07498813975887023 | validation: 0.07143807986985039]
	TIME [epoch: 8.72 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0734582096633619		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.0733253660860776		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.07339178787471975 | validation: 0.07536681814765234]
	TIME [epoch: 8.73 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0738256103097105		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.06697526996473044		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.07040044013722047 | validation: 0.07448494415114004]
	TIME [epoch: 8.71 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07393812731296168		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.06630240210960246		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.07012026471128209 | validation: 0.0799805092869166]
	TIME [epoch: 8.7 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07605320576611244		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 0.07358346906290864		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 0.07481833741451054 | validation: 0.07421078707788983]
	TIME [epoch: 8.72 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06733225995074554		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.07057972573771734		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 0.06895599284423143 | validation: 0.0763486275506176]
	TIME [epoch: 8.73 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07477472880797773		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.07075180318656372		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.07276326599727073 | validation: 0.07151629810182107]
	TIME [epoch: 8.73 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06257956784392846		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.07817210114068131		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.07037583449230488 | validation: 0.07461784327627904]
	TIME [epoch: 8.71 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06717994735670733		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.0693208098754304		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.06825037861606888 | validation: 0.07306634576048883]
	TIME [epoch: 8.71 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06926551890781366		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.07078818259102092		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.07002685074941727 | validation: 0.07381940773232294]
	TIME [epoch: 8.72 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07500422249761204		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.07504623397874231		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.07502522823817717 | validation: 0.08443228928450495]
	TIME [epoch: 8.73 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07144280460710363		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.07227338810717494		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.0718580963571393 | validation: 0.07586365036487853]
	TIME [epoch: 8.72 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07234595876875848		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.07523474755350426		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 0.07379035316113136 | validation: 0.07484567718150759]
	TIME [epoch: 8.71 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07749707232745358		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 0.07276864201431621		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 0.0751328571708849 | validation: 0.07982417597537919]
	TIME [epoch: 8.72 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06925542276347466		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 0.07008920924336683		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 0.06967231600342072 | validation: 0.07926227107763895]
	TIME [epoch: 8.71 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07559720658840106		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 0.0750414581005165		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 0.07531933234445878 | validation: 0.07605087918668027]
	TIME [epoch: 8.74 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06666596190521308		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 0.06584897150860868		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 0.06625746670691088 | validation: 0.07885258515533952]
	TIME [epoch: 8.72 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06801620215630398		[learning rate: 0.00016124]
		[batch 20/20] avg loss: 0.06989616521338374		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 0.06895618368484384 | validation: 0.07674515568172008]
	TIME [epoch: 8.7 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07369611627369674		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 0.08334828587656751		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 0.07852220107513212 | validation: 0.08864440507540061]
	TIME [epoch: 8.72 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07568637676934128		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 0.08168088722390417		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 0.07868363199662272 | validation: 0.07795931249227515]
	TIME [epoch: 8.72 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06125173037920485		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 0.077320911796358		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 0.0692863210877814 | validation: 0.08693135700577215]
	TIME [epoch: 8.73 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07433752151644529		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 0.07355368472794918		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.07394560312219725 | validation: 0.07490137018669428]
	TIME [epoch: 8.71 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06793025619477741		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 0.07300907099342786		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 0.07046966359410264 | validation: 0.07203813501022581]
	TIME [epoch: 8.71 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07282064904067469		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 0.07535194469603004		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 0.07408629686835239 | validation: 0.07988019212471834]
	TIME [epoch: 8.72 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07394350144002998		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.0738042665756467		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 0.07387388400783834 | validation: 0.07657661863022397]
	TIME [epoch: 8.72 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07105106863601765		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 0.0656839945803536		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 0.06836753160818562 | validation: 0.07427621051557218]
	TIME [epoch: 8.74 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06878566258767391		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 0.06209114742401859		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 0.06543840500584626 | validation: 0.07538077823015371]
	TIME [epoch: 8.73 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07397785878479544		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 0.06470766643562145		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 0.06934276261020843 | validation: 0.07600115042895525]
	TIME [epoch: 8.73 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06741279479492404		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 0.07018045563585865		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 0.06879662521539133 | validation: 0.07241529049034374]
	TIME [epoch: 8.72 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06463231923034687		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 0.07170949003253468		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 0.06817090463144079 | validation: 0.07769459265149613]
	TIME [epoch: 8.73 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06429754564540845		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 0.07243427634512774		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 0.06836591099526809 | validation: 0.0683258108709764]
	TIME [epoch: 8.72 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07081134764979072		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.07181373252967682		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 0.07131254008973377 | validation: 0.08046155930701779]
	TIME [epoch: 8.72 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07819197811329198		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 0.07186379179942107		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 0.07502788495635654 | validation: 0.07661751241656277]
	TIME [epoch: 8.72 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07730963297407387		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 0.07352643868173783		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 0.07541803582790585 | validation: 0.07118358689070717]
	TIME [epoch: 8.71 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06419716858835404		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 0.06913334028377255		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 0.06666525443606329 | validation: 0.07672729308721081]
	TIME [epoch: 8.73 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06667322909049689		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 0.07527175892719738		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 0.07097249400884713 | validation: 0.08197803284902802]
	TIME [epoch: 8.71 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07072596538632828		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 0.07714093169344391		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 0.0739334485398861 | validation: 0.0798357775975673]
	TIME [epoch: 8.7 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0647927694666499		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 0.07290034874161742		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 0.06884655910413365 | validation: 0.07746127873657885]
	TIME [epoch: 8.7 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07129998226259178		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 0.0697728943441439		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 0.07053643830336784 | validation: 0.07181041591350476]
	TIME [epoch: 8.72 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0702960647939602		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 0.06707342934243488		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 0.06868474706819754 | validation: 0.0759284020328182]
	TIME [epoch: 8.74 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07051828075397508		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 0.07054987586272086		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.07053407830834797 | validation: 0.07630755438148998]
	TIME [epoch: 8.74 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07484212333788572		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 0.08367007160820784		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 0.07925609747304678 | validation: 0.08544648346491655]
	TIME [epoch: 8.71 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08684502733343538		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 0.06970678057515213		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 0.07827590395429376 | validation: 0.07412152993776552]
	TIME [epoch: 8.72 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07260324402578151		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.06852525792023068		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 0.07056425097300609 | validation: 0.07388654958958506]
	TIME [epoch: 8.72 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07066405837025722		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 0.07317686063572358		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 0.07192045950299039 | validation: 0.0733129337757406]
	TIME [epoch: 8.73 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06530836358724867		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 0.07367916491459212		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 0.0694937642509204 | validation: 0.06950848954109799]
	TIME [epoch: 8.72 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0659671709444681		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 0.07397112195010178		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 0.06996914644728494 | validation: 0.09089686439573781]
	TIME [epoch: 8.71 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07097769984355959		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 0.07302725817833906		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 0.07200247901094932 | validation: 0.07791958733714807]
	TIME [epoch: 8.72 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07605415187855125		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 0.06413881198768992		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 0.0700964819331206 | validation: 0.07485291617994531]
	TIME [epoch: 8.73 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06262493897683043		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 0.07258434761536327		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 0.06760464329609683 | validation: 0.07266637442354953]
	TIME [epoch: 8.73 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0688309578509882		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.0686276081984469		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 0.06872928302471754 | validation: 0.07412778114548899]
	TIME [epoch: 8.71 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06400190994201234		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 0.07620535551121824		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 0.07010363272661528 | validation: 0.06924582326060744]
	TIME [epoch: 8.71 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07683046611097546		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 0.07154774962733215		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 0.07418910786915381 | validation: 0.06908995582236307]
	TIME [epoch: 8.73 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06310467532171934		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 0.07125106560947522		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 0.06717787046559728 | validation: 0.08089838702776775]
	TIME [epoch: 8.74 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06613363416102092		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 0.07111243049323754		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 0.06862303232712924 | validation: 0.09708897956607904]
	TIME [epoch: 8.73 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07655257154852185		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 0.07817848469449731		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 0.07736552812150957 | validation: 0.08249158784198057]
	TIME [epoch: 8.7 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07899280094325702		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 0.0706861958049034		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 0.07483949837408019 | validation: 0.07585943875942358]
	TIME [epoch: 8.72 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965405295926531		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 0.08131910801496915		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 0.07548658048711722 | validation: 0.07925437524202347]
	TIME [epoch: 8.71 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07904590944552174		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 0.06821269097028111		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 0.07362930020790141 | validation: 0.08473855674265003]
	TIME [epoch: 8.75 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.071356632833085		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 0.07624370499752807		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.07380016891530657 | validation: 0.07624621046398422]
	TIME [epoch: 8.71 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07438276607189666		[learning rate: 0.0001309]
		[batch 20/20] avg loss: 0.07587158403186929		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 0.07512717505188296 | validation: 0.07527554950619235]
	TIME [epoch: 8.71 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07144162512437051		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 0.07092211468776308		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 0.07118186990606681 | validation: 0.0792503243005777]
	TIME [epoch: 8.71 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06887534520954265		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.06845472037664543		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 0.06866503279309405 | validation: 0.06925290195599204]
	TIME [epoch: 8.71 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07238173262231146		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 0.08039379067984165		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 0.07638776165107654 | validation: 0.07837258308095456]
	TIME [epoch: 8.73 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07340232066472135		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 0.07309772350476354		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 0.07325002208474245 | validation: 0.07600011801468895]
	TIME [epoch: 8.72 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07672334451223942		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 0.07151495684987343		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 0.07411915068105643 | validation: 0.07577948376061427]
	TIME [epoch: 8.72 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07024100626764876		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 0.07604544688693349		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 0.07314322657729112 | validation: 0.06913857806364072]
	TIME [epoch: 8.73 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06986169569173346		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 0.06526699737966506		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 0.06756434653569925 | validation: 0.07041890626471196]
	TIME [epoch: 8.73 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073936151254633		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 0.07987277342633917		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 0.07530606746944275 | validation: 0.08357223825409871]
	TIME [epoch: 8.73 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07594534238232145		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.07863140993345516		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 0.07728837615788832 | validation: 0.07545058960572626]
	TIME [epoch: 8.72 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06931325349764035		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 0.07368109325877853		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 0.07149717337820945 | validation: 0.07917432677420742]
	TIME [epoch: 8.71 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06931485095570225		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 0.07211381006819358		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 0.07071433051194792 | validation: 0.0858260184112024]
	TIME [epoch: 8.72 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07324393822203687		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 0.07481164087677863		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 0.07402778954940775 | validation: 0.07935334553702739]
	TIME [epoch: 8.72 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07551976589194132		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 0.06566594331966366		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 0.07059285460580247 | validation: 0.07814094050884274]
	TIME [epoch: 8.71 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0738966208454947		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 0.0665814530907905		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 0.07023903696814258 | validation: 0.07922845988455353]
	TIME [epoch: 8.71 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0638956524844806		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 0.0741532105755345		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 0.06902443153000755 | validation: 0.07899556984672254]
	TIME [epoch: 8.71 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07236374275471821		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.07744642001178217		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.07490508138325021 | validation: 0.0822932847221533]
	TIME [epoch: 8.71 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07500106494427815		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.06606519747178322		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.0705331312080307 | validation: 0.07415862796752641]
	TIME [epoch: 8.74 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06963028793093665		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.06808899922052396		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.06885964357573031 | validation: 0.07180627446765643]
	TIME [epoch: 8.71 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.065785204944868		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.07187217088455614		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.06882868791471206 | validation: 0.07495074043991976]
	TIME [epoch: 8.7 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06735805849875229		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.06812155829392745		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.06773980839633988 | validation: 0.0706523163416138]
	TIME [epoch: 8.73 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06462924197542172		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.066605669773354		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.06561745587438786 | validation: 0.0767708373842802]
	TIME [epoch: 8.74 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07424220183152035		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.07426363969224989		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.07425292076188512 | validation: 0.08343465181740325]
	TIME [epoch: 8.72 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06867966543322426		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.06790551625437427		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.06829259084379928 | validation: 0.07441257146394717]
	TIME [epoch: 8.7 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06488359268823747		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.0723039929067302		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.06859379279748384 | validation: 0.07412350535954765]
	TIME [epoch: 8.7 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06817469111543101		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.07881558894073601		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.07349514002808352 | validation: 0.08465460635348436]
	TIME [epoch: 8.71 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07167986803351702		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.0663228037861725		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.06900133590984477 | validation: 0.0719342801939284]
	TIME [epoch: 8.73 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0691259336450771		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.06714899614992305		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.06813746489750004 | validation: 0.07620412846240743]
	TIME [epoch: 8.72 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07064099093432046		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.07299144367824234		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.07181621730628139 | validation: 0.08306826719302589]
	TIME [epoch: 8.77 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06923989341890814		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.07530032369529627		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.0722701085571022 | validation: 0.08345885140057001]
	TIME [epoch: 8.71 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07042153674999367		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.06448367462603741		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.06745260568801556 | validation: 0.07197029128220803]
	TIME [epoch: 8.71 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06768794079525678		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.06737021303869112		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.06752907691697393 | validation: 0.07554702765749226]
	TIME [epoch: 8.73 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06891645660636232		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.07365134576581897		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.07128390118609065 | validation: 0.0771007668512546]
	TIME [epoch: 8.72 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06985167094385525		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.06662499815866592		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.06823833455126058 | validation: 0.07398508546016608]
	TIME [epoch: 8.72 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07301259210708136		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.06728357364942605		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.07014808287825372 | validation: 0.07371887740482988]
	TIME [epoch: 8.72 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07377651802344327		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.06744290789425579		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.07060971295884953 | validation: 0.07083077684268994]
	TIME [epoch: 8.74 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07417895604216876		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.06714969152930768		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.07066432378573823 | validation: 0.06838117650149104]
	TIME [epoch: 8.73 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06572966716254423		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.06971225631441333		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.06772096173847879 | validation: 0.07070067453319906]
	TIME [epoch: 8.72 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06717958295409011		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.06876313453560381		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.06797135874484697 | validation: 0.07418710289393436]
	TIME [epoch: 8.71 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06938319697247251		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.06722074013552445		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.06830196855399849 | validation: 0.07440215200567851]
	TIME [epoch: 8.71 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404373931234443		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.07707151906053228		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.07055762918643835 | validation: 0.06985532718559274]
	TIME [epoch: 8.73 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06558849234349119		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.06864499438827824		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.06711674336588472 | validation: 0.07413730392968279]
	TIME [epoch: 8.73 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0718206554731122		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.06701639869767255		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.06941852708539235 | validation: 0.06830854992019657]
	TIME [epoch: 8.72 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06467553354503529		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 0.07272988152536164		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 0.06870270753519844 | validation: 0.07259575012002155]
	TIME [epoch: 8.7 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06842362375688141		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 0.06227592769297363		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 0.06534977572492752 | validation: 0.07123014995774199]
	TIME [epoch: 8.72 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.061509829882961733		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 0.06976303273533087		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 0.06563643130914629 | validation: 0.07347693293045103]
	TIME [epoch: 8.72 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06454521353935491		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 0.072532063134221		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 0.06853863833678796 | validation: 0.08271123073738078]
	TIME [epoch: 8.72 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07276163349788169		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.06199335630048215		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 0.0673774948991819 | validation: 0.068395557252171]
	TIME [epoch: 8.71 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06358037386318798		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 0.07437277622870997		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 0.06897657504594898 | validation: 0.07556685659563983]
	TIME [epoch: 8.7 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08167289478992121		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 0.06511264444090828		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 0.07339276961541474 | validation: 0.07095515427399522]
	TIME [epoch: 8.72 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07330487059066967		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 0.07154685711031139		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 0.07242586385049053 | validation: 0.0747426384438736]
	TIME [epoch: 8.73 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06614650909397687		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 0.06792987513873068		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 0.06703819211635378 | validation: 0.07918165256224259]
	TIME [epoch: 8.72 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06712266759429501		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 0.06306748762602724		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 0.06509507761016112 | validation: 0.07584364560174621]
	TIME [epoch: 8.72 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06686990682826763		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 0.06964107672152813		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 0.06825549177489787 | validation: 0.07067548951942186]
	TIME [epoch: 8.71 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404517559614946		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 0.06616831467142034		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 0.06510674513378491 | validation: 0.06544836862178073]
	TIME [epoch: 8.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240218_115012/states/model_tr_study1_999.pth
	Model improved!!!
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.07300460337463546		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 0.07054564842421882		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 0.07177512589942714 | validation: 0.07016234578374379]
	TIME [epoch: 8.72 sec]
Finished training in 8822.641 seconds.
