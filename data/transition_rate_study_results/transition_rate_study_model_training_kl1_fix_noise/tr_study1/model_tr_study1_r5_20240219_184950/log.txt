Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r5', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 749628241

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.104415307932785		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.562567740642233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.83349152428751 | validation: 9.744411421863932]
	TIME [epoch: 79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.149462670349694		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.973549911551471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.06150629095058 | validation: 9.424768461917575]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.805110872717883		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.426381165412435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.61574601906516 | validation: 7.624132334578105]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.197723841099906		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.176860872632538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.187292356866221 | validation: 7.99113945493541]
	TIME [epoch: 8.19 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.31901672514134		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.665820791218595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.992418758179966 | validation: 7.459424088838522]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.65415697713617		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.720658096949357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.6874075370427635 | validation: 10.095013015848613]
	TIME [epoch: 8.19 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.202884694392846		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.77947162630749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.491178160350169 | validation: 6.301630007758105]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.623056835214014		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.299180760972422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.461118798093219 | validation: 5.649819861725144]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.080418070019226		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.945123769186255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0127709196027395 | validation: 6.6539249656917026]
	TIME [epoch: 8.2 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.1923133354909		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.1982606451493805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.69528699032014 | validation: 5.678140096219149]
	TIME [epoch: 8.18 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.226222679698844		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3627665371665527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.794494608432698 | validation: 2.912452231960728]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2001630568511397		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8224340014440594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0112985291475995 | validation: 2.2412825325158288]
	TIME [epoch: 8.46 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4959865618858754		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.975287861528288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7356372117070817 | validation: 2.6326561579738126]
	TIME [epoch: 8.21 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5262483223990198		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.689085320827858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6076668216134395 | validation: 3.5004459618921677]
	TIME [epoch: 8.19 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.435272153081179		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.683616329672744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.559444241376961 | validation: 4.008632789538433]
	TIME [epoch: 8.19 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5182071024926302		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1821536493993436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.350180375945987 | validation: 2.1923391021601275]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0887022273197835		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3280654212769747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2083838242983793 | validation: 2.1473521850660466]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0355003175729594		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3482373422504708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1918688299117153 | validation: 1.4981263027809404]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3777358971910862		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9640862273704065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.170911062280747 | validation: 1.5032784160735504]
	TIME [epoch: 8.19 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1477935538960593		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3910675286177776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.269430541256919 | validation: 1.6264358995315815]
	TIME [epoch: 8.21 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.057283878969897		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.628512590174673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.842898234572285 | validation: 1.6888712172152964]
	TIME [epoch: 8.2 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7626542463770085		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8596630040655149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8111586252212626 | validation: 1.4799816757971038]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.10780120433953		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7963050541850212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9520531292622756 | validation: 1.6599982673620044]
	TIME [epoch: 8.17 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6716055085510757		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.733788500293606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2026970044223413 | validation: 6.211733220265713]
	TIME [epoch: 8.2 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.317827230474308		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.095876830041605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.206852030257956 | validation: 2.1451016309024076]
	TIME [epoch: 8.17 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7576492119573228		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.722937703097033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7402934575271778 | validation: 1.7009395349176197]
	TIME [epoch: 8.17 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.808749641471128		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.788755823702719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7987527325869235 | validation: 2.2078952127742357]
	TIME [epoch: 8.18 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8353203628398274		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8051623792423264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8202413710410768 | validation: 1.8915699492093818]
	TIME [epoch: 8.2 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7471060367328648		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6710162952155811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.709061165974223 | validation: 1.6001356484707472]
	TIME [epoch: 8.18 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7140724858652117		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6810493629660843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6975609244156478 | validation: 1.526860711990653]
	TIME [epoch: 8.16 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6566554499999397		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.678966359395138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.667810904697539 | validation: 1.5503431199515338]
	TIME [epoch: 8.18 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5901970743745049		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6596102885072597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6249036814408822 | validation: 1.509296176198261]
	TIME [epoch: 8.2 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6350676400345852		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7141172037121557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.67459242187337 | validation: 1.5122746579086175]
	TIME [epoch: 8.19 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.587715677068365		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6839941484937058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6358549127810356 | validation: 1.4910805119381427]
	TIME [epoch: 8.17 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6470525121173936		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6413368977780105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.644194704947702 | validation: 2.5443835800695855]
	TIME [epoch: 8.18 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.243822898581058		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7056217426675535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9747223206243056 | validation: 1.6522000308297056]
	TIME [epoch: 8.18 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.526938298691777		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.709083441930824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6180108703113003 | validation: 1.4823365021334016]
	TIME [epoch: 8.19 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7141294005931773		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5308868892206018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6225081449068894 | validation: 1.3581878151326316]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5631458227799437		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6551824728010391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6091641477904914 | validation: 1.4004166105375204]
	TIME [epoch: 8.18 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5897266951579205		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6563230333556422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6230248642567815 | validation: 1.4679713989855254]
	TIME [epoch: 8.18 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5664098130377797		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6754767827106516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6209432978742153 | validation: 1.6427401954746432]
	TIME [epoch: 8.2 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6543014079617813		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.566828828346026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6105651181539038 | validation: 1.5108581445402982]
	TIME [epoch: 8.18 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6464382892242095		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.548183865409491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5973110773168502 | validation: 1.5142102216269584]
	TIME [epoch: 8.17 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5816677199034195		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5761682698363018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5789179948698604 | validation: 1.3967626747283288]
	TIME [epoch: 8.18 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.553846066875743		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4874072323379217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.520626649606832 | validation: 1.4709104644550242]
	TIME [epoch: 8.19 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5204044549384808		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4229978625835458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4717011587610134 | validation: 1.3834901938115676]
	TIME [epoch: 8.18 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.496820162071032		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5140085500050142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.505414356038023 | validation: 1.4487148731786457]
	TIME [epoch: 8.17 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5913687844635223		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5868685571608172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5891186708121698 | validation: 1.6190485275767488]
	TIME [epoch: 8.16 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6160154561550584		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4938024120024707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5549089340787645 | validation: 1.4904767392416949]
	TIME [epoch: 8.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4962709395505542		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5425914527638729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5194311961572138 | validation: 1.403514760121034]
	TIME [epoch: 8.16 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4382889954036502		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 1.5808198368801925		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 1.5095544161419214 | validation: 1.6310540722805944]
	TIME [epoch: 8.16 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5828651658107002		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 1.5953731791519803		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.58911917248134 | validation: 1.3826780971166392]
	TIME [epoch: 8.17 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4130277223002952		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 1.4434894310285327		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 1.428258576664414 | validation: 1.3602835513624973]
	TIME [epoch: 8.2 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3979235580870817		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 1.478973552365592		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 1.4384485552263369 | validation: 1.5193730507803953]
	TIME [epoch: 8.18 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4085684669246756		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 1.4619119618907914		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 1.4352402144077336 | validation: 1.306231684907906]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3243147646113975		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 1.3959025526601685		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 1.3601086586357831 | validation: 1.1984674200897034]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3378217130756789		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 1.3737054609726047		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 1.3557635870241418 | validation: 1.235151906497542]
	TIME [epoch: 8.2 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3646178500503074		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 1.351285315402988		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 1.3579515827266477 | validation: 1.3274499902380263]
	TIME [epoch: 8.18 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2869517916911		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 1.499095713425862		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 1.393023752558481 | validation: 1.3627263833023113]
	TIME [epoch: 8.18 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1588199029267696		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 1.4974865372541648		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 1.328153220090467 | validation: 1.3881864111841251]
	TIME [epoch: 8.17 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.334630390552078		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 1.3319987195871883		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 1.333314555069633 | validation: 1.3091836471279048]
	TIME [epoch: 8.2 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3985638708988275		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 1.1943545993346745		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 1.2964592351167508 | validation: 1.0766488161397554]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3027766947997041		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 1.2385547077424566		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 1.27066570127108 | validation: 1.5475084965950427]
	TIME [epoch: 8.18 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4020975288124928		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 1.249093497525068		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 1.3255955131687802 | validation: 1.2560102052805773]
	TIME [epoch: 8.19 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.35903024753262		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 1.4142025368908822		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 1.3866163922117511 | validation: 1.1887042499986253]
	TIME [epoch: 8.2 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.220089524655417		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 1.2036599499238547		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 1.211874737289636 | validation: 1.1385360257333965]
	TIME [epoch: 8.18 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1812536591094012		[learning rate: 0.009619]
		[batch 20/20] avg loss: 1.2660771853125292		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 1.223665422210965 | validation: 1.0487761047198125]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1699704790566783		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 1.2578780063598016		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 1.2139242427082397 | validation: 1.168554766682542]
	TIME [epoch: 8.17 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1927046429358321		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 1.2041844969439772		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 1.1984445699399047 | validation: 1.517219914170139]
	TIME [epoch: 8.19 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2009945732190883		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 1.1227120946943105		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 1.1618533339566997 | validation: 0.9624526830389826]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1272200022864207		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 1.2080420068295123		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 1.1676310045579663 | validation: 1.3201751133902864]
	TIME [epoch: 8.17 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0958736649271494		[learning rate: 0.009506]
		[batch 20/20] avg loss: 1.3498240191847466		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 1.2228488420559478 | validation: 1.1560273595376807]
	TIME [epoch: 8.17 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1050374372167515		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 1.191714016429441		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 1.1483757268230965 | validation: 0.9662883236411232]
	TIME [epoch: 8.19 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1155180574012449		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 1.1168856842033628		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 1.1162018708023038 | validation: 0.9432278949559064]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.24414522456806		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 1.1660387690627694		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 1.2050919968154146 | validation: 1.0523899859750048]
	TIME [epoch: 8.17 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1691205964346991		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 1.0698021702885872		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 1.119461383361643 | validation: 1.65322641927206]
	TIME [epoch: 8.17 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1577533711736048		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 1.2481695662110952		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 1.2029614686923502 | validation: 1.186627846877279]
	TIME [epoch: 8.19 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0594068258480172		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 1.2099917980169619		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 1.1346993119324895 | validation: 1.082658304869411]
	TIME [epoch: 8.18 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0261795994358236		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 1.1562591386585286		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 1.091219369047176 | validation: 1.0647758087398538]
	TIME [epoch: 8.17 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9742318816996111		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 1.0693855805552495		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 1.0218087311274302 | validation: 1.2993048924503108]
	TIME [epoch: 8.18 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.207862030393555		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 1.0315466582975197		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 1.1197043443455375 | validation: 1.0774416130416078]
	TIME [epoch: 8.2 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.093082194580958		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 1.0921813585139315		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 1.0926317765474447 | validation: 0.9045927052590845]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9221872607006107		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 1.1677563167871081		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 1.0449717887438594 | validation: 1.120520675939269]
	TIME [epoch: 8.2 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0465636675914332		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 1.039364381250847		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 1.0429640244211402 | validation: 0.7951872916592458]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0243794153868995		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 1.086400532059446		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 1.0553899737231727 | validation: 0.9425673996106401]
	TIME [epoch: 8.21 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.05060285670152		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.8997982020953298		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.975200529398425 | validation: 1.246579331008061]
	TIME [epoch: 8.18 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0010231225914736		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.9658598875492947		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.9834415050703843 | validation: 1.2802982046464686]
	TIME [epoch: 8.18 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9551659486040804		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 1.2196372717355235		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 1.087401610169802 | validation: 1.3371024896854058]
	TIME [epoch: 8.18 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8563053244074046		[learning rate: 0.009132]
		[batch 20/20] avg loss: 1.0910968742378768		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.9737010993226407 | validation: 0.6367619803219389]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7762061388418006		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.9450411545206748		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.860623646681238 | validation: 0.8004488330697083]
	TIME [epoch: 8.18 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8623374882903387		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.7525631935561418		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.8074503409232403 | validation: 0.5924846511482943]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7874755819132039		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 1.0036081054995774		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.8955418437063909 | validation: 0.6810830925209148]
	TIME [epoch: 8.17 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.650038368641748		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.7063362328026209		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.6781873007221846 | validation: 0.5411357706644946]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0393856331242728		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.6793372295626906		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.8593614313434816 | validation: 0.8381471624414856]
	TIME [epoch: 8.18 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7337319089267245		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.7654166727526537		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.7495742908396892 | validation: 1.2832362257969014]
	TIME [epoch: 8.17 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8007931113293184		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.5310274985718338		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.6659103049505761 | validation: 1.0393657925387227]
	TIME [epoch: 8.17 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8053278141400894		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.6378826176218318		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.7216052158809605 | validation: 0.7950705467198644]
	TIME [epoch: 8.21 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6746990375905633		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.4959237812000327		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.585311409395298 | validation: 0.4466282890379678]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5955592082245327		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.7373369904047945		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.6664480993146636 | validation: 0.41345842993095216]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6262311698532661		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 1.0586854129150267		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.8424582913841464 | validation: 0.6186302598023375]
	TIME [epoch: 8.18 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5281460361068724		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.5049657925139919		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.5165559143104322 | validation: 0.48946219385161427]
	TIME [epoch: 8.2 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7006908319828262		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 0.547292885031079		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.6239918585069527 | validation: 0.5155475887501791]
	TIME [epoch: 8.17 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5867600968323892		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.7129323968976543		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 0.6498462468650217 | validation: 1.0547774692084129]
	TIME [epoch: 8.16 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7011336165805477		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.6275668378850604		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.6643502272328041 | validation: 0.37246729622039243]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5720568530449303		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.5428578882462816		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.557457370645606 | validation: 0.32535682410025285]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5487768703378643		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.5821452148213585		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.5654610425796114 | validation: 0.3661637090016961]
	TIME [epoch: 8.17 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5206979966868686		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.6004460751740397		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.560572035930454 | validation: 0.38942803294331657]
	TIME [epoch: 8.17 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5763008360313757		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.8941459517037771		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.7352233938675764 | validation: 0.4998065695044784]
	TIME [epoch: 8.17 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4875186362383156		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.5989424592269431		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.5432305477326294 | validation: 0.31223991750471514]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4563251779034621		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.46618310072126945		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.4612541393123658 | validation: 0.31088779635399943]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6369005781268811		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.8934530527771652		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.7651768154520232 | validation: 0.5297527786066738]
	TIME [epoch: 8.17 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7795050188157915		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 0.5764689666573567		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.6779869927365741 | validation: 0.4655860570320799]
	TIME [epoch: 8.17 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.535841952953078		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.5650881613808405		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.5504650571669594 | validation: 0.48546641364104226]
	TIME [epoch: 8.19 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.615342719368543		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.5007702854800206		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.5580565024242818 | validation: 0.4231139801788662]
	TIME [epoch: 8.16 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4730925721578453		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.5728114358245312		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.5229520039911881 | validation: 0.538978982616634]
	TIME [epoch: 8.16 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5155679281121258		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.6690664597594516		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.5923171939357886 | validation: 0.3456242713483528]
	TIME [epoch: 8.17 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5139583795936545		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.4523670566788921		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.4831627181362734 | validation: 0.6327486153836173]
	TIME [epoch: 8.19 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6091118440399137		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 0.7159726741342787		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.6625422590870962 | validation: 0.6152873169721623]
	TIME [epoch: 8.17 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.56372383696449		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.6509724481586513		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.6073481425615708 | validation: 0.766705911839069]
	TIME [epoch: 8.16 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5447305181875169		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.46396352867829754		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.5043470234329074 | validation: 0.477788393894235]
	TIME [epoch: 8.17 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4744016603626502		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.44441108137541113		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.4594063708690307 | validation: 0.5333154847156518]
	TIME [epoch: 8.2 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5717848930168331		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.4591532433031496		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.5154690681599914 | validation: 0.497837080701897]
	TIME [epoch: 8.17 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8310325813590245		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.4992685877126751		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.6651505845358499 | validation: 0.742562329312371]
	TIME [epoch: 8.17 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4823307548181943		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.8071059088234493		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.6447183318208218 | validation: 0.31674616270730677]
	TIME [epoch: 8.17 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38964678300595434		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.4201396643236784		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.4048932236648164 | validation: 0.43635819857799196]
	TIME [epoch: 8.19 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5871201773359896		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.4495597875418638		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.5183399824389266 | validation: 0.2668335337974477]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5338635257227783		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.49812308815645956		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.515993306939619 | validation: 0.5330772262756398]
	TIME [epoch: 8.18 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8559373910851431		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.662508981270123		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.759223186177633 | validation: 0.7267975624420795]
	TIME [epoch: 8.18 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5326260083315064		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.41049340317582617		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.47155970575366635 | validation: 0.18914365791270119]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43963815229559444		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.5899769526801634		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.5148075524878788 | validation: 0.6032542577911802]
	TIME [epoch: 8.18 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6256924569137143		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.5193479043670766		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.5725201806403954 | validation: 0.3141124717326718]
	TIME [epoch: 8.18 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5270357628704823		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.4869125680176075		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.5069741654440448 | validation: 0.7859152042988241]
	TIME [epoch: 8.18 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6405749261957344		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.7343693611101869		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.6874721436529606 | validation: 0.5233122789359572]
	TIME [epoch: 8.21 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5219299006088906		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.510973019786465		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.5164514601976778 | validation: 0.3300600194122359]
	TIME [epoch: 8.18 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7893408721591004		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.6024660845591396		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.69590347835912 | validation: 0.4115199589261102]
	TIME [epoch: 8.18 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.497972840514032		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.5653653346992742		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.5316690876066531 | validation: 0.6810589500924848]
	TIME [epoch: 8.18 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7138930853415967		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.634877134161527		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.6743851097515619 | validation: 0.4874932293142333]
	TIME [epoch: 8.2 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7462682682114349		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.5195530859340998		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.6329106770727676 | validation: 0.2894963737340396]
	TIME [epoch: 8.19 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4893180029985336		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.5607457897268422		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.525031896362688 | validation: 0.29023778121982724]
	TIME [epoch: 8.18 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4981146983057534		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.7759046829068972		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.6370096906063252 | validation: 0.5557180862854787]
	TIME [epoch: 8.18 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8916060369531678		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.5041832068536853		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.6978946219034265 | validation: 0.41964520647498366]
	TIME [epoch: 8.2 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5550160459526353		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.5859723944696346		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.5704942202111348 | validation: 0.30307780472375445]
	TIME [epoch: 8.18 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41099540578073246		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.6111851150036234		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.5110902603921779 | validation: 1.1112007084813564]
	TIME [epoch: 8.18 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5210428749040632		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.8203072867148897		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.6706750808094766 | validation: 0.4567810442502708]
	TIME [epoch: 8.17 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46752714063582557		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.48464392707147425		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.47608553385364993 | validation: 0.3550588372116086]
	TIME [epoch: 8.21 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46740410517590514		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.466815202525048		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.46710965385047654 | validation: 0.3961472164238178]
	TIME [epoch: 8.18 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5070560855568176		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.3914218716650475		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.44923897861093254 | validation: 0.458215048613319]
	TIME [epoch: 8.18 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5042311203647893		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.4586149889097217		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.48142305463725543 | validation: 0.401356848097489]
	TIME [epoch: 8.17 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45937755564517024		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.3715474809610427		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.41546251830310643 | validation: 0.30547819498835477]
	TIME [epoch: 8.21 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46245896782435036		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.5072730976595119		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.4848660327419311 | validation: 0.8332707989227954]
	TIME [epoch: 8.19 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5080104385601013		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.7093809633252709		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.6086957009426862 | validation: 0.28395828087694774]
	TIME [epoch: 8.18 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4799059279059542		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.42809529314651373		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.4540006105262339 | validation: 0.307392785457654]
	TIME [epoch: 8.18 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39424226594067824		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.6358979448173614		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.5150701053790198 | validation: 0.37505054482235656]
	TIME [epoch: 8.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43931860255687133		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.5409636974306038		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.49014114999373765 | validation: 0.26982280055296326]
	TIME [epoch: 8.18 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.555909197155118		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.5726833964367921		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.564296296795955 | validation: 0.9382405608511978]
	TIME [epoch: 8.18 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5030946892332444		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.428549050410125		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.4658218698216848 | validation: 0.37899459304828925]
	TIME [epoch: 8.18 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4033467214989911		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.432459310124268		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.4179030158116297 | validation: 0.49472511139479813]
	TIME [epoch: 8.2 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44605459394322144		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.47379584412616416		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.45992521903469286 | validation: 0.5274485537496092]
	TIME [epoch: 8.18 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5325534334974943		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.42797370426541886		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.48026356888145666 | validation: 0.38988662023344267]
	TIME [epoch: 8.18 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3706146460112596		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.32849303553956977		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.34955384077541474 | validation: 0.6819930057648473]
	TIME [epoch: 8.18 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5017800020418672		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.40612173251491007		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.4539508672783887 | validation: 0.5204586985026134]
	TIME [epoch: 8.2 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6020270741193838		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.5425653550728582		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.5722962145961209 | validation: 0.4462297507529284]
	TIME [epoch: 8.19 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42602438322680636		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.39630708185637764		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.411165732541592 | validation: 0.26642845263149967]
	TIME [epoch: 8.18 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3986222558258392		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.3914916812630371		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.3950569685444381 | validation: 0.6907533176618083]
	TIME [epoch: 8.17 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44100043290716506		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.394449742929819		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.4177250879184921 | validation: 0.2843019299085228]
	TIME [epoch: 8.19 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4276440846872768		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.4476960251786912		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.437670054932984 | validation: 0.22407244589926945]
	TIME [epoch: 8.19 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3472396615682941		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.41800049203472084		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.38262007680150745 | validation: 0.36148053962127447]
	TIME [epoch: 8.18 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37871744354389364		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.41987712806905036		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.39929728580647195 | validation: 0.19349597063272903]
	TIME [epoch: 8.17 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44254723295486353		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.3682092111278907		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.4053782220413771 | validation: 0.22810227198825309]
	TIME [epoch: 8.19 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3499027308971025		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.4279887932014167		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.38894576204925957 | validation: 0.2520851205760527]
	TIME [epoch: 8.19 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4469442225102198		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.31804677982174473		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.38249550116598235 | validation: 0.260706422211951]
	TIME [epoch: 8.17 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4047640367626597		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.43365733217150926		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.4192106844670844 | validation: 0.6380953040165971]
	TIME [epoch: 8.17 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39524920944753106		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.7281645037519447		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.5617068565997381 | validation: 0.49679160837053454]
	TIME [epoch: 8.19 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5707525977975909		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.5918496471321687		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.5813011224648799 | validation: 0.20401032084805998]
	TIME [epoch: 8.19 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3330959110610351		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.3834152264602522		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.3582555687606436 | validation: 0.3893850782674423]
	TIME [epoch: 8.17 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4205967581518742		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.41691610608210705		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.4187564321169906 | validation: 0.19186580014869786]
	TIME [epoch: 8.17 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44499182199541354		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.3020053945190078		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.37349860825721065 | validation: 0.22807575782280706]
	TIME [epoch: 8.19 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40464687014555534		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.3556894813019531		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.38016817572375416 | validation: 0.7373019695298777]
	TIME [epoch: 8.18 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4510125438830829		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.5145215587751608		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.4827670513291219 | validation: 0.21651365420230267]
	TIME [epoch: 8.17 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35947246830696516		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.34796267623827853		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.3537175722726219 | validation: 0.3539202340692902]
	TIME [epoch: 8.18 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36312312503652133		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.4829440933803125		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.4230336092084169 | validation: 0.2096384455937401]
	TIME [epoch: 8.18 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3208511391323534		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.3129893249055854		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.3169202320189694 | validation: 0.2230756804615642]
	TIME [epoch: 8.2 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37814745735492167		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.4480198436841135		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.4130836505195177 | validation: 0.5156595320306936]
	TIME [epoch: 8.17 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.352477793079235		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.33520562245980795		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.34384170776952144 | validation: 0.30672149392065906]
	TIME [epoch: 8.18 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5658749769136995		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.4945002532175544		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.5301876150656268 | validation: 0.5735332637764953]
	TIME [epoch: 8.18 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3543492205115347		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.37618435020262264		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.3652667853570787 | validation: 0.605294384289556]
	TIME [epoch: 8.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3333820820246337		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.3913626960970594		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.3623723890608466 | validation: 0.4175161656157436]
	TIME [epoch: 8.17 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43174067222741497		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.30951093201736773		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.37062580212239127 | validation: 0.4317147280493257]
	TIME [epoch: 8.17 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4643066368486761		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.31604401926145687		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.3901753280550665 | validation: 0.34594086093480925]
	TIME [epoch: 8.18 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3601111986628217		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.40088363121580856		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.3804974149393151 | validation: 0.3074374418391873]
	TIME [epoch: 8.2 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4285861827638822		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.3109406726191962		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.36976342769153925 | validation: 0.5419840492683309]
	TIME [epoch: 8.17 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36177303056477006		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.4192745840495259		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.39052380730714786 | validation: 0.3160968630304516]
	TIME [epoch: 8.17 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26378586298818407		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.3510785059756609		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.3074321844819225 | validation: 0.17105443875607076]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6086014593922544		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.4050600946868748		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.5068307770395646 | validation: 0.28700988223596424]
	TIME [epoch: 8.2 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3074069730386914		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.3265399901590891		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.3169734815988902 | validation: 0.22985771112487768]
	TIME [epoch: 8.18 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4002505066159854		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.3924011943620699		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.3963258504890277 | validation: 0.31046577235571654]
	TIME [epoch: 8.17 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34369119035358237		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.3911685688561767		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.3674298796048796 | validation: 0.4565937758070381]
	TIME [epoch: 8.18 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4997909313817777		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.44363502621690876		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.4717129787993432 | validation: 0.2744628043375064]
	TIME [epoch: 8.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.425889309734123		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.2990410578342443		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.3624651837841836 | validation: 0.306558910050652]
	TIME [epoch: 8.17 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3694294019328638		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.329583894807711		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.34950664837028733 | validation: 0.662421692805509]
	TIME [epoch: 8.17 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3871245516863424		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.3846001868086124		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.3858623692474774 | validation: 0.19839783841010494]
	TIME [epoch: 8.17 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3247937851110451		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.37411851778909433		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.34945615145006975 | validation: 0.3303065374701607]
	TIME [epoch: 8.2 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34588285035339744		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.3955774848584569		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.37073016760592714 | validation: 0.6997694994865127]
	TIME [epoch: 8.17 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3531476598499549		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.3305814439093404		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.3418645518796476 | validation: 0.21780586360324078]
	TIME [epoch: 8.17 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46682921238397146		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.3065934652643339		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.38671133882415265 | validation: 0.27107599503274615]
	TIME [epoch: 8.18 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3361963458908824		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.35248849957948825		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.3443424227351853 | validation: 0.4889804981700123]
	TIME [epoch: 8.2 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3742402671838888		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.3366780747738721		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.35545917097888047 | validation: 0.49234704947955643]
	TIME [epoch: 8.17 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31292052855075675		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.31204658939117846		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.3124835589709676 | validation: 0.20015053510324377]
	TIME [epoch: 8.17 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2571990569014616		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.31797263971329104		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.2875858483073763 | validation: 0.4487218354951149]
	TIME [epoch: 8.16 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3781136629489876		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.35605419850312753		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.36708393072605755 | validation: 0.28772731989424805]
	TIME [epoch: 8.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3099557134935916		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.4343680801546759		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.37216189682413364 | validation: 0.4778843116917385]
	TIME [epoch: 8.17 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34639793899237714		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.280794531555553		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.313596235273965 | validation: 0.19948926106319895]
	TIME [epoch: 8.17 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3180639968044883		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.3638690646849009		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.34096653074469463 | validation: 0.29877988558982466]
	TIME [epoch: 8.17 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36677111486194125		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.4459037238922492		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.40633741937709517 | validation: 0.3544645287966978]
	TIME [epoch: 8.19 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31389422510685017		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.32723820087208805		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.32056621298946913 | validation: 0.35759320714192366]
	TIME [epoch: 8.17 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3351459080769036		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.296939093667293		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.3160425008720983 | validation: 0.2470557067927468]
	TIME [epoch: 8.17 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28548051814430053		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.3918827874670167		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.33868165280565854 | validation: 0.48695225541640524]
	TIME [epoch: 8.16 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.440893722139466		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.38645665794937256		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.4136751900444193 | validation: 0.31360424061882763]
	TIME [epoch: 8.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3318779726619764		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.43540493491979654		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.38364145379088654 | validation: 0.3011991362388516]
	TIME [epoch: 8.17 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48724590330526674		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.3116607295519779		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.39945331642862236 | validation: 0.19627600585557586]
	TIME [epoch: 8.17 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3299432744257569		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.4269266745131103		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.3784349744694335 | validation: 0.1882188320716891]
	TIME [epoch: 8.17 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3627167654255583		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.8053789410828571		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.5840478532542078 | validation: 0.39486655171801904]
	TIME [epoch: 8.2 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3081736953312819		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.25091076117940025		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.27954222825534114 | validation: 0.6963917887002179]
	TIME [epoch: 8.17 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3777298297668025		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.4317598903108129		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.4047448600388076 | validation: 0.306324548860581]
	TIME [epoch: 8.18 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2926014020526886		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.3593104559124054		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.325955928982547 | validation: 0.24865974587288361]
	TIME [epoch: 8.17 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3711877446381871		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.30085083183266104		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.3360192882354241 | validation: 0.6553697563315217]
	TIME [epoch: 8.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3789848193273269		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.5029459220273369		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.4409653706773319 | validation: 0.31897823657024005]
	TIME [epoch: 8.17 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32687927739348355		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.2846692565404408		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.30577426696696225 | validation: 0.2790580460122559]
	TIME [epoch: 8.16 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.331309469046352		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.6495104034595646		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.49040993625295826 | validation: 0.2646885739588537]
	TIME [epoch: 8.18 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32352887677192343		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.26530048431509334		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.29441468054350844 | validation: 0.2082051927868806]
	TIME [epoch: 8.19 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3597761994852483		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.958485547327195		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.6591308734062218 | validation: 1.0088421722741905]
	TIME [epoch: 8.18 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7173682884500219		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.34550985084189345		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.5314390696459577 | validation: 0.4547751201499596]
	TIME [epoch: 8.17 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5170072332750513		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.33117455752155384		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.4240908953983025 | validation: 0.3764547751058949]
	TIME [epoch: 8.17 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3678305393982392		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.37913892966171814		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.37348473452997866 | validation: 0.26535581121385865]
	TIME [epoch: 8.18 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3412435041295757		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.2685312026196022		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.30488735337458894 | validation: 0.3532735746860678]
	TIME [epoch: 8.18 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4067144133746874		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.3315565448233425		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.36913547909901495 | validation: 0.2653339264514836]
	TIME [epoch: 8.17 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3043381672432305		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.32875961148280364		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.3165488893630171 | validation: 0.26954971603755834]
	TIME [epoch: 8.17 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36685544177058		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.28595109250982553		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.32640326714020274 | validation: 0.6790136546111073]
	TIME [epoch: 8.19 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5278121602746211		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.31179914298998757		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.4198056516323043 | validation: 0.26990798596245147]
	TIME [epoch: 8.18 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30029440838628807		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.3330998393441296		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.3166971238652088 | validation: 0.1895868898465546]
	TIME [epoch: 8.19 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34903785318313113		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.4800932424271255		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.4145655478051283 | validation: 0.7450542735294732]
	TIME [epoch: 8.19 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.389585122006178		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.34303145763188386		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.3663082898190309 | validation: 0.7431076700125128]
	TIME [epoch: 8.2 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4250827294274419		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.3567438754774867		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.39091330245246436 | validation: 0.3457879970851123]
	TIME [epoch: 8.19 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3447185756686694		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.31228060155184173		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.32849958861025563 | validation: 0.28519972861170556]
	TIME [epoch: 8.2 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.271154674471666		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.37038593906818523		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.32077030676992563 | validation: 0.3341292720834427]
	TIME [epoch: 8.19 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29932567429680484		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.34036144874118524		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.319843561518995 | validation: 0.49702221730247303]
	TIME [epoch: 8.2 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33322491380148783		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.415893457915113		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.37455918585830045 | validation: 0.3832739425127629]
	TIME [epoch: 8.21 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3430930002154245		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.34830128787327697		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.3456971440443507 | validation: 0.27024302241859016]
	TIME [epoch: 8.19 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2723622605131547		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.3522575422461699		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.31230990137966225 | validation: 0.319587443615539]
	TIME [epoch: 8.19 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2963744168178299		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.2644773471964065		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.2804258820071182 | validation: 0.1871365889987715]
	TIME [epoch: 8.21 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30121370810449544		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.3374535830958505		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.31933364560017297 | validation: 0.4485689222304114]
	TIME [epoch: 8.21 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36073559272628103		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.32271366429485704		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.34172462851056906 | validation: 0.2514871692610341]
	TIME [epoch: 8.19 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6927448788706835		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.34823654254521375		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.5204907107079487 | validation: 0.24923581705458517]
	TIME [epoch: 8.15 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3275119259695278		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.2938405161105741		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.31067622104005094 | validation: 0.23740915671986446]
	TIME [epoch: 8.17 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2974053797280118		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.34252174631941285		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.31996356302371226 | validation: 0.17989120063575545]
	TIME [epoch: 8.19 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2545008580652103		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.3748556144861679		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.3146782362756891 | validation: 0.18483245036608964]
	TIME [epoch: 8.21 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29451498735981874		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.7054400748827186		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.4999775311212688 | validation: 0.17121413093067495]
	TIME [epoch: 8.2 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2504769492424564		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.3257323297300602		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.2881046394862583 | validation: 0.27662457009728947]
	TIME [epoch: 8.19 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2649607056946034		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.3330480045954972		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.2990043551450502 | validation: 0.32044876961430513]
	TIME [epoch: 8.22 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42216822149293465		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.30685789228401983		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.36451305688847724 | validation: 0.32378829635505235]
	TIME [epoch: 8.19 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3649407429034335		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.3098509530511918		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.3373958479773126 | validation: 0.19394072295541512]
	TIME [epoch: 8.18 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27303974890983695		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.36328869422809273		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.31816422156896484 | validation: 0.2562024072424072]
	TIME [epoch: 8.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28632717686045656		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.3069464150745296		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.29663679596749304 | validation: 0.4167478119602698]
	TIME [epoch: 8.21 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3042930163339399		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.30363144633108596		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.30396223133251304 | validation: 0.3383823141692893]
	TIME [epoch: 8.17 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28110457115786525		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.3514277014767869		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.31626613631732614 | validation: 0.3882342699772658]
	TIME [epoch: 8.17 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26765265532087373		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.27888649376747016		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.2732695745441719 | validation: 0.23301585971155817]
	TIME [epoch: 8.17 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30057313282622133		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.32865228316884654		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.31461270799753394 | validation: 0.2655516259937135]
	TIME [epoch: 8.21 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32601853643694023		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.28917706870406146		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.3075978025705008 | validation: 0.15577280627540335]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29361379573294594		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.3004643778598157		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.2970390867963808 | validation: 0.3264821815812333]
	TIME [epoch: 8.17 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2401332860186073		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.3899294961075799		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.31503139106309364 | validation: 0.5457846915032745]
	TIME [epoch: 8.17 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34830971399297206		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 0.40584371896366		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 0.3770767164783161 | validation: 0.19921021241591808]
	TIME [epoch: 8.19 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2886537084355301		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.3226335239688837		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.3056436162022068 | validation: 0.27650748656565394]
	TIME [epoch: 8.17 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4273527807925638		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 0.24510333210968716		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 0.3362280564511254 | validation: 0.26742721538465464]
	TIME [epoch: 8.17 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2992329670014905		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.32946299030996873		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.31434797865572955 | validation: 0.2003400291181944]
	TIME [epoch: 8.17 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25448891945783597		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.27583820416370025		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.26516356181076806 | validation: 0.27026586338721525]
	TIME [epoch: 8.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30798359958466903		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.27679099956157177		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.2923872995731205 | validation: 0.22428109335104393]
	TIME [epoch: 8.16 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3541882659369389		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.2965947003372144		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.32539148313707666 | validation: 0.18114722380323292]
	TIME [epoch: 8.17 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31325664990607793		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.266201569915948		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.2897291099110129 | validation: 0.15329788673468292]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27412935956293616		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.31514012024957894		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.29463473990625755 | validation: 0.29166858061421536]
	TIME [epoch: 8.2 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22347332782041568		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.2694185098769915		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.2464459188487036 | validation: 0.30922731247533697]
	TIME [epoch: 8.16 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2975780430435139		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 0.2910313378861977		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.29430469046485574 | validation: 0.1545442573591276]
	TIME [epoch: 8.16 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27497405320484697		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.46046317523126873		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.36771861421805785 | validation: 0.34760234650179345]
	TIME [epoch: 8.17 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2864270356263666		[learning rate: 0.0057755]
		[batch 20/20] avg loss: 0.30303038520760883		[learning rate: 0.0057687]
	Learning Rate: 0.00576869
	LOSS [training: 0.2947287104169878 | validation: 0.2568040417295917]
	TIME [epoch: 8.19 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2739942345371473		[learning rate: 0.0057619]
		[batch 20/20] avg loss: 0.26109830486839014		[learning rate: 0.0057551]
	Learning Rate: 0.00575508
	LOSS [training: 0.2675462697027687 | validation: 0.27636696162064317]
	TIME [epoch: 8.17 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3198980933061245		[learning rate: 0.0057483]
		[batch 20/20] avg loss: 0.2939395268253969		[learning rate: 0.0057415]
	Learning Rate: 0.0057415
	LOSS [training: 0.30691881006576066 | validation: 0.2313451739359179]
	TIME [epoch: 8.17 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3338000714136305		[learning rate: 0.0057347]
		[batch 20/20] avg loss: 0.25769357289378		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.29574682215370524 | validation: 0.5285410211050602]
	TIME [epoch: 8.17 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2785872074012089		[learning rate: 0.0057212]
		[batch 20/20] avg loss: 0.25681443789467506		[learning rate: 0.0057144]
	Learning Rate: 0.00571445
	LOSS [training: 0.2677008226479419 | validation: 0.5851542545547717]
	TIME [epoch: 8.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.355585177520284		[learning rate: 0.0057077]
		[batch 20/20] avg loss: 0.31272710812602156		[learning rate: 0.005701]
	Learning Rate: 0.00570097
	LOSS [training: 0.3341561428231528 | validation: 0.2533399765904884]
	TIME [epoch: 8.17 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23506390981075592		[learning rate: 0.0056942]
		[batch 20/20] avg loss: 0.4006471888572391		[learning rate: 0.0056875]
	Learning Rate: 0.00568752
	LOSS [training: 0.3178555493339975 | validation: 0.20765321445351825]
	TIME [epoch: 8.17 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2975649979316529		[learning rate: 0.0056808]
		[batch 20/20] avg loss: 0.2300706541873871		[learning rate: 0.0056741]
	Learning Rate: 0.00567411
	LOSS [training: 0.2638178260595201 | validation: 0.1817346495609404]
	TIME [epoch: 8.17 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2666178339482085		[learning rate: 0.0056674]
		[batch 20/20] avg loss: 0.32587595451633605		[learning rate: 0.0056607]
	Learning Rate: 0.00566072
	LOSS [training: 0.2962468942322723 | validation: 0.22269292219754006]
	TIME [epoch: 8.2 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2823305840683036		[learning rate: 0.005654]
		[batch 20/20] avg loss: 0.2100247442175372		[learning rate: 0.0056474]
	Learning Rate: 0.00564737
	LOSS [training: 0.24617766414292047 | validation: 0.5211506252896142]
	TIME [epoch: 8.17 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.547437477403893		[learning rate: 0.0056407]
		[batch 20/20] avg loss: 0.3363060840545254		[learning rate: 0.005634]
	Learning Rate: 0.00563405
	LOSS [training: 0.4418717807292092 | validation: 0.15015255026048413]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2369823598957323		[learning rate: 0.0056274]
		[batch 20/20] avg loss: 0.2583304752222174		[learning rate: 0.0056208]
	Learning Rate: 0.00562076
	LOSS [training: 0.24765641755897488 | validation: 0.2965170671816379]
	TIME [epoch: 8.17 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2760162172880806		[learning rate: 0.0056141]
		[batch 20/20] avg loss: 0.3357039660849457		[learning rate: 0.0056075]
	Learning Rate: 0.0056075
	LOSS [training: 0.3058600916865132 | validation: 0.20928382283734395]
	TIME [epoch: 8.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20426670408976683		[learning rate: 0.0056009]
		[batch 20/20] avg loss: 0.2440105929693155		[learning rate: 0.0055943]
	Learning Rate: 0.00559427
	LOSS [training: 0.22413864852954113 | validation: 0.37875048622057894]
	TIME [epoch: 8.17 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30589879726643165		[learning rate: 0.0055877]
		[batch 20/20] avg loss: 0.2439652492547088		[learning rate: 0.0055811]
	Learning Rate: 0.00558108
	LOSS [training: 0.2749320232605702 | validation: 0.22345447253528294]
	TIME [epoch: 8.17 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28827864457849905		[learning rate: 0.0055745]
		[batch 20/20] avg loss: 0.2758648258150302		[learning rate: 0.0055679]
	Learning Rate: 0.00556791
	LOSS [training: 0.28207173519676465 | validation: 0.14832233654634622]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27335877140412873		[learning rate: 0.0055613]
		[batch 20/20] avg loss: 0.45503752840006184		[learning rate: 0.0055548]
	Learning Rate: 0.00555478
	LOSS [training: 0.3641981499020953 | validation: 0.31080092611662885]
	TIME [epoch: 8.19 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2918690496495969		[learning rate: 0.0055482]
		[batch 20/20] avg loss: 0.22288018049819786		[learning rate: 0.0055417]
	Learning Rate: 0.00554167
	LOSS [training: 0.2573746150738974 | validation: 0.21589901077355012]
	TIME [epoch: 8.18 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24163932234202612		[learning rate: 0.0055351]
		[batch 20/20] avg loss: 0.2610171072555478		[learning rate: 0.0055286]
	Learning Rate: 0.0055286
	LOSS [training: 0.2513282147987869 | validation: 0.21260264678273677]
	TIME [epoch: 8.17 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27699843404310925		[learning rate: 0.0055221]
		[batch 20/20] avg loss: 0.270437977702192		[learning rate: 0.0055156]
	Learning Rate: 0.00551556
	LOSS [training: 0.27371820587265067 | validation: 0.20310291915206838]
	TIME [epoch: 8.17 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24499073304164826		[learning rate: 0.0055091]
		[batch 20/20] avg loss: 0.2988595497756638		[learning rate: 0.0055026]
	Learning Rate: 0.00550255
	LOSS [training: 0.27192514140865603 | validation: 0.2942011602753286]
	TIME [epoch: 8.19 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31615918359382267		[learning rate: 0.0054961]
		[batch 20/20] avg loss: 0.23989314649187038		[learning rate: 0.0054896]
	Learning Rate: 0.00548957
	LOSS [training: 0.2780261650428466 | validation: 0.18084971088589602]
	TIME [epoch: 8.17 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27514872209937563		[learning rate: 0.0054831]
		[batch 20/20] avg loss: 0.23720509531539974		[learning rate: 0.0054766]
	Learning Rate: 0.00547662
	LOSS [training: 0.25617690870738763 | validation: 0.23008294560622097]
	TIME [epoch: 8.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29024995788837465		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.25508609441397556		[learning rate: 0.0054637]
	Learning Rate: 0.0054637
	LOSS [training: 0.2726680261511751 | validation: 0.28983618762852015]
	TIME [epoch: 8.17 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3624715083777233		[learning rate: 0.0054573]
		[batch 20/20] avg loss: 0.24919419929900882		[learning rate: 0.0054508]
	Learning Rate: 0.00545082
	LOSS [training: 0.30583285383836606 | validation: 0.16504401087587173]
	TIME [epoch: 8.2 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30112800435060516		[learning rate: 0.0054444]
		[batch 20/20] avg loss: 0.3489599323383694		[learning rate: 0.005438]
	Learning Rate: 0.00543796
	LOSS [training: 0.32504396834448734 | validation: 0.18060693663219385]
	TIME [epoch: 8.18 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2982340610950046		[learning rate: 0.0054315]
		[batch 20/20] avg loss: 0.26584963372119297		[learning rate: 0.0054251]
	Learning Rate: 0.00542513
	LOSS [training: 0.28204184740809884 | validation: 0.20465207734714336]
	TIME [epoch: 8.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23742620602726555		[learning rate: 0.0054187]
		[batch 20/20] avg loss: 0.28387349125447614		[learning rate: 0.0054123]
	Learning Rate: 0.00541233
	LOSS [training: 0.2606498486408709 | validation: 0.2511115678411847]
	TIME [epoch: 8.17 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24672506957525267		[learning rate: 0.0054059]
		[batch 20/20] avg loss: 0.2796526324281652		[learning rate: 0.0053996]
	Learning Rate: 0.00539957
	LOSS [training: 0.2631888510017089 | validation: 1.6591434166377046]
	TIME [epoch: 8.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42937667222484865		[learning rate: 0.0053932]
		[batch 20/20] avg loss: 0.26928014444251075		[learning rate: 0.0053868]
	Learning Rate: 0.00538683
	LOSS [training: 0.34932840833367973 | validation: 0.13876132444649164]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2534831971421671		[learning rate: 0.0053805]
		[batch 20/20] avg loss: 0.21090086688653403		[learning rate: 0.0053741]
	Learning Rate: 0.00537412
	LOSS [training: 0.2321920320143506 | validation: 0.3964889575477124]
	TIME [epoch: 8.17 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2549689619236248		[learning rate: 0.0053678]
		[batch 20/20] avg loss: 0.42342487381940763		[learning rate: 0.0053614]
	Learning Rate: 0.00536145
	LOSS [training: 0.33919691787151623 | validation: 0.13109438823552527]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30632815078681774		[learning rate: 0.0053551]
		[batch 20/20] avg loss: 0.26442654807314736		[learning rate: 0.0053488]
	Learning Rate: 0.0053488
	LOSS [training: 0.28537734942998255 | validation: 0.19312175040805063]
	TIME [epoch: 8.19 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23693206568842223		[learning rate: 0.0053425]
		[batch 20/20] avg loss: 0.24480671290379213		[learning rate: 0.0053362]
	Learning Rate: 0.00533618
	LOSS [training: 0.2408693892961072 | validation: 0.16415252497429372]
	TIME [epoch: 8.16 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.314871330083084		[learning rate: 0.0053299]
		[batch 20/20] avg loss: 0.36776857857483425		[learning rate: 0.0053236]
	Learning Rate: 0.0053236
	LOSS [training: 0.3413199543289591 | validation: 0.18693846742777914]
	TIME [epoch: 8.17 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23848975209217843		[learning rate: 0.0053173]
		[batch 20/20] avg loss: 0.23311188816969283		[learning rate: 0.005311]
	Learning Rate: 0.00531104
	LOSS [training: 0.23580082013093567 | validation: 0.17093466385177467]
	TIME [epoch: 8.16 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25984672755394483		[learning rate: 0.0053048]
		[batch 20/20] avg loss: 0.3055123990157148		[learning rate: 0.0052985]
	Learning Rate: 0.00529851
	LOSS [training: 0.28267956328482974 | validation: 0.40548304016580783]
	TIME [epoch: 8.18 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23557616702147283		[learning rate: 0.0052923]
		[batch 20/20] avg loss: 0.5656893489542251		[learning rate: 0.005286]
	Learning Rate: 0.00528601
	LOSS [training: 0.4006327579878489 | validation: 0.22101267874269845]
	TIME [epoch: 8.16 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31648848349364683		[learning rate: 0.0052798]
		[batch 20/20] avg loss: 0.32408715223643075		[learning rate: 0.0052735]
	Learning Rate: 0.00527354
	LOSS [training: 0.32028781786503874 | validation: 0.4560186073591938]
	TIME [epoch: 8.15 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29749952741409		[learning rate: 0.0052673]
		[batch 20/20] avg loss: 0.2771333494406715		[learning rate: 0.0052611]
	Learning Rate: 0.0052611
	LOSS [training: 0.28731643842738075 | validation: 0.2631946679632769]
	TIME [epoch: 8.16 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35067162024633813		[learning rate: 0.0052549]
		[batch 20/20] avg loss: 0.22694512693982682		[learning rate: 0.0052487]
	Learning Rate: 0.00524869
	LOSS [training: 0.28880837359308253 | validation: 0.12886788273049252]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2376337164184501		[learning rate: 0.0052425]
		[batch 20/20] avg loss: 0.2834927151585885		[learning rate: 0.0052363]
	Learning Rate: 0.00523631
	LOSS [training: 0.26056321578851926 | validation: 0.22489626222619752]
	TIME [epoch: 8.18 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27064018419381586		[learning rate: 0.0052301]
		[batch 20/20] avg loss: 0.2739998994371043		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.27232004181546 | validation: 0.20376438508431738]
	TIME [epoch: 8.16 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27296604175764644		[learning rate: 0.0052178]
		[batch 20/20] avg loss: 0.23326215608710837		[learning rate: 0.0052116]
	Learning Rate: 0.00521164
	LOSS [training: 0.2531140989223774 | validation: 0.42620646996219114]
	TIME [epoch: 8.16 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2504054606637661		[learning rate: 0.0052055]
		[batch 20/20] avg loss: 0.2502197375161159		[learning rate: 0.0051993]
	Learning Rate: 0.00519935
	LOSS [training: 0.25031259908994097 | validation: 0.2524772043418931]
	TIME [epoch: 8.19 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2724872708201046		[learning rate: 0.0051932]
		[batch 20/20] avg loss: 0.296799795539252		[learning rate: 0.0051871]
	Learning Rate: 0.00518708
	LOSS [training: 0.2846435331796783 | validation: 0.3076706404465105]
	TIME [epoch: 8.18 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35773320488311605		[learning rate: 0.005181]
		[batch 20/20] avg loss: 0.475865935567127		[learning rate: 0.0051748]
	Learning Rate: 0.00517485
	LOSS [training: 0.4167995702251215 | validation: 0.23250651786197007]
	TIME [epoch: 8.17 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4424803526910317		[learning rate: 0.0051687]
		[batch 20/20] avg loss: 0.55157750191004		[learning rate: 0.0051626]
	Learning Rate: 0.00516264
	LOSS [training: 0.4970289273005358 | validation: 0.3551377356364095]
	TIME [epoch: 8.17 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35197468270841414		[learning rate: 0.0051565]
		[batch 20/20] avg loss: 0.33145894100896744		[learning rate: 0.0051505]
	Learning Rate: 0.00515046
	LOSS [training: 0.34171681185869074 | validation: 0.13048208754691265]
	TIME [epoch: 8.18 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21120910221835115		[learning rate: 0.0051444]
		[batch 20/20] avg loss: 0.30427341261417384		[learning rate: 0.0051383]
	Learning Rate: 0.00513831
	LOSS [training: 0.2577412574162625 | validation: 0.23209033213346428]
	TIME [epoch: 8.19 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28012183073312114		[learning rate: 0.0051322]
		[batch 20/20] avg loss: 0.22709974465004854		[learning rate: 0.0051262]
	Learning Rate: 0.00512619
	LOSS [training: 0.25361078769158485 | validation: 0.24174483187761725]
	TIME [epoch: 8.17 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21436894247103927		[learning rate: 0.0051201]
		[batch 20/20] avg loss: 0.2936193753110291		[learning rate: 0.0051141]
	Learning Rate: 0.0051141
	LOSS [training: 0.2539941588910342 | validation: 0.3166452151581486]
	TIME [epoch: 8.17 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23256986350251824		[learning rate: 0.0051081]
		[batch 20/20] avg loss: 0.31622956662314233		[learning rate: 0.005102]
	Learning Rate: 0.00510204
	LOSS [training: 0.2743997150628303 | validation: 0.23072283951213074]
	TIME [epoch: 8.19 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3551249587559144		[learning rate: 0.005096]
		[batch 20/20] avg loss: 0.2220910806743611		[learning rate: 0.00509]
	Learning Rate: 0.00509
	LOSS [training: 0.28860801971513783 | validation: 0.250884549061081]
	TIME [epoch: 8.18 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23159933188691353		[learning rate: 0.005084]
		[batch 20/20] avg loss: 0.3288722153429176		[learning rate: 0.005078]
	Learning Rate: 0.00507799
	LOSS [training: 0.2802357736149156 | validation: 0.3277448141581286]
	TIME [epoch: 8.17 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2509040473227561		[learning rate: 0.005072]
		[batch 20/20] avg loss: 0.27009443463298655		[learning rate: 0.005066]
	Learning Rate: 0.00506602
	LOSS [training: 0.26049924097787136 | validation: 0.23206622813355954]
	TIME [epoch: 8.17 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26784757849901586		[learning rate: 0.00506]
		[batch 20/20] avg loss: 0.30217222718138476		[learning rate: 0.0050541]
	Learning Rate: 0.00505407
	LOSS [training: 0.2850099028402003 | validation: 0.2509108393495856]
	TIME [epoch: 8.19 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2853032394771493		[learning rate: 0.0050481]
		[batch 20/20] avg loss: 0.2804458715431019		[learning rate: 0.0050421]
	Learning Rate: 0.00504215
	LOSS [training: 0.2828745555101257 | validation: 0.213318859692358]
	TIME [epoch: 8.18 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2312039216566933		[learning rate: 0.0050362]
		[batch 20/20] avg loss: 0.2287249760434325		[learning rate: 0.0050303]
	Learning Rate: 0.00503025
	LOSS [training: 0.2299644488500629 | validation: 0.2986137866637108]
	TIME [epoch: 8.17 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3935457918047984		[learning rate: 0.0050243]
		[batch 20/20] avg loss: 0.2299328562728704		[learning rate: 0.0050184]
	Learning Rate: 0.00501839
	LOSS [training: 0.3117393240388345 | validation: 0.23818500703169598]
	TIME [epoch: 8.17 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3207494474223992		[learning rate: 0.0050125]
		[batch 20/20] avg loss: 0.23088564205913015		[learning rate: 0.0050065]
	Learning Rate: 0.00500655
	LOSS [training: 0.2758175447407647 | validation: 0.1771520828126984]
	TIME [epoch: 8.18 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3071560693396648		[learning rate: 0.0050006]
		[batch 20/20] avg loss: 0.24655860445385955		[learning rate: 0.0049947]
	Learning Rate: 0.00499474
	LOSS [training: 0.2768573368967621 | validation: 0.21706801531578498]
	TIME [epoch: 8.19 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28972375572776965		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.33635209259100246		[learning rate: 0.004983]
	Learning Rate: 0.00498296
	LOSS [training: 0.313037924159386 | validation: 0.19340861723868585]
	TIME [epoch: 8.17 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2479718617702546		[learning rate: 0.0049771]
		[batch 20/20] avg loss: 0.2980659935548765		[learning rate: 0.0049712]
	Learning Rate: 0.0049712
	LOSS [training: 0.2730189276625656 | validation: 0.18874916286119722]
	TIME [epoch: 8.18 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3681674724192207		[learning rate: 0.0049653]
		[batch 20/20] avg loss: 0.28479624930031755		[learning rate: 0.0049595]
	Learning Rate: 0.00495948
	LOSS [training: 0.3264818608597691 | validation: 0.18184797219017237]
	TIME [epoch: 8.18 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2711863210822032		[learning rate: 0.0049536]
		[batch 20/20] avg loss: 0.2498360175091095		[learning rate: 0.0049478]
	Learning Rate: 0.00494778
	LOSS [training: 0.2605111692956563 | validation: 0.16844009222479983]
	TIME [epoch: 8.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25290665375291865		[learning rate: 0.0049419]
		[batch 20/20] avg loss: 0.263982519533481		[learning rate: 0.0049361]
	Learning Rate: 0.00493611
	LOSS [training: 0.2584445866431998 | validation: 0.25454886915209973]
	TIME [epoch: 8.17 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2674603122401259		[learning rate: 0.0049303]
		[batch 20/20] avg loss: 0.2622741882659073		[learning rate: 0.0049245]
	Learning Rate: 0.00492446
	LOSS [training: 0.2648672502530166 | validation: 0.16924193227378914]
	TIME [epoch: 8.17 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29079437866210867		[learning rate: 0.0049187]
		[batch 20/20] avg loss: 0.2491510895623811		[learning rate: 0.0049128]
	Learning Rate: 0.00491285
	LOSS [training: 0.2699727341122449 | validation: 0.4378133941049157]
	TIME [epoch: 8.17 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2139076557545257		[learning rate: 0.0049071]
		[batch 20/20] avg loss: 0.2654943091055296		[learning rate: 0.0049013]
	Learning Rate: 0.00490126
	LOSS [training: 0.23970098243002766 | validation: 0.31721003608715015]
	TIME [epoch: 8.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23791468041625913		[learning rate: 0.0048955]
		[batch 20/20] avg loss: 0.2754268349835318		[learning rate: 0.0048897]
	Learning Rate: 0.0048897
	LOSS [training: 0.25667075769989545 | validation: 1.0255630779563947]
	TIME [epoch: 8.17 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35027681805556554		[learning rate: 0.0048839]
		[batch 20/20] avg loss: 0.24255043803129958		[learning rate: 0.0048782]
	Learning Rate: 0.00487816
	LOSS [training: 0.2964136280434325 | validation: 0.3058499514709325]
	TIME [epoch: 8.18 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3219367605707535		[learning rate: 0.0048724]
		[batch 20/20] avg loss: 0.2898341718545583		[learning rate: 0.0048667]
	Learning Rate: 0.00486666
	LOSS [training: 0.3058854662126559 | validation: 0.1919150053131761]
	TIME [epoch: 8.17 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24324412691662864		[learning rate: 0.0048609]
		[batch 20/20] avg loss: 0.26434845396935847		[learning rate: 0.0048552]
	Learning Rate: 0.00485518
	LOSS [training: 0.2537962904429935 | validation: 0.13860123631776333]
	TIME [epoch: 8.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20509121703360744		[learning rate: 0.0048494]
		[batch 20/20] avg loss: 0.2812505752942531		[learning rate: 0.0048437]
	Learning Rate: 0.00484372
	LOSS [training: 0.24317089616393028 | validation: 0.19993835319210904]
	TIME [epoch: 8.17 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2283590608659412		[learning rate: 0.004838]
		[batch 20/20] avg loss: 0.22752714473768557		[learning rate: 0.0048323]
	Learning Rate: 0.0048323
	LOSS [training: 0.22794310280181337 | validation: 0.2191568074085134]
	TIME [epoch: 8.17 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37721144936571455		[learning rate: 0.0048266]
		[batch 20/20] avg loss: 0.3064650402916399		[learning rate: 0.0048209]
	Learning Rate: 0.0048209
	LOSS [training: 0.34183824482867714 | validation: 0.3523534824443972]
	TIME [epoch: 8.18 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23856035986455626		[learning rate: 0.0048152]
		[batch 20/20] avg loss: 0.24386613116890848		[learning rate: 0.0048095]
	Learning Rate: 0.00480953
	LOSS [training: 0.2412132455167324 | validation: 0.17470151133178924]
	TIME [epoch: 8.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16893199600097208		[learning rate: 0.0048039]
		[batch 20/20] avg loss: 0.2540301220776021		[learning rate: 0.0047982]
	Learning Rate: 0.00479818
	LOSS [training: 0.2114810590392871 | validation: 0.20568741622879094]
	TIME [epoch: 8.18 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2275197145821517		[learning rate: 0.0047925]
		[batch 20/20] avg loss: 0.2166345836680547		[learning rate: 0.0047869]
	Learning Rate: 0.00478687
	LOSS [training: 0.22207714912510318 | validation: 0.22321842392013358]
	TIME [epoch: 8.17 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21249137023601855		[learning rate: 0.0047812]
		[batch 20/20] avg loss: 0.24461034906696194		[learning rate: 0.0047756]
	Learning Rate: 0.00477557
	LOSS [training: 0.2285508596514902 | validation: 0.20124892822124704]
	TIME [epoch: 8.18 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2152918522166576		[learning rate: 0.0047699]
		[batch 20/20] avg loss: 0.2698925842360941		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.24259221822637586 | validation: 0.2764569655166828]
	TIME [epoch: 8.2 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22457138700186224		[learning rate: 0.0047587]
		[batch 20/20] avg loss: 0.2939908094585085		[learning rate: 0.0047531]
	Learning Rate: 0.00475307
	LOSS [training: 0.2592810982301853 | validation: 0.14947972572302032]
	TIME [epoch: 8.17 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21663916014193224		[learning rate: 0.0047475]
		[batch 20/20] avg loss: 0.2423785417783281		[learning rate: 0.0047419]
	Learning Rate: 0.00474186
	LOSS [training: 0.22950885096013013 | validation: 1.0760707096498032]
	TIME [epoch: 8.18 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38881188894917973		[learning rate: 0.0047363]
		[batch 20/20] avg loss: 0.21888473658751964		[learning rate: 0.0047307]
	Learning Rate: 0.00473067
	LOSS [training: 0.30384831276834967 | validation: 0.17379806272934012]
	TIME [epoch: 8.17 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2543705120622578		[learning rate: 0.0047251]
		[batch 20/20] avg loss: 0.2829095788853303		[learning rate: 0.0047195]
	Learning Rate: 0.00471952
	LOSS [training: 0.2686400454737941 | validation: 0.16066733277396875]
	TIME [epoch: 8.2 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2464906291884501		[learning rate: 0.0047139]
		[batch 20/20] avg loss: 0.2865630372366807		[learning rate: 0.0047084]
	Learning Rate: 0.00470838
	LOSS [training: 0.26652683321256543 | validation: 0.20669039840555337]
	TIME [epoch: 8.16 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2710564572677788		[learning rate: 0.0047028]
		[batch 20/20] avg loss: 0.24965668465453708		[learning rate: 0.0046973]
	Learning Rate: 0.00469728
	LOSS [training: 0.260356570961158 | validation: 0.1842886775537359]
	TIME [epoch: 8.16 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24465251619003525		[learning rate: 0.0046917]
		[batch 20/20] avg loss: 0.2194776753556759		[learning rate: 0.0046862]
	Learning Rate: 0.0046862
	LOSS [training: 0.23206509577285553 | validation: 0.1380485877591211]
	TIME [epoch: 8.17 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22814686547037238		[learning rate: 0.0046807]
		[batch 20/20] avg loss: 0.19682490691350712		[learning rate: 0.0046751]
	Learning Rate: 0.00467514
	LOSS [training: 0.21248588619193973 | validation: 0.22704229236315454]
	TIME [epoch: 8.19 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.302280738337647		[learning rate: 0.0046696]
		[batch 20/20] avg loss: 0.2223216665580802		[learning rate: 0.0046641]
	Learning Rate: 0.00466411
	LOSS [training: 0.2623012024478636 | validation: 0.17663614870029215]
	TIME [epoch: 8.18 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17515598099422827		[learning rate: 0.0046586]
		[batch 20/20] avg loss: 0.3254340318693272		[learning rate: 0.0046531]
	Learning Rate: 0.00465311
	LOSS [training: 0.2502950064317777 | validation: 0.11749950263936532]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28455972898240983		[learning rate: 0.0046476]
		[batch 20/20] avg loss: 0.2050248280471576		[learning rate: 0.0046421]
	Learning Rate: 0.00464214
	LOSS [training: 0.24479227851478372 | validation: 0.3009559157765397]
	TIME [epoch: 8.18 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2759295651884985		[learning rate: 0.0046367]
		[batch 20/20] avg loss: 0.2591684776744089		[learning rate: 0.0046312]
	Learning Rate: 0.00463119
	LOSS [training: 0.2675490214314537 | validation: 0.28463911637905276]
	TIME [epoch: 8.19 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2497429740027759		[learning rate: 0.0046257]
		[batch 20/20] avg loss: 0.22837804487135424		[learning rate: 0.0046203]
	Learning Rate: 0.00462026
	LOSS [training: 0.2390605094370651 | validation: 0.2491391615513641]
	TIME [epoch: 8.18 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29538603607361624		[learning rate: 0.0046148]
		[batch 20/20] avg loss: 0.2106783048622059		[learning rate: 0.0046094]
	Learning Rate: 0.00460936
	LOSS [training: 0.25303217046791104 | validation: 0.2116993727604613]
	TIME [epoch: 8.17 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20691241544169667		[learning rate: 0.0046039]
		[batch 20/20] avg loss: 0.23429467863367365		[learning rate: 0.0045985]
	Learning Rate: 0.00459849
	LOSS [training: 0.22060354703768517 | validation: 0.1769536371396792]
	TIME [epoch: 8.17 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2547535170733987		[learning rate: 0.0045931]
		[batch 20/20] avg loss: 0.25458772833321924		[learning rate: 0.0045876]
	Learning Rate: 0.00458764
	LOSS [training: 0.254670622703309 | validation: 0.2058048335321126]
	TIME [epoch: 8.2 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23680479510806457		[learning rate: 0.0045822]
		[batch 20/20] avg loss: 0.2257060009266906		[learning rate: 0.0045768]
	Learning Rate: 0.00457682
	LOSS [training: 0.23125539801737763 | validation: 0.12068104201277939]
	TIME [epoch: 8.17 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23538998764977775		[learning rate: 0.0045714]
		[batch 20/20] avg loss: 0.22033768717038277		[learning rate: 0.004566]
	Learning Rate: 0.00456603
	LOSS [training: 0.22786383741008026 | validation: 0.3016537915379857]
	TIME [epoch: 8.18 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3407365187935941		[learning rate: 0.0045606]
		[batch 20/20] avg loss: 0.20363757073909383		[learning rate: 0.0045553]
	Learning Rate: 0.00455526
	LOSS [training: 0.27218704476634403 | validation: 0.16159025354685988]
	TIME [epoch: 8.17 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1778528702910968		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.21137822750519225		[learning rate: 0.0045445]
	Learning Rate: 0.00454451
	LOSS [training: 0.19461554889814453 | validation: 0.24392865696973937]
	TIME [epoch: 8.19 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2902037690420136		[learning rate: 0.0045391]
		[batch 20/20] avg loss: 0.21311682400652487		[learning rate: 0.0045338]
	Learning Rate: 0.00453379
	LOSS [training: 0.25166029652426924 | validation: 0.17657899821834938]
	TIME [epoch: 8.17 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21824439411427976		[learning rate: 0.0045284]
		[batch 20/20] avg loss: 0.21682292138030684		[learning rate: 0.0045231]
	Learning Rate: 0.0045231
	LOSS [training: 0.21753365774729333 | validation: 0.24756757678432922]
	TIME [epoch: 8.17 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43444422908815045		[learning rate: 0.0045178]
		[batch 20/20] avg loss: 0.316077681593833		[learning rate: 0.0045124]
	Learning Rate: 0.00451243
	LOSS [training: 0.37526095534099174 | validation: 0.20200291528013803]
	TIME [epoch: 8.17 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25650746572179894		[learning rate: 0.0045071]
		[batch 20/20] avg loss: 0.2554547733213701		[learning rate: 0.0045018]
	Learning Rate: 0.00450178
	LOSS [training: 0.25598111952158453 | validation: 0.1395449555575743]
	TIME [epoch: 8.2 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3099445603338916		[learning rate: 0.0044965]
		[batch 20/20] avg loss: 0.22330424431969903		[learning rate: 0.0044912]
	Learning Rate: 0.00449116
	LOSS [training: 0.26662440232679535 | validation: 0.15228761441592092]
	TIME [epoch: 8.17 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2513179860969178		[learning rate: 0.0044859]
		[batch 20/20] avg loss: 0.21612328088920413		[learning rate: 0.0044806]
	Learning Rate: 0.00448057
	LOSS [training: 0.233720633493061 | validation: 0.2154303500442613]
	TIME [epoch: 8.17 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20778822919820955		[learning rate: 0.0044753]
		[batch 20/20] avg loss: 0.25575166303592145		[learning rate: 0.00447]
	Learning Rate: 0.00447
	LOSS [training: 0.23176994611706547 | validation: 0.1573644256403535]
	TIME [epoch: 8.17 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21725682656237583		[learning rate: 0.0044647]
		[batch 20/20] avg loss: 0.2891499415059526		[learning rate: 0.0044595]
	Learning Rate: 0.00445946
	LOSS [training: 0.2532033840341642 | validation: 0.10664515414914344]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23052441430244386		[learning rate: 0.0044542]
		[batch 20/20] avg loss: 0.19873433053132689		[learning rate: 0.0044489]
	Learning Rate: 0.00444894
	LOSS [training: 0.21462937241688537 | validation: 0.144167363541173]
	TIME [epoch: 8.18 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22278798731461205		[learning rate: 0.0044437]
		[batch 20/20] avg loss: 0.2422009729497355		[learning rate: 0.0044384]
	Learning Rate: 0.00443844
	LOSS [training: 0.23249448013217372 | validation: 0.1562501803779724]
	TIME [epoch: 8.18 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21506022300343122		[learning rate: 0.0044332]
		[batch 20/20] avg loss: 0.2265602301167243		[learning rate: 0.004428]
	Learning Rate: 0.00442797
	LOSS [training: 0.22081022656007782 | validation: 0.16163499109082502]
	TIME [epoch: 8.17 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16795456856918417		[learning rate: 0.0044227]
		[batch 20/20] avg loss: 0.21648637254365152		[learning rate: 0.0044175]
	Learning Rate: 0.00441753
	LOSS [training: 0.19222047055641786 | validation: 0.28945392336704207]
	TIME [epoch: 8.19 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26501583663538075		[learning rate: 0.0044123]
		[batch 20/20] avg loss: 0.2549814474349682		[learning rate: 0.0044071]
	Learning Rate: 0.00440711
	LOSS [training: 0.2599986420351746 | validation: 0.19401964621248236]
	TIME [epoch: 8.17 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18089200899989955		[learning rate: 0.0044019]
		[batch 20/20] avg loss: 0.2718886897450891		[learning rate: 0.0043967]
	Learning Rate: 0.00439671
	LOSS [training: 0.22639034937249428 | validation: 0.13588814837909016]
	TIME [epoch: 8.17 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22005086327389423		[learning rate: 0.0043915]
		[batch 20/20] avg loss: 0.2553828377776422		[learning rate: 0.0043863]
	Learning Rate: 0.00438634
	LOSS [training: 0.23771685052576821 | validation: 0.24261422285410503]
	TIME [epoch: 8.16 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2502700487360273		[learning rate: 0.0043812]
		[batch 20/20] avg loss: 0.2120262771578089		[learning rate: 0.004376]
	Learning Rate: 0.004376
	LOSS [training: 0.23114816294691815 | validation: 0.2210611475953023]
	TIME [epoch: 8.19 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18225063579337453		[learning rate: 0.0043708]
		[batch 20/20] avg loss: 0.20876699389606176		[learning rate: 0.0043657]
	Learning Rate: 0.00436567
	LOSS [training: 0.19550881484471813 | validation: 0.2169168350170556]
	TIME [epoch: 8.19 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2607730701737744		[learning rate: 0.0043605]
		[batch 20/20] avg loss: 0.2145835511939635		[learning rate: 0.0043554]
	Learning Rate: 0.00435538
	LOSS [training: 0.23767831068386897 | validation: 0.29421521158016617]
	TIME [epoch: 8.16 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2315197137938108		[learning rate: 0.0043502]
		[batch 20/20] avg loss: 0.2443377003381471		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.23792870706597893 | validation: 0.1888245723197055]
	TIME [epoch: 8.17 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1719080483386862		[learning rate: 0.00434]
		[batch 20/20] avg loss: 0.2196723098570629		[learning rate: 0.0043349]
	Learning Rate: 0.00433485
	LOSS [training: 0.19579017909787455 | validation: 0.14047914282409904]
	TIME [epoch: 8.18 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1701000611555398		[learning rate: 0.0043297]
		[batch 20/20] avg loss: 0.22595126644306102		[learning rate: 0.0043246]
	Learning Rate: 0.00432463
	LOSS [training: 0.1980256637993004 | validation: 0.24510115404635982]
	TIME [epoch: 8.19 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23576331809161505		[learning rate: 0.0043195]
		[batch 20/20] avg loss: 0.21748419001155667		[learning rate: 0.0043144]
	Learning Rate: 0.00431443
	LOSS [training: 0.22662375405158586 | validation: 0.25162072433284477]
	TIME [epoch: 8.17 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20424745959439963		[learning rate: 0.0043093]
		[batch 20/20] avg loss: 0.2156398600740789		[learning rate: 0.0043042]
	Learning Rate: 0.00430425
	LOSS [training: 0.2099436598342393 | validation: 0.17837751197099033]
	TIME [epoch: 8.17 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2386787456412227		[learning rate: 0.0042992]
		[batch 20/20] avg loss: 0.36028887164417106		[learning rate: 0.0042941]
	Learning Rate: 0.0042941
	LOSS [training: 0.29948380864269686 | validation: 0.21793729732674133]
	TIME [epoch: 8.18 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2613825793963623		[learning rate: 0.004289]
		[batch 20/20] avg loss: 0.2791431479739605		[learning rate: 0.004284]
	Learning Rate: 0.00428397
	LOSS [training: 0.2702628636851614 | validation: 0.37887352714518846]
	TIME [epoch: 8.19 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19291807832340782		[learning rate: 0.0042789]
		[batch 20/20] avg loss: 0.22367027772624737		[learning rate: 0.0042739]
	Learning Rate: 0.00427386
	LOSS [training: 0.2082941780248276 | validation: 0.21400655914006272]
	TIME [epoch: 8.17 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23990144811950306		[learning rate: 0.0042688]
		[batch 20/20] avg loss: 0.2337496338953282		[learning rate: 0.0042638]
	Learning Rate: 0.00426378
	LOSS [training: 0.23682554100741568 | validation: 0.28285899189779523]
	TIME [epoch: 8.17 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22257165452159083		[learning rate: 0.0042587]
		[batch 20/20] avg loss: 0.22738965319060997		[learning rate: 0.0042537]
	Learning Rate: 0.00425372
	LOSS [training: 0.2249806538561004 | validation: 0.27762162954474534]
	TIME [epoch: 8.2 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26954262910048915		[learning rate: 0.0042487]
		[batch 20/20] avg loss: 0.19768234498852105		[learning rate: 0.0042437]
	Learning Rate: 0.00424369
	LOSS [training: 0.23361248704450516 | validation: 0.22230677216140116]
	TIME [epoch: 8.18 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2729492139850903		[learning rate: 0.0042387]
		[batch 20/20] avg loss: 0.3445816764405832		[learning rate: 0.0042337]
	Learning Rate: 0.00423368
	LOSS [training: 0.3087654452128367 | validation: 0.5024460404730003]
	TIME [epoch: 8.17 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27027937139226876		[learning rate: 0.0042287]
		[batch 20/20] avg loss: 0.21343300916910346		[learning rate: 0.0042237]
	Learning Rate: 0.00422369
	LOSS [training: 0.2418561902806861 | validation: 0.17879557267257157]
	TIME [epoch: 8.16 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2653561392359475		[learning rate: 0.0042187]
		[batch 20/20] avg loss: 0.20225775047405622		[learning rate: 0.0042137]
	Learning Rate: 0.00421373
	LOSS [training: 0.23380694485500184 | validation: 0.18550887119131373]
	TIME [epoch: 8.19 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22245262880635036		[learning rate: 0.0042088]
		[batch 20/20] avg loss: 0.20409664481617948		[learning rate: 0.0042038]
	Learning Rate: 0.00420379
	LOSS [training: 0.21327463681126493 | validation: 0.1698169641656771]
	TIME [epoch: 8.18 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2678643880953037		[learning rate: 0.0041988]
		[batch 20/20] avg loss: 0.28003547837894394		[learning rate: 0.0041939]
	Learning Rate: 0.00419387
	LOSS [training: 0.27394993323712385 | validation: 0.1897219025840062]
	TIME [epoch: 8.18 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23058601158183767		[learning rate: 0.0041889]
		[batch 20/20] avg loss: 0.20393466602009638		[learning rate: 0.004184]
	Learning Rate: 0.00418398
	LOSS [training: 0.217260338800967 | validation: 0.30071004428080156]
	TIME [epoch: 8.17 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22728956315615298		[learning rate: 0.004179]
		[batch 20/20] avg loss: 0.21900398717496122		[learning rate: 0.0041741]
	Learning Rate: 0.00417411
	LOSS [training: 0.22314677516555714 | validation: 0.16996059452738954]
	TIME [epoch: 8.18 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22255008513110983		[learning rate: 0.0041692]
		[batch 20/20] avg loss: 0.29438291957764784		[learning rate: 0.0041643]
	Learning Rate: 0.00416427
	LOSS [training: 0.25846650235437874 | validation: 0.2885709732430219]
	TIME [epoch: 8.2 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.127062961606462		[learning rate: 0.0041594]
		[batch 20/20] avg loss: 0.20678440857839758		[learning rate: 0.0041544]
	Learning Rate: 0.00415444
	LOSS [training: 1.1669236850924298 | validation: 0.12381613921901943]
	TIME [epoch: 8.17 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21138537533829727		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.3051424503292939		[learning rate: 0.0041446]
	Learning Rate: 0.00414464
	LOSS [training: 0.2582639128337955 | validation: 0.25276956887745644]
	TIME [epoch: 8.16 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22928362220746284		[learning rate: 0.0041398]
		[batch 20/20] avg loss: 0.1765708079102676		[learning rate: 0.0041349]
	Learning Rate: 0.00413487
	LOSS [training: 0.20292721505886518 | validation: 0.27520077797504766]
	TIME [epoch: 8.17 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2628674729260486		[learning rate: 0.00413]
		[batch 20/20] avg loss: 0.19946319180938815		[learning rate: 0.0041251]
	Learning Rate: 0.00412511
	LOSS [training: 0.23116533236771836 | validation: 0.2980277207659536]
	TIME [epoch: 8.2 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20689207162401213		[learning rate: 0.0041202]
		[batch 20/20] avg loss: 0.22764099125630036		[learning rate: 0.0041154]
	Learning Rate: 0.00411538
	LOSS [training: 0.21726653144015623 | validation: 0.3172490672041883]
	TIME [epoch: 8.17 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2616906240383089		[learning rate: 0.0041105]
		[batch 20/20] avg loss: 0.1975678148931914		[learning rate: 0.0041057]
	Learning Rate: 0.00410568
	LOSS [training: 0.22962921946575016 | validation: 0.2417089915658224]
	TIME [epoch: 8.18 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28022565999353855		[learning rate: 0.0041008]
		[batch 20/20] avg loss: 0.20359513460095918		[learning rate: 0.004096]
	Learning Rate: 0.00409599
	LOSS [training: 0.2419103972972489 | validation: 0.2394729624149735]
	TIME [epoch: 8.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20652704857565163		[learning rate: 0.0040912]
		[batch 20/20] avg loss: 0.261688722175914		[learning rate: 0.0040863]
	Learning Rate: 0.00408633
	LOSS [training: 0.23410788537578284 | validation: 0.18983916669274936]
	TIME [epoch: 8.2 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24169797315064895		[learning rate: 0.0040815]
		[batch 20/20] avg loss: 0.22711101446138698		[learning rate: 0.0040767]
	Learning Rate: 0.00407669
	LOSS [training: 0.23440449380601797 | validation: 0.25866170274209893]
	TIME [epoch: 8.17 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25150566408702385		[learning rate: 0.0040719]
		[batch 20/20] avg loss: 0.23577209891332487		[learning rate: 0.0040671]
	Learning Rate: 0.00406707
	LOSS [training: 0.24363888150017435 | validation: 0.16168406645234804]
	TIME [epoch: 8.17 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32427168435216625		[learning rate: 0.0040623]
		[batch 20/20] avg loss: 0.22777164599328428		[learning rate: 0.0040575]
	Learning Rate: 0.00405748
	LOSS [training: 0.2760216651727253 | validation: 0.25197864674876236]
	TIME [epoch: 8.17 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2001250951335128		[learning rate: 0.0040527]
		[batch 20/20] avg loss: 0.23143317057205143		[learning rate: 0.0040479]
	Learning Rate: 0.00404791
	LOSS [training: 0.2157791328527821 | validation: 0.3082768481736666]
	TIME [epoch: 8.2 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2959402745183909		[learning rate: 0.0040431]
		[batch 20/20] avg loss: 0.2277006896630779		[learning rate: 0.0040384]
	Learning Rate: 0.00403836
	LOSS [training: 0.2618204820907345 | validation: 0.10416974636885525]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2130422441513904		[learning rate: 0.0040336]
		[batch 20/20] avg loss: 0.15994223657599743		[learning rate: 0.0040288]
	Learning Rate: 0.00402883
	LOSS [training: 0.1864922403636939 | validation: 0.1422059540061623]
	TIME [epoch: 8.19 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17558919362704836		[learning rate: 0.0040241]
		[batch 20/20] avg loss: 0.2117955523109018		[learning rate: 0.0040193]
	Learning Rate: 0.00401933
	LOSS [training: 0.19369237296897507 | validation: 0.18736190399513963]
	TIME [epoch: 8.19 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21571071761563315		[learning rate: 0.0040146]
		[batch 20/20] avg loss: 0.18017250512049288		[learning rate: 0.0040099]
	Learning Rate: 0.00400985
	LOSS [training: 0.19794161136806301 | validation: 0.29164224879805944]
	TIME [epoch: 8.22 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20115552590578561		[learning rate: 0.0040051]
		[batch 20/20] avg loss: 0.2209120582404296		[learning rate: 0.0040004]
	Learning Rate: 0.00400039
	LOSS [training: 0.21103379207310763 | validation: 0.3001806546867066]
	TIME [epoch: 8.19 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3256458313137791		[learning rate: 0.0039957]
		[batch 20/20] avg loss: 0.2004124683460423		[learning rate: 0.003991]
	Learning Rate: 0.00399096
	LOSS [training: 0.2630291498299107 | validation: 0.1288189451141434]
	TIME [epoch: 8.19 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20587634004142688		[learning rate: 0.0039862]
		[batch 20/20] avg loss: 0.13688705856418606		[learning rate: 0.0039815]
	Learning Rate: 0.00398154
	LOSS [training: 0.1713816993028065 | validation: 0.1141710037602594]
	TIME [epoch: 8.19 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19392936184435078		[learning rate: 0.0039768]
		[batch 20/20] avg loss: 0.1927595057766639		[learning rate: 0.0039721]
	Learning Rate: 0.00397215
	LOSS [training: 0.1933444338105073 | validation: 0.16757296982556752]
	TIME [epoch: 8.22 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22396755419020065		[learning rate: 0.0039675]
		[batch 20/20] avg loss: 0.254541278757027		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.23925441647361384 | validation: 0.20042729184178054]
	TIME [epoch: 8.19 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15802183594553362		[learning rate: 0.0039581]
		[batch 20/20] avg loss: 0.1797228466461718		[learning rate: 0.0039534]
	Learning Rate: 0.00395343
	LOSS [training: 0.16887234129585268 | validation: 0.27046863866142756]
	TIME [epoch: 8.19 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2252374276201446		[learning rate: 0.0039488]
		[batch 20/20] avg loss: 0.21675043039091463		[learning rate: 0.0039441]
	Learning Rate: 0.00394411
	LOSS [training: 0.2209939290055296 | validation: 0.1860001589725797]
	TIME [epoch: 8.19 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22692044027742347		[learning rate: 0.0039395]
		[batch 20/20] avg loss: 0.17774636297716073		[learning rate: 0.0039348]
	Learning Rate: 0.0039348
	LOSS [training: 0.20233340162729213 | validation: 0.15526791440766224]
	TIME [epoch: 8.22 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18369024027793096		[learning rate: 0.0039302]
		[batch 20/20] avg loss: 0.18105768100726954		[learning rate: 0.0039255]
	Learning Rate: 0.00392552
	LOSS [training: 0.1823739606426003 | validation: 0.18244484735836264]
	TIME [epoch: 8.19 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18587005121466843		[learning rate: 0.0039209]
		[batch 20/20] avg loss: 0.22770280710794105		[learning rate: 0.0039163]
	Learning Rate: 0.00391626
	LOSS [training: 0.2067864291613047 | validation: 0.13972522947189212]
	TIME [epoch: 8.19 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23850164768137866		[learning rate: 0.0039116]
		[batch 20/20] avg loss: 0.16690369570980168		[learning rate: 0.003907]
	Learning Rate: 0.00390702
	LOSS [training: 0.2027026716955902 | validation: 0.20035068272945178]
	TIME [epoch: 8.19 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23043126352817103		[learning rate: 0.0039024]
		[batch 20/20] avg loss: 0.20881620375026047		[learning rate: 0.0038978]
	Learning Rate: 0.00389781
	LOSS [training: 0.21962373363921572 | validation: 0.2495968087956312]
	TIME [epoch: 8.22 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2104060055323022		[learning rate: 0.0038932]
		[batch 20/20] avg loss: 0.2328506979491347		[learning rate: 0.0038886]
	Learning Rate: 0.00388861
	LOSS [training: 0.22162835174071843 | validation: 0.17694085777643273]
	TIME [epoch: 8.19 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16609876076945979		[learning rate: 0.003884]
		[batch 20/20] avg loss: 0.2232421983294938		[learning rate: 0.0038794]
	Learning Rate: 0.00387944
	LOSS [training: 0.19467047954947683 | validation: 0.15426909062247668]
	TIME [epoch: 8.19 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25330785919061494		[learning rate: 0.0038749]
		[batch 20/20] avg loss: 0.19573124828690242		[learning rate: 0.0038703]
	Learning Rate: 0.00387029
	LOSS [training: 0.22451955373875868 | validation: 0.18065770864912858]
	TIME [epoch: 8.19 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20304746683097213		[learning rate: 0.0038657]
		[batch 20/20] avg loss: 0.18997542760995131		[learning rate: 0.0038612]
	Learning Rate: 0.00386116
	LOSS [training: 0.1965114472204617 | validation: 0.2729917961471511]
	TIME [epoch: 8.21 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22026808202092374		[learning rate: 0.0038566]
		[batch 20/20] avg loss: 0.22036923062316097		[learning rate: 0.0038521]
	Learning Rate: 0.00385205
	LOSS [training: 0.22031865632204234 | validation: 0.24497482464096215]
	TIME [epoch: 8.19 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22093157761903176		[learning rate: 0.0038475]
		[batch 20/20] avg loss: 0.22565843159844529		[learning rate: 0.003843]
	Learning Rate: 0.00384297
	LOSS [training: 0.22329500460873858 | validation: 0.2987166540883997]
	TIME [epoch: 8.18 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22814341458400117		[learning rate: 0.0038384]
		[batch 20/20] avg loss: 0.23010183622170594		[learning rate: 0.0038339]
	Learning Rate: 0.0038339
	LOSS [training: 0.2291226254028535 | validation: 0.2595707676777499]
	TIME [epoch: 8.18 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19429825754658334		[learning rate: 0.0038294]
		[batch 20/20] avg loss: 0.2276606619382771		[learning rate: 0.0038249]
	Learning Rate: 0.00382486
	LOSS [training: 0.2109794597424302 | validation: 0.2096197942932978]
	TIME [epoch: 8.21 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18105683291981045		[learning rate: 0.0038203]
		[batch 20/20] avg loss: 0.20750468120692878		[learning rate: 0.0038158]
	Learning Rate: 0.00381584
	LOSS [training: 0.1942807570633696 | validation: 0.18522365754832038]
	TIME [epoch: 8.19 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18240119058916202		[learning rate: 0.0038113]
		[batch 20/20] avg loss: 0.17208066606188507		[learning rate: 0.0038068]
	Learning Rate: 0.00380684
	LOSS [training: 0.17724092832552357 | validation: 0.37959734804557455]
	TIME [epoch: 8.18 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22140369281938516		[learning rate: 0.0038023]
		[batch 20/20] avg loss: 0.19212594598471736		[learning rate: 0.0037979]
	Learning Rate: 0.00379786
	LOSS [training: 0.20676481940205127 | validation: 0.2914546428664606]
	TIME [epoch: 8.19 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21730590639384512		[learning rate: 0.0037934]
		[batch 20/20] avg loss: 0.17003355526880498		[learning rate: 0.0037889]
	Learning Rate: 0.0037889
	LOSS [training: 0.19366973083132505 | validation: 0.14860422577454102]
	TIME [epoch: 8.21 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20677901641101223		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.19611783036754443		[learning rate: 0.00378]
	Learning Rate: 0.00377996
	LOSS [training: 0.20144842338927832 | validation: 0.11059502745082381]
	TIME [epoch: 8.19 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2158761743367455		[learning rate: 0.0037755]
		[batch 20/20] avg loss: 0.20422956552400776		[learning rate: 0.003771]
	Learning Rate: 0.00377104
	LOSS [training: 0.2100528699303766 | validation: 0.1701499716568361]
	TIME [epoch: 8.18 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2074242638204764		[learning rate: 0.0037666]
		[batch 20/20] avg loss: 0.2140028477427743		[learning rate: 0.0037621]
	Learning Rate: 0.00376215
	LOSS [training: 0.2107135557816254 | validation: 0.09764755594035505]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20075732089851095		[learning rate: 0.0037577]
		[batch 20/20] avg loss: 0.2218832187091429		[learning rate: 0.0037533]
	Learning Rate: 0.00375327
	LOSS [training: 0.21132026980382695 | validation: 0.32839365720517255]
	TIME [epoch: 8.21 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21040486165694555		[learning rate: 0.0037488]
		[batch 20/20] avg loss: 0.15608895668493034		[learning rate: 0.0037444]
	Learning Rate: 0.00374442
	LOSS [training: 0.18324690917093794 | validation: 0.19352320779603516]
	TIME [epoch: 8.19 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21897763716112753		[learning rate: 0.00374]
		[batch 20/20] avg loss: 0.14868326324606157		[learning rate: 0.0037356]
	Learning Rate: 0.00373559
	LOSS [training: 0.1838304502035945 | validation: 0.10877287941602995]
	TIME [epoch: 8.18 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2381144256163535		[learning rate: 0.0037312]
		[batch 20/20] avg loss: 0.24381790351346927		[learning rate: 0.0037268]
	Learning Rate: 0.00372678
	LOSS [training: 0.2409661645649114 | validation: 0.27913321045925005]
	TIME [epoch: 8.19 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18779756139437423		[learning rate: 0.0037224]
		[batch 20/20] avg loss: 0.1785050504471778		[learning rate: 0.003718]
	Learning Rate: 0.00371799
	LOSS [training: 0.18315130592077603 | validation: 0.1826798649264323]
	TIME [epoch: 8.21 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20647671820427177		[learning rate: 0.0037136]
		[batch 20/20] avg loss: 0.1779890955004227		[learning rate: 0.0037092]
	Learning Rate: 0.00370922
	LOSS [training: 0.19223290685234723 | validation: 0.17403769143458792]
	TIME [epoch: 8.19 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1642985720504802		[learning rate: 0.0037048]
		[batch 20/20] avg loss: 0.16655650384590098		[learning rate: 0.0037005]
	Learning Rate: 0.00370047
	LOSS [training: 0.1654275379481906 | validation: 0.1828710053679522]
	TIME [epoch: 8.18 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24002348529390458		[learning rate: 0.0036961]
		[batch 20/20] avg loss: 0.22068985046334685		[learning rate: 0.0036917]
	Learning Rate: 0.00369174
	LOSS [training: 0.23035666787862574 | validation: 0.18664114764289969]
	TIME [epoch: 8.19 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.179089749520915		[learning rate: 0.0036874]
		[batch 20/20] avg loss: 0.18444593814240975		[learning rate: 0.003683]
	Learning Rate: 0.00368303
	LOSS [training: 0.18176784383166236 | validation: 0.24895640863421353]
	TIME [epoch: 8.21 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25403008877169564		[learning rate: 0.0036787]
		[batch 20/20] avg loss: 0.1838319507861346		[learning rate: 0.0036743]
	Learning Rate: 0.00367434
	LOSS [training: 0.2189310197789151 | validation: 0.11170775902403102]
	TIME [epoch: 8.19 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19325582968442045		[learning rate: 0.00367]
		[batch 20/20] avg loss: 0.23824987476897422		[learning rate: 0.0036657]
	Learning Rate: 0.00366567
	LOSS [training: 0.21575285222669732 | validation: 0.1215546902200339]
	TIME [epoch: 8.18 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17766857857742285		[learning rate: 0.0036613]
		[batch 20/20] avg loss: 0.15023238587177773		[learning rate: 0.003657]
	Learning Rate: 0.00365703
	LOSS [training: 0.1639504822246003 | validation: 0.6489095776370266]
	TIME [epoch: 8.19 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31043629808819906		[learning rate: 0.0036527]
		[batch 20/20] avg loss: 0.15305462390130672		[learning rate: 0.0036484]
	Learning Rate: 0.0036484
	LOSS [training: 0.23174546099475285 | validation: 0.11595730938953099]
	TIME [epoch: 8.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21749780686335574		[learning rate: 0.0036441]
		[batch 20/20] avg loss: 0.2136396749527866		[learning rate: 0.0036398]
	Learning Rate: 0.00363979
	LOSS [training: 0.21556874090807118 | validation: 0.19924373344924376]
	TIME [epoch: 8.19 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.247036251069206		[learning rate: 0.0036355]
		[batch 20/20] avg loss: 0.20588030292100384		[learning rate: 0.0036312]
	Learning Rate: 0.00363121
	LOSS [training: 0.22645827699510485 | validation: 0.14477212290852737]
	TIME [epoch: 8.18 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2170959570548941		[learning rate: 0.0036269]
		[batch 20/20] avg loss: 0.1673327920942301		[learning rate: 0.0036226]
	Learning Rate: 0.00362264
	LOSS [training: 0.1922143745745621 | validation: 0.19113479076279166]
	TIME [epoch: 8.19 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15977593130990037		[learning rate: 0.0036184]
		[batch 20/20] avg loss: 0.19344090295685942		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.17660841713337988 | validation: 0.19069380726681967]
	TIME [epoch: 8.21 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22351341139551512		[learning rate: 0.0036098]
		[batch 20/20] avg loss: 0.21393186781298815		[learning rate: 0.0036056]
	Learning Rate: 0.00360557
	LOSS [training: 0.21872263960425156 | validation: 0.14449003164617044]
	TIME [epoch: 8.18 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18325400397985456		[learning rate: 0.0036013]
		[batch 20/20] avg loss: 0.18974688234506668		[learning rate: 0.0035971]
	Learning Rate: 0.00359707
	LOSS [training: 0.18650044316246062 | validation: 0.1440793990718912]
	TIME [epoch: 8.19 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21593040903955085		[learning rate: 0.0035928]
		[batch 20/20] avg loss: 0.18784450942388367		[learning rate: 0.0035886]
	Learning Rate: 0.00358858
	LOSS [training: 0.20188745923171725 | validation: 0.151919591887684]
	TIME [epoch: 8.18 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20927237394094228		[learning rate: 0.0035843]
		[batch 20/20] avg loss: 0.1726219059263415		[learning rate: 0.0035801]
	Learning Rate: 0.00358012
	LOSS [training: 0.19094713993364193 | validation: 0.269911448889798]
	TIME [epoch: 8.2 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1936974393753587		[learning rate: 0.0035759]
		[batch 20/20] avg loss: 0.206565691552918		[learning rate: 0.0035717]
	Learning Rate: 0.00357167
	LOSS [training: 0.20013156546413832 | validation: 0.16489694089348528]
	TIME [epoch: 8.19 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20978236942938797		[learning rate: 0.0035675]
		[batch 20/20] avg loss: 0.23118960956454782		[learning rate: 0.0035632]
	Learning Rate: 0.00356325
	LOSS [training: 0.22048598949696788 | validation: 0.1510395315024239]
	TIME [epoch: 8.18 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19768221561682003		[learning rate: 0.003559]
		[batch 20/20] avg loss: 0.20728000109434755		[learning rate: 0.0035548]
	Learning Rate: 0.00355484
	LOSS [training: 0.2024811083555838 | validation: 0.11423840981849065]
	TIME [epoch: 8.19 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1596865458038596		[learning rate: 0.0035506]
		[batch 20/20] avg loss: 0.23203699456859597		[learning rate: 0.0035465]
	Learning Rate: 0.00354646
	LOSS [training: 0.1958617701862278 | validation: 0.22442171834649227]
	TIME [epoch: 8.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2060913130660175		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.19754488911122353		[learning rate: 0.0035381]
	Learning Rate: 0.00353809
	LOSS [training: 0.2018181010886205 | validation: 0.12947956316711137]
	TIME [epoch: 8.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1811807632358886		[learning rate: 0.0035339]
		[batch 20/20] avg loss: 0.19480342717564886		[learning rate: 0.0035297]
	Learning Rate: 0.00352975
	LOSS [training: 0.18799209520576873 | validation: 0.23566566575709674]
	TIME [epoch: 8.18 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18552422884739733		[learning rate: 0.0035256]
		[batch 20/20] avg loss: 0.19272617308845313		[learning rate: 0.0035214]
	Learning Rate: 0.00352142
	LOSS [training: 0.18912520096792526 | validation: 0.13957869738702158]
	TIME [epoch: 8.18 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17920328917022293		[learning rate: 0.0035173]
		[batch 20/20] avg loss: 0.219874134464507		[learning rate: 0.0035131]
	Learning Rate: 0.00351311
	LOSS [training: 0.19953871181736496 | validation: 0.13846312783882775]
	TIME [epoch: 8.19 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16951595721605556		[learning rate: 0.003509]
		[batch 20/20] avg loss: 0.1756880930071849		[learning rate: 0.0035048]
	Learning Rate: 0.00350483
	LOSS [training: 0.1726020251116202 | validation: 0.13878136980010447]
	TIME [epoch: 8.19 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16007825070661766		[learning rate: 0.0035007]
		[batch 20/20] avg loss: 0.1981900604055564		[learning rate: 0.0034966]
	Learning Rate: 0.00349656
	LOSS [training: 0.17913415555608705 | validation: 0.11971781095864878]
	TIME [epoch: 8.18 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1908225649571051		[learning rate: 0.0034924]
		[batch 20/20] avg loss: 0.16019783229787043		[learning rate: 0.0034883]
	Learning Rate: 0.00348831
	LOSS [training: 0.17551019862748776 | validation: 0.2791382503314781]
	TIME [epoch: 8.18 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20245348584571166		[learning rate: 0.0034842]
		[batch 20/20] avg loss: 0.27081843683413637		[learning rate: 0.0034801]
	Learning Rate: 0.00348008
	LOSS [training: 0.23663596133992404 | validation: 0.23606550165424847]
	TIME [epoch: 8.2 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18555041150961885		[learning rate: 0.003476]
		[batch 20/20] avg loss: 0.17023351484196825		[learning rate: 0.0034719]
	Learning Rate: 0.00347187
	LOSS [training: 0.17789196317579353 | validation: 0.12615128324673341]
	TIME [epoch: 8.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20057589178830532		[learning rate: 0.0034678]
		[batch 20/20] avg loss: 0.278062432518535		[learning rate: 0.0034637]
	Learning Rate: 0.00346369
	LOSS [training: 0.23931916215342014 | validation: 0.2069232607780036]
	TIME [epoch: 8.18 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15966505095191946		[learning rate: 0.0034596]
		[batch 20/20] avg loss: 0.18038179789010006		[learning rate: 0.0034555]
	Learning Rate: 0.00345552
	LOSS [training: 0.17002342442100976 | validation: 0.1736694292049869]
	TIME [epoch: 8.19 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16797334003052478		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.153901553167973		[learning rate: 0.0034474]
	Learning Rate: 0.00344736
	LOSS [training: 0.16093744659924886 | validation: 0.14027469053493247]
	TIME [epoch: 8.2 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2507614578627074		[learning rate: 0.0034433]
		[batch 20/20] avg loss: 0.2129582627142287		[learning rate: 0.0034392]
	Learning Rate: 0.00343923
	LOSS [training: 0.23185986028846806 | validation: 0.2608662871215876]
	TIME [epoch: 8.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23481441386956262		[learning rate: 0.0034352]
		[batch 20/20] avg loss: 0.14768804040235506		[learning rate: 0.0034311]
	Learning Rate: 0.00343112
	LOSS [training: 0.19125122713595882 | validation: 0.11706544311845492]
	TIME [epoch: 8.18 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1767380320701198		[learning rate: 0.0034271]
		[batch 20/20] avg loss: 0.2237381736447662		[learning rate: 0.003423]
	Learning Rate: 0.00342303
	LOSS [training: 0.20023810285744298 | validation: 0.2266307513484281]
	TIME [epoch: 8.19 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20178365826650574		[learning rate: 0.003419]
		[batch 20/20] avg loss: 0.22377189492242647		[learning rate: 0.003415]
	Learning Rate: 0.00341495
	LOSS [training: 0.21277777659446612 | validation: 0.1380466640975659]
	TIME [epoch: 8.19 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1840345783041189		[learning rate: 0.0034109]
		[batch 20/20] avg loss: 0.17602181159153307		[learning rate: 0.0034069]
	Learning Rate: 0.0034069
	LOSS [training: 0.180028194947826 | validation: 0.17473276869298712]
	TIME [epoch: 8.21 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16005140124378942		[learning rate: 0.0034029]
		[batch 20/20] avg loss: 0.19291848979548348		[learning rate: 0.0033989]
	Learning Rate: 0.00339886
	LOSS [training: 0.17648494551963642 | validation: 0.22183971284985704]
	TIME [epoch: 8.18 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.238806155808446		[learning rate: 0.0033948]
		[batch 20/20] avg loss: 0.18518679375472766		[learning rate: 0.0033908]
	Learning Rate: 0.00339084
	LOSS [training: 0.21199647478158684 | validation: 0.19402127349386922]
	TIME [epoch: 8.19 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19313997387826726		[learning rate: 0.0033868]
		[batch 20/20] avg loss: 0.1866441624209797		[learning rate: 0.0033828]
	Learning Rate: 0.00338284
	LOSS [training: 0.18989206814962353 | validation: 0.3215936276970141]
	TIME [epoch: 8.19 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.167753932958618		[learning rate: 0.0033789]
		[batch 20/20] avg loss: 0.18398966588310187		[learning rate: 0.0033749]
	Learning Rate: 0.00337487
	LOSS [training: 0.17587179942085993 | validation: 0.4157344291819054]
	TIME [epoch: 8.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2651620259278961		[learning rate: 0.0033709]
		[batch 20/20] avg loss: 0.20389327201055432		[learning rate: 0.0033669]
	Learning Rate: 0.0033669
	LOSS [training: 0.23452764896922512 | validation: 0.13995405601529334]
	TIME [epoch: 8.18 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17754340063514365		[learning rate: 0.0033629]
		[batch 20/20] avg loss: 0.2047659631698569		[learning rate: 0.003359]
	Learning Rate: 0.00335896
	LOSS [training: 0.1911546819025003 | validation: 0.14747284382546477]
	TIME [epoch: 8.18 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.236188940508961		[learning rate: 0.003355]
		[batch 20/20] avg loss: 0.19269131620188623		[learning rate: 0.003351]
	Learning Rate: 0.00335104
	LOSS [training: 0.2144401283554236 | validation: 0.20694153499315393]
	TIME [epoch: 8.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14771007619529505		[learning rate: 0.0033471]
		[batch 20/20] avg loss: 0.1439627747351815		[learning rate: 0.0033431]
	Learning Rate: 0.00334313
	LOSS [training: 0.14583642546523828 | validation: 0.10968174864498323]
	TIME [epoch: 8.23 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1728760908117334		[learning rate: 0.0033392]
		[batch 20/20] avg loss: 0.13824811409349466		[learning rate: 0.0033352]
	Learning Rate: 0.00333525
	LOSS [training: 0.155562102452614 | validation: 0.09819587006027505]
	TIME [epoch: 8.19 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1739240602263154		[learning rate: 0.0033313]
		[batch 20/20] avg loss: 0.1997819709619735		[learning rate: 0.0033274]
	Learning Rate: 0.00332738
	LOSS [training: 0.1868530155941444 | validation: 0.1328807031972874]
	TIME [epoch: 8.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17485550502132363		[learning rate: 0.0033235]
		[batch 20/20] avg loss: 0.17306121919282152		[learning rate: 0.0033195]
	Learning Rate: 0.00331953
	LOSS [training: 0.17395836210707258 | validation: 0.19943038354790021]
	TIME [epoch: 8.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16953958850760648		[learning rate: 0.0033156]
		[batch 20/20] avg loss: 0.16452865030477076		[learning rate: 0.0033117]
	Learning Rate: 0.0033117
	LOSS [training: 0.1670341194061886 | validation: 0.204922847424074]
	TIME [epoch: 8.22 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2629681328073278		[learning rate: 0.0033078]
		[batch 20/20] avg loss: 0.17797245273086643		[learning rate: 0.0033039]
	Learning Rate: 0.00330389
	LOSS [training: 0.22047029276909708 | validation: 0.13255503823635562]
	TIME [epoch: 8.19 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14060830289056558		[learning rate: 0.0033]
		[batch 20/20] avg loss: 0.21035542534842638		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.17548186411949596 | validation: 0.20997903404693286]
	TIME [epoch: 8.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2979860690880099		[learning rate: 0.0032922]
		[batch 20/20] avg loss: 0.19035770730811402		[learning rate: 0.0032883]
	Learning Rate: 0.00328832
	LOSS [training: 0.24417188819806196 | validation: 0.2420347172083545]
	TIME [epoch: 8.19 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1899298813332311		[learning rate: 0.0032844]
		[batch 20/20] avg loss: 0.19924898969300314		[learning rate: 0.0032806]
	Learning Rate: 0.00328057
	LOSS [training: 0.19458943551311708 | validation: 0.1191270175457444]
	TIME [epoch: 8.23 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16053283657613582		[learning rate: 0.0032767]
		[batch 20/20] avg loss: 0.16182695632036517		[learning rate: 0.0032728]
	Learning Rate: 0.00327283
	LOSS [training: 0.16117989644825048 | validation: 0.20346095980397702]
	TIME [epoch: 8.21 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17787027546380163		[learning rate: 0.003269]
		[batch 20/20] avg loss: 0.14814667899627296		[learning rate: 0.0032651]
	Learning Rate: 0.00326511
	LOSS [training: 0.16300847723003728 | validation: 0.17571552551648129]
	TIME [epoch: 8.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15919440546605232		[learning rate: 0.0032613]
		[batch 20/20] avg loss: 0.14935362917151723		[learning rate: 0.0032574]
	Learning Rate: 0.00325741
	LOSS [training: 0.15427401731878476 | validation: 0.18947234947913622]
	TIME [epoch: 8.19 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2398868716391831		[learning rate: 0.0032536]
		[batch 20/20] avg loss: 0.15765181161735173		[learning rate: 0.0032497]
	Learning Rate: 0.00324972
	LOSS [training: 0.1987693416282674 | validation: 0.11667940142208573]
	TIME [epoch: 8.19 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1667261007242014		[learning rate: 0.0032459]
		[batch 20/20] avg loss: 0.1859372052358438		[learning rate: 0.0032421]
	Learning Rate: 0.00324206
	LOSS [training: 0.17633165298002257 | validation: 0.1702772034007844]
	TIME [epoch: 8.17 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26357327381821416		[learning rate: 0.0032382]
		[batch 20/20] avg loss: 0.22565346055454558		[learning rate: 0.0032344]
	Learning Rate: 0.00323441
	LOSS [training: 0.24461336718637985 | validation: 0.17093601458904523]
	TIME [epoch: 8.17 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19020215073157715		[learning rate: 0.0032306]
		[batch 20/20] avg loss: 0.17927277103431244		[learning rate: 0.0032268]
	Learning Rate: 0.00322678
	LOSS [training: 0.18473746088294477 | validation: 0.1856572570273543]
	TIME [epoch: 8.21 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1962528095232873		[learning rate: 0.003223]
		[batch 20/20] avg loss: 0.2036334310103265		[learning rate: 0.0032192]
	Learning Rate: 0.00321917
	LOSS [training: 0.19994312026680688 | validation: 0.14769618010506785]
	TIME [epoch: 8.24 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20133086918016488		[learning rate: 0.0032154]
		[batch 20/20] avg loss: 0.24601035641155225		[learning rate: 0.0032116]
	Learning Rate: 0.00321157
	LOSS [training: 0.22367061279585848 | validation: 0.27118861855600507]
	TIME [epoch: 8.19 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15620768295490925		[learning rate: 0.0032078]
		[batch 20/20] avg loss: 0.1380658581490708		[learning rate: 0.003204]
	Learning Rate: 0.003204
	LOSS [training: 0.14713677055199004 | validation: 0.2609980939903782]
	TIME [epoch: 8.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18447760898965462		[learning rate: 0.0032002]
		[batch 20/20] avg loss: 0.19196748430264338		[learning rate: 0.0031964]
	Learning Rate: 0.00319644
	LOSS [training: 0.18822254664614899 | validation: 0.17131723739510563]
	TIME [epoch: 8.19 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22496327947104272		[learning rate: 0.0031927]
		[batch 20/20] avg loss: 0.21234374122970906		[learning rate: 0.0031889]
	Learning Rate: 0.0031889
	LOSS [training: 0.21865351035037586 | validation: 0.16749728997252686]
	TIME [epoch: 8.21 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1774345919626768		[learning rate: 0.0031851]
		[batch 20/20] avg loss: 0.17353073432191493		[learning rate: 0.0031814]
	Learning Rate: 0.00318138
	LOSS [training: 0.17548266314229582 | validation: 0.1257205849166773]
	TIME [epoch: 8.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14377147702508355		[learning rate: 0.0031776]
		[batch 20/20] avg loss: 0.17945150913456048		[learning rate: 0.0031739]
	Learning Rate: 0.00317387
	LOSS [training: 0.161611493079822 | validation: 0.2647633562088011]
	TIME [epoch: 8.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2264121312763426		[learning rate: 0.0031701]
		[batch 20/20] avg loss: 0.18567572733957613		[learning rate: 0.0031664]
	Learning Rate: 0.00316639
	LOSS [training: 0.2060439293079594 | validation: 0.17622331709170094]
	TIME [epoch: 8.18 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18422885811448209		[learning rate: 0.0031627]
		[batch 20/20] avg loss: 0.154389850502274		[learning rate: 0.0031589]
	Learning Rate: 0.00315892
	LOSS [training: 0.16930935430837807 | validation: 0.24260637782057626]
	TIME [epoch: 8.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19091873336891035		[learning rate: 0.0031552]
		[batch 20/20] avg loss: 0.16857398246464578		[learning rate: 0.0031515]
	Learning Rate: 0.00315147
	LOSS [training: 0.17974635791677807 | validation: 0.1494960010556662]
	TIME [epoch: 8.18 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16421464808238134		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.1577916942558248		[learning rate: 0.003144]
	Learning Rate: 0.00314403
	LOSS [training: 0.16100317116910304 | validation: 0.13160306326047017]
	TIME [epoch: 8.18 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14816544396778938		[learning rate: 0.0031403]
		[batch 20/20] avg loss: 0.20400416508164576		[learning rate: 0.0031366]
	Learning Rate: 0.00313662
	LOSS [training: 0.1760848045247176 | validation: 0.24460249542706664]
	TIME [epoch: 8.18 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17754618240100956		[learning rate: 0.0031329]
		[batch 20/20] avg loss: 0.18712084755252345		[learning rate: 0.0031292]
	Learning Rate: 0.00312922
	LOSS [training: 0.1823335149767665 | validation: 0.15694916079054302]
	TIME [epoch: 8.2 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1550748560643486		[learning rate: 0.0031255]
		[batch 20/20] avg loss: 0.14430843672960608		[learning rate: 0.0031218]
	Learning Rate: 0.00312184
	LOSS [training: 0.14969164639697732 | validation: 0.1862765170053993]
	TIME [epoch: 8.19 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20294393449332584		[learning rate: 0.0031182]
		[batch 20/20] avg loss: 0.18136181851713595		[learning rate: 0.0031145]
	Learning Rate: 0.00311447
	LOSS [training: 0.1921528765052309 | validation: 0.14973526646931717]
	TIME [epoch: 8.18 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18717349706209777		[learning rate: 0.0031108]
		[batch 20/20] avg loss: 0.13041661898868986		[learning rate: 0.0031071]
	Learning Rate: 0.00310713
	LOSS [training: 0.1587950580253938 | validation: 0.08141382159477625]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1322695233173742		[learning rate: 0.0031035]
		[batch 20/20] avg loss: 0.1837570415237681		[learning rate: 0.0030998]
	Learning Rate: 0.0030998
	LOSS [training: 0.15801328242057114 | validation: 0.12751649671086812]
	TIME [epoch: 8.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18360671175437193		[learning rate: 0.0030961]
		[batch 20/20] avg loss: 0.1867379176438582		[learning rate: 0.0030925]
	Learning Rate: 0.00309249
	LOSS [training: 0.18517231469911508 | validation: 0.20664877878932914]
	TIME [epoch: 8.18 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1972281201091606		[learning rate: 0.0030888]
		[batch 20/20] avg loss: 0.16851306127525237		[learning rate: 0.0030852]
	Learning Rate: 0.00308519
	LOSS [training: 0.18287059069220649 | validation: 0.13941361450514453]
	TIME [epoch: 8.17 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17872377764948422		[learning rate: 0.0030815]
		[batch 20/20] avg loss: 0.21214088404159298		[learning rate: 0.0030779]
	Learning Rate: 0.00307791
	LOSS [training: 0.19543233084553857 | validation: 0.17495995307997955]
	TIME [epoch: 8.18 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1520971072047982		[learning rate: 0.0030743]
		[batch 20/20] avg loss: 0.19074946638019977		[learning rate: 0.0030707]
	Learning Rate: 0.00307065
	LOSS [training: 0.171423286792499 | validation: 0.3046203446814092]
	TIME [epoch: 8.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19058698754617664		[learning rate: 0.003067]
		[batch 20/20] avg loss: 0.15768527747248579		[learning rate: 0.0030634]
	Learning Rate: 0.00306341
	LOSS [training: 0.17413613250933122 | validation: 0.29308923442581514]
	TIME [epoch: 8.19 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.165520256320057		[learning rate: 0.0030598]
		[batch 20/20] avg loss: 0.14193861335991348		[learning rate: 0.0030562]
	Learning Rate: 0.00305618
	LOSS [training: 0.15372943483998525 | validation: 0.14723693551579642]
	TIME [epoch: 8.17 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17845646816738892		[learning rate: 0.0030526]
		[batch 20/20] avg loss: 0.1403880756289278		[learning rate: 0.003049]
	Learning Rate: 0.00304897
	LOSS [training: 0.15942227189815839 | validation: 0.15903299053964204]
	TIME [epoch: 8.18 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1532847792663326		[learning rate: 0.0030454]
		[batch 20/20] avg loss: 0.13641670835110317		[learning rate: 0.0030418]
	Learning Rate: 0.00304178
	LOSS [training: 0.14485074380871787 | validation: 0.10952483129452237]
	TIME [epoch: 8.21 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21130568971249702		[learning rate: 0.0030382]
		[batch 20/20] avg loss: 0.16892681182242558		[learning rate: 0.0030346]
	Learning Rate: 0.00303461
	LOSS [training: 0.1901162507674613 | validation: 0.12963380469451746]
	TIME [epoch: 8.18 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16707997408671618		[learning rate: 0.003031]
		[batch 20/20] avg loss: 0.1767926455081726		[learning rate: 0.0030274]
	Learning Rate: 0.00302745
	LOSS [training: 0.17193630979744443 | validation: 0.20492933727356547]
	TIME [epoch: 8.17 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15543098165323274		[learning rate: 0.0030239]
		[batch 20/20] avg loss: 0.1659408716508502		[learning rate: 0.0030203]
	Learning Rate: 0.00302031
	LOSS [training: 0.16068592665204148 | validation: 0.2240189006010282]
	TIME [epoch: 8.17 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21237781653167226		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.16685940081576817		[learning rate: 0.0030132]
	Learning Rate: 0.00301318
	LOSS [training: 0.1896186086737202 | validation: 0.12774267849330265]
	TIME [epoch: 8.2 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1374492843361733		[learning rate: 0.0030096]
		[batch 20/20] avg loss: 0.15037206801712757		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.1439106761766505 | validation: 0.14526689963268033]
	TIME [epoch: 8.18 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1728677615671293		[learning rate: 0.0030025]
		[batch 20/20] avg loss: 0.18199958990893553		[learning rate: 0.002999]
	Learning Rate: 0.00299899
	LOSS [training: 0.1774336757380324 | validation: 0.14746490824056063]
	TIME [epoch: 8.17 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14950033928033385		[learning rate: 0.0029954]
		[batch 20/20] avg loss: 0.18506605408159113		[learning rate: 0.0029919]
	Learning Rate: 0.00299191
	LOSS [training: 0.1672831966809625 | validation: 0.16555602101869848]
	TIME [epoch: 8.17 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15662454787202915		[learning rate: 0.0029884]
		[batch 20/20] avg loss: 0.18754768165093885		[learning rate: 0.0029849]
	Learning Rate: 0.00298485
	LOSS [training: 0.17208611476148397 | validation: 0.32443190482993367]
	TIME [epoch: 8.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18627183168852576		[learning rate: 0.0029813]
		[batch 20/20] avg loss: 0.16486193074852307		[learning rate: 0.0029778]
	Learning Rate: 0.00297781
	LOSS [training: 0.17556688121852443 | validation: 0.1189151323375892]
	TIME [epoch: 8.18 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16695043259453665		[learning rate: 0.0029743]
		[batch 20/20] avg loss: 0.17752079184343225		[learning rate: 0.0029708]
	Learning Rate: 0.00297079
	LOSS [training: 0.17223561221898448 | validation: 0.30767491323920604]
	TIME [epoch: 8.18 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15795485177193064		[learning rate: 0.0029673]
		[batch 20/20] avg loss: 0.15746922570610541		[learning rate: 0.0029638]
	Learning Rate: 0.00296378
	LOSS [training: 0.157712038739018 | validation: 0.1732618194315566]
	TIME [epoch: 8.18 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1907356211169044		[learning rate: 0.0029603]
		[batch 20/20] avg loss: 0.15904616828996515		[learning rate: 0.0029568]
	Learning Rate: 0.00295679
	LOSS [training: 0.17489089470343475 | validation: 0.2636235081386533]
	TIME [epoch: 8.19 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22050758053080313		[learning rate: 0.0029533]
		[batch 20/20] avg loss: 0.1632539969449625		[learning rate: 0.0029498]
	Learning Rate: 0.00294982
	LOSS [training: 0.19188078873788283 | validation: 0.2229336477368762]
	TIME [epoch: 8.19 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2052087174325628		[learning rate: 0.0029463]
		[batch 20/20] avg loss: 0.1643118776055329		[learning rate: 0.0029429]
	Learning Rate: 0.00294286
	LOSS [training: 0.18476029751904782 | validation: 0.2957984734833412]
	TIME [epoch: 8.19 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20678638348297046		[learning rate: 0.0029394]
		[batch 20/20] avg loss: 0.2190599464209641		[learning rate: 0.0029359]
	Learning Rate: 0.00293592
	LOSS [training: 0.21292316495196725 | validation: 0.11040726061379157]
	TIME [epoch: 8.18 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16990050315999233		[learning rate: 0.0029325]
		[batch 20/20] avg loss: 0.1495805749817251		[learning rate: 0.002929]
	Learning Rate: 0.00292899
	LOSS [training: 0.15974053907085875 | validation: 0.11363223823092175]
	TIME [epoch: 8.19 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17691660109691737		[learning rate: 0.0029255]
		[batch 20/20] avg loss: 0.1659448777411386		[learning rate: 0.0029221]
	Learning Rate: 0.00292208
	LOSS [training: 0.17143073941902798 | validation: 0.11636182808368473]
	TIME [epoch: 8.19 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1736212164293846		[learning rate: 0.0029186]
		[batch 20/20] avg loss: 0.16726883630513328		[learning rate: 0.0029152]
	Learning Rate: 0.00291519
	LOSS [training: 0.17044502636725894 | validation: 0.14457624248078027]
	TIME [epoch: 8.18 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14550652792464985		[learning rate: 0.0029117]
		[batch 20/20] avg loss: 0.1898852462900792		[learning rate: 0.0029083]
	Learning Rate: 0.00290831
	LOSS [training: 0.16769588710736452 | validation: 0.2886689711589141]
	TIME [epoch: 8.18 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17678304954246413		[learning rate: 0.0029049]
		[batch 20/20] avg loss: 0.1609640001203629		[learning rate: 0.0029015]
	Learning Rate: 0.00290145
	LOSS [training: 0.1688735248314135 | validation: 0.4310096145471253]
	TIME [epoch: 8.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22527904138427096		[learning rate: 0.002898]
		[batch 20/20] avg loss: 0.18623131384537886		[learning rate: 0.0028946]
	Learning Rate: 0.00289461
	LOSS [training: 0.20575517761482492 | validation: 0.14726621610540805]
	TIME [epoch: 8.19 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17595409001117412		[learning rate: 0.0028912]
		[batch 20/20] avg loss: 0.1617925512429046		[learning rate: 0.0028878]
	Learning Rate: 0.00288778
	LOSS [training: 0.16887332062703936 | validation: 0.22749126205441902]
	TIME [epoch: 8.19 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16434310557702383		[learning rate: 0.0028844]
		[batch 20/20] avg loss: 0.13740706026238872		[learning rate: 0.002881]
	Learning Rate: 0.00288097
	LOSS [training: 0.15087508291970625 | validation: 0.11340647818617322]
	TIME [epoch: 8.18 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20755946460681227		[learning rate: 0.0028776]
		[batch 20/20] avg loss: 0.19278911175554445		[learning rate: 0.0028742]
	Learning Rate: 0.00287417
	LOSS [training: 0.20017428818117838 | validation: 0.15434694029809964]
	TIME [epoch: 8.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13049561478692873		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.20908951514114554		[learning rate: 0.0028674]
	Learning Rate: 0.00286739
	LOSS [training: 0.16979256496403716 | validation: 0.1265009930517391]
	TIME [epoch: 8.19 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17503988457604747		[learning rate: 0.002864]
		[batch 20/20] avg loss: 0.16664346577339573		[learning rate: 0.0028606]
	Learning Rate: 0.00286063
	LOSS [training: 0.17084167517472157 | validation: 0.20472686447234048]
	TIME [epoch: 8.17 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.171467153945616		[learning rate: 0.0028573]
		[batch 20/20] avg loss: 0.1683469453418655		[learning rate: 0.0028539]
	Learning Rate: 0.00285388
	LOSS [training: 0.16990704964374076 | validation: 0.17723578746325475]
	TIME [epoch: 8.17 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1539583776570908		[learning rate: 0.0028505]
		[batch 20/20] avg loss: 0.18120503150824147		[learning rate: 0.0028471]
	Learning Rate: 0.00284715
	LOSS [training: 0.16758170458266614 | validation: 0.18262696826606528]
	TIME [epoch: 8.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16593188064894118		[learning rate: 0.0028438]
		[batch 20/20] avg loss: 0.1710973437436764		[learning rate: 0.0028404]
	Learning Rate: 0.00284043
	LOSS [training: 0.1685146121963088 | validation: 0.12093364457391183]
	TIME [epoch: 8.19 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1492814607496044		[learning rate: 0.0028371]
		[batch 20/20] avg loss: 0.1379355677144614		[learning rate: 0.0028337]
	Learning Rate: 0.00283373
	LOSS [training: 0.1436085142320329 | validation: 0.10405639014114744]
	TIME [epoch: 8.18 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15366717857178708		[learning rate: 0.0028304]
		[batch 20/20] avg loss: 0.12671643186457965		[learning rate: 0.002827]
	Learning Rate: 0.00282705
	LOSS [training: 0.14019180521818336 | validation: 0.14767182133557094]
	TIME [epoch: 8.17 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19809617625478193		[learning rate: 0.0028237]
		[batch 20/20] avg loss: 0.17522680618376693		[learning rate: 0.0028204]
	Learning Rate: 0.00282038
	LOSS [training: 0.18666149121927442 | validation: 0.111713411731734]
	TIME [epoch: 8.18 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1649280040077637		[learning rate: 0.0028171]
		[batch 20/20] avg loss: 0.19993080128438306		[learning rate: 0.0028137]
	Learning Rate: 0.00281373
	LOSS [training: 0.18242940264607338 | validation: 0.13940418488176037]
	TIME [epoch: 8.19 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13495670292781298		[learning rate: 0.0028104]
		[batch 20/20] avg loss: 0.13978510061411092		[learning rate: 0.0028071]
	Learning Rate: 0.00280709
	LOSS [training: 0.13737090177096198 | validation: 0.09499986070530841]
	TIME [epoch: 8.18 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12452831263498723		[learning rate: 0.0028038]
		[batch 20/20] avg loss: 0.14285708430758765		[learning rate: 0.0028005]
	Learning Rate: 0.00280047
	LOSS [training: 0.1336926984712874 | validation: 0.13860312321174587]
	TIME [epoch: 8.17 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15676825763945357		[learning rate: 0.0027972]
		[batch 20/20] avg loss: 0.17673639203365155		[learning rate: 0.0027939]
	Learning Rate: 0.00279386
	LOSS [training: 0.16675232483655258 | validation: 0.22259197521494545]
	TIME [epoch: 8.18 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20541924102674888		[learning rate: 0.0027906]
		[batch 20/20] avg loss: 0.1457574084289407		[learning rate: 0.0027873]
	Learning Rate: 0.00278727
	LOSS [training: 0.1755883247278448 | validation: 0.11888833463338741]
	TIME [epoch: 8.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15849368150716076		[learning rate: 0.002784]
		[batch 20/20] avg loss: 0.15431684325045375		[learning rate: 0.0027807]
	Learning Rate: 0.0027807
	LOSS [training: 0.15640526237880725 | validation: 0.15816812863930324]
	TIME [epoch: 8.18 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.157812189814927		[learning rate: 0.0027774]
		[batch 20/20] avg loss: 0.18628123111709718		[learning rate: 0.0027741]
	Learning Rate: 0.00277414
	LOSS [training: 0.1720467104660121 | validation: 0.138416260588055]
	TIME [epoch: 8.18 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14695570211831682		[learning rate: 0.0027709]
		[batch 20/20] avg loss: 0.13754517989223553		[learning rate: 0.0027676]
	Learning Rate: 0.00276759
	LOSS [training: 0.1422504410052762 | validation: 0.09174687767878192]
	TIME [epoch: 8.18 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14299817781535917		[learning rate: 0.0027643]
		[batch 20/20] avg loss: 0.19219138281813056		[learning rate: 0.0027611]
	Learning Rate: 0.00276107
	LOSS [training: 0.16759478031674488 | validation: 0.2501417199678141]
	TIME [epoch: 8.21 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1494647475194797		[learning rate: 0.0027578]
		[batch 20/20] avg loss: 0.15341918432019772		[learning rate: 0.0027546]
	Learning Rate: 0.00275455
	LOSS [training: 0.15144196591983872 | validation: 0.1528551310793381]
	TIME [epoch: 8.17 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16388879194281786		[learning rate: 0.0027513]
		[batch 20/20] avg loss: 0.18385267848364464		[learning rate: 0.0027481]
	Learning Rate: 0.00274806
	LOSS [training: 0.17387073521323124 | validation: 0.11977508883685763]
	TIME [epoch: 8.18 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1408173863178456		[learning rate: 0.0027448]
		[batch 20/20] avg loss: 0.15681670339449824		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.14881704485617192 | validation: 0.106495512094463]
	TIME [epoch: 8.18 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13191565481937645		[learning rate: 0.0027383]
		[batch 20/20] avg loss: 0.12640250989291904		[learning rate: 0.0027351]
	Learning Rate: 0.00273511
	LOSS [training: 0.12915908235614776 | validation: 0.23853654224183585]
	TIME [epoch: 8.21 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14340785542267767		[learning rate: 0.0027319]
		[batch 20/20] avg loss: 0.12257078245025303		[learning rate: 0.0027287]
	Learning Rate: 0.00272866
	LOSS [training: 0.13298931893646532 | validation: 0.09343595560598583]
	TIME [epoch: 8.18 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16550659897078351		[learning rate: 0.0027254]
		[batch 20/20] avg loss: 0.16111199882402416		[learning rate: 0.0027222]
	Learning Rate: 0.00272222
	LOSS [training: 0.16330929889740384 | validation: 0.16574193456802463]
	TIME [epoch: 8.18 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14864965043336895		[learning rate: 0.002719]
		[batch 20/20] avg loss: 0.13865435937478793		[learning rate: 0.0027158]
	Learning Rate: 0.0027158
	LOSS [training: 0.14365200490407842 | validation: 0.09908463752606067]
	TIME [epoch: 8.19 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16298477640882192		[learning rate: 0.0027126]
		[batch 20/20] avg loss: 0.12706760324063002		[learning rate: 0.0027094]
	Learning Rate: 0.00270939
	LOSS [training: 0.14502618982472598 | validation: 0.17638706617008615]
	TIME [epoch: 8.2 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2031478727259218		[learning rate: 0.0027062]
		[batch 20/20] avg loss: 0.18290433336881534		[learning rate: 0.002703]
	Learning Rate: 0.002703
	LOSS [training: 0.19302610304736856 | validation: 0.39407806784624233]
	TIME [epoch: 8.18 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28631624375226283		[learning rate: 0.0026998]
		[batch 20/20] avg loss: 0.14236850025223427		[learning rate: 0.0026966]
	Learning Rate: 0.00269662
	LOSS [training: 0.2143423720022486 | validation: 0.09092040652534791]
	TIME [epoch: 8.17 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16058165509803882		[learning rate: 0.0026934]
		[batch 20/20] avg loss: 0.1321406098835482		[learning rate: 0.0026903]
	Learning Rate: 0.00269026
	LOSS [training: 0.1463611324907935 | validation: 0.10072580973835019]
	TIME [epoch: 8.18 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1215663419916208		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.18209558511513882		[learning rate: 0.0026839]
	Learning Rate: 0.00268392
	LOSS [training: 0.15183096355337986 | validation: 0.21247788011214738]
	TIME [epoch: 8.2 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1372408992961381		[learning rate: 0.0026808]
		[batch 20/20] avg loss: 0.19145010942167334		[learning rate: 0.0026776]
	Learning Rate: 0.00267759
	LOSS [training: 0.16434550435890574 | validation: 0.11132947578717674]
	TIME [epoch: 8.18 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15722048914884265		[learning rate: 0.0026744]
		[batch 20/20] avg loss: 0.12641703217910427		[learning rate: 0.0026713]
	Learning Rate: 0.00267127
	LOSS [training: 0.14181876066397348 | validation: 0.19149819967508747]
	TIME [epoch: 8.18 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18895966820396523		[learning rate: 0.0026681]
		[batch 20/20] avg loss: 0.14765600204180465		[learning rate: 0.002665]
	Learning Rate: 0.00266497
	LOSS [training: 0.16830783512288494 | validation: 0.13337620905241734]
	TIME [epoch: 8.18 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1587908424456261		[learning rate: 0.0026618]
		[batch 20/20] avg loss: 0.15006334962287932		[learning rate: 0.0026587]
	Learning Rate: 0.00265868
	LOSS [training: 0.1544270960342527 | validation: 0.08263347219044191]
	TIME [epoch: 8.21 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13178913962582384		[learning rate: 0.0026555]
		[batch 20/20] avg loss: 0.12736718288933718		[learning rate: 0.0026524]
	Learning Rate: 0.00265241
	LOSS [training: 0.12957816125758054 | validation: 0.0956709274646543]
	TIME [epoch: 8.17 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15060806951484046		[learning rate: 0.0026493]
		[batch 20/20] avg loss: 0.1422889930540195		[learning rate: 0.0026462]
	Learning Rate: 0.00264616
	LOSS [training: 0.14644853128442997 | validation: 0.17289763379449374]
	TIME [epoch: 8.18 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1499714135507246		[learning rate: 0.002643]
		[batch 20/20] avg loss: 0.20067379389457995		[learning rate: 0.0026399]
	Learning Rate: 0.00263991
	LOSS [training: 0.1753226037226523 | validation: 0.118587718463008]
	TIME [epoch: 8.17 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19238933989213175		[learning rate: 0.0026368]
		[batch 20/20] avg loss: 0.13308720395383422		[learning rate: 0.0026337]
	Learning Rate: 0.00263369
	LOSS [training: 0.16273827192298299 | validation: 0.26280428320426874]
	TIME [epoch: 8.21 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.171648348829576		[learning rate: 0.0026306]
		[batch 20/20] avg loss: 0.17281977732365686		[learning rate: 0.0026275]
	Learning Rate: 0.00262747
	LOSS [training: 0.17223406307661643 | validation: 0.13676247594567012]
	TIME [epoch: 8.17 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14035017823121615		[learning rate: 0.0026244]
		[batch 20/20] avg loss: 0.20489666267445047		[learning rate: 0.0026213]
	Learning Rate: 0.00262128
	LOSS [training: 0.17262342045283324 | validation: 0.16775800732954618]
	TIME [epoch: 8.18 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19992152348528286		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.14997505779002718		[learning rate: 0.0026151]
	Learning Rate: 0.00261509
	LOSS [training: 0.17494829063765505 | validation: 0.12864679396427964]
	TIME [epoch: 8.18 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12644210895622418		[learning rate: 0.002612]
		[batch 20/20] avg loss: 0.16727662445988772		[learning rate: 0.0026089]
	Learning Rate: 0.00260892
	LOSS [training: 0.14685936670805594 | validation: 0.12240290370461493]
	TIME [epoch: 8.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14482400693855885		[learning rate: 0.0026058]
		[batch 20/20] avg loss: 0.1264152712487615		[learning rate: 0.0026028]
	Learning Rate: 0.00260277
	LOSS [training: 0.13561963909366018 | validation: 0.17770446976214915]
	TIME [epoch: 8.18 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1404513387065708		[learning rate: 0.0025997]
		[batch 20/20] avg loss: 0.16431659379399452		[learning rate: 0.0025966]
	Learning Rate: 0.00259663
	LOSS [training: 0.15238396625028267 | validation: 0.1508376336530333]
	TIME [epoch: 8.18 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15878903886133475		[learning rate: 0.0025936]
		[batch 20/20] avg loss: 0.157382850484395		[learning rate: 0.0025905]
	Learning Rate: 0.00259051
	LOSS [training: 0.15808594467286485 | validation: 0.1764890858957387]
	TIME [epoch: 8.17 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15811371885470063		[learning rate: 0.0025874]
		[batch 20/20] avg loss: 0.14094894518524523		[learning rate: 0.0025844]
	Learning Rate: 0.0025844
	LOSS [training: 0.1495313320199729 | validation: 0.11308921541581854]
	TIME [epoch: 8.21 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1464663489705203		[learning rate: 0.0025813]
		[batch 20/20] avg loss: 0.1324510911338347		[learning rate: 0.0025783]
	Learning Rate: 0.0025783
	LOSS [training: 0.13945872005217747 | validation: 0.13434639231663384]
	TIME [epoch: 8.18 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1294951491960467		[learning rate: 0.0025753]
		[batch 20/20] avg loss: 0.1151150053381628		[learning rate: 0.0025722]
	Learning Rate: 0.00257222
	LOSS [training: 0.12230507726710474 | validation: 0.13738002320383352]
	TIME [epoch: 8.17 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13313576322526574		[learning rate: 0.0025692]
		[batch 20/20] avg loss: 0.1447098128351722		[learning rate: 0.0025662]
	Learning Rate: 0.00256615
	LOSS [training: 0.13892278803021896 | validation: 0.11333608000796737]
	TIME [epoch: 8.18 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16036516360570188		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.13235475362083277		[learning rate: 0.0025601]
	Learning Rate: 0.0025601
	LOSS [training: 0.1463599586132673 | validation: 0.16771434743869573]
	TIME [epoch: 8.21 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18358978344466315		[learning rate: 0.0025571]
		[batch 20/20] avg loss: 0.1825220528124631		[learning rate: 0.0025541]
	Learning Rate: 0.00255406
	LOSS [training: 0.18305591812856314 | validation: 0.10196803595150146]
	TIME [epoch: 8.19 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15614419447460506		[learning rate: 0.002551]
		[batch 20/20] avg loss: 0.13923395501062805		[learning rate: 0.002548]
	Learning Rate: 0.00254803
	LOSS [training: 0.14768907474261653 | validation: 0.1472225505884555]
	TIME [epoch: 8.18 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.159762013174869		[learning rate: 0.002545]
		[batch 20/20] avg loss: 0.13737400654223098		[learning rate: 0.002542]
	Learning Rate: 0.00254202
	LOSS [training: 0.14856800985854998 | validation: 0.280735196009511]
	TIME [epoch: 8.18 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16611583577624994		[learning rate: 0.002539]
		[batch 20/20] avg loss: 0.16264604279019754		[learning rate: 0.002536]
	Learning Rate: 0.00253603
	LOSS [training: 0.1643809392832237 | validation: 0.09740533435290982]
	TIME [epoch: 8.21 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15986399792565903		[learning rate: 0.002533]
		[batch 20/20] avg loss: 0.1835988613317672		[learning rate: 0.00253]
	Learning Rate: 0.00253004
	LOSS [training: 0.17173142962871307 | validation: 0.15575262158963046]
	TIME [epoch: 8.18 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2168316332287698		[learning rate: 0.0025271]
		[batch 20/20] avg loss: 0.11928273269244646		[learning rate: 0.0025241]
	Learning Rate: 0.00252408
	LOSS [training: 0.16805718296060815 | validation: 0.20786169404394583]
	TIME [epoch: 8.18 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1539136092083357		[learning rate: 0.0025211]
		[batch 20/20] avg loss: 0.13753048126416795		[learning rate: 0.0025181]
	Learning Rate: 0.00251812
	LOSS [training: 0.14572204523625182 | validation: 0.16257135907890616]
	TIME [epoch: 8.18 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13956291857503286		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.13690999846646773		[learning rate: 0.0025122]
	Learning Rate: 0.00251218
	LOSS [training: 0.1382364585207503 | validation: 0.12638889090004674]
	TIME [epoch: 8.21 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1567487038835643		[learning rate: 0.0025092]
		[batch 20/20] avg loss: 0.16216136041933332		[learning rate: 0.0025063]
	Learning Rate: 0.00250626
	LOSS [training: 0.1594550321514488 | validation: 0.1395822110464668]
	TIME [epoch: 8.18 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1247456845796117		[learning rate: 0.0025033]
		[batch 20/20] avg loss: 0.13458974845209698		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.12966771651585438 | validation: 0.12045434815157698]
	TIME [epoch: 8.18 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1748149307472726		[learning rate: 0.0024974]
		[batch 20/20] avg loss: 0.11692623289882183		[learning rate: 0.0024944]
	Learning Rate: 0.00249445
	LOSS [training: 0.14587058182304719 | validation: 0.1277929228082621]
	TIME [epoch: 8.18 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2056895645947686		[learning rate: 0.0024915]
		[batch 20/20] avg loss: 0.22220502105395262		[learning rate: 0.0024886]
	Learning Rate: 0.00248856
	LOSS [training: 0.2139472928243606 | validation: 0.09581348872059225]
	TIME [epoch: 8.19 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14758846090953712		[learning rate: 0.0024856]
		[batch 20/20] avg loss: 0.1422980906759898		[learning rate: 0.0024827]
	Learning Rate: 0.00248269
	LOSS [training: 0.1449432757927635 | validation: 0.1589252047847715]
	TIME [epoch: 8.19 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1393602039497373		[learning rate: 0.0024798]
		[batch 20/20] avg loss: 0.11624409839470107		[learning rate: 0.0024768]
	Learning Rate: 0.00247684
	LOSS [training: 0.1278021511722192 | validation: 0.1414500918753166]
	TIME [epoch: 8.17 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16196257485461069		[learning rate: 0.0024739]
		[batch 20/20] avg loss: 0.155746515569042		[learning rate: 0.002471]
	Learning Rate: 0.00247099
	LOSS [training: 0.15885454521182635 | validation: 0.18393833472676818]
	TIME [epoch: 8.18 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18312552757705802		[learning rate: 0.0024681]
		[batch 20/20] avg loss: 0.1339798718180787		[learning rate: 0.0024652]
	Learning Rate: 0.00246517
	LOSS [training: 0.15855269969756836 | validation: 0.10998756579213242]
	TIME [epoch: 8.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14497446814294634		[learning rate: 0.0024623]
		[batch 20/20] avg loss: 0.12073602054623185		[learning rate: 0.0024594]
	Learning Rate: 0.00245935
	LOSS [training: 0.13285524434458912 | validation: 0.11145218821371741]
	TIME [epoch: 8.19 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12886654509282253		[learning rate: 0.0024564]
		[batch 20/20] avg loss: 0.15079989305035563		[learning rate: 0.0024535]
	Learning Rate: 0.00245355
	LOSS [training: 0.13983321907158908 | validation: 0.09973076216854529]
	TIME [epoch: 8.18 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12418018704255455		[learning rate: 0.0024507]
		[batch 20/20] avg loss: 0.1444101960898679		[learning rate: 0.0024478]
	Learning Rate: 0.00244776
	LOSS [training: 0.13429519156621122 | validation: 0.1685768386999688]
	TIME [epoch: 8.18 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13130116512585324		[learning rate: 0.0024449]
		[batch 20/20] avg loss: 0.13521838431277444		[learning rate: 0.002442]
	Learning Rate: 0.00244199
	LOSS [training: 0.13325977471931386 | validation: 0.19223344953572247]
	TIME [epoch: 8.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13586225239459493		[learning rate: 0.0024391]
		[batch 20/20] avg loss: 0.1476401929754409		[learning rate: 0.0024362]
	Learning Rate: 0.00243623
	LOSS [training: 0.1417512226850179 | validation: 0.1399660150477623]
	TIME [epoch: 8.19 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1423532557794974		[learning rate: 0.0024334]
		[batch 20/20] avg loss: 0.1501539256227558		[learning rate: 0.0024305]
	Learning Rate: 0.00243048
	LOSS [training: 0.1462535907011266 | validation: 0.16063675496356414]
	TIME [epoch: 8.19 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1387252529433989		[learning rate: 0.0024276]
		[batch 20/20] avg loss: 0.1444303030556276		[learning rate: 0.0024247]
	Learning Rate: 0.00242475
	LOSS [training: 0.14157777799951327 | validation: 0.09038319976085826]
	TIME [epoch: 8.18 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1059936134745284		[learning rate: 0.0024219]
		[batch 20/20] avg loss: 0.12173161495633877		[learning rate: 0.002419]
	Learning Rate: 0.00241903
	LOSS [training: 0.11386261421543357 | validation: 0.085129200039024]
	TIME [epoch: 8.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1312306546946847		[learning rate: 0.0024162]
		[batch 20/20] avg loss: 0.17347194725183365		[learning rate: 0.0024133]
	Learning Rate: 0.00241332
	LOSS [training: 0.1523513009732592 | validation: 0.12385010563305801]
	TIME [epoch: 8.18 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14423336245373092		[learning rate: 0.0024105]
		[batch 20/20] avg loss: 0.14994735007179086		[learning rate: 0.0024076]
	Learning Rate: 0.00240763
	LOSS [training: 0.14709035626276087 | validation: 0.12538332283927023]
	TIME [epoch: 8.18 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12862592691011318		[learning rate: 0.0024048]
		[batch 20/20] avg loss: 0.14300415021542523		[learning rate: 0.002402]
	Learning Rate: 0.00240195
	LOSS [training: 0.1358150385627692 | validation: 0.13616190555553118]
	TIME [epoch: 8.18 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14528572186870087		[learning rate: 0.0023991]
		[batch 20/20] avg loss: 0.12416079472390396		[learning rate: 0.0023963]
	Learning Rate: 0.00239628
	LOSS [training: 0.13472325829630244 | validation: 0.12037443755081945]
	TIME [epoch: 8.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11969480914516466		[learning rate: 0.0023935]
		[batch 20/20] avg loss: 0.14610239860668478		[learning rate: 0.0023906]
	Learning Rate: 0.00239063
	LOSS [training: 0.13289860387592473 | validation: 0.24231545304209223]
	TIME [epoch: 8.19 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16792326511991157		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.12803228175803374		[learning rate: 0.002385]
	Learning Rate: 0.00238499
	LOSS [training: 0.14797777343897262 | validation: 0.15880385477706874]
	TIME [epoch: 8.18 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1372522374258051		[learning rate: 0.0023822]
		[batch 20/20] avg loss: 0.16268065193928907		[learning rate: 0.0023794]
	Learning Rate: 0.00237937
	LOSS [training: 0.1499664446825471 | validation: 0.10774079476548748]
	TIME [epoch: 8.18 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10173425005185437		[learning rate: 0.0023766]
		[batch 20/20] avg loss: 0.15225900917916727		[learning rate: 0.0023738]
	Learning Rate: 0.00237375
	LOSS [training: 0.1269966296155108 | validation: 0.10143289167255481]
	TIME [epoch: 8.19 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12532585333089055		[learning rate: 0.002371]
		[batch 20/20] avg loss: 0.1459346145670942		[learning rate: 0.0023682]
	Learning Rate: 0.00236816
	LOSS [training: 0.13563023394899237 | validation: 0.2598673568334778]
	TIME [epoch: 8.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15922078369485854		[learning rate: 0.0023654]
		[batch 20/20] avg loss: 0.12525585443967252		[learning rate: 0.0023626]
	Learning Rate: 0.00236257
	LOSS [training: 0.1422383190672655 | validation: 0.1260056163007986]
	TIME [epoch: 8.17 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11965212630267048		[learning rate: 0.0023598]
		[batch 20/20] avg loss: 0.13546139173221655		[learning rate: 0.002357]
	Learning Rate: 0.002357
	LOSS [training: 0.12755675901744354 | validation: 0.27587960212911944]
	TIME [epoch: 8.19 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16322004106153284		[learning rate: 0.0023542]
		[batch 20/20] avg loss: 0.15744280434984623		[learning rate: 0.0023514]
	Learning Rate: 0.00235144
	LOSS [training: 0.16033142270568956 | validation: 0.12738911871005176]
	TIME [epoch: 8.18 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14136253856985684		[learning rate: 0.0023487]
		[batch 20/20] avg loss: 0.16286267554389458		[learning rate: 0.0023459]
	Learning Rate: 0.00234589
	LOSS [training: 0.15211260705687568 | validation: 0.1274541096756872]
	TIME [epoch: 8.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12706530720629144		[learning rate: 0.0023431]
		[batch 20/20] avg loss: 0.16815383382856722		[learning rate: 0.0023404]
	Learning Rate: 0.00234036
	LOSS [training: 0.14760957051742934 | validation: 0.22173485195502726]
	TIME [epoch: 8.18 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19397284647742927		[learning rate: 0.0023376]
		[batch 20/20] avg loss: 0.1309995779272117		[learning rate: 0.0023348]
	Learning Rate: 0.00233484
	LOSS [training: 0.1624862122023205 | validation: 0.14755595157838378]
	TIME [epoch: 8.17 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14569473345072423		[learning rate: 0.0023321]
		[batch 20/20] avg loss: 0.16132246971398162		[learning rate: 0.0023293]
	Learning Rate: 0.00232933
	LOSS [training: 0.1535086015823529 | validation: 0.130380709926844]
	TIME [epoch: 8.18 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1368505607922709		[learning rate: 0.0023266]
		[batch 20/20] avg loss: 0.13084608917884774		[learning rate: 0.0023238]
	Learning Rate: 0.00232383
	LOSS [training: 0.1338483249855593 | validation: 0.13591009828824602]
	TIME [epoch: 8.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14481262744137205		[learning rate: 0.0023211]
		[batch 20/20] avg loss: 0.17705130801828567		[learning rate: 0.0023184]
	Learning Rate: 0.00231835
	LOSS [training: 0.16093196772982887 | validation: 0.18230233017562544]
	TIME [epoch: 8.17 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1614317930332458		[learning rate: 0.0023156]
		[batch 20/20] avg loss: 0.15834661499374753		[learning rate: 0.0023129]
	Learning Rate: 0.00231288
	LOSS [training: 0.15988920401349668 | validation: 0.16388839074256156]
	TIME [epoch: 8.18 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14491977054442032		[learning rate: 0.0023102]
		[batch 20/20] avg loss: 0.10737602455094993		[learning rate: 0.0023074]
	Learning Rate: 0.00230743
	LOSS [training: 0.12614789754768513 | validation: 0.12651115873665278]
	TIME [epoch: 8.18 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14448209470643247		[learning rate: 0.0023047]
		[batch 20/20] avg loss: 0.1447855963776846		[learning rate: 0.002302]
	Learning Rate: 0.00230199
	LOSS [training: 0.14463384554205855 | validation: 0.15632068643121]
	TIME [epoch: 8.21 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12288766166985272		[learning rate: 0.0022993]
		[batch 20/20] avg loss: 0.14031163707982333		[learning rate: 0.0022966]
	Learning Rate: 0.00229656
	LOSS [training: 0.131599649374838 | validation: 0.07263987575866981]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.123776356757444		[learning rate: 0.0022938]
		[batch 20/20] avg loss: 0.15052409620743862		[learning rate: 0.0022911]
	Learning Rate: 0.00229114
	LOSS [training: 0.13715022648244132 | validation: 0.19844096297580271]
	TIME [epoch: 8.18 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1591763529498334		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.13276427380993647		[learning rate: 0.0022857]
	Learning Rate: 0.00228573
	LOSS [training: 0.14597031337988492 | validation: 0.09372686350512136]
	TIME [epoch: 8.18 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15077760424693284		[learning rate: 0.002283]
		[batch 20/20] avg loss: 0.14905893013868873		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.1499182671928108 | validation: 0.12337364269395201]
	TIME [epoch: 8.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1065783816468044		[learning rate: 0.0022777]
		[batch 20/20] avg loss: 0.1567436980449008		[learning rate: 0.002275]
	Learning Rate: 0.00227496
	LOSS [training: 0.1316610398458526 | validation: 0.1767694173359843]
	TIME [epoch: 8.18 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13341449605106262		[learning rate: 0.0022723]
		[batch 20/20] avg loss: 0.14763231757681824		[learning rate: 0.0022696]
	Learning Rate: 0.0022696
	LOSS [training: 0.14052340681394043 | validation: 0.07939671501513293]
	TIME [epoch: 8.17 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11068628301734199		[learning rate: 0.0022669]
		[batch 20/20] avg loss: 0.17380434485971694		[learning rate: 0.0022642]
	Learning Rate: 0.00226424
	LOSS [training: 0.1422453139385295 | validation: 0.13971162655320252]
	TIME [epoch: 8.18 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13438844307526981		[learning rate: 0.0022616]
		[batch 20/20] avg loss: 0.1308315936938178		[learning rate: 0.0022589]
	Learning Rate: 0.0022589
	LOSS [training: 0.13261001838454384 | validation: 0.0930184697552614]
	TIME [epoch: 8.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14645062635244088		[learning rate: 0.0022562]
		[batch 20/20] avg loss: 0.1497630225037362		[learning rate: 0.0022536]
	Learning Rate: 0.00225357
	LOSS [training: 0.14810682442808856 | validation: 0.15639469477819956]
	TIME [epoch: 8.18 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1442781691325788		[learning rate: 0.0022509]
		[batch 20/20] avg loss: 0.12485366649622806		[learning rate: 0.0022483]
	Learning Rate: 0.00224826
	LOSS [training: 0.13456591781440344 | validation: 0.11521590863868242]
	TIME [epoch: 8.17 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15800752169113497		[learning rate: 0.0022456]
		[batch 20/20] avg loss: 0.16240195952943765		[learning rate: 0.002243]
	Learning Rate: 0.00224295
	LOSS [training: 0.1602047406102863 | validation: 0.15349453968612986]
	TIME [epoch: 8.18 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15057915648885142		[learning rate: 0.0022403]
		[batch 20/20] avg loss: 0.1157681638197807		[learning rate: 0.0022377]
	Learning Rate: 0.00223766
	LOSS [training: 0.13317366015431603 | validation: 0.08650054445166501]
	TIME [epoch: 8.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13713077511154365		[learning rate: 0.002235]
		[batch 20/20] avg loss: 0.1378680535206674		[learning rate: 0.0022324]
	Learning Rate: 0.00223239
	LOSS [training: 0.1374994143161055 | validation: 0.1538314690692249]
	TIME [epoch: 8.18 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13872695098968388		[learning rate: 0.0022298]
		[batch 20/20] avg loss: 0.12552735118346053		[learning rate: 0.0022271]
	Learning Rate: 0.00222712
	LOSS [training: 0.13212715108657222 | validation: 0.07749819276271311]
	TIME [epoch: 8.18 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1206494728291301		[learning rate: 0.0022245]
		[batch 20/20] avg loss: 0.1432206979422592		[learning rate: 0.0022219]
	Learning Rate: 0.00222187
	LOSS [training: 0.13193508538569462 | validation: 0.09684965417593462]
	TIME [epoch: 8.18 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13858025811456187		[learning rate: 0.0022192]
		[batch 20/20] avg loss: 0.1169233425443871		[learning rate: 0.0022166]
	Learning Rate: 0.00221663
	LOSS [training: 0.12775180032947447 | validation: 0.1337276973728812]
	TIME [epoch: 8.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14479401098903094		[learning rate: 0.002214]
		[batch 20/20] avg loss: 0.13042080804706807		[learning rate: 0.0022114]
	Learning Rate: 0.0022114
	LOSS [training: 0.13760740951804948 | validation: 0.1831924300342218]
	TIME [epoch: 8.17 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12492946384405634		[learning rate: 0.0022088]
		[batch 20/20] avg loss: 0.11508940903311157		[learning rate: 0.0022062]
	Learning Rate: 0.00220618
	LOSS [training: 0.12000943643858393 | validation: 0.13887852650753965]
	TIME [epoch: 8.17 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1369350482432673		[learning rate: 0.0022036]
		[batch 20/20] avg loss: 0.13522951986555382		[learning rate: 0.002201]
	Learning Rate: 0.00220098
	LOSS [training: 0.13608228405441058 | validation: 0.16300960854946078]
	TIME [epoch: 8.17 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13399780621870014		[learning rate: 0.0021984]
		[batch 20/20] avg loss: 0.14525197284790775		[learning rate: 0.0021958]
	Learning Rate: 0.00219578
	LOSS [training: 0.13962488953330393 | validation: 0.1621199645529422]
	TIME [epoch: 8.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13902407204909034		[learning rate: 0.0021932]
		[batch 20/20] avg loss: 0.11989172911309479		[learning rate: 0.0021906]
	Learning Rate: 0.0021906
	LOSS [training: 0.12945790058109258 | validation: 0.14486587969285608]
	TIME [epoch: 8.18 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15110828227119275		[learning rate: 0.002188]
		[batch 20/20] avg loss: 0.10401017319428711		[learning rate: 0.0021854]
	Learning Rate: 0.00218544
	LOSS [training: 0.12755922773273992 | validation: 0.09665208110487536]
	TIME [epoch: 8.17 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10349066205729937		[learning rate: 0.0021829]
		[batch 20/20] avg loss: 0.15067304205648144		[learning rate: 0.0021803]
	Learning Rate: 0.00218028
	LOSS [training: 0.12708185205689043 | validation: 0.17760161132362534]
	TIME [epoch: 8.17 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1688282804946889		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.13780435085991233		[learning rate: 0.0021751]
	Learning Rate: 0.00217514
	LOSS [training: 0.15331631567730064 | validation: 0.08013620040605995]
	TIME [epoch: 8.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11235619707799102		[learning rate: 0.0021726]
		[batch 20/20] avg loss: 0.15550598790905637		[learning rate: 0.00217]
	Learning Rate: 0.00217001
	LOSS [training: 0.1339310924935237 | validation: 0.11705429087987432]
	TIME [epoch: 8.17 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13979525552150046		[learning rate: 0.0021674]
		[batch 20/20] avg loss: 0.1694509786813003		[learning rate: 0.0021649]
	Learning Rate: 0.00216489
	LOSS [training: 0.1546231171014004 | validation: 0.09394776192206038]
	TIME [epoch: 8.17 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13154696264439564		[learning rate: 0.0021623]
		[batch 20/20] avg loss: 0.14997030840056594		[learning rate: 0.0021598]
	Learning Rate: 0.00215978
	LOSS [training: 0.14075863552248077 | validation: 0.10622439744494211]
	TIME [epoch: 8.17 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11475211793080653		[learning rate: 0.0021572]
		[batch 20/20] avg loss: 0.1179794700701979		[learning rate: 0.0021547]
	Learning Rate: 0.00215469
	LOSS [training: 0.1163657940005022 | validation: 0.16010003615051133]
	TIME [epoch: 8.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15805719411083355		[learning rate: 0.0021521]
		[batch 20/20] avg loss: 0.12744475328891305		[learning rate: 0.0021496]
	Learning Rate: 0.00214961
	LOSS [training: 0.14275097369987327 | validation: 0.1372462284428394]
	TIME [epoch: 8.18 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12877214664009387		[learning rate: 0.0021471]
		[batch 20/20] avg loss: 0.1659939556800532		[learning rate: 0.0021445]
	Learning Rate: 0.00214454
	LOSS [training: 0.14738305116007352 | validation: 0.10542033357321631]
	TIME [epoch: 8.18 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1490770624862719		[learning rate: 0.002142]
		[batch 20/20] avg loss: 0.15350982221687168		[learning rate: 0.0021395]
	Learning Rate: 0.00213948
	LOSS [training: 0.15129344235157177 | validation: 0.08462763549957385]
	TIME [epoch: 8.17 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1642555380755797		[learning rate: 0.002137]
		[batch 20/20] avg loss: 0.23778756053287467		[learning rate: 0.0021344]
	Learning Rate: 0.00213443
	LOSS [training: 0.2010215493042272 | validation: 0.09405082571639387]
	TIME [epoch: 8.19 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15573525838985502		[learning rate: 0.0021319]
		[batch 20/20] avg loss: 0.1542470171385081		[learning rate: 0.0021294]
	Learning Rate: 0.0021294
	LOSS [training: 0.15499113776418158 | validation: 0.07579117312695262]
	TIME [epoch: 8.18 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10061486464672105		[learning rate: 0.0021269]
		[batch 20/20] avg loss: 0.212204368486648		[learning rate: 0.0021244]
	Learning Rate: 0.00212437
	LOSS [training: 0.15640961656668456 | validation: 0.13560046759583644]
	TIME [epoch: 8.17 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1113794659570831		[learning rate: 0.0021219]
		[batch 20/20] avg loss: 0.14944614541057644		[learning rate: 0.0021194]
	Learning Rate: 0.00211936
	LOSS [training: 0.13041280568382976 | validation: 0.15632951166075343]
	TIME [epoch: 8.18 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11191094917765387		[learning rate: 0.0021169]
		[batch 20/20] avg loss: 0.14470386348279085		[learning rate: 0.0021144]
	Learning Rate: 0.00211436
	LOSS [training: 0.12830740633022236 | validation: 0.1467369806511482]
	TIME [epoch: 8.19 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13600931023347024		[learning rate: 0.0021119]
		[batch 20/20] avg loss: 0.16000761510086695		[learning rate: 0.0021094]
	Learning Rate: 0.00210938
	LOSS [training: 0.1480084626671686 | validation: 0.14350935831256567]
	TIME [epoch: 8.18 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15246569595199894		[learning rate: 0.0021069]
		[batch 20/20] avg loss: 0.11966093483852938		[learning rate: 0.0021044]
	Learning Rate: 0.0021044
	LOSS [training: 0.13606331539526417 | validation: 0.202798381956113]
	TIME [epoch: 8.17 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1365582295462659		[learning rate: 0.0021019]
		[batch 20/20] avg loss: 0.13372702215507148		[learning rate: 0.0020994]
	Learning Rate: 0.00209944
	LOSS [training: 0.13514262585066866 | validation: 0.11584969996699682]
	TIME [epoch: 8.18 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12910666521194628		[learning rate: 0.002097]
		[batch 20/20] avg loss: 0.1325524868077267		[learning rate: 0.0020945]
	Learning Rate: 0.00209448
	LOSS [training: 0.1308295760098365 | validation: 0.08201944492989467]
	TIME [epoch: 8.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13741623311214218		[learning rate: 0.002092]
		[batch 20/20] avg loss: 0.1729934862190646		[learning rate: 0.0020895]
	Learning Rate: 0.00208954
	LOSS [training: 0.15520485966560338 | validation: 0.10263738402739112]
	TIME [epoch: 8.17 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15271396039735846		[learning rate: 0.0020871]
		[batch 20/20] avg loss: 0.14391314954057693		[learning rate: 0.0020846]
	Learning Rate: 0.00208461
	LOSS [training: 0.1483135549689677 | validation: 0.11645645211523564]
	TIME [epoch: 8.18 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1355325145919287		[learning rate: 0.0020822]
		[batch 20/20] avg loss: 0.13319302268799987		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.13436276863996427 | validation: 0.12445384267907486]
	TIME [epoch: 8.17 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1604193229202767		[learning rate: 0.0020772]
		[batch 20/20] avg loss: 0.12861852001314372		[learning rate: 0.0020748]
	Learning Rate: 0.00207479
	LOSS [training: 0.14451892146671022 | validation: 0.07669156540951529]
	TIME [epoch: 8.18 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12133554030134289		[learning rate: 0.0020723]
		[batch 20/20] avg loss: 0.13637455622198366		[learning rate: 0.0020699]
	Learning Rate: 0.0020699
	LOSS [training: 0.12885504826166327 | validation: 0.12571587945012905]
	TIME [epoch: 8.17 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16853415553955198		[learning rate: 0.0020675]
		[batch 20/20] avg loss: 0.14895763059824427		[learning rate: 0.002065]
	Learning Rate: 0.00206501
	LOSS [training: 0.15874589306889814 | validation: 0.10738343522749645]
	TIME [epoch: 8.18 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1119494886606716		[learning rate: 0.0020626]
		[batch 20/20] avg loss: 0.13963231659270367		[learning rate: 0.0020601]
	Learning Rate: 0.00206014
	LOSS [training: 0.12579090262668763 | validation: 0.08099209082085745]
	TIME [epoch: 8.17 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15198537238670928		[learning rate: 0.0020577]
		[batch 20/20] avg loss: 0.11005059623167343		[learning rate: 0.0020553]
	Learning Rate: 0.00205528
	LOSS [training: 0.13101798430919137 | validation: 0.14150575177966257]
	TIME [epoch: 8.19 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13568042542283906		[learning rate: 0.0020529]
		[batch 20/20] avg loss: 0.1168097996696487		[learning rate: 0.0020504]
	Learning Rate: 0.00205044
	LOSS [training: 0.12624511254624388 | validation: 0.16088161227335906]
	TIME [epoch: 8.18 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1233637490483663		[learning rate: 0.002048]
		[batch 20/20] avg loss: 0.13252461823280476		[learning rate: 0.0020456]
	Learning Rate: 0.0020456
	LOSS [training: 0.12794418364058552 | validation: 0.19766134170802124]
	TIME [epoch: 8.18 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13395186320704616		[learning rate: 0.0020432]
		[batch 20/20] avg loss: 0.14158488443300596		[learning rate: 0.0020408]
	Learning Rate: 0.00204077
	LOSS [training: 0.1377683738200261 | validation: 0.08410143074710197]
	TIME [epoch: 8.18 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1212705584439581		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.147553161205066		[learning rate: 0.002036]
	Learning Rate: 0.00203596
	LOSS [training: 0.13441185982451204 | validation: 0.13400819678129458]
	TIME [epoch: 8.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14302263581505553		[learning rate: 0.0020336]
		[batch 20/20] avg loss: 0.1555069125115655		[learning rate: 0.0020312]
	Learning Rate: 0.00203116
	LOSS [training: 0.14926477416331055 | validation: 0.10798685949914925]
	TIME [epoch: 8.19 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12077527590963136		[learning rate: 0.0020288]
		[batch 20/20] avg loss: 0.13570674086380427		[learning rate: 0.0020264]
	Learning Rate: 0.00202637
	LOSS [training: 0.12824100838671787 | validation: 0.16024194579879142]
	TIME [epoch: 8.18 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13548358702491659		[learning rate: 0.002024]
		[batch 20/20] avg loss: 0.13934167192608202		[learning rate: 0.0020216]
	Learning Rate: 0.00202159
	LOSS [training: 0.13741262947549931 | validation: 0.0806609970331789]
	TIME [epoch: 8.17 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1249970034287637		[learning rate: 0.0020192]
		[batch 20/20] avg loss: 0.14734735726433487		[learning rate: 0.0020168]
	Learning Rate: 0.00201682
	LOSS [training: 0.13617218034654927 | validation: 0.08472105611346092]
	TIME [epoch: 8.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397343599675505		[learning rate: 0.0020144]
		[batch 20/20] avg loss: 0.12236455547693922		[learning rate: 0.0020121]
	Learning Rate: 0.00201206
	LOSS [training: 0.13104945772224483 | validation: 0.0852423008744402]
	TIME [epoch: 8.19 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0981263809118689		[learning rate: 0.0020097]
		[batch 20/20] avg loss: 0.1628770075216484		[learning rate: 0.0020073]
	Learning Rate: 0.00200731
	LOSS [training: 0.13050169421675867 | validation: 0.17597469167882465]
	TIME [epoch: 8.18 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13567059802055406		[learning rate: 0.0020049]
		[batch 20/20] avg loss: 0.11991264561706805		[learning rate: 0.0020026]
	Learning Rate: 0.00200258
	LOSS [training: 0.12779162181881104 | validation: 0.20100236862658635]
	TIME [epoch: 8.17 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12535435742066808		[learning rate: 0.0020002]
		[batch 20/20] avg loss: 0.10563068459975282		[learning rate: 0.0019979]
	Learning Rate: 0.00199786
	LOSS [training: 0.11549252101021043 | validation: 0.14316562428127677]
	TIME [epoch: 8.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12239951367820541		[learning rate: 0.0019955]
		[batch 20/20] avg loss: 0.15382349461938002		[learning rate: 0.0019931]
	Learning Rate: 0.00199314
	LOSS [training: 0.13811150414879272 | validation: 0.13025518808758388]
	TIME [epoch: 8.19 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12993609967115927		[learning rate: 0.0019908]
		[batch 20/20] avg loss: 0.15362925402114547		[learning rate: 0.0019884]
	Learning Rate: 0.00198844
	LOSS [training: 0.14178267684615234 | validation: 0.10225919522857585]
	TIME [epoch: 8.18 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11991238881350155		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.15492719959226925		[learning rate: 0.0019838]
	Learning Rate: 0.00198375
	LOSS [training: 0.13741979420288541 | validation: 0.14661602302715915]
	TIME [epoch: 8.18 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1333740392229969		[learning rate: 0.0019814]
		[batch 20/20] avg loss: 0.13622892043034399		[learning rate: 0.0019791]
	Learning Rate: 0.00197907
	LOSS [training: 0.13480147982667046 | validation: 0.08633921439120071]
	TIME [epoch: 8.18 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1394987808539793		[learning rate: 0.0019767]
		[batch 20/20] avg loss: 0.14754078836608928		[learning rate: 0.0019744]
	Learning Rate: 0.0019744
	LOSS [training: 0.14351978461003426 | validation: 0.11003860776723937]
	TIME [epoch: 8.19 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1262774202280653		[learning rate: 0.0019721]
		[batch 20/20] avg loss: 0.1592989337714007		[learning rate: 0.0019697]
	Learning Rate: 0.00196975
	LOSS [training: 0.142788176999733 | validation: 0.18483924565343937]
	TIME [epoch: 8.17 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12472918886428067		[learning rate: 0.0019674]
		[batch 20/20] avg loss: 0.13387967646829324		[learning rate: 0.0019651]
	Learning Rate: 0.0019651
	LOSS [training: 0.12930443266628694 | validation: 0.1119319389625383]
	TIME [epoch: 8.18 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10799662732504596		[learning rate: 0.0019628]
		[batch 20/20] avg loss: 0.12407809816048465		[learning rate: 0.0019605]
	Learning Rate: 0.00196046
	LOSS [training: 0.1160373627427653 | validation: 0.06980632550370221]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10446263028588902		[learning rate: 0.0019582]
		[batch 20/20] avg loss: 0.11341163064636732		[learning rate: 0.0019558]
	Learning Rate: 0.00195584
	LOSS [training: 0.10893713046612818 | validation: 0.08172480280891452]
	TIME [epoch: 8.18 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17508892499941875		[learning rate: 0.0019535]
		[batch 20/20] avg loss: 0.1621107189868068		[learning rate: 0.0019512]
	Learning Rate: 0.00195123
	LOSS [training: 0.16859982199311277 | validation: 0.12542291584368154]
	TIME [epoch: 8.17 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1771383845121462		[learning rate: 0.0019489]
		[batch 20/20] avg loss: 0.14133315567533816		[learning rate: 0.0019466]
	Learning Rate: 0.00194662
	LOSS [training: 0.1592357700937422 | validation: 0.11132745338642892]
	TIME [epoch: 8.17 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10449546582493832		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.11332179790232581		[learning rate: 0.001942]
	Learning Rate: 0.00194203
	LOSS [training: 0.10890863186363206 | validation: 0.15628880901399622]
	TIME [epoch: 8.17 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1469517022074316		[learning rate: 0.0019397]
		[batch 20/20] avg loss: 0.13298432466398907		[learning rate: 0.0019375]
	Learning Rate: 0.00193745
	LOSS [training: 0.13996801343571036 | validation: 0.3373125793624784]
	TIME [epoch: 8.19 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15945407969051756		[learning rate: 0.0019352]
		[batch 20/20] avg loss: 0.12098011076599524		[learning rate: 0.0019329]
	Learning Rate: 0.00193288
	LOSS [training: 0.14021709522825643 | validation: 0.12193787918017099]
	TIME [epoch: 8.16 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10876176375155219		[learning rate: 0.0019306]
		[batch 20/20] avg loss: 0.10857417978112345		[learning rate: 0.0019283]
	Learning Rate: 0.00192832
	LOSS [training: 0.10866797176633783 | validation: 0.07552417334031622]
	TIME [epoch: 8.16 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11209420495426851		[learning rate: 0.001926]
		[batch 20/20] avg loss: 0.12005499903373455		[learning rate: 0.0019238]
	Learning Rate: 0.00192377
	LOSS [training: 0.11607460199400157 | validation: 0.20490178394949446]
	TIME [epoch: 8.17 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13458615247047784		[learning rate: 0.0019215]
		[batch 20/20] avg loss: 0.10940946478941031		[learning rate: 0.0019192]
	Learning Rate: 0.00191924
	LOSS [training: 0.12199780862994405 | validation: 0.08590843934521242]
	TIME [epoch: 8.19 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11127794327083426		[learning rate: 0.001917]
		[batch 20/20] avg loss: 0.11278409040540487		[learning rate: 0.0019147]
	Learning Rate: 0.00191471
	LOSS [training: 0.11203101683811958 | validation: 0.06916105000064407]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_750.pth
	Model improved!!!
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14562375249046167		[learning rate: 0.0019124]
		[batch 20/20] avg loss: 0.12883890032174497		[learning rate: 0.0019102]
	Learning Rate: 0.00191019
	LOSS [training: 0.1372313264061033 | validation: 0.0734797306582889]
	TIME [epoch: 8.17 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12778762838454294		[learning rate: 0.0019079]
		[batch 20/20] avg loss: 0.11950058808384403		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.12364410823419347 | validation: 0.1367487849445533]
	TIME [epoch: 8.17 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15378408853286377		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.10911740504816633		[learning rate: 0.0019012]
	Learning Rate: 0.00190119
	LOSS [training: 0.13145074679051505 | validation: 0.059036330122772714]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11417882689822108		[learning rate: 0.0018989]
		[batch 20/20] avg loss: 0.12752774141273288		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.12085328415547698 | validation: 0.10019044602863239]
	TIME [epoch: 8.16 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16466415316417599		[learning rate: 0.0018945]
		[batch 20/20] avg loss: 0.1234023513705127		[learning rate: 0.0018922]
	Learning Rate: 0.00189223
	LOSS [training: 0.1440332522673443 | validation: 0.07771250282531193]
	TIME [epoch: 8.15 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09737903407596085		[learning rate: 0.00189]
		[batch 20/20] avg loss: 0.1250683965933724		[learning rate: 0.0018878]
	Learning Rate: 0.00188777
	LOSS [training: 0.11122371533466664 | validation: 0.10125079385279565]
	TIME [epoch: 8.16 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16841950261076039		[learning rate: 0.0018855]
		[batch 20/20] avg loss: 0.11806880312434773		[learning rate: 0.0018833]
	Learning Rate: 0.00188332
	LOSS [training: 0.14324415286755404 | validation: 0.10003442912858744]
	TIME [epoch: 8.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10097256291881004		[learning rate: 0.0018811]
		[batch 20/20] avg loss: 0.10752949582169706		[learning rate: 0.0018789]
	Learning Rate: 0.00187887
	LOSS [training: 0.10425102937025357 | validation: 0.1497404066819714]
	TIME [epoch: 8.16 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13201817313366523		[learning rate: 0.0018767]
		[batch 20/20] avg loss: 0.1588614151218253		[learning rate: 0.0018744]
	Learning Rate: 0.00187444
	LOSS [training: 0.14543979412774527 | validation: 0.24924418804717927]
	TIME [epoch: 8.16 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13946764787726676		[learning rate: 0.0018722]
		[batch 20/20] avg loss: 0.11964244444158272		[learning rate: 0.00187]
	Learning Rate: 0.00187002
	LOSS [training: 0.12955504615942476 | validation: 0.09061934869679664]
	TIME [epoch: 8.16 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10881332760844473		[learning rate: 0.0018678]
		[batch 20/20] avg loss: 0.1172078025699628		[learning rate: 0.0018656]
	Learning Rate: 0.00186561
	LOSS [training: 0.11301056508920378 | validation: 0.10303602213514212]
	TIME [epoch: 8.18 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11575910927609168		[learning rate: 0.0018634]
		[batch 20/20] avg loss: 0.12424938892535149		[learning rate: 0.0018612]
	Learning Rate: 0.00186121
	LOSS [training: 0.1200042491007216 | validation: 0.12074381764986922]
	TIME [epoch: 8.16 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1054073416814482		[learning rate: 0.001859]
		[batch 20/20] avg loss: 0.1481107498098059		[learning rate: 0.0018568]
	Learning Rate: 0.00185682
	LOSS [training: 0.12675904574562705 | validation: 0.12320574601242187]
	TIME [epoch: 8.16 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11745913170446695		[learning rate: 0.0018546]
		[batch 20/20] avg loss: 0.11101289893369448		[learning rate: 0.0018524]
	Learning Rate: 0.00185244
	LOSS [training: 0.11423601531908072 | validation: 0.12105438210305625]
	TIME [epoch: 8.15 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11715881683043608		[learning rate: 0.0018503]
		[batch 20/20] avg loss: 0.09570906895985422		[learning rate: 0.0018481]
	Learning Rate: 0.00184807
	LOSS [training: 0.10643394289514514 | validation: 0.07752739660521729]
	TIME [epoch: 8.18 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1379946224777036		[learning rate: 0.0018459]
		[batch 20/20] avg loss: 0.14705670518037234		[learning rate: 0.0018437]
	Learning Rate: 0.00184371
	LOSS [training: 0.14252566382903797 | validation: 0.13473249905879628]
	TIME [epoch: 8.16 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13361539355066232		[learning rate: 0.0018415]
		[batch 20/20] avg loss: 0.136632430081723		[learning rate: 0.0018394]
	Learning Rate: 0.00183936
	LOSS [training: 0.13512391181619268 | validation: 0.10160173344789293]
	TIME [epoch: 8.16 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1292326621652541		[learning rate: 0.0018372]
		[batch 20/20] avg loss: 0.12568882314146151		[learning rate: 0.001835]
	Learning Rate: 0.00183502
	LOSS [training: 0.12746074265335777 | validation: 0.15287022393449543]
	TIME [epoch: 8.16 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13065355643980445		[learning rate: 0.0018329]
		[batch 20/20] avg loss: 0.11598270632221933		[learning rate: 0.0018307]
	Learning Rate: 0.00183069
	LOSS [training: 0.12331813138101189 | validation: 0.09047022154917257]
	TIME [epoch: 8.19 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1473628677646687		[learning rate: 0.0018285]
		[batch 20/20] avg loss: 0.11991455668903639		[learning rate: 0.0018264]
	Learning Rate: 0.00182637
	LOSS [training: 0.13363871222685259 | validation: 0.09063188280104108]
	TIME [epoch: 8.16 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11885639507576515		[learning rate: 0.0018242]
		[batch 20/20] avg loss: 0.1475882144902216		[learning rate: 0.0018221]
	Learning Rate: 0.00182207
	LOSS [training: 0.13322230478299335 | validation: 0.25299835216052813]
	TIME [epoch: 8.16 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14389507470538093		[learning rate: 0.0018199]
		[batch 20/20] avg loss: 0.11923164674889182		[learning rate: 0.0018178]
	Learning Rate: 0.00181777
	LOSS [training: 0.13156336072713637 | validation: 0.10193266481200305]
	TIME [epoch: 8.17 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12807844771861265		[learning rate: 0.0018156]
		[batch 20/20] avg loss: 0.11571904100693942		[learning rate: 0.0018135]
	Learning Rate: 0.00181348
	LOSS [training: 0.12189874436277603 | validation: 0.12724503726584652]
	TIME [epoch: 8.17 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15107305423881215		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.12925116513150314		[learning rate: 0.0018092]
	Learning Rate: 0.0018092
	LOSS [training: 0.14016210968515758 | validation: 0.14093144402680755]
	TIME [epoch: 8.16 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1281529732077058		[learning rate: 0.0018071]
		[batch 20/20] avg loss: 0.1139741512839475		[learning rate: 0.0018049]
	Learning Rate: 0.00180493
	LOSS [training: 0.12106356224582666 | validation: 0.07090276184146699]
	TIME [epoch: 8.16 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11611826064308306		[learning rate: 0.0018028]
		[batch 20/20] avg loss: 0.15934191343322213		[learning rate: 0.0018007]
	Learning Rate: 0.00180068
	LOSS [training: 0.1377300870381526 | validation: 0.1008540012185981]
	TIME [epoch: 8.16 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1159512828533406		[learning rate: 0.0017986]
		[batch 20/20] avg loss: 0.11599871849401158		[learning rate: 0.0017964]
	Learning Rate: 0.00179643
	LOSS [training: 0.1159750006736761 | validation: 0.07295367504178367]
	TIME [epoch: 8.19 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11682005305553422		[learning rate: 0.0017943]
		[batch 20/20] avg loss: 0.13740362371732212		[learning rate: 0.0017922]
	Learning Rate: 0.00179219
	LOSS [training: 0.12711183838642814 | validation: 0.09544143106398512]
	TIME [epoch: 8.17 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10840664917320775		[learning rate: 0.0017901]
		[batch 20/20] avg loss: 0.08175751984114783		[learning rate: 0.001788]
	Learning Rate: 0.00178796
	LOSS [training: 0.09508208450717778 | validation: 0.09419953614546793]
	TIME [epoch: 8.16 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12883149066723884		[learning rate: 0.0017859]
		[batch 20/20] avg loss: 0.11852681149853721		[learning rate: 0.0017837]
	Learning Rate: 0.00178375
	LOSS [training: 0.12367915108288803 | validation: 0.15405413975731302]
	TIME [epoch: 8.17 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11341242892036821		[learning rate: 0.0017816]
		[batch 20/20] avg loss: 0.13494662112043565		[learning rate: 0.0017795]
	Learning Rate: 0.00177954
	LOSS [training: 0.12417952502040193 | validation: 0.14611568851133067]
	TIME [epoch: 8.18 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1259641017452617		[learning rate: 0.0017774]
		[batch 20/20] avg loss: 0.11529881174654506		[learning rate: 0.0017753]
	Learning Rate: 0.00177534
	LOSS [training: 0.12063145674590339 | validation: 0.0784405270965205]
	TIME [epoch: 8.17 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10449036479496261		[learning rate: 0.0017732]
		[batch 20/20] avg loss: 0.0906158623041565		[learning rate: 0.0017712]
	Learning Rate: 0.00177115
	LOSS [training: 0.09755311354955956 | validation: 0.10724587333645513]
	TIME [epoch: 8.16 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11416672629838094		[learning rate: 0.0017691]
		[batch 20/20] avg loss: 0.15264797629549456		[learning rate: 0.001767]
	Learning Rate: 0.00176698
	LOSS [training: 0.13340735129693776 | validation: 0.20524646628374354]
	TIME [epoch: 8.16 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1499105268330768		[learning rate: 0.0017649]
		[batch 20/20] avg loss: 0.1316461992455154		[learning rate: 0.0017628]
	Learning Rate: 0.00176281
	LOSS [training: 0.1407783630392961 | validation: 0.1180059467341043]
	TIME [epoch: 8.19 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14138194274562882		[learning rate: 0.0017607]
		[batch 20/20] avg loss: 0.14246637364321543		[learning rate: 0.0017587]
	Learning Rate: 0.00175865
	LOSS [training: 0.1419241581944221 | validation: 0.1939279977509695]
	TIME [epoch: 8.17 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14435440885500522		[learning rate: 0.0017566]
		[batch 20/20] avg loss: 0.1279500666057624		[learning rate: 0.0017545]
	Learning Rate: 0.0017545
	LOSS [training: 0.13615223773038382 | validation: 0.10913765292624518]
	TIME [epoch: 8.17 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12781282670950195		[learning rate: 0.0017524]
		[batch 20/20] avg loss: 0.14155968877765263		[learning rate: 0.0017504]
	Learning Rate: 0.00175036
	LOSS [training: 0.1346862577435773 | validation: 0.1290090210460494]
	TIME [epoch: 8.17 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1379639297250515		[learning rate: 0.0017483]
		[batch 20/20] avg loss: 0.13382264598305885		[learning rate: 0.0017462]
	Learning Rate: 0.00174623
	LOSS [training: 0.13589328785405513 | validation: 0.15139361226158327]
	TIME [epoch: 8.18 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1245983633946802		[learning rate: 0.0017442]
		[batch 20/20] avg loss: 0.14177553100782175		[learning rate: 0.0017421]
	Learning Rate: 0.00174212
	LOSS [training: 0.133186947201251 | validation: 0.10539319680531115]
	TIME [epoch: 8.17 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15187218427590715		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.09478090633047301		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.1233265453031901 | validation: 0.08287733894683894]
	TIME [epoch: 8.15 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09815288427336212		[learning rate: 0.001736]
		[batch 20/20] avg loss: 0.11844136130999568		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.10829712279167891 | validation: 0.0814429532793432]
	TIME [epoch: 8.15 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11503980551522727		[learning rate: 0.0017319]
		[batch 20/20] avg loss: 0.11656526892434967		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.11580253721978846 | validation: 0.10711868374775699]
	TIME [epoch: 8.18 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1017739847169344		[learning rate: 0.0017278]
		[batch 20/20] avg loss: 0.11649992245962064		[learning rate: 0.0017257]
	Learning Rate: 0.00172574
	LOSS [training: 0.10913695358827749 | validation: 0.1343216342446629]
	TIME [epoch: 8.16 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12917901894937964		[learning rate: 0.0017237]
		[batch 20/20] avg loss: 0.1510455186033794		[learning rate: 0.0017217]
	Learning Rate: 0.00172167
	LOSS [training: 0.14011226877637947 | validation: 0.10129856175703889]
	TIME [epoch: 8.16 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10749897794111726		[learning rate: 0.0017196]
		[batch 20/20] avg loss: 0.13720288313000273		[learning rate: 0.0017176]
	Learning Rate: 0.0017176
	LOSS [training: 0.12235093053556001 | validation: 0.11141875838151259]
	TIME [epoch: 8.15 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10777693674898085		[learning rate: 0.0017156]
		[batch 20/20] avg loss: 0.09348940580747321		[learning rate: 0.0017136]
	Learning Rate: 0.00171355
	LOSS [training: 0.10063317127822705 | validation: 0.09798624783259068]
	TIME [epoch: 8.18 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10635215499768114		[learning rate: 0.0017115]
		[batch 20/20] avg loss: 0.10772886123503275		[learning rate: 0.0017095]
	Learning Rate: 0.00170951
	LOSS [training: 0.10704050811635697 | validation: 0.06492478594897216]
	TIME [epoch: 8.17 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.133100348260196		[learning rate: 0.0017075]
		[batch 20/20] avg loss: 0.10926122558656826		[learning rate: 0.0017055]
	Learning Rate: 0.00170548
	LOSS [training: 0.12118078692338215 | validation: 0.1209395597717532]
	TIME [epoch: 8.16 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1432824411587071		[learning rate: 0.0017035]
		[batch 20/20] avg loss: 0.09361695553853391		[learning rate: 0.0017015]
	Learning Rate: 0.00170146
	LOSS [training: 0.11844969834862051 | validation: 0.13921165506823518]
	TIME [epoch: 8.17 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1248880563104322		[learning rate: 0.0016994]
		[batch 20/20] avg loss: 0.1173665266845898		[learning rate: 0.0016974]
	Learning Rate: 0.00169744
	LOSS [training: 0.12112729149751103 | validation: 0.09067678459260942]
	TIME [epoch: 8.18 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12378067192840406		[learning rate: 0.0016954]
		[batch 20/20] avg loss: 0.12617553212000227		[learning rate: 0.0016934]
	Learning Rate: 0.00169344
	LOSS [training: 0.12497810202420316 | validation: 0.10626188399442671]
	TIME [epoch: 8.19 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13685071222284126		[learning rate: 0.0016914]
		[batch 20/20] avg loss: 0.12170326807603735		[learning rate: 0.0016894]
	Learning Rate: 0.00168944
	LOSS [training: 0.1292769901494393 | validation: 0.10334431349852544]
	TIME [epoch: 8.16 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11224805187646478		[learning rate: 0.0016874]
		[batch 20/20] avg loss: 0.13572043094234812		[learning rate: 0.0016855]
	Learning Rate: 0.00168546
	LOSS [training: 0.12398424140940645 | validation: 0.08911846666285635]
	TIME [epoch: 8.15 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1020329601844843		[learning rate: 0.0016835]
		[batch 20/20] avg loss: 0.11686696287843239		[learning rate: 0.0016815]
	Learning Rate: 0.00168148
	LOSS [training: 0.10944996153145832 | validation: 0.15438617735597898]
	TIME [epoch: 8.18 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1557338850731426		[learning rate: 0.0016795]
		[batch 20/20] avg loss: 0.10365944207069237		[learning rate: 0.0016775]
	Learning Rate: 0.00167752
	LOSS [training: 0.1296966635719175 | validation: 0.08734435889423312]
	TIME [epoch: 8.17 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09569615008344268		[learning rate: 0.0016755]
		[batch 20/20] avg loss: 0.11140054094809093		[learning rate: 0.0016736]
	Learning Rate: 0.00167356
	LOSS [training: 0.10354834551576679 | validation: 0.09037655690156451]
	TIME [epoch: 8.17 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10875057335467979		[learning rate: 0.0016716]
		[batch 20/20] avg loss: 0.1451299857932525		[learning rate: 0.0016696]
	Learning Rate: 0.00166961
	LOSS [training: 0.12694027957396617 | validation: 0.15058675400061017]
	TIME [epoch: 8.16 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11968096492013833		[learning rate: 0.0016676]
		[batch 20/20] avg loss: 0.11227565739704826		[learning rate: 0.0016657]
	Learning Rate: 0.00166567
	LOSS [training: 0.11597831115859329 | validation: 0.12094793567287207]
	TIME [epoch: 8.18 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.134404985625137		[learning rate: 0.0016637]
		[batch 20/20] avg loss: 0.09902891976807679		[learning rate: 0.0016617]
	Learning Rate: 0.00166174
	LOSS [training: 0.11671695269660691 | validation: 0.10578397696170372]
	TIME [epoch: 8.18 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10054477551248797		[learning rate: 0.0016598]
		[batch 20/20] avg loss: 0.12582750127222422		[learning rate: 0.0016578]
	Learning Rate: 0.00165782
	LOSS [training: 0.11318613839235614 | validation: 0.1620076494698329]
	TIME [epoch: 8.17 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12346139778758929		[learning rate: 0.0016559]
		[batch 20/20] avg loss: 0.10340620463223194		[learning rate: 0.0016539]
	Learning Rate: 0.00165391
	LOSS [training: 0.11343380120991062 | validation: 0.09522722452442031]
	TIME [epoch: 8.17 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11527219836348057		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.12143106400122453		[learning rate: 0.00165]
	Learning Rate: 0.00165001
	LOSS [training: 0.11835163118235256 | validation: 0.11431294961603483]
	TIME [epoch: 8.37 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12249167407701297		[learning rate: 0.0016481]
		[batch 20/20] avg loss: 0.09644510937711578		[learning rate: 0.0016461]
	Learning Rate: 0.00164612
	LOSS [training: 0.10946839172706438 | validation: 0.07345682157044173]
	TIME [epoch: 8.18 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10186446832005051		[learning rate: 0.0016442]
		[batch 20/20] avg loss: 0.16013138563129276		[learning rate: 0.0016422]
	Learning Rate: 0.00164224
	LOSS [training: 0.1309979269756716 | validation: 0.15442186799660113]
	TIME [epoch: 8.15 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14698896099123812		[learning rate: 0.0016403]
		[batch 20/20] avg loss: 0.11618288996228825		[learning rate: 0.0016384]
	Learning Rate: 0.00163836
	LOSS [training: 0.13158592547676318 | validation: 0.0680526414330334]
	TIME [epoch: 8.16 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08942910621753335		[learning rate: 0.0016364]
		[batch 20/20] avg loss: 0.104812646402709		[learning rate: 0.0016345]
	Learning Rate: 0.0016345
	LOSS [training: 0.09712087631012116 | validation: 0.1393874763139838]
	TIME [epoch: 8.16 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13670963709813583		[learning rate: 0.0016326]
		[batch 20/20] avg loss: 0.11054368774154136		[learning rate: 0.0016306]
	Learning Rate: 0.00163064
	LOSS [training: 0.12362666241983859 | validation: 0.09846835690069546]
	TIME [epoch: 8.18 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1382232483620925		[learning rate: 0.0016287]
		[batch 20/20] avg loss: 0.1549610874010589		[learning rate: 0.0016268]
	Learning Rate: 0.0016268
	LOSS [training: 0.1465921678815757 | validation: 0.16535432247180964]
	TIME [epoch: 8.16 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15112847828816792		[learning rate: 0.0016249]
		[batch 20/20] avg loss: 0.1078640866472769		[learning rate: 0.001623]
	Learning Rate: 0.00162296
	LOSS [training: 0.12949628246772243 | validation: 0.09081897911173763]
	TIME [epoch: 8.16 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10976686831703089		[learning rate: 0.001621]
		[batch 20/20] avg loss: 0.10088958165622393		[learning rate: 0.0016191]
	Learning Rate: 0.00161913
	LOSS [training: 0.10532822498662742 | validation: 0.09912142729230733]
	TIME [epoch: 8.16 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11044295475427908		[learning rate: 0.0016172]
		[batch 20/20] avg loss: 0.13436055410467174		[learning rate: 0.0016153]
	Learning Rate: 0.00161531
	LOSS [training: 0.12240175442947543 | validation: 0.08426749578343691]
	TIME [epoch: 8.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15394387002740823		[learning rate: 0.0016134]
		[batch 20/20] avg loss: 0.0928224352013155		[learning rate: 0.0016115]
	Learning Rate: 0.0016115
	LOSS [training: 0.12338315261436188 | validation: 0.08642745459536737]
	TIME [epoch: 8.14 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14583015755246986		[learning rate: 0.0016096]
		[batch 20/20] avg loss: 0.11154608506834576		[learning rate: 0.0016077]
	Learning Rate: 0.0016077
	LOSS [training: 0.1286881213104078 | validation: 0.07819049366676382]
	TIME [epoch: 8.15 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11618043115291388		[learning rate: 0.0016058]
		[batch 20/20] avg loss: 0.1251164592980602		[learning rate: 0.0016039]
	Learning Rate: 0.00160391
	LOSS [training: 0.12064844522548705 | validation: 0.05569345611086471]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_825.pth
	Model improved!!!
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11688583075331427		[learning rate: 0.001602]
		[batch 20/20] avg loss: 0.09840532236750557		[learning rate: 0.0016001]
	Learning Rate: 0.00160012
	LOSS [training: 0.10764557656040992 | validation: 0.10463515771452822]
	TIME [epoch: 8.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09914149804806911		[learning rate: 0.0015982]
		[batch 20/20] avg loss: 0.09730080069724516		[learning rate: 0.0015964]
	Learning Rate: 0.00159635
	LOSS [training: 0.09822114937265715 | validation: 0.09659433133874597]
	TIME [epoch: 8.17 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10777344735312358		[learning rate: 0.0015945]
		[batch 20/20] avg loss: 0.09769830313081909		[learning rate: 0.0015926]
	Learning Rate: 0.00159258
	LOSS [training: 0.10273587524197132 | validation: 0.08835421367895957]
	TIME [epoch: 8.16 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0883425986149263		[learning rate: 0.0015907]
		[batch 20/20] avg loss: 0.10751457811636775		[learning rate: 0.0015888]
	Learning Rate: 0.00158883
	LOSS [training: 0.09792858836564701 | validation: 0.13357185152258888]
	TIME [epoch: 8.16 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11619574292155237		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.12861294985405153		[learning rate: 0.0015851]
	Learning Rate: 0.00158508
	LOSS [training: 0.12240434638780195 | validation: 0.11990412290221726]
	TIME [epoch: 8.19 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11303924600900997		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.12934668503588068		[learning rate: 0.0015813]
	Learning Rate: 0.00158134
	LOSS [training: 0.12119296552244534 | validation: 0.11311967511658128]
	TIME [epoch: 8.16 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12276739330857729		[learning rate: 0.0015795]
		[batch 20/20] avg loss: 0.1281256952845034		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.12544654429654034 | validation: 0.07091039619367444]
	TIME [epoch: 8.16 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08793868235762273		[learning rate: 0.0015757]
		[batch 20/20] avg loss: 0.1164863839707675		[learning rate: 0.0015739]
	Learning Rate: 0.00157389
	LOSS [training: 0.10221253316419512 | validation: 0.10419183704544153]
	TIME [epoch: 8.16 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10790265529141015		[learning rate: 0.001572]
		[batch 20/20] avg loss: 0.13037235507925676		[learning rate: 0.0015702]
	Learning Rate: 0.00157018
	LOSS [training: 0.11913750518533343 | validation: 0.07039817349835509]
	TIME [epoch: 8.19 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08823009099152804		[learning rate: 0.0015683]
		[batch 20/20] avg loss: 0.11786640709980216		[learning rate: 0.0015665]
	Learning Rate: 0.00156647
	LOSS [training: 0.10304824904566508 | validation: 0.08072569157814403]
	TIME [epoch: 8.17 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11748970596384478		[learning rate: 0.0015646]
		[batch 20/20] avg loss: 0.09246820859132636		[learning rate: 0.0015628]
	Learning Rate: 0.00156278
	LOSS [training: 0.10497895727758555 | validation: 0.11102725761112556]
	TIME [epoch: 8.16 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1095249383494626		[learning rate: 0.0015609]
		[batch 20/20] avg loss: 0.098401848444164		[learning rate: 0.0015591]
	Learning Rate: 0.00155909
	LOSS [training: 0.10396339339681329 | validation: 0.09477310020679411]
	TIME [epoch: 8.16 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10854329018123969		[learning rate: 0.0015573]
		[batch 20/20] avg loss: 0.12237076191357188		[learning rate: 0.0015554]
	Learning Rate: 0.00155541
	LOSS [training: 0.11545702604740579 | validation: 0.08992594962908397]
	TIME [epoch: 8.19 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1304145746009852		[learning rate: 0.0015536]
		[batch 20/20] avg loss: 0.11548340316191101		[learning rate: 0.0015517]
	Learning Rate: 0.00155175
	LOSS [training: 0.12294898888144815 | validation: 0.09584201011173243]
	TIME [epoch: 8.16 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1548916620050594		[learning rate: 0.0015499]
		[batch 20/20] avg loss: 0.11627768755683839		[learning rate: 0.0015481]
	Learning Rate: 0.00154809
	LOSS [training: 0.13558467478094888 | validation: 0.11988231602338206]
	TIME [epoch: 8.16 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11292792566973768		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.14708013435691886		[learning rate: 0.0015444]
	Learning Rate: 0.00154443
	LOSS [training: 0.13000403001332825 | validation: 0.12177157721721253]
	TIME [epoch: 8.17 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0963894458117206		[learning rate: 0.0015426]
		[batch 20/20] avg loss: 0.12160348635995626		[learning rate: 0.0015408]
	Learning Rate: 0.00154079
	LOSS [training: 0.10899646608583842 | validation: 0.12633501958424545]
	TIME [epoch: 8.19 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10456748152510735		[learning rate: 0.001539]
		[batch 20/20] avg loss: 0.10367180522546109		[learning rate: 0.0015372]
	Learning Rate: 0.00153716
	LOSS [training: 0.1041196433752842 | validation: 0.10717870787366615]
	TIME [epoch: 8.16 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1099288075114406		[learning rate: 0.0015353]
		[batch 20/20] avg loss: 0.13431931802240044		[learning rate: 0.0015335]
	Learning Rate: 0.00153353
	LOSS [training: 0.12212406276692053 | validation: 0.16454952882493445]
	TIME [epoch: 8.16 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13025909738111824		[learning rate: 0.0015317]
		[batch 20/20] avg loss: 0.11329166652591616		[learning rate: 0.0015299]
	Learning Rate: 0.00152991
	LOSS [training: 0.1217753819535172 | validation: 0.13898792050255823]
	TIME [epoch: 8.16 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11575218137565672		[learning rate: 0.0015281]
		[batch 20/20] avg loss: 0.11896930997506887		[learning rate: 0.0015263]
	Learning Rate: 0.0015263
	LOSS [training: 0.11736074567536278 | validation: 0.0899834498765041]
	TIME [epoch: 8.19 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1116402734010709		[learning rate: 0.0015245]
		[batch 20/20] avg loss: 0.10149510316381809		[learning rate: 0.0015227]
	Learning Rate: 0.0015227
	LOSS [training: 0.10656768828244448 | validation: 0.08972109022317508]
	TIME [epoch: 8.17 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10176561294742228		[learning rate: 0.0015209]
		[batch 20/20] avg loss: 0.11781481380672185		[learning rate: 0.0015191]
	Learning Rate: 0.00151911
	LOSS [training: 0.10979021337707207 | validation: 0.10691817047142183]
	TIME [epoch: 8.15 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11311618689800448		[learning rate: 0.0015173]
		[batch 20/20] avg loss: 0.11948577354669221		[learning rate: 0.0015155]
	Learning Rate: 0.00151553
	LOSS [training: 0.11630098022234833 | validation: 0.11724819073980741]
	TIME [epoch: 8.16 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11991300139558814		[learning rate: 0.0015137]
		[batch 20/20] avg loss: 0.10389663460161729		[learning rate: 0.001512]
	Learning Rate: 0.00151195
	LOSS [training: 0.1119048179986027 | validation: 0.10720871400455384]
	TIME [epoch: 8.19 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1065613968452402		[learning rate: 0.0015102]
		[batch 20/20] avg loss: 0.1324252449793811		[learning rate: 0.0015084]
	Learning Rate: 0.00150839
	LOSS [training: 0.11949332091231066 | validation: 0.18117943801613334]
	TIME [epoch: 8.16 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12944936362627818		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.1396832560921767		[learning rate: 0.0015048]
	Learning Rate: 0.00150483
	LOSS [training: 0.13456630985922743 | validation: 0.1245148603008597]
	TIME [epoch: 8.16 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11996446545411085		[learning rate: 0.0015031]
		[batch 20/20] avg loss: 0.13034111966409428		[learning rate: 0.0015013]
	Learning Rate: 0.00150128
	LOSS [training: 0.12515279255910255 | validation: 0.06540261841886787]
	TIME [epoch: 8.15 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.093283516270252		[learning rate: 0.0014995]
		[batch 20/20] avg loss: 0.1175209185316715		[learning rate: 0.0014977]
	Learning Rate: 0.00149774
	LOSS [training: 0.10540221740096176 | validation: 0.21910041225642596]
	TIME [epoch: 8.19 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12514169050274151		[learning rate: 0.001496]
		[batch 20/20] avg loss: 0.13616042645631327		[learning rate: 0.0014942]
	Learning Rate: 0.0014942
	LOSS [training: 0.13065105847952738 | validation: 0.10063371731057376]
	TIME [epoch: 8.17 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11113269667431391		[learning rate: 0.0014924]
		[batch 20/20] avg loss: 0.1297098765321443		[learning rate: 0.0014907]
	Learning Rate: 0.00149068
	LOSS [training: 0.12042128660322912 | validation: 0.157524702035751]
	TIME [epoch: 8.16 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11659164019812499		[learning rate: 0.0014889]
		[batch 20/20] avg loss: 0.14649304002618185		[learning rate: 0.0014872]
	Learning Rate: 0.00148716
	LOSS [training: 0.13154234011215338 | validation: 0.10033212459275045]
	TIME [epoch: 8.15 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11046952913825461		[learning rate: 0.0014854]
		[batch 20/20] avg loss: 0.10090075201652397		[learning rate: 0.0014837]
	Learning Rate: 0.00148366
	LOSS [training: 0.10568514057738929 | validation: 0.08961037516347312]
	TIME [epoch: 8.19 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11781200688870438		[learning rate: 0.0014819]
		[batch 20/20] avg loss: 0.11181076265074878		[learning rate: 0.0014802]
	Learning Rate: 0.00148016
	LOSS [training: 0.11481138476972659 | validation: 0.08213187801258508]
	TIME [epoch: 8.17 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10669743641070287		[learning rate: 0.0014784]
		[batch 20/20] avg loss: 0.1297625949428552		[learning rate: 0.0014767]
	Learning Rate: 0.00147667
	LOSS [training: 0.11823001567677902 | validation: 0.09551481436093771]
	TIME [epoch: 8.16 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12938984188273056		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.09228545425462777		[learning rate: 0.0014732]
	Learning Rate: 0.00147318
	LOSS [training: 0.11083764806867917 | validation: 0.0825458763435189]
	TIME [epoch: 8.16 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12035952113626977		[learning rate: 0.0014714]
		[batch 20/20] avg loss: 0.1169790645845008		[learning rate: 0.0014697]
	Learning Rate: 0.00146971
	LOSS [training: 0.11866929286038527 | validation: 0.10992502168315368]
	TIME [epoch: 8.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1221278270594692		[learning rate: 0.001468]
		[batch 20/20] avg loss: 0.12446707508786466		[learning rate: 0.0014662]
	Learning Rate: 0.00146624
	LOSS [training: 0.12329745107366692 | validation: 0.2157081254694776]
	TIME [epoch: 8.17 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12896803254464015		[learning rate: 0.0014645]
		[batch 20/20] avg loss: 0.14613092086505053		[learning rate: 0.0014628]
	Learning Rate: 0.00146278
	LOSS [training: 0.13754947670484535 | validation: 0.12085190732345935]
	TIME [epoch: 8.16 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1169671577663192		[learning rate: 0.0014611]
		[batch 20/20] avg loss: 0.11155558897452922		[learning rate: 0.0014593]
	Learning Rate: 0.00145933
	LOSS [training: 0.11426137337042419 | validation: 0.1034494687427295]
	TIME [epoch: 8.17 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1269233290349831		[learning rate: 0.0014576]
		[batch 20/20] avg loss: 0.11561874155668908		[learning rate: 0.0014559]
	Learning Rate: 0.00145589
	LOSS [training: 0.12127103529583608 | validation: 0.1329312740232351]
	TIME [epoch: 8.17 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12414524602195687		[learning rate: 0.0014542]
		[batch 20/20] avg loss: 0.11526523060450858		[learning rate: 0.0014525]
	Learning Rate: 0.00145245
	LOSS [training: 0.11970523831323272 | validation: 0.0976309489936143]
	TIME [epoch: 8.17 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11679016969849995		[learning rate: 0.0014507]
		[batch 20/20] avg loss: 0.14221085925177104		[learning rate: 0.001449]
	Learning Rate: 0.00144903
	LOSS [training: 0.1295005144751355 | validation: 0.07149536088329557]
	TIME [epoch: 8.16 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10607448200277611		[learning rate: 0.0014473]
		[batch 20/20] avg loss: 0.11722882310432574		[learning rate: 0.0014456]
	Learning Rate: 0.00144561
	LOSS [training: 0.11165165255355092 | validation: 0.2769005443664887]
	TIME [epoch: 8.16 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1353917973813051		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.12137353224172034		[learning rate: 0.0014422]
	Learning Rate: 0.0014422
	LOSS [training: 0.12838266481151273 | validation: 0.11957560002104926]
	TIME [epoch: 8.17 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10627956950327175		[learning rate: 0.0014405]
		[batch 20/20] avg loss: 0.10976966138541495		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.10802461544434336 | validation: 0.1141673683965829]
	TIME [epoch: 8.17 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10166646271960214		[learning rate: 0.0014371]
		[batch 20/20] avg loss: 0.10933458066254105		[learning rate: 0.0014354]
	Learning Rate: 0.0014354
	LOSS [training: 0.1055005216910716 | validation: 0.1087471219471543]
	TIME [epoch: 8.15 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10186424170434849		[learning rate: 0.0014337]
		[batch 20/20] avg loss: 0.1073451087649846		[learning rate: 0.001432]
	Learning Rate: 0.00143202
	LOSS [training: 0.10460467523466654 | validation: 0.09097944438669286]
	TIME [epoch: 8.15 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10940967819563785		[learning rate: 0.0014303]
		[batch 20/20] avg loss: 0.11550865364417577		[learning rate: 0.0014286]
	Learning Rate: 0.00142864
	LOSS [training: 0.11245916591990682 | validation: 0.09407309895456575]
	TIME [epoch: 8.17 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1039994468554692		[learning rate: 0.001427]
		[batch 20/20] avg loss: 0.11631862566160973		[learning rate: 0.0014253]
	Learning Rate: 0.00142527
	LOSS [training: 0.11015903625853944 | validation: 0.1488107208989392]
	TIME [epoch: 8.16 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11655490598507648		[learning rate: 0.0014236]
		[batch 20/20] avg loss: 0.12146344485561397		[learning rate: 0.0014219]
	Learning Rate: 0.00142191
	LOSS [training: 0.11900917542034524 | validation: 0.07749589818189564]
	TIME [epoch: 8.15 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10382636950163575		[learning rate: 0.0014202]
		[batch 20/20] avg loss: 0.10557679130819327		[learning rate: 0.0014186]
	Learning Rate: 0.00141855
	LOSS [training: 0.10470158040491451 | validation: 0.06319218650234883]
	TIME [epoch: 8.16 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09005363380926165		[learning rate: 0.0014169]
		[batch 20/20] avg loss: 0.09992667534490676		[learning rate: 0.0014152]
	Learning Rate: 0.00141521
	LOSS [training: 0.09499015457708423 | validation: 0.09681005003973508]
	TIME [epoch: 8.17 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10843479306902974		[learning rate: 0.0014135]
		[batch 20/20] avg loss: 0.09476266832496166		[learning rate: 0.0014119]
	Learning Rate: 0.00141187
	LOSS [training: 0.10159873069699572 | validation: 0.10271784222829416]
	TIME [epoch: 8.15 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10938530725113324		[learning rate: 0.0014102]
		[batch 20/20] avg loss: 0.14722227005369265		[learning rate: 0.0014085]
	Learning Rate: 0.00140854
	LOSS [training: 0.12830378865241296 | validation: 0.07866535286519673]
	TIME [epoch: 8.14 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09214152811787253		[learning rate: 0.0014069]
		[batch 20/20] avg loss: 0.11953160199073146		[learning rate: 0.0014052]
	Learning Rate: 0.00140522
	LOSS [training: 0.10583656505430199 | validation: 0.0661319024584544]
	TIME [epoch: 8.14 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10793543454894365		[learning rate: 0.0014036]
		[batch 20/20] avg loss: 0.12207010308330732		[learning rate: 0.0014019]
	Learning Rate: 0.0014019
	LOSS [training: 0.11500276881612552 | validation: 0.07629092961296173]
	TIME [epoch: 8.16 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11244604425119456		[learning rate: 0.0014002]
		[batch 20/20] avg loss: 0.11570757452461371		[learning rate: 0.0013986]
	Learning Rate: 0.0013986
	LOSS [training: 0.11407680938790417 | validation: 0.06287407095602933]
	TIME [epoch: 8.15 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11559701682011132		[learning rate: 0.0013969]
		[batch 20/20] avg loss: 0.09785097152135641		[learning rate: 0.0013953]
	Learning Rate: 0.0013953
	LOSS [training: 0.10672399417073386 | validation: 0.0836478510207054]
	TIME [epoch: 8.18 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15175658324265015		[learning rate: 0.0013937]
		[batch 20/20] avg loss: 0.11018339886858422		[learning rate: 0.001392]
	Learning Rate: 0.00139201
	LOSS [training: 0.13096999105561716 | validation: 0.15304571588094534]
	TIME [epoch: 8.19 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11062288390764996		[learning rate: 0.0013904]
		[batch 20/20] avg loss: 0.09716055776849356		[learning rate: 0.0013887]
	Learning Rate: 0.00138872
	LOSS [training: 0.10389172083807179 | validation: 0.11423921842505562]
	TIME [epoch: 8.18 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10910194425214201		[learning rate: 0.0013871]
		[batch 20/20] avg loss: 0.10058029232265946		[learning rate: 0.0013854]
	Learning Rate: 0.00138545
	LOSS [training: 0.10484111828740075 | validation: 0.06842523137380013]
	TIME [epoch: 8.19 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09364254862214408		[learning rate: 0.0013838]
		[batch 20/20] avg loss: 0.10490911288650709		[learning rate: 0.0013822]
	Learning Rate: 0.00138218
	LOSS [training: 0.0992758307543256 | validation: 0.11406111998027407]
	TIME [epoch: 8.18 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09886982467757874		[learning rate: 0.0013805]
		[batch 20/20] avg loss: 0.0939321715046407		[learning rate: 0.0013789]
	Learning Rate: 0.00137892
	LOSS [training: 0.09640099809110972 | validation: 0.0818539594704777]
	TIME [epoch: 8.16 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11810008332976149		[learning rate: 0.0013773]
		[batch 20/20] avg loss: 0.14007139238966054		[learning rate: 0.0013757]
	Learning Rate: 0.00137567
	LOSS [training: 0.129085737859711 | validation: 0.09832286718018912]
	TIME [epoch: 8.15 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10172206158690131		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.09288135340021224		[learning rate: 0.0013724]
	Learning Rate: 0.00137242
	LOSS [training: 0.09730170749355678 | validation: 0.10809865530376932]
	TIME [epoch: 8.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12014298693157817		[learning rate: 0.0013708]
		[batch 20/20] avg loss: 0.1054245345052957		[learning rate: 0.0013692]
	Learning Rate: 0.00136918
	LOSS [training: 0.11278376071843695 | validation: 0.1720644232819027]
	TIME [epoch: 8.18 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1336404771294497		[learning rate: 0.0013676]
		[batch 20/20] avg loss: 0.1045998792020725		[learning rate: 0.001366]
	Learning Rate: 0.00136595
	LOSS [training: 0.11912017816576108 | validation: 0.16665386256118656]
	TIME [epoch: 8.15 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1250990902131909		[learning rate: 0.0013643]
		[batch 20/20] avg loss: 0.11504454995078679		[learning rate: 0.0013627]
	Learning Rate: 0.00136273
	LOSS [training: 0.12007182008198887 | validation: 0.09431809521317613]
	TIME [epoch: 8.14 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09451749714371177		[learning rate: 0.0013611]
		[batch 20/20] avg loss: 0.09645186436581564		[learning rate: 0.0013595]
	Learning Rate: 0.00135952
	LOSS [training: 0.09548468075476371 | validation: 0.11988528879325576]
	TIME [epoch: 8.16 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13079323542580285		[learning rate: 0.0013579]
		[batch 20/20] avg loss: 0.09559606360225383		[learning rate: 0.0013563]
	Learning Rate: 0.00135631
	LOSS [training: 0.11319464951402831 | validation: 0.1202497133241118]
	TIME [epoch: 8.16 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13142305126592357		[learning rate: 0.0013547]
		[batch 20/20] avg loss: 0.10508807687884367		[learning rate: 0.0013531]
	Learning Rate: 0.00135311
	LOSS [training: 0.11825556407238365 | validation: 0.10177095405004812]
	TIME [epoch: 8.16 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11370939242584133		[learning rate: 0.0013515]
		[batch 20/20] avg loss: 0.11569609230490736		[learning rate: 0.0013499]
	Learning Rate: 0.00134992
	LOSS [training: 0.11470274236537434 | validation: 0.0542970779608621]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_898.pth
	Model improved!!!
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09244047194841058		[learning rate: 0.0013483]
		[batch 20/20] avg loss: 0.10498694554385217		[learning rate: 0.0013467]
	Learning Rate: 0.00134673
	LOSS [training: 0.09871370874613136 | validation: 0.10063378458238398]
	TIME [epoch: 8.17 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11802285846445622		[learning rate: 0.0013451]
		[batch 20/20] avg loss: 0.10180111218326245		[learning rate: 0.0013436]
	Learning Rate: 0.00134356
	LOSS [training: 0.10991198532385935 | validation: 0.10873101254473795]
	TIME [epoch: 8.15 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09950738996667437		[learning rate: 0.001342]
		[batch 20/20] avg loss: 0.1196850374474362		[learning rate: 0.0013404]
	Learning Rate: 0.00134039
	LOSS [training: 0.10959621370705526 | validation: 0.14180159480358012]
	TIME [epoch: 8.15 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11590907068317706		[learning rate: 0.0013388]
		[batch 20/20] avg loss: 0.12989304820966835		[learning rate: 0.0013372]
	Learning Rate: 0.00133723
	LOSS [training: 0.12290105944642271 | validation: 0.17185689515457236]
	TIME [epoch: 8.15 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09908079604847936		[learning rate: 0.0013356]
		[batch 20/20] avg loss: 0.0976474317412939		[learning rate: 0.0013341]
	Learning Rate: 0.00133407
	LOSS [training: 0.09836411389488663 | validation: 0.07989615752698187]
	TIME [epoch: 8.16 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09466539404348331		[learning rate: 0.0013325]
		[batch 20/20] avg loss: 0.09693170057069436		[learning rate: 0.0013309]
	Learning Rate: 0.00133093
	LOSS [training: 0.09579854730708882 | validation: 0.0717077333139739]
	TIME [epoch: 8.16 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10099613721514666		[learning rate: 0.0013294]
		[batch 20/20] avg loss: 0.09357433831107898		[learning rate: 0.0013278]
	Learning Rate: 0.00132779
	LOSS [training: 0.09728523776311282 | validation: 0.08483620889681089]
	TIME [epoch: 8.15 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09192214035196873		[learning rate: 0.0013262]
		[batch 20/20] avg loss: 0.0926631784202678		[learning rate: 0.0013247]
	Learning Rate: 0.00132465
	LOSS [training: 0.09229265938611826 | validation: 0.11717544409917711]
	TIME [epoch: 8.15 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09403465551864357		[learning rate: 0.0013231]
		[batch 20/20] avg loss: 0.09570580767973867		[learning rate: 0.0013215]
	Learning Rate: 0.00132153
	LOSS [training: 0.09487023159919114 | validation: 0.07350478571571964]
	TIME [epoch: 8.17 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10393233912009162		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.08474139070024164		[learning rate: 0.0013184]
	Learning Rate: 0.00131841
	LOSS [training: 0.09433686491016664 | validation: 0.10745612784337297]
	TIME [epoch: 8.14 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0896959847936948		[learning rate: 0.0013169]
		[batch 20/20] avg loss: 0.14259582051552738		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.11614590265461108 | validation: 0.14444216481471256]
	TIME [epoch: 8.14 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10984770379495876		[learning rate: 0.0013138]
		[batch 20/20] avg loss: 0.08393258147493521		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.09689014263494698 | validation: 0.060044603646052894]
	TIME [epoch: 8.14 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11269868738320513		[learning rate: 0.0013107]
		[batch 20/20] avg loss: 0.1032663619283001		[learning rate: 0.0013091]
	Learning Rate: 0.0013091
	LOSS [training: 0.10798252465575264 | validation: 0.07015227568113422]
	TIME [epoch: 8.17 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09668238121291135		[learning rate: 0.0013076]
		[batch 20/20] avg loss: 0.10608210852254454		[learning rate: 0.001306]
	Learning Rate: 0.00130602
	LOSS [training: 0.10138224486772796 | validation: 0.06609103168483782]
	TIME [epoch: 8.15 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09733652992019973		[learning rate: 0.0013045]
		[batch 20/20] avg loss: 0.09466567350083814		[learning rate: 0.0013029]
	Learning Rate: 0.00130294
	LOSS [training: 0.09600110171051893 | validation: 0.07119701179740262]
	TIME [epoch: 8.16 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08887515122861048		[learning rate: 0.0013014]
		[batch 20/20] avg loss: 0.08596853410797192		[learning rate: 0.0012999]
	Learning Rate: 0.00129986
	LOSS [training: 0.0874218426682912 | validation: 0.10418836319506394]
	TIME [epoch: 8.15 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12095322064180278		[learning rate: 0.0012983]
		[batch 20/20] avg loss: 0.09022731919055132		[learning rate: 0.0012968]
	Learning Rate: 0.0012968
	LOSS [training: 0.10559026991617707 | validation: 0.07896374056792571]
	TIME [epoch: 8.16 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09207307961990108		[learning rate: 0.0012953]
		[batch 20/20] avg loss: 0.10557095452759033		[learning rate: 0.0012937]
	Learning Rate: 0.00129374
	LOSS [training: 0.09882201707374569 | validation: 0.09245850233294883]
	TIME [epoch: 8.14 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11271178330861527		[learning rate: 0.0012922]
		[batch 20/20] avg loss: 0.09244979884255394		[learning rate: 0.0012907]
	Learning Rate: 0.00129069
	LOSS [training: 0.10258079107558463 | validation: 0.08925166681212299]
	TIME [epoch: 8.15 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0970128761525452		[learning rate: 0.0012892]
		[batch 20/20] avg loss: 0.09440979784241697		[learning rate: 0.0012876]
	Learning Rate: 0.00128764
	LOSS [training: 0.0957113369974811 | validation: 0.09302408865532157]
	TIME [epoch: 8.15 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09132086157253791		[learning rate: 0.0012861]
		[batch 20/20] avg loss: 0.12633808859180898		[learning rate: 0.0012846]
	Learning Rate: 0.0012846
	LOSS [training: 0.10882947508217344 | validation: 0.0889922671262868]
	TIME [epoch: 8.18 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10714838072520946		[learning rate: 0.0012831]
		[batch 20/20] avg loss: 0.09294714167438084		[learning rate: 0.0012816]
	Learning Rate: 0.00128157
	LOSS [training: 0.10004776119979517 | validation: 0.07250700407335622]
	TIME [epoch: 8.15 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10713331115473912		[learning rate: 0.0012801]
		[batch 20/20] avg loss: 0.10502897128015924		[learning rate: 0.0012786]
	Learning Rate: 0.00127855
	LOSS [training: 0.10608114121744919 | validation: 0.12358890962355282]
	TIME [epoch: 8.15 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12195685677020818		[learning rate: 0.001277]
		[batch 20/20] avg loss: 0.09789992464811718		[learning rate: 0.0012755]
	Learning Rate: 0.00127553
	LOSS [training: 0.10992839070916267 | validation: 0.08379683490592855]
	TIME [epoch: 8.16 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11349237676810257		[learning rate: 0.001274]
		[batch 20/20] avg loss: 0.11881424150101372		[learning rate: 0.0012725]
	Learning Rate: 0.00127253
	LOSS [training: 0.11615330913455815 | validation: 0.08043102695504074]
	TIME [epoch: 8.16 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11473759365897296		[learning rate: 0.001271]
		[batch 20/20] avg loss: 0.10144657165361184		[learning rate: 0.0012695]
	Learning Rate: 0.00126952
	LOSS [training: 0.10809208265629242 | validation: 0.06651658041124253]
	TIME [epoch: 8.15 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09412351697334498		[learning rate: 0.001268]
		[batch 20/20] avg loss: 0.11731288339133365		[learning rate: 0.0012665]
	Learning Rate: 0.00126653
	LOSS [training: 0.10571820018233932 | validation: 0.08069965124452783]
	TIME [epoch: 8.14 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08538257285746192		[learning rate: 0.001265]
		[batch 20/20] avg loss: 0.09252111881112819		[learning rate: 0.0012635]
	Learning Rate: 0.00126354
	LOSS [training: 0.08895184583429507 | validation: 0.07010062184578388]
	TIME [epoch: 8.14 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09099711053221961		[learning rate: 0.0012621]
		[batch 20/20] avg loss: 0.09370714378214631		[learning rate: 0.0012606]
	Learning Rate: 0.00126056
	LOSS [training: 0.09235212715718297 | validation: 0.08558235369673496]
	TIME [epoch: 8.18 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08412185534139348		[learning rate: 0.0012591]
		[batch 20/20] avg loss: 0.10306742911722167		[learning rate: 0.0012576]
	Learning Rate: 0.00125759
	LOSS [training: 0.09359464222930755 | validation: 0.10055816824150215]
	TIME [epoch: 8.15 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08345341714487511		[learning rate: 0.0012561]
		[batch 20/20] avg loss: 0.1108162939284792		[learning rate: 0.0012546]
	Learning Rate: 0.00125462
	LOSS [training: 0.09713485553667718 | validation: 0.11706126297606737]
	TIME [epoch: 8.15 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10250839389720112		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.10435873397961		[learning rate: 0.0012517]
	Learning Rate: 0.00125166
	LOSS [training: 0.10343356393840557 | validation: 0.14037480473134856]
	TIME [epoch: 8.14 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10502245951233438		[learning rate: 0.0012502]
		[batch 20/20] avg loss: 0.10421040172062432		[learning rate: 0.0012487]
	Learning Rate: 0.00124871
	LOSS [training: 0.10461643061647934 | validation: 0.10293996319993741]
	TIME [epoch: 8.16 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10340085774053662		[learning rate: 0.0012472]
		[batch 20/20] avg loss: 0.09648905537239275		[learning rate: 0.0012458]
	Learning Rate: 0.00124576
	LOSS [training: 0.09994495655646471 | validation: 0.09765105769147359]
	TIME [epoch: 8.15 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09622327062640666		[learning rate: 0.0012443]
		[batch 20/20] avg loss: 0.09280111652873142		[learning rate: 0.0012428]
	Learning Rate: 0.00124283
	LOSS [training: 0.09451219357756904 | validation: 0.11345653521449106]
	TIME [epoch: 8.15 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09588031841455122		[learning rate: 0.0012414]
		[batch 20/20] avg loss: 0.11055247100188355		[learning rate: 0.0012399]
	Learning Rate: 0.00123989
	LOSS [training: 0.10321639470821738 | validation: 0.11554143487754076]
	TIME [epoch: 8.14 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09599971795083219		[learning rate: 0.0012384]
		[batch 20/20] avg loss: 0.08853668799507315		[learning rate: 0.001237]
	Learning Rate: 0.00123697
	LOSS [training: 0.09226820297295266 | validation: 0.10089023187721702]
	TIME [epoch: 8.17 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08643070100694691		[learning rate: 0.0012355]
		[batch 20/20] avg loss: 0.09334786809252568		[learning rate: 0.0012341]
	Learning Rate: 0.00123405
	LOSS [training: 0.08988928454973633 | validation: 0.11901743586907805]
	TIME [epoch: 8.16 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10978362131355694		[learning rate: 0.0012326]
		[batch 20/20] avg loss: 0.13041488828515976		[learning rate: 0.0012311]
	Learning Rate: 0.00123114
	LOSS [training: 0.12009925479935835 | validation: 0.11447803614033802]
	TIME [epoch: 8.15 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09760614912982166		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.09104170911275118		[learning rate: 0.0012282]
	Learning Rate: 0.00122824
	LOSS [training: 0.09432392912128641 | validation: 0.057220721906317815]
	TIME [epoch: 8.15 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08992295818348364		[learning rate: 0.0012268]
		[batch 20/20] avg loss: 0.10750014634697044		[learning rate: 0.0012253]
	Learning Rate: 0.00122534
	LOSS [training: 0.09871155226522704 | validation: 0.18126238418680454]
	TIME [epoch: 8.16 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09373703058464697		[learning rate: 0.0012239]
		[batch 20/20] avg loss: 0.09471920459215825		[learning rate: 0.0012224]
	Learning Rate: 0.00122245
	LOSS [training: 0.09422811758840259 | validation: 0.07620976945235208]
	TIME [epoch: 8.15 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09347397758179839		[learning rate: 0.001221]
		[batch 20/20] avg loss: 0.11446898838193106		[learning rate: 0.0012196]
	Learning Rate: 0.00121957
	LOSS [training: 0.10397148298186472 | validation: 0.11652516461191126]
	TIME [epoch: 8.15 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09235749456032019		[learning rate: 0.0012181]
		[batch 20/20] avg loss: 0.10361610093545717		[learning rate: 0.0012167]
	Learning Rate: 0.00121669
	LOSS [training: 0.09798679774788868 | validation: 0.09205844716776002]
	TIME [epoch: 8.14 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08694901855792589		[learning rate: 0.0012153]
		[batch 20/20] avg loss: 0.09598454098925188		[learning rate: 0.0012138]
	Learning Rate: 0.00121382
	LOSS [training: 0.09146677977358889 | validation: 0.0996278991686757]
	TIME [epoch: 8.16 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09482637160835802		[learning rate: 0.0012124]
		[batch 20/20] avg loss: 0.10280110833485567		[learning rate: 0.001211]
	Learning Rate: 0.00121096
	LOSS [training: 0.09881373997160685 | validation: 0.0745644192142749]
	TIME [epoch: 8.15 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09242484843879266		[learning rate: 0.0012095]
		[batch 20/20] avg loss: 0.11969221491087305		[learning rate: 0.0012081]
	Learning Rate: 0.0012081
	LOSS [training: 0.10605853167483284 | validation: 0.1536549267234339]
	TIME [epoch: 8.14 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12079562342887906		[learning rate: 0.0012067]
		[batch 20/20] avg loss: 0.12740493105830436		[learning rate: 0.0012052]
	Learning Rate: 0.00120525
	LOSS [training: 0.12410027724359171 | validation: 0.08435013916241307]
	TIME [epoch: 8.14 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13142768986664344		[learning rate: 0.0012038]
		[batch 20/20] avg loss: 0.1147747033669079		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 0.12310119661677565 | validation: 0.06823009306392959]
	TIME [epoch: 8.14 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09522084314763386		[learning rate: 0.001201]
		[batch 20/20] avg loss: 0.07822491398208205		[learning rate: 0.0011996]
	Learning Rate: 0.00119957
	LOSS [training: 0.08672287856485796 | validation: 0.07674123178798001]
	TIME [epoch: 8.15 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08134786716234149		[learning rate: 0.0011982]
		[batch 20/20] avg loss: 0.08579991932247288		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.0835738932424072 | validation: 0.07216625771450189]
	TIME [epoch: 8.14 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1246410274491444		[learning rate: 0.0011953]
		[batch 20/20] avg loss: 0.13056790483195033		[learning rate: 0.0011939]
	Learning Rate: 0.00119392
	LOSS [training: 0.12760446614054738 | validation: 0.09038466804857627]
	TIME [epoch: 8.14 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877538065750492		[learning rate: 0.0011925]
		[batch 20/20] avg loss: 0.09884218831442791		[learning rate: 0.0011911]
	Learning Rate: 0.0011911
	LOSS [training: 0.10380878448596642 | validation: 0.10150600240646818]
	TIME [epoch: 8.14 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0878294944128997		[learning rate: 0.0011897]
		[batch 20/20] avg loss: 0.09906404992663606		[learning rate: 0.0011883]
	Learning Rate: 0.00118829
	LOSS [training: 0.09344677216976789 | validation: 0.09408803123344683]
	TIME [epoch: 8.16 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08287503886183534		[learning rate: 0.0011869]
		[batch 20/20] avg loss: 0.0848978497918981		[learning rate: 0.0011855]
	Learning Rate: 0.00118549
	LOSS [training: 0.0838864443268667 | validation: 0.06616093113373386]
	TIME [epoch: 8.15 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10693866673320152		[learning rate: 0.0011841]
		[batch 20/20] avg loss: 0.11129166024615778		[learning rate: 0.0011827]
	Learning Rate: 0.00118269
	LOSS [training: 0.10911516348967962 | validation: 0.07370304586629055]
	TIME [epoch: 8.15 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10545268904422325		[learning rate: 0.0011813]
		[batch 20/20] avg loss: 0.10629015507515968		[learning rate: 0.0011799]
	Learning Rate: 0.0011799
	LOSS [training: 0.10587142205969147 | validation: 0.08931687605055476]
	TIME [epoch: 8.16 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09057391297428685		[learning rate: 0.0011785]
		[batch 20/20] avg loss: 0.09478681805432473		[learning rate: 0.0011771]
	Learning Rate: 0.00117712
	LOSS [training: 0.0926803655143058 | validation: 0.08260612647496346]
	TIME [epoch: 8.18 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09978715296059502		[learning rate: 0.0011757]
		[batch 20/20] avg loss: 0.08501958118099313		[learning rate: 0.0011743]
	Learning Rate: 0.00117434
	LOSS [training: 0.09240336707079408 | validation: 0.0996591301434206]
	TIME [epoch: 8.14 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10280813143463792		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.10104142087580936		[learning rate: 0.0011716]
	Learning Rate: 0.00117157
	LOSS [training: 0.10192477615522363 | validation: 0.05737860427513911]
	TIME [epoch: 8.14 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07793625409638316		[learning rate: 0.0011702]
		[batch 20/20] avg loss: 0.07766214241133806		[learning rate: 0.0011688]
	Learning Rate: 0.00116881
	LOSS [training: 0.0777991982538606 | validation: 0.07783626559882911]
	TIME [epoch: 8.14 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08158805937844674		[learning rate: 0.0011674]
		[batch 20/20] avg loss: 0.08332955281582807		[learning rate: 0.0011661]
	Learning Rate: 0.00116605
	LOSS [training: 0.08245880609713739 | validation: 0.09663546970794085]
	TIME [epoch: 8.17 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08743650682138286		[learning rate: 0.0011647]
		[batch 20/20] avg loss: 0.07478090346819113		[learning rate: 0.0011633]
	Learning Rate: 0.0011633
	LOSS [training: 0.08110870514478699 | validation: 0.07134667299225209]
	TIME [epoch: 8.15 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09471032258827687		[learning rate: 0.0011619]
		[batch 20/20] avg loss: 0.10898711728665647		[learning rate: 0.0011606]
	Learning Rate: 0.00116056
	LOSS [training: 0.10184871993746669 | validation: 0.09098217584296098]
	TIME [epoch: 8.14 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10914145499368284		[learning rate: 0.0011592]
		[batch 20/20] avg loss: 0.08218880405549864		[learning rate: 0.0011578]
	Learning Rate: 0.00115782
	LOSS [training: 0.09566512952459073 | validation: 0.09388385875641661]
	TIME [epoch: 8.14 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0934582435490076		[learning rate: 0.0011565]
		[batch 20/20] avg loss: 0.09879235519077206		[learning rate: 0.0011551]
	Learning Rate: 0.00115509
	LOSS [training: 0.09612529936988982 | validation: 0.06636670910775092]
	TIME [epoch: 8.17 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09475293154042011		[learning rate: 0.0011537]
		[batch 20/20] avg loss: 0.10750019846131549		[learning rate: 0.0011524]
	Learning Rate: 0.00115236
	LOSS [training: 0.10112656500086781 | validation: 0.06493210521886535]
	TIME [epoch: 8.15 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09957773432141706		[learning rate: 0.001151]
		[batch 20/20] avg loss: 0.10319793359735716		[learning rate: 0.0011496]
	Learning Rate: 0.00114965
	LOSS [training: 0.10138783395938711 | validation: 0.08288901058436629]
	TIME [epoch: 8.16 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09379771299437621		[learning rate: 0.0011483]
		[batch 20/20] avg loss: 0.0972819257081089		[learning rate: 0.0011469]
	Learning Rate: 0.00114693
	LOSS [training: 0.09553981935124256 | validation: 0.08794597461705701]
	TIME [epoch: 8.15 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09574288443983034		[learning rate: 0.0011456]
		[batch 20/20] avg loss: 0.1055286833988742		[learning rate: 0.0011442]
	Learning Rate: 0.00114423
	LOSS [training: 0.10063578391935227 | validation: 0.09971636553799518]
	TIME [epoch: 8.18 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08622406758239763		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.10390175673738493		[learning rate: 0.0011415]
	Learning Rate: 0.00114153
	LOSS [training: 0.0950629121598913 | validation: 0.08084780050034321]
	TIME [epoch: 8.16 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09083427028809761		[learning rate: 0.0011402]
		[batch 20/20] avg loss: 0.11174837482071072		[learning rate: 0.0011388]
	Learning Rate: 0.00113884
	LOSS [training: 0.10129132255440416 | validation: 0.07006405128547404]
	TIME [epoch: 8.15 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08385346451724675		[learning rate: 0.0011375]
		[batch 20/20] avg loss: 0.15315365184639984		[learning rate: 0.0011362]
	Learning Rate: 0.00113615
	LOSS [training: 0.11850355818182327 | validation: 0.09376009663323405]
	TIME [epoch: 8.15 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11602450394422359		[learning rate: 0.0011348]
		[batch 20/20] avg loss: 0.09143590359783234		[learning rate: 0.0011335]
	Learning Rate: 0.00113347
	LOSS [training: 0.10373020377102796 | validation: 0.06569288568870796]
	TIME [epoch: 8.17 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09225756852552071		[learning rate: 0.0011321]
		[batch 20/20] avg loss: 0.08206883853243327		[learning rate: 0.0011308]
	Learning Rate: 0.0011308
	LOSS [training: 0.08716320352897697 | validation: 0.08829875455552916]
	TIME [epoch: 8.14 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08007681801014288		[learning rate: 0.0011295]
		[batch 20/20] avg loss: 0.09581133289351856		[learning rate: 0.0011281]
	Learning Rate: 0.00112813
	LOSS [training: 0.08794407545183074 | validation: 0.1220254121709571]
	TIME [epoch: 8.13 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10727108221872568		[learning rate: 0.0011268]
		[batch 20/20] avg loss: 0.08937791492912975		[learning rate: 0.0011255]
	Learning Rate: 0.00112547
	LOSS [training: 0.09832449857392773 | validation: 0.08474895806355473]
	TIME [epoch: 8.15 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0957291466015637		[learning rate: 0.0011241]
		[batch 20/20] avg loss: 0.0959515028856898		[learning rate: 0.0011228]
	Learning Rate: 0.00112281
	LOSS [training: 0.09584032474362676 | validation: 0.16696682926033563]
	TIME [epoch: 8.17 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11016830958190098		[learning rate: 0.0011215]
		[batch 20/20] avg loss: 0.07272542314897332		[learning rate: 0.0011202]
	Learning Rate: 0.00112017
	LOSS [training: 0.09144686636543715 | validation: 0.0771960558185813]
	TIME [epoch: 8.15 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08478471607900971		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.10581798600973927		[learning rate: 0.0011175]
	Learning Rate: 0.00111752
	LOSS [training: 0.09530135104437448 | validation: 0.09025667009981037]
	TIME [epoch: 8.15 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08934511267368932		[learning rate: 0.0011162]
		[batch 20/20] avg loss: 0.08095324695862485		[learning rate: 0.0011149]
	Learning Rate: 0.00111489
	LOSS [training: 0.08514917981615708 | validation: 0.07375833393294748]
	TIME [epoch: 8.15 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08002164196794448		[learning rate: 0.0011136]
		[batch 20/20] avg loss: 0.10443449582112022		[learning rate: 0.0011123]
	Learning Rate: 0.00111226
	LOSS [training: 0.09222806889453236 | validation: 0.08168902698287818]
	TIME [epoch: 8.18 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1342751557008056		[learning rate: 0.0011109]
		[batch 20/20] avg loss: 0.0995276985861753		[learning rate: 0.0011096]
	Learning Rate: 0.00110963
	LOSS [training: 0.11690142714349047 | validation: 0.09429882309510021]
	TIME [epoch: 8.15 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1240762101921478		[learning rate: 0.0011083]
		[batch 20/20] avg loss: 0.0862100137297619		[learning rate: 0.001107]
	Learning Rate: 0.00110702
	LOSS [training: 0.10514311196095485 | validation: 0.07043853677404313]
	TIME [epoch: 8.15 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08098444129021382		[learning rate: 0.0011057]
		[batch 20/20] avg loss: 0.1049483252725932		[learning rate: 0.0011044]
	Learning Rate: 0.0011044
	LOSS [training: 0.0929663832814035 | validation: 0.11382845948693214]
	TIME [epoch: 8.15 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10220613505412132		[learning rate: 0.0011031]
		[batch 20/20] avg loss: 0.09064469213217652		[learning rate: 0.0011018]
	Learning Rate: 0.0011018
	LOSS [training: 0.09642541359314889 | validation: 0.07899536237580747]
	TIME [epoch: 8.17 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07383914192244427		[learning rate: 0.0011005]
		[batch 20/20] avg loss: 0.09348619718093279		[learning rate: 0.0010992]
	Learning Rate: 0.0010992
	LOSS [training: 0.08366266955168854 | validation: 0.08582788776481792]
	TIME [epoch: 8.15 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08437102360876925		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.10874477594911096		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.0965578997789401 | validation: 0.08151665375182596]
	TIME [epoch: 8.15 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10553317593792697		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.1345943482506294		[learning rate: 0.001094]
	Learning Rate: 0.00109402
	LOSS [training: 0.12006376209427819 | validation: 0.0678228507865952]
	TIME [epoch: 8.15 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09137462696262141		[learning rate: 0.0010927]
		[batch 20/20] avg loss: 0.10938008560964221		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.1003773562861318 | validation: 0.08471320406276159]
	TIME [epoch: 8.18 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10527748663914349		[learning rate: 0.0010902]
		[batch 20/20] avg loss: 0.06688229368063318		[learning rate: 0.0010889]
	Learning Rate: 0.00108887
	LOSS [training: 0.08607989015988833 | validation: 0.07924785756894236]
	TIME [epoch: 8.16 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10118396937620722		[learning rate: 0.0010876]
		[batch 20/20] avg loss: 0.10213157325208613		[learning rate: 0.0010863]
	Learning Rate: 0.0010863
	LOSS [training: 0.10165777131414669 | validation: 0.08489695331950543]
	TIME [epoch: 8.15 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09735350598918266		[learning rate: 0.001085]
		[batch 20/20] avg loss: 0.09607452208050472		[learning rate: 0.0010837]
	Learning Rate: 0.00108373
	LOSS [training: 0.09671401403484367 | validation: 0.05980214666091653]
	TIME [epoch: 8.15 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07913722101416577		[learning rate: 0.0010825]
		[batch 20/20] avg loss: 0.09729405873142687		[learning rate: 0.0010812]
	Learning Rate: 0.00108118
	LOSS [training: 0.08821563987279632 | validation: 0.0667575664946064]
	TIME [epoch: 8.17 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1061815498852613		[learning rate: 0.0010799]
		[batch 20/20] avg loss: 0.08749416609079017		[learning rate: 0.0010786]
	Learning Rate: 0.00107863
	LOSS [training: 0.09683785798802572 | validation: 0.11052028909736616]
	TIME [epoch: 8.16 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08530525928414005		[learning rate: 0.0010774]
		[batch 20/20] avg loss: 0.12389703009053266		[learning rate: 0.0010761]
	Learning Rate: 0.00107608
	LOSS [training: 0.10460114468733636 | validation: 0.09502644861022563]
	TIME [epoch: 8.16 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0927278064317842		[learning rate: 0.0010748]
		[batch 20/20] avg loss: 0.11276464900772645		[learning rate: 0.0010735]
	Learning Rate: 0.00107355
	LOSS [training: 0.1027462277197553 | validation: 0.1257725038545339]
	TIME [epoch: 8.14 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09802881282979702		[learning rate: 0.0010723]
		[batch 20/20] avg loss: 0.09249695721619597		[learning rate: 0.001071]
	Learning Rate: 0.00107101
	LOSS [training: 0.09526288502299649 | validation: 0.06644427061466551]
	TIME [epoch: 8.16 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07541891642569568		[learning rate: 0.0010697]
		[batch 20/20] avg loss: 0.07890629426005476		[learning rate: 0.0010685]
	Learning Rate: 0.00106849
	LOSS [training: 0.07716260534287522 | validation: 0.10400652564446664]
	TIME [epoch: 8.15 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11267965392245274		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.08028989539931639		[learning rate: 0.001066]
	Learning Rate: 0.00106597
	LOSS [training: 0.09648477466088458 | validation: 0.06621214339177002]
	TIME [epoch: 8.14 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08852578076253162		[learning rate: 0.0010647]
		[batch 20/20] avg loss: 0.10071459707610701		[learning rate: 0.0010635]
	Learning Rate: 0.00106345
	LOSS [training: 0.09462018891931931 | validation: 0.07571059230392323]
	TIME [epoch: 8.14 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08104815930754683		[learning rate: 0.0010622]
		[batch 20/20] avg loss: 0.09757817930718979		[learning rate: 0.0010609]
	Learning Rate: 0.00106094
	LOSS [training: 0.08931316930736832 | validation: 0.08704597118331368]
	TIME [epoch: 8.17 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10052405561676195		[learning rate: 0.0010597]
		[batch 20/20] avg loss: 0.10489959887638452		[learning rate: 0.0010584]
	Learning Rate: 0.00105844
	LOSS [training: 0.10271182724657324 | validation: 0.12187308357608363]
	TIME [epoch: 8.16 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10692989833584339		[learning rate: 0.0010572]
		[batch 20/20] avg loss: 0.12292741061434946		[learning rate: 0.0010559]
	Learning Rate: 0.00105594
	LOSS [training: 0.11492865447509644 | validation: 0.09468342596486783]
	TIME [epoch: 8.15 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0961106138786691		[learning rate: 0.0010547]
		[batch 20/20] avg loss: 0.10785948594819854		[learning rate: 0.0010535]
	Learning Rate: 0.00105345
	LOSS [training: 0.10198504991343382 | validation: 0.0821823033512823]
	TIME [epoch: 8.16 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09654414690771293		[learning rate: 0.0010522]
		[batch 20/20] avg loss: 0.10690167274990328		[learning rate: 0.001051]
	Learning Rate: 0.00105097
	LOSS [training: 0.10172290982880812 | validation: 0.12829688262353808]
	TIME [epoch: 8.17 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11254175000769957		[learning rate: 0.0010497]
		[batch 20/20] avg loss: 0.09684309458059348		[learning rate: 0.0010485]
	Learning Rate: 0.00104849
	LOSS [training: 0.10469242229414652 | validation: 0.10596172684616469]
	TIME [epoch: 8.16 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11453467043746834		[learning rate: 0.0010473]
		[batch 20/20] avg loss: 0.09251143357427594		[learning rate: 0.001046]
	Learning Rate: 0.00104602
	LOSS [training: 0.10352305200587213 | validation: 0.09094192760177419]
	TIME [epoch: 8.16 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08267291164178635		[learning rate: 0.0010448]
		[batch 20/20] avg loss: 0.08630509613941441		[learning rate: 0.0010435]
	Learning Rate: 0.00104355
	LOSS [training: 0.08448900389060039 | validation: 0.07273943302096547]
	TIME [epoch: 8.16 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08263576998869053		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.09152265366580667		[learning rate: 0.0010411]
	Learning Rate: 0.00104109
	LOSS [training: 0.08707921182724862 | validation: 0.06262895771293008]
	TIME [epoch: 8.16 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09109296570978312		[learning rate: 0.0010399]
		[batch 20/20] avg loss: 0.07303856687432053		[learning rate: 0.0010386]
	Learning Rate: 0.00103863
	LOSS [training: 0.08206576629205183 | validation: 0.08648486702729816]
	TIME [epoch: 8.16 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1013846784017655		[learning rate: 0.0010374]
		[batch 20/20] avg loss: 0.09988684503911947		[learning rate: 0.0010362]
	Learning Rate: 0.00103618
	LOSS [training: 0.1006357617204425 | validation: 0.07310113991254104]
	TIME [epoch: 8.14 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07468850117160239		[learning rate: 0.001035]
		[batch 20/20] avg loss: 0.10807306246197328		[learning rate: 0.0010337]
	Learning Rate: 0.00103374
	LOSS [training: 0.09138078181678784 | validation: 0.10824461707763149]
	TIME [epoch: 8.15 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12592023182939863		[learning rate: 0.0010325]
		[batch 20/20] avg loss: 0.09777418465770209		[learning rate: 0.0010313]
	Learning Rate: 0.0010313
	LOSS [training: 0.11184720824355036 | validation: 0.09557737113789772]
	TIME [epoch: 8.16 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09954663816116735		[learning rate: 0.0010301]
		[batch 20/20] avg loss: 0.08447799883571426		[learning rate: 0.0010289]
	Learning Rate: 0.00102887
	LOSS [training: 0.0920123184984408 | validation: 0.07296975754987256]
	TIME [epoch: 8.16 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08050352833103061		[learning rate: 0.0010277]
		[batch 20/20] avg loss: 0.10368578148437076		[learning rate: 0.0010264]
	Learning Rate: 0.00102644
	LOSS [training: 0.09209465490770068 | validation: 0.10455514088100554]
	TIME [epoch: 8.15 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11620382335429813		[learning rate: 0.0010252]
		[batch 20/20] avg loss: 0.10225721312409226		[learning rate: 0.001024]
	Learning Rate: 0.00102402
	LOSS [training: 0.1092305182391952 | validation: 0.06861762468487614]
	TIME [epoch: 8.15 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08272410693108455		[learning rate: 0.0010228]
		[batch 20/20] avg loss: 0.09363598226426818		[learning rate: 0.0010216]
	Learning Rate: 0.0010216
	LOSS [training: 0.08818004459767637 | validation: 0.0795544662944854]
	TIME [epoch: 8.15 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08430465438575714		[learning rate: 0.0010204]
		[batch 20/20] avg loss: 0.09256211301882335		[learning rate: 0.0010192]
	Learning Rate: 0.00101919
	LOSS [training: 0.08843338370229024 | validation: 0.08039487625033723]
	TIME [epoch: 8.17 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09151302383916335		[learning rate: 0.001018]
		[batch 20/20] avg loss: 0.08905694211240889		[learning rate: 0.0010168]
	Learning Rate: 0.00101679
	LOSS [training: 0.0902849829757861 | validation: 0.06952877791792388]
	TIME [epoch: 8.15 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08584786659092354		[learning rate: 0.0010156]
		[batch 20/20] avg loss: 0.07925701900355785		[learning rate: 0.0010144]
	Learning Rate: 0.00101439
	LOSS [training: 0.0825524427972407 | validation: 0.0614806215314468]
	TIME [epoch: 8.16 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.092164341642205		[learning rate: 0.0010132]
		[batch 20/20] avg loss: 0.09369546303690468		[learning rate: 0.001012]
	Learning Rate: 0.001012
	LOSS [training: 0.09292990233955484 | validation: 0.07474957171512267]
	TIME [epoch: 8.14 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10683744152185093		[learning rate: 0.0010108]
		[batch 20/20] avg loss: 0.06961586971937007		[learning rate: 0.0010096]
	Learning Rate: 0.00100961
	LOSS [training: 0.08822665562061048 | validation: 0.07280170500387229]
	TIME [epoch: 8.17 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12686809104978428		[learning rate: 0.0010084]
		[batch 20/20] avg loss: 0.09676138382265415		[learning rate: 0.0010072]
	Learning Rate: 0.00100723
	LOSS [training: 0.11181473743621924 | validation: 0.06947860764305316]
	TIME [epoch: 8.15 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10314325956127217		[learning rate: 0.001006]
		[batch 20/20] avg loss: 0.11851510175516478		[learning rate: 0.0010049]
	Learning Rate: 0.00100485
	LOSS [training: 0.11082918065821849 | validation: 0.10914025364278494]
	TIME [epoch: 8.14 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08407877905995913		[learning rate: 0.0010037]
		[batch 20/20] avg loss: 0.08190884651802506		[learning rate: 0.0010025]
	Learning Rate: 0.00100248
	LOSS [training: 0.0829938127889921 | validation: 0.11887609989192119]
	TIME [epoch: 8.15 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10721411088732062		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.08151317259363365		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.09436364174047715 | validation: 0.07371826886559978]
	TIME [epoch: 8.17 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07826674214478506		[learning rate: 0.00099894]
		[batch 20/20] avg loss: 0.07578806049294685		[learning rate: 0.00099776]
	Learning Rate: 0.000997759
	LOSS [training: 0.07702740131886596 | validation: 0.0636269452135982]
	TIME [epoch: 8.14 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0734780477417711		[learning rate: 0.00099658]
		[batch 20/20] avg loss: 0.08102481109826469		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.07725142942001788 | validation: 0.08293914950048963]
	TIME [epoch: 8.14 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09045362707548561		[learning rate: 0.00099423]
		[batch 20/20] avg loss: 0.08269065210810028		[learning rate: 0.00099306]
	Learning Rate: 0.000993057
	LOSS [training: 0.08657213959179297 | validation: 0.06831273535132143]
	TIME [epoch: 8.14 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10893405977741717		[learning rate: 0.00099189]
		[batch 20/20] avg loss: 0.1046487640271209		[learning rate: 0.00099071]
	Learning Rate: 0.000990715
	LOSS [training: 0.10679141190226904 | validation: 0.09189250217292756]
	TIME [epoch: 8.17 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09549500063911379		[learning rate: 0.00098955]
		[batch 20/20] avg loss: 0.09106156159486414		[learning rate: 0.00098838]
	Learning Rate: 0.000988378
	LOSS [training: 0.09327828111698896 | validation: 0.1060428164381293]
	TIME [epoch: 8.15 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07633318391104087		[learning rate: 0.00098721]
		[batch 20/20] avg loss: 0.08954798463156038		[learning rate: 0.00098605]
	Learning Rate: 0.000986047
	LOSS [training: 0.08294058427130063 | validation: 0.08208529021870707]
	TIME [epoch: 8.14 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0716745335232913		[learning rate: 0.00098488]
		[batch 20/20] avg loss: 0.0907809507522577		[learning rate: 0.00098372]
	Learning Rate: 0.000983721
	LOSS [training: 0.08122774213777452 | validation: 0.061408785358200914]
	TIME [epoch: 8.16 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10171893733126791		[learning rate: 0.00098256]
		[batch 20/20] avg loss: 0.09470441104045987		[learning rate: 0.0009814]
	Learning Rate: 0.0009814
	LOSS [training: 0.09821167418586389 | validation: 0.09855543327069624]
	TIME [epoch: 8.16 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09080813443264604		[learning rate: 0.00098024]
		[batch 20/20] avg loss: 0.07989306954614131		[learning rate: 0.00097909]
	Learning Rate: 0.000979085
	LOSS [training: 0.08535060198939366 | validation: 0.07308091659612265]
	TIME [epoch: 8.16 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09037824518892976		[learning rate: 0.00097793]
		[batch 20/20] avg loss: 0.08368224680917377		[learning rate: 0.00097678]
	Learning Rate: 0.000976776
	LOSS [training: 0.08703024599905175 | validation: 0.06844956518142642]
	TIME [epoch: 8.15 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09227430470304718		[learning rate: 0.00097562]
		[batch 20/20] avg loss: 0.07865695948554499		[learning rate: 0.00097447]
	Learning Rate: 0.000974472
	LOSS [training: 0.0854656320942961 | validation: 0.07188446667282278]
	TIME [epoch: 8.14 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07199024522323128		[learning rate: 0.00097332]
		[batch 20/20] avg loss: 0.09124272129844307		[learning rate: 0.00097217]
	Learning Rate: 0.000972173
	LOSS [training: 0.08161648326083717 | validation: 0.10812249768329099]
	TIME [epoch: 8.18 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10675422445792368		[learning rate: 0.00097103]
		[batch 20/20] avg loss: 0.07894341522764488		[learning rate: 0.00096988]
	Learning Rate: 0.00096988
	LOSS [training: 0.09284881984278429 | validation: 0.0668525869443894]
	TIME [epoch: 8.16 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09551339253531174		[learning rate: 0.00096874]
		[batch 20/20] avg loss: 0.07867695412000295		[learning rate: 0.00096759]
	Learning Rate: 0.000967592
	LOSS [training: 0.08709517332765734 | validation: 0.05478089665910952]
	TIME [epoch: 8.16 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0798048454111141		[learning rate: 0.00096645]
		[batch 20/20] avg loss: 0.09241852144955708		[learning rate: 0.00096531]
	Learning Rate: 0.00096531
	LOSS [training: 0.0861116834303356 | validation: 0.05049012640277541]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1040.pth
	Model improved!!!
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08319356320653924		[learning rate: 0.00096417]
		[batch 20/20] avg loss: 0.09243042198929037		[learning rate: 0.00096303]
	Learning Rate: 0.000963033
	LOSS [training: 0.08781199259791482 | validation: 0.06866672684094126]
	TIME [epoch: 8.17 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08066505173989533		[learning rate: 0.0009619]
		[batch 20/20] avg loss: 0.09126696863084499		[learning rate: 0.00096076]
	Learning Rate: 0.000960761
	LOSS [training: 0.08596601018537015 | validation: 0.07465466473671889]
	TIME [epoch: 8.15 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07785898442935232		[learning rate: 0.00095963]
		[batch 20/20] avg loss: 0.09055109503211142		[learning rate: 0.00095849]
	Learning Rate: 0.000958495
	LOSS [training: 0.08420503973073187 | validation: 0.07407637587117069]
	TIME [epoch: 8.15 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15170650921010118		[learning rate: 0.00095736]
		[batch 20/20] avg loss: 0.1295681449158431		[learning rate: 0.00095623]
	Learning Rate: 0.000956234
	LOSS [training: 0.1406373270629721 | validation: 0.0808532897541868]
	TIME [epoch: 8.14 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08275873562093146		[learning rate: 0.00095511]
		[batch 20/20] avg loss: 0.09912042659556877		[learning rate: 0.00095398]
	Learning Rate: 0.000953978
	LOSS [training: 0.09093958110825012 | validation: 0.07055451131372928]
	TIME [epoch: 8.17 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10293122872563398		[learning rate: 0.00095285]
		[batch 20/20] avg loss: 0.09729155665210254		[learning rate: 0.00095173]
	Learning Rate: 0.000951728
	LOSS [training: 0.10011139268886826 | validation: 0.11052509980908462]
	TIME [epoch: 8.15 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08310325144263073		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.09403662237621076		[learning rate: 0.00094948]
	Learning Rate: 0.000949483
	LOSS [training: 0.08856993690942075 | validation: 0.06633725607891368]
	TIME [epoch: 8.15 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07346584442278023		[learning rate: 0.00094836]
		[batch 20/20] avg loss: 0.07592133508048879		[learning rate: 0.00094724]
	Learning Rate: 0.000947243
	LOSS [training: 0.07469358975163451 | validation: 0.06302632449179019]
	TIME [epoch: 8.15 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08188997648734764		[learning rate: 0.00094613]
		[batch 20/20] avg loss: 0.10072737523231742		[learning rate: 0.00094501]
	Learning Rate: 0.000945009
	LOSS [training: 0.09130867585983252 | validation: 0.09826562921528391]
	TIME [epoch: 8.18 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0971276901739572		[learning rate: 0.00094389]
		[batch 20/20] avg loss: 0.0746268094060744		[learning rate: 0.00094278]
	Learning Rate: 0.00094278
	LOSS [training: 0.0858772497900158 | validation: 0.08076476364097768]
	TIME [epoch: 8.15 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10313018564651275		[learning rate: 0.00094167]
		[batch 20/20] avg loss: 0.07933292429703007		[learning rate: 0.00094056]
	Learning Rate: 0.000940556
	LOSS [training: 0.09123155497177142 | validation: 0.06886671112960288]
	TIME [epoch: 8.15 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10131812883539841		[learning rate: 0.00093945]
		[batch 20/20] avg loss: 0.07439672801571526		[learning rate: 0.00093834]
	Learning Rate: 0.000938337
	LOSS [training: 0.08785742842555684 | validation: 0.0816023241409786]
	TIME [epoch: 8.14 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08316429642940708		[learning rate: 0.00093723]
		[batch 20/20] avg loss: 0.10003590299107183		[learning rate: 0.00093612]
	Learning Rate: 0.000936124
	LOSS [training: 0.09160009971023945 | validation: 0.08408289472345899]
	TIME [epoch: 8.17 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09498636710016704		[learning rate: 0.00093502]
		[batch 20/20] avg loss: 0.0813435171838633		[learning rate: 0.00093392]
	Learning Rate: 0.000933916
	LOSS [training: 0.08816494214201519 | validation: 0.0661942218347964]
	TIME [epoch: 8.16 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08439671931642587		[learning rate: 0.00093281]
		[batch 20/20] avg loss: 0.08680112631212145		[learning rate: 0.00093171]
	Learning Rate: 0.000931713
	LOSS [training: 0.08559892281427364 | validation: 0.07061428429469233]
	TIME [epoch: 8.14 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10014903198856957		[learning rate: 0.00093061]
		[batch 20/20] avg loss: 0.08959636768778478		[learning rate: 0.00092951]
	Learning Rate: 0.000929515
	LOSS [training: 0.09487269983817717 | validation: 0.07340396705928978]
	TIME [epoch: 8.15 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09939511834894535		[learning rate: 0.00092842]
		[batch 20/20] avg loss: 0.0915835419216686		[learning rate: 0.00092732]
	Learning Rate: 0.000927322
	LOSS [training: 0.09548933013530697 | validation: 0.08295839130727761]
	TIME [epoch: 8.15 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.095706412130245		[learning rate: 0.00092623]
		[batch 20/20] avg loss: 0.10056210792263916		[learning rate: 0.00092513]
	Learning Rate: 0.000925135
	LOSS [training: 0.09813426002644207 | validation: 0.07277532348153772]
	TIME [epoch: 8.15 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09145232524077776		[learning rate: 0.00092404]
		[batch 20/20] avg loss: 0.08470020217699363		[learning rate: 0.00092295]
	Learning Rate: 0.000922953
	LOSS [training: 0.0880762637088857 | validation: 0.06033118093686625]
	TIME [epoch: 8.15 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08393590290793798		[learning rate: 0.00092186]
		[batch 20/20] avg loss: 0.08143844435377692		[learning rate: 0.00092078]
	Learning Rate: 0.000920776
	LOSS [training: 0.08268717363085747 | validation: 0.05593600567309692]
	TIME [epoch: 8.15 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07222206455379901		[learning rate: 0.00091969]
		[batch 20/20] avg loss: 0.09720138333904925		[learning rate: 0.0009186]
	Learning Rate: 0.000918604
	LOSS [training: 0.08471172394642415 | validation: 0.05716888100401946]
	TIME [epoch: 8.16 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09594329084073597		[learning rate: 0.00091752]
		[batch 20/20] avg loss: 0.09312933403138306		[learning rate: 0.00091644]
	Learning Rate: 0.000916437
	LOSS [training: 0.0945363124360595 | validation: 0.07169468829110137]
	TIME [epoch: 8.16 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08401811054479506		[learning rate: 0.00091536]
		[batch 20/20] avg loss: 0.08944939303306436		[learning rate: 0.00091428]
	Learning Rate: 0.000914275
	LOSS [training: 0.08673375178892971 | validation: 0.05805661223539278]
	TIME [epoch: 8.15 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07492783786243648		[learning rate: 0.0009132]
		[batch 20/20] avg loss: 0.08886760167440785		[learning rate: 0.00091212]
	Learning Rate: 0.000912119
	LOSS [training: 0.08189771976842218 | validation: 0.09365114326460448]
	TIME [epoch: 8.15 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.115844432742588		[learning rate: 0.00091104]
		[batch 20/20] avg loss: 0.10035250251451815		[learning rate: 0.00090997]
	Learning Rate: 0.000909967
	LOSS [training: 0.10809846762855309 | validation: 0.10158608394765255]
	TIME [epoch: 8.16 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09243229693499419		[learning rate: 0.00090889]
		[batch 20/20] avg loss: 0.09167815675191233		[learning rate: 0.00090782]
	Learning Rate: 0.00090782
	LOSS [training: 0.09205522684345327 | validation: 0.08620714211932753]
	TIME [epoch: 8.15 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10001801725787159		[learning rate: 0.00090675]
		[batch 20/20] avg loss: 0.09193402671434481		[learning rate: 0.00090568]
	Learning Rate: 0.000905679
	LOSS [training: 0.09597602198610819 | validation: 0.05741240091861183]
	TIME [epoch: 8.15 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07146561845739549		[learning rate: 0.00090461]
		[batch 20/20] avg loss: 0.08274487827261437		[learning rate: 0.00090354]
	Learning Rate: 0.000903543
	LOSS [training: 0.0771052483650049 | validation: 0.07483428336489595]
	TIME [epoch: 8.15 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08728619000452435		[learning rate: 0.00090248]
		[batch 20/20] avg loss: 0.08468210927464033		[learning rate: 0.00090141]
	Learning Rate: 0.000901411
	LOSS [training: 0.08598414963958234 | validation: 0.08843002014859899]
	TIME [epoch: 8.17 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09816182496898686		[learning rate: 0.00090035]
		[batch 20/20] avg loss: 0.08504838038006077		[learning rate: 0.00089929]
	Learning Rate: 0.000899285
	LOSS [training: 0.0916051026745238 | validation: 0.06445794829270243]
	TIME [epoch: 8.16 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08056902299506277		[learning rate: 0.00089822]
		[batch 20/20] avg loss: 0.08669201561358564		[learning rate: 0.00089716]
	Learning Rate: 0.000897164
	LOSS [training: 0.0836305193043242 | validation: 0.09598694807954565]
	TIME [epoch: 8.15 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10939973344800598		[learning rate: 0.00089611]
		[batch 20/20] avg loss: 0.091759626039153		[learning rate: 0.00089505]
	Learning Rate: 0.000895048
	LOSS [training: 0.10057967974357948 | validation: 0.06865996025655598]
	TIME [epoch: 8.15 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09415970389710245		[learning rate: 0.00089399]
		[batch 20/20] avg loss: 0.08950892143863189		[learning rate: 0.00089294]
	Learning Rate: 0.000892936
	LOSS [training: 0.09183431266786715 | validation: 0.08081694669766357]
	TIME [epoch: 8.15 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09856032668187181		[learning rate: 0.00089188]
		[batch 20/20] avg loss: 0.0846528035612052		[learning rate: 0.00089083]
	Learning Rate: 0.00089083
	LOSS [training: 0.0916065651215385 | validation: 0.04864519962957638]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1074.pth
	Model improved!!!
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07682679214458844		[learning rate: 0.00088978]
		[batch 20/20] avg loss: 0.06900761262883058		[learning rate: 0.00088873]
	Learning Rate: 0.000888729
	LOSS [training: 0.07291720238670951 | validation: 0.07794282868650446]
	TIME [epoch: 8.16 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09278100190420165		[learning rate: 0.00088768]
		[batch 20/20] avg loss: 0.0757613143399959		[learning rate: 0.00088663]
	Learning Rate: 0.000886632
	LOSS [training: 0.08427115812209876 | validation: 0.07184241481551362]
	TIME [epoch: 8.15 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0862149904425491		[learning rate: 0.00088559]
		[batch 20/20] avg loss: 0.07032251502204788		[learning rate: 0.00088454]
	Learning Rate: 0.000884541
	LOSS [training: 0.0782687527322985 | validation: 0.061729718819287266]
	TIME [epoch: 8.16 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0825799383436975		[learning rate: 0.0008835]
		[batch 20/20] avg loss: 0.07558290696967361		[learning rate: 0.00088245]
	Learning Rate: 0.000882454
	LOSS [training: 0.07908142265668555 | validation: 0.06461927821294124]
	TIME [epoch: 8.16 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07604806288540907		[learning rate: 0.00088141]
		[batch 20/20] avg loss: 0.07855332380476186		[learning rate: 0.00088037]
	Learning Rate: 0.000880373
	LOSS [training: 0.07730069334508546 | validation: 0.07341951651554754]
	TIME [epoch: 8.15 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08236562879805208		[learning rate: 0.00087933]
		[batch 20/20] avg loss: 0.07479103705349402		[learning rate: 0.0008783]
	Learning Rate: 0.000878296
	LOSS [training: 0.07857833292577307 | validation: 0.08219545238199467]
	TIME [epoch: 8.14 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07366052819219379		[learning rate: 0.00087726]
		[batch 20/20] avg loss: 0.08278055025433555		[learning rate: 0.00087622]
	Learning Rate: 0.000876224
	LOSS [training: 0.07822053922326466 | validation: 0.0747940895420908]
	TIME [epoch: 8.15 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07133750448534117		[learning rate: 0.00087519]
		[batch 20/20] avg loss: 0.08766215113944895		[learning rate: 0.00087416]
	Learning Rate: 0.000874157
	LOSS [training: 0.07949982781239504 | validation: 0.08303861724638971]
	TIME [epoch: 8.16 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0858378993805024		[learning rate: 0.00087313]
		[batch 20/20] avg loss: 0.0911054240786513		[learning rate: 0.0008721]
	Learning Rate: 0.000872096
	LOSS [training: 0.08847166172957684 | validation: 0.10639676158196518]
	TIME [epoch: 8.15 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09249156862838592		[learning rate: 0.00087107]
		[batch 20/20] avg loss: 0.0944524192312389		[learning rate: 0.00087004]
	Learning Rate: 0.000870038
	LOSS [training: 0.0934719939298124 | validation: 0.0933254942328869]
	TIME [epoch: 8.16 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0914887621887892		[learning rate: 0.00086901]
		[batch 20/20] avg loss: 0.10250281357931149		[learning rate: 0.00086799]
	Learning Rate: 0.000867986
	LOSS [training: 0.09699578788405032 | validation: 0.0894050835383121]
	TIME [epoch: 8.16 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09821408073790233		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.11414317762219602		[learning rate: 0.00086594]
	Learning Rate: 0.000865939
	LOSS [training: 0.10617862918004917 | validation: 0.12532783015780147]
	TIME [epoch: 8.18 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0938691463246875		[learning rate: 0.00086492]
		[batch 20/20] avg loss: 0.0834193240594397		[learning rate: 0.0008639]
	Learning Rate: 0.000863896
	LOSS [training: 0.08864423519206358 | validation: 0.0654965275153736]
	TIME [epoch: 8.16 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09411205605539041		[learning rate: 0.00086288]
		[batch 20/20] avg loss: 0.09327907949048667		[learning rate: 0.00086186]
	Learning Rate: 0.000861858
	LOSS [training: 0.09369556777293854 | validation: 0.0728140535645128]
	TIME [epoch: 8.15 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07454870439894286		[learning rate: 0.00086084]
		[batch 20/20] avg loss: 0.0718369962718249		[learning rate: 0.00085983]
	Learning Rate: 0.000859825
	LOSS [training: 0.07319285033538388 | validation: 0.061985418378148194]
	TIME [epoch: 8.15 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08228572474195331		[learning rate: 0.00085881]
		[batch 20/20] avg loss: 0.07260917654851753		[learning rate: 0.0008578]
	Learning Rate: 0.000857797
	LOSS [training: 0.07744745064523542 | validation: 0.06211442119631115]
	TIME [epoch: 8.16 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10590980805114332		[learning rate: 0.00085678]
		[batch 20/20] avg loss: 0.08584662014210268		[learning rate: 0.00085577]
	Learning Rate: 0.000855774
	LOSS [training: 0.09587821409662299 | validation: 0.08235128703873357]
	TIME [epoch: 8.15 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08838053508778695		[learning rate: 0.00085476]
		[batch 20/20] avg loss: 0.08530402885039394		[learning rate: 0.00085376]
	Learning Rate: 0.000853755
	LOSS [training: 0.08684228196909044 | validation: 0.07647461042446772]
	TIME [epoch: 8.15 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09399903137097985		[learning rate: 0.00085275]
		[batch 20/20] avg loss: 0.09455798555865742		[learning rate: 0.00085174]
	Learning Rate: 0.000851741
	LOSS [training: 0.09427850846481864 | validation: 0.06481423466448123]
	TIME [epoch: 8.14 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09201058892661075		[learning rate: 0.00085074]
		[batch 20/20] avg loss: 0.076228258353392		[learning rate: 0.00084973]
	Learning Rate: 0.000849732
	LOSS [training: 0.08411942364000138 | validation: 0.07936501040844024]
	TIME [epoch: 8.18 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09182442548514574		[learning rate: 0.00084873]
		[batch 20/20] avg loss: 0.07590348079590933		[learning rate: 0.00084773]
	Learning Rate: 0.000847728
	LOSS [training: 0.08386395314052753 | validation: 0.05518200765026687]
	TIME [epoch: 8.15 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08942121106423005		[learning rate: 0.00084673]
		[batch 20/20] avg loss: 0.09767085456131287		[learning rate: 0.00084573]
	Learning Rate: 0.000845728
	LOSS [training: 0.09354603281277148 | validation: 0.08688903591467285]
	TIME [epoch: 8.15 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08842843647382984		[learning rate: 0.00084473]
		[batch 20/20] avg loss: 0.09547921066782153		[learning rate: 0.00084373]
	Learning Rate: 0.000843733
	LOSS [training: 0.09195382357082568 | validation: 0.08989312963579643]
	TIME [epoch: 8.16 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09229444719925044		[learning rate: 0.00084274]
		[batch 20/20] avg loss: 0.09588327384120346		[learning rate: 0.00084174]
	Learning Rate: 0.000841743
	LOSS [training: 0.09408886052022696 | validation: 0.07405164441057643]
	TIME [epoch: 8.18 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08473568255518693		[learning rate: 0.00084075]
		[batch 20/20] avg loss: 0.08568484271023954		[learning rate: 0.00083976]
	Learning Rate: 0.000839757
	LOSS [training: 0.08521026263271322 | validation: 0.1074129931922706]
	TIME [epoch: 8.15 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09044754252964403		[learning rate: 0.00083877]
		[batch 20/20] avg loss: 0.09954477670361564		[learning rate: 0.00083778]
	Learning Rate: 0.000837777
	LOSS [training: 0.09499615961662984 | validation: 0.08386363805283746]
	TIME [epoch: 8.15 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08903748021118063		[learning rate: 0.00083679]
		[batch 20/20] avg loss: 0.08704443817464061		[learning rate: 0.0008358]
	Learning Rate: 0.0008358
	LOSS [training: 0.08804095919291062 | validation: 0.062404303447239046]
	TIME [epoch: 8.14 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07010152474568469		[learning rate: 0.00083481]
		[batch 20/20] avg loss: 0.07959402157049293		[learning rate: 0.00083383]
	Learning Rate: 0.000833829
	LOSS [training: 0.0748477731580888 | validation: 0.07380770995166]
	TIME [epoch: 8.18 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11587146585767683		[learning rate: 0.00083284]
		[batch 20/20] avg loss: 0.0904466682654614		[learning rate: 0.00083186]
	Learning Rate: 0.000831862
	LOSS [training: 0.1031590670615691 | validation: 0.1038320417298205]
	TIME [epoch: 8.15 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10045122823064241		[learning rate: 0.00083088]
		[batch 20/20] avg loss: 0.09355433518632629		[learning rate: 0.0008299]
	Learning Rate: 0.0008299
	LOSS [training: 0.09700278170848435 | validation: 0.10960734768652064]
	TIME [epoch: 8.15 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.089585583171822		[learning rate: 0.00082892]
		[batch 20/20] avg loss: 0.0785342775889328		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.0840599303803774 | validation: 0.09008686895175846]
	TIME [epoch: 8.15 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08067735803423645		[learning rate: 0.00082697]
		[batch 20/20] avg loss: 0.08179744531986874		[learning rate: 0.00082599]
	Learning Rate: 0.000825989
	LOSS [training: 0.0812374016770526 | validation: 0.05056199904097401]
	TIME [epoch: 8.17 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07100820382284938		[learning rate: 0.00082501]
		[batch 20/20] avg loss: 0.07689659516684329		[learning rate: 0.00082404]
	Learning Rate: 0.000824041
	LOSS [training: 0.07395239949484633 | validation: 0.0820782202212984]
	TIME [epoch: 8.16 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06904785384534644		[learning rate: 0.00082307]
		[batch 20/20] avg loss: 0.09067363599155867		[learning rate: 0.0008221]
	Learning Rate: 0.000822097
	LOSS [training: 0.07986074491845255 | validation: 0.11480066460980053]
	TIME [epoch: 8.15 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09478716346916918		[learning rate: 0.00082113]
		[batch 20/20] avg loss: 0.08068328906030839		[learning rate: 0.00082016]
	Learning Rate: 0.000820158
	LOSS [training: 0.08773522626473877 | validation: 0.08645786933796432]
	TIME [epoch: 8.15 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08775079669405147		[learning rate: 0.00081919]
		[batch 20/20] avg loss: 0.08230909907868084		[learning rate: 0.00081822]
	Learning Rate: 0.000818223
	LOSS [training: 0.08502994788636616 | validation: 0.07996980280678137]
	TIME [epoch: 8.17 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09617234677876449		[learning rate: 0.00081726]
		[batch 20/20] avg loss: 0.0800137803546722		[learning rate: 0.00081629]
	Learning Rate: 0.000816293
	LOSS [training: 0.08809306356671834 | validation: 0.06633005831084716]
	TIME [epoch: 8.16 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08397320145349196		[learning rate: 0.00081533]
		[batch 20/20] avg loss: 0.09158030142887695		[learning rate: 0.00081437]
	Learning Rate: 0.000814368
	LOSS [training: 0.08777675144118446 | validation: 0.0974720825271138]
	TIME [epoch: 8.15 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08648755492694012		[learning rate: 0.00081341]
		[batch 20/20] avg loss: 0.08798157589618208		[learning rate: 0.00081245]
	Learning Rate: 0.000812447
	LOSS [training: 0.08723456541156109 | validation: 0.08076391424150695]
	TIME [epoch: 8.16 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0842839650080341		[learning rate: 0.00081149]
		[batch 20/20] avg loss: 0.09549743362591631		[learning rate: 0.00081053]
	Learning Rate: 0.00081053
	LOSS [training: 0.08989069931697521 | validation: 0.08157218394429427]
	TIME [epoch: 8.18 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07705003651149608		[learning rate: 0.00080957]
		[batch 20/20] avg loss: 0.06963377945826492		[learning rate: 0.00080862]
	Learning Rate: 0.000808618
	LOSS [training: 0.07334190798488052 | validation: 0.05749358393972207]
	TIME [epoch: 8.15 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07708643286348693		[learning rate: 0.00080766]
		[batch 20/20] avg loss: 0.08258134696318428		[learning rate: 0.00080671]
	Learning Rate: 0.000806711
	LOSS [training: 0.0798338899133356 | validation: 0.0873335933059528]
	TIME [epoch: 8.15 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07884097960615102		[learning rate: 0.00080576]
		[batch 20/20] avg loss: 0.08086185344652248		[learning rate: 0.00080481]
	Learning Rate: 0.000804808
	LOSS [training: 0.07985141652633676 | validation: 0.09518614392501105]
	TIME [epoch: 8.15 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08801069609671376		[learning rate: 0.00080386]
		[batch 20/20] avg loss: 0.0785169179725517		[learning rate: 0.00080291]
	Learning Rate: 0.00080291
	LOSS [training: 0.08326380703463274 | validation: 0.05312368743097608]
	TIME [epoch: 8.17 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08988621744805976		[learning rate: 0.00080196]
		[batch 20/20] avg loss: 0.07510842568484774		[learning rate: 0.00080102]
	Learning Rate: 0.000801016
	LOSS [training: 0.08249732156645376 | validation: 0.06620666949556753]
	TIME [epoch: 8.16 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09018061768044319		[learning rate: 0.00080007]
		[batch 20/20] avg loss: 0.09715865277479356		[learning rate: 0.00079913]
	Learning Rate: 0.000799126
	LOSS [training: 0.09366963522761837 | validation: 0.06414703045146476]
	TIME [epoch: 8.15 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07585464908094466		[learning rate: 0.00079818]
		[batch 20/20] avg loss: 0.07799927272302513		[learning rate: 0.00079724]
	Learning Rate: 0.000797241
	LOSS [training: 0.07692696090198489 | validation: 0.057651985464798786]
	TIME [epoch: 8.15 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07060797177452688		[learning rate: 0.0007963]
		[batch 20/20] avg loss: 0.09214702238390406		[learning rate: 0.00079536]
	Learning Rate: 0.000795361
	LOSS [training: 0.08137749707921546 | validation: 0.06923157178909989]
	TIME [epoch: 8.18 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07688448863671773		[learning rate: 0.00079442]
		[batch 20/20] avg loss: 0.09666413738076914		[learning rate: 0.00079348]
	Learning Rate: 0.000793484
	LOSS [training: 0.08677431300874341 | validation: 0.09372000154537]
	TIME [epoch: 8.17 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07930195967138469		[learning rate: 0.00079255]
		[batch 20/20] avg loss: 0.08924871828033558		[learning rate: 0.00079161]
	Learning Rate: 0.000791613
	LOSS [training: 0.08427533897586015 | validation: 0.09709213020705622]
	TIME [epoch: 8.16 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07916416060057224		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.07432576986641395		[learning rate: 0.00078975]
	Learning Rate: 0.000789745
	LOSS [training: 0.0767449652334931 | validation: 0.06890767225189323]
	TIME [epoch: 8.15 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0660140867236964		[learning rate: 0.00078881]
		[batch 20/20] avg loss: 0.0836942784436369		[learning rate: 0.00078788]
	Learning Rate: 0.000787882
	LOSS [training: 0.07485418258366663 | validation: 0.08405895015194029]
	TIME [epoch: 8.17 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08778340615592728		[learning rate: 0.00078695]
		[batch 20/20] avg loss: 0.09790749939891676		[learning rate: 0.00078602]
	Learning Rate: 0.000786024
	LOSS [training: 0.09284545277742204 | validation: 0.07939656773777677]
	TIME [epoch: 8.16 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1006586146170512		[learning rate: 0.0007851]
		[batch 20/20] avg loss: 0.08349495585090762		[learning rate: 0.00078417]
	Learning Rate: 0.00078417
	LOSS [training: 0.09207678523397939 | validation: 0.07408914315076079]
	TIME [epoch: 8.15 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08801597007076245		[learning rate: 0.00078324]
		[batch 20/20] avg loss: 0.08694943603703062		[learning rate: 0.00078232]
	Learning Rate: 0.00078232
	LOSS [training: 0.08748270305389654 | validation: 0.05605170280947551]
	TIME [epoch: 8.16 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0664942226775462		[learning rate: 0.0007814]
		[batch 20/20] avg loss: 0.08454277957551662		[learning rate: 0.00078047]
	Learning Rate: 0.000780475
	LOSS [training: 0.07551850112653143 | validation: 0.09677232430379022]
	TIME [epoch: 8.17 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09795028426926156		[learning rate: 0.00077955]
		[batch 20/20] avg loss: 0.09312387980796158		[learning rate: 0.00077863]
	Learning Rate: 0.000778634
	LOSS [training: 0.09553708203861157 | validation: 0.06868745192572265]
	TIME [epoch: 8.16 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08551776661179046		[learning rate: 0.00077772]
		[batch 20/20] avg loss: 0.07864115585082539		[learning rate: 0.0007768]
	Learning Rate: 0.000776797
	LOSS [training: 0.08207946123130791 | validation: 0.06405572774138]
	TIME [epoch: 8.16 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08316378816703882		[learning rate: 0.00077588]
		[batch 20/20] avg loss: 0.11441919440982459		[learning rate: 0.00077496]
	Learning Rate: 0.000774965
	LOSS [training: 0.0987914912884317 | validation: 0.1596419211109007]
	TIME [epoch: 8.15 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11236982304957699		[learning rate: 0.00077405]
		[batch 20/20] avg loss: 0.09474314996849983		[learning rate: 0.00077314]
	Learning Rate: 0.000773137
	LOSS [training: 0.10355648650903841 | validation: 0.10288762172142613]
	TIME [epoch: 8.17 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0791626937047174		[learning rate: 0.00077222]
		[batch 20/20] avg loss: 0.08152343232602435		[learning rate: 0.00077131]
	Learning Rate: 0.000771313
	LOSS [training: 0.08034306301537086 | validation: 0.06700776157797902]
	TIME [epoch: 8.16 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09084906835501103		[learning rate: 0.0007704]
		[batch 20/20] avg loss: 0.09398484096956101		[learning rate: 0.00076949]
	Learning Rate: 0.000769494
	LOSS [training: 0.09241695466228603 | validation: 0.07370208517519813]
	TIME [epoch: 8.15 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08213996893195444		[learning rate: 0.00076859]
		[batch 20/20] avg loss: 0.0818813720171947		[learning rate: 0.00076768]
	Learning Rate: 0.000767679
	LOSS [training: 0.08201067047457454 | validation: 0.07160602587880151]
	TIME [epoch: 8.15 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08008759047621132		[learning rate: 0.00076677]
		[batch 20/20] avg loss: 0.08835898651340761		[learning rate: 0.00076587]
	Learning Rate: 0.000765868
	LOSS [training: 0.08422328849480948 | validation: 0.07146094404399826]
	TIME [epoch: 8.16 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08249938417826877		[learning rate: 0.00076496]
		[batch 20/20] avg loss: 0.09016405278524622		[learning rate: 0.00076406]
	Learning Rate: 0.000764061
	LOSS [training: 0.0863317184817575 | validation: 0.10600073117089069]
	TIME [epoch: 8.15 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08755609780262755		[learning rate: 0.00076316]
		[batch 20/20] avg loss: 0.08491284156441245		[learning rate: 0.00076226]
	Learning Rate: 0.000762259
	LOSS [training: 0.08623446968352 | validation: 0.061681342534476644]
	TIME [epoch: 8.16 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07634978284206892		[learning rate: 0.00076136]
		[batch 20/20] avg loss: 0.08574145866258534		[learning rate: 0.00076046]
	Learning Rate: 0.000760461
	LOSS [training: 0.08104562075232713 | validation: 0.08826356248389275]
	TIME [epoch: 8.16 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0746003941364625		[learning rate: 0.00075956]
		[batch 20/20] avg loss: 0.08956816355101227		[learning rate: 0.00075867]
	Learning Rate: 0.000758667
	LOSS [training: 0.0820842788437374 | validation: 0.10068210517374655]
	TIME [epoch: 8.16 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09820818019563224		[learning rate: 0.00075777]
		[batch 20/20] avg loss: 0.10357212450513953		[learning rate: 0.00075688]
	Learning Rate: 0.000756877
	LOSS [training: 0.1008901523503859 | validation: 0.09063709186394021]
	TIME [epoch: 8.17 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09032014076237675		[learning rate: 0.00075598]
		[batch 20/20] avg loss: 0.08058781603114767		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.08545397839676222 | validation: 0.11603345335511989]
	TIME [epoch: 8.15 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07145276538183745		[learning rate: 0.0007542]
		[batch 20/20] avg loss: 0.08025289787896968		[learning rate: 0.00075331]
	Learning Rate: 0.000753311
	LOSS [training: 0.07585283163040357 | validation: 0.06681489969108231]
	TIME [epoch: 8.15 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0853707374352441		[learning rate: 0.00075242]
		[batch 20/20] avg loss: 0.0778206980770968		[learning rate: 0.00075153]
	Learning Rate: 0.000751534
	LOSS [training: 0.08159571775617044 | validation: 0.05301128335016059]
	TIME [epoch: 8.15 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09925329005675657		[learning rate: 0.00075065]
		[batch 20/20] avg loss: 0.08229540195015742		[learning rate: 0.00074976]
	Learning Rate: 0.000749761
	LOSS [training: 0.09077434600345699 | validation: 0.08412275536915172]
	TIME [epoch: 8.18 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08471405026503362		[learning rate: 0.00074888]
		[batch 20/20] avg loss: 0.08640662038752195		[learning rate: 0.00074799]
	Learning Rate: 0.000747993
	LOSS [training: 0.08556033532627778 | validation: 0.05574063716609502]
	TIME [epoch: 8.16 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0843487091711075		[learning rate: 0.00074711]
		[batch 20/20] avg loss: 0.08848969719323571		[learning rate: 0.00074623]
	Learning Rate: 0.000746228
	LOSS [training: 0.08641920318217161 | validation: 0.08074111084583743]
	TIME [epoch: 8.14 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06898420331603375		[learning rate: 0.00074535]
		[batch 20/20] avg loss: 0.08304130389677919		[learning rate: 0.00074447]
	Learning Rate: 0.000744468
	LOSS [training: 0.07601275360640647 | validation: 0.07304707618895749]
	TIME [epoch: 8.13 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08180454145641096		[learning rate: 0.00074359]
		[batch 20/20] avg loss: 0.09039592321095859		[learning rate: 0.00074271]
	Learning Rate: 0.000742712
	LOSS [training: 0.08610023233368477 | validation: 0.06677073670889122]
	TIME [epoch: 8.17 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0843725400037266		[learning rate: 0.00074184]
		[batch 20/20] avg loss: 0.07641602599970096		[learning rate: 0.00074096]
	Learning Rate: 0.00074096
	LOSS [training: 0.08039428300171379 | validation: 0.04899486440968027]
	TIME [epoch: 8.14 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08019977975024449		[learning rate: 0.00074009]
		[batch 20/20] avg loss: 0.1028714308217673		[learning rate: 0.00073921]
	Learning Rate: 0.000739212
	LOSS [training: 0.09153560528600588 | validation: 0.06236670638691308]
	TIME [epoch: 8.15 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08666667852103475		[learning rate: 0.00073834]
		[batch 20/20] avg loss: 0.0701061457780562		[learning rate: 0.00073747]
	Learning Rate: 0.000737469
	LOSS [training: 0.07838641214954548 | validation: 0.07945293275079283]
	TIME [epoch: 8.15 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0683342076565327		[learning rate: 0.0007366]
		[batch 20/20] avg loss: 0.07923303550975844		[learning rate: 0.00073573]
	Learning Rate: 0.000735729
	LOSS [training: 0.07378362158314557 | validation: 0.06257946830116362]
	TIME [epoch: 8.18 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08121477313330946		[learning rate: 0.00073486]
		[batch 20/20] avg loss: 0.07417964256665402		[learning rate: 0.00073399]
	Learning Rate: 0.000733994
	LOSS [training: 0.07769720784998171 | validation: 0.07438329006831224]
	TIME [epoch: 8.15 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06820992390192546		[learning rate: 0.00073313]
		[batch 20/20] avg loss: 0.08986507073091256		[learning rate: 0.00073226]
	Learning Rate: 0.000732262
	LOSS [training: 0.07903749731641899 | validation: 0.06512573379740563]
	TIME [epoch: 8.14 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09143478851439715		[learning rate: 0.0007314]
		[batch 20/20] avg loss: 0.06449650053439862		[learning rate: 0.00073054]
	Learning Rate: 0.000730535
	LOSS [training: 0.07796564452439789 | validation: 0.05454426636624218]
	TIME [epoch: 8.15 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07540676349996893		[learning rate: 0.00072967]
		[batch 20/20] avg loss: 0.07309028319613262		[learning rate: 0.00072881]
	Learning Rate: 0.000728812
	LOSS [training: 0.07424852334805074 | validation: 0.0694804270063525]
	TIME [epoch: 8.17 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07771415658376117		[learning rate: 0.00072795]
		[batch 20/20] avg loss: 0.08881201974820885		[learning rate: 0.00072709]
	Learning Rate: 0.000727093
	LOSS [training: 0.08326308816598499 | validation: 0.06317829106447279]
	TIME [epoch: 8.16 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09053657481373514		[learning rate: 0.00072623]
		[batch 20/20] avg loss: 0.06032116598187851		[learning rate: 0.00072538]
	Learning Rate: 0.000725378
	LOSS [training: 0.07542887039780684 | validation: 0.05618298172520177]
	TIME [epoch: 8.15 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07878899725055535		[learning rate: 0.00072452]
		[batch 20/20] avg loss: 0.0853552509187854		[learning rate: 0.00072367]
	Learning Rate: 0.000723666
	LOSS [training: 0.08207212408467038 | validation: 0.06979247460682536]
	TIME [epoch: 8.15 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08737388244946673		[learning rate: 0.00072281]
		[batch 20/20] avg loss: 0.08184666651759667		[learning rate: 0.00072196]
	Learning Rate: 0.000721959
	LOSS [training: 0.08461027448353169 | validation: 0.05522946395207305]
	TIME [epoch: 8.17 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07166223472155991		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.07098045313471213		[learning rate: 0.00072026]
	Learning Rate: 0.000720257
	LOSS [training: 0.07132134392813602 | validation: 0.050694238635139104]
	TIME [epoch: 8.15 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10115369395534927		[learning rate: 0.00071941]
		[batch 20/20] avg loss: 0.06844111128067165		[learning rate: 0.00071856]
	Learning Rate: 0.000718557
	LOSS [training: 0.08479740261801047 | validation: 0.053766003766481926]
	TIME [epoch: 8.15 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0791243654393002		[learning rate: 0.00071771]
		[batch 20/20] avg loss: 0.06466321866559369		[learning rate: 0.00071686]
	Learning Rate: 0.000716863
	LOSS [training: 0.07189379205244692 | validation: 0.05524148115355432]
	TIME [epoch: 8.16 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08170300575928423		[learning rate: 0.00071602]
		[batch 20/20] avg loss: 0.08696602268188162		[learning rate: 0.00071517]
	Learning Rate: 0.000715172
	LOSS [training: 0.08433451422058291 | validation: 0.07993012487882792]
	TIME [epoch: 8.17 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07829424313798172		[learning rate: 0.00071433]
		[batch 20/20] avg loss: 0.08544420266921533		[learning rate: 0.00071348]
	Learning Rate: 0.000713485
	LOSS [training: 0.08186922290359852 | validation: 0.08993384656499709]
	TIME [epoch: 8.15 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07539320219074894		[learning rate: 0.00071264]
		[batch 20/20] avg loss: 0.09894991778437615		[learning rate: 0.0007118]
	Learning Rate: 0.000711802
	LOSS [training: 0.08717155998756253 | validation: 0.06304559573881793]
	TIME [epoch: 8.15 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08402299771286545		[learning rate: 0.00071096]
		[batch 20/20] avg loss: 0.08103327769595452		[learning rate: 0.00071012]
	Learning Rate: 0.000710123
	LOSS [training: 0.08252813770440996 | validation: 0.0624704403785972]
	TIME [epoch: 8.15 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07830827523455358		[learning rate: 0.00070928]
		[batch 20/20] avg loss: 0.10957366403471062		[learning rate: 0.00070845]
	Learning Rate: 0.000708448
	LOSS [training: 0.0939409696346321 | validation: 0.0809176671364395]
	TIME [epoch: 8.17 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07474934411228021		[learning rate: 0.00070761]
		[batch 20/20] avg loss: 0.09328980804420384		[learning rate: 0.00070678]
	Learning Rate: 0.000706776
	LOSS [training: 0.08401957607824202 | validation: 0.09923018637889817]
	TIME [epoch: 8.15 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08294921953062522		[learning rate: 0.00070594]
		[batch 20/20] avg loss: 0.07687831879482772		[learning rate: 0.00070511]
	Learning Rate: 0.000705109
	LOSS [training: 0.07991376916272647 | validation: 0.08081042988697332]
	TIME [epoch: 8.14 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09144480676008432		[learning rate: 0.00070428]
		[batch 20/20] avg loss: 0.06325710971104498		[learning rate: 0.00070345]
	Learning Rate: 0.000703446
	LOSS [training: 0.07735095823556466 | validation: 0.06279101445090808]
	TIME [epoch: 8.14 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08183317353773341		[learning rate: 0.00070262]
		[batch 20/20] avg loss: 0.08714613009418133		[learning rate: 0.00070179]
	Learning Rate: 0.000701787
	LOSS [training: 0.08448965181595737 | validation: 0.09602174904084568]
	TIME [epoch: 8.17 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08608086500293938		[learning rate: 0.00070096]
		[batch 20/20] avg loss: 0.11653059868640343		[learning rate: 0.00070013]
	Learning Rate: 0.000700131
	LOSS [training: 0.10130573184467143 | validation: 0.09299568362489524]
	TIME [epoch: 8.16 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08405935358144659		[learning rate: 0.00069931]
		[batch 20/20] avg loss: 0.09843897621431398		[learning rate: 0.00069848]
	Learning Rate: 0.00069848
	LOSS [training: 0.09124916489788028 | validation: 0.09139270337331731]
	TIME [epoch: 8.16 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08631991922805053		[learning rate: 0.00069766]
		[batch 20/20] avg loss: 0.07610433701661587		[learning rate: 0.00069683]
	Learning Rate: 0.000696832
	LOSS [training: 0.08121212812233321 | validation: 0.04831804925340664]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1178.pth
	Model improved!!!
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07258733888138719		[learning rate: 0.00069601]
		[batch 20/20] avg loss: 0.08282039385570697		[learning rate: 0.00069519]
	Learning Rate: 0.000695188
	LOSS [training: 0.07770386636854706 | validation: 0.06099214885163927]
	TIME [epoch: 8.19 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08669570401110174		[learning rate: 0.00069437]
		[batch 20/20] avg loss: 0.07904791705176024		[learning rate: 0.00069355]
	Learning Rate: 0.000693549
	LOSS [training: 0.08287181053143099 | validation: 0.08184645848121376]
	TIME [epoch: 8.17 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07561116299683066		[learning rate: 0.00069273]
		[batch 20/20] avg loss: 0.10407426204017249		[learning rate: 0.00069191]
	Learning Rate: 0.000691913
	LOSS [training: 0.0898427125185016 | validation: 0.07231500464119114]
	TIME [epoch: 8.16 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09849472156520582		[learning rate: 0.0006911]
		[batch 20/20] avg loss: 0.0766023510574055		[learning rate: 0.00069028]
	Learning Rate: 0.00069028
	LOSS [training: 0.08754853631130566 | validation: 0.07820669070479297]
	TIME [epoch: 8.17 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08118169556344904		[learning rate: 0.00068947]
		[batch 20/20] avg loss: 0.07576673202819857		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.07847421379582378 | validation: 0.0664358486946634]
	TIME [epoch: 8.18 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08876588993766277		[learning rate: 0.00068784]
		[batch 20/20] avg loss: 0.0723186002288502		[learning rate: 0.00068703]
	Learning Rate: 0.000687028
	LOSS [training: 0.0805422450832565 | validation: 0.06060154496507433]
	TIME [epoch: 8.17 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07662009495146362		[learning rate: 0.00068622]
		[batch 20/20] avg loss: 0.06607118698393266		[learning rate: 0.00068541]
	Learning Rate: 0.000685407
	LOSS [training: 0.07134564096769816 | validation: 0.05166665626440117]
	TIME [epoch: 8.16 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08701461232443247		[learning rate: 0.0006846]
		[batch 20/20] avg loss: 0.07929239275972143		[learning rate: 0.00068379]
	Learning Rate: 0.00068379
	LOSS [training: 0.08315350254207696 | validation: 0.07013800492301688]
	TIME [epoch: 8.16 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08111445646427268		[learning rate: 0.00068298]
		[batch 20/20] avg loss: 0.058220312644739036		[learning rate: 0.00068218]
	Learning Rate: 0.000682178
	LOSS [training: 0.06966738455450587 | validation: 0.0651078352486808]
	TIME [epoch: 8.19 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07978171092188971		[learning rate: 0.00068137]
		[batch 20/20] avg loss: 0.06778117220745353		[learning rate: 0.00068057]
	Learning Rate: 0.000680568
	LOSS [training: 0.07378144156467162 | validation: 0.07397653962678585]
	TIME [epoch: 8.17 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08653418500647797		[learning rate: 0.00067977]
		[batch 20/20] avg loss: 0.08611238938181387		[learning rate: 0.00067896]
	Learning Rate: 0.000678963
	LOSS [training: 0.08632328719414592 | validation: 0.08416379626850017]
	TIME [epoch: 8.18 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08234128601771654		[learning rate: 0.00067816]
		[batch 20/20] avg loss: 0.06712771773828152		[learning rate: 0.00067736]
	Learning Rate: 0.000677362
	LOSS [training: 0.07473450187799903 | validation: 0.056136769466687206]
	TIME [epoch: 8.17 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06461361535207355		[learning rate: 0.00067656]
		[batch 20/20] avg loss: 0.08693005782661613		[learning rate: 0.00067576]
	Learning Rate: 0.000675764
	LOSS [training: 0.07577183658934483 | validation: 0.06546862247286922]
	TIME [epoch: 8.19 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07463341874679442		[learning rate: 0.00067497]
		[batch 20/20] avg loss: 0.08709426144006863		[learning rate: 0.00067417]
	Learning Rate: 0.00067417
	LOSS [training: 0.08086384009343153 | validation: 0.07477973238419239]
	TIME [epoch: 8.17 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08301290568703322		[learning rate: 0.00067337]
		[batch 20/20] avg loss: 0.09032986577933198		[learning rate: 0.00067258]
	Learning Rate: 0.000672579
	LOSS [training: 0.0866713857331826 | validation: 0.05726837311795199]
	TIME [epoch: 8.16 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08426999893802908		[learning rate: 0.00067179]
		[batch 20/20] avg loss: 0.10275868889861162		[learning rate: 0.00067099]
	Learning Rate: 0.000670993
	LOSS [training: 0.09351434391832034 | validation: 0.15414486829378876]
	TIME [epoch: 8.17 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11867611155936868		[learning rate: 0.0006702]
		[batch 20/20] avg loss: 0.08324946690805984		[learning rate: 0.00066941]
	Learning Rate: 0.00066941
	LOSS [training: 0.10096278923371424 | validation: 0.07288405583223323]
	TIME [epoch: 8.18 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07574627850543333		[learning rate: 0.00066862]
		[batch 20/20] avg loss: 0.08661034469728378		[learning rate: 0.00066783]
	Learning Rate: 0.000667831
	LOSS [training: 0.08117831160135856 | validation: 0.06607940444323156]
	TIME [epoch: 8.18 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08266184901066208		[learning rate: 0.00066704]
		[batch 20/20] avg loss: 0.07981119140826806		[learning rate: 0.00066626]
	Learning Rate: 0.000666256
	LOSS [training: 0.08123652020946506 | validation: 0.05097558825270503]
	TIME [epoch: 8.17 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06481596304499568		[learning rate: 0.00066547]
		[batch 20/20] avg loss: 0.07405498833615602		[learning rate: 0.00066468]
	Learning Rate: 0.000664684
	LOSS [training: 0.06943547569057586 | validation: 0.05940001944873172]
	TIME [epoch: 8.17 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07494198299601472		[learning rate: 0.0006639]
		[batch 20/20] avg loss: 0.06744660580971829		[learning rate: 0.00066312]
	Learning Rate: 0.000663116
	LOSS [training: 0.0711942944028665 | validation: 0.0723582791873142]
	TIME [epoch: 8.19 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07766998837106855		[learning rate: 0.00066233]
		[batch 20/20] avg loss: 0.07590350326451181		[learning rate: 0.00066155]
	Learning Rate: 0.000661552
	LOSS [training: 0.07678674581779017 | validation: 0.0739037704747875]
	TIME [epoch: 8.18 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08905273436344123		[learning rate: 0.00066077]
		[batch 20/20] avg loss: 0.08313309874247247		[learning rate: 0.00065999]
	Learning Rate: 0.000659992
	LOSS [training: 0.08609291655295685 | validation: 0.05823611051390418]
	TIME [epoch: 8.17 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0731829394329611		[learning rate: 0.00065921]
		[batch 20/20] avg loss: 0.07829769909741544		[learning rate: 0.00065843]
	Learning Rate: 0.000658435
	LOSS [training: 0.07574031926518827 | validation: 0.06093501408085357]
	TIME [epoch: 8.17 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073360069653718		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.07777129344006528		[learning rate: 0.00065688]
	Learning Rate: 0.000656882
	LOSS [training: 0.07425244706830124 | validation: 0.04752221213270369]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1203.pth
	Model improved!!!
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08655190406711344		[learning rate: 0.00065611]
		[batch 20/20] avg loss: 0.06421087356518536		[learning rate: 0.00065533]
	Learning Rate: 0.000655332
	LOSS [training: 0.07538138881614939 | validation: 0.053540968056326396]
	TIME [epoch: 8.18 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07168992531286417		[learning rate: 0.00065456]
		[batch 20/20] avg loss: 0.07855465413577928		[learning rate: 0.00065379]
	Learning Rate: 0.000653786
	LOSS [training: 0.07512228972432175 | validation: 0.06336612911037158]
	TIME [epoch: 8.17 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07062154765530607		[learning rate: 0.00065301]
		[batch 20/20] avg loss: 0.07057106933283466		[learning rate: 0.00065224]
	Learning Rate: 0.000652244
	LOSS [training: 0.07059630849407036 | validation: 0.06475672535515693]
	TIME [epoch: 8.16 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07162757851276393		[learning rate: 0.00065147]
		[batch 20/20] avg loss: 0.07483908702216287		[learning rate: 0.00065071]
	Learning Rate: 0.000650706
	LOSS [training: 0.07323333276746338 | validation: 0.04721485570327133]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07341738886268448		[learning rate: 0.00064994]
		[batch 20/20] avg loss: 0.06966554819439749		[learning rate: 0.00064917]
	Learning Rate: 0.000649171
	LOSS [training: 0.071541468528541 | validation: 0.06023095591889584]
	TIME [epoch: 8.18 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07365437847816916		[learning rate: 0.0006484]
		[batch 20/20] avg loss: 0.08601797126905725		[learning rate: 0.00064764]
	Learning Rate: 0.000647639
	LOSS [training: 0.07983617487361319 | validation: 0.07327890022148599]
	TIME [epoch: 8.17 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07113342769888888		[learning rate: 0.00064688]
		[batch 20/20] avg loss: 0.08039633954556925		[learning rate: 0.00064611]
	Learning Rate: 0.000646112
	LOSS [training: 0.07576488362222907 | validation: 0.06315041424485907]
	TIME [epoch: 8.16 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.076553830037324		[learning rate: 0.00064535]
		[batch 20/20] avg loss: 0.0824903357109134		[learning rate: 0.00064459]
	Learning Rate: 0.000644588
	LOSS [training: 0.07952208287411872 | validation: 0.0706185248824183]
	TIME [epoch: 8.19 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0687469840261529		[learning rate: 0.00064383]
		[batch 20/20] avg loss: 0.0773345696304661		[learning rate: 0.00064307]
	Learning Rate: 0.000643067
	LOSS [training: 0.07304077682830949 | validation: 0.05616447649901918]
	TIME [epoch: 8.18 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0738762925814575		[learning rate: 0.00064231]
		[batch 20/20] avg loss: 0.06299291870830073		[learning rate: 0.00064155]
	Learning Rate: 0.00064155
	LOSS [training: 0.0684346056448791 | validation: 0.07618420521397014]
	TIME [epoch: 8.18 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07259713250783542		[learning rate: 0.00064079]
		[batch 20/20] avg loss: 0.0807462180495854		[learning rate: 0.00064004]
	Learning Rate: 0.000640037
	LOSS [training: 0.07667167527871041 | validation: 0.058823508159057444]
	TIME [epoch: 8.16 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08107318647311124		[learning rate: 0.00063928]
		[batch 20/20] avg loss: 0.07654758338238635		[learning rate: 0.00063853]
	Learning Rate: 0.000638527
	LOSS [training: 0.07881038492774878 | validation: 0.05830323605083504]
	TIME [epoch: 8.18 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08839877591663352		[learning rate: 0.00063777]
		[batch 20/20] avg loss: 0.07323858176212968		[learning rate: 0.00063702]
	Learning Rate: 0.000637021
	LOSS [training: 0.0808186788393816 | validation: 0.07340218800681521]
	TIME [epoch: 8.18 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09156041806131827		[learning rate: 0.00063627]
		[batch 20/20] avg loss: 0.07366133491990037		[learning rate: 0.00063552]
	Learning Rate: 0.000635519
	LOSS [training: 0.08261087649060932 | validation: 0.070877142501474]
	TIME [epoch: 8.17 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06487197997886358		[learning rate: 0.00063477]
		[batch 20/20] avg loss: 0.0745013146277049		[learning rate: 0.00063402]
	Learning Rate: 0.000634019
	LOSS [training: 0.06968664730328424 | validation: 0.060398502351987834]
	TIME [epoch: 8.16 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07411758420380002		[learning rate: 0.00063327]
		[batch 20/20] avg loss: 0.07039605982121021		[learning rate: 0.00063252]
	Learning Rate: 0.000632524
	LOSS [training: 0.07225682201250512 | validation: 0.054234124684929844]
	TIME [epoch: 8.18 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09119905901413851		[learning rate: 0.00063178]
		[batch 20/20] avg loss: 0.06238426004668213		[learning rate: 0.00063103]
	Learning Rate: 0.000631032
	LOSS [training: 0.0767916595304103 | validation: 0.06414237522389829]
	TIME [epoch: 8.18 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07615678279173596		[learning rate: 0.00063029]
		[batch 20/20] avg loss: 0.0847503344231754		[learning rate: 0.00062954]
	Learning Rate: 0.000629543
	LOSS [training: 0.08045355860745566 | validation: 0.09163120626956191]
	TIME [epoch: 8.16 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07646120405652007		[learning rate: 0.0006288]
		[batch 20/20] avg loss: 0.08192621049425192		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.07919370727538597 | validation: 0.05165686379168287]
	TIME [epoch: 8.17 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07146216380675666		[learning rate: 0.00062732]
		[batch 20/20] avg loss: 0.0652046903829006		[learning rate: 0.00062658]
	Learning Rate: 0.000626577
	LOSS [training: 0.06833342709482865 | validation: 0.0573993388701997]
	TIME [epoch: 8.17 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06900876151213288		[learning rate: 0.00062584]
		[batch 20/20] avg loss: 0.08144115183197645		[learning rate: 0.0006251]
	Learning Rate: 0.000625099
	LOSS [training: 0.07522495667205467 | validation: 0.07028512127156679]
	TIME [epoch: 8.19 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08400110570306149		[learning rate: 0.00062436]
		[batch 20/20] avg loss: 0.07288203024761479		[learning rate: 0.00062362]
	Learning Rate: 0.000623624
	LOSS [training: 0.07844156797533815 | validation: 0.05648589380328749]
	TIME [epoch: 8.16 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07749411300326645		[learning rate: 0.00062289]
		[batch 20/20] avg loss: 0.08891657346846818		[learning rate: 0.00062215]
	Learning Rate: 0.000622153
	LOSS [training: 0.08320534323586733 | validation: 0.13966258408939808]
	TIME [epoch: 8.17 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08773924493407928		[learning rate: 0.00062142]
		[batch 20/20] avg loss: 0.06776443627181777		[learning rate: 0.00062069]
	Learning Rate: 0.000620686
	LOSS [training: 0.07775184060294853 | validation: 0.0640792225954143]
	TIME [epoch: 8.17 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07750835254203882		[learning rate: 0.00061995]
		[batch 20/20] avg loss: 0.07491097171759471		[learning rate: 0.00061922]
	Learning Rate: 0.000619222
	LOSS [training: 0.07620966212981677 | validation: 0.058858971228689244]
	TIME [epoch: 8.19 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07759093005593706		[learning rate: 0.00061849]
		[batch 20/20] avg loss: 0.06514152280458738		[learning rate: 0.00061776]
	Learning Rate: 0.000617761
	LOSS [training: 0.07136622643026222 | validation: 0.04433832185470114]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1229.pth
	Model improved!!!
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965020217715391		[learning rate: 0.00061703]
		[batch 20/20] avg loss: 0.07533005140739991		[learning rate: 0.0006163]
	Learning Rate: 0.000616304
	LOSS [training: 0.07249012679227691 | validation: 0.07589942016363638]
	TIME [epoch: 8.17 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08042397845966667		[learning rate: 0.00061558]
		[batch 20/20] avg loss: 0.08042244804010257		[learning rate: 0.00061485]
	Learning Rate: 0.00061485
	LOSS [training: 0.08042321324988463 | validation: 0.06595613217699248]
	TIME [epoch: 8.18 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07047797035408349		[learning rate: 0.00061412]
		[batch 20/20] avg loss: 0.07269885966200848		[learning rate: 0.0006134]
	Learning Rate: 0.0006134
	LOSS [training: 0.071588415008046 | validation: 0.07721522174565093]
	TIME [epoch: 8.18 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08399794836549285		[learning rate: 0.00061268]
		[batch 20/20] avg loss: 0.09900087995963197		[learning rate: 0.00061195]
	Learning Rate: 0.000611953
	LOSS [training: 0.0914994141625624 | validation: 0.08794267123911095]
	TIME [epoch: 8.15 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08880250157330911		[learning rate: 0.00061123]
		[batch 20/20] avg loss: 0.09159899150680609		[learning rate: 0.00061051]
	Learning Rate: 0.000610509
	LOSS [training: 0.0902007465400576 | validation: 0.056284587145252515]
	TIME [epoch: 8.16 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09327050875085097		[learning rate: 0.00060979]
		[batch 20/20] avg loss: 0.08097334526593084		[learning rate: 0.00060907]
	Learning Rate: 0.000609069
	LOSS [training: 0.0871219270083909 | validation: 0.07310532232773075]
	TIME [epoch: 8.17 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08611399502731046		[learning rate: 0.00060835]
		[batch 20/20] avg loss: 0.07979053003160154		[learning rate: 0.00060763]
	Learning Rate: 0.000607632
	LOSS [training: 0.08295226252945599 | validation: 0.0512838399661257]
	TIME [epoch: 8.18 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07215414225560637		[learning rate: 0.00060692]
		[batch 20/20] avg loss: 0.07450227536001978		[learning rate: 0.0006062]
	Learning Rate: 0.000606199
	LOSS [training: 0.0733282088078131 | validation: 0.07232589137073264]
	TIME [epoch: 8.16 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08275782400730634		[learning rate: 0.00060548]
		[batch 20/20] avg loss: 0.08377667051392375		[learning rate: 0.00060477]
	Learning Rate: 0.000604769
	LOSS [training: 0.08326724726061505 | validation: 0.08879447943566147]
	TIME [epoch: 8.15 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06669521136682402		[learning rate: 0.00060406]
		[batch 20/20] avg loss: 0.0720762700731539		[learning rate: 0.00060334]
	Learning Rate: 0.000603343
	LOSS [training: 0.06938574071998896 | validation: 0.06740538219421498]
	TIME [epoch: 8.17 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06115322502044682		[learning rate: 0.00060263]
		[batch 20/20] avg loss: 0.06969475114670018		[learning rate: 0.00060192]
	Learning Rate: 0.00060192
	LOSS [training: 0.0654239880835735 | validation: 0.07082198762702074]
	TIME [epoch: 8.19 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08053304067660429		[learning rate: 0.00060121]
		[batch 20/20] avg loss: 0.0761654100660297		[learning rate: 0.0006005]
	Learning Rate: 0.0006005
	LOSS [training: 0.078349225371317 | validation: 0.0643240087335787]
	TIME [epoch: 8.18 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07311817045297779		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.06222248430676062		[learning rate: 0.00059908]
	Learning Rate: 0.000599083
	LOSS [training: 0.06767032737986922 | validation: 0.06256242021425022]
	TIME [epoch: 8.16 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07946540348805267		[learning rate: 0.00059838]
		[batch 20/20] avg loss: 0.0689770591479282		[learning rate: 0.00059767]
	Learning Rate: 0.00059767
	LOSS [training: 0.07422123131799044 | validation: 0.08520561510738925]
	TIME [epoch: 8.17 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09946154885765189		[learning rate: 0.00059696]
		[batch 20/20] avg loss: 0.07657999675120927		[learning rate: 0.00059626]
	Learning Rate: 0.00059626
	LOSS [training: 0.08802077280443057 | validation: 0.06442590139843316]
	TIME [epoch: 8.18 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06978336028540785		[learning rate: 0.00059556]
		[batch 20/20] avg loss: 0.0726072165251284		[learning rate: 0.00059485]
	Learning Rate: 0.000594854
	LOSS [training: 0.07119528840526812 | validation: 0.07738980519761152]
	TIME [epoch: 8.17 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07177573444457269		[learning rate: 0.00059415]
		[batch 20/20] avg loss: 0.07283622595858898		[learning rate: 0.00059345]
	Learning Rate: 0.000593451
	LOSS [training: 0.07230598020158084 | validation: 0.05915226563518074]
	TIME [epoch: 8.16 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07621303573694779		[learning rate: 0.00059275]
		[batch 20/20] avg loss: 0.08589672593860645		[learning rate: 0.00059205]
	Learning Rate: 0.000592051
	LOSS [training: 0.08105488083777712 | validation: 0.09519125502475746]
	TIME [epoch: 8.18 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08367111825082596		[learning rate: 0.00059135]
		[batch 20/20] avg loss: 0.07591631408857274		[learning rate: 0.00059065]
	Learning Rate: 0.000590654
	LOSS [training: 0.07979371616969935 | validation: 0.06526878259478064]
	TIME [epoch: 8.19 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06656139448277491		[learning rate: 0.00058996]
		[batch 20/20] avg loss: 0.09666925370167517		[learning rate: 0.00058926]
	Learning Rate: 0.000589261
	LOSS [training: 0.08161532409222504 | validation: 0.060315807508699824]
	TIME [epoch: 8.16 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07589597858670448		[learning rate: 0.00058857]
		[batch 20/20] avg loss: 0.10197685353316406		[learning rate: 0.00058787]
	Learning Rate: 0.000587871
	LOSS [training: 0.08893641605993426 | validation: 0.05645362962476938]
	TIME [epoch: 8.16 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06750731840267801		[learning rate: 0.00058718]
		[batch 20/20] avg loss: 0.08171423972400894		[learning rate: 0.00058648]
	Learning Rate: 0.000586484
	LOSS [training: 0.07461077906334349 | validation: 0.06326453503634219]
	TIME [epoch: 8.17 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06994176420892999		[learning rate: 0.00058579]
		[batch 20/20] avg loss: 0.06462898239034225		[learning rate: 0.0005851]
	Learning Rate: 0.000585101
	LOSS [training: 0.0672853732996361 | validation: 0.0801208911254884]
	TIME [epoch: 8.19 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07664277497363794		[learning rate: 0.00058441]
		[batch 20/20] avg loss: 0.06196262911587238		[learning rate: 0.00058372]
	Learning Rate: 0.000583721
	LOSS [training: 0.06930270204475517 | validation: 0.07107312756640598]
	TIME [epoch: 8.17 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07695959416853332		[learning rate: 0.00058303]
		[batch 20/20] avg loss: 0.08700564415916509		[learning rate: 0.00058234]
	Learning Rate: 0.000582344
	LOSS [training: 0.08198261916384922 | validation: 0.067219228601458]
	TIME [epoch: 8.16 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07021420813484085		[learning rate: 0.00058166]
		[batch 20/20] avg loss: 0.07484878366876475		[learning rate: 0.00058097]
	Learning Rate: 0.00058097
	LOSS [training: 0.07253149590180279 | validation: 0.07508186478189817]
	TIME [epoch: 8.16 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07439211238141501		[learning rate: 0.00058028]
		[batch 20/20] avg loss: 0.07561219119597243		[learning rate: 0.0005796]
	Learning Rate: 0.0005796
	LOSS [training: 0.07500215178869371 | validation: 0.06403049304937176]
	TIME [epoch: 8.18 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07256345519534828		[learning rate: 0.00057892]
		[batch 20/20] avg loss: 0.06025231810020221		[learning rate: 0.00057823]
	Learning Rate: 0.000578233
	LOSS [training: 0.06640788664777522 | validation: 0.07254272424647283]
	TIME [epoch: 8.17 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06474263916853298		[learning rate: 0.00057755]
		[batch 20/20] avg loss: 0.06540405541193		[learning rate: 0.00057687]
	Learning Rate: 0.000576869
	LOSS [training: 0.06507334729023148 | validation: 0.05946899619072697]
	TIME [epoch: 8.16 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07904170810915573		[learning rate: 0.00057619]
		[batch 20/20] avg loss: 0.08187242217700966		[learning rate: 0.00057551]
	Learning Rate: 0.000575508
	LOSS [training: 0.08045706514308269 | validation: 0.05587985041571972]
	TIME [epoch: 8.16 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06734108310321338		[learning rate: 0.00057483]
		[batch 20/20] avg loss: 0.06612513590285696		[learning rate: 0.00057415]
	Learning Rate: 0.00057415
	LOSS [training: 0.06673310950303515 | validation: 0.06862326806601333]
	TIME [epoch: 8.18 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06469387898999798		[learning rate: 0.00057347]
		[batch 20/20] avg loss: 0.07944136640928459		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.07206762269964127 | validation: 0.0803084133738418]
	TIME [epoch: 8.17 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07643257978382492		[learning rate: 0.00057212]
		[batch 20/20] avg loss: 0.07524533090142882		[learning rate: 0.00057144]
	Learning Rate: 0.000571445
	LOSS [training: 0.07583895534262684 | validation: 0.10215413242747112]
	TIME [epoch: 8.17 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08608656577591259		[learning rate: 0.00057077]
		[batch 20/20] avg loss: 0.06390469859097368		[learning rate: 0.0005701]
	Learning Rate: 0.000570097
	LOSS [training: 0.07499563218344314 | validation: 0.06234403039263077]
	TIME [epoch: 8.16 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07956428272559618		[learning rate: 0.00056942]
		[batch 20/20] avg loss: 0.07597796097603948		[learning rate: 0.00056875]
	Learning Rate: 0.000568752
	LOSS [training: 0.07777112185081783 | validation: 0.08421737235880511]
	TIME [epoch: 8.18 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07328740555370286		[learning rate: 0.00056808]
		[batch 20/20] avg loss: 0.07156718682002057		[learning rate: 0.00056741]
	Learning Rate: 0.000567411
	LOSS [training: 0.07242729618686172 | validation: 0.06293937708185414]
	TIME [epoch: 8.16 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07150099847229727		[learning rate: 0.00056674]
		[batch 20/20] avg loss: 0.06784192127004933		[learning rate: 0.00056607]
	Learning Rate: 0.000566072
	LOSS [training: 0.06967145987117329 | validation: 0.06472817735412367]
	TIME [epoch: 8.16 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0882507246981136		[learning rate: 0.0005654]
		[batch 20/20] avg loss: 0.07801328381016434		[learning rate: 0.00056474]
	Learning Rate: 0.000564737
	LOSS [training: 0.08313200425413896 | validation: 0.053829227572610744]
	TIME [epoch: 8.15 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06798009368996302		[learning rate: 0.00056407]
		[batch 20/20] avg loss: 0.07192988621426674		[learning rate: 0.0005634]
	Learning Rate: 0.000563405
	LOSS [training: 0.06995498995211488 | validation: 0.054037904564662204]
	TIME [epoch: 8.19 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07744720959487948		[learning rate: 0.00056274]
		[batch 20/20] avg loss: 0.07897441023753389		[learning rate: 0.00056208]
	Learning Rate: 0.000562076
	LOSS [training: 0.07821080991620669 | validation: 0.05974566292810059]
	TIME [epoch: 8.17 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06965361754100122		[learning rate: 0.00056141]
		[batch 20/20] avg loss: 0.07374899791766946		[learning rate: 0.00056075]
	Learning Rate: 0.00056075
	LOSS [training: 0.07170130772933536 | validation: 0.056735751298478604]
	TIME [epoch: 8.17 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07174925426276192		[learning rate: 0.00056009]
		[batch 20/20] avg loss: 0.07722932391653221		[learning rate: 0.00055943]
	Learning Rate: 0.000559427
	LOSS [training: 0.07448928908964707 | validation: 0.0861700320664838]
	TIME [epoch: 8.16 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06835317467977536		[learning rate: 0.00055877]
		[batch 20/20] avg loss: 0.07786518123273752		[learning rate: 0.00055811]
	Learning Rate: 0.000558108
	LOSS [training: 0.07310917795625645 | validation: 0.09348411793401828]
	TIME [epoch: 8.19 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07305654639731227		[learning rate: 0.00055745]
		[batch 20/20] avg loss: 0.06836268549095373		[learning rate: 0.00055679]
	Learning Rate: 0.000556791
	LOSS [training: 0.070709615944133 | validation: 0.0593556601424141]
	TIME [epoch: 8.17 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07236675683378023		[learning rate: 0.00055613]
		[batch 20/20] avg loss: 0.07567918920762681		[learning rate: 0.00055548]
	Learning Rate: 0.000555478
	LOSS [training: 0.07402297302070353 | validation: 0.07018463244895526]
	TIME [epoch: 8.16 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07280086031508155		[learning rate: 0.00055482]
		[batch 20/20] avg loss: 0.08070094022886166		[learning rate: 0.00055417]
	Learning Rate: 0.000554167
	LOSS [training: 0.0767509002719716 | validation: 0.11134387008749883]
	TIME [epoch: 8.16 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09069999925690002		[learning rate: 0.00055351]
		[batch 20/20] avg loss: 0.08500641201380063		[learning rate: 0.00055286]
	Learning Rate: 0.00055286
	LOSS [training: 0.08785320563535035 | validation: 0.08245067423431354]
	TIME [epoch: 8.18 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08838514205888431		[learning rate: 0.00055221]
		[batch 20/20] avg loss: 0.07446083733702966		[learning rate: 0.00055156]
	Learning Rate: 0.000551556
	LOSS [training: 0.081422989697957 | validation: 0.07081028195090928]
	TIME [epoch: 8.16 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0766968990437727		[learning rate: 0.00055091]
		[batch 20/20] avg loss: 0.07388910176723443		[learning rate: 0.00055026]
	Learning Rate: 0.000550255
	LOSS [training: 0.07529300040550355 | validation: 0.058916827453365454]
	TIME [epoch: 8.18 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0709708362411874		[learning rate: 0.00054961]
		[batch 20/20] avg loss: 0.0916456440234871		[learning rate: 0.00054896]
	Learning Rate: 0.000548957
	LOSS [training: 0.08130824013233726 | validation: 0.06134017398622911]
	TIME [epoch: 8.17 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05969967961634345		[learning rate: 0.00054831]
		[batch 20/20] avg loss: 0.07598969448389542		[learning rate: 0.00054766]
	Learning Rate: 0.000547662
	LOSS [training: 0.06784468705011945 | validation: 0.06251972223584995]
	TIME [epoch: 8.19 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06647920393918058		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.0791679017243401		[learning rate: 0.00054637]
	Learning Rate: 0.00054637
	LOSS [training: 0.07282355283176034 | validation: 0.056321062997580475]
	TIME [epoch: 8.17 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06599577458382402		[learning rate: 0.00054573]
		[batch 20/20] avg loss: 0.06600674799915125		[learning rate: 0.00054508]
	Learning Rate: 0.000545082
	LOSS [training: 0.06600126129148765 | validation: 0.053467271674055225]
	TIME [epoch: 8.16 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07094501551212258		[learning rate: 0.00054444]
		[batch 20/20] avg loss: 0.06994041720894774		[learning rate: 0.0005438]
	Learning Rate: 0.000543796
	LOSS [training: 0.07044271636053515 | validation: 0.048683003745526574]
	TIME [epoch: 8.16 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07375304515647983		[learning rate: 0.00054315]
		[batch 20/20] avg loss: 0.07107068892681516		[learning rate: 0.00054251]
	Learning Rate: 0.000542513
	LOSS [training: 0.07241186704164751 | validation: 0.06434064454017312]
	TIME [epoch: 8.17 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07791663922435535		[learning rate: 0.00054187]
		[batch 20/20] avg loss: 0.07059261213304314		[learning rate: 0.00054123]
	Learning Rate: 0.000541233
	LOSS [training: 0.07425462567869925 | validation: 0.05193429300982547]
	TIME [epoch: 8.18 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07627388712596052		[learning rate: 0.00054059]
		[batch 20/20] avg loss: 0.08116774141370216		[learning rate: 0.00053996]
	Learning Rate: 0.000539957
	LOSS [training: 0.07872081426983132 | validation: 0.06776708342678872]
	TIME [epoch: 8.15 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06380136185855928		[learning rate: 0.00053932]
		[batch 20/20] avg loss: 0.07085484920630235		[learning rate: 0.00053868]
	Learning Rate: 0.000538683
	LOSS [training: 0.06732810553243082 | validation: 0.059002390486093834]
	TIME [epoch: 8.16 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06642759658514978		[learning rate: 0.00053805]
		[batch 20/20] avg loss: 0.07850610903429285		[learning rate: 0.00053741]
	Learning Rate: 0.000537412
	LOSS [training: 0.0724668528097213 | validation: 0.0793139662801544]
	TIME [epoch: 8.18 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07465846171514172		[learning rate: 0.00053678]
		[batch 20/20] avg loss: 0.07601019661699195		[learning rate: 0.00053614]
	Learning Rate: 0.000536145
	LOSS [training: 0.07533432916606683 | validation: 0.05719569630986686]
	TIME [epoch: 8.16 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06449589197094403		[learning rate: 0.00053551]
		[batch 20/20] avg loss: 0.06760417132032116		[learning rate: 0.00053488]
	Learning Rate: 0.00053488
	LOSS [training: 0.0660500316456326 | validation: 0.044891580530844744]
	TIME [epoch: 8.17 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058228091413637606		[learning rate: 0.00053425]
		[batch 20/20] avg loss: 0.07961914046117975		[learning rate: 0.00053362]
	Learning Rate: 0.000533618
	LOSS [training: 0.06892361593740867 | validation: 0.05204949373468647]
	TIME [epoch: 8.16 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06651722729224155		[learning rate: 0.00053299]
		[batch 20/20] avg loss: 0.07778505953608539		[learning rate: 0.00053236]
	Learning Rate: 0.00053236
	LOSS [training: 0.07215114341416347 | validation: 0.05913012510587407]
	TIME [epoch: 8.18 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07307866870441745		[learning rate: 0.00053173]
		[batch 20/20] avg loss: 0.07124257801430074		[learning rate: 0.0005311]
	Learning Rate: 0.000531104
	LOSS [training: 0.0721606233593591 | validation: 0.05379398710315543]
	TIME [epoch: 8.17 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06297263015534653		[learning rate: 0.00053048]
		[batch 20/20] avg loss: 0.07178054795250452		[learning rate: 0.00052985]
	Learning Rate: 0.000529851
	LOSS [training: 0.06737658905392553 | validation: 0.04721475144793544]
	TIME [epoch: 8.17 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06967164244327415		[learning rate: 0.00052923]
		[batch 20/20] avg loss: 0.08297012239568338		[learning rate: 0.0005286]
	Learning Rate: 0.000528601
	LOSS [training: 0.07632088241947874 | validation: 0.06211907815374576]
	TIME [epoch: 8.16 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07210619476965811		[learning rate: 0.00052798]
		[batch 20/20] avg loss: 0.061220605237063505		[learning rate: 0.00052735]
	Learning Rate: 0.000527354
	LOSS [training: 0.06666340000336081 | validation: 0.05084599371220511]
	TIME [epoch: 8.18 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07328809587983078		[learning rate: 0.00052673]
		[batch 20/20] avg loss: 0.07604513256953646		[learning rate: 0.00052611]
	Learning Rate: 0.00052611
	LOSS [training: 0.07466661422468364 | validation: 0.08872407950350289]
	TIME [epoch: 8.18 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08638568305891808		[learning rate: 0.00052549]
		[batch 20/20] avg loss: 0.06663005001072281		[learning rate: 0.00052487]
	Learning Rate: 0.000524869
	LOSS [training: 0.07650786653482043 | validation: 0.04902269480252296]
	TIME [epoch: 8.16 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061243183674698644		[learning rate: 0.00052425]
		[batch 20/20] avg loss: 0.07882763548183508		[learning rate: 0.00052363]
	Learning Rate: 0.000523631
	LOSS [training: 0.07003540957826684 | validation: 0.08931470994394987]
	TIME [epoch: 8.16 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08050467272141909		[learning rate: 0.00052301]
		[batch 20/20] avg loss: 0.07077967639785517		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.07564217455963715 | validation: 0.0491639515547101]
	TIME [epoch: 8.17 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06227421673924022		[learning rate: 0.00052178]
		[batch 20/20] avg loss: 0.07755737161599753		[learning rate: 0.00052116]
	Learning Rate: 0.000521164
	LOSS [training: 0.06991579417761888 | validation: 0.06585983761741068]
	TIME [epoch: 8.19 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07478084794739856		[learning rate: 0.00052055]
		[batch 20/20] avg loss: 0.07421761360720196		[learning rate: 0.00051993]
	Learning Rate: 0.000519935
	LOSS [training: 0.07449923077730025 | validation: 0.06775418587225272]
	TIME [epoch: 8.16 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08635836285363904		[learning rate: 0.00051932]
		[batch 20/20] avg loss: 0.07724345352514836		[learning rate: 0.00051871]
	Learning Rate: 0.000518708
	LOSS [training: 0.08180090818939371 | validation: 0.04663688862833383]
	TIME [epoch: 8.16 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06125561110982204		[learning rate: 0.0005181]
		[batch 20/20] avg loss: 0.06468495420121975		[learning rate: 0.00051748]
	Learning Rate: 0.000517485
	LOSS [training: 0.06297028265552089 | validation: 0.05031038159213547]
	TIME [epoch: 8.16 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06811823395880752		[learning rate: 0.00051687]
		[batch 20/20] avg loss: 0.07417243714554436		[learning rate: 0.00051626]
	Learning Rate: 0.000516264
	LOSS [training: 0.07114533555217593 | validation: 0.07162233895457588]
	TIME [epoch: 8.19 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0898193156169116		[learning rate: 0.00051565]
		[batch 20/20] avg loss: 0.0791906813121454		[learning rate: 0.00051505]
	Learning Rate: 0.000515046
	LOSS [training: 0.0845049984645285 | validation: 0.06616357865154274]
	TIME [epoch: 8.16 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06699638713398666		[learning rate: 0.00051444]
		[batch 20/20] avg loss: 0.06739580619374348		[learning rate: 0.00051383]
	Learning Rate: 0.000513831
	LOSS [training: 0.06719609666386506 | validation: 0.06015101433968033]
	TIME [epoch: 8.16 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06821434677340521		[learning rate: 0.00051322]
		[batch 20/20] avg loss: 0.07253003919828109		[learning rate: 0.00051262]
	Learning Rate: 0.000512619
	LOSS [training: 0.07037219298584316 | validation: 0.05880574010092228]
	TIME [epoch: 8.17 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0826054096133817		[learning rate: 0.00051201]
		[batch 20/20] avg loss: 0.07673272510287861		[learning rate: 0.00051141]
	Learning Rate: 0.00051141
	LOSS [training: 0.07966906735813015 | validation: 0.06116675894210135]
	TIME [epoch: 8.19 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0851726493618706		[learning rate: 0.00051081]
		[batch 20/20] avg loss: 0.0653516944638702		[learning rate: 0.0005102]
	Learning Rate: 0.000510204
	LOSS [training: 0.0752621719128704 | validation: 0.08153594330981251]
	TIME [epoch: 8.17 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07237662361885194		[learning rate: 0.0005096]
		[batch 20/20] avg loss: 0.08017321468454061		[learning rate: 0.000509]
	Learning Rate: 0.000509
	LOSS [training: 0.07627491915169628 | validation: 0.06530866958884075]
	TIME [epoch: 8.16 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08468228784763372		[learning rate: 0.0005084]
		[batch 20/20] avg loss: 0.06613156625715265		[learning rate: 0.0005078]
	Learning Rate: 0.0005078
	LOSS [training: 0.07540692705239319 | validation: 0.07538180255709932]
	TIME [epoch: 8.17 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07855786448178527		[learning rate: 0.0005072]
		[batch 20/20] avg loss: 0.07537138716704274		[learning rate: 0.0005066]
	Learning Rate: 0.000506602
	LOSS [training: 0.07696462582441399 | validation: 0.06320729179427531]
	TIME [epoch: 8.18 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05842554254895625		[learning rate: 0.000506]
		[batch 20/20] avg loss: 0.07108777073415293		[learning rate: 0.00050541]
	Learning Rate: 0.000505407
	LOSS [training: 0.06475665664155458 | validation: 0.08412281515034917]
	TIME [epoch: 8.16 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06834626726183313		[learning rate: 0.00050481]
		[batch 20/20] avg loss: 0.09390935688102911		[learning rate: 0.00050421]
	Learning Rate: 0.000504215
	LOSS [training: 0.08112781207143113 | validation: 0.06481545500917893]
	TIME [epoch: 8.16 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07410819967552446		[learning rate: 0.00050362]
		[batch 20/20] avg loss: 0.07893756015909167		[learning rate: 0.00050303]
	Learning Rate: 0.000503025
	LOSS [training: 0.07652287991730806 | validation: 0.08381178600933552]
	TIME [epoch: 8.15 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06186889555398882		[learning rate: 0.00050243]
		[batch 20/20] avg loss: 0.07624629292271881		[learning rate: 0.00050184]
	Learning Rate: 0.000501839
	LOSS [training: 0.06905759423835381 | validation: 0.08395673689884411]
	TIME [epoch: 8.19 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07578556286298098		[learning rate: 0.00050125]
		[batch 20/20] avg loss: 0.07126443227448734		[learning rate: 0.00050065]
	Learning Rate: 0.000500655
	LOSS [training: 0.07352499756873417 | validation: 0.053942319238048556]
	TIME [epoch: 8.15 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07637540323208		[learning rate: 0.00050006]
		[batch 20/20] avg loss: 0.08150200219728795		[learning rate: 0.00049947]
	Learning Rate: 0.000499474
	LOSS [training: 0.07893870271468399 | validation: 0.08882945254546902]
	TIME [epoch: 8.15 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08354344851527373		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.08513038404094232		[learning rate: 0.0004983]
	Learning Rate: 0.000498296
	LOSS [training: 0.084336916278108 | validation: 0.060084067078244985]
	TIME [epoch: 8.17 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08771828765256687		[learning rate: 0.00049771]
		[batch 20/20] avg loss: 0.08234369995213789		[learning rate: 0.00049712]
	Learning Rate: 0.00049712
	LOSS [training: 0.08503099380235236 | validation: 0.052194824957428776]
	TIME [epoch: 8.19 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07277142189533126		[learning rate: 0.00049653]
		[batch 20/20] avg loss: 0.06448466544252054		[learning rate: 0.00049595]
	Learning Rate: 0.000495948
	LOSS [training: 0.06862804366892591 | validation: 0.05373335652152455]
	TIME [epoch: 8.16 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06758652980352091		[learning rate: 0.00049536]
		[batch 20/20] avg loss: 0.08574137359668654		[learning rate: 0.00049478]
	Learning Rate: 0.000494778
	LOSS [training: 0.07666395170010372 | validation: 0.05671265403618212]
	TIME [epoch: 8.16 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07082822969652332		[learning rate: 0.00049419]
		[batch 20/20] avg loss: 0.07109537345876024		[learning rate: 0.00049361]
	Learning Rate: 0.000493611
	LOSS [training: 0.07096180157764176 | validation: 0.05408141129096246]
	TIME [epoch: 8.17 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0689494530469376		[learning rate: 0.00049303]
		[batch 20/20] avg loss: 0.07293395576811137		[learning rate: 0.00049245]
	Learning Rate: 0.000492446
	LOSS [training: 0.07094170440752448 | validation: 0.09145622997418684]
	TIME [epoch: 8.18 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08022747530642518		[learning rate: 0.00049187]
		[batch 20/20] avg loss: 0.06462591582534416		[learning rate: 0.00049128]
	Learning Rate: 0.000491285
	LOSS [training: 0.07242669556588467 | validation: 0.0626875627010843]
	TIME [epoch: 8.16 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0650752474147748		[learning rate: 0.0004907]
		[batch 20/20] avg loss: 0.06749182580336073		[learning rate: 0.00049013]
	Learning Rate: 0.000490126
	LOSS [training: 0.06628353660906776 | validation: 0.04992777881510099]
	TIME [epoch: 8.16 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0766422365172345		[learning rate: 0.00048955]
		[batch 20/20] avg loss: 0.06159821318236055		[learning rate: 0.00048897]
	Learning Rate: 0.00048897
	LOSS [training: 0.06912022484979755 | validation: 0.05999798177667013]
	TIME [epoch: 8.16 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05849553421979241		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.0690538255552015		[learning rate: 0.00048782]
	Learning Rate: 0.000487816
	LOSS [training: 0.06377467988749694 | validation: 0.05405400652408093]
	TIME [epoch: 8.18 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06164176542835696		[learning rate: 0.00048724]
		[batch 20/20] avg loss: 0.07020944340271056		[learning rate: 0.00048667]
	Learning Rate: 0.000486666
	LOSS [training: 0.06592560441553377 | validation: 0.0591098593257639]
	TIME [epoch: 8.16 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07187904479598138		[learning rate: 0.00048609]
		[batch 20/20] avg loss: 0.07205774916218391		[learning rate: 0.00048552]
	Learning Rate: 0.000485518
	LOSS [training: 0.07196839697908265 | validation: 0.05168695014863266]
	TIME [epoch: 8.17 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056556275218199406		[learning rate: 0.00048494]
		[batch 20/20] avg loss: 0.06973636682460241		[learning rate: 0.00048437]
	Learning Rate: 0.000484372
	LOSS [training: 0.0631463210214009 | validation: 0.07002668041458138]
	TIME [epoch: 8.16 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0794317743062701		[learning rate: 0.0004838]
		[batch 20/20] avg loss: 0.06836200796985911		[learning rate: 0.00048323]
	Learning Rate: 0.00048323
	LOSS [training: 0.07389689113806461 | validation: 0.07439769573289995]
	TIME [epoch: 8.18 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06381787857788321		[learning rate: 0.00048266]
		[batch 20/20] avg loss: 0.06880520552445246		[learning rate: 0.00048209]
	Learning Rate: 0.00048209
	LOSS [training: 0.06631154205116783 | validation: 0.052080242852761985]
	TIME [epoch: 8.16 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07546118097206116		[learning rate: 0.00048152]
		[batch 20/20] avg loss: 0.07327217631073442		[learning rate: 0.00048095]
	Learning Rate: 0.000480953
	LOSS [training: 0.0743666786413978 | validation: 0.06532719123887859]
	TIME [epoch: 8.15 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07784849489856549		[learning rate: 0.00048039]
		[batch 20/20] avg loss: 0.07200525553966834		[learning rate: 0.00047982]
	Learning Rate: 0.000479818
	LOSS [training: 0.07492687521911692 | validation: 0.061659325730427414]
	TIME [epoch: 8.15 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07162818743696511		[learning rate: 0.00047925]
		[batch 20/20] avg loss: 0.07058053760227365		[learning rate: 0.00047869]
	Learning Rate: 0.000478687
	LOSS [training: 0.0711043625196194 | validation: 0.06078440829499198]
	TIME [epoch: 8.18 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0749320037941391		[learning rate: 0.00047812]
		[batch 20/20] avg loss: 0.07345306942372254		[learning rate: 0.00047756]
	Learning Rate: 0.000477557
	LOSS [training: 0.07419253660893081 | validation: 0.05666511134093252]
	TIME [epoch: 8.16 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06861781741802922		[learning rate: 0.00047699]
		[batch 20/20] avg loss: 0.08288890133870229		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.07575335937836576 | validation: 0.06636824736306048]
	TIME [epoch: 8.16 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08046626626523005		[learning rate: 0.00047587]
		[batch 20/20] avg loss: 0.09437812285468526		[learning rate: 0.00047531]
	Learning Rate: 0.000475307
	LOSS [training: 0.08742219455995764 | validation: 0.06296634183415632]
	TIME [epoch: 8.15 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07050904012630307		[learning rate: 0.00047475]
		[batch 20/20] avg loss: 0.06721139329766045		[learning rate: 0.00047419]
	Learning Rate: 0.000474186
	LOSS [training: 0.06886021671198175 | validation: 0.05561412741574087]
	TIME [epoch: 8.19 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06746963462892776		[learning rate: 0.00047363]
		[batch 20/20] avg loss: 0.06959721412513017		[learning rate: 0.00047307]
	Learning Rate: 0.000473067
	LOSS [training: 0.06853342437702896 | validation: 0.08518337676215905]
	TIME [epoch: 8.16 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07989902244435253		[learning rate: 0.00047251]
		[batch 20/20] avg loss: 0.06622732090261237		[learning rate: 0.00047195]
	Learning Rate: 0.000471951
	LOSS [training: 0.07306317167348246 | validation: 0.05792810778233304]
	TIME [epoch: 8.16 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07589823386060733		[learning rate: 0.00047139]
		[batch 20/20] avg loss: 0.07428122123610228		[learning rate: 0.00047084]
	Learning Rate: 0.000470838
	LOSS [training: 0.0750897275483548 | validation: 0.06140802001401003]
	TIME [epoch: 8.16 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06328469116601668		[learning rate: 0.00047028]
		[batch 20/20] avg loss: 0.0786609460432244		[learning rate: 0.00046973]
	Learning Rate: 0.000469728
	LOSS [training: 0.07097281860462054 | validation: 0.06917981042168733]
	TIME [epoch: 8.19 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06825139817431139		[learning rate: 0.00046917]
		[batch 20/20] avg loss: 0.0626990544973269		[learning rate: 0.00046862]
	Learning Rate: 0.00046862
	LOSS [training: 0.06547522633581913 | validation: 0.050370193899244194]
	TIME [epoch: 8.16 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07094900823978531		[learning rate: 0.00046807]
		[batch 20/20] avg loss: 0.07430749983370451		[learning rate: 0.00046751]
	Learning Rate: 0.000467514
	LOSS [training: 0.0726282540367449 | validation: 0.04865013167124835]
	TIME [epoch: 8.16 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05833706866900139		[learning rate: 0.00046696]
		[batch 20/20] avg loss: 0.06722088286341935		[learning rate: 0.00046641]
	Learning Rate: 0.000466411
	LOSS [training: 0.06277897576621036 | validation: 0.06666285144475215]
	TIME [epoch: 8.16 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0713757695503766		[learning rate: 0.00046586]
		[batch 20/20] avg loss: 0.08592213564301249		[learning rate: 0.00046531]
	Learning Rate: 0.000465311
	LOSS [training: 0.07864895259669455 | validation: 0.05879480179990635]
	TIME [epoch: 8.17 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404459583919755		[learning rate: 0.00046476]
		[batch 20/20] avg loss: 0.08262730429901495		[learning rate: 0.00046421]
	Learning Rate: 0.000464214
	LOSS [training: 0.07333595006910625 | validation: 0.06612052378337203]
	TIME [epoch: 8.17 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06354267204562607		[learning rate: 0.00046367]
		[batch 20/20] avg loss: 0.08550861264806042		[learning rate: 0.00046312]
	Learning Rate: 0.000463119
	LOSS [training: 0.07452564234684325 | validation: 0.0653484797852403]
	TIME [epoch: 8.16 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07706312532098601		[learning rate: 0.00046257]
		[batch 20/20] avg loss: 0.07404651493160698		[learning rate: 0.00046203]
	Learning Rate: 0.000462026
	LOSS [training: 0.07555482012629648 | validation: 0.04903089408532065]
	TIME [epoch: 8.16 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06657524449389271		[learning rate: 0.00046148]
		[batch 20/20] avg loss: 0.062298198105631544		[learning rate: 0.00046094]
	Learning Rate: 0.000460936
	LOSS [training: 0.0644367212997621 | validation: 0.05232432168557859]
	TIME [epoch: 8.18 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07012411437758229		[learning rate: 0.00046039]
		[batch 20/20] avg loss: 0.07290120396508494		[learning rate: 0.00045985]
	Learning Rate: 0.000459849
	LOSS [training: 0.07151265917133359 | validation: 0.06365750001109596]
	TIME [epoch: 8.17 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0666290777374943		[learning rate: 0.00045931]
		[batch 20/20] avg loss: 0.07717215023383965		[learning rate: 0.00045876]
	Learning Rate: 0.000458764
	LOSS [training: 0.07190061398566698 | validation: 0.04686964240165438]
	TIME [epoch: 8.16 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06514640792683485		[learning rate: 0.00045822]
		[batch 20/20] avg loss: 0.0659745941699688		[learning rate: 0.00045768]
	Learning Rate: 0.000457682
	LOSS [training: 0.06556050104840182 | validation: 0.04823286772569257]
	TIME [epoch: 8.16 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07126045860356599		[learning rate: 0.00045714]
		[batch 20/20] avg loss: 0.09622575503701022		[learning rate: 0.0004566]
	Learning Rate: 0.000456603
	LOSS [training: 0.0837431068202881 | validation: 0.06168101546497627]
	TIME [epoch: 8.18 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07153952496419104		[learning rate: 0.00045606]
		[batch 20/20] avg loss: 0.07052722534904168		[learning rate: 0.00045553]
	Learning Rate: 0.000455526
	LOSS [training: 0.07103337515661637 | validation: 0.05093026869960239]
	TIME [epoch: 8.16 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06585277279052251		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.07221170090794574		[learning rate: 0.00045445]
	Learning Rate: 0.000454451
	LOSS [training: 0.06903223684923412 | validation: 0.05437871543298354]
	TIME [epoch: 8.16 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06504423608479655		[learning rate: 0.00045391]
		[batch 20/20] avg loss: 0.07901541129142334		[learning rate: 0.00045338]
	Learning Rate: 0.000453379
	LOSS [training: 0.07202982368810995 | validation: 0.06896500373225697]
	TIME [epoch: 8.16 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07041615844802711		[learning rate: 0.00045284]
		[batch 20/20] avg loss: 0.07182105146436586		[learning rate: 0.00045231]
	Learning Rate: 0.00045231
	LOSS [training: 0.07111860495619649 | validation: 0.06661192861678883]
	TIME [epoch: 8.18 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06551299721156363		[learning rate: 0.00045178]
		[batch 20/20] avg loss: 0.07094275323661396		[learning rate: 0.00045124]
	Learning Rate: 0.000451243
	LOSS [training: 0.06822787522408882 | validation: 0.049904967887807995]
	TIME [epoch: 8.17 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0627670533873818		[learning rate: 0.00045071]
		[batch 20/20] avg loss: 0.07288824524467888		[learning rate: 0.00045018]
	Learning Rate: 0.000450178
	LOSS [training: 0.06782764931603033 | validation: 0.08445926093414885]
	TIME [epoch: 8.15 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07685854501944556		[learning rate: 0.00044965]
		[batch 20/20] avg loss: 0.07509860056875368		[learning rate: 0.00044912]
	Learning Rate: 0.000449116
	LOSS [training: 0.07597857279409961 | validation: 0.07122298765127467]
	TIME [epoch: 8.15 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10660552159373624		[learning rate: 0.00044859]
		[batch 20/20] avg loss: 0.08689055384089898		[learning rate: 0.00044806]
	Learning Rate: 0.000448057
	LOSS [training: 0.0967480377173176 | validation: 0.07067104718897782]
	TIME [epoch: 8.16 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07090156319119176		[learning rate: 0.00044753]
		[batch 20/20] avg loss: 0.07140316982731648		[learning rate: 0.000447]
	Learning Rate: 0.000447
	LOSS [training: 0.07115236650925413 | validation: 0.06567152108549823]
	TIME [epoch: 8.16 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06852131511081459		[learning rate: 0.00044647]
		[batch 20/20] avg loss: 0.07137204883514547		[learning rate: 0.00044595]
	Learning Rate: 0.000445946
	LOSS [training: 0.06994668197298003 | validation: 0.07053256657953205]
	TIME [epoch: 8.16 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07793257548714803		[learning rate: 0.00044542]
		[batch 20/20] avg loss: 0.06153258696582133		[learning rate: 0.00044489]
	Learning Rate: 0.000444894
	LOSS [training: 0.06973258122648468 | validation: 0.07790038799306018]
	TIME [epoch: 8.14 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06439372165376915		[learning rate: 0.00044437]
		[batch 20/20] avg loss: 0.06488591679093444		[learning rate: 0.00044384]
	Learning Rate: 0.000443844
	LOSS [training: 0.06463981922235179 | validation: 0.07458232908610242]
	TIME [epoch: 8.16 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06583305194941573		[learning rate: 0.00044332]
		[batch 20/20] avg loss: 0.06983409994501186		[learning rate: 0.0004428]
	Learning Rate: 0.000442797
	LOSS [training: 0.0678335759472138 | validation: 0.0653993568330506]
	TIME [epoch: 8.18 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06281696797585117		[learning rate: 0.00044227]
		[batch 20/20] avg loss: 0.06593517978993726		[learning rate: 0.00044175]
	Learning Rate: 0.000441753
	LOSS [training: 0.06437607388289421 | validation: 0.0638039934099558]
	TIME [epoch: 8.16 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07190695654735792		[learning rate: 0.00044123]
		[batch 20/20] avg loss: 0.0700767583710879		[learning rate: 0.00044071]
	Learning Rate: 0.000440711
	LOSS [training: 0.07099185745922293 | validation: 0.050912782538497285]
	TIME [epoch: 8.16 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06904534258359259		[learning rate: 0.00044019]
		[batch 20/20] avg loss: 0.07565672546717706		[learning rate: 0.00043967]
	Learning Rate: 0.000439671
	LOSS [training: 0.07235103402538481 | validation: 0.05397926694937563]
	TIME [epoch: 8.15 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06560839390038839		[learning rate: 0.00043915]
		[batch 20/20] avg loss: 0.06974617022228718		[learning rate: 0.00043863]
	Learning Rate: 0.000438634
	LOSS [training: 0.0676772820613378 | validation: 0.061811676826193845]
	TIME [epoch: 8.18 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06549744968020497		[learning rate: 0.00043812]
		[batch 20/20] avg loss: 0.0832255409002811		[learning rate: 0.0004376]
	Learning Rate: 0.0004376
	LOSS [training: 0.07436149529024304 | validation: 0.0669003964046246]
	TIME [epoch: 8.16 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06451159986447598		[learning rate: 0.00043708]
		[batch 20/20] avg loss: 0.062279056394768176		[learning rate: 0.00043657]
	Learning Rate: 0.000436567
	LOSS [training: 0.06339532812962208 | validation: 0.053073563494748205]
	TIME [epoch: 8.15 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06168745535984632		[learning rate: 0.00043605]
		[batch 20/20] avg loss: 0.05965611704521767		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.06067178620253201 | validation: 0.05432676057651496]
	TIME [epoch: 8.16 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05788834726139436		[learning rate: 0.00043502]
		[batch 20/20] avg loss: 0.07316444366396316		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.06552639546267877 | validation: 0.05171324972591215]
	TIME [epoch: 8.18 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06583070241960112		[learning rate: 0.000434]
		[batch 20/20] avg loss: 0.06232796460206362		[learning rate: 0.00043349]
	Learning Rate: 0.000433485
	LOSS [training: 0.06407933351083236 | validation: 0.06290752447842846]
	TIME [epoch: 8.16 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06294970381802335		[learning rate: 0.00043297]
		[batch 20/20] avg loss: 0.07340253628250132		[learning rate: 0.00043246]
	Learning Rate: 0.000432463
	LOSS [training: 0.06817612005026234 | validation: 0.04556479726926564]
	TIME [epoch: 8.17 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06314366514296299		[learning rate: 0.00043195]
		[batch 20/20] avg loss: 0.06804417307591476		[learning rate: 0.00043144]
	Learning Rate: 0.000431443
	LOSS [training: 0.06559391910943888 | validation: 0.06955763108233116]
	TIME [epoch: 8.15 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0655332214709061		[learning rate: 0.00043093]
		[batch 20/20] avg loss: 0.0611529378373107		[learning rate: 0.00043042]
	Learning Rate: 0.000430425
	LOSS [training: 0.06334307965410842 | validation: 0.05845847904542018]
	TIME [epoch: 8.18 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06304502724148724		[learning rate: 0.00042992]
		[batch 20/20] avg loss: 0.06349904090469263		[learning rate: 0.00042941]
	Learning Rate: 0.00042941
	LOSS [training: 0.06327203407308993 | validation: 0.07052990723604256]
	TIME [epoch: 8.16 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07366899680220915		[learning rate: 0.0004289]
		[batch 20/20] avg loss: 0.05821634318851878		[learning rate: 0.0004284]
	Learning Rate: 0.000428397
	LOSS [training: 0.06594266999536397 | validation: 0.06470043366708925]
	TIME [epoch: 8.16 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06613044101837014		[learning rate: 0.00042789]
		[batch 20/20] avg loss: 0.0659868575018385		[learning rate: 0.00042739]
	Learning Rate: 0.000427386
	LOSS [training: 0.06605864926010432 | validation: 0.06026641059358835]
	TIME [epoch: 8.16 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06726597523556688		[learning rate: 0.00042688]
		[batch 20/20] avg loss: 0.06892533735997669		[learning rate: 0.00042638]
	Learning Rate: 0.000426378
	LOSS [training: 0.06809565629777178 | validation: 0.057358735768431465]
	TIME [epoch: 8.17 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062101275853505		[learning rate: 0.00042587]
		[batch 20/20] avg loss: 0.06967360183476312		[learning rate: 0.00042537]
	Learning Rate: 0.000425372
	LOSS [training: 0.06588743884413406 | validation: 0.04860987186617374]
	TIME [epoch: 8.15 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07016360886077719		[learning rate: 0.00042487]
		[batch 20/20] avg loss: 0.06374498374406094		[learning rate: 0.00042437]
	Learning Rate: 0.000424369
	LOSS [training: 0.06695429630241907 | validation: 0.052844870218920204]
	TIME [epoch: 8.15 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05608681051745672		[learning rate: 0.00042387]
		[batch 20/20] avg loss: 0.06498858571081076		[learning rate: 0.00042337]
	Learning Rate: 0.000423368
	LOSS [training: 0.06053769811413375 | validation: 0.05650130131029209]
	TIME [epoch: 8.15 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06544555527945156		[learning rate: 0.00042287]
		[batch 20/20] avg loss: 0.06399645322439376		[learning rate: 0.00042237]
	Learning Rate: 0.000422369
	LOSS [training: 0.06472100425192266 | validation: 0.05307205249389198]
	TIME [epoch: 8.19 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06031636235949038		[learning rate: 0.00042187]
		[batch 20/20] avg loss: 0.07826771044410592		[learning rate: 0.00042137]
	Learning Rate: 0.000421373
	LOSS [training: 0.06929203640179815 | validation: 0.0862897230748364]
	TIME [epoch: 8.15 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06954887063393561		[learning rate: 0.00042088]
		[batch 20/20] avg loss: 0.07236380784958003		[learning rate: 0.00042038]
	Learning Rate: 0.000420379
	LOSS [training: 0.07095633924175783 | validation: 0.0682810240576757]
	TIME [epoch: 8.16 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062087939650347666		[learning rate: 0.00041988]
		[batch 20/20] avg loss: 0.07923799991661117		[learning rate: 0.00041939]
	Learning Rate: 0.000419387
	LOSS [training: 0.07066296978347944 | validation: 0.04690232623299626]
	TIME [epoch: 8.16 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07153496516191361		[learning rate: 0.00041889]
		[batch 20/20] avg loss: 0.07418052712583897		[learning rate: 0.0004184]
	Learning Rate: 0.000418398
	LOSS [training: 0.07285774614387629 | validation: 0.05351292035859814]
	TIME [epoch: 8.17 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06587479819040434		[learning rate: 0.0004179]
		[batch 20/20] avg loss: 0.06806033391400008		[learning rate: 0.00041741]
	Learning Rate: 0.000417411
	LOSS [training: 0.0669675660522022 | validation: 0.05166534183694177]
	TIME [epoch: 8.16 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06631415744303493		[learning rate: 0.00041692]
		[batch 20/20] avg loss: 0.07321420851659652		[learning rate: 0.00041643]
	Learning Rate: 0.000416427
	LOSS [training: 0.06976418297981571 | validation: 0.05351227473739818]
	TIME [epoch: 8.14 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06836538765860403		[learning rate: 0.00041594]
		[batch 20/20] avg loss: 0.06121950668986359		[learning rate: 0.00041544]
	Learning Rate: 0.000415444
	LOSS [training: 0.0647924471742338 | validation: 0.05500194971507312]
	TIME [epoch: 8.15 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05608211860934249		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.06190681745490695		[learning rate: 0.00041446]
	Learning Rate: 0.000414464
	LOSS [training: 0.0589944680321247 | validation: 0.04979585159711061]
	TIME [epoch: 8.18 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06669445000072588		[learning rate: 0.00041398]
		[batch 20/20] avg loss: 0.06855633696611027		[learning rate: 0.00041349]
	Learning Rate: 0.000413487
	LOSS [training: 0.06762539348341806 | validation: 0.05970381744567945]
	TIME [epoch: 8.15 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059262394835681166		[learning rate: 0.000413]
		[batch 20/20] avg loss: 0.06692714819616964		[learning rate: 0.00041251]
	Learning Rate: 0.000412511
	LOSS [training: 0.06309477151592541 | validation: 0.06812191074839008]
	TIME [epoch: 8.15 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06922910036991922		[learning rate: 0.00041202]
		[batch 20/20] avg loss: 0.06819146655412181		[learning rate: 0.00041154]
	Learning Rate: 0.000411538
	LOSS [training: 0.0687102834620205 | validation: 0.04507960827337007]
	TIME [epoch: 8.14 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06633041601639975		[learning rate: 0.00041105]
		[batch 20/20] avg loss: 0.0771764399994885		[learning rate: 0.00041057]
	Learning Rate: 0.000410568
	LOSS [training: 0.07175342800794413 | validation: 0.06187112954590025]
	TIME [epoch: 8.17 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0656333085579424		[learning rate: 0.00041008]
		[batch 20/20] avg loss: 0.06701635222289092		[learning rate: 0.0004096]
	Learning Rate: 0.000409599
	LOSS [training: 0.06632483039041667 | validation: 0.07479978017822564]
	TIME [epoch: 8.15 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07023088550137185		[learning rate: 0.00040912]
		[batch 20/20] avg loss: 0.06277450550662447		[learning rate: 0.00040863]
	Learning Rate: 0.000408633
	LOSS [training: 0.06650269550399816 | validation: 0.0488541959396081]
	TIME [epoch: 8.15 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07057214079359732		[learning rate: 0.00040815]
		[batch 20/20] avg loss: 0.05967588182057328		[learning rate: 0.00040767]
	Learning Rate: 0.000407669
	LOSS [training: 0.06512401130708531 | validation: 0.0517946358464263]
	TIME [epoch: 8.15 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06434731692853365		[learning rate: 0.00040719]
		[batch 20/20] avg loss: 0.06668561856200511		[learning rate: 0.00040671]
	Learning Rate: 0.000406707
	LOSS [training: 0.06551646774526938 | validation: 0.060769411750534255]
	TIME [epoch: 8.18 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06561704266973525		[learning rate: 0.00040623]
		[batch 20/20] avg loss: 0.0792417734837592		[learning rate: 0.00040575]
	Learning Rate: 0.000405748
	LOSS [training: 0.07242940807674722 | validation: 0.0975920391289732]
	TIME [epoch: 8.15 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06589629705800064		[learning rate: 0.00040527]
		[batch 20/20] avg loss: 0.07199351063676554		[learning rate: 0.00040479]
	Learning Rate: 0.000404791
	LOSS [training: 0.06894490384738311 | validation: 0.0672210729110089]
	TIME [epoch: 8.15 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06816379672278565		[learning rate: 0.00040431]
		[batch 20/20] avg loss: 0.07996101189120622		[learning rate: 0.00040384]
	Learning Rate: 0.000403836
	LOSS [training: 0.07406240430699593 | validation: 0.07534229101414394]
	TIME [epoch: 8.14 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07520876102591267		[learning rate: 0.00040336]
		[batch 20/20] avg loss: 0.06877085739123927		[learning rate: 0.00040288]
	Learning Rate: 0.000402883
	LOSS [training: 0.07198980920857596 | validation: 0.05831208616722069]
	TIME [epoch: 8.17 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06657066576190157		[learning rate: 0.00040241]
		[batch 20/20] avg loss: 0.05811221633148025		[learning rate: 0.00040193]
	Learning Rate: 0.000401933
	LOSS [training: 0.06234144104669091 | validation: 0.04335165440808242]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1411.pth
	Model improved!!!
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07660084390448835		[learning rate: 0.00040146]
		[batch 20/20] avg loss: 0.07323302767743337		[learning rate: 0.00040099]
	Learning Rate: 0.000400985
	LOSS [training: 0.07491693579096084 | validation: 0.06688059724681478]
	TIME [epoch: 8.15 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059426724790742146		[learning rate: 0.00040051]
		[batch 20/20] avg loss: 0.06381879442420393		[learning rate: 0.00040004]
	Learning Rate: 0.000400039
	LOSS [training: 0.06162275960747303 | validation: 0.05741269440652615]
	TIME [epoch: 8.14 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07427278729969544		[learning rate: 0.00039957]
		[batch 20/20] avg loss: 0.06389551518538428		[learning rate: 0.0003991]
	Learning Rate: 0.000399096
	LOSS [training: 0.06908415124253985 | validation: 0.0638423581444229]
	TIME [epoch: 8.17 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06602127951076542		[learning rate: 0.00039862]
		[batch 20/20] avg loss: 0.07768525672207802		[learning rate: 0.00039815]
	Learning Rate: 0.000398154
	LOSS [training: 0.0718532681164217 | validation: 0.0660401192045327]
	TIME [epoch: 8.16 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06357372234581213		[learning rate: 0.00039768]
		[batch 20/20] avg loss: 0.06510741292735897		[learning rate: 0.00039721]
	Learning Rate: 0.000397215
	LOSS [training: 0.06434056763658555 | validation: 0.04765940772262136]
	TIME [epoch: 8.15 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07597779353912378		[learning rate: 0.00039675]
		[batch 20/20] avg loss: 0.07190322071733939		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.07394050712823158 | validation: 0.061481175493962016]
	TIME [epoch: 8.16 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07106339147449199		[learning rate: 0.00039581]
		[batch 20/20] avg loss: 0.07950628244438798		[learning rate: 0.00039534]
	Learning Rate: 0.000395343
	LOSS [training: 0.07528483695944 | validation: 0.05791092189324383]
	TIME [epoch: 8.18 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07425419278613134		[learning rate: 0.00039488]
		[batch 20/20] avg loss: 0.0674634758487525		[learning rate: 0.00039441]
	Learning Rate: 0.000394411
	LOSS [training: 0.07085883431744192 | validation: 0.06006190876421445]
	TIME [epoch: 8.14 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06677040369953384		[learning rate: 0.00039395]
		[batch 20/20] avg loss: 0.0832607125330713		[learning rate: 0.00039348]
	Learning Rate: 0.00039348
	LOSS [training: 0.07501555811630256 | validation: 0.06113335035027407]
	TIME [epoch: 8.15 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0763116221213608		[learning rate: 0.00039302]
		[batch 20/20] avg loss: 0.06548563851410852		[learning rate: 0.00039255]
	Learning Rate: 0.000392552
	LOSS [training: 0.07089863031773466 | validation: 0.04592989397179363]
	TIME [epoch: 8.14 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059433242551714574		[learning rate: 0.00039209]
		[batch 20/20] avg loss: 0.07309122292623685		[learning rate: 0.00039163]
	Learning Rate: 0.000391626
	LOSS [training: 0.06626223273897572 | validation: 0.05131389967599244]
	TIME [epoch: 8.17 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062267256679133964		[learning rate: 0.00039116]
		[batch 20/20] avg loss: 0.06189843945922291		[learning rate: 0.0003907]
	Learning Rate: 0.000390702
	LOSS [training: 0.06208284806917843 | validation: 0.05922579822694624]
	TIME [epoch: 8.15 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061899797235956934		[learning rate: 0.00039024]
		[batch 20/20] avg loss: 0.06744101455914502		[learning rate: 0.00038978]
	Learning Rate: 0.000389781
	LOSS [training: 0.06467040589755099 | validation: 0.06037931589525372]
	TIME [epoch: 8.15 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06125391876582377		[learning rate: 0.00038932]
		[batch 20/20] avg loss: 0.06972688417475119		[learning rate: 0.00038886]
	Learning Rate: 0.000388861
	LOSS [training: 0.06549040147028747 | validation: 0.0625975396062844]
	TIME [epoch: 8.15 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06941474451566065		[learning rate: 0.0003884]
		[batch 20/20] avg loss: 0.0955593270437491		[learning rate: 0.00038794]
	Learning Rate: 0.000387944
	LOSS [training: 0.08248703577970487 | validation: 0.052269801104463304]
	TIME [epoch: 8.16 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06493040968633927		[learning rate: 0.00038749]
		[batch 20/20] avg loss: 0.06347644237808056		[learning rate: 0.00038703]
	Learning Rate: 0.000387029
	LOSS [training: 0.06420342603220991 | validation: 0.0470209082187988]
	TIME [epoch: 8.15 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06773096496173606		[learning rate: 0.00038657]
		[batch 20/20] avg loss: 0.06126683564129666		[learning rate: 0.00038612]
	Learning Rate: 0.000386116
	LOSS [training: 0.06449890030151637 | validation: 0.06839443857815361]
	TIME [epoch: 8.15 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05684017846625479		[learning rate: 0.00038566]
		[batch 20/20] avg loss: 0.0666263863003136		[learning rate: 0.00038521]
	Learning Rate: 0.000385205
	LOSS [training: 0.061733282383284196 | validation: 0.0641946177014066]
	TIME [epoch: 8.15 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07200174402670283		[learning rate: 0.00038475]
		[batch 20/20] avg loss: 0.06243123345683187		[learning rate: 0.0003843]
	Learning Rate: 0.000384297
	LOSS [training: 0.06721648874176736 | validation: 0.05693379698009299]
	TIME [epoch: 8.16 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07896876725238919		[learning rate: 0.00038384]
		[batch 20/20] avg loss: 0.06314632501033743		[learning rate: 0.00038339]
	Learning Rate: 0.00038339
	LOSS [training: 0.0710575461313633 | validation: 0.07657928904988981]
	TIME [epoch: 8.15 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08035089141687947		[learning rate: 0.00038294]
		[batch 20/20] avg loss: 0.06452755002570373		[learning rate: 0.00038249]
	Learning Rate: 0.000382486
	LOSS [training: 0.07243922072129161 | validation: 0.07194354807095009]
	TIME [epoch: 8.14 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07139085776983754		[learning rate: 0.00038203]
		[batch 20/20] avg loss: 0.06524139201147812		[learning rate: 0.00038158]
	Learning Rate: 0.000381584
	LOSS [training: 0.06831612489065783 | validation: 0.051489343438128776]
	TIME [epoch: 8.16 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06595304149182522		[learning rate: 0.00038113]
		[batch 20/20] avg loss: 0.058457226682510556		[learning rate: 0.00038068]
	Learning Rate: 0.000380684
	LOSS [training: 0.06220513408716788 | validation: 0.053477782603520825]
	TIME [epoch: 8.16 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06548447143863997		[learning rate: 0.00038023]
		[batch 20/20] avg loss: 0.06814368187891127		[learning rate: 0.00037979]
	Learning Rate: 0.000379786
	LOSS [training: 0.06681407665877563 | validation: 0.0549574812988286]
	TIME [epoch: 8.16 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05945966254273612		[learning rate: 0.00037934]
		[batch 20/20] avg loss: 0.0634725010399663		[learning rate: 0.00037889]
	Learning Rate: 0.00037889
	LOSS [training: 0.061466081791351226 | validation: 0.06395328746805949]
	TIME [epoch: 8.15 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06790330454548613		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.05542695819989152		[learning rate: 0.000378]
	Learning Rate: 0.000377996
	LOSS [training: 0.06166513137268883 | validation: 0.04181624864563727]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1437.pth
	Model improved!!!
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05263328366706312		[learning rate: 0.00037755]
		[batch 20/20] avg loss: 0.0749011273002588		[learning rate: 0.0003771]
	Learning Rate: 0.000377104
	LOSS [training: 0.06376720548366097 | validation: 0.05470609097989056]
	TIME [epoch: 8.18 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06128659323433262		[learning rate: 0.00037666]
		[batch 20/20] avg loss: 0.06564338084310932		[learning rate: 0.00037621]
	Learning Rate: 0.000376215
	LOSS [training: 0.06346498703872097 | validation: 0.04989533221750207]
	TIME [epoch: 8.16 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060390061788443486		[learning rate: 0.00037577]
		[batch 20/20] avg loss: 0.06615736994748805		[learning rate: 0.00037533]
	Learning Rate: 0.000375327
	LOSS [training: 0.06327371586796576 | validation: 0.06291754579039385]
	TIME [epoch: 8.15 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06886051784474284		[learning rate: 0.00037488]
		[batch 20/20] avg loss: 0.061572541171878235		[learning rate: 0.00037444]
	Learning Rate: 0.000374442
	LOSS [training: 0.06521652950831056 | validation: 0.05759330583268729]
	TIME [epoch: 8.15 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0701607613794156		[learning rate: 0.000374]
		[batch 20/20] avg loss: 0.05909722103626695		[learning rate: 0.00037356]
	Learning Rate: 0.000373559
	LOSS [training: 0.06462899120784128 | validation: 0.053488631499561325]
	TIME [epoch: 8.16 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06319146848405703		[learning rate: 0.00037312]
		[batch 20/20] avg loss: 0.06764039355750794		[learning rate: 0.00037268]
	Learning Rate: 0.000372678
	LOSS [training: 0.0654159310207825 | validation: 0.06705502884926795]
	TIME [epoch: 8.16 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07192926790920634		[learning rate: 0.00037224]
		[batch 20/20] avg loss: 0.06912490685893635		[learning rate: 0.0003718]
	Learning Rate: 0.000371799
	LOSS [training: 0.07052708738407135 | validation: 0.05612131763957734]
	TIME [epoch: 8.15 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06211831294185936		[learning rate: 0.00037136]
		[batch 20/20] avg loss: 0.07777048486565945		[learning rate: 0.00037092]
	Learning Rate: 0.000370922
	LOSS [training: 0.06994439890375942 | validation: 0.06830399802524868]
	TIME [epoch: 8.13 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06890911073995833		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.0697654142256463		[learning rate: 0.00037005]
	Learning Rate: 0.000370047
	LOSS [training: 0.06933726248280231 | validation: 0.05603665268963845]
	TIME [epoch: 8.16 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05455581613827848		[learning rate: 0.00036961]
		[batch 20/20] avg loss: 0.08366070294888003		[learning rate: 0.00036917]
	Learning Rate: 0.000369174
	LOSS [training: 0.06910825954357927 | validation: 0.06502098349203025]
	TIME [epoch: 8.16 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07987627972198676		[learning rate: 0.00036874]
		[batch 20/20] avg loss: 0.07242897300060705		[learning rate: 0.0003683]
	Learning Rate: 0.000368303
	LOSS [training: 0.07615262636129691 | validation: 0.06303852703469681]
	TIME [epoch: 8.15 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06321587785499684		[learning rate: 0.00036787]
		[batch 20/20] avg loss: 0.07353813558693842		[learning rate: 0.00036743]
	Learning Rate: 0.000367434
	LOSS [training: 0.06837700672096762 | validation: 0.05902215781825498]
	TIME [epoch: 8.15 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06647155493769535		[learning rate: 0.000367]
		[batch 20/20] avg loss: 0.05396780051866421		[learning rate: 0.00036657]
	Learning Rate: 0.000366567
	LOSS [training: 0.06021967772817979 | validation: 0.060227945603285316]
	TIME [epoch: 8.15 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06330183340094775		[learning rate: 0.00036613]
		[batch 20/20] avg loss: 0.07505452711575644		[learning rate: 0.0003657]
	Learning Rate: 0.000365703
	LOSS [training: 0.0691781802583521 | validation: 0.07516507471883878]
	TIME [epoch: 8.18 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07138635091775179		[learning rate: 0.00036527]
		[batch 20/20] avg loss: 0.06411359046307032		[learning rate: 0.00036484]
	Learning Rate: 0.00036484
	LOSS [training: 0.06774997069041105 | validation: 0.07381458152480275]
	TIME [epoch: 8.14 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06770263907678853		[learning rate: 0.00036441]
		[batch 20/20] avg loss: 0.06584832210414544		[learning rate: 0.00036398]
	Learning Rate: 0.000363979
	LOSS [training: 0.06677548059046698 | validation: 0.05162873232942927]
	TIME [epoch: 8.14 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06422917868822914		[learning rate: 0.00036355]
		[batch 20/20] avg loss: 0.08757726880250533		[learning rate: 0.00036312]
	Learning Rate: 0.000363121
	LOSS [training: 0.07590322374536723 | validation: 0.042218047707872325]
	TIME [epoch: 8.15 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06256459040106571		[learning rate: 0.00036269]
		[batch 20/20] avg loss: 0.06964769168418528		[learning rate: 0.00036226]
	Learning Rate: 0.000362264
	LOSS [training: 0.0661061410426255 | validation: 0.06909651619026716]
	TIME [epoch: 8.17 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05993741196610038		[learning rate: 0.00036184]
		[batch 20/20] avg loss: 0.05988234657670264		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.0599098792714015 | validation: 0.06546299562255337]
	TIME [epoch: 8.15 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06300334511326279		[learning rate: 0.00036098]
		[batch 20/20] avg loss: 0.05936242984489003		[learning rate: 0.00036056]
	Learning Rate: 0.000360557
	LOSS [training: 0.06118288747907641 | validation: 0.04952147639811517]
	TIME [epoch: 8.14 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0689189505308327		[learning rate: 0.00036013]
		[batch 20/20] avg loss: 0.0564184529343039		[learning rate: 0.00035971]
	Learning Rate: 0.000359707
	LOSS [training: 0.06266870173256829 | validation: 0.047489321605593944]
	TIME [epoch: 8.15 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060553108594247215		[learning rate: 0.00035928]
		[batch 20/20] avg loss: 0.058996185396663		[learning rate: 0.00035886]
	Learning Rate: 0.000358858
	LOSS [training: 0.059774646995455094 | validation: 0.06793548273146267]
	TIME [epoch: 8.17 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08932990376384833		[learning rate: 0.00035843]
		[batch 20/20] avg loss: 0.06383734250438246		[learning rate: 0.00035801]
	Learning Rate: 0.000358012
	LOSS [training: 0.0765836231341154 | validation: 0.055782868787912956]
	TIME [epoch: 8.14 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0603025693237466		[learning rate: 0.00035759]
		[batch 20/20] avg loss: 0.057820950226684174		[learning rate: 0.00035717]
	Learning Rate: 0.000357167
	LOSS [training: 0.0590617597752154 | validation: 0.06124866938315493]
	TIME [epoch: 8.15 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.064013920438455		[learning rate: 0.00035675]
		[batch 20/20] avg loss: 0.07093150862594003		[learning rate: 0.00035632]
	Learning Rate: 0.000356325
	LOSS [training: 0.06747271453219753 | validation: 0.05045351448844629]
	TIME [epoch: 8.15 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06114619025286198		[learning rate: 0.0003559]
		[batch 20/20] avg loss: 0.06226970475089667		[learning rate: 0.00035548]
	Learning Rate: 0.000355484
	LOSS [training: 0.06170794750187932 | validation: 0.05334826756436994]
	TIME [epoch: 8.17 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05678692969435247		[learning rate: 0.00035506]
		[batch 20/20] avg loss: 0.06847025503298801		[learning rate: 0.00035465]
	Learning Rate: 0.000354646
	LOSS [training: 0.06262859236367026 | validation: 0.056347916682463466]
	TIME [epoch: 8.15 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06493145614848031		[learning rate: 0.00035423]
		[batch 20/20] avg loss: 0.0652143078198312		[learning rate: 0.00035381]
	Learning Rate: 0.000353809
	LOSS [training: 0.06507288198415576 | validation: 0.05817089770145739]
	TIME [epoch: 8.15 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06198707673038887		[learning rate: 0.00035339]
		[batch 20/20] avg loss: 0.06121143047460871		[learning rate: 0.00035297]
	Learning Rate: 0.000352975
	LOSS [training: 0.061599253602498794 | validation: 0.046714322525551986]
	TIME [epoch: 8.15 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06416122471135983		[learning rate: 0.00035256]
		[batch 20/20] avg loss: 0.07272524593768294		[learning rate: 0.00035214]
	Learning Rate: 0.000352142
	LOSS [training: 0.06844323532452137 | validation: 0.04925696154612627]
	TIME [epoch: 8.18 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06778130230111047		[learning rate: 0.00035173]
		[batch 20/20] avg loss: 0.061043748863793934		[learning rate: 0.00035131]
	Learning Rate: 0.000351311
	LOSS [training: 0.06441252558245221 | validation: 0.05059989042380063]
	TIME [epoch: 8.15 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06187342896406904		[learning rate: 0.0003509]
		[batch 20/20] avg loss: 0.06561121660555289		[learning rate: 0.00035048]
	Learning Rate: 0.000350483
	LOSS [training: 0.06374232278481096 | validation: 0.060972537310135244]
	TIME [epoch: 8.15 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06405513106579395		[learning rate: 0.00035007]
		[batch 20/20] avg loss: 0.06205306992071356		[learning rate: 0.00034966]
	Learning Rate: 0.000349656
	LOSS [training: 0.06305410049325376 | validation: 0.05643931571881948]
	TIME [epoch: 8.14 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05941441178769501		[learning rate: 0.00034924]
		[batch 20/20] avg loss: 0.07442999629300637		[learning rate: 0.00034883]
	Learning Rate: 0.000348831
	LOSS [training: 0.06692220404035071 | validation: 0.05519332697956823]
	TIME [epoch: 8.17 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05824831337254015		[learning rate: 0.00034842]
		[batch 20/20] avg loss: 0.05767273304612726		[learning rate: 0.00034801]
	Learning Rate: 0.000348008
	LOSS [training: 0.05796052320933372 | validation: 0.0530576983229991]
	TIME [epoch: 8.15 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0566838445848117		[learning rate: 0.0003476]
		[batch 20/20] avg loss: 0.05913903434897787		[learning rate: 0.00034719]
	Learning Rate: 0.000347187
	LOSS [training: 0.05791143946689479 | validation: 0.05939488882413342]
	TIME [epoch: 8.15 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06306119481874126		[learning rate: 0.00034678]
		[batch 20/20] avg loss: 0.05932161755557589		[learning rate: 0.00034637]
	Learning Rate: 0.000346369
	LOSS [training: 0.06119140618715857 | validation: 0.0556813819998352]
	TIME [epoch: 8.16 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06353718305961585		[learning rate: 0.00034596]
		[batch 20/20] avg loss: 0.05729189078660527		[learning rate: 0.00034555]
	Learning Rate: 0.000345552
	LOSS [training: 0.060414536923110565 | validation: 0.04369639217969393]
	TIME [epoch: 8.16 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07246854148449852		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.055575176172158756		[learning rate: 0.00034474]
	Learning Rate: 0.000344736
	LOSS [training: 0.06402185882832864 | validation: 0.05404394434195703]
	TIME [epoch: 8.15 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0668576971242923		[learning rate: 0.00034433]
		[batch 20/20] avg loss: 0.06207315336878394		[learning rate: 0.00034392]
	Learning Rate: 0.000343923
	LOSS [training: 0.06446542524653813 | validation: 0.05814104219810304]
	TIME [epoch: 8.14 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05954726499655564		[learning rate: 0.00034352]
		[batch 20/20] avg loss: 0.06694354939396792		[learning rate: 0.00034311]
	Learning Rate: 0.000343112
	LOSS [training: 0.06324540719526177 | validation: 0.057497368210825114]
	TIME [epoch: 8.16 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057410762178239795		[learning rate: 0.00034271]
		[batch 20/20] avg loss: 0.07061933206399072		[learning rate: 0.0003423]
	Learning Rate: 0.000342303
	LOSS [training: 0.06401504712111528 | validation: 0.07494791981370857]
	TIME [epoch: 8.16 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06567003206014355		[learning rate: 0.0003419]
		[batch 20/20] avg loss: 0.058152807857134094		[learning rate: 0.0003415]
	Learning Rate: 0.000341495
	LOSS [training: 0.061911419958638816 | validation: 0.04932900075944009]
	TIME [epoch: 8.15 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0664155030357232		[learning rate: 0.00034109]
		[batch 20/20] avg loss: 0.05625665919081885		[learning rate: 0.00034069]
	Learning Rate: 0.00034069
	LOSS [training: 0.06133608111327101 | validation: 0.04506258575002707]
	TIME [epoch: 8.14 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07147700402019559		[learning rate: 0.00034029]
		[batch 20/20] avg loss: 0.055877642239037906		[learning rate: 0.00033989]
	Learning Rate: 0.000339886
	LOSS [training: 0.06367732312961674 | validation: 0.045574700706576904]
	TIME [epoch: 8.15 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06472981901326973		[learning rate: 0.00033948]
		[batch 20/20] avg loss: 0.06337327548437025		[learning rate: 0.00033908]
	Learning Rate: 0.000339084
	LOSS [training: 0.06405154724881998 | validation: 0.05105952376818347]
	TIME [epoch: 8.16 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06574919264806772		[learning rate: 0.00033868]
		[batch 20/20] avg loss: 0.06383118518016623		[learning rate: 0.00033828]
	Learning Rate: 0.000338284
	LOSS [training: 0.06479018891411699 | validation: 0.05568004546455224]
	TIME [epoch: 8.15 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06203620881717479		[learning rate: 0.00033789]
		[batch 20/20] avg loss: 0.06180911542501462		[learning rate: 0.00033749]
	Learning Rate: 0.000337487
	LOSS [training: 0.06192266212109471 | validation: 0.04683460358841467]
	TIME [epoch: 8.14 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059688332664407984		[learning rate: 0.00033709]
		[batch 20/20] avg loss: 0.06092834786110435		[learning rate: 0.00033669]
	Learning Rate: 0.00033669
	LOSS [training: 0.060308340262756166 | validation: 0.04691990118278419]
	TIME [epoch: 8.15 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06181278166365802		[learning rate: 0.00033629]
		[batch 20/20] avg loss: 0.06021733005879809		[learning rate: 0.0003359]
	Learning Rate: 0.000335896
	LOSS [training: 0.06101505586122806 | validation: 0.04684666098381498]
	TIME [epoch: 8.16 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06752992431361884		[learning rate: 0.0003355]
		[batch 20/20] avg loss: 0.06482226997694561		[learning rate: 0.0003351]
	Learning Rate: 0.000335104
	LOSS [training: 0.06617609714528222 | validation: 0.07457559993523796]
	TIME [epoch: 8.16 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06494759179780854		[learning rate: 0.00033471]
		[batch 20/20] avg loss: 0.07051089873161434		[learning rate: 0.00033431]
	Learning Rate: 0.000334313
	LOSS [training: 0.06772924526471144 | validation: 0.06115972257246783]
	TIME [epoch: 8.14 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07030448203465992		[learning rate: 0.00033392]
		[batch 20/20] avg loss: 0.056495006692931726		[learning rate: 0.00033352]
	Learning Rate: 0.000333525
	LOSS [training: 0.06339974436379583 | validation: 0.0693564034325812]
	TIME [epoch: 8.15 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07418160712032781		[learning rate: 0.00033313]
		[batch 20/20] avg loss: 0.06574074623056966		[learning rate: 0.00033274]
	Learning Rate: 0.000332738
	LOSS [training: 0.06996117667544874 | validation: 0.045205309888209255]
	TIME [epoch: 8.17 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06589970728178718		[learning rate: 0.00033235]
		[batch 20/20] avg loss: 0.06444976773616048		[learning rate: 0.00033195]
	Learning Rate: 0.000331953
	LOSS [training: 0.06517473750897383 | validation: 0.05349047301866616]
	TIME [epoch: 8.15 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06786658924709617		[learning rate: 0.00033156]
		[batch 20/20] avg loss: 0.060815635629082584		[learning rate: 0.00033117]
	Learning Rate: 0.00033117
	LOSS [training: 0.06434111243808938 | validation: 0.053504721181988936]
	TIME [epoch: 8.14 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06968900786639433		[learning rate: 0.00033078]
		[batch 20/20] avg loss: 0.07211982776589804		[learning rate: 0.00033039]
	Learning Rate: 0.000330389
	LOSS [training: 0.07090441781614618 | validation: 0.05678131111379062]
	TIME [epoch: 8.15 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06156816282968963		[learning rate: 0.00033]
		[batch 20/20] avg loss: 0.07056087445525161		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.06606451864247062 | validation: 0.05434661595802415]
	TIME [epoch: 8.17 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07228647832410594		[learning rate: 0.00032922]
		[batch 20/20] avg loss: 0.057821641015462354		[learning rate: 0.00032883]
	Learning Rate: 0.000328832
	LOSS [training: 0.06505405966978414 | validation: 0.05639268126366853]
	TIME [epoch: 8.16 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06505409453028851		[learning rate: 0.00032844]
		[batch 20/20] avg loss: 0.0621882090121217		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.06362115177120509 | validation: 0.0425363688347833]
	TIME [epoch: 8.14 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06401635767894273		[learning rate: 0.00032767]
		[batch 20/20] avg loss: 0.0631392716347418		[learning rate: 0.00032728]
	Learning Rate: 0.000327283
	LOSS [training: 0.06357781465684229 | validation: 0.05801046081711704]
	TIME [epoch: 8.16 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06412076728360142		[learning rate: 0.0003269]
		[batch 20/20] avg loss: 0.06771346496900733		[learning rate: 0.00032651]
	Learning Rate: 0.000326511
	LOSS [training: 0.06591711612630438 | validation: 0.05029621644749475]
	TIME [epoch: 8.17 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06430607337288359		[learning rate: 0.00032613]
		[batch 20/20] avg loss: 0.055221919093472314		[learning rate: 0.00032574]
	Learning Rate: 0.00032574
	LOSS [training: 0.05976399623317794 | validation: 0.046363310150819355]
	TIME [epoch: 8.17 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06473839093795994		[learning rate: 0.00032536]
		[batch 20/20] avg loss: 0.06301324661432925		[learning rate: 0.00032497]
	Learning Rate: 0.000324972
	LOSS [training: 0.0638758187761446 | validation: 0.05080553836768259]
	TIME [epoch: 8.15 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05960839817121871		[learning rate: 0.00032459]
		[batch 20/20] avg loss: 0.06073403195132478		[learning rate: 0.00032421]
	Learning Rate: 0.000324206
	LOSS [training: 0.060171215061271745 | validation: 0.05149669410982685]
	TIME [epoch: 8.15 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07020834510119235		[learning rate: 0.00032382]
		[batch 20/20] avg loss: 0.05853815540095829		[learning rate: 0.00032344]
	Learning Rate: 0.000323441
	LOSS [training: 0.06437325025107532 | validation: 0.05261604100469661]
	TIME [epoch: 8.16 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0570758615393542		[learning rate: 0.00032306]
		[batch 20/20] avg loss: 0.06640282375865017		[learning rate: 0.00032268]
	Learning Rate: 0.000322678
	LOSS [training: 0.061739342649002174 | validation: 0.06712513234908968]
	TIME [epoch: 8.16 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0654323533542203		[learning rate: 0.0003223]
		[batch 20/20] avg loss: 0.056250471873008835		[learning rate: 0.00032192]
	Learning Rate: 0.000321917
	LOSS [training: 0.06084141261361457 | validation: 0.05034410346424726]
	TIME [epoch: 8.14 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05542321255124054		[learning rate: 0.00032154]
		[batch 20/20] avg loss: 0.05928700510630005		[learning rate: 0.00032116]
	Learning Rate: 0.000321157
	LOSS [training: 0.05735510882877028 | validation: 0.03921145868383177]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1506.pth
	Model improved!!!
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0587889273852172		[learning rate: 0.00032078]
		[batch 20/20] avg loss: 0.06612452503850426		[learning rate: 0.0003204]
	Learning Rate: 0.0003204
	LOSS [training: 0.06245672621186073 | validation: 0.056628012896176444]
	TIME [epoch: 8.16 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07022467436148214		[learning rate: 0.00032002]
		[batch 20/20] avg loss: 0.06400168414752562		[learning rate: 0.00031964]
	Learning Rate: 0.000319644
	LOSS [training: 0.06711317925450387 | validation: 0.058068272906422844]
	TIME [epoch: 8.15 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0581133309090253		[learning rate: 0.00031927]
		[batch 20/20] avg loss: 0.05385721630705288		[learning rate: 0.00031889]
	Learning Rate: 0.00031889
	LOSS [training: 0.0559852736080391 | validation: 0.04903841168857593]
	TIME [epoch: 8.14 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06089355632555225		[learning rate: 0.00031851]
		[batch 20/20] avg loss: 0.06570177670074452		[learning rate: 0.00031814]
	Learning Rate: 0.000318138
	LOSS [training: 0.0632976665131484 | validation: 0.053177821547492676]
	TIME [epoch: 8.15 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0508587385957994		[learning rate: 0.00031776]
		[batch 20/20] avg loss: 0.058317589135387836		[learning rate: 0.00031739]
	Learning Rate: 0.000317387
	LOSS [training: 0.05458816386559362 | validation: 0.07103696297846834]
	TIME [epoch: 8.15 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05927473205788815		[learning rate: 0.00031701]
		[batch 20/20] avg loss: 0.0638312462154679		[learning rate: 0.00031664]
	Learning Rate: 0.000316639
	LOSS [training: 0.061552989136678025 | validation: 0.05456729030946327]
	TIME [epoch: 8.16 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07146137962322616		[learning rate: 0.00031627]
		[batch 20/20] avg loss: 0.06251248546006635		[learning rate: 0.00031589]
	Learning Rate: 0.000315892
	LOSS [training: 0.06698693254164625 | validation: 0.03701071962751451]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1513.pth
	Model improved!!!
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05950482033494384		[learning rate: 0.00031552]
		[batch 20/20] avg loss: 0.05842523471227369		[learning rate: 0.00031515]
	Learning Rate: 0.000315147
	LOSS [training: 0.05896502752360876 | validation: 0.04412822076373231]
	TIME [epoch: 8.16 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07863959277120187		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.07225121633663402		[learning rate: 0.0003144]
	Learning Rate: 0.000314403
	LOSS [training: 0.07544540455391795 | validation: 0.059933161603967285]
	TIME [epoch: 8.15 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06460345759485213		[learning rate: 0.00031403]
		[batch 20/20] avg loss: 0.05750990167248883		[learning rate: 0.00031366]
	Learning Rate: 0.000313662
	LOSS [training: 0.06105667963367049 | validation: 0.05077664860650575]
	TIME [epoch: 8.16 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0674998170167933		[learning rate: 0.00031329]
		[batch 20/20] avg loss: 0.06215495879042778		[learning rate: 0.00031292]
	Learning Rate: 0.000312922
	LOSS [training: 0.06482738790361056 | validation: 0.03730740858362248]
	TIME [epoch: 8.14 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052984482359124584		[learning rate: 0.00031255]
		[batch 20/20] avg loss: 0.06017335364781501		[learning rate: 0.00031218]
	Learning Rate: 0.000312184
	LOSS [training: 0.05657891800346979 | validation: 0.044010136397672095]
	TIME [epoch: 8.15 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06193690950857239		[learning rate: 0.00031182]
		[batch 20/20] avg loss: 0.06265693747156023		[learning rate: 0.00031145]
	Learning Rate: 0.000311447
	LOSS [training: 0.06229692349006631 | validation: 0.05674390597166684]
	TIME [epoch: 8.14 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.064174493127233		[learning rate: 0.00031108]
		[batch 20/20] avg loss: 0.06137104358035402		[learning rate: 0.00031071]
	Learning Rate: 0.000310713
	LOSS [training: 0.0627727683537935 | validation: 0.041064122713713]
	TIME [epoch: 8.16 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058446948073422066		[learning rate: 0.00031035]
		[batch 20/20] avg loss: 0.06796110041270463		[learning rate: 0.00030998]
	Learning Rate: 0.00030998
	LOSS [training: 0.06320402424306334 | validation: 0.06203843502931798]
	TIME [epoch: 8.14 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05981099579801565		[learning rate: 0.00030961]
		[batch 20/20] avg loss: 0.05634661588361004		[learning rate: 0.00030925]
	Learning Rate: 0.000309249
	LOSS [training: 0.058078805840812854 | validation: 0.05286331898650077]
	TIME [epoch: 8.14 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05833218667357566		[learning rate: 0.00030888]
		[batch 20/20] avg loss: 0.06210149323068923		[learning rate: 0.00030852]
	Learning Rate: 0.000308519
	LOSS [training: 0.06021683995213245 | validation: 0.06049873281401713]
	TIME [epoch: 8.15 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06313850785028147		[learning rate: 0.00030815]
		[batch 20/20] avg loss: 0.07061334536342541		[learning rate: 0.00030779]
	Learning Rate: 0.000307791
	LOSS [training: 0.06687592660685343 | validation: 0.05846320299094073]
	TIME [epoch: 8.16 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06253221186235974		[learning rate: 0.00030743]
		[batch 20/20] avg loss: 0.06131208903446811		[learning rate: 0.00030707]
	Learning Rate: 0.000307065
	LOSS [training: 0.061922150448413926 | validation: 0.08082156245051586]
	TIME [epoch: 8.13 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06596517715681811		[learning rate: 0.0003067]
		[batch 20/20] avg loss: 0.06405495316399172		[learning rate: 0.00030634]
	Learning Rate: 0.000306341
	LOSS [training: 0.06501006516040492 | validation: 0.052810994365154205]
	TIME [epoch: 8.14 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06517819324347818		[learning rate: 0.00030598]
		[batch 20/20] avg loss: 0.06772690166905929		[learning rate: 0.00030562]
	Learning Rate: 0.000305618
	LOSS [training: 0.06645254745626875 | validation: 0.04630384021698051]
	TIME [epoch: 8.15 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06631640519867495		[learning rate: 0.00030526]
		[batch 20/20] avg loss: 0.06778695704511382		[learning rate: 0.0003049]
	Learning Rate: 0.000304897
	LOSS [training: 0.06705168112189439 | validation: 0.05660530187891776]
	TIME [epoch: 8.16 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07142246312803636		[learning rate: 0.00030454]
		[batch 20/20] avg loss: 0.06749459754842786		[learning rate: 0.00030418]
	Learning Rate: 0.000304178
	LOSS [training: 0.06945853033823211 | validation: 0.05585348328424233]
	TIME [epoch: 8.14 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06755476626899341		[learning rate: 0.00030382]
		[batch 20/20] avg loss: 0.059933327538126045		[learning rate: 0.00030346]
	Learning Rate: 0.000303461
	LOSS [training: 0.06374404690355974 | validation: 0.04752010630517822]
	TIME [epoch: 8.14 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06007333853429192		[learning rate: 0.0003031]
		[batch 20/20] avg loss: 0.056751667080575385		[learning rate: 0.00030274]
	Learning Rate: 0.000302745
	LOSS [training: 0.05841250280743365 | validation: 0.052091949062285536]
	TIME [epoch: 8.14 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06620353365218187		[learning rate: 0.00030239]
		[batch 20/20] avg loss: 0.05983087749747941		[learning rate: 0.00030203]
	Learning Rate: 0.000302031
	LOSS [training: 0.06301720557483065 | validation: 0.06857530989054113]
	TIME [epoch: 8.16 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06938286172591145		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 0.061384054991949655		[learning rate: 0.00030132]
	Learning Rate: 0.000301318
	LOSS [training: 0.06538345835893054 | validation: 0.03885220442214736]
	TIME [epoch: 8.15 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06121134550307493		[learning rate: 0.00030096]
		[batch 20/20] avg loss: 0.06116759588795899		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.06118947069551696 | validation: 0.05148112402872064]
	TIME [epoch: 8.13 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058477984380580515		[learning rate: 0.00030025]
		[batch 20/20] avg loss: 0.06574724132865428		[learning rate: 0.0002999]
	Learning Rate: 0.000299899
	LOSS [training: 0.06211261285461739 | validation: 0.048004149391251805]
	TIME [epoch: 8.15 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05346606504233612		[learning rate: 0.00029954]
		[batch 20/20] avg loss: 0.06347170536680449		[learning rate: 0.00029919]
	Learning Rate: 0.000299191
	LOSS [training: 0.058468885204570296 | validation: 0.059065424042249015]
	TIME [epoch: 8.16 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06865436173575175		[learning rate: 0.00029884]
		[batch 20/20] avg loss: 0.05979602345996891		[learning rate: 0.00029849]
	Learning Rate: 0.000298485
	LOSS [training: 0.06422519259786033 | validation: 0.06674719950181605]
	TIME [epoch: 8.15 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06407526825755443		[learning rate: 0.00029813]
		[batch 20/20] avg loss: 0.06413845822953705		[learning rate: 0.00029778]
	Learning Rate: 0.000297781
	LOSS [training: 0.06410686324354574 | validation: 0.04387744652270727]
	TIME [epoch: 8.14 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05462871640761655		[learning rate: 0.00029743]
		[batch 20/20] avg loss: 0.05761631719551299		[learning rate: 0.00029708]
	Learning Rate: 0.000297079
	LOSS [training: 0.056122516801564784 | validation: 0.05296435708956191]
	TIME [epoch: 8.15 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05472627132232667		[learning rate: 0.00029673]
		[batch 20/20] avg loss: 0.061222133909482626		[learning rate: 0.00029638]
	Learning Rate: 0.000296378
	LOSS [training: 0.05797420261590465 | validation: 0.043992548523144256]
	TIME [epoch: 8.17 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06452253781822839		[learning rate: 0.00029603]
		[batch 20/20] avg loss: 0.06499417222209808		[learning rate: 0.00029568]
	Learning Rate: 0.000295679
	LOSS [training: 0.06475835502016322 | validation: 0.05219455534291211]
	TIME [epoch: 8.15 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06732426168814132		[learning rate: 0.00029533]
		[batch 20/20] avg loss: 0.05269297908980029		[learning rate: 0.00029498]
	Learning Rate: 0.000294982
	LOSS [training: 0.06000862038897081 | validation: 0.05248316307274017]
	TIME [epoch: 8.14 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06579432963489885		[learning rate: 0.00029463]
		[batch 20/20] avg loss: 0.054830937436790995		[learning rate: 0.00029429]
	Learning Rate: 0.000294286
	LOSS [training: 0.06031263353584493 | validation: 0.052235916052545524]
	TIME [epoch: 8.14 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06274987947398827		[learning rate: 0.00029394]
		[batch 20/20] avg loss: 0.058286231558629685		[learning rate: 0.00029359]
	Learning Rate: 0.000293592
	LOSS [training: 0.06051805551630897 | validation: 0.061664629230387694]
	TIME [epoch: 8.16 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06233522611684968		[learning rate: 0.00029325]
		[batch 20/20] avg loss: 0.06014000494832883		[learning rate: 0.0002929]
	Learning Rate: 0.000292899
	LOSS [training: 0.06123761553258925 | validation: 0.0534081082116018]
	TIME [epoch: 8.14 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055774344326906276		[learning rate: 0.00029255]
		[batch 20/20] avg loss: 0.06077389054360276		[learning rate: 0.00029221]
	Learning Rate: 0.000292208
	LOSS [training: 0.058274117435254534 | validation: 0.04146375182534104]
	TIME [epoch: 8.14 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06541085135554112		[learning rate: 0.00029186]
		[batch 20/20] avg loss: 0.06436456963721446		[learning rate: 0.00029152]
	Learning Rate: 0.000291519
	LOSS [training: 0.06488771049637779 | validation: 0.056268565426232986]
	TIME [epoch: 8.15 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049782110130631226		[learning rate: 0.00029117]
		[batch 20/20] avg loss: 0.06580059779990996		[learning rate: 0.00029083]
	Learning Rate: 0.000290831
	LOSS [training: 0.057791353965270596 | validation: 0.053974166865276084]
	TIME [epoch: 8.17 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05528191489449415		[learning rate: 0.00029049]
		[batch 20/20] avg loss: 0.07264362480312486		[learning rate: 0.00029015]
	Learning Rate: 0.000290145
	LOSS [training: 0.0639627698488095 | validation: 0.07434010182110526]
	TIME [epoch: 8.15 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06512092506575888		[learning rate: 0.0002898]
		[batch 20/20] avg loss: 0.06315654395497147		[learning rate: 0.00028946]
	Learning Rate: 0.000289461
	LOSS [training: 0.06413873451036518 | validation: 0.058077889542859966]
	TIME [epoch: 8.14 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054619184359123676		[learning rate: 0.00028912]
		[batch 20/20] avg loss: 0.06693265613867684		[learning rate: 0.00028878]
	Learning Rate: 0.000288778
	LOSS [training: 0.06077592024890025 | validation: 0.05589255639502382]
	TIME [epoch: 8.15 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06261345450823982		[learning rate: 0.00028844]
		[batch 20/20] avg loss: 0.060337436743610515		[learning rate: 0.0002881]
	Learning Rate: 0.000288097
	LOSS [training: 0.06147544562592516 | validation: 0.04469312407185384]
	TIME [epoch: 8.17 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060358897102993005		[learning rate: 0.00028776]
		[batch 20/20] avg loss: 0.05469392832297141		[learning rate: 0.00028742]
	Learning Rate: 0.000287417
	LOSS [training: 0.0575264127129822 | validation: 0.04883572686429825]
	TIME [epoch: 8.15 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06512470505792263		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.05293659582228986		[learning rate: 0.00028674]
	Learning Rate: 0.000286739
	LOSS [training: 0.05903065044010623 | validation: 0.044204300094649276]
	TIME [epoch: 8.15 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05862761902832142		[learning rate: 0.0002864]
		[batch 20/20] avg loss: 0.06995515122897603		[learning rate: 0.00028606]
	Learning Rate: 0.000286063
	LOSS [training: 0.06429138512864874 | validation: 0.045410782212138456]
	TIME [epoch: 8.14 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062252189026211865		[learning rate: 0.00028573]
		[batch 20/20] avg loss: 0.05902842737427418		[learning rate: 0.00028539]
	Learning Rate: 0.000285388
	LOSS [training: 0.060640308200243034 | validation: 0.04667383056462106]
	TIME [epoch: 8.17 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060360853291452375		[learning rate: 0.00028505]
		[batch 20/20] avg loss: 0.0671548376314879		[learning rate: 0.00028471]
	Learning Rate: 0.000284715
	LOSS [training: 0.06375784546147013 | validation: 0.046376916273165175]
	TIME [epoch: 8.16 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06152270098956403		[learning rate: 0.00028438]
		[batch 20/20] avg loss: 0.059864950442638974		[learning rate: 0.00028404]
	Learning Rate: 0.000284043
	LOSS [training: 0.06069382571610149 | validation: 0.04140326666549711]
	TIME [epoch: 8.13 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060438159019106805		[learning rate: 0.00028371]
		[batch 20/20] avg loss: 0.061074420162812414		[learning rate: 0.00028337]
	Learning Rate: 0.000283373
	LOSS [training: 0.0607562895909596 | validation: 0.04719274519730472]
	TIME [epoch: 8.15 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058095728188054294		[learning rate: 0.00028304]
		[batch 20/20] avg loss: 0.0630869437498387		[learning rate: 0.0002827]
	Learning Rate: 0.000282705
	LOSS [training: 0.0605913359689465 | validation: 0.052253543801036265]
	TIME [epoch: 8.16 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056465020608924624		[learning rate: 0.00028237]
		[batch 20/20] avg loss: 0.06743615596457261		[learning rate: 0.00028204]
	Learning Rate: 0.000282038
	LOSS [training: 0.06195058828674861 | validation: 0.061658321542261585]
	TIME [epoch: 8.16 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06331372998093618		[learning rate: 0.00028171]
		[batch 20/20] avg loss: 0.06530025585007562		[learning rate: 0.00028137]
	Learning Rate: 0.000281373
	LOSS [training: 0.06430699291550593 | validation: 0.03899447041905671]
	TIME [epoch: 8.15 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060117210186444246		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.06839437185598973		[learning rate: 0.00028071]
	Learning Rate: 0.000280709
	LOSS [training: 0.06425579102121699 | validation: 0.04375307609861086]
	TIME [epoch: 8.15 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058663203954974576		[learning rate: 0.00028038]
		[batch 20/20] avg loss: 0.05654950437138986		[learning rate: 0.00028005]
	Learning Rate: 0.000280047
	LOSS [training: 0.057606354163182216 | validation: 0.04992525200106321]
	TIME [epoch: 8.16 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05921182745086647		[learning rate: 0.00027972]
		[batch 20/20] avg loss: 0.05981918433211862		[learning rate: 0.00027939]
	Learning Rate: 0.000279386
	LOSS [training: 0.05951550589149255 | validation: 0.04982570553499074]
	TIME [epoch: 8.15 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06395011197477452		[learning rate: 0.00027906]
		[batch 20/20] avg loss: 0.05207091104573682		[learning rate: 0.00027873]
	Learning Rate: 0.000278727
	LOSS [training: 0.05801051151025567 | validation: 0.039723058773631456]
	TIME [epoch: 8.16 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05346369678059727		[learning rate: 0.0002784]
		[batch 20/20] avg loss: 0.06038541311658398		[learning rate: 0.00027807]
	Learning Rate: 0.00027807
	LOSS [training: 0.056924554948590625 | validation: 0.052940870943613286]
	TIME [epoch: 8.14 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05448152552375409		[learning rate: 0.00027774]
		[batch 20/20] avg loss: 0.05628799469222971		[learning rate: 0.00027741]
	Learning Rate: 0.000277414
	LOSS [training: 0.05538476010799189 | validation: 0.04407427859024544]
	TIME [epoch: 8.15 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05481245129557455		[learning rate: 0.00027709]
		[batch 20/20] avg loss: 0.058159796450904425		[learning rate: 0.00027676]
	Learning Rate: 0.000276759
	LOSS [training: 0.05648612387323948 | validation: 0.05031820376420758]
	TIME [epoch: 8.15 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05704394521231217		[learning rate: 0.00027643]
		[batch 20/20] avg loss: 0.04942226257144473		[learning rate: 0.00027611]
	Learning Rate: 0.000276107
	LOSS [training: 0.05323310389187845 | validation: 0.05412086818942299]
	TIME [epoch: 8.16 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0594074711330946		[learning rate: 0.00027578]
		[batch 20/20] avg loss: 0.060270727631749996		[learning rate: 0.00027546]
	Learning Rate: 0.000275455
	LOSS [training: 0.05983909938242231 | validation: 0.05122393081702124]
	TIME [epoch: 8.14 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058108419180566175		[learning rate: 0.00027513]
		[batch 20/20] avg loss: 0.059687485619668565		[learning rate: 0.00027481]
	Learning Rate: 0.000274806
	LOSS [training: 0.05889795240011737 | validation: 0.05305233065988075]
	TIME [epoch: 8.16 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061021016256172525		[learning rate: 0.00027448]
		[batch 20/20] avg loss: 0.06417416146692254		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.06259758886154754 | validation: 0.0530449080229729]
	TIME [epoch: 8.16 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.064263946450995		[learning rate: 0.00027383]
		[batch 20/20] avg loss: 0.06246141383831858		[learning rate: 0.00027351]
	Learning Rate: 0.000273511
	LOSS [training: 0.06336268014465679 | validation: 0.05272050555395913]
	TIME [epoch: 8.15 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05943237015615395		[learning rate: 0.00027319]
		[batch 20/20] avg loss: 0.06559437948537677		[learning rate: 0.00027287]
	Learning Rate: 0.000272866
	LOSS [training: 0.06251337482076537 | validation: 0.04810957084441266]
	TIME [epoch: 8.13 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06855546469769505		[learning rate: 0.00027254]
		[batch 20/20] avg loss: 0.05742748123960893		[learning rate: 0.00027222]
	Learning Rate: 0.000272222
	LOSS [training: 0.06299147296865198 | validation: 0.035040964260767984]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1576.pth
	Model improved!!!
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06344339905632043		[learning rate: 0.0002719]
		[batch 20/20] avg loss: 0.056256284582567276		[learning rate: 0.00027158]
	Learning Rate: 0.00027158
	LOSS [training: 0.05984984181944385 | validation: 0.048940452886135556]
	TIME [epoch: 8.17 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059067029131366244		[learning rate: 0.00027126]
		[batch 20/20] avg loss: 0.06104975950031906		[learning rate: 0.00027094]
	Learning Rate: 0.000270939
	LOSS [training: 0.060058394315842645 | validation: 0.04764731046845479]
	TIME [epoch: 8.14 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059750813513744803		[learning rate: 0.00027062]
		[batch 20/20] avg loss: 0.05161180517871248		[learning rate: 0.0002703]
	Learning Rate: 0.0002703
	LOSS [training: 0.05568130934622865 | validation: 0.043921612684918655]
	TIME [epoch: 8.15 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06637522044155325		[learning rate: 0.00026998]
		[batch 20/20] avg loss: 0.05844538667058559		[learning rate: 0.00026966]
	Learning Rate: 0.000269662
	LOSS [training: 0.06241030355606941 | validation: 0.05572003269401734]
	TIME [epoch: 8.16 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06455435014183952		[learning rate: 0.00026934]
		[batch 20/20] avg loss: 0.06259051071012489		[learning rate: 0.00026903]
	Learning Rate: 0.000269026
	LOSS [training: 0.06357243042598221 | validation: 0.04923187673989228]
	TIME [epoch: 8.15 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0542703655414273		[learning rate: 0.00026871]
		[batch 20/20] avg loss: 0.05646330848460611		[learning rate: 0.00026839]
	Learning Rate: 0.000268392
	LOSS [training: 0.0553668370130167 | validation: 0.04753272027762602]
	TIME [epoch: 8.15 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06468116260091003		[learning rate: 0.00026808]
		[batch 20/20] avg loss: 0.05515133973009737		[learning rate: 0.00026776]
	Learning Rate: 0.000267759
	LOSS [training: 0.0599162511655037 | validation: 0.05086106465039897]
	TIME [epoch: 8.14 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061818765020843866		[learning rate: 0.00026744]
		[batch 20/20] avg loss: 0.07166659662411064		[learning rate: 0.00026713]
	Learning Rate: 0.000267127
	LOSS [training: 0.06674268082247725 | validation: 0.05711093796237584]
	TIME [epoch: 8.16 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06261963951906149		[learning rate: 0.00026681]
		[batch 20/20] avg loss: 0.060488614949005946		[learning rate: 0.0002665]
	Learning Rate: 0.000266497
	LOSS [training: 0.06155412723403371 | validation: 0.046348615700924065]
	TIME [epoch: 8.15 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062025776989820834		[learning rate: 0.00026618]
		[batch 20/20] avg loss: 0.06077166923890452		[learning rate: 0.00026587]
	Learning Rate: 0.000265868
	LOSS [training: 0.061398723114362674 | validation: 0.04423061794587651]
	TIME [epoch: 8.13 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055391634594505655		[learning rate: 0.00026555]
		[batch 20/20] avg loss: 0.06323650979922649		[learning rate: 0.00026524]
	Learning Rate: 0.000265241
	LOSS [training: 0.059314072196866074 | validation: 0.04370048869574332]
	TIME [epoch: 8.14 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05972422217031935		[learning rate: 0.00026493]
		[batch 20/20] avg loss: 0.060399739257406035		[learning rate: 0.00026462]
	Learning Rate: 0.000264616
	LOSS [training: 0.06006198071386269 | validation: 0.07580078987797939]
	TIME [epoch: 8.15 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07976489024890891		[learning rate: 0.0002643]
		[batch 20/20] avg loss: 0.05984783113819514		[learning rate: 0.00026399]
	Learning Rate: 0.000263991
	LOSS [training: 0.069806360693552 | validation: 0.050806538359735476]
	TIME [epoch: 8.17 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05814491910270699		[learning rate: 0.00026368]
		[batch 20/20] avg loss: 0.06107668510565935		[learning rate: 0.00026337]
	Learning Rate: 0.000263369
	LOSS [training: 0.05961080210418318 | validation: 0.0510019178066491]
	TIME [epoch: 8.17 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05670141366745799		[learning rate: 0.00026306]
		[batch 20/20] avg loss: 0.05374785802460212		[learning rate: 0.00026275]
	Learning Rate: 0.000262747
	LOSS [training: 0.05522463584603007 | validation: 0.050332630415853785]
	TIME [epoch: 8.15 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05662677962050318		[learning rate: 0.00026244]
		[batch 20/20] avg loss: 0.05778513341697037		[learning rate: 0.00026213]
	Learning Rate: 0.000262128
	LOSS [training: 0.05720595651873676 | validation: 0.048603204823553486]
	TIME [epoch: 8.16 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060164604751577457		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.0558974477544534		[learning rate: 0.00026151]
	Learning Rate: 0.000261509
	LOSS [training: 0.05803102625301543 | validation: 0.04835077121502625]
	TIME [epoch: 8.16 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06610063295553116		[learning rate: 0.0002612]
		[batch 20/20] avg loss: 0.05506813253121541		[learning rate: 0.00026089]
	Learning Rate: 0.000260892
	LOSS [training: 0.06058438274337328 | validation: 0.03830525135265776]
	TIME [epoch: 8.15 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05556279551382025		[learning rate: 0.00026058]
		[batch 20/20] avg loss: 0.05583607670488051		[learning rate: 0.00026028]
	Learning Rate: 0.000260277
	LOSS [training: 0.05569943610935037 | validation: 0.05265058851057314]
	TIME [epoch: 8.14 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0631736098242722		[learning rate: 0.00025997]
		[batch 20/20] avg loss: 0.05886873460992511		[learning rate: 0.00025966]
	Learning Rate: 0.000259663
	LOSS [training: 0.06102117221709864 | validation: 0.0557892661721264]
	TIME [epoch: 8.14 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05767428247145491		[learning rate: 0.00025936]
		[batch 20/20] avg loss: 0.05458278279664763		[learning rate: 0.00025905]
	Learning Rate: 0.000259051
	LOSS [training: 0.056128532634051265 | validation: 0.05462461960250303]
	TIME [epoch: 8.17 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05241434976162013		[learning rate: 0.00025874]
		[batch 20/20] avg loss: 0.06319069957483003		[learning rate: 0.00025844]
	Learning Rate: 0.00025844
	LOSS [training: 0.05780252466822507 | validation: 0.0585769159800325]
	TIME [epoch: 8.14 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0662317780759714		[learning rate: 0.00025813]
		[batch 20/20] avg loss: 0.06325305961246455		[learning rate: 0.00025783]
	Learning Rate: 0.00025783
	LOSS [training: 0.06474241884421798 | validation: 0.05927707604571922]
	TIME [epoch: 8.15 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05976171632251459		[learning rate: 0.00025753]
		[batch 20/20] avg loss: 0.05608217077111236		[learning rate: 0.00025722]
	Learning Rate: 0.000257222
	LOSS [training: 0.05792194354681348 | validation: 0.034174517511481164]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1600.pth
	Model improved!!!
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05574579423843131		[learning rate: 0.00025692]
		[batch 20/20] avg loss: 0.06767865967698261		[learning rate: 0.00025662]
	Learning Rate: 0.000256615
	LOSS [training: 0.061712226957706964 | validation: 0.04351429446457725]
	TIME [epoch: 8.17 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07398613769143483		[learning rate: 0.00025631]
		[batch 20/20] avg loss: 0.06442722057991128		[learning rate: 0.00025601]
	Learning Rate: 0.00025601
	LOSS [training: 0.06920667913567306 | validation: 0.055354882533846475]
	TIME [epoch: 8.14 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058835262537641396		[learning rate: 0.00025571]
		[batch 20/20] avg loss: 0.058228535691200387		[learning rate: 0.00025541]
	Learning Rate: 0.000255406
	LOSS [training: 0.058531899114420895 | validation: 0.047814579859503124]
	TIME [epoch: 8.15 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05967558088827211		[learning rate: 0.0002551]
		[batch 20/20] avg loss: 0.06260176491302509		[learning rate: 0.0002548]
	Learning Rate: 0.000254803
	LOSS [training: 0.061138672900648595 | validation: 0.04550231976723339]
	TIME [epoch: 8.14 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05034685315042805		[learning rate: 0.0002545]
		[batch 20/20] avg loss: 0.05784021847478689		[learning rate: 0.0002542]
	Learning Rate: 0.000254202
	LOSS [training: 0.05409353581260747 | validation: 0.056498999591185414]
	TIME [epoch: 8.16 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06015702239402394		[learning rate: 0.0002539]
		[batch 20/20] avg loss: 0.06490731567335357		[learning rate: 0.0002536]
	Learning Rate: 0.000253603
	LOSS [training: 0.06253216903368876 | validation: 0.05649987356704733]
	TIME [epoch: 8.15 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056562394486915725		[learning rate: 0.0002533]
		[batch 20/20] avg loss: 0.059792856584880635		[learning rate: 0.000253]
	Learning Rate: 0.000253004
	LOSS [training: 0.058177625535898184 | validation: 0.06079627981677027]
	TIME [epoch: 8.15 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06313954015655612		[learning rate: 0.00025271]
		[batch 20/20] avg loss: 0.059122853059291325		[learning rate: 0.00025241]
	Learning Rate: 0.000252408
	LOSS [training: 0.061131196607923724 | validation: 0.050967421409144106]
	TIME [epoch: 8.15 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055398121794602836		[learning rate: 0.00025211]
		[batch 20/20] avg loss: 0.05862703594148595		[learning rate: 0.00025181]
	Learning Rate: 0.000251812
	LOSS [training: 0.05701257886804438 | validation: 0.04317537236474245]
	TIME [epoch: 8.16 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05861035820692802		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 0.05136945059997576		[learning rate: 0.00025122]
	Learning Rate: 0.000251218
	LOSS [training: 0.05498990440345188 | validation: 0.05325378082488879]
	TIME [epoch: 8.16 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05119071331683199		[learning rate: 0.00025092]
		[batch 20/20] avg loss: 0.06410803845881077		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.057649375887821375 | validation: 0.05358493324561159]
	TIME [epoch: 8.14 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05403910769440151		[learning rate: 0.00025033]
		[batch 20/20] avg loss: 0.057661373656450673		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.055850240675426086 | validation: 0.05602450156284543]
	TIME [epoch: 8.15 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07191641745717317		[learning rate: 0.00024974]
		[batch 20/20] avg loss: 0.06138092913836084		[learning rate: 0.00024944]
	Learning Rate: 0.000249445
	LOSS [training: 0.066648673297767 | validation: 0.06540520047805311]
	TIME [epoch: 8.18 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06485810984968378		[learning rate: 0.00024915]
		[batch 20/20] avg loss: 0.06210893094089971		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.06348352039529173 | validation: 0.04674241502501864]
	TIME [epoch: 8.15 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057938608441077934		[learning rate: 0.00024856]
		[batch 20/20] avg loss: 0.055572308471513655		[learning rate: 0.00024827]
	Learning Rate: 0.000248269
	LOSS [training: 0.056755458456295795 | validation: 0.050383669768302775]
	TIME [epoch: 8.15 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0542623799819435		[learning rate: 0.00024798]
		[batch 20/20] avg loss: 0.06470033577376129		[learning rate: 0.00024768]
	Learning Rate: 0.000247684
	LOSS [training: 0.0594813578778524 | validation: 0.05581752812196317]
	TIME [epoch: 8.14 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05956672478977848		[learning rate: 0.00024739]
		[batch 20/20] avg loss: 0.06031595897789429		[learning rate: 0.0002471]
	Learning Rate: 0.000247099
	LOSS [training: 0.059941341883836395 | validation: 0.051633787679634446]
	TIME [epoch: 8.18 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055976649216596105		[learning rate: 0.00024681]
		[batch 20/20] avg loss: 0.05418898463656756		[learning rate: 0.00024652]
	Learning Rate: 0.000246517
	LOSS [training: 0.05508281692658181 | validation: 0.05554042863201341]
	TIME [epoch: 8.15 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059357461199827634		[learning rate: 0.00024623]
		[batch 20/20] avg loss: 0.05146331600023833		[learning rate: 0.00024594]
	Learning Rate: 0.000245935
	LOSS [training: 0.05541038860003298 | validation: 0.04905810033781549]
	TIME [epoch: 8.15 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05477056992926145		[learning rate: 0.00024564]
		[batch 20/20] avg loss: 0.07155288873050492		[learning rate: 0.00024535]
	Learning Rate: 0.000245355
	LOSS [training: 0.06316172932988318 | validation: 0.07484159316249298]
	TIME [epoch: 8.14 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07189869521442774		[learning rate: 0.00024507]
		[batch 20/20] avg loss: 0.053726608639739194		[learning rate: 0.00024478]
	Learning Rate: 0.000244776
	LOSS [training: 0.06281265192708345 | validation: 0.0452913434140249]
	TIME [epoch: 8.17 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06176447404327664		[learning rate: 0.00024449]
		[batch 20/20] avg loss: 0.05417655200618331		[learning rate: 0.0002442]
	Learning Rate: 0.000244199
	LOSS [training: 0.057970513024729976 | validation: 0.04762709813841779]
	TIME [epoch: 8.15 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05075879296518067		[learning rate: 0.00024391]
		[batch 20/20] avg loss: 0.05894723682389229		[learning rate: 0.00024362]
	Learning Rate: 0.000243623
	LOSS [training: 0.054853014894536464 | validation: 0.06278228603216494]
	TIME [epoch: 8.15 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055636777569835136		[learning rate: 0.00024334]
		[batch 20/20] avg loss: 0.06414105481727317		[learning rate: 0.00024305]
	Learning Rate: 0.000243048
	LOSS [training: 0.05988891619355415 | validation: 0.041827077128886066]
	TIME [epoch: 8.15 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05013306840313144		[learning rate: 0.00024276]
		[batch 20/20] avg loss: 0.05162958090603718		[learning rate: 0.00024247]
	Learning Rate: 0.000242475
	LOSS [training: 0.050881324654584316 | validation: 0.04862114892626288]
	TIME [epoch: 8.17 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06670228548250297		[learning rate: 0.00024219]
		[batch 20/20] avg loss: 0.04770192010401168		[learning rate: 0.0002419]
	Learning Rate: 0.000241903
	LOSS [training: 0.05720210279325731 | validation: 0.055615556060215496]
	TIME [epoch: 8.15 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051832864797530284		[learning rate: 0.00024162]
		[batch 20/20] avg loss: 0.0627928652846366		[learning rate: 0.00024133]
	Learning Rate: 0.000241332
	LOSS [training: 0.05731286504108344 | validation: 0.04431244145551848]
	TIME [epoch: 8.14 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05815964641383411		[learning rate: 0.00024105]
		[batch 20/20] avg loss: 0.053949674892967384		[learning rate: 0.00024076]
	Learning Rate: 0.000240763
	LOSS [training: 0.056054660653400756 | validation: 0.044788381413194786]
	TIME [epoch: 8.15 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052896695743464936		[learning rate: 0.00024048]
		[batch 20/20] avg loss: 0.05363409006084231		[learning rate: 0.0002402]
	Learning Rate: 0.000240195
	LOSS [training: 0.05326539290215363 | validation: 0.052723680938825936]
	TIME [epoch: 8.16 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06638613622065306		[learning rate: 0.00023991]
		[batch 20/20] avg loss: 0.058454730077467945		[learning rate: 0.00023963]
	Learning Rate: 0.000239628
	LOSS [training: 0.062420433149060504 | validation: 0.045465172608217065]
	TIME [epoch: 8.14 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05500511231223174		[learning rate: 0.00023935]
		[batch 20/20] avg loss: 0.0470913206595571		[learning rate: 0.00023906]
	Learning Rate: 0.000239063
	LOSS [training: 0.05104821648589443 | validation: 0.053758510011884114]
	TIME [epoch: 8.14 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055612568124398964		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.05034938476392968		[learning rate: 0.0002385]
	Learning Rate: 0.000238499
	LOSS [training: 0.05298097644416433 | validation: 0.04050780588837174]
	TIME [epoch: 8.14 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051045187150287066		[learning rate: 0.00023822]
		[batch 20/20] avg loss: 0.057277426634961034		[learning rate: 0.00023794]
	Learning Rate: 0.000237937
	LOSS [training: 0.054161306892624037 | validation: 0.04883944567057927]
	TIME [epoch: 8.18 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051121398742752934		[learning rate: 0.00023766]
		[batch 20/20] avg loss: 0.05429875836386282		[learning rate: 0.00023738]
	Learning Rate: 0.000237375
	LOSS [training: 0.05271007855330788 | validation: 0.041692331254448524]
	TIME [epoch: 8.16 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0568478161279769		[learning rate: 0.0002371]
		[batch 20/20] avg loss: 0.06546006952263608		[learning rate: 0.00023682]
	Learning Rate: 0.000236816
	LOSS [training: 0.06115394282530649 | validation: 0.05665501987727225]
	TIME [epoch: 8.16 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06867495286773276		[learning rate: 0.00023654]
		[batch 20/20] avg loss: 0.060668316567464575		[learning rate: 0.00023626]
	Learning Rate: 0.000236257
	LOSS [training: 0.06467163471759867 | validation: 0.05848793439282812]
	TIME [epoch: 8.14 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05814783296704157		[learning rate: 0.00023598]
		[batch 20/20] avg loss: 0.059200409163086144		[learning rate: 0.0002357]
	Learning Rate: 0.0002357
	LOSS [training: 0.05867412106506387 | validation: 0.044112076561636085]
	TIME [epoch: 8.17 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05384820616638697		[learning rate: 0.00023542]
		[batch 20/20] avg loss: 0.04917188152922527		[learning rate: 0.00023514]
	Learning Rate: 0.000235144
	LOSS [training: 0.05151004384780612 | validation: 0.04468130751079383]
	TIME [epoch: 8.16 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05441691576675599		[learning rate: 0.00023487]
		[batch 20/20] avg loss: 0.05694652232521428		[learning rate: 0.00023459]
	Learning Rate: 0.000234589
	LOSS [training: 0.055681719045985144 | validation: 0.044220500274072844]
	TIME [epoch: 8.15 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061003451792816986		[learning rate: 0.00023431]
		[batch 20/20] avg loss: 0.05536432615001672		[learning rate: 0.00023404]
	Learning Rate: 0.000234036
	LOSS [training: 0.05818388897141687 | validation: 0.05293471337524794]
	TIME [epoch: 8.14 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04959071348977986		[learning rate: 0.00023376]
		[batch 20/20] avg loss: 0.05154093823211966		[learning rate: 0.00023348]
	Learning Rate: 0.000233484
	LOSS [training: 0.05056582586094975 | validation: 0.049702926208242154]
	TIME [epoch: 8.17 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05254047029680312		[learning rate: 0.00023321]
		[batch 20/20] avg loss: 0.056250083855474434		[learning rate: 0.00023293]
	Learning Rate: 0.000232933
	LOSS [training: 0.05439527707613877 | validation: 0.053777185138761396]
	TIME [epoch: 8.16 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06452503433218046		[learning rate: 0.00023266]
		[batch 20/20] avg loss: 0.05600095058866639		[learning rate: 0.00023238]
	Learning Rate: 0.000232383
	LOSS [training: 0.060262992460423416 | validation: 0.04565625685454864]
	TIME [epoch: 8.14 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05267875859238387		[learning rate: 0.00023211]
		[batch 20/20] avg loss: 0.055226514028296404		[learning rate: 0.00023184]
	Learning Rate: 0.000231835
	LOSS [training: 0.05395263631034014 | validation: 0.05239917181478666]
	TIME [epoch: 8.14 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05527281436074749		[learning rate: 0.00023156]
		[batch 20/20] avg loss: 0.048170245764520225		[learning rate: 0.00023129]
	Learning Rate: 0.000231288
	LOSS [training: 0.05172153006263386 | validation: 0.04840069958592586]
	TIME [epoch: 8.16 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05959454955261023		[learning rate: 0.00023102]
		[batch 20/20] avg loss: 0.05659063140717888		[learning rate: 0.00023074]
	Learning Rate: 0.000230743
	LOSS [training: 0.05809259047989457 | validation: 0.04153247627108001]
	TIME [epoch: 8.15 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051656259473763955		[learning rate: 0.00023047]
		[batch 20/20] avg loss: 0.05809487922368618		[learning rate: 0.0002302]
	Learning Rate: 0.000230198
	LOSS [training: 0.05487556934872506 | validation: 0.048696712565407446]
	TIME [epoch: 8.14 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06160127449508218		[learning rate: 0.00022993]
		[batch 20/20] avg loss: 0.06692772692682644		[learning rate: 0.00022966]
	Learning Rate: 0.000229656
	LOSS [training: 0.06426450071095433 | validation: 0.050796119961561935]
	TIME [epoch: 8.15 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05194241894069387		[learning rate: 0.00022938]
		[batch 20/20] avg loss: 0.056787245584628845		[learning rate: 0.00022911]
	Learning Rate: 0.000229114
	LOSS [training: 0.05436483226266135 | validation: 0.049052548736779396]
	TIME [epoch: 8.16 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054252984750960374		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 0.0609817321170355		[learning rate: 0.00022857]
	Learning Rate: 0.000228573
	LOSS [training: 0.057617358433997935 | validation: 0.046425317249928794]
	TIME [epoch: 8.16 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046065105735575526		[learning rate: 0.0002283]
		[batch 20/20] avg loss: 0.061226111115913806		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.05364560842574467 | validation: 0.05353491604098245]
	TIME [epoch: 8.14 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061000189186394736		[learning rate: 0.00022777]
		[batch 20/20] avg loss: 0.05649644627051429		[learning rate: 0.0002275]
	Learning Rate: 0.000227496
	LOSS [training: 0.05874831772845451 | validation: 0.04923355775886151]
	TIME [epoch: 8.14 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05660446143855678		[learning rate: 0.00022723]
		[batch 20/20] avg loss: 0.0523360222747699		[learning rate: 0.00022696]
	Learning Rate: 0.00022696
	LOSS [training: 0.05447024185666335 | validation: 0.049434924293224124]
	TIME [epoch: 8.15 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05030233519281187		[learning rate: 0.00022669]
		[batch 20/20] avg loss: 0.05731154593098797		[learning rate: 0.00022642]
	Learning Rate: 0.000226424
	LOSS [training: 0.05380694056189992 | validation: 0.048374758005031465]
	TIME [epoch: 8.16 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058546431530623465		[learning rate: 0.00022616]
		[batch 20/20] avg loss: 0.05348524904587011		[learning rate: 0.00022589]
	Learning Rate: 0.00022589
	LOSS [training: 0.056015840288246785 | validation: 0.05290341431745304]
	TIME [epoch: 8.15 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05910986213825289		[learning rate: 0.00022562]
		[batch 20/20] avg loss: 0.05060112034401183		[learning rate: 0.00022536]
	Learning Rate: 0.000225357
	LOSS [training: 0.054855491241132356 | validation: 0.03483924570081778]
	TIME [epoch: 8.14 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05541247347059346		[learning rate: 0.00022509]
		[batch 20/20] avg loss: 0.06081764630456651		[learning rate: 0.00022483]
	Learning Rate: 0.000224826
	LOSS [training: 0.058115059887579965 | validation: 0.04603461234442929]
	TIME [epoch: 8.14 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059231348177047796		[learning rate: 0.00022456]
		[batch 20/20] avg loss: 0.05166315774144582		[learning rate: 0.0002243]
	Learning Rate: 0.000224295
	LOSS [training: 0.055447252959246796 | validation: 0.044842600961954734]
	TIME [epoch: 8.16 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053169514228346204		[learning rate: 0.00022403]
		[batch 20/20] avg loss: 0.05187547254764844		[learning rate: 0.00022377]
	Learning Rate: 0.000223766
	LOSS [training: 0.052522493387997325 | validation: 0.04462377151735159]
	TIME [epoch: 8.14 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05366882204113653		[learning rate: 0.0002235]
		[batch 20/20] avg loss: 0.0520278563634526		[learning rate: 0.00022324]
	Learning Rate: 0.000223239
	LOSS [training: 0.05284833920229456 | validation: 0.042873104704732976]
	TIME [epoch: 8.14 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05922568182701861		[learning rate: 0.00022298]
		[batch 20/20] avg loss: 0.053715906927328846		[learning rate: 0.00022271]
	Learning Rate: 0.000222712
	LOSS [training: 0.056470794377173714 | validation: 0.041510106638053365]
	TIME [epoch: 8.15 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05703529339221913		[learning rate: 0.00022245]
		[batch 20/20] avg loss: 0.06226218003901986		[learning rate: 0.00022219]
	Learning Rate: 0.000222187
	LOSS [training: 0.0596487367156195 | validation: 0.041492599535009785]
	TIME [epoch: 8.16 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051655107934776526		[learning rate: 0.00022192]
		[batch 20/20] avg loss: 0.056523204958481434		[learning rate: 0.00022166]
	Learning Rate: 0.000221663
	LOSS [training: 0.05408915644662898 | validation: 0.044327851499957854]
	TIME [epoch: 8.14 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05179838892664555		[learning rate: 0.0002214]
		[batch 20/20] avg loss: 0.056947623099367996		[learning rate: 0.00022114]
	Learning Rate: 0.00022114
	LOSS [training: 0.0543730060130068 | validation: 0.04458673906198995]
	TIME [epoch: 8.14 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062365455694030535		[learning rate: 0.00022088]
		[batch 20/20] avg loss: 0.06385084818955741		[learning rate: 0.00022062]
	Learning Rate: 0.000220618
	LOSS [training: 0.06310815194179399 | validation: 0.05019658619423904]
	TIME [epoch: 8.16 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05154475216166916		[learning rate: 0.00022036]
		[batch 20/20] avg loss: 0.06109996862110055		[learning rate: 0.0002201]
	Learning Rate: 0.000220098
	LOSS [training: 0.056322360391384864 | validation: 0.05415595053719896]
	TIME [epoch: 8.17 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05138997473308839		[learning rate: 0.00021984]
		[batch 20/20] avg loss: 0.05285628251232176		[learning rate: 0.00021958]
	Learning Rate: 0.000219578
	LOSS [training: 0.05212312862270507 | validation: 0.039448799622784414]
	TIME [epoch: 8.15 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04490518033751702		[learning rate: 0.00021932]
		[batch 20/20] avg loss: 0.05652837893330587		[learning rate: 0.00021906]
	Learning Rate: 0.000219061
	LOSS [training: 0.050716779635411435 | validation: 0.06528830403621814]
	TIME [epoch: 8.15 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05731929599844714		[learning rate: 0.0002188]
		[batch 20/20] avg loss: 0.057671514610331286		[learning rate: 0.00021854]
	Learning Rate: 0.000218544
	LOSS [training: 0.057495405304389215 | validation: 0.03963393980908092]
	TIME [epoch: 8.15 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054233376922577056		[learning rate: 0.00021829]
		[batch 20/20] avg loss: 0.04804505554116871		[learning rate: 0.00021803]
	Learning Rate: 0.000218028
	LOSS [training: 0.05113921623187288 | validation: 0.05142079636134923]
	TIME [epoch: 8.17 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05935082266649679		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.04678197087225865		[learning rate: 0.00021751]
	Learning Rate: 0.000217514
	LOSS [training: 0.05306639676937772 | validation: 0.03504102427875848]
	TIME [epoch: 8.14 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05163119581843628		[learning rate: 0.00021726]
		[batch 20/20] avg loss: 0.05453480317623054		[learning rate: 0.000217]
	Learning Rate: 0.000217001
	LOSS [training: 0.053082999497333416 | validation: 0.04613492893435471]
	TIME [epoch: 8.15 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05931843491379814		[learning rate: 0.00021674]
		[batch 20/20] avg loss: 0.05438350539666999		[learning rate: 0.00021649]
	Learning Rate: 0.000216489
	LOSS [training: 0.05685097015523406 | validation: 0.04766262015052805]
	TIME [epoch: 8.15 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05459774268052886		[learning rate: 0.00021623]
		[batch 20/20] avg loss: 0.05565907860234787		[learning rate: 0.00021598]
	Learning Rate: 0.000215978
	LOSS [training: 0.05512841064143838 | validation: 0.03840371370223052]
	TIME [epoch: 8.18 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04730892162810742		[learning rate: 0.00021572]
		[batch 20/20] avg loss: 0.04796449221400155		[learning rate: 0.00021547]
	Learning Rate: 0.000215469
	LOSS [training: 0.04763670692105447 | validation: 0.047122112598885435]
	TIME [epoch: 8.14 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05338578334405804		[learning rate: 0.00021521]
		[batch 20/20] avg loss: 0.049032695290864854		[learning rate: 0.00021496]
	Learning Rate: 0.000214961
	LOSS [training: 0.05120923931746144 | validation: 0.05136905628404754]
	TIME [epoch: 8.16 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0495884516258535		[learning rate: 0.00021471]
		[batch 20/20] avg loss: 0.05601622165069205		[learning rate: 0.00021445]
	Learning Rate: 0.000214454
	LOSS [training: 0.05280233663827277 | validation: 0.04510617120309614]
	TIME [epoch: 8.15 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058745045239414775		[learning rate: 0.0002142]
		[batch 20/20] avg loss: 0.048540986879132705		[learning rate: 0.00021395]
	Learning Rate: 0.000213948
	LOSS [training: 0.053643016059273727 | validation: 0.04298791365547733]
	TIME [epoch: 8.17 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05628016465548358		[learning rate: 0.0002137]
		[batch 20/20] avg loss: 0.04720913294633887		[learning rate: 0.00021344]
	Learning Rate: 0.000213443
	LOSS [training: 0.05174464880091122 | validation: 0.04967198050065598]
	TIME [epoch: 8.15 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047509033129812736		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.05315783708485587		[learning rate: 0.00021294]
	Learning Rate: 0.00021294
	LOSS [training: 0.050333435107334304 | validation: 0.0436266041595898]
	TIME [epoch: 8.15 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05305514266486032		[learning rate: 0.00021269]
		[batch 20/20] avg loss: 0.05498331824412657		[learning rate: 0.00021244]
	Learning Rate: 0.000212437
	LOSS [training: 0.05401923045449345 | validation: 0.04283642380014399]
	TIME [epoch: 8.14 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05507241150474516		[learning rate: 0.00021219]
		[batch 20/20] avg loss: 0.05559808993417557		[learning rate: 0.00021194]
	Learning Rate: 0.000211936
	LOSS [training: 0.05533525071946035 | validation: 0.04755380593792077]
	TIME [epoch: 8.18 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053181672814083425		[learning rate: 0.00021169]
		[batch 20/20] avg loss: 0.05451959334551015		[learning rate: 0.00021144]
	Learning Rate: 0.000211436
	LOSS [training: 0.053850633079796796 | validation: 0.04395191234303909]
	TIME [epoch: 8.15 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04918495833413489		[learning rate: 0.00021119]
		[batch 20/20] avg loss: 0.05318275266923232		[learning rate: 0.00021094]
	Learning Rate: 0.000210937
	LOSS [training: 0.051183855501683605 | validation: 0.04256749042212195]
	TIME [epoch: 8.15 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04914759566903086		[learning rate: 0.00021069]
		[batch 20/20] avg loss: 0.05838370043451977		[learning rate: 0.00021044]
	Learning Rate: 0.00021044
	LOSS [training: 0.053765648051775305 | validation: 0.04453448579237125]
	TIME [epoch: 8.15 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06657730523876917		[learning rate: 0.00021019]
		[batch 20/20] avg loss: 0.05240239091091396		[learning rate: 0.00020994]
	Learning Rate: 0.000209944
	LOSS [training: 0.05948984807484157 | validation: 0.04121554605520672]
	TIME [epoch: 8.17 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05525542864396871		[learning rate: 0.0002097]
		[batch 20/20] avg loss: 0.05097132221061355		[learning rate: 0.00020945]
	Learning Rate: 0.000209448
	LOSS [training: 0.053113375427291123 | validation: 0.04311049920540397]
	TIME [epoch: 8.15 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04902369415260286		[learning rate: 0.0002092]
		[batch 20/20] avg loss: 0.05650948450112607		[learning rate: 0.00020895]
	Learning Rate: 0.000208954
	LOSS [training: 0.052766589326864455 | validation: 0.0407027490220823]
	TIME [epoch: 8.15 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04944612248177348		[learning rate: 0.00020871]
		[batch 20/20] avg loss: 0.05478703723320005		[learning rate: 0.00020846]
	Learning Rate: 0.000208461
	LOSS [training: 0.052116579857486765 | validation: 0.046017526647278]
	TIME [epoch: 8.15 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05192713121868312		[learning rate: 0.00020822]
		[batch 20/20] avg loss: 0.05202818135421226		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.051977656286447696 | validation: 0.04160038478039438]
	TIME [epoch: 8.17 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057951304274431195		[learning rate: 0.00020772]
		[batch 20/20] avg loss: 0.052326054542209864		[learning rate: 0.00020748]
	Learning Rate: 0.000207479
	LOSS [training: 0.055138679408320526 | validation: 0.04423696981143923]
	TIME [epoch: 8.15 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04769884978385873		[learning rate: 0.00020723]
		[batch 20/20] avg loss: 0.054928953647420585		[learning rate: 0.00020699]
	Learning Rate: 0.00020699
	LOSS [training: 0.05131390171563967 | validation: 0.05546831775000924]
	TIME [epoch: 8.15 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051221215709367815		[learning rate: 0.00020675]
		[batch 20/20] avg loss: 0.05947403884950949		[learning rate: 0.0002065]
	Learning Rate: 0.000206501
	LOSS [training: 0.05534762727943866 | validation: 0.03887651142255612]
	TIME [epoch: 8.15 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05479076381103755		[learning rate: 0.00020626]
		[batch 20/20] avg loss: 0.051268288725579945		[learning rate: 0.00020601]
	Learning Rate: 0.000206014
	LOSS [training: 0.053029526268308744 | validation: 0.040745429938891436]
	TIME [epoch: 8.16 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050026656415612526		[learning rate: 0.00020577]
		[batch 20/20] avg loss: 0.06478544634184139		[learning rate: 0.00020553]
	Learning Rate: 0.000205528
	LOSS [training: 0.05740605137872695 | validation: 0.06220625854338651]
	TIME [epoch: 8.15 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06523248853927856		[learning rate: 0.00020529]
		[batch 20/20] avg loss: 0.04775020384412737		[learning rate: 0.00020504]
	Learning Rate: 0.000205044
	LOSS [training: 0.056491346191702964 | validation: 0.04109755669018906]
	TIME [epoch: 8.14 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04732852662780439		[learning rate: 0.0002048]
		[batch 20/20] avg loss: 0.05262251848011749		[learning rate: 0.00020456]
	Learning Rate: 0.00020456
	LOSS [training: 0.04997552255396092 | validation: 0.03812311869526956]
	TIME [epoch: 8.14 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05853357462586932		[learning rate: 0.00020432]
		[batch 20/20] avg loss: 0.05215268448560725		[learning rate: 0.00020408]
	Learning Rate: 0.000204077
	LOSS [training: 0.05534312955573828 | validation: 0.047146842731016825]
	TIME [epoch: 8.16 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05376229649693907		[learning rate: 0.00020384]
		[batch 20/20] avg loss: 0.05083443279290075		[learning rate: 0.0002036]
	Learning Rate: 0.000203596
	LOSS [training: 0.052298364644919916 | validation: 0.043481975209554435]
	TIME [epoch: 8.15 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061444305363731334		[learning rate: 0.00020336]
		[batch 20/20] avg loss: 0.055956638658495525		[learning rate: 0.00020312]
	Learning Rate: 0.000203116
	LOSS [training: 0.05870047201111343 | validation: 0.04086953652421411]
	TIME [epoch: 8.15 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06241029416228163		[learning rate: 0.00020288]
		[batch 20/20] avg loss: 0.051939799724260785		[learning rate: 0.00020264]
	Learning Rate: 0.000202637
	LOSS [training: 0.057175046943271204 | validation: 0.04789546101923482]
	TIME [epoch: 8.14 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06352091265654604		[learning rate: 0.0002024]
		[batch 20/20] avg loss: 0.055822685226426305		[learning rate: 0.00020216]
	Learning Rate: 0.000202159
	LOSS [training: 0.059671798941486165 | validation: 0.05218536489002451]
	TIME [epoch: 8.16 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05293955495257663		[learning rate: 0.00020192]
		[batch 20/20] avg loss: 0.05260611103028484		[learning rate: 0.00020168]
	Learning Rate: 0.000201682
	LOSS [training: 0.05277283299143074 | validation: 0.04419970954506855]
	TIME [epoch: 8.14 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05318219248857052		[learning rate: 0.00020144]
		[batch 20/20] avg loss: 0.06400903229390308		[learning rate: 0.00020121]
	Learning Rate: 0.000201206
	LOSS [training: 0.0585956123912368 | validation: 0.05296797564583697]
	TIME [epoch: 8.13 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055731970107339365		[learning rate: 0.00020097]
		[batch 20/20] avg loss: 0.054546361971323454		[learning rate: 0.00020073]
	Learning Rate: 0.000200731
	LOSS [training: 0.0551391660393314 | validation: 0.06150734259911153]
	TIME [epoch: 8.14 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05777089632321467		[learning rate: 0.00020049]
		[batch 20/20] avg loss: 0.06677700035798154		[learning rate: 0.00020026]
	Learning Rate: 0.000200258
	LOSS [training: 0.062273948340598094 | validation: 0.04907741878786061]
	TIME [epoch: 8.16 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04850813230643203		[learning rate: 0.00020002]
		[batch 20/20] avg loss: 0.05471922525648486		[learning rate: 0.00019979]
	Learning Rate: 0.000199786
	LOSS [training: 0.051613678781458464 | validation: 0.043575259795275614]
	TIME [epoch: 8.15 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05310693965609241		[learning rate: 0.00019955]
		[batch 20/20] avg loss: 0.05106052816112382		[learning rate: 0.00019931]
	Learning Rate: 0.000199314
	LOSS [training: 0.05208373390860812 | validation: 0.05197896005980721]
	TIME [epoch: 8.15 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04992249731972437		[learning rate: 0.00019908]
		[batch 20/20] avg loss: 0.05369386869478056		[learning rate: 0.00019884]
	Learning Rate: 0.000198844
	LOSS [training: 0.05180818300725246 | validation: 0.04220242143877686]
	TIME [epoch: 8.14 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054065838916752804		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.06342253059595895		[learning rate: 0.00019838]
	Learning Rate: 0.000198375
	LOSS [training: 0.05874418475635588 | validation: 0.05362893773598869]
	TIME [epoch: 8.15 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058449776618894014		[learning rate: 0.00019814]
		[batch 20/20] avg loss: 0.051261842351815703		[learning rate: 0.00019791]
	Learning Rate: 0.000197907
	LOSS [training: 0.05485580948535486 | validation: 0.04689074776266239]
	TIME [epoch: 8.18 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06391114818135835		[learning rate: 0.00019767]
		[batch 20/20] avg loss: 0.04757734004425333		[learning rate: 0.00019744]
	Learning Rate: 0.00019744
	LOSS [training: 0.05574424411280583 | validation: 0.0398481416265742]
	TIME [epoch: 8.14 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05107297015842721		[learning rate: 0.00019721]
		[batch 20/20] avg loss: 0.058001218586582484		[learning rate: 0.00019697]
	Learning Rate: 0.000196975
	LOSS [training: 0.05453709437250485 | validation: 0.04445376189434107]
	TIME [epoch: 8.14 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05359482002040432		[learning rate: 0.00019674]
		[batch 20/20] avg loss: 0.057662974856706684		[learning rate: 0.00019651]
	Learning Rate: 0.00019651
	LOSS [training: 0.055628897438555494 | validation: 0.05492373361572918]
	TIME [epoch: 8.16 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05879519310859245		[learning rate: 0.00019628]
		[batch 20/20] avg loss: 0.05090231056529446		[learning rate: 0.00019605]
	Learning Rate: 0.000196046
	LOSS [training: 0.054848751836943446 | validation: 0.053023281295500824]
	TIME [epoch: 8.15 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057504189971655274		[learning rate: 0.00019582]
		[batch 20/20] avg loss: 0.051159594267929785		[learning rate: 0.00019558]
	Learning Rate: 0.000195584
	LOSS [training: 0.05433189211979254 | validation: 0.04161178238474626]
	TIME [epoch: 8.14 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0584100694695982		[learning rate: 0.00019535]
		[batch 20/20] avg loss: 0.05567044107026242		[learning rate: 0.00019512]
	Learning Rate: 0.000195123
	LOSS [training: 0.057040255269930305 | validation: 0.048353233466541204]
	TIME [epoch: 8.13 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05793110061170076		[learning rate: 0.00019489]
		[batch 20/20] avg loss: 0.05041125408841861		[learning rate: 0.00019466]
	Learning Rate: 0.000194662
	LOSS [training: 0.0541711773500597 | validation: 0.04677090134220031]
	TIME [epoch: 8.16 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05213832695457841		[learning rate: 0.00019443]
		[batch 20/20] avg loss: 0.05871156029750624		[learning rate: 0.0001942]
	Learning Rate: 0.000194203
	LOSS [training: 0.05542494362604232 | validation: 0.05581467097978525]
	TIME [epoch: 8.16 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057752711075114406		[learning rate: 0.00019397]
		[batch 20/20] avg loss: 0.05450273750649918		[learning rate: 0.00019375]
	Learning Rate: 0.000193745
	LOSS [training: 0.05612772429080679 | validation: 0.054370999516660495]
	TIME [epoch: 8.16 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0583483706147873		[learning rate: 0.00019352]
		[batch 20/20] avg loss: 0.05264399834100599		[learning rate: 0.00019329]
	Learning Rate: 0.000193288
	LOSS [training: 0.05549618447789665 | validation: 0.06082636726890153]
	TIME [epoch: 8.14 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05411567582508249		[learning rate: 0.00019306]
		[batch 20/20] avg loss: 0.046757323464691344		[learning rate: 0.00019283]
	Learning Rate: 0.000192832
	LOSS [training: 0.05043649964488691 | validation: 0.044211418801228114]
	TIME [epoch: 8.14 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050405949961141186		[learning rate: 0.0001926]
		[batch 20/20] avg loss: 0.06858170955488639		[learning rate: 0.00019238]
	Learning Rate: 0.000192377
	LOSS [training: 0.05949382975801378 | validation: 0.057918915029891965]
	TIME [epoch: 8.17 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0570808083526995		[learning rate: 0.00019215]
		[batch 20/20] avg loss: 0.05146992941172405		[learning rate: 0.00019192]
	Learning Rate: 0.000191923
	LOSS [training: 0.05427536888221178 | validation: 0.04415469980943268]
	TIME [epoch: 8.14 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055939004146347636		[learning rate: 0.0001917]
		[batch 20/20] avg loss: 0.05721073024021607		[learning rate: 0.00019147]
	Learning Rate: 0.000191471
	LOSS [training: 0.05657486719328185 | validation: 0.04128994681113045]
	TIME [epoch: 8.13 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05070985227917085		[learning rate: 0.00019124]
		[batch 20/20] avg loss: 0.049736730656124974		[learning rate: 0.00019102]
	Learning Rate: 0.000191019
	LOSS [training: 0.05022329146764791 | validation: 0.04612597038727329]
	TIME [epoch: 8.15 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054148047000952805		[learning rate: 0.00019079]
		[batch 20/20] avg loss: 0.05207715415178512		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: 0.05311260057636896 | validation: 0.051755863610098624]
	TIME [epoch: 8.17 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053892053499281764		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 0.04714398751832796		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.05051802050880485 | validation: 0.05138985965089509]
	TIME [epoch: 8.15 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05021798561498213		[learning rate: 0.00018989]
		[batch 20/20] avg loss: 0.06677746623336106		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.058497725924171594 | validation: 0.047685580315384446]
	TIME [epoch: 8.15 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052628022712875255		[learning rate: 0.00018945]
		[batch 20/20] avg loss: 0.05168753143439012		[learning rate: 0.00018922]
	Learning Rate: 0.000189223
	LOSS [training: 0.052157777073632686 | validation: 0.04989639141644895]
	TIME [epoch: 8.15 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04922459119496298		[learning rate: 0.000189]
		[batch 20/20] avg loss: 0.05764109281380188		[learning rate: 0.00018878]
	Learning Rate: 0.000188777
	LOSS [training: 0.05343284200438242 | validation: 0.05105787252973683]
	TIME [epoch: 8.17 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053606670306604966		[learning rate: 0.00018855]
		[batch 20/20] avg loss: 0.05719528451018909		[learning rate: 0.00018833]
	Learning Rate: 0.000188332
	LOSS [training: 0.055400977408397015 | validation: 0.050371379328487294]
	TIME [epoch: 8.15 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06103745020628167		[learning rate: 0.00018811]
		[batch 20/20] avg loss: 0.052079869692012656		[learning rate: 0.00018789]
	Learning Rate: 0.000187887
	LOSS [training: 0.05655865994914716 | validation: 0.04332071132945085]
	TIME [epoch: 8.14 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05012215434532039		[learning rate: 0.00018767]
		[batch 20/20] avg loss: 0.06016127149242138		[learning rate: 0.00018744]
	Learning Rate: 0.000187444
	LOSS [training: 0.05514171291887089 | validation: 0.03760506929526407]
	TIME [epoch: 8.14 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05188915700038495		[learning rate: 0.00018722]
		[batch 20/20] avg loss: 0.050375882488954714		[learning rate: 0.000187]
	Learning Rate: 0.000187002
	LOSS [training: 0.05113251974466983 | validation: 0.05745576047082353]
	TIME [epoch: 8.16 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06273583236435938		[learning rate: 0.00018678]
		[batch 20/20] avg loss: 0.05919723663788311		[learning rate: 0.00018656]
	Learning Rate: 0.000186561
	LOSS [training: 0.06096653450112124 | validation: 0.04995718013385978]
	TIME [epoch: 8.14 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054135659490504716		[learning rate: 0.00018634]
		[batch 20/20] avg loss: 0.05376521903013255		[learning rate: 0.00018612]
	Learning Rate: 0.000186121
	LOSS [training: 0.05395043926031864 | validation: 0.04015980056578084]
	TIME [epoch: 8.14 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05480969925855128		[learning rate: 0.0001859]
		[batch 20/20] avg loss: 0.055113951225002546		[learning rate: 0.00018568]
	Learning Rate: 0.000185682
	LOSS [training: 0.05496182524177693 | validation: 0.036145086111770384]
	TIME [epoch: 8.14 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05226840301732326		[learning rate: 0.00018546]
		[batch 20/20] avg loss: 0.055124616381683		[learning rate: 0.00018524]
	Learning Rate: 0.000185244
	LOSS [training: 0.053696509699503125 | validation: 0.05200962498150317]
	TIME [epoch: 8.15 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048933683899322324		[learning rate: 0.00018503]
		[batch 20/20] avg loss: 0.05156300885346423		[learning rate: 0.00018481]
	Learning Rate: 0.000184807
	LOSS [training: 0.050248346376393284 | validation: 0.047841874510581923]
	TIME [epoch: 8.15 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04972770937437078		[learning rate: 0.00018459]
		[batch 20/20] avg loss: 0.059426736134397376		[learning rate: 0.00018437]
	Learning Rate: 0.000184371
	LOSS [training: 0.05457722275438407 | validation: 0.04569347177264842]
	TIME [epoch: 8.15 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05955192006005017		[learning rate: 0.00018415]
		[batch 20/20] avg loss: 0.04676183980214245		[learning rate: 0.00018394]
	Learning Rate: 0.000183936
	LOSS [training: 0.0531568799310963 | validation: 0.04279795656673149]
	TIME [epoch: 8.14 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05319892386045359		[learning rate: 0.00018372]
		[batch 20/20] avg loss: 0.05391126289859459		[learning rate: 0.0001835]
	Learning Rate: 0.000183502
	LOSS [training: 0.0535550933795241 | validation: 0.04707880909512366]
	TIME [epoch: 8.16 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0465306409758837		[learning rate: 0.00018329]
		[batch 20/20] avg loss: 0.052276065931121704		[learning rate: 0.00018307]
	Learning Rate: 0.000183069
	LOSS [training: 0.049403353453502705 | validation: 0.03489149802533242]
	TIME [epoch: 8.14 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051004249961818425		[learning rate: 0.00018285]
		[batch 20/20] avg loss: 0.05985165418871524		[learning rate: 0.00018264]
	Learning Rate: 0.000182637
	LOSS [training: 0.05542795207526683 | validation: 0.03901198794198252]
	TIME [epoch: 8.14 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052022091411984016		[learning rate: 0.00018242]
		[batch 20/20] avg loss: 0.05252185557074207		[learning rate: 0.00018221]
	Learning Rate: 0.000182207
	LOSS [training: 0.05227197349136305 | validation: 0.05114268353391824]
	TIME [epoch: 8.14 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04486355168221025		[learning rate: 0.00018199]
		[batch 20/20] avg loss: 0.05314746564855832		[learning rate: 0.00018178]
	Learning Rate: 0.000181777
	LOSS [training: 0.049005508665384286 | validation: 0.04768917780613187]
	TIME [epoch: 8.18 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05080453978392787		[learning rate: 0.00018156]
		[batch 20/20] avg loss: 0.04757988701739387		[learning rate: 0.00018135]
	Learning Rate: 0.000181348
	LOSS [training: 0.04919221340066087 | validation: 0.04451635703673625]
	TIME [epoch: 8.15 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04651908168049894		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.05241532688603271		[learning rate: 0.00018092]
	Learning Rate: 0.00018092
	LOSS [training: 0.04946720428326583 | validation: 0.04682120259553331]
	TIME [epoch: 8.14 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059744693458757624		[learning rate: 0.00018071]
		[batch 20/20] avg loss: 0.050137655157536744		[learning rate: 0.00018049]
	Learning Rate: 0.000180493
	LOSS [training: 0.05494117430814718 | validation: 0.03327373744768217]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1750.pth
	Model improved!!!
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05056120633563743		[learning rate: 0.00018028]
		[batch 20/20] avg loss: 0.05418513972530825		[learning rate: 0.00018007]
	Learning Rate: 0.000180068
	LOSS [training: 0.05237317303047284 | validation: 0.042652948182660855]
	TIME [epoch: 8.17 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05448017875570431		[learning rate: 0.00017986]
		[batch 20/20] avg loss: 0.050270477729359074		[learning rate: 0.00017964]
	Learning Rate: 0.000179643
	LOSS [training: 0.052375328242531695 | validation: 0.041965413149708385]
	TIME [epoch: 8.15 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051619401739515125		[learning rate: 0.00017943]
		[batch 20/20] avg loss: 0.052835547670157745		[learning rate: 0.00017922]
	Learning Rate: 0.000179219
	LOSS [training: 0.05222747470483644 | validation: 0.05750632882071759]
	TIME [epoch: 8.15 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05130916557220977		[learning rate: 0.00017901]
		[batch 20/20] avg loss: 0.055795982888968976		[learning rate: 0.0001788]
	Learning Rate: 0.000178796
	LOSS [training: 0.053552574230589386 | validation: 0.04366929560593685]
	TIME [epoch: 8.15 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05019795843664289		[learning rate: 0.00017859]
		[batch 20/20] avg loss: 0.061117076321384446		[learning rate: 0.00017837]
	Learning Rate: 0.000178375
	LOSS [training: 0.055657517379013664 | validation: 0.046711847728572516]
	TIME [epoch: 8.17 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05153257440980104		[learning rate: 0.00017816]
		[batch 20/20] avg loss: 0.052260763075664775		[learning rate: 0.00017795]
	Learning Rate: 0.000177954
	LOSS [training: 0.05189666874273291 | validation: 0.05101749253815564]
	TIME [epoch: 8.14 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06151482859205952		[learning rate: 0.00017774]
		[batch 20/20] avg loss: 0.06422213050733581		[learning rate: 0.00017753]
	Learning Rate: 0.000177534
	LOSS [training: 0.06286847954969768 | validation: 0.052386009364078556]
	TIME [epoch: 8.14 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05235531599155755		[learning rate: 0.00017732]
		[batch 20/20] avg loss: 0.05360196489415856		[learning rate: 0.00017712]
	Learning Rate: 0.000177115
	LOSS [training: 0.05297864044285807 | validation: 0.044887585307507354]
	TIME [epoch: 8.15 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04679826695699921		[learning rate: 0.00017691]
		[batch 20/20] avg loss: 0.057470934702428866		[learning rate: 0.0001767]
	Learning Rate: 0.000176698
	LOSS [training: 0.05213460082971404 | validation: 0.037878235558818106]
	TIME [epoch: 8.16 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04348929384611154		[learning rate: 0.00017649]
		[batch 20/20] avg loss: 0.056310669930862076		[learning rate: 0.00017628]
	Learning Rate: 0.000176281
	LOSS [training: 0.04989998188848681 | validation: 0.03922558875277698]
	TIME [epoch: 8.15 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051028873192571135		[learning rate: 0.00017607]
		[batch 20/20] avg loss: 0.04885311461009463		[learning rate: 0.00017587]
	Learning Rate: 0.000175865
	LOSS [training: 0.04994099390133288 | validation: 0.03582328596563514]
	TIME [epoch: 8.14 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04599101840747317		[learning rate: 0.00017566]
		[batch 20/20] avg loss: 0.05314423880030579		[learning rate: 0.00017545]
	Learning Rate: 0.00017545
	LOSS [training: 0.04956762860388948 | validation: 0.041325605443750794]
	TIME [epoch: 8.14 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049912133443756675		[learning rate: 0.00017524]
		[batch 20/20] avg loss: 0.05306516000995518		[learning rate: 0.00017504]
	Learning Rate: 0.000175036
	LOSS [training: 0.05148864672685592 | validation: 0.040843095167315206]
	TIME [epoch: 8.17 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06413306905899278		[learning rate: 0.00017483]
		[batch 20/20] avg loss: 0.05525033485714494		[learning rate: 0.00017462]
	Learning Rate: 0.000174623
	LOSS [training: 0.05969170195806887 | validation: 0.04382063271763972]
	TIME [epoch: 8.16 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053126098234694386		[learning rate: 0.00017442]
		[batch 20/20] avg loss: 0.05595588382783957		[learning rate: 0.00017421]
	Learning Rate: 0.000174212
	LOSS [training: 0.05454099103126697 | validation: 0.054777133382870796]
	TIME [epoch: 8.16 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055367108037722046		[learning rate: 0.00017401]
		[batch 20/20] avg loss: 0.05383322701513595		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 0.054600167526429 | validation: 0.04847495318910844]
	TIME [epoch: 8.14 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05679324883857008		[learning rate: 0.0001736]
		[batch 20/20] avg loss: 0.04982287486757471		[learning rate: 0.00017339]
	Learning Rate: 0.000173391
	LOSS [training: 0.05330806185307239 | validation: 0.04024193463468331]
	TIME [epoch: 8.17 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05733467256640205		[learning rate: 0.00017319]
		[batch 20/20] avg loss: 0.05571868982175697		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.05652668119407952 | validation: 0.05146650931691378]
	TIME [epoch: 8.15 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056942412017994756		[learning rate: 0.00017278]
		[batch 20/20] avg loss: 0.049965637221344		[learning rate: 0.00017257]
	Learning Rate: 0.000172574
	LOSS [training: 0.053454024619669374 | validation: 0.04435953507965479]
	TIME [epoch: 8.14 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046779768062012665		[learning rate: 0.00017237]
		[batch 20/20] avg loss: 0.05606110775066486		[learning rate: 0.00017217]
	Learning Rate: 0.000172167
	LOSS [training: 0.05142043790633875 | validation: 0.04810663102806916]
	TIME [epoch: 8.15 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056073123322229704		[learning rate: 0.00017196]
		[batch 20/20] avg loss: 0.05509188484675698		[learning rate: 0.00017176]
	Learning Rate: 0.00017176
	LOSS [training: 0.055582504084493334 | validation: 0.04255293561534017]
	TIME [epoch: 8.16 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04797606180323278		[learning rate: 0.00017156]
		[batch 20/20] avg loss: 0.059875632543172866		[learning rate: 0.00017136]
	Learning Rate: 0.000171355
	LOSS [training: 0.05392584717320283 | validation: 0.0500716373979384]
	TIME [epoch: 8.15 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056910557477097515		[learning rate: 0.00017115]
		[batch 20/20] avg loss: 0.04877464069046071		[learning rate: 0.00017095]
	Learning Rate: 0.000170951
	LOSS [training: 0.0528425990837791 | validation: 0.048831989793746367]
	TIME [epoch: 8.15 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049581685697964194		[learning rate: 0.00017075]
		[batch 20/20] avg loss: 0.05737996123608082		[learning rate: 0.00017055]
	Learning Rate: 0.000170548
	LOSS [training: 0.05348082346702251 | validation: 0.04591563975179948]
	TIME [epoch: 8.14 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04977022668042612		[learning rate: 0.00017035]
		[batch 20/20] avg loss: 0.05602146546527255		[learning rate: 0.00017015]
	Learning Rate: 0.000170146
	LOSS [training: 0.05289584607284933 | validation: 0.04597930917913613]
	TIME [epoch: 8.15 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04650104143348871		[learning rate: 0.00016994]
		[batch 20/20] avg loss: 0.05156960435420638		[learning rate: 0.00016974]
	Learning Rate: 0.000169744
	LOSS [training: 0.049035322893847544 | validation: 0.04571305593050132]
	TIME [epoch: 8.16 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05126078820252208		[learning rate: 0.00016954]
		[batch 20/20] avg loss: 0.050083339306112994		[learning rate: 0.00016934]
	Learning Rate: 0.000169344
	LOSS [training: 0.05067206375431754 | validation: 0.05456114586513916]
	TIME [epoch: 8.14 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052563092200566965		[learning rate: 0.00016914]
		[batch 20/20] avg loss: 0.04635807333847192		[learning rate: 0.00016894]
	Learning Rate: 0.000168944
	LOSS [training: 0.04946058276951944 | validation: 0.043158962416668896]
	TIME [epoch: 8.15 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047540782652263774		[learning rate: 0.00016874]
		[batch 20/20] avg loss: 0.06350811436945335		[learning rate: 0.00016855]
	Learning Rate: 0.000168546
	LOSS [training: 0.05552444851085856 | validation: 0.07388336852430565]
	TIME [epoch: 8.17 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06791001112884272		[learning rate: 0.00016835]
		[batch 20/20] avg loss: 0.05128334802158119		[learning rate: 0.00016815]
	Learning Rate: 0.000168148
	LOSS [training: 0.059596679575211954 | validation: 0.037282626462025924]
	TIME [epoch: 8.14 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047184231803014974		[learning rate: 0.00016795]
		[batch 20/20] avg loss: 0.051659209454314584		[learning rate: 0.00016775]
	Learning Rate: 0.000167752
	LOSS [training: 0.049421720628664775 | validation: 0.03781687681042083]
	TIME [epoch: 8.14 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051891337700373644		[learning rate: 0.00016755]
		[batch 20/20] avg loss: 0.0490657689641914		[learning rate: 0.00016736]
	Learning Rate: 0.000167356
	LOSS [training: 0.050478553332282516 | validation: 0.046478610569094046]
	TIME [epoch: 8.14 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0537109457810123		[learning rate: 0.00016716]
		[batch 20/20] avg loss: 0.04621658895313485		[learning rate: 0.00016696]
	Learning Rate: 0.000166961
	LOSS [training: 0.04996376736707358 | validation: 0.038873372252466495]
	TIME [epoch: 8.16 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0521726704433957		[learning rate: 0.00016676]
		[batch 20/20] avg loss: 0.04814510038733227		[learning rate: 0.00016657]
	Learning Rate: 0.000166567
	LOSS [training: 0.05015888541536399 | validation: 0.043251376354517666]
	TIME [epoch: 8.16 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050195758914653756		[learning rate: 0.00016637]
		[batch 20/20] avg loss: 0.05104752339090544		[learning rate: 0.00016617]
	Learning Rate: 0.000166174
	LOSS [training: 0.050621641152779596 | validation: 0.048143014321744455]
	TIME [epoch: 8.14 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050274194904585866		[learning rate: 0.00016598]
		[batch 20/20] avg loss: 0.0485215070590619		[learning rate: 0.00016578]
	Learning Rate: 0.000165782
	LOSS [training: 0.04939785098182388 | validation: 0.04794664731844649]
	TIME [epoch: 8.15 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0544861735288219		[learning rate: 0.00016559]
		[batch 20/20] avg loss: 0.05193229133902801		[learning rate: 0.00016539]
	Learning Rate: 0.000165391
	LOSS [training: 0.05320923243392497 | validation: 0.0415460160504883]
	TIME [epoch: 8.15 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04827643378804941		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.05409309431378282		[learning rate: 0.000165]
	Learning Rate: 0.000165001
	LOSS [training: 0.05118476405091613 | validation: 0.04454091348477341]
	TIME [epoch: 8.17 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05106598661354324		[learning rate: 0.00016481]
		[batch 20/20] avg loss: 0.05251674712396037		[learning rate: 0.00016461]
	Learning Rate: 0.000164612
	LOSS [training: 0.0517913668687518 | validation: 0.04667480851981527]
	TIME [epoch: 8.15 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05427420489821875		[learning rate: 0.00016442]
		[batch 20/20] avg loss: 0.055764348770270186		[learning rate: 0.00016422]
	Learning Rate: 0.000164224
	LOSS [training: 0.05501927683424447 | validation: 0.04281281583156717]
	TIME [epoch: 8.15 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04607515473255777		[learning rate: 0.00016403]
		[batch 20/20] avg loss: 0.05432124587936116		[learning rate: 0.00016384]
	Learning Rate: 0.000163836
	LOSS [training: 0.05019820030595947 | validation: 0.04659659992783406]
	TIME [epoch: 8.14 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056161075537767946		[learning rate: 0.00016364]
		[batch 20/20] avg loss: 0.052680691908052205		[learning rate: 0.00016345]
	Learning Rate: 0.00016345
	LOSS [training: 0.05442088372291006 | validation: 0.04904080770508147]
	TIME [epoch: 8.16 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04785374738091418		[learning rate: 0.00016326]
		[batch 20/20] avg loss: 0.0534131746161977		[learning rate: 0.00016306]
	Learning Rate: 0.000163064
	LOSS [training: 0.050633460998555944 | validation: 0.049428882605023955]
	TIME [epoch: 8.15 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045964533359876915		[learning rate: 0.00016287]
		[batch 20/20] avg loss: 0.04807484968996324		[learning rate: 0.00016268]
	Learning Rate: 0.00016268
	LOSS [training: 0.04701969152492008 | validation: 0.050932031402541746]
	TIME [epoch: 8.14 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051290009909169286		[learning rate: 0.00016249]
		[batch 20/20] avg loss: 0.048720341271645685		[learning rate: 0.0001623]
	Learning Rate: 0.000162296
	LOSS [training: 0.05000517559040749 | validation: 0.04447900454404231]
	TIME [epoch: 8.14 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04561402410706702		[learning rate: 0.0001621]
		[batch 20/20] avg loss: 0.050307387305928306		[learning rate: 0.00016191]
	Learning Rate: 0.000161913
	LOSS [training: 0.047960705706497665 | validation: 0.04068258368270918]
	TIME [epoch: 8.16 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04705963285800725		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.051515136747720056		[learning rate: 0.00016153]
	Learning Rate: 0.000161531
	LOSS [training: 0.049287384802863654 | validation: 0.05148406807678278]
	TIME [epoch: 8.15 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04819962478862012		[learning rate: 0.00016134]
		[batch 20/20] avg loss: 0.05671895786428206		[learning rate: 0.00016115]
	Learning Rate: 0.00016115
	LOSS [training: 0.052459291326451093 | validation: 0.04455497381488015]
	TIME [epoch: 8.15 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052024996341529996		[learning rate: 0.00016096]
		[batch 20/20] avg loss: 0.050440996431365445		[learning rate: 0.00016077]
	Learning Rate: 0.00016077
	LOSS [training: 0.05123299638644772 | validation: 0.04166864475232005]
	TIME [epoch: 8.15 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06138780815207171		[learning rate: 0.00016058]
		[batch 20/20] avg loss: 0.051387052392516654		[learning rate: 0.00016039]
	Learning Rate: 0.000160391
	LOSS [training: 0.05638743027229419 | validation: 0.04734878843644914]
	TIME [epoch: 8.16 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0510111764789557		[learning rate: 0.0001602]
		[batch 20/20] avg loss: 0.055078623277685076		[learning rate: 0.00016001]
	Learning Rate: 0.000160012
	LOSS [training: 0.05304489987832038 | validation: 0.044350551566975684]
	TIME [epoch: 8.14 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04806719710875781		[learning rate: 0.00015982]
		[batch 20/20] avg loss: 0.0526172897779952		[learning rate: 0.00015964]
	Learning Rate: 0.000159635
	LOSS [training: 0.05034224344337651 | validation: 0.04045592807382321]
	TIME [epoch: 8.13 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04755232023839638		[learning rate: 0.00015945]
		[batch 20/20] avg loss: 0.05236727420438876		[learning rate: 0.00015926]
	Learning Rate: 0.000159258
	LOSS [training: 0.04995979722139258 | validation: 0.04867608485856978]
	TIME [epoch: 8.15 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04487842715637328		[learning rate: 0.00015907]
		[batch 20/20] avg loss: 0.05448726833271946		[learning rate: 0.00015888]
	Learning Rate: 0.000158883
	LOSS [training: 0.04968284774454636 | validation: 0.052717364972222944]
	TIME [epoch: 8.16 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05791120261382064		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 0.04987540281153312		[learning rate: 0.00015851]
	Learning Rate: 0.000158508
	LOSS [training: 0.05389330271267688 | validation: 0.0424879102522879]
	TIME [epoch: 8.14 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049074384248496555		[learning rate: 0.00015832]
		[batch 20/20] avg loss: 0.0547103129954243		[learning rate: 0.00015813]
	Learning Rate: 0.000158134
	LOSS [training: 0.05189234862196044 | validation: 0.05365369417746822]
	TIME [epoch: 8.14 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05543196279093404		[learning rate: 0.00015795]
		[batch 20/20] avg loss: 0.05614588194887628		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.05578892236990516 | validation: 0.05426198208016135]
	TIME [epoch: 8.13 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05558037982935745		[learning rate: 0.00015757]
		[batch 20/20] avg loss: 0.050502600112734906		[learning rate: 0.00015739]
	Learning Rate: 0.000157389
	LOSS [training: 0.053041489971046185 | validation: 0.043721051164313615]
	TIME [epoch: 8.17 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04844165872305525		[learning rate: 0.0001572]
		[batch 20/20] avg loss: 0.04822259872826383		[learning rate: 0.00015702]
	Learning Rate: 0.000157018
	LOSS [training: 0.048332128725659534 | validation: 0.04364231266112127]
	TIME [epoch: 8.15 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048569724109866416		[learning rate: 0.00015683]
		[batch 20/20] avg loss: 0.05168773103045125		[learning rate: 0.00015665]
	Learning Rate: 0.000156647
	LOSS [training: 0.050128727570158835 | validation: 0.04003118541475207]
	TIME [epoch: 8.15 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050560359668957756		[learning rate: 0.00015646]
		[batch 20/20] avg loss: 0.05626714504974054		[learning rate: 0.00015628]
	Learning Rate: 0.000156278
	LOSS [training: 0.053413752359349156 | validation: 0.03935223141207794]
	TIME [epoch: 8.16 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055244970465475864		[learning rate: 0.00015609]
		[batch 20/20] avg loss: 0.04151896365158402		[learning rate: 0.00015591]
	Learning Rate: 0.000155909
	LOSS [training: 0.04838196705852994 | validation: 0.039624639086748174]
	TIME [epoch: 8.17 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04705298566316227		[learning rate: 0.00015573]
		[batch 20/20] avg loss: 0.048528470037958654		[learning rate: 0.00015554]
	Learning Rate: 0.000155541
	LOSS [training: 0.04779072785056047 | validation: 0.04319007780762091]
	TIME [epoch: 8.15 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04730546896730452		[learning rate: 0.00015536]
		[batch 20/20] avg loss: 0.05619935210080085		[learning rate: 0.00015517]
	Learning Rate: 0.000155175
	LOSS [training: 0.05175241053405268 | validation: 0.05063022659634217]
	TIME [epoch: 8.15 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05686048116421778		[learning rate: 0.00015499]
		[batch 20/20] avg loss: 0.04836344891592695		[learning rate: 0.00015481]
	Learning Rate: 0.000154809
	LOSS [training: 0.05261196504007237 | validation: 0.04235495511324301]
	TIME [epoch: 8.14 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050959713206889155		[learning rate: 0.00015463]
		[batch 20/20] avg loss: 0.04768581547325084		[learning rate: 0.00015444]
	Learning Rate: 0.000154443
	LOSS [training: 0.049322764340070005 | validation: 0.044405688459150376]
	TIME [epoch: 8.17 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05088861292752581		[learning rate: 0.00015426]
		[batch 20/20] avg loss: 0.04715486651945676		[learning rate: 0.00015408]
	Learning Rate: 0.000154079
	LOSS [training: 0.04902173972349129 | validation: 0.032194979576483256]
	TIME [epoch: 8.14 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1817.pth
	Model improved!!!
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047868017573985955		[learning rate: 0.0001539]
		[batch 20/20] avg loss: 0.04942160527258721		[learning rate: 0.00015372]
	Learning Rate: 0.000153716
	LOSS [training: 0.04864481142328658 | validation: 0.040661185019719936]
	TIME [epoch: 8.15 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04394844260336683		[learning rate: 0.00015353]
		[batch 20/20] avg loss: 0.050707897306519624		[learning rate: 0.00015335]
	Learning Rate: 0.000153353
	LOSS [training: 0.047328169954943225 | validation: 0.047705294582677124]
	TIME [epoch: 8.15 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056634546417106676		[learning rate: 0.00015317]
		[batch 20/20] avg loss: 0.052198537403718145		[learning rate: 0.00015299]
	Learning Rate: 0.000152991
	LOSS [training: 0.05441654191041242 | validation: 0.045481592385700584]
	TIME [epoch: 8.17 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05273339059523672		[learning rate: 0.00015281]
		[batch 20/20] avg loss: 0.05328932342094535		[learning rate: 0.00015263]
	Learning Rate: 0.00015263
	LOSS [training: 0.05301135700809103 | validation: 0.0459551511376388]
	TIME [epoch: 8.16 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05200418767908556		[learning rate: 0.00015245]
		[batch 20/20] avg loss: 0.052237197026775574		[learning rate: 0.00015227]
	Learning Rate: 0.00015227
	LOSS [training: 0.05212069235293056 | validation: 0.052972390191729044]
	TIME [epoch: 8.16 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05780666699174149		[learning rate: 0.00015209]
		[batch 20/20] avg loss: 0.04943307337055476		[learning rate: 0.00015191]
	Learning Rate: 0.000151911
	LOSS [training: 0.053619870181148124 | validation: 0.045839156560901845]
	TIME [epoch: 8.15 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05174770393266861		[learning rate: 0.00015173]
		[batch 20/20] avg loss: 0.04799625168982031		[learning rate: 0.00015155]
	Learning Rate: 0.000151553
	LOSS [training: 0.04987197781124446 | validation: 0.03963160498087951]
	TIME [epoch: 8.18 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04749518523786067		[learning rate: 0.00015137]
		[batch 20/20] avg loss: 0.05462969536360433		[learning rate: 0.0001512]
	Learning Rate: 0.000151195
	LOSS [training: 0.051062440300732516 | validation: 0.04166872876301995]
	TIME [epoch: 8.15 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055350818455560905		[learning rate: 0.00015102]
		[batch 20/20] avg loss: 0.05068065664694954		[learning rate: 0.00015084]
	Learning Rate: 0.000150839
	LOSS [training: 0.053015737551255215 | validation: 0.0433579409176903]
	TIME [epoch: 8.15 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051694969998865546		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.04809397187507446		[learning rate: 0.00015048]
	Learning Rate: 0.000150483
	LOSS [training: 0.049894470936970005 | validation: 0.045957466709424355]
	TIME [epoch: 8.16 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043816752998996386		[learning rate: 0.00015031]
		[batch 20/20] avg loss: 0.05001986317772199		[learning rate: 0.00015013]
	Learning Rate: 0.000150128
	LOSS [training: 0.04691830808835919 | validation: 0.03920721594313775]
	TIME [epoch: 8.17 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04787779088340765		[learning rate: 0.00014995]
		[batch 20/20] avg loss: 0.04767751339817294		[learning rate: 0.00014977]
	Learning Rate: 0.000149774
	LOSS [training: 0.0477776521407903 | validation: 0.03973525593128774]
	TIME [epoch: 8.16 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05667827690593694		[learning rate: 0.0001496]
		[batch 20/20] avg loss: 0.04740337504523222		[learning rate: 0.00014942]
	Learning Rate: 0.000149421
	LOSS [training: 0.052040825975584584 | validation: 0.046295426105890006]
	TIME [epoch: 8.16 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046298053410123344		[learning rate: 0.00014924]
		[batch 20/20] avg loss: 0.05018119186352914		[learning rate: 0.00014907]
	Learning Rate: 0.000149068
	LOSS [training: 0.048239622636826233 | validation: 0.04549375735548104]
	TIME [epoch: 8.16 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04253373263327771		[learning rate: 0.00014889]
		[batch 20/20] avg loss: 0.05568806104169255		[learning rate: 0.00014872]
	Learning Rate: 0.000148716
	LOSS [training: 0.04911089683748514 | validation: 0.05645858041569912]
	TIME [epoch: 8.17 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04912305576929746		[learning rate: 0.00014854]
		[batch 20/20] avg loss: 0.045690871311890424		[learning rate: 0.00014837]
	Learning Rate: 0.000148366
	LOSS [training: 0.04740696354059394 | validation: 0.04146734907714895]
	TIME [epoch: 8.16 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050204958006092114		[learning rate: 0.00014819]
		[batch 20/20] avg loss: 0.05059942559891266		[learning rate: 0.00014802]
	Learning Rate: 0.000148016
	LOSS [training: 0.0504021918025024 | validation: 0.027388579322315657]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240219_184950/states/model_tr_study1_1834.pth
	Model improved!!!
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05071165014926339		[learning rate: 0.00014784]
		[batch 20/20] avg loss: 0.05034597911519534		[learning rate: 0.00014767]
	Learning Rate: 0.000147667
	LOSS [training: 0.05052881463222936 | validation: 0.03378601117212937]
	TIME [epoch: 8.15 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051288952853145595		[learning rate: 0.00014749]
		[batch 20/20] avg loss: 0.05095453269701929		[learning rate: 0.00014732]
	Learning Rate: 0.000147318
	LOSS [training: 0.051121742775082434 | validation: 0.04671759274280629]
	TIME [epoch: 8.18 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04774462868703568		[learning rate: 0.00014714]
		[batch 20/20] avg loss: 0.052755237727823		[learning rate: 0.00014697]
	Learning Rate: 0.000146971
	LOSS [training: 0.05024993320742934 | validation: 0.043273921698636336]
	TIME [epoch: 8.16 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04611389877527618		[learning rate: 0.0001468]
		[batch 20/20] avg loss: 0.05369018027828833		[learning rate: 0.00014662]
	Learning Rate: 0.000146624
	LOSS [training: 0.049902039526782245 | validation: 0.04897553768191463]
	TIME [epoch: 8.15 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05106151561065153		[learning rate: 0.00014645]
		[batch 20/20] avg loss: 0.056895606355412486		[learning rate: 0.00014628]
	Learning Rate: 0.000146278
	LOSS [training: 0.05397856098303203 | validation: 0.05685951880676981]
	TIME [epoch: 8.16 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05376546027254717		[learning rate: 0.00014611]
		[batch 20/20] avg loss: 0.05169173327319713		[learning rate: 0.00014593]
	Learning Rate: 0.000145933
	LOSS [training: 0.05272859677287215 | validation: 0.04662880670234812]
	TIME [epoch: 8.17 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0533829945438686		[learning rate: 0.00014576]
		[batch 20/20] avg loss: 0.050061067278145674		[learning rate: 0.00014559]
	Learning Rate: 0.000145589
	LOSS [training: 0.05172203091100715 | validation: 0.04554424393912629]
	TIME [epoch: 8.16 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058267533035120736		[learning rate: 0.00014542]
		[batch 20/20] avg loss: 0.05733310519552089		[learning rate: 0.00014525]
	Learning Rate: 0.000145245
	LOSS [training: 0.05780031911532082 | validation: 0.054580920251743006]
	TIME [epoch: 8.17 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04987719435449231		[learning rate: 0.00014507]
		[batch 20/20] avg loss: 0.05626852168256582		[learning rate: 0.0001449]
	Learning Rate: 0.000144903
	LOSS [training: 0.05307285801852907 | validation: 0.04698765072723098]
	TIME [epoch: 8.15 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054658721372865614		[learning rate: 0.00014473]
		[batch 20/20] avg loss: 0.05035347393631773		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 0.05250609765459167 | validation: 0.04099558844513539]
	TIME [epoch: 8.18 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.054910102098000416		[learning rate: 0.00014439]
		[batch 20/20] avg loss: 0.050373714311325445		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.05264190820466293 | validation: 0.044960643647984425]
	TIME [epoch: 8.16 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04463261788569276		[learning rate: 0.00014405]
		[batch 20/20] avg loss: 0.056712233741426574		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.05067242581355967 | validation: 0.04973607614019154]
	TIME [epoch: 8.16 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05225926564147253		[learning rate: 0.00014371]
		[batch 20/20] avg loss: 0.05761457320962551		[learning rate: 0.00014354]
	Learning Rate: 0.00014354
	LOSS [training: 0.05493691942554903 | validation: 0.05058912312819294]
	TIME [epoch: 8.15 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0613815266814951		[learning rate: 0.00014337]
		[batch 20/20] avg loss: 0.05317770327293375		[learning rate: 0.0001432]
	Learning Rate: 0.000143202
	LOSS [training: 0.05727961497721442 | validation: 0.042272727428911284]
	TIME [epoch: 8.17 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046428027356949696		[learning rate: 0.00014303]
		[batch 20/20] avg loss: 0.05760087071234223		[learning rate: 0.00014286]
	Learning Rate: 0.000142864
	LOSS [training: 0.052014449034645985 | validation: 0.037195101403523834]
	TIME [epoch: 8.16 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05545745253472569		[learning rate: 0.0001427]
		[batch 20/20] avg loss: 0.05033055424810454		[learning rate: 0.00014253]
	Learning Rate: 0.000142527
	LOSS [training: 0.05289400339141511 | validation: 0.0441220766041633]
	TIME [epoch: 8.16 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04993219688000493		[learning rate: 0.00014236]
		[batch 20/20] avg loss: 0.05270218375902373		[learning rate: 0.00014219]
	Learning Rate: 0.000142191
	LOSS [training: 0.05131719031951434 | validation: 0.0380092279690673]
	TIME [epoch: 8.15 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04410349187075252		[learning rate: 0.00014202]
		[batch 20/20] avg loss: 0.046422236777490536		[learning rate: 0.00014186]
	Learning Rate: 0.000141855
	LOSS [training: 0.045262864324121535 | validation: 0.040041468798330385]
	TIME [epoch: 8.17 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049132324789723715		[learning rate: 0.00014169]
		[batch 20/20] avg loss: 0.05186102434370944		[learning rate: 0.00014152]
	Learning Rate: 0.000141521
	LOSS [training: 0.05049667456671657 | validation: 0.045641994616769405]
	TIME [epoch: 8.17 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04947086771174746		[learning rate: 0.00014135]
		[batch 20/20] avg loss: 0.05194678850316349		[learning rate: 0.00014119]
	Learning Rate: 0.000141187
	LOSS [training: 0.05070882810745546 | validation: 0.04812638377814825]
	TIME [epoch: 8.15 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05264177061901535		[learning rate: 0.00014102]
		[batch 20/20] avg loss: 0.05021115707963671		[learning rate: 0.00014085]
	Learning Rate: 0.000140854
	LOSS [training: 0.05142646384932603 | validation: 0.04636852909130626]
	TIME [epoch: 8.16 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05352433056009727		[learning rate: 0.00014069]
		[batch 20/20] avg loss: 0.04990475205327053		[learning rate: 0.00014052]
	Learning Rate: 0.000140522
	LOSS [training: 0.05171454130668389 | validation: 0.04371432320457541]
	TIME [epoch: 8.17 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048800849380999996		[learning rate: 0.00014036]
		[batch 20/20] avg loss: 0.04666306067226844		[learning rate: 0.00014019]
	Learning Rate: 0.00014019
	LOSS [training: 0.04773195502663423 | validation: 0.045703100180035844]
	TIME [epoch: 8.17 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0517154929230489		[learning rate: 0.00014002]
		[batch 20/20] avg loss: 0.0482121301191036		[learning rate: 0.00013986]
	Learning Rate: 0.00013986
	LOSS [training: 0.04996381152107625 | validation: 0.050628540086442075]
	TIME [epoch: 8.15 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04774615847952925		[learning rate: 0.00013969]
		[batch 20/20] avg loss: 0.049530606165272696		[learning rate: 0.00013953]
	Learning Rate: 0.00013953
	LOSS [training: 0.04863838232240098 | validation: 0.04928450884187477]
	TIME [epoch: 8.16 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04706654317936986		[learning rate: 0.00013937]
		[batch 20/20] avg loss: 0.05426645203774324		[learning rate: 0.0001392]
	Learning Rate: 0.000139201
	LOSS [training: 0.05066649760855655 | validation: 0.04315304680358069]
	TIME [epoch: 8.18 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05276297010766924		[learning rate: 0.00013904]
		[batch 20/20] avg loss: 0.04660876288342408		[learning rate: 0.00013887]
	Learning Rate: 0.000138872
	LOSS [training: 0.04968586649554667 | validation: 0.03962100131801339]
	TIME [epoch: 8.17 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052323902484398964		[learning rate: 0.00013871]
		[batch 20/20] avg loss: 0.03990360488417724		[learning rate: 0.00013854]
	Learning Rate: 0.000138545
	LOSS [training: 0.0461137536842881 | validation: 0.040003990527236466]
	TIME [epoch: 8.16 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04767018236138052		[learning rate: 0.00013838]
		[batch 20/20] avg loss: 0.05273753586468578		[learning rate: 0.00013822]
	Learning Rate: 0.000138218
	LOSS [training: 0.050203859113033156 | validation: 0.047209136005461706]
	TIME [epoch: 8.15 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050432251315867		[learning rate: 0.00013805]
		[batch 20/20] avg loss: 0.05075115332607819		[learning rate: 0.00013789]
	Learning Rate: 0.000137892
	LOSS [training: 0.0505917023209726 | validation: 0.036877453513187414]
	TIME [epoch: 8.17 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04854194087791943		[learning rate: 0.00013773]
		[batch 20/20] avg loss: 0.04919528541457211		[learning rate: 0.00013757]
	Learning Rate: 0.000137567
	LOSS [training: 0.048868613146245764 | validation: 0.04931059229814323]
	TIME [epoch: 8.16 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04785364735633071		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.05614424086785447		[learning rate: 0.00013724]
	Learning Rate: 0.000137242
	LOSS [training: 0.05199894411209259 | validation: 0.04146068612090946]
	TIME [epoch: 8.15 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0515562651706285		[learning rate: 0.00013708]
		[batch 20/20] avg loss: 0.047606651622796606		[learning rate: 0.00013692]
	Learning Rate: 0.000136918
	LOSS [training: 0.04958145839671255 | validation: 0.04206267637007975]
	TIME [epoch: 8.15 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05359219559203593		[learning rate: 0.00013676]
		[batch 20/20] avg loss: 0.047656968060643665		[learning rate: 0.0001366]
	Learning Rate: 0.000136595
	LOSS [training: 0.0506245818263398 | validation: 0.044690296834277665]
	TIME [epoch: 8.16 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050816679053296884		[learning rate: 0.00013643]
		[batch 20/20] avg loss: 0.05369869180096058		[learning rate: 0.00013627]
	Learning Rate: 0.000136273
	LOSS [training: 0.05225768542712874 | validation: 0.04325946718334732]
	TIME [epoch: 8.17 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04580093017926264		[learning rate: 0.00013611]
		[batch 20/20] avg loss: 0.05097893575200221		[learning rate: 0.00013595]
	Learning Rate: 0.000135952
	LOSS [training: 0.04838993296563244 | validation: 0.04569276236480581]
	TIME [epoch: 8.15 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047470320355484645		[learning rate: 0.00013579]
		[batch 20/20] avg loss: 0.05491292880008478		[learning rate: 0.00013563]
	Learning Rate: 0.000135631
	LOSS [training: 0.05119162457778472 | validation: 0.0394245834496066]
	TIME [epoch: 8.16 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04638826407210637		[learning rate: 0.00013547]
		[batch 20/20] avg loss: 0.04831051925638079		[learning rate: 0.00013531]
	Learning Rate: 0.000135311
	LOSS [training: 0.04734939166424359 | validation: 0.046558004069850834]
	TIME [epoch: 8.16 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04424421935434885		[learning rate: 0.00013515]
		[batch 20/20] avg loss: 0.05099043641156718		[learning rate: 0.00013499]
	Learning Rate: 0.000134992
	LOSS [training: 0.04761732788295802 | validation: 0.04561150811726139]
	TIME [epoch: 8.17 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05822242786609266		[learning rate: 0.00013483]
		[batch 20/20] avg loss: 0.044488022771425655		[learning rate: 0.00013467]
	Learning Rate: 0.000134673
	LOSS [training: 0.051355225318759176 | validation: 0.04205390959397015]
	TIME [epoch: 8.16 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048528239971075186		[learning rate: 0.00013451]
		[batch 20/20] avg loss: 0.05543646364644976		[learning rate: 0.00013436]
	Learning Rate: 0.000134356
	LOSS [training: 0.05198235180876247 | validation: 0.04263636640448615]
	TIME [epoch: 8.15 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06156666248499003		[learning rate: 0.0001342]
		[batch 20/20] avg loss: 0.04762966079906532		[learning rate: 0.00013404]
	Learning Rate: 0.000134039
	LOSS [training: 0.05459816164202767 | validation: 0.03973619761586611]
	TIME [epoch: 8.16 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049580125637737824		[learning rate: 0.00013388]
		[batch 20/20] avg loss: 0.04611967621852023		[learning rate: 0.00013372]
	Learning Rate: 0.000133723
	LOSS [training: 0.04784990092812901 | validation: 0.03972540010058102]
	TIME [epoch: 8.18 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04905692420297592		[learning rate: 0.00013356]
		[batch 20/20] avg loss: 0.049805256772721185		[learning rate: 0.00013341]
	Learning Rate: 0.000133407
	LOSS [training: 0.04943109048784854 | validation: 0.04407535466538995]
	TIME [epoch: 8.15 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049999559298126384		[learning rate: 0.00013325]
		[batch 20/20] avg loss: 0.0513168271843442		[learning rate: 0.00013309]
	Learning Rate: 0.000133093
	LOSS [training: 0.05065819324123531 | validation: 0.03857581802921335]
	TIME [epoch: 8.15 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04416102630709519		[learning rate: 0.00013294]
		[batch 20/20] avg loss: 0.05536490310697674		[learning rate: 0.00013278]
	Learning Rate: 0.000132779
	LOSS [training: 0.04976296470703597 | validation: 0.047973294077221]
	TIME [epoch: 8.15 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04991379681619468		[learning rate: 0.00013262]
		[batch 20/20] avg loss: 0.04783789988729591		[learning rate: 0.00013247]
	Learning Rate: 0.000132465
	LOSS [training: 0.048875848351745285 | validation: 0.05515010335175529]
	TIME [epoch: 8.18 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052249777703924936		[learning rate: 0.00013231]
		[batch 20/20] avg loss: 0.04791770901403713		[learning rate: 0.00013215]
	Learning Rate: 0.000132153
	LOSS [training: 0.05008374335898104 | validation: 0.04014295244194348]
	TIME [epoch: 8.15 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05268369910709828		[learning rate: 0.000132]
		[batch 20/20] avg loss: 0.04833214500784617		[learning rate: 0.00013184]
	Learning Rate: 0.000131841
	LOSS [training: 0.05050792205747222 | validation: 0.03919163462166081]
	TIME [epoch: 8.15 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04823859877244936		[learning rate: 0.00013169]
		[batch 20/20] avg loss: 0.04626463255140556		[learning rate: 0.00013153]
	Learning Rate: 0.00013153
	LOSS [training: 0.04725161566192746 | validation: 0.03834254494964344]
	TIME [epoch: 8.15 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046004224262953225		[learning rate: 0.00013138]
		[batch 20/20] avg loss: 0.04818113265085147		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.04709267845690234 | validation: 0.03896511550651584]
	TIME [epoch: 8.19 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050568035142598		[learning rate: 0.00013107]
		[batch 20/20] avg loss: 0.050764631858153456		[learning rate: 0.00013091]
	Learning Rate: 0.00013091
	LOSS [training: 0.050666333500375714 | validation: 0.0442861938584348]
	TIME [epoch: 8.15 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050311607674526436		[learning rate: 0.00013076]
		[batch 20/20] avg loss: 0.04951306035108343		[learning rate: 0.0001306]
	Learning Rate: 0.000130602
	LOSS [training: 0.04991233401280493 | validation: 0.046335812365062914]
	TIME [epoch: 8.15 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05523378432485471		[learning rate: 0.00013045]
		[batch 20/20] avg loss: 0.048438290351009765		[learning rate: 0.00013029]
	Learning Rate: 0.000130294
	LOSS [training: 0.051836037337932225 | validation: 0.04016678991680318]
	TIME [epoch: 8.15 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04584504781680162		[learning rate: 0.00013014]
		[batch 20/20] avg loss: 0.051873165229154625		[learning rate: 0.00012999]
	Learning Rate: 0.000129986
	LOSS [training: 0.04885910652297813 | validation: 0.04339321385414932]
	TIME [epoch: 8.18 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046809455540675206		[learning rate: 0.00012983]
		[batch 20/20] avg loss: 0.04589081377233475		[learning rate: 0.00012968]
	Learning Rate: 0.00012968
	LOSS [training: 0.04635013465650499 | validation: 0.04234552330292301]
	TIME [epoch: 8.16 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05210257693475354		[learning rate: 0.00012953]
		[batch 20/20] avg loss: 0.04787937339540707		[learning rate: 0.00012937]
	Learning Rate: 0.000129374
	LOSS [training: 0.049990975165080304 | validation: 0.03980958384189162]
	TIME [epoch: 8.16 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04779384459602644		[learning rate: 0.00012922]
		[batch 20/20] avg loss: 0.04637180717086478		[learning rate: 0.00012907]
	Learning Rate: 0.000129069
	LOSS [training: 0.04708282588344561 | validation: 0.037669042497772524]
	TIME [epoch: 8.16 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05000980382698421		[learning rate: 0.00012892]
		[batch 20/20] avg loss: 0.045115072970456416		[learning rate: 0.00012876]
	Learning Rate: 0.000128764
	LOSS [training: 0.04756243839872031 | validation: 0.04724849809831906]
	TIME [epoch: 8.19 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04955440472773141		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.05004242561562786		[learning rate: 0.00012846]
	Learning Rate: 0.00012846
	LOSS [training: 0.04979841517167964 | validation: 0.05051934280909988]
	TIME [epoch: 8.15 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0495850473953793		[learning rate: 0.00012831]
		[batch 20/20] avg loss: 0.0543324674598771		[learning rate: 0.00012816]
	Learning Rate: 0.000128157
	LOSS [training: 0.05195875742762821 | validation: 0.03527433420288825]
	TIME [epoch: 8.16 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05074784461283631		[learning rate: 0.00012801]
		[batch 20/20] avg loss: 0.04833899518939472		[learning rate: 0.00012786]
	Learning Rate: 0.000127855
	LOSS [training: 0.049543419901115514 | validation: 0.03736581463034444]
	TIME [epoch: 8.15 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050207334849589745		[learning rate: 0.0001277]
		[batch 20/20] avg loss: 0.0421944992038792		[learning rate: 0.00012755]
	Learning Rate: 0.000127553
	LOSS [training: 0.04620091702673446 | validation: 0.04205703121530408]
	TIME [epoch: 8.17 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04813295893179926		[learning rate: 0.0001274]
		[batch 20/20] avg loss: 0.04434595047936479		[learning rate: 0.00012725]
	Learning Rate: 0.000127253
	LOSS [training: 0.046239454705582037 | validation: 0.03894868348880255]
	TIME [epoch: 8.15 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04590715923995038		[learning rate: 0.0001271]
		[batch 20/20] avg loss: 0.04733594784834023		[learning rate: 0.00012695]
	Learning Rate: 0.000126952
	LOSS [training: 0.04662155354414531 | validation: 0.045789377009031296]
	TIME [epoch: 8.16 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050244714167899905		[learning rate: 0.0001268]
		[batch 20/20] avg loss: 0.04170317963245857		[learning rate: 0.00012665]
	Learning Rate: 0.000126653
	LOSS [training: 0.04597394690017924 | validation: 0.0364340694753459]
	TIME [epoch: 8.16 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044821650835034275		[learning rate: 0.0001265]
		[batch 20/20] avg loss: 0.04989800945360067		[learning rate: 0.00012635]
	Learning Rate: 0.000126354
	LOSS [training: 0.04735983014431748 | validation: 0.043047277278164806]
	TIME [epoch: 8.18 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05253899173282521		[learning rate: 0.00012621]
		[batch 20/20] avg loss: 0.04774186841753451		[learning rate: 0.00012606]
	Learning Rate: 0.000126056
	LOSS [training: 0.05014043007517986 | validation: 0.03930662824011348]
	TIME [epoch: 8.16 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05007306041347873		[learning rate: 0.00012591]
		[batch 20/20] avg loss: 0.050083932760039127		[learning rate: 0.00012576]
	Learning Rate: 0.000125759
	LOSS [training: 0.05007849658675893 | validation: 0.04948853051998889]
	TIME [epoch: 8.16 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05196822332913105		[learning rate: 0.00012561]
		[batch 20/20] avg loss: 0.051576770403037764		[learning rate: 0.00012546]
	Learning Rate: 0.000125462
	LOSS [training: 0.05177249686608439 | validation: 0.05322934390971072]
	TIME [epoch: 8.16 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05082841242033953		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.054063280489298995		[learning rate: 0.00012517]
	Learning Rate: 0.000125166
	LOSS [training: 0.052445846454819256 | validation: 0.04447624375154043]
	TIME [epoch: 8.18 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050591318700211496		[learning rate: 0.00012502]
		[batch 20/20] avg loss: 0.049308888555010574		[learning rate: 0.00012487]
	Learning Rate: 0.000124871
	LOSS [training: 0.049950103627611035 | validation: 0.046430845023367556]
	TIME [epoch: 8.16 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04611829313070512		[learning rate: 0.00012472]
		[batch 20/20] avg loss: 0.050592431251585204		[learning rate: 0.00012458]
	Learning Rate: 0.000124576
	LOSS [training: 0.04835536219114517 | validation: 0.0514324144785365]
	TIME [epoch: 8.15 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04653921744063859		[learning rate: 0.00012443]
		[batch 20/20] avg loss: 0.04955544945366499		[learning rate: 0.00012428]
	Learning Rate: 0.000124283
	LOSS [training: 0.048047333447151785 | validation: 0.04304222586788775]
	TIME [epoch: 8.15 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05023138939868148		[learning rate: 0.00012414]
		[batch 20/20] avg loss: 0.04836210460059372		[learning rate: 0.00012399]
	Learning Rate: 0.000123989
	LOSS [training: 0.0492967469996376 | validation: 0.045573053079020515]
	TIME [epoch: 8.18 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04645946153311164		[learning rate: 0.00012384]
		[batch 20/20] avg loss: 0.044023545322373514		[learning rate: 0.0001237]
	Learning Rate: 0.000123697
	LOSS [training: 0.04524150342774257 | validation: 0.04646306643962261]
	TIME [epoch: 8.16 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04696342556797961		[learning rate: 0.00012355]
		[batch 20/20] avg loss: 0.04444474693119583		[learning rate: 0.00012341]
	Learning Rate: 0.000123405
	LOSS [training: 0.045704086249587716 | validation: 0.045465327166659464]
	TIME [epoch: 8.16 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04666813687113576		[learning rate: 0.00012326]
		[batch 20/20] avg loss: 0.04736924414993092		[learning rate: 0.00012311]
	Learning Rate: 0.000123114
	LOSS [training: 0.04701869051053334 | validation: 0.039815760623508056]
	TIME [epoch: 8.16 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04899583535761946		[learning rate: 0.00012297]
		[batch 20/20] avg loss: 0.046124395490184905		[learning rate: 0.00012282]
	Learning Rate: 0.000122824
	LOSS [training: 0.047560115423902184 | validation: 0.039427990073691514]
	TIME [epoch: 8.17 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04674818669589701		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.051018007042332836		[learning rate: 0.00012253]
	Learning Rate: 0.000122534
	LOSS [training: 0.04888309686911492 | validation: 0.042819640716873685]
	TIME [epoch: 8.17 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047178027847204096		[learning rate: 0.00012239]
		[batch 20/20] avg loss: 0.04320409736286742		[learning rate: 0.00012224]
	Learning Rate: 0.000122245
	LOSS [training: 0.04519106260503576 | validation: 0.04419993194711751]
	TIME [epoch: 8.16 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04632672676225701		[learning rate: 0.0001221]
		[batch 20/20] avg loss: 0.046208329841647564		[learning rate: 0.00012196]
	Learning Rate: 0.000121957
	LOSS [training: 0.04626752830195229 | validation: 0.041542749706471906]
	TIME [epoch: 8.16 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05129833169317026		[learning rate: 0.00012181]
		[batch 20/20] avg loss: 0.04635459428434617		[learning rate: 0.00012167]
	Learning Rate: 0.000121669
	LOSS [training: 0.048826462988758215 | validation: 0.04801353973829822]
	TIME [epoch: 8.18 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04922162424423025		[learning rate: 0.00012153]
		[batch 20/20] avg loss: 0.04754568962445202		[learning rate: 0.00012138]
	Learning Rate: 0.000121382
	LOSS [training: 0.048383656934341136 | validation: 0.04111433756231328]
	TIME [epoch: 8.17 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04758239083098647		[learning rate: 0.00012124]
		[batch 20/20] avg loss: 0.04879824859784921		[learning rate: 0.0001211]
	Learning Rate: 0.000121096
	LOSS [training: 0.04819031971441785 | validation: 0.047868058674051306]
	TIME [epoch: 8.17 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04756292970584493		[learning rate: 0.00012095]
		[batch 20/20] avg loss: 0.04413719658953961		[learning rate: 0.00012081]
	Learning Rate: 0.00012081
	LOSS [training: 0.045850063147692265 | validation: 0.04001049243160289]
	TIME [epoch: 8.16 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042816972904972564		[learning rate: 0.00012067]
		[batch 20/20] avg loss: 0.043526742256496116		[learning rate: 0.00012052]
	Learning Rate: 0.000120525
	LOSS [training: 0.043171857580734344 | validation: 0.04674754635848126]
	TIME [epoch: 8.18 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04902357719568909		[learning rate: 0.00012038]
		[batch 20/20] avg loss: 0.05497536554321756		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: 0.05199947136945332 | validation: 0.04574592730534818]
	TIME [epoch: 8.16 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046569810655589836		[learning rate: 0.0001201]
		[batch 20/20] avg loss: 0.055476770627748986		[learning rate: 0.00011996]
	Learning Rate: 0.000119957
	LOSS [training: 0.051023290641669404 | validation: 0.042403645223642164]
	TIME [epoch: 8.16 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05402272424711609		[learning rate: 0.00011982]
		[batch 20/20] avg loss: 0.047549662967605426		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.05078619360736074 | validation: 0.04896944286564919]
	TIME [epoch: 8.16 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047550536910180755		[learning rate: 0.00011953]
		[batch 20/20] avg loss: 0.05314012989043737		[learning rate: 0.00011939]
	Learning Rate: 0.000119392
	LOSS [training: 0.05034533340030907 | validation: 0.047067419311694454]
	TIME [epoch: 8.18 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04984822636428252		[learning rate: 0.00011925]
		[batch 20/20] avg loss: 0.048112890451018846		[learning rate: 0.00011911]
	Learning Rate: 0.00011911
	LOSS [training: 0.04898055840765069 | validation: 0.03727350078699089]
	TIME [epoch: 8.17 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051550790673468946		[learning rate: 0.00011897]
		[batch 20/20] avg loss: 0.043997310099542164		[learning rate: 0.00011883]
	Learning Rate: 0.000118829
	LOSS [training: 0.04777405038650555 | validation: 0.038253721550489374]
	TIME [epoch: 8.16 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04475974415726967		[learning rate: 0.00011869]
		[batch 20/20] avg loss: 0.04979625430528237		[learning rate: 0.00011855]
	Learning Rate: 0.000118549
	LOSS [training: 0.04727799923127602 | validation: 0.04328664656374286]
	TIME [epoch: 8.16 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04979440340251793		[learning rate: 0.00011841]
		[batch 20/20] avg loss: 0.04813695896658687		[learning rate: 0.00011827]
	Learning Rate: 0.000118269
	LOSS [training: 0.0489656811845524 | validation: 0.034756348688703156]
	TIME [epoch: 8.18 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04570287900693039		[learning rate: 0.00011813]
		[batch 20/20] avg loss: 0.04751651925982132		[learning rate: 0.00011799]
	Learning Rate: 0.00011799
	LOSS [training: 0.046609699133375856 | validation: 0.04350274452770428]
	TIME [epoch: 8.17 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05002518258999852		[learning rate: 0.00011785]
		[batch 20/20] avg loss: 0.05225415007805535		[learning rate: 0.00011771]
	Learning Rate: 0.000117712
	LOSS [training: 0.05113966633402693 | validation: 0.03918719049213549]
	TIME [epoch: 8.16 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051690186078670666		[learning rate: 0.00011757]
		[batch 20/20] avg loss: 0.045579132007057654		[learning rate: 0.00011743]
	Learning Rate: 0.000117434
	LOSS [training: 0.04863465904286417 | validation: 0.04801031175318678]
	TIME [epoch: 8.16 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052030917882719296		[learning rate: 0.0001173]
		[batch 20/20] avg loss: 0.047611859808587835		[learning rate: 0.00011716]
	Learning Rate: 0.000117157
	LOSS [training: 0.04982138884565356 | validation: 0.04829543669861651]
	TIME [epoch: 8.17 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050427471986401504		[learning rate: 0.00011702]
		[batch 20/20] avg loss: 0.04707191947839315		[learning rate: 0.00011688]
	Learning Rate: 0.000116881
	LOSS [training: 0.04874969573239733 | validation: 0.046676238900656045]
	TIME [epoch: 8.17 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05398686953725509		[learning rate: 0.00011674]
		[batch 20/20] avg loss: 0.04494138277700929		[learning rate: 0.00011661]
	Learning Rate: 0.000116605
	LOSS [training: 0.04946412615713218 | validation: 0.0390021650068299]
	TIME [epoch: 8.16 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04498369081461753		[learning rate: 0.00011647]
		[batch 20/20] avg loss: 0.049206559759602655		[learning rate: 0.00011633]
	Learning Rate: 0.00011633
	LOSS [training: 0.0470951252871101 | validation: 0.03798464166303499]
	TIME [epoch: 8.16 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0566070986555076		[learning rate: 0.00011619]
		[batch 20/20] avg loss: 0.0535989022708841		[learning rate: 0.00011606]
	Learning Rate: 0.000116056
	LOSS [training: 0.05510300046319585 | validation: 0.04730271423845611]
	TIME [epoch: 8.17 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05323869135308401		[learning rate: 0.00011592]
		[batch 20/20] avg loss: 0.05052966477810553		[learning rate: 0.00011578]
	Learning Rate: 0.000115782
	LOSS [training: 0.051884178065594776 | validation: 0.046681707925014096]
	TIME [epoch: 8.16 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04771136691520107		[learning rate: 0.00011565]
		[batch 20/20] avg loss: 0.05330058409581804		[learning rate: 0.00011551]
	Learning Rate: 0.000115509
	LOSS [training: 0.05050597550550956 | validation: 0.042776103905584256]
	TIME [epoch: 8.16 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051972509915615736		[learning rate: 0.00011537]
		[batch 20/20] avg loss: 0.04503988272769101		[learning rate: 0.00011524]
	Learning Rate: 0.000115236
	LOSS [training: 0.04850619632165337 | validation: 0.037464893982164925]
	TIME [epoch: 8.15 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04658884791057986		[learning rate: 0.0001151]
		[batch 20/20] avg loss: 0.04492274756262939		[learning rate: 0.00011496]
	Learning Rate: 0.000114965
	LOSS [training: 0.045755797736604625 | validation: 0.0482322262134479]
	TIME [epoch: 8.17 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04815294291101375		[learning rate: 0.00011483]
		[batch 20/20] avg loss: 0.042058263878012606		[learning rate: 0.00011469]
	Learning Rate: 0.000114693
	LOSS [training: 0.04510560339451318 | validation: 0.03975279753776779]
	TIME [epoch: 8.16 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049045422010864405		[learning rate: 0.00011456]
		[batch 20/20] avg loss: 0.048563941855946896		[learning rate: 0.00011442]
	Learning Rate: 0.000114423
	LOSS [training: 0.04880468193340565 | validation: 0.0381171383257662]
	TIME [epoch: 8.16 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048358697853994756		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.051366838724594424		[learning rate: 0.00011415]
	Learning Rate: 0.000114153
	LOSS [training: 0.04986276828929459 | validation: 0.04291885143628113]
	TIME [epoch: 8.15 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04919646355369374		[learning rate: 0.00011402]
		[batch 20/20] avg loss: 0.04996739882516684		[learning rate: 0.00011388]
	Learning Rate: 0.000113884
	LOSS [training: 0.04958193118943029 | validation: 0.03767743135631741]
	TIME [epoch: 8.17 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04756377741960062		[learning rate: 0.00011375]
		[batch 20/20] avg loss: 0.04391085154693824		[learning rate: 0.00011362]
	Learning Rate: 0.000113615
	LOSS [training: 0.04573731448326943 | validation: 0.03793796874939932]
	TIME [epoch: 8.16 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05044690894474868		[learning rate: 0.00011348]
		[batch 20/20] avg loss: 0.04949009307548059		[learning rate: 0.00011335]
	Learning Rate: 0.000113347
	LOSS [training: 0.04996850101011463 | validation: 0.048345128791460565]
	TIME [epoch: 8.16 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04520637113636598		[learning rate: 0.00011321]
		[batch 20/20] avg loss: 0.05212497848706945		[learning rate: 0.00011308]
	Learning Rate: 0.00011308
	LOSS [training: 0.04866567481171773 | validation: 0.041301436167244326]
	TIME [epoch: 8.16 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05180750689252096		[learning rate: 0.00011295]
		[batch 20/20] avg loss: 0.0484941617333705		[learning rate: 0.00011281]
	Learning Rate: 0.000112813
	LOSS [training: 0.05015083431294572 | validation: 0.04659132659578806]
	TIME [epoch: 8.16 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046455224845730954		[learning rate: 0.00011268]
		[batch 20/20] avg loss: 0.04341434618537925		[learning rate: 0.00011255]
	Learning Rate: 0.000112547
	LOSS [training: 0.0449347855155551 | validation: 0.0374209766128433]
	TIME [epoch: 8.17 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048902726216866414		[learning rate: 0.00011241]
		[batch 20/20] avg loss: 0.0498664132910447		[learning rate: 0.00011228]
	Learning Rate: 0.000112281
	LOSS [training: 0.04938456975395556 | validation: 0.04300895821546899]
	TIME [epoch: 8.16 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04950464989802219		[learning rate: 0.00011215]
		[batch 20/20] avg loss: 0.0459664991407604		[learning rate: 0.00011202]
	Learning Rate: 0.000112017
	LOSS [training: 0.0477355745193913 | validation: 0.04570555514554279]
	TIME [epoch: 8.16 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04930318644041858		[learning rate: 0.00011188]
		[batch 20/20] avg loss: 0.047216432662947366		[learning rate: 0.00011175]
	Learning Rate: 0.000111752
	LOSS [training: 0.04825980955168298 | validation: 0.038349292775530994]
	TIME [epoch: 8.16 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05100597158573113		[learning rate: 0.00011162]
		[batch 20/20] avg loss: 0.04975176081399317		[learning rate: 0.00011149]
	Learning Rate: 0.000111489
	LOSS [training: 0.05037886619986216 | validation: 0.04052736088793632]
	TIME [epoch: 8.18 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04930558719532722		[learning rate: 0.00011136]
		[batch 20/20] avg loss: 0.047007118516142934		[learning rate: 0.00011123]
	Learning Rate: 0.000111226
	LOSS [training: 0.04815635285573507 | validation: 0.03963289523026664]
	TIME [epoch: 8.15 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05188487606385609		[learning rate: 0.00011109]
		[batch 20/20] avg loss: 0.05020297210409798		[learning rate: 0.00011096]
	Learning Rate: 0.000110963
	LOSS [training: 0.05104392408397703 | validation: 0.044264679963733046]
	TIME [epoch: 8.16 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04910577608913265		[learning rate: 0.00011083]
		[batch 20/20] avg loss: 0.05126479099178988		[learning rate: 0.0001107]
	Learning Rate: 0.000110702
	LOSS [training: 0.050185283540461256 | validation: 0.03753167707678657]
	TIME [epoch: 8.15 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0447742503834725		[learning rate: 0.00011057]
		[batch 20/20] avg loss: 0.05307883464448354		[learning rate: 0.00011044]
	Learning Rate: 0.00011044
	LOSS [training: 0.04892654251397802 | validation: 0.042764618514243166]
	TIME [epoch: 8.19 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0490991036944556		[learning rate: 0.00011031]
		[batch 20/20] avg loss: 0.054018620134716866		[learning rate: 0.00011018]
	Learning Rate: 0.00011018
	LOSS [training: 0.05155886191458624 | validation: 0.03771091763155259]
	TIME [epoch: 8.15 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046565273027271745		[learning rate: 0.00011005]
		[batch 20/20] avg loss: 0.05799908844852286		[learning rate: 0.00010992]
	Learning Rate: 0.00010992
	LOSS [training: 0.052282180737897296 | validation: 0.043803226510804545]
	TIME [epoch: 8.16 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05049530405283674		[learning rate: 0.00010979]
		[batch 20/20] avg loss: 0.050568687708292745		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: 0.05053199588056474 | validation: 0.040968912711841075]
	TIME [epoch: 8.16 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046238533860638815		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 0.052823601205889616		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.04953106753326422 | validation: 0.0397906468127822]
	TIME [epoch: 8.18 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053652016493286346		[learning rate: 0.00010927]
		[batch 20/20] avg loss: 0.050987237159766684		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.05231962682652651 | validation: 0.042369665483918434]
	TIME [epoch: 8.15 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04615047578381189		[learning rate: 0.00010902]
		[batch 20/20] avg loss: 0.054416393047621615		[learning rate: 0.00010889]
	Learning Rate: 0.000108887
	LOSS [training: 0.05028343441571674 | validation: 0.0389339334038669]
	TIME [epoch: 8.16 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04542010629588368		[learning rate: 0.00010876]
		[batch 20/20] avg loss: 0.051904371786347825		[learning rate: 0.00010863]
	Learning Rate: 0.00010863
	LOSS [training: 0.04866223904111575 | validation: 0.04169649008511204]
	TIME [epoch: 8.16 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05399305294359227		[learning rate: 0.0001085]
		[batch 20/20] avg loss: 0.05182170845097872		[learning rate: 0.00010837]
	Learning Rate: 0.000108373
	LOSS [training: 0.05290738069728549 | validation: 0.035073323110902666]
	TIME [epoch: 8.18 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050226476980503476		[learning rate: 0.00010825]
		[batch 20/20] avg loss: 0.049345893848130686		[learning rate: 0.00010812]
	Learning Rate: 0.000108118
	LOSS [training: 0.04978618541431707 | validation: 0.042746122566855366]
	TIME [epoch: 8.16 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04280084465493219		[learning rate: 0.00010799]
		[batch 20/20] avg loss: 0.05196978730311018		[learning rate: 0.00010786]
	Learning Rate: 0.000107863
	LOSS [training: 0.047385315979021195 | validation: 0.052735075064250805]
	TIME [epoch: 8.15 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052645299858808815		[learning rate: 0.00010774]
		[batch 20/20] avg loss: 0.058647860232970236		[learning rate: 0.00010761]
	Learning Rate: 0.000107608
	LOSS [training: 0.05564658004588953 | validation: 0.04359944101377962]
	TIME [epoch: 8.16 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05238971930734091		[learning rate: 0.00010748]
		[batch 20/20] avg loss: 0.04791238912178661		[learning rate: 0.00010735]
	Learning Rate: 0.000107355
	LOSS [training: 0.05015105421456375 | validation: 0.047089674099064525]
	TIME [epoch: 8.17 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0549038743813286		[learning rate: 0.00010723]
		[batch 20/20] avg loss: 0.05085549118490014		[learning rate: 0.0001071]
	Learning Rate: 0.000107101
	LOSS [training: 0.05287968278311437 | validation: 0.042995974006706475]
	TIME [epoch: 8.15 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05370743126120285		[learning rate: 0.00010697]
		[batch 20/20] avg loss: 0.04718855497300793		[learning rate: 0.00010685]
	Learning Rate: 0.000106849
	LOSS [training: 0.05044799311710539 | validation: 0.04032873345266946]
	TIME [epoch: 8.16 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045850950404083025		[learning rate: 0.00010672]
		[batch 20/20] avg loss: 0.04503295674085725		[learning rate: 0.0001066]
	Learning Rate: 0.000106597
	LOSS [training: 0.045441953572470134 | validation: 0.044962745514205135]
	TIME [epoch: 8.15 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047778929200113736		[learning rate: 0.00010647]
		[batch 20/20] avg loss: 0.04747549238627809		[learning rate: 0.00010635]
	Learning Rate: 0.000106345
	LOSS [training: 0.047627210793195915 | validation: 0.043760763520563066]
	TIME [epoch: 8.18 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0515547858075972		[learning rate: 0.00010622]
		[batch 20/20] avg loss: 0.055170932747262445		[learning rate: 0.00010609]
	Learning Rate: 0.000106094
	LOSS [training: 0.05336285927742983 | validation: 0.054158176361480376]
	TIME [epoch: 8.16 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05165810413927219		[learning rate: 0.00010597]
		[batch 20/20] avg loss: 0.047317847324380664		[learning rate: 0.00010584]
	Learning Rate: 0.000105844
	LOSS [training: 0.049487975731826415 | validation: 0.03756137728761505]
	TIME [epoch: 8.16 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04398986397344521		[learning rate: 0.00010572]
		[batch 20/20] avg loss: 0.054007986053339285		[learning rate: 0.00010559]
	Learning Rate: 0.000105594
	LOSS [training: 0.048998925013392244 | validation: 0.03958799404678476]
	TIME [epoch: 8.15 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05398755172644254		[learning rate: 0.00010547]
		[batch 20/20] avg loss: 0.04390625415132647		[learning rate: 0.00010535]
	Learning Rate: 0.000105345
	LOSS [training: 0.048946902938884505 | validation: 0.041277246872308944]
	TIME [epoch: 8.18 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051784244803494583		[learning rate: 0.00010522]
		[batch 20/20] avg loss: 0.04444133672531562		[learning rate: 0.0001051]
	Learning Rate: 0.000105097
	LOSS [training: 0.048112790764405104 | validation: 0.03909867607130299]
	TIME [epoch: 8.15 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04647951359717833		[learning rate: 0.00010497]
		[batch 20/20] avg loss: 0.047188866174370236		[learning rate: 0.00010485]
	Learning Rate: 0.000104849
	LOSS [training: 0.04683418988577428 | validation: 0.041250580561420155]
	TIME [epoch: 8.15 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04801017360662329		[learning rate: 0.00010473]
		[batch 20/20] avg loss: 0.05392709766612992		[learning rate: 0.0001046]
	Learning Rate: 0.000104602
	LOSS [training: 0.0509686356363766 | validation: 0.03490445428356573]
	TIME [epoch: 8.16 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042071083198127635		[learning rate: 0.00010448]
		[batch 20/20] avg loss: 0.05695822770873153		[learning rate: 0.00010435]
	Learning Rate: 0.000104355
	LOSS [training: 0.049514655453429576 | validation: 0.03739644168590585]
	TIME [epoch: 8.18 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04982540747921117		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.0529398165177369		[learning rate: 0.00010411]
	Learning Rate: 0.000104109
	LOSS [training: 0.051382611998474034 | validation: 0.044218463650505434]
	TIME [epoch: 8.16 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058629434827365066		[learning rate: 0.00010399]
		[batch 20/20] avg loss: 0.04734950316186551		[learning rate: 0.00010386]
	Learning Rate: 0.000103863
	LOSS [training: 0.052989468994615284 | validation: 0.043948603594532044]
	TIME [epoch: 8.15 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047033541370891004		[learning rate: 0.00010374]
		[batch 20/20] avg loss: 0.04826668530270764		[learning rate: 0.00010362]
	Learning Rate: 0.000103618
	LOSS [training: 0.04765011333679932 | validation: 0.03497662434520722]
	TIME [epoch: 8.15 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045052904169258015		[learning rate: 0.0001035]
		[batch 20/20] avg loss: 0.04912437210261429		[learning rate: 0.00010337]
	Learning Rate: 0.000103374
	LOSS [training: 0.04708863813593615 | validation: 0.041955644358769695]
	TIME [epoch: 8.18 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04669344764014061		[learning rate: 0.00010325]
		[batch 20/20] avg loss: 0.04517617951534804		[learning rate: 0.00010313]
	Learning Rate: 0.00010313
	LOSS [training: 0.04593481357774432 | validation: 0.03914168847901494]
	TIME [epoch: 8.16 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04981145720313599		[learning rate: 0.00010301]
		[batch 20/20] avg loss: 0.04678416725601469		[learning rate: 0.00010289]
	Learning Rate: 0.000102887
	LOSS [training: 0.04829781222957533 | validation: 0.03974233542481026]
	TIME [epoch: 8.16 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04336221249677389		[learning rate: 0.00010277]
		[batch 20/20] avg loss: 0.057260349449286055		[learning rate: 0.00010264]
	Learning Rate: 0.000102644
	LOSS [training: 0.05031128097302997 | validation: 0.04263319808051684]
	TIME [epoch: 8.15 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0495476345627339		[learning rate: 0.00010252]
		[batch 20/20] avg loss: 0.044522595316165134		[learning rate: 0.0001024]
	Learning Rate: 0.000102402
	LOSS [training: 0.04703511493944952 | validation: 0.04672255625500944]
	TIME [epoch: 8.19 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050931960553814236		[learning rate: 0.00010228]
		[batch 20/20] avg loss: 0.046237672219435436		[learning rate: 0.00010216]
	Learning Rate: 0.00010216
	LOSS [training: 0.04858481638662483 | validation: 0.036110482928244314]
	TIME [epoch: 8.15 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04238155893658048		[learning rate: 0.00010204]
		[batch 20/20] avg loss: 0.04533332488800499		[learning rate: 0.00010192]
	Learning Rate: 0.000101919
	LOSS [training: 0.043857441912292724 | validation: 0.043654396665927915]
	TIME [epoch: 8.17 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05134059099597266		[learning rate: 0.0001018]
		[batch 20/20] avg loss: 0.04634040811035152		[learning rate: 0.00010168]
	Learning Rate: 0.000101679
	LOSS [training: 0.04884049955316209 | validation: 0.03812025109504501]
	TIME [epoch: 8.15 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046800057563174596		[learning rate: 0.00010156]
		[batch 20/20] avg loss: 0.0465296355104871		[learning rate: 0.00010144]
	Learning Rate: 0.000101439
	LOSS [training: 0.046664846536830853 | validation: 0.042535800527588305]
	TIME [epoch: 8.17 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04730133190399395		[learning rate: 0.00010132]
		[batch 20/20] avg loss: 0.047717720800474706		[learning rate: 0.0001012]
	Learning Rate: 0.0001012
	LOSS [training: 0.04750952635223432 | validation: 0.04256886258535863]
	TIME [epoch: 8.15 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04978283147134581		[learning rate: 0.00010108]
		[batch 20/20] avg loss: 0.05169163946070186		[learning rate: 0.00010096]
	Learning Rate: 0.000100961
	LOSS [training: 0.05073723546602384 | validation: 0.054060307768725384]
	TIME [epoch: 8.16 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048335450919957765		[learning rate: 0.00010084]
		[batch 20/20] avg loss: 0.054007388077241314		[learning rate: 0.00010072]
	Learning Rate: 0.000100723
	LOSS [training: 0.051171419498599546 | validation: 0.04251846977989035]
	TIME [epoch: 8.15 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04330820404049464		[learning rate: 0.0001006]
		[batch 20/20] avg loss: 0.0574395930436461		[learning rate: 0.00010049]
	Learning Rate: 0.000100485
	LOSS [training: 0.050373898542070375 | validation: 0.04095091820219681]
	TIME [epoch: 8.18 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04780874430030273		[learning rate: 0.00010037]
		[batch 20/20] avg loss: 0.043491582489286855		[learning rate: 0.00010025]
	Learning Rate: 0.000100248
	LOSS [training: 0.04565016339479479 | validation: 0.03712207691096952]
	TIME [epoch: 8.15 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047696992629217796		[learning rate: 0.00010013]
		[batch 20/20] avg loss: 0.05455786626475069		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 0.05112742944698423 | validation: 0.042669292274504125]
	TIME [epoch: 8.15 sec]
Finished training in 16492.427 seconds.
