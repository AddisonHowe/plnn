Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r0', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2504057183

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.419423917754147		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.824416648040245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.121920282897195 | validation: 8.482887236756424]
	TIME [epoch: 79.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.605962928393929		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.33872793585166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.472345432122792 | validation: 7.2148677563865125]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.797384632048063		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.670921123034209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.734152877541137 | validation: 8.251229693889993]
	TIME [epoch: 8.2 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.586908782358519		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.327218704677021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.457063743517769 | validation: 6.907597943793883]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.67875649391853		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.364802114263488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.52177930409101 | validation: 6.7895689055405395]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.25436490066353		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.934971030395033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.094667965529284 | validation: 6.400116087253155]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.252857572197151		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.868410779467401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.060634175832275 | validation: 6.408329712395268]
	TIME [epoch: 8.19 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.6054770493324755		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.9539034508340976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.279690250083287 | validation: 5.452133868882394]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.3099421550298		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.954253897878341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.132098026454072 | validation: 5.339276536150498]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.39652003112195		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.9169457247293487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.156732877925649 | validation: 3.5329183882760598]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.254509502260775		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.186811816591332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2206606594260534 | validation: 3.201518737547454]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7058152801622364		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.311249174881455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.508532227521846 | validation: 2.0471114385243996]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.070165720803052		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4351939920312784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.252679856417165 | validation: 1.8805962360110002]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6871299698270825		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6893908181505588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6882603939888206 | validation: 1.4165372114021637]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7072239560723432		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9436267307546398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8254253434134913 | validation: 1.4771914935459143]
	TIME [epoch: 8.19 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5609134874439508		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4856694880353158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5232914877396333 | validation: 1.778301238757798]
	TIME [epoch: 8.17 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6041745082282706		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5657175238732948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5849460160507824 | validation: 1.4450808382179547]
	TIME [epoch: 8.16 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.548487773915808		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6751858054630169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6118367896894124 | validation: 1.3735071763771223]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.683605657318338		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.733844349558895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7087250034386166 | validation: 1.8844213467450732]
	TIME [epoch: 8.19 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.701938825939635		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.718696985914931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7103179059272826 | validation: 1.3611515164764019]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6211381288603364		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7031169562344224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6621275425473794 | validation: 1.9846519932091407]
	TIME [epoch: 8.17 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.625060399283759		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4747567469318006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5499085731077797 | validation: 1.6035670275448073]
	TIME [epoch: 8.17 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.406131536370577		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6420806025685377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5241060694695574 | validation: 1.550970243974702]
	TIME [epoch: 8.2 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4972502785806565		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5330943470858789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.515172312833268 | validation: 1.426046463971566]
	TIME [epoch: 8.16 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3963510170434232		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5629378821060818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4796444495747525 | validation: 1.1859272885234389]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.388658694723958		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4995971660288803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.444127930376419 | validation: 1.4574048926162957]
	TIME [epoch: 8.16 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4906875596654985		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4345417518135113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.462614655739505 | validation: 1.2589437453329468]
	TIME [epoch: 8.19 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4780648806984638		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3542373501759002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.416151115437182 | validation: 1.420819552389878]
	TIME [epoch: 8.16 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4295504256920668		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2975279610562969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3635391933741816 | validation: 1.6042213883435856]
	TIME [epoch: 8.15 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.303153561440919		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3250658910759612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3141097262584398 | validation: 1.1861773382391068]
	TIME [epoch: 8.15 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.476486453008293		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.32799427845088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4022403657295868 | validation: 1.2646721569620576]
	TIME [epoch: 8.18 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2894746576731546		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2955989576812998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2925368076772272 | validation: 1.2518896836367823]
	TIME [epoch: 8.16 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.253074258994302		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4018941732688988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3274842161316003 | validation: 1.389287789409671]
	TIME [epoch: 8.14 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.294984157789891		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3733513822183538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3341677700041221 | validation: 1.3037223045326056]
	TIME [epoch: 8.17 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2958837279033038		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2144325893039523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2551581586036278 | validation: 2.139100132277323]
	TIME [epoch: 8.17 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.760690912900478		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2080230746786853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9843569937895817 | validation: 1.3344503006247872]
	TIME [epoch: 8.15 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5520481495914307		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1232403040279422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3376442268096866 | validation: 1.158560089478072]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1819908293632317		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2419326363794638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2119617328713477 | validation: 1.1433192927376805]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2189699431060392		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2318173673147474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2253936552103932 | validation: 1.3920177418870625]
	TIME [epoch: 8.17 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.209191460048769		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1420876833185116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17563957168364 | validation: 1.4349268893785372]
	TIME [epoch: 8.14 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1788511673963746		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.179569583468735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.179210375432555 | validation: 1.6818871753687776]
	TIME [epoch: 8.14 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.364105573723796		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.504991827244089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4345487004839423 | validation: 1.593133886072522]
	TIME [epoch: 8.17 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4546276767707829		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1408741840357126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.297750930403248 | validation: 1.1626989695706917]
	TIME [epoch: 8.15 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1408002005667486		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.242399023096374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1915996118315613 | validation: 1.128304054160209]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1079263460044446		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.256673852643305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1823000993238748 | validation: 1.22699899164454]
	TIME [epoch: 8.16 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0985322525588377		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.145450376432681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1219913144957596 | validation: 0.9618833658290638]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.223517362649394		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0630693010198315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.143293331834613 | validation: 1.2133043831616437]
	TIME [epoch: 8.16 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1039718804725243		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2285491939543056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.166260537213415 | validation: 1.0266473388129627]
	TIME [epoch: 8.16 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.060850874540247		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1109451787442053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.085898026642226 | validation: 1.0121065581090694]
	TIME [epoch: 8.15 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.006793509643963		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0842616785406403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0455275940923017 | validation: 1.1063434245454027]
	TIME [epoch: 8.18 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0395798001933785		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 1.0870709997582346		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 1.0633253999758066 | validation: 1.0150964662692947]
	TIME [epoch: 8.16 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0904217782089982		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 1.087153922452917		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.088787850330958 | validation: 1.068086927928076]
	TIME [epoch: 8.16 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.117524750710818		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 1.0289886566099125		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 1.0732567036603653 | validation: 1.0816547574409305]
	TIME [epoch: 8.15 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.057329124387585		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 1.0117195844799345		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 1.0345243544337595 | validation: 0.9911098773932236]
	TIME [epoch: 8.18 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0844921129077183		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 1.0945108150095564		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 1.0895014639586371 | validation: 1.341278743936035]
	TIME [epoch: 8.16 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0184492022761762		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 0.9603019039851814		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 0.9893755531306787 | validation: 0.868120717033229]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9278996848043137		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 1.0247484292660256		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 0.9763240570351698 | validation: 0.9221560789951049]
	TIME [epoch: 8.19 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9739560072521662		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 0.9243005848749982		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 0.9491282960635822 | validation: 0.8583857676490512]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9289524832386151		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 0.9015932904789892		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 0.9152728868588023 | validation: 1.0795738418679643]
	TIME [epoch: 8.18 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9254596219451171		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 1.0026930327718448		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 0.9640763273584811 | validation: 0.9743115374741247]
	TIME [epoch: 8.18 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0367402592489643		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 0.899648357540541		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 0.9681943083947526 | validation: 0.9111118378877433]
	TIME [epoch: 8.19 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8900575094044271		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 0.8178621848595403		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 0.8539598471319836 | validation: 1.4483617475797232]
	TIME [epoch: 8.21 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0391839650027832		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 0.8592583000926709		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 0.9492211325477271 | validation: 0.9341351241403458]
	TIME [epoch: 8.18 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8554661017391758		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 0.8213605890800879		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 0.838413345409632 | validation: 1.2489910549545349]
	TIME [epoch: 8.18 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9374304164449626		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 0.8191695476117445		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 0.8782999820283536 | validation: 0.8117746419415026]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8925521645094723		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 1.0326939902511523		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 0.9626230773803123 | validation: 0.9731447540455629]
	TIME [epoch: 8.21 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.86073969694619		[learning rate: 0.009619]
		[batch 20/20] avg loss: 0.8502410333157677		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 0.8554903651309788 | validation: 0.8367545062651829]
	TIME [epoch: 8.18 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.81970209120361		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 0.8583460728515669		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 0.8390240820275885 | validation: 1.2949915928051936]
	TIME [epoch: 8.18 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0017948868579314		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 1.0046071337465072		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 1.0032010103022195 | validation: 0.8229533840255225]
	TIME [epoch: 8.18 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9695545753714926		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 0.7450298172940698		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 0.8572921963327813 | validation: 0.8276460948594233]
	TIME [epoch: 8.2 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7607144238194246		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 0.8841158189523455		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 0.822415121385885 | validation: 0.7681907024529926]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8239427175112057		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.9161646263686262		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 0.8700536719399159 | validation: 0.7953898600199206]
	TIME [epoch: 8.18 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7749178729849903		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 0.8504198445379652		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 0.8126688587614777 | validation: 0.9086092172055908]
	TIME [epoch: 8.17 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7663969217221897		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 0.7949280366655099		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 0.7806624791938498 | validation: 0.8803722443977448]
	TIME [epoch: 8.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7430177376108533		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 0.8607051215665183		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 0.8018614295886859 | validation: 0.6248639394928427]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7871324039005677		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 0.6681072574256659		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 0.727619830663117 | validation: 0.6112014707707666]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7014827493759621		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 0.7563764509196071		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 0.7289296001477845 | validation: 0.6377892967391716]
	TIME [epoch: 8.16 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7020828740625129		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 0.7184181993157447		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 0.7102505366891287 | validation: 0.8162443349502779]
	TIME [epoch: 8.19 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8713754661825153		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 0.8345327318083517		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 0.8529540989954334 | validation: 1.1182738928187381]
	TIME [epoch: 8.17 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8847597810980361		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 0.6940502354948601		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 0.7894050082964481 | validation: 0.6321415303097718]
	TIME [epoch: 8.16 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7342139570983794		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 0.7935201547495834		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 0.7638670559239814 | validation: 0.8204142478238478]
	TIME [epoch: 8.16 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7224442672577278		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 0.7387789036065316		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 0.7306115854321297 | validation: 0.7154645239160996]
	TIME [epoch: 8.19 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6780027452820317		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 0.7341190135505238		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 0.7060608794162777 | validation: 0.6540538009889586]
	TIME [epoch: 8.17 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8796644080701934		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 0.7443599607180866		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 0.8120121843941399 | validation: 1.0016823083993887]
	TIME [epoch: 8.16 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7242983384099972		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 0.6825735976917631		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 0.70343596805088 | validation: 0.48882645221642324]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6128581255964481		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.715943143923593		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.6644006347600206 | validation: 0.9946164795925394]
	TIME [epoch: 8.19 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8143405916373906		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.6711910044380318		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.7427657980377113 | validation: 0.5297237388716388]
	TIME [epoch: 8.17 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7184747235759616		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 0.6637710429122328		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 0.6911228832440972 | validation: 0.7659996203399483]
	TIME [epoch: 8.16 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6679197105899661		[learning rate: 0.009132]
		[batch 20/20] avg loss: 0.6652174599369971		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.6665685852634816 | validation: 0.6861842173494908]
	TIME [epoch: 8.16 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6262464846586864		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.6307710722850937		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.6285087784718901 | validation: 0.7677581424782283]
	TIME [epoch: 8.18 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5788658232281627		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.7090695747156462		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.6439676989719044 | validation: 0.484241644564994]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5813732984500672		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 0.6534616580445222		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.6174174782472948 | validation: 0.391036385352901]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6882687305253209		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.578702904398907		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.6334858174621141 | validation: 0.7405239889898019]
	TIME [epoch: 8.17 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6009086798425691		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.622827346273988		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.6118680130582785 | validation: 0.5019196830111944]
	TIME [epoch: 8.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5575240740297736		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.7317390313916331		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.6446315527107034 | validation: 0.6546536331265507]
	TIME [epoch: 8.17 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6854567725011215		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.626962089597346		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.6562094310492339 | validation: 0.48039771367444317]
	TIME [epoch: 8.16 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6303951425174373		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.6836606955894706		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.6570279190534539 | validation: 0.9065665733439134]
	TIME [epoch: 8.17 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5786296656053838		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.5897848813081047		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.5842072734567442 | validation: 0.5727844382955349]
	TIME [epoch: 8.18 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6921435222658145		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.6181440605892504		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.6551437914275325 | validation: 0.5008184908575337]
	TIME [epoch: 8.18 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5792859975563052		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 0.6982757277374494		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.6387808626468774 | validation: 0.5550482508890465]
	TIME [epoch: 8.17 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7382279404064822		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.6580976579460002		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.6981627991762412 | validation: 0.4602746025990673]
	TIME [epoch: 8.17 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6892817837495164		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 0.6625263599078905		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.6759040718287035 | validation: 1.6417259717212316]
	TIME [epoch: 8.19 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.077934334639724		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.9615510787869865		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 1.0197427067133553 | validation: 0.7879584094646367]
	TIME [epoch: 8.18 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.791387530227904		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.5397573022687933		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.6655724162483486 | validation: 0.43162940932757565]
	TIME [epoch: 8.17 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5908247863042964		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.5655256885166613		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.5781752374104788 | validation: 1.324003966322018]
	TIME [epoch: 8.17 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8443119658540192		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.5640793470031811		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.7041956564286 | validation: 0.7359939603666963]
	TIME [epoch: 8.19 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7438352925953213		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.8065503123197557		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.7751928024575385 | validation: 0.42414981474540003]
	TIME [epoch: 8.18 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5394298662673118		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.7206825059470613		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.6300561861071865 | validation: 0.5665883144686328]
	TIME [epoch: 8.17 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5247506028450255		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.7727224079901958		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.6487365054176106 | validation: 0.7811748571523975]
	TIME [epoch: 8.16 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7768767880791925		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.5900937120532077		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.6834852500662 | validation: 0.45933273676823544]
	TIME [epoch: 8.18 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6359725275932421		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.6443797798434326		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.6401761537183375 | validation: 1.6533930334769509]
	TIME [epoch: 8.18 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7235233548908815		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 1.0596654729819828		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.8915944139364325 | validation: 0.6856158231302436]
	TIME [epoch: 8.17 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5860822258293567		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.6269023647355972		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.6064922952824771 | validation: 0.5070987675270099]
	TIME [epoch: 8.17 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6533323260962745		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.7149755805468223		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.6841539533215484 | validation: 1.3364043643301526]
	TIME [epoch: 8.2 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.841504311617487		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.6299716150784468		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.7357379633479669 | validation: 0.9432559823422221]
	TIME [epoch: 8.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8875709298165047		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.6624081908190634		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.7749895603177841 | validation: 0.47374799551765645]
	TIME [epoch: 8.18 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5761749258414439		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.596215876265493		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.5861954010534685 | validation: 0.6903912731857058]
	TIME [epoch: 8.19 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8208117708628608		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 1.0627336676769665		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.9417727192699139 | validation: 0.5228776585352666]
	TIME [epoch: 8.2 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7998367735744765		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.6983832982735241		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.7491100359240003 | validation: 1.5680749893614625]
	TIME [epoch: 8.21 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7849653016958824		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.6984522082034561		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.7417087549496696 | validation: 0.6234434793159611]
	TIME [epoch: 8.19 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.634657076811301		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.7189252626499768		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.676791169730639 | validation: 0.6657612112248121]
	TIME [epoch: 8.19 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.575856761857802		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.5714979875974645		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.5736773747276332 | validation: 0.6189910792338605]
	TIME [epoch: 8.19 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5231085713189888		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.5849314728040571		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.554020022061523 | validation: 0.40027147233928656]
	TIME [epoch: 8.22 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8114672192379251		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.6344035319263587		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.7229353755821417 | validation: 0.5802586637395265]
	TIME [epoch: 8.19 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5796820696362733		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.4791109020637707		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.529396485850022 | validation: 0.34578442527055836]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4972878483472498		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.644857578575597		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.5710727134614233 | validation: 0.4912547084060633]
	TIME [epoch: 8.16 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41486592587332743		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.788995243389051		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.6019305846311891 | validation: 0.5640656823581678]
	TIME [epoch: 8.17 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5610197438174602		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.792853377556439		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.6769365606869496 | validation: 1.0075465207111443]
	TIME [epoch: 8.14 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6810060603358927		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.5676447238033766		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.6243253920696347 | validation: 0.7648137702870093]
	TIME [epoch: 8.13 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6882570630068418		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.8675442389842936		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.7779006509955677 | validation: 0.7491967095377084]
	TIME [epoch: 8.14 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5550575633793186		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.5616956725975817		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.5583766179884502 | validation: 0.2924626816590999]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5036645363497635		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.5371331512679385		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.520398843808851 | validation: 0.3601003556011054]
	TIME [epoch: 8.19 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5538853749091042		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.6756043797663104		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.6147448773377073 | validation: 0.6680757226619491]
	TIME [epoch: 8.17 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5390961852721023		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.5156522437924174		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.5273742145322597 | validation: 0.3185853487350798]
	TIME [epoch: 8.18 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4155999911047209		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.6072441491842124		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.5114220701444666 | validation: 0.6004756688935596]
	TIME [epoch: 8.19 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5386873886598252		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.45421315044323496		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.49645026955153015 | validation: 0.6921895278577829]
	TIME [epoch: 8.16 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.505129122658374		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.44840163398590105		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.4767653783221375 | validation: 0.5863336328739577]
	TIME [epoch: 8.17 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5653613054056443		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.5391746836659539		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.5522679945357992 | validation: 0.5149336480721057]
	TIME [epoch: 8.17 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4627457213332963		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.7055320897623247		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.5841389055478106 | validation: 2.1172387405047894]
	TIME [epoch: 8.19 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.609963189473589		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.6160031354476444		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.6129831624606167 | validation: 0.7727853496886132]
	TIME [epoch: 8.15 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45083917631896825		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.6724023695692136		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.5616207729440909 | validation: 0.8738310560952205]
	TIME [epoch: 8.15 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6471584038525087		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.8736937671965042		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.7604260855245065 | validation: 0.6047383629519802]
	TIME [epoch: 8.15 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5907494358676815		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.5274828911095002		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.5591161634885908 | validation: 1.1190587418996594]
	TIME [epoch: 8.18 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7232490670074964		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.42066705669628435		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.5719580618518902 | validation: 0.2975535584785505]
	TIME [epoch: 8.15 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5511269714001897		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.6672252017786823		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.6091760865894361 | validation: 0.38385116947616155]
	TIME [epoch: 8.16 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5700816483776204		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.5719034396907272		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.5709925440341739 | validation: 0.43061290695442545]
	TIME [epoch: 8.15 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5477256473450511		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.7649312947017732		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.6563284710234122 | validation: 0.6005026841236637]
	TIME [epoch: 8.18 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2299130245645657		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.6070505245324982		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.9184817745485322 | validation: 0.5269825268375242]
	TIME [epoch: 8.15 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44284192248368176		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.6236564419111421		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.533249182197412 | validation: 1.6467210095819018]
	TIME [epoch: 8.16 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6955751140717791		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.47836322150073673		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.586969167786258 | validation: 0.5655802033787198]
	TIME [epoch: 8.15 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5241599375732326		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.5090061182019563		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.5165830278875945 | validation: 0.5600335932928326]
	TIME [epoch: 8.18 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5121992308164501		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.48403359455016226		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.4981164126833061 | validation: 0.4745992577384812]
	TIME [epoch: 8.16 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45266409153655607		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.46537472735843377		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.4590194094474948 | validation: 0.4047585204690806]
	TIME [epoch: 8.17 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4615310718649144		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.541604212171882		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.5015676420183982 | validation: 0.40861966978173336]
	TIME [epoch: 8.15 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5098026542870059		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.5083320410793022		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.509067347683154 | validation: 0.4381449113118816]
	TIME [epoch: 8.18 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5113794296160343		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.5414349500322528		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.5264071898241437 | validation: 0.4442791539268737]
	TIME [epoch: 8.16 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5073445796522584		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.4272712537204719		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.46730791668636523 | validation: 0.44987851117320354]
	TIME [epoch: 8.16 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4427887193700252		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.49718701944922994		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.46998786940962767 | validation: 0.44748817732805096]
	TIME [epoch: 8.15 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4205939706356182		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.37037013766678095		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.3954820541511995 | validation: 0.41248083508265465]
	TIME [epoch: 8.18 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36772278998086005		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.42991046458767046		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.39881662728426526 | validation: 0.3245359080418488]
	TIME [epoch: 8.15 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44222650060820656		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.4829951841200885		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.4626108423641476 | validation: 0.4594711135087402]
	TIME [epoch: 8.16 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5474541718310297		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.4394403336056779		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.4934472527183539 | validation: 0.452737881687]
	TIME [epoch: 8.16 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4431455225334527		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.4256655183467057		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.43440552044007913 | validation: 0.5578139131805135]
	TIME [epoch: 8.18 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46687215059975723		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.7039204369906563		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.5853962937952069 | validation: 0.4505357234636446]
	TIME [epoch: 8.16 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4791273949346002		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.35227340253689554		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.4157003987357479 | validation: 0.2909220653197897]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41632791795331114		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.34256934160456687		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.37944862977893895 | validation: 0.6200250506328407]
	TIME [epoch: 8.15 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34710924939581955		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.472806062239302		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.4099576558175608 | validation: 0.5089017279398987]
	TIME [epoch: 8.17 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3883868218092326		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.4439645456430831		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.4161756837261579 | validation: 0.47647464313018]
	TIME [epoch: 8.16 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45926299174999147		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.4459924430558324		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.45262771740291197 | validation: 0.4291277868073139]
	TIME [epoch: 8.15 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3409031182141108		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.3437270406082822		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.3423150794111965 | validation: 0.3626591101873029]
	TIME [epoch: 8.14 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29888056072346536		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.3702957173722946		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.33458813904788 | validation: 0.5414623848776393]
	TIME [epoch: 8.17 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4441261661203292		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.43513036865575083		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.43962826738803995 | validation: 0.27677362331127797]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3712399977236863		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.5232510768477464		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.44724553728571637 | validation: 0.7523063001823898]
	TIME [epoch: 8.2 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3550968408914222		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.4001740385719382		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.3776354397316802 | validation: 0.29885167980555]
	TIME [epoch: 8.18 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40062842493288003		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.3874132702420084		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.39402084758744416 | validation: 0.40661465929030216]
	TIME [epoch: 8.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4141122981195279		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.3428707087786593		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.37849150344909366 | validation: 0.4120283374338083]
	TIME [epoch: 8.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4236600505216798		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.3618119923508628		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.39273602143627134 | validation: 0.17678555871995766]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32733097371908426		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.3683167451964895		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.34782385945778693 | validation: 0.34843724884369054]
	TIME [epoch: 8.19 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35826173031513747		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.45257639036516684		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.4054190603401522 | validation: 0.3200372212923234]
	TIME [epoch: 8.2 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37071278493064147		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.34475650751279724		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.35773464622171935 | validation: 0.37238215350863413]
	TIME [epoch: 8.19 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3435955246458899		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.3481676903280365		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.3458816074869632 | validation: 0.4077409295954388]
	TIME [epoch: 8.18 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41650100248490096		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.35576770920834583		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.38613435584662337 | validation: 0.2593477556881319]
	TIME [epoch: 8.18 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28814898382030285		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.40182616851258873		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.34498757616644576 | validation: 0.5394161632172877]
	TIME [epoch: 8.2 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.345160886274072		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.35307637493580424		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.3491186306049381 | validation: 0.45579437593350425]
	TIME [epoch: 8.19 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33246896051066566		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.3549392647638689		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.3437041126372673 | validation: 0.5845935094178996]
	TIME [epoch: 8.18 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4532845996266784		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.3590807859688202		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.4061826927977493 | validation: 0.5615156494555577]
	TIME [epoch: 8.18 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.628821880801772		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.47032027504854435		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.5495710779251582 | validation: 0.24463680489174466]
	TIME [epoch: 8.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3810150647753658		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.2838864727557201		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.33245076876554297 | validation: 0.4970560867663939]
	TIME [epoch: 8.2 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4336121801483744		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.3858625346762795		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.409737357412327 | validation: 0.3154140268736068]
	TIME [epoch: 8.18 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2990156525344108		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.47662379062115734		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.3878197215777841 | validation: 0.6707821408351453]
	TIME [epoch: 8.18 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3889469075079881		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.32589568589466855		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.3574212967013283 | validation: 0.4278717825085506]
	TIME [epoch: 8.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3288580306485892		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.38059735724926924		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.3547276939489293 | validation: 1.027683130671821]
	TIME [epoch: 8.19 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49206472318903505		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.46504455130403854		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.4785546372465368 | validation: 0.35533792128721453]
	TIME [epoch: 8.18 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3363759226548049		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.36683287718553126		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.35160439992016806 | validation: 0.2860817596906984]
	TIME [epoch: 8.18 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.357172791581434		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.2671513177574468		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.3121620546694404 | validation: 0.2298369355761588]
	TIME [epoch: 8.19 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33632763630059387		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.434463408792152		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.38539552254637294 | validation: 0.5223086438273596]
	TIME [epoch: 8.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4678245130884008		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.3618582139480572		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.41484136351822903 | validation: 0.248702222723303]
	TIME [epoch: 8.18 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39379582128135526		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.40977752545757007		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.40178667336946267 | validation: 0.5375745553282548]
	TIME [epoch: 8.18 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34537825140975675		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.34255938243138123		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.3439688169205691 | validation: 0.5990979230618468]
	TIME [epoch: 8.19 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40083846378931004		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.3299948052375384		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.36541663451342415 | validation: 0.35374049384479256]
	TIME [epoch: 8.2 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41870184559316337		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.3398700683937486		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.379285956993456 | validation: 0.34676377147512416]
	TIME [epoch: 8.18 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3233802114261356		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.31548008698890595		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.3194301492075207 | validation: 0.33292231551247836]
	TIME [epoch: 8.18 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3637205660813086		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.38133541997201487		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.37252799302666173 | validation: 0.3677589119704323]
	TIME [epoch: 8.18 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3688251302870741		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.2558177008214767		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.3123214155542754 | validation: 0.1925961748416814]
	TIME [epoch: 8.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2759240899578451		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.3132742243576702		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.29459915715775764 | validation: 0.3576827730608567]
	TIME [epoch: 8.18 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41631249764477707		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.3040382171800994		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.3601753574124383 | validation: 0.23126970444581907]
	TIME [epoch: 8.17 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31502772058998024		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.3575665027660807		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.3362971116780305 | validation: 0.5354384493967785]
	TIME [epoch: 8.18 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37486128447805905		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.35732361805298707		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.3660924512655231 | validation: 0.39792343124901314]
	TIME [epoch: 8.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4848825268596622		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.3150905486707214		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.3999865377651918 | validation: 0.2798284287344565]
	TIME [epoch: 8.17 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34561140132813606		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.38609285321492354		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.36585212727152977 | validation: 0.45865504971038024]
	TIME [epoch: 8.17 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40958527205595646		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.3173569086049949		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.36347109033047564 | validation: 0.3479404453246861]
	TIME [epoch: 8.18 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34211469877463335		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.45371868958950107		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.39791669418206715 | validation: 0.30838707989496594]
	TIME [epoch: 8.21 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4026372075964305		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.36366804473079706		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.3831526261636138 | validation: 0.33452838654680844]
	TIME [epoch: 8.17 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4241714932581262		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.2726541492430102		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.3484128212505682 | validation: 0.256088141409413]
	TIME [epoch: 8.17 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4007381372526423		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.2974277190388232		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.3490829281457328 | validation: 0.36420184137661976]
	TIME [epoch: 8.17 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.328472067175699		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.2992372822231508		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.3138546746994249 | validation: 0.21677129009932825]
	TIME [epoch: 8.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2921692703062864		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.3744239770662928		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.3332966236862897 | validation: 0.27807288676725167]
	TIME [epoch: 8.17 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.324186231736346		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.3873870547555069		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.3557866432459264 | validation: 0.22363821758707395]
	TIME [epoch: 8.17 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2847510990443792		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.3758596270856993		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.3303053630650392 | validation: 0.2940806307490236]
	TIME [epoch: 8.17 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.298089892039625		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.3800215611700161		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.33905572660482053 | validation: 0.3436519330742258]
	TIME [epoch: 8.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3552229376693505		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.29299568620453115		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.3241093119369408 | validation: 0.2278574455759001]
	TIME [epoch: 8.17 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3701123711791551		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.2364648773239646		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.3032886242515599 | validation: 0.41636176166675787]
	TIME [epoch: 8.17 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3322077930315038		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.26136536267649635		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.2967865778540001 | validation: 0.24038230196100596]
	TIME [epoch: 8.17 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3279963006028138		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.31069739167221877		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.3193468461375163 | validation: 0.4933590022038167]
	TIME [epoch: 8.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32794137764940395		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.2821665793946203		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.3050539785220122 | validation: 0.7583836941286567]
	TIME [epoch: 8.17 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.366535355596867		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.2687354340645525		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.31763539483070985 | validation: 0.2347672358776014]
	TIME [epoch: 8.16 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2988973043527762		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.30171239492742064		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.3003048496400985 | validation: 0.21405789556189597]
	TIME [epoch: 8.17 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.264000534342776		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.3234234723166778		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.2937120033297269 | validation: 0.3895666237297204]
	TIME [epoch: 8.19 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4073314714173433		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.3952886412469494		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.4013100563321463 | validation: 0.21450908527858564]
	TIME [epoch: 8.17 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2914124909799061		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.32903464174069325		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.3102235663602997 | validation: 0.29104067782991994]
	TIME [epoch: 8.17 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36880832863923235		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.38800869009797684		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.3784085093686046 | validation: 0.38672109980208585]
	TIME [epoch: 8.16 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27357593491016907		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.2634650960069398		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.26852051545855443 | validation: 0.2711200114233843]
	TIME [epoch: 8.19 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28504371620696506		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.49682125205999317		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.39093248413347914 | validation: 0.29422545432529246]
	TIME [epoch: 8.17 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2866650069213389		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.27224568953435496		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.27945534822784696 | validation: 0.23019953977748145]
	TIME [epoch: 8.17 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2959848584404333		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.31428719510400854		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.3051360267722209 | validation: 0.5485556322553123]
	TIME [epoch: 8.17 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3503538651164845		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.31119642660750024		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.3307751458619923 | validation: 0.2665501250288296]
	TIME [epoch: 8.19 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2573071449480945		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.30069757547317294		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.2790023602106336 | validation: 0.3564952187588204]
	TIME [epoch: 8.17 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32699390556894725		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.27017712993604437		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.29858551775249575 | validation: 0.2406806740285366]
	TIME [epoch: 8.16 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27063839466617645		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.28751857774002143		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.2790784862030989 | validation: 0.2207688757716533]
	TIME [epoch: 8.16 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2577483244084896		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.30729791195141204		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.28252311817995085 | validation: 0.2891999590228155]
	TIME [epoch: 8.19 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43525963518382704		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.3198607254178488		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.3775601803008379 | validation: 0.2608903538032901]
	TIME [epoch: 8.17 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3134524990052643		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.2893495127475863		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.30140100587642527 | validation: 0.2890110450597685]
	TIME [epoch: 8.16 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3059455365682576		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.32978141726429955		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.3178634769162786 | validation: 0.44109462555037693]
	TIME [epoch: 8.16 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3670337703864831		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.23428323103551213		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.30065850071099764 | validation: 0.19131549554234945]
	TIME [epoch: 8.19 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36049504212571226		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.2776678943395992		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.31908146823265576 | validation: 0.3876277274547253]
	TIME [epoch: 8.17 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3143442233063304		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.28845151827327964		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.30139787078980496 | validation: 0.5565473741015112]
	TIME [epoch: 8.16 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37242166261733856		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.33003714581310184		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.3512294042152203 | validation: 0.44910908841918185]
	TIME [epoch: 8.16 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2702082750463094		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.3440104718362554		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.30710937344128236 | validation: 0.20889253908489508]
	TIME [epoch: 8.18 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39929802219629557		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.3035144444617635		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.3514062333290294 | validation: 0.26072875179440136]
	TIME [epoch: 8.18 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2689020532299779		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.26454199134691964		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.26672202228844877 | validation: 0.22975076606986466]
	TIME [epoch: 8.16 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29174897577935954		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.2905204346314114		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.29113470520538554 | validation: 0.22699511861913918]
	TIME [epoch: 8.16 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2901821691088201		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.30754302288002333		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.29886259599442166 | validation: 0.2654106447454859]
	TIME [epoch: 8.18 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2923026256168549		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.31381809522852544		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.3030603604226902 | validation: 0.2294388449982101]
	TIME [epoch: 8.18 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22682095149298345		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.3517257323797733		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.2892733419363783 | validation: 0.37000085241010505]
	TIME [epoch: 8.16 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3636505365673728		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.2858073821913677		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.3247289593793703 | validation: 0.4496150612124489]
	TIME [epoch: 8.16 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2562813196553109		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.3507873729488461		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.30353434630207843 | validation: 0.3246927695813437]
	TIME [epoch: 8.18 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23705279055671666		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.4192640088857722		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.3281583997212444 | validation: 0.25526474391730536]
	TIME [epoch: 8.17 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2742865591043321		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.3729966225876934		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.32364159084601274 | validation: 0.4277221770094419]
	TIME [epoch: 8.17 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.250630886982466		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.27157882528309424		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.2611048561327801 | validation: 0.27197524712153553]
	TIME [epoch: 8.17 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2768902737817542		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.27852138998956016		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.27770583188565723 | validation: 0.331312517971296]
	TIME [epoch: 8.18 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2598847399435019		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.30037365300763363		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.2801291964755678 | validation: 0.2932913156412384]
	TIME [epoch: 8.18 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3038270195830539		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.3618031246733384		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.3328150721281962 | validation: 0.34141042563669766]
	TIME [epoch: 8.16 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2876428542753718		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.22653161111226564		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.25708723269381867 | validation: 0.3022546210542092]
	TIME [epoch: 8.16 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3390053892337995		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.2287760547779052		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.28389072200585236 | validation: 0.44458642236527113]
	TIME [epoch: 8.17 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33513383136925695		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.3263438945797922		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.3307388629745245 | validation: 0.17074271981634617]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41226339298555237		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.2752024742679917		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.34373293362677204 | validation: 0.16657015025464367]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3101453985722527		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.32258462977032104		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.3163650141712868 | validation: 0.2869355852926241]
	TIME [epoch: 8.16 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2684233245235392		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.38210317144033384		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.3252632479819365 | validation: 0.2647947489130675]
	TIME [epoch: 8.17 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27041549368019646		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.3483414094844988		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.30937845158234767 | validation: 0.5077164341138011]
	TIME [epoch: 8.17 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3615657045618327		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.25381108986152295		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.3076883972116778 | validation: 0.19438944078091053]
	TIME [epoch: 8.16 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27321397500202316		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 0.2596905179236726		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 0.26645224646284793 | validation: 0.32559543856902623]
	TIME [epoch: 8.16 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2417827840102469		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.28147345423857734		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.26162811912441203 | validation: 0.20060501738653871]
	TIME [epoch: 8.16 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32608374717564914		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 0.3564248692364066		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 0.34125430820602787 | validation: 0.34883713144740053]
	TIME [epoch: 8.18 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2989450007379859		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.25357485578381117		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.2762599282608985 | validation: 0.2263355843151299]
	TIME [epoch: 8.16 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3836291020601047		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.24228728734421381		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.31295819470215924 | validation: 0.32384562948366363]
	TIME [epoch: 8.16 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23266746261352883		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.3215897358147016		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.2771285992141152 | validation: 0.2113723062458884]
	TIME [epoch: 8.16 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27482558580963384		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.32021674952666757		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.29752116766815073 | validation: 0.2516464040851616]
	TIME [epoch: 8.19 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2576762588262066		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.3358314861644961		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.2967538724953513 | validation: 0.2634776782560207]
	TIME [epoch: 8.15 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23804156242695745		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.35221603967999066		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.29512880105347405 | validation: 0.20918106151023225]
	TIME [epoch: 8.16 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27049247824610245		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.2648822653252736		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.2676873717856881 | validation: 0.26291810023957857]
	TIME [epoch: 8.15 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24292621945542958		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 0.28403318264001404		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.2634797010477218 | validation: 0.16558664294626402]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2735075370580392		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.30181945458613896		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.28766349582208905 | validation: 0.23159497711917654]
	TIME [epoch: 8.16 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24357889328654622		[learning rate: 0.0057755]
		[batch 20/20] avg loss: 0.33223572648084587		[learning rate: 0.0057687]
	Learning Rate: 0.00576869
	LOSS [training: 0.28790730988369606 | validation: 0.14672396319083156]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2860700562286018		[learning rate: 0.0057619]
		[batch 20/20] avg loss: 0.22575326654344718		[learning rate: 0.0057551]
	Learning Rate: 0.00575508
	LOSS [training: 0.2559116613860245 | validation: 0.30895241154331676]
	TIME [epoch: 8.16 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21649272658261504		[learning rate: 0.0057483]
		[batch 20/20] avg loss: 0.3650158818252993		[learning rate: 0.0057415]
	Learning Rate: 0.0057415
	LOSS [training: 0.29075430420395715 | validation: 0.2880472397070927]
	TIME [epoch: 8.18 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25939042913552957		[learning rate: 0.0057347]
		[batch 20/20] avg loss: 0.3049566576018438		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.2821735433686867 | validation: 0.17088150186590756]
	TIME [epoch: 8.15 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28393986976429425		[learning rate: 0.0057212]
		[batch 20/20] avg loss: 0.27670929095123975		[learning rate: 0.0057144]
	Learning Rate: 0.00571445
	LOSS [training: 0.2803245803577671 | validation: 0.3083802100194801]
	TIME [epoch: 8.15 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2729822713528764		[learning rate: 0.0057077]
		[batch 20/20] avg loss: 0.2623064502640428		[learning rate: 0.005701]
	Learning Rate: 0.00570097
	LOSS [training: 0.26764436080845966 | validation: 0.24615387717621207]
	TIME [epoch: 8.16 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3622357999229699		[learning rate: 0.0056942]
		[batch 20/20] avg loss: 0.34602695362999947		[learning rate: 0.0056875]
	Learning Rate: 0.00568752
	LOSS [training: 0.35413137677648465 | validation: 0.25995056231128655]
	TIME [epoch: 8.18 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2197788979529322		[learning rate: 0.0056808]
		[batch 20/20] avg loss: 0.2253177264089258		[learning rate: 0.0056741]
	Learning Rate: 0.00567411
	LOSS [training: 0.222548312180929 | validation: 0.1600705277053116]
	TIME [epoch: 8.16 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31110284771871716		[learning rate: 0.0056674]
		[batch 20/20] avg loss: 0.26964636366517036		[learning rate: 0.0056607]
	Learning Rate: 0.00566072
	LOSS [training: 0.2903746056919438 | validation: 0.478812403937937]
	TIME [epoch: 8.15 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26010837540284715		[learning rate: 0.005654]
		[batch 20/20] avg loss: 0.2662261930966264		[learning rate: 0.0056474]
	Learning Rate: 0.00564737
	LOSS [training: 0.26316728424973673 | validation: 0.33236004049042894]
	TIME [epoch: 8.15 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.322136500844283		[learning rate: 0.0056407]
		[batch 20/20] avg loss: 0.24018838615210977		[learning rate: 0.005634]
	Learning Rate: 0.00563405
	LOSS [training: 0.2811624434981964 | validation: 0.19533009339497207]
	TIME [epoch: 8.18 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28123932192729123		[learning rate: 0.0056274]
		[batch 20/20] avg loss: 0.26367054705466614		[learning rate: 0.0056208]
	Learning Rate: 0.00562076
	LOSS [training: 0.27245493449097874 | validation: 0.24356959835474898]
	TIME [epoch: 8.15 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22903734263239328		[learning rate: 0.0056141]
		[batch 20/20] avg loss: 0.23324034832808457		[learning rate: 0.0056075]
	Learning Rate: 0.0056075
	LOSS [training: 0.2311388454802389 | validation: 0.25583235919837405]
	TIME [epoch: 8.15 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32847622948083005		[learning rate: 0.0056009]
		[batch 20/20] avg loss: 0.27637523167909933		[learning rate: 0.0055943]
	Learning Rate: 0.00559427
	LOSS [training: 0.30242573057996475 | validation: 0.15938425489276303]
	TIME [epoch: 8.15 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2575864065315032		[learning rate: 0.0055877]
		[batch 20/20] avg loss: 0.26633146697290366		[learning rate: 0.0055811]
	Learning Rate: 0.00558108
	LOSS [training: 0.2619589367522034 | validation: 0.20311952842673994]
	TIME [epoch: 8.17 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23159141374059772		[learning rate: 0.0055745]
		[batch 20/20] avg loss: 0.2572121418159212		[learning rate: 0.0055679]
	Learning Rate: 0.00556791
	LOSS [training: 0.2444017777782595 | validation: 0.33027563610628263]
	TIME [epoch: 8.15 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2576844224560234		[learning rate: 0.0055613]
		[batch 20/20] avg loss: 0.2757629289303915		[learning rate: 0.0055548]
	Learning Rate: 0.00555478
	LOSS [training: 0.26672367569320754 | validation: 0.22831302568342074]
	TIME [epoch: 8.15 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21238648580457173		[learning rate: 0.0055482]
		[batch 20/20] avg loss: 0.36362766761869086		[learning rate: 0.0055417]
	Learning Rate: 0.00554167
	LOSS [training: 0.2880070767116314 | validation: 0.24911726747184165]
	TIME [epoch: 8.15 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25396782436837717		[learning rate: 0.0055351]
		[batch 20/20] avg loss: 0.24904367905574185		[learning rate: 0.0055286]
	Learning Rate: 0.0055286
	LOSS [training: 0.25150575171205963 | validation: 0.21315643408572518]
	TIME [epoch: 8.18 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3075845320778868		[learning rate: 0.0055221]
		[batch 20/20] avg loss: 0.2807392913792263		[learning rate: 0.0055156]
	Learning Rate: 0.00551556
	LOSS [training: 0.29416191172855655 | validation: 0.20093081051708994]
	TIME [epoch: 8.16 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2081007559185096		[learning rate: 0.0055091]
		[batch 20/20] avg loss: 0.2899879970491411		[learning rate: 0.0055026]
	Learning Rate: 0.00550255
	LOSS [training: 0.24904437648382532 | validation: 0.4031341071652918]
	TIME [epoch: 8.15 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27120737083883545		[learning rate: 0.0054961]
		[batch 20/20] avg loss: 0.21317912924329688		[learning rate: 0.0054896]
	Learning Rate: 0.00548957
	LOSS [training: 0.24219325004106612 | validation: 0.23693579210932664]
	TIME [epoch: 8.16 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25495822540470553		[learning rate: 0.0054831]
		[batch 20/20] avg loss: 0.21007500148248895		[learning rate: 0.0054766]
	Learning Rate: 0.00547662
	LOSS [training: 0.23251661344359728 | validation: 0.33569292344739365]
	TIME [epoch: 8.18 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2239261627509693		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.30333596460668105		[learning rate: 0.0054637]
	Learning Rate: 0.0054637
	LOSS [training: 0.26363106367882516 | validation: 0.2049807763711347]
	TIME [epoch: 8.16 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2613494166985598		[learning rate: 0.0054573]
		[batch 20/20] avg loss: 0.24045063069908162		[learning rate: 0.0054508]
	Learning Rate: 0.00545082
	LOSS [training: 0.2509000236988207 | validation: 0.3014635376739783]
	TIME [epoch: 8.16 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32354969368137165		[learning rate: 0.0054444]
		[batch 20/20] avg loss: 0.24537967993491364		[learning rate: 0.005438]
	Learning Rate: 0.00543796
	LOSS [training: 0.28446468680814263 | validation: 0.21254127328146047]
	TIME [epoch: 8.15 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27724541943838565		[learning rate: 0.0054315]
		[batch 20/20] avg loss: 0.2377764271987747		[learning rate: 0.0054251]
	Learning Rate: 0.00542513
	LOSS [training: 0.25751092331858016 | validation: 0.2102681074254275]
	TIME [epoch: 8.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2758277319203512		[learning rate: 0.0054187]
		[batch 20/20] avg loss: 0.27944614143233293		[learning rate: 0.0054123]
	Learning Rate: 0.00541233
	LOSS [training: 0.2776369366763421 | validation: 0.11598763341782836]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16380769498341702		[learning rate: 0.0054059]
		[batch 20/20] avg loss: 0.265473280163581		[learning rate: 0.0053996]
	Learning Rate: 0.00539957
	LOSS [training: 0.214640487573499 | validation: 0.26030924917387654]
	TIME [epoch: 8.15 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27024683049659803		[learning rate: 0.0053932]
		[batch 20/20] avg loss: 0.27785452424114243		[learning rate: 0.0053868]
	Learning Rate: 0.00538683
	LOSS [training: 0.2740506773688701 | validation: 0.23034771282010008]
	TIME [epoch: 8.15 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25739703211287385		[learning rate: 0.0053805]
		[batch 20/20] avg loss: 0.27751571006253456		[learning rate: 0.0053741]
	Learning Rate: 0.00537412
	LOSS [training: 0.2674563710877042 | validation: 0.16956614438966927]
	TIME [epoch: 8.18 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21410193170898495		[learning rate: 0.0053678]
		[batch 20/20] avg loss: 0.24988898376772783		[learning rate: 0.0053614]
	Learning Rate: 0.00536145
	LOSS [training: 0.23199545773835636 | validation: 0.2301688838391695]
	TIME [epoch: 8.16 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.299341275248069		[learning rate: 0.0053551]
		[batch 20/20] avg loss: 0.23710621295130035		[learning rate: 0.0053488]
	Learning Rate: 0.0053488
	LOSS [training: 0.2682237440996847 | validation: 0.6992833022397337]
	TIME [epoch: 8.15 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32541985792482014		[learning rate: 0.0053425]
		[batch 20/20] avg loss: 0.273796862050896		[learning rate: 0.0053362]
	Learning Rate: 0.00533618
	LOSS [training: 0.299608359987858 | validation: 0.1980581373656956]
	TIME [epoch: 8.14 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22588457853326815		[learning rate: 0.0053299]
		[batch 20/20] avg loss: 0.2769601922059615		[learning rate: 0.0053236]
	Learning Rate: 0.0053236
	LOSS [training: 0.2514223853696148 | validation: 0.29048924584110514]
	TIME [epoch: 8.17 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31857461716747937		[learning rate: 0.0053173]
		[batch 20/20] avg loss: 0.24150489384103543		[learning rate: 0.005311]
	Learning Rate: 0.00531104
	LOSS [training: 0.2800397555042574 | validation: 0.22219131168093922]
	TIME [epoch: 8.16 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23426170970959811		[learning rate: 0.0053048]
		[batch 20/20] avg loss: 0.23816037199600068		[learning rate: 0.0052985]
	Learning Rate: 0.00529851
	LOSS [training: 0.2362110408527994 | validation: 0.30455072098409197]
	TIME [epoch: 8.16 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29767773351678456		[learning rate: 0.0052923]
		[batch 20/20] avg loss: 0.25473648206178784		[learning rate: 0.005286]
	Learning Rate: 0.00528601
	LOSS [training: 0.27620710778928614 | validation: 0.2947660003548542]
	TIME [epoch: 8.15 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23820615587930577		[learning rate: 0.0052798]
		[batch 20/20] avg loss: 0.2927288771677924		[learning rate: 0.0052735]
	Learning Rate: 0.00527354
	LOSS [training: 0.2654675165235491 | validation: 0.3445735178642817]
	TIME [epoch: 8.18 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28489307910513517		[learning rate: 0.0052673]
		[batch 20/20] avg loss: 0.27512144157929314		[learning rate: 0.0052611]
	Learning Rate: 0.0052611
	LOSS [training: 0.2800072603422142 | validation: 0.2448514995197987]
	TIME [epoch: 8.16 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2660834356761584		[learning rate: 0.0052549]
		[batch 20/20] avg loss: 0.2886148825255782		[learning rate: 0.0052487]
	Learning Rate: 0.00524869
	LOSS [training: 0.27734915910086844 | validation: 0.2523836363579862]
	TIME [epoch: 8.16 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24859223133726407		[learning rate: 0.0052425]
		[batch 20/20] avg loss: 0.309690361333773		[learning rate: 0.0052363]
	Learning Rate: 0.00523631
	LOSS [training: 0.27914129633551854 | validation: 0.2877873779195528]
	TIME [epoch: 8.16 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22366244875253294		[learning rate: 0.0052301]
		[batch 20/20] avg loss: 0.2269751179389053		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.2253187833457191 | validation: 0.18428174052708798]
	TIME [epoch: 8.17 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28953676613758644		[learning rate: 0.0052178]
		[batch 20/20] avg loss: 0.2579815124962264		[learning rate: 0.0052116]
	Learning Rate: 0.00521164
	LOSS [training: 0.27375913931690643 | validation: 0.4506052126772979]
	TIME [epoch: 8.17 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3143305770255237		[learning rate: 0.0052055]
		[batch 20/20] avg loss: 0.256699484116206		[learning rate: 0.0051993]
	Learning Rate: 0.00519935
	LOSS [training: 0.2855150305708648 | validation: 0.17952514980033993]
	TIME [epoch: 8.15 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23032285949605327		[learning rate: 0.0051932]
		[batch 20/20] avg loss: 0.28860630593674313		[learning rate: 0.0051871]
	Learning Rate: 0.00518708
	LOSS [training: 0.2594645827163982 | validation: 0.3312414999583785]
	TIME [epoch: 8.16 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22602584417600827		[learning rate: 0.005181]
		[batch 20/20] avg loss: 0.2854263681130855		[learning rate: 0.0051748]
	Learning Rate: 0.00517485
	LOSS [training: 0.25572610614454694 | validation: 0.1711059102690533]
	TIME [epoch: 8.18 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2894223135151729		[learning rate: 0.0051687]
		[batch 20/20] avg loss: 0.20537348092308946		[learning rate: 0.0051626]
	Learning Rate: 0.00516264
	LOSS [training: 0.24739789721913125 | validation: 0.14800158205964084]
	TIME [epoch: 8.17 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21136461901944995		[learning rate: 0.0051565]
		[batch 20/20] avg loss: 0.2220510801692769		[learning rate: 0.0051505]
	Learning Rate: 0.00515046
	LOSS [training: 0.21670784959436343 | validation: 0.23459686131542387]
	TIME [epoch: 8.16 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26517978952643856		[learning rate: 0.0051444]
		[batch 20/20] avg loss: 0.44582921244836876		[learning rate: 0.0051383]
	Learning Rate: 0.00513831
	LOSS [training: 0.3555045009874037 | validation: 0.19669307649356962]
	TIME [epoch: 8.17 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1986373336684375		[learning rate: 0.0051322]
		[batch 20/20] avg loss: 0.2908567234819123		[learning rate: 0.0051262]
	Learning Rate: 0.00512619
	LOSS [training: 0.24474702857517489 | validation: 0.3009025311800855]
	TIME [epoch: 8.2 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2574099543139641		[learning rate: 0.0051201]
		[batch 20/20] avg loss: 0.2572817449320648		[learning rate: 0.0051141]
	Learning Rate: 0.0051141
	LOSS [training: 0.25734584962301443 | validation: 0.20101789242278334]
	TIME [epoch: 8.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2641406043162263		[learning rate: 0.0051081]
		[batch 20/20] avg loss: 0.21972359992171242		[learning rate: 0.005102]
	Learning Rate: 0.00510204
	LOSS [training: 0.24193210211896937 | validation: 0.18882545735772538]
	TIME [epoch: 8.17 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3038076758950677		[learning rate: 0.005096]
		[batch 20/20] avg loss: 0.2175697144793265		[learning rate: 0.00509]
	Learning Rate: 0.00509
	LOSS [training: 0.2606886951871971 | validation: 0.16864757975475075]
	TIME [epoch: 8.18 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2434535823440252		[learning rate: 0.005084]
		[batch 20/20] avg loss: 0.20069674777556967		[learning rate: 0.005078]
	Learning Rate: 0.00507799
	LOSS [training: 0.22207516505979746 | validation: 0.2269199767996936]
	TIME [epoch: 8.21 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26887763225761246		[learning rate: 0.005072]
		[batch 20/20] avg loss: 0.26393952247729635		[learning rate: 0.005066]
	Learning Rate: 0.00506602
	LOSS [training: 0.26640857736745444 | validation: 0.20060121428304362]
	TIME [epoch: 8.19 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21293726708591604		[learning rate: 0.00506]
		[batch 20/20] avg loss: 0.2660327647989214		[learning rate: 0.0050541]
	Learning Rate: 0.00505407
	LOSS [training: 0.23948501594241872 | validation: 0.163375005530791]
	TIME [epoch: 8.19 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2293314341312632		[learning rate: 0.0050481]
		[batch 20/20] avg loss: 0.3081091392368601		[learning rate: 0.0050421]
	Learning Rate: 0.00504215
	LOSS [training: 0.26872028668406156 | validation: 0.2254474788096121]
	TIME [epoch: 8.2 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31177110947878717		[learning rate: 0.0050362]
		[batch 20/20] avg loss: 0.20376902507642672		[learning rate: 0.0050303]
	Learning Rate: 0.00503025
	LOSS [training: 0.257770067277607 | validation: 0.27390539101472394]
	TIME [epoch: 8.19 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21941469296828958		[learning rate: 0.0050243]
		[batch 20/20] avg loss: 0.2307167926757307		[learning rate: 0.0050184]
	Learning Rate: 0.00501839
	LOSS [training: 0.2250657428220101 | validation: 0.23431251347209958]
	TIME [epoch: 8.2 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20472794136383418		[learning rate: 0.0050125]
		[batch 20/20] avg loss: 0.24070182224513204		[learning rate: 0.0050065]
	Learning Rate: 0.00500655
	LOSS [training: 0.22271488180448312 | validation: 0.2230727180166111]
	TIME [epoch: 8.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1859763924072416		[learning rate: 0.0050006]
		[batch 20/20] avg loss: 0.28035601575559627		[learning rate: 0.0049947]
	Learning Rate: 0.00499474
	LOSS [training: 0.23316620408141894 | validation: 0.1647516941262862]
	TIME [epoch: 8.19 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2176582423127215		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.20083074055660713		[learning rate: 0.004983]
	Learning Rate: 0.00498296
	LOSS [training: 0.20924449143466434 | validation: 0.2922437137445863]
	TIME [epoch: 8.19 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23418844082864823		[learning rate: 0.0049771]
		[batch 20/20] avg loss: 0.242574945222262		[learning rate: 0.0049712]
	Learning Rate: 0.0049712
	LOSS [training: 0.23838169302545512 | validation: 0.35554833600636954]
	TIME [epoch: 8.22 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21440636308063105		[learning rate: 0.0049653]
		[batch 20/20] avg loss: 0.19038046235360215		[learning rate: 0.0049595]
	Learning Rate: 0.00495948
	LOSS [training: 0.2023934127171166 | validation: 0.20910163994634923]
	TIME [epoch: 8.21 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1761054647206016		[learning rate: 0.0049536]
		[batch 20/20] avg loss: 0.18751121700854312		[learning rate: 0.0049478]
	Learning Rate: 0.00494778
	LOSS [training: 0.18180834086457234 | validation: 0.17456015365908442]
	TIME [epoch: 8.2 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2495432901623305		[learning rate: 0.0049419]
		[batch 20/20] avg loss: 0.22622558315905156		[learning rate: 0.0049361]
	Learning Rate: 0.00493611
	LOSS [training: 0.237884436660691 | validation: 0.30842820283363104]
	TIME [epoch: 8.2 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31262000970533077		[learning rate: 0.0049303]
		[batch 20/20] avg loss: 0.3104396379656005		[learning rate: 0.0049245]
	Learning Rate: 0.00492446
	LOSS [training: 0.3115298238354656 | validation: 0.19616088514500554]
	TIME [epoch: 8.21 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21382736593727883		[learning rate: 0.0049187]
		[batch 20/20] avg loss: 0.2034893177298263		[learning rate: 0.0049128]
	Learning Rate: 0.00491285
	LOSS [training: 0.20865834183355259 | validation: 0.2736770402590771]
	TIME [epoch: 8.18 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24894184335635278		[learning rate: 0.0049071]
		[batch 20/20] avg loss: 0.2083777637906176		[learning rate: 0.0049013]
	Learning Rate: 0.00490126
	LOSS [training: 0.22865980357348517 | validation: 0.18821265465419607]
	TIME [epoch: 8.18 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16873049676934165		[learning rate: 0.0048955]
		[batch 20/20] avg loss: 0.25298775058735834		[learning rate: 0.0048897]
	Learning Rate: 0.0048897
	LOSS [training: 0.21085912367835 | validation: 0.16299168573569264]
	TIME [epoch: 8.19 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19474264231809663		[learning rate: 0.0048839]
		[batch 20/20] avg loss: 0.2865623192601078		[learning rate: 0.0048782]
	Learning Rate: 0.00487816
	LOSS [training: 0.24065248078910223 | validation: 0.2391987124325552]
	TIME [epoch: 8.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20981357850576718		[learning rate: 0.0048724]
		[batch 20/20] avg loss: 0.21235356272555123		[learning rate: 0.0048667]
	Learning Rate: 0.00486666
	LOSS [training: 0.21108357061565922 | validation: 0.16450094059332243]
	TIME [epoch: 8.18 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23133929126083008		[learning rate: 0.0048609]
		[batch 20/20] avg loss: 0.23005347538361942		[learning rate: 0.0048552]
	Learning Rate: 0.00485518
	LOSS [training: 0.23069638332222478 | validation: 0.318392189528908]
	TIME [epoch: 8.17 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2474135011594024		[learning rate: 0.0048494]
		[batch 20/20] avg loss: 0.17879958680910973		[learning rate: 0.0048437]
	Learning Rate: 0.00484372
	LOSS [training: 0.21310654398425605 | validation: 0.156531683824374]
	TIME [epoch: 8.17 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21108855536774662		[learning rate: 0.004838]
		[batch 20/20] avg loss: 0.2907666566262714		[learning rate: 0.0048323]
	Learning Rate: 0.0048323
	LOSS [training: 0.2509276059970089 | validation: 0.22751911249982465]
	TIME [epoch: 8.18 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21241799562552158		[learning rate: 0.0048266]
		[batch 20/20] avg loss: 0.15276175691757793		[learning rate: 0.0048209]
	Learning Rate: 0.0048209
	LOSS [training: 0.18258987627154974 | validation: 0.13186940247664777]
	TIME [epoch: 8.17 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24080714620134275		[learning rate: 0.0048152]
		[batch 20/20] avg loss: 0.2590054790235151		[learning rate: 0.0048095]
	Learning Rate: 0.00480953
	LOSS [training: 0.24990631261242896 | validation: 0.2252741278753843]
	TIME [epoch: 8.17 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18253635669565355		[learning rate: 0.0048039]
		[batch 20/20] avg loss: 0.24472963441549161		[learning rate: 0.0047982]
	Learning Rate: 0.00479818
	LOSS [training: 0.2136329955555726 | validation: 0.16753429626216126]
	TIME [epoch: 8.18 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18607181548253404		[learning rate: 0.0047925]
		[batch 20/20] avg loss: 0.2518137290966084		[learning rate: 0.0047869]
	Learning Rate: 0.00478687
	LOSS [training: 0.2189427722895712 | validation: 0.24539064465695948]
	TIME [epoch: 8.19 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15659548712456733		[learning rate: 0.0047812]
		[batch 20/20] avg loss: 0.21163084214160674		[learning rate: 0.0047756]
	Learning Rate: 0.00477557
	LOSS [training: 0.18411316463308702 | validation: 0.2177657448962941]
	TIME [epoch: 8.17 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2559840051568359		[learning rate: 0.0047699]
		[batch 20/20] avg loss: 0.2489475078629028		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.25246575650986935 | validation: 0.17567054093049175]
	TIME [epoch: 8.16 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21380850218962064		[learning rate: 0.0047587]
		[batch 20/20] avg loss: 0.2146789322074208		[learning rate: 0.0047531]
	Learning Rate: 0.00475307
	LOSS [training: 0.21424371719852076 | validation: 0.23427500515504848]
	TIME [epoch: 8.16 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25621713922418166		[learning rate: 0.0047475]
		[batch 20/20] avg loss: 0.24658102426985753		[learning rate: 0.0047419]
	Learning Rate: 0.00474186
	LOSS [training: 0.25139908174701964 | validation: 0.14411292369444695]
	TIME [epoch: 8.19 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2068009431573607		[learning rate: 0.0047363]
		[batch 20/20] avg loss: 0.2181594911954389		[learning rate: 0.0047307]
	Learning Rate: 0.00473067
	LOSS [training: 0.2124802171763998 | validation: 0.1718081690072124]
	TIME [epoch: 8.17 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2584854334429021		[learning rate: 0.0047251]
		[batch 20/20] avg loss: 0.211728605614763		[learning rate: 0.0047195]
	Learning Rate: 0.00471952
	LOSS [training: 0.2351070195288325 | validation: 0.15864343500973413]
	TIME [epoch: 8.17 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24251150485676692		[learning rate: 0.0047139]
		[batch 20/20] avg loss: 0.21508627982097267		[learning rate: 0.0047084]
	Learning Rate: 0.00470838
	LOSS [training: 0.2287988923388698 | validation: 0.2285755125015836]
	TIME [epoch: 8.16 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19936889664536278		[learning rate: 0.0047028]
		[batch 20/20] avg loss: 0.20074070177034842		[learning rate: 0.0046973]
	Learning Rate: 0.00469728
	LOSS [training: 0.20005479920785557 | validation: 0.1812521120186278]
	TIME [epoch: 8.19 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2735238451686908		[learning rate: 0.0046917]
		[batch 20/20] avg loss: 0.21574209957998067		[learning rate: 0.0046862]
	Learning Rate: 0.0046862
	LOSS [training: 0.24463297237433573 | validation: 0.20794579299467394]
	TIME [epoch: 8.17 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20449256832664425		[learning rate: 0.0046807]
		[batch 20/20] avg loss: 0.20772338703564414		[learning rate: 0.0046751]
	Learning Rate: 0.00467514
	LOSS [training: 0.20610797768114417 | validation: 0.14215459566457084]
	TIME [epoch: 8.17 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19319036477538065		[learning rate: 0.0046696]
		[batch 20/20] avg loss: 0.23223821270623218		[learning rate: 0.0046641]
	Learning Rate: 0.00466411
	LOSS [training: 0.21271428874080645 | validation: 0.22943247950960477]
	TIME [epoch: 8.16 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2311951819139107		[learning rate: 0.0046586]
		[batch 20/20] avg loss: 0.2642201402689022		[learning rate: 0.0046531]
	Learning Rate: 0.00465311
	LOSS [training: 0.24770766109140646 | validation: 0.14686604073280657]
	TIME [epoch: 8.19 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20006639263745002		[learning rate: 0.0046476]
		[batch 20/20] avg loss: 0.3074525894644299		[learning rate: 0.0046421]
	Learning Rate: 0.00464214
	LOSS [training: 0.2537594910509399 | validation: 0.26949715878393776]
	TIME [epoch: 8.16 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2251413500433737		[learning rate: 0.0046367]
		[batch 20/20] avg loss: 0.19155795194613734		[learning rate: 0.0046312]
	Learning Rate: 0.00463119
	LOSS [training: 0.2083496509947555 | validation: 0.19743494456516197]
	TIME [epoch: 8.16 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2164242557750756		[learning rate: 0.0046257]
		[batch 20/20] avg loss: 0.19644278527346742		[learning rate: 0.0046203]
	Learning Rate: 0.00462026
	LOSS [training: 0.20643352052427155 | validation: 0.17356714167689924]
	TIME [epoch: 8.16 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17917982674117747		[learning rate: 0.0046148]
		[batch 20/20] avg loss: 0.27436047866930646		[learning rate: 0.0046094]
	Learning Rate: 0.00460936
	LOSS [training: 0.22677015270524206 | validation: 0.2358599504515614]
	TIME [epoch: 8.19 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2462865952933364		[learning rate: 0.0046039]
		[batch 20/20] avg loss: 0.3508060504185729		[learning rate: 0.0045985]
	Learning Rate: 0.00459849
	LOSS [training: 0.2985463228559546 | validation: 0.3008377682401731]
	TIME [epoch: 8.17 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20490747419844565		[learning rate: 0.0045931]
		[batch 20/20] avg loss: 0.233880873061258		[learning rate: 0.0045876]
	Learning Rate: 0.00458764
	LOSS [training: 0.21939417362985184 | validation: 0.22395778552974635]
	TIME [epoch: 8.16 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1769691380829554		[learning rate: 0.0045822]
		[batch 20/20] avg loss: 0.1842929094731432		[learning rate: 0.0045768]
	Learning Rate: 0.00457682
	LOSS [training: 0.18063102377804935 | validation: 0.3110655107506346]
	TIME [epoch: 8.17 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23391791896909106		[learning rate: 0.0045714]
		[batch 20/20] avg loss: 0.23633368152668993		[learning rate: 0.004566]
	Learning Rate: 0.00456603
	LOSS [training: 0.2351258002478905 | validation: 0.17496189452814867]
	TIME [epoch: 8.19 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22180140460390704		[learning rate: 0.0045606]
		[batch 20/20] avg loss: 0.21842909045489675		[learning rate: 0.0045553]
	Learning Rate: 0.00455526
	LOSS [training: 0.22011524752940187 | validation: 0.15339230200264117]
	TIME [epoch: 8.16 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1722486323632284		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.27679468571823274		[learning rate: 0.0045445]
	Learning Rate: 0.00454451
	LOSS [training: 0.22452165904073054 | validation: 0.19950171873427797]
	TIME [epoch: 8.16 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23159119040797496		[learning rate: 0.0045391]
		[batch 20/20] avg loss: 0.19730122101613184		[learning rate: 0.0045338]
	Learning Rate: 0.00453379
	LOSS [training: 0.21444620571205336 | validation: 0.214342364073676]
	TIME [epoch: 8.17 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22764544092111677		[learning rate: 0.0045284]
		[batch 20/20] avg loss: 0.20873233887525103		[learning rate: 0.0045231]
	Learning Rate: 0.0045231
	LOSS [training: 0.21818888989818386 | validation: 0.3181916800623499]
	TIME [epoch: 8.19 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28718138984207225		[learning rate: 0.0045178]
		[batch 20/20] avg loss: 0.25137782324175795		[learning rate: 0.0045124]
	Learning Rate: 0.00451243
	LOSS [training: 0.26927960654191513 | validation: 0.1462166266635353]
	TIME [epoch: 8.17 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1469064437261029		[learning rate: 0.0045071]
		[batch 20/20] avg loss: 0.23810556411037523		[learning rate: 0.0045018]
	Learning Rate: 0.00450178
	LOSS [training: 0.19250600391823908 | validation: 0.3403845715449605]
	TIME [epoch: 8.17 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2383069809649338		[learning rate: 0.0044965]
		[batch 20/20] avg loss: 0.3002564028363558		[learning rate: 0.0044912]
	Learning Rate: 0.00449116
	LOSS [training: 0.2692816919006448 | validation: 0.16707775125231783]
	TIME [epoch: 8.17 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23033712959255653		[learning rate: 0.0044859]
		[batch 20/20] avg loss: 0.3776123298477595		[learning rate: 0.0044806]
	Learning Rate: 0.00448057
	LOSS [training: 0.30397472972015793 | validation: 0.15609475124355504]
	TIME [epoch: 8.19 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19092852000878013		[learning rate: 0.0044753]
		[batch 20/20] avg loss: 0.22464180306578121		[learning rate: 0.00447]
	Learning Rate: 0.00447
	LOSS [training: 0.2077851615372807 | validation: 0.1858882626348487]
	TIME [epoch: 8.19 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20400648650731248		[learning rate: 0.0044647]
		[batch 20/20] avg loss: 0.22945869708609648		[learning rate: 0.0044595]
	Learning Rate: 0.00445946
	LOSS [training: 0.21673259179670445 | validation: 0.2432996431570197]
	TIME [epoch: 8.18 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21386597004824814		[learning rate: 0.0044542]
		[batch 20/20] avg loss: 0.25198065847707846		[learning rate: 0.0044489]
	Learning Rate: 0.00444894
	LOSS [training: 0.23292331426266336 | validation: 0.3331852198065324]
	TIME [epoch: 8.17 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1921828443911011		[learning rate: 0.0044437]
		[batch 20/20] avg loss: 0.1785721252230234		[learning rate: 0.0044384]
	Learning Rate: 0.00443844
	LOSS [training: 0.18537748480706223 | validation: 0.2034769947452934]
	TIME [epoch: 8.19 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17646042393525502		[learning rate: 0.0044332]
		[batch 20/20] avg loss: 0.2415116020503254		[learning rate: 0.004428]
	Learning Rate: 0.00442797
	LOSS [training: 0.20898601299279015 | validation: 0.31373798410108733]
	TIME [epoch: 8.19 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20047444666135847		[learning rate: 0.0044227]
		[batch 20/20] avg loss: 0.2295999346929026		[learning rate: 0.0044175]
	Learning Rate: 0.00441753
	LOSS [training: 0.21503719067713054 | validation: 0.16392539305120996]
	TIME [epoch: 8.17 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22995265087418945		[learning rate: 0.0044123]
		[batch 20/20] avg loss: 0.26009511560491516		[learning rate: 0.0044071]
	Learning Rate: 0.00440711
	LOSS [training: 0.24502388323955232 | validation: 0.20953163921497417]
	TIME [epoch: 8.17 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2102546444513969		[learning rate: 0.0044019]
		[batch 20/20] avg loss: 0.19161870856491128		[learning rate: 0.0043967]
	Learning Rate: 0.00439671
	LOSS [training: 0.20093667650815408 | validation: 0.3814300822207357]
	TIME [epoch: 8.18 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25144873884433927		[learning rate: 0.0043915]
		[batch 20/20] avg loss: 0.2750761341495692		[learning rate: 0.0043863]
	Learning Rate: 0.00438634
	LOSS [training: 0.26326243649695424 | validation: 0.15278756701461643]
	TIME [epoch: 8.18 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16779536118208177		[learning rate: 0.0043812]
		[batch 20/20] avg loss: 0.2095049830472267		[learning rate: 0.004376]
	Learning Rate: 0.004376
	LOSS [training: 0.18865017211465424 | validation: 0.2836613557721248]
	TIME [epoch: 8.17 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25028309751670286		[learning rate: 0.0043708]
		[batch 20/20] avg loss: 0.18546059816342422		[learning rate: 0.0043657]
	Learning Rate: 0.00436567
	LOSS [training: 0.2178718478400635 | validation: 0.31288596647503975]
	TIME [epoch: 8.16 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23822827214310785		[learning rate: 0.0043605]
		[batch 20/20] avg loss: 0.20371291914022374		[learning rate: 0.0043554]
	Learning Rate: 0.00435538
	LOSS [training: 0.2209705956416658 | validation: 0.18667396853578846]
	TIME [epoch: 8.19 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25664640413886386		[learning rate: 0.0043502]
		[batch 20/20] avg loss: 0.2153388561492185		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.2359926301440412 | validation: 0.17043137587179963]
	TIME [epoch: 8.18 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28809274101434384		[learning rate: 0.00434]
		[batch 20/20] avg loss: 0.17253344935091824		[learning rate: 0.0043349]
	Learning Rate: 0.00433485
	LOSS [training: 0.23031309518263102 | validation: 0.1090636708020172]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1833918567663863		[learning rate: 0.0043297]
		[batch 20/20] avg loss: 0.19352009227271488		[learning rate: 0.0043246]
	Learning Rate: 0.00432463
	LOSS [training: 0.18845597451955065 | validation: 0.1679051237373841]
	TIME [epoch: 8.15 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21602701435780802		[learning rate: 0.0043195]
		[batch 20/20] avg loss: 0.2765466364960673		[learning rate: 0.0043144]
	Learning Rate: 0.00431443
	LOSS [training: 0.24628682542693764 | validation: 0.11865742034008114]
	TIME [epoch: 8.16 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20289248171642038		[learning rate: 0.0043093]
		[batch 20/20] avg loss: 0.2172766932688853		[learning rate: 0.0043042]
	Learning Rate: 0.00430425
	LOSS [training: 0.21008458749265282 | validation: 0.2951369378956976]
	TIME [epoch: 8.16 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2853071847512888		[learning rate: 0.0042992]
		[batch 20/20] avg loss: 0.1844598553839241		[learning rate: 0.0042941]
	Learning Rate: 0.0042941
	LOSS [training: 0.23488352006760643 | validation: 0.19129088346290016]
	TIME [epoch: 8.14 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2171395838119446		[learning rate: 0.004289]
		[batch 20/20] avg loss: 0.18812884009489594		[learning rate: 0.004284]
	Learning Rate: 0.00428397
	LOSS [training: 0.20263421195342027 | validation: 0.2676873607672455]
	TIME [epoch: 8.14 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21662431602009785		[learning rate: 0.0042789]
		[batch 20/20] avg loss: 0.17963552305514374		[learning rate: 0.0042739]
	Learning Rate: 0.00427386
	LOSS [training: 0.19812991953762077 | validation: 0.22262354138845875]
	TIME [epoch: 8.16 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2544383531584316		[learning rate: 0.0042688]
		[batch 20/20] avg loss: 0.16728700805675512		[learning rate: 0.0042638]
	Learning Rate: 0.00426378
	LOSS [training: 0.21086268060759336 | validation: 0.29563981606806067]
	TIME [epoch: 8.16 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1778955532644006		[learning rate: 0.0042587]
		[batch 20/20] avg loss: 0.2045464406868493		[learning rate: 0.0042537]
	Learning Rate: 0.00425372
	LOSS [training: 0.19122099697562495 | validation: 0.2897789236045007]
	TIME [epoch: 8.14 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22007730726972116		[learning rate: 0.0042487]
		[batch 20/20] avg loss: 0.1796035713608447		[learning rate: 0.0042437]
	Learning Rate: 0.00424369
	LOSS [training: 0.19984043931528297 | validation: 0.11703910937341445]
	TIME [epoch: 8.14 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1797358053267695		[learning rate: 0.0042387]
		[batch 20/20] avg loss: 0.22133807863871366		[learning rate: 0.0042337]
	Learning Rate: 0.00423368
	LOSS [training: 0.20053694198274158 | validation: 0.13614463977925795]
	TIME [epoch: 8.14 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23516558329697917		[learning rate: 0.0042287]
		[batch 20/20] avg loss: 0.2223439393668481		[learning rate: 0.0042237]
	Learning Rate: 0.00422369
	LOSS [training: 0.2287547613319137 | validation: 0.2562124291811527]
	TIME [epoch: 8.17 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19800982114123578		[learning rate: 0.0042187]
		[batch 20/20] avg loss: 0.1914217433273698		[learning rate: 0.0042137]
	Learning Rate: 0.00421373
	LOSS [training: 0.19471578223430278 | validation: 0.17172118415384438]
	TIME [epoch: 8.15 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18034858746759513		[learning rate: 0.0042088]
		[batch 20/20] avg loss: 0.19938483170032628		[learning rate: 0.0042038]
	Learning Rate: 0.00420379
	LOSS [training: 0.18986670958396074 | validation: 0.2514113660273546]
	TIME [epoch: 8.14 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2426100057221134		[learning rate: 0.0041988]
		[batch 20/20] avg loss: 0.18750891114959528		[learning rate: 0.0041939]
	Learning Rate: 0.00419387
	LOSS [training: 0.21505945843585428 | validation: 0.3247456128498543]
	TIME [epoch: 8.15 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19657895766829245		[learning rate: 0.0041889]
		[batch 20/20] avg loss: 0.20729391597565439		[learning rate: 0.004184]
	Learning Rate: 0.00418398
	LOSS [training: 0.20193643682197343 | validation: 0.18244932708399456]
	TIME [epoch: 8.17 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15552719621821315		[learning rate: 0.004179]
		[batch 20/20] avg loss: 0.17098857899794717		[learning rate: 0.0041741]
	Learning Rate: 0.00417411
	LOSS [training: 0.1632578876080802 | validation: 0.12884282247355255]
	TIME [epoch: 8.14 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1724393378547696		[learning rate: 0.0041692]
		[batch 20/20] avg loss: 0.1808979495420796		[learning rate: 0.0041643]
	Learning Rate: 0.00416427
	LOSS [training: 0.17666864369842458 | validation: 0.2624934636298537]
	TIME [epoch: 8.14 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2769915786148306		[learning rate: 0.0041594]
		[batch 20/20] avg loss: 0.2218271589330615		[learning rate: 0.0041544]
	Learning Rate: 0.00415444
	LOSS [training: 0.24940936877394604 | validation: 0.1584437078092885]
	TIME [epoch: 8.14 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1852762720283575		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.1956643050177969		[learning rate: 0.0041446]
	Learning Rate: 0.00414464
	LOSS [training: 0.19047028852307718 | validation: 0.22033894315465458]
	TIME [epoch: 8.17 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2270568037330878		[learning rate: 0.0041398]
		[batch 20/20] avg loss: 0.1478524688532557		[learning rate: 0.0041349]
	Learning Rate: 0.00413487
	LOSS [training: 0.18745463629317174 | validation: 0.2066727061108684]
	TIME [epoch: 8.14 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17169589819816788		[learning rate: 0.00413]
		[batch 20/20] avg loss: 0.2488942081040645		[learning rate: 0.0041251]
	Learning Rate: 0.00412511
	LOSS [training: 0.21029505315111616 | validation: 0.3209659845390136]
	TIME [epoch: 8.14 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18145590708366993		[learning rate: 0.0041202]
		[batch 20/20] avg loss: 0.2039691227279441		[learning rate: 0.0041154]
	Learning Rate: 0.00411538
	LOSS [training: 0.19271251490580696 | validation: 0.17592560099706445]
	TIME [epoch: 8.15 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23260422500177408		[learning rate: 0.0041105]
		[batch 20/20] avg loss: 0.23315470308772168		[learning rate: 0.0041057]
	Learning Rate: 0.00410568
	LOSS [training: 0.23287946404474794 | validation: 0.14649600684696074]
	TIME [epoch: 8.17 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15806679452695618		[learning rate: 0.0041008]
		[batch 20/20] avg loss: 0.1921847404231721		[learning rate: 0.004096]
	Learning Rate: 0.00409599
	LOSS [training: 0.17512576747506414 | validation: 0.17630128324845473]
	TIME [epoch: 8.14 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1896141930153941		[learning rate: 0.0040912]
		[batch 20/20] avg loss: 0.16229678164739886		[learning rate: 0.0040863]
	Learning Rate: 0.00408633
	LOSS [training: 0.1759554873313965 | validation: 0.11179457839014212]
	TIME [epoch: 8.14 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2004371419532069		[learning rate: 0.0040815]
		[batch 20/20] avg loss: 0.22759397912268434		[learning rate: 0.0040767]
	Learning Rate: 0.00407669
	LOSS [training: 0.21401556053794563 | validation: 0.3218962773727181]
	TIME [epoch: 8.14 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2237423936093844		[learning rate: 0.0040719]
		[batch 20/20] avg loss: 0.1828254864243741		[learning rate: 0.0040671]
	Learning Rate: 0.00406707
	LOSS [training: 0.20328394001687924 | validation: 0.42008299062133186]
	TIME [epoch: 8.16 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22633179913611765		[learning rate: 0.0040623]
		[batch 20/20] avg loss: 0.1574866176015277		[learning rate: 0.0040575]
	Learning Rate: 0.00405748
	LOSS [training: 0.1919092083688227 | validation: 0.24166814747305326]
	TIME [epoch: 8.15 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19917119430478222		[learning rate: 0.0040527]
		[batch 20/20] avg loss: 0.25302277779836424		[learning rate: 0.0040479]
	Learning Rate: 0.00404791
	LOSS [training: 0.22609698605157322 | validation: 0.27270439487325504]
	TIME [epoch: 8.13 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17137333634883886		[learning rate: 0.0040431]
		[batch 20/20] avg loss: 0.2189754124499535		[learning rate: 0.0040384]
	Learning Rate: 0.00403836
	LOSS [training: 0.19517437439939617 | validation: 0.1560900727674261]
	TIME [epoch: 8.14 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18396833997613118		[learning rate: 0.0040336]
		[batch 20/20] avg loss: 0.2352339746145556		[learning rate: 0.0040288]
	Learning Rate: 0.00402883
	LOSS [training: 0.2096011572953434 | validation: 0.24126202888005255]
	TIME [epoch: 8.17 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2934570962234707		[learning rate: 0.0040241]
		[batch 20/20] avg loss: 0.14209953945170378		[learning rate: 0.0040193]
	Learning Rate: 0.00401933
	LOSS [training: 0.21777831783758725 | validation: 0.10556933408608105]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18661735882294803		[learning rate: 0.0040146]
		[batch 20/20] avg loss: 0.19363562747065763		[learning rate: 0.0040099]
	Learning Rate: 0.00400985
	LOSS [training: 0.1901264931468028 | validation: 0.18640736554450454]
	TIME [epoch: 8.15 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19756453403615032		[learning rate: 0.0040051]
		[batch 20/20] avg loss: 0.19385739447830025		[learning rate: 0.0040004]
	Learning Rate: 0.00400039
	LOSS [training: 0.19571096425722526 | validation: 0.2124688980557172]
	TIME [epoch: 8.14 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23152235662250292		[learning rate: 0.0039957]
		[batch 20/20] avg loss: 0.32528100045806385		[learning rate: 0.003991]
	Learning Rate: 0.00399096
	LOSS [training: 0.2784016785402834 | validation: 0.1697950863914977]
	TIME [epoch: 8.17 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1504013398538207		[learning rate: 0.0039862]
		[batch 20/20] avg loss: 0.15440610284071168		[learning rate: 0.0039815]
	Learning Rate: 0.00398154
	LOSS [training: 0.15240372134726618 | validation: 0.13647021910199444]
	TIME [epoch: 8.15 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16106543018996816		[learning rate: 0.0039768]
		[batch 20/20] avg loss: 0.17972395026318927		[learning rate: 0.0039721]
	Learning Rate: 0.00397215
	LOSS [training: 0.1703946902265787 | validation: 0.28349189226673616]
	TIME [epoch: 8.15 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23799519769253757		[learning rate: 0.0039675]
		[batch 20/20] avg loss: 0.18910082678941528		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.21354801224097647 | validation: 0.14674165431767358]
	TIME [epoch: 8.15 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18194622866025348		[learning rate: 0.0039581]
		[batch 20/20] avg loss: 0.2022854222191511		[learning rate: 0.0039534]
	Learning Rate: 0.00395343
	LOSS [training: 0.19211582543970226 | validation: 0.19706656771152942]
	TIME [epoch: 8.17 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25617249906621203		[learning rate: 0.0039488]
		[batch 20/20] avg loss: 0.19182776095593485		[learning rate: 0.0039441]
	Learning Rate: 0.00394411
	LOSS [training: 0.22400013001107344 | validation: 0.12693725597168376]
	TIME [epoch: 8.15 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15638508586907363		[learning rate: 0.0039395]
		[batch 20/20] avg loss: 0.16397964640330687		[learning rate: 0.0039348]
	Learning Rate: 0.0039348
	LOSS [training: 0.16018236613619025 | validation: 0.16978757476701636]
	TIME [epoch: 8.15 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2538925545364886		[learning rate: 0.0039302]
		[batch 20/20] avg loss: 0.2077649333064553		[learning rate: 0.0039255]
	Learning Rate: 0.00392552
	LOSS [training: 0.230828743921472 | validation: 0.20285845806470432]
	TIME [epoch: 8.15 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1410356693119061		[learning rate: 0.0039209]
		[batch 20/20] avg loss: 0.20694292768568837		[learning rate: 0.0039163]
	Learning Rate: 0.00391626
	LOSS [training: 0.1739892984987972 | validation: 0.1950746310494722]
	TIME [epoch: 8.17 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20840157849601404		[learning rate: 0.0039116]
		[batch 20/20] avg loss: 0.19027853698470196		[learning rate: 0.003907]
	Learning Rate: 0.00390702
	LOSS [training: 0.19934005774035798 | validation: 0.1311619867424518]
	TIME [epoch: 8.15 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17645661732953105		[learning rate: 0.0039024]
		[batch 20/20] avg loss: 0.37701906006777053		[learning rate: 0.0038978]
	Learning Rate: 0.00389781
	LOSS [training: 0.2767378386986507 | validation: 0.3530429293845038]
	TIME [epoch: 8.14 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22266111102739541		[learning rate: 0.0038932]
		[batch 20/20] avg loss: 0.17666836830481863		[learning rate: 0.0038886]
	Learning Rate: 0.00388861
	LOSS [training: 0.19966473966610704 | validation: 0.1861746231017995]
	TIME [epoch: 8.14 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29638114275409044		[learning rate: 0.003884]
		[batch 20/20] avg loss: 0.1605217743541302		[learning rate: 0.0038794]
	Learning Rate: 0.00387944
	LOSS [training: 0.22845145855411028 | validation: 0.16657408556645903]
	TIME [epoch: 8.17 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1534705105131674		[learning rate: 0.0038749]
		[batch 20/20] avg loss: 0.2783354773142478		[learning rate: 0.0038703]
	Learning Rate: 0.00387029
	LOSS [training: 0.2159029939137076 | validation: 0.22756119788459986]
	TIME [epoch: 8.14 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23476006665144183		[learning rate: 0.0038657]
		[batch 20/20] avg loss: 0.1784602787626088		[learning rate: 0.0038612]
	Learning Rate: 0.00386116
	LOSS [training: 0.2066101727070253 | validation: 0.1688804316532806]
	TIME [epoch: 8.14 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16538641241857294		[learning rate: 0.0038566]
		[batch 20/20] avg loss: 0.2573055454082841		[learning rate: 0.0038521]
	Learning Rate: 0.00385205
	LOSS [training: 0.2113459789134285 | validation: 0.19713282181728758]
	TIME [epoch: 8.14 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22327514746714355		[learning rate: 0.0038475]
		[batch 20/20] avg loss: 0.20923993481223216		[learning rate: 0.003843]
	Learning Rate: 0.00384297
	LOSS [training: 0.21625754113968781 | validation: 0.1797011353164859]
	TIME [epoch: 8.17 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1960084610295687		[learning rate: 0.0038384]
		[batch 20/20] avg loss: 0.15777218251760283		[learning rate: 0.0038339]
	Learning Rate: 0.0038339
	LOSS [training: 0.1768903217735857 | validation: 0.27446371818111803]
	TIME [epoch: 8.16 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17124712271059644		[learning rate: 0.0038294]
		[batch 20/20] avg loss: 0.2596171175776445		[learning rate: 0.0038249]
	Learning Rate: 0.00382486
	LOSS [training: 0.21543212014412044 | validation: 0.2131015139644127]
	TIME [epoch: 8.15 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17151754181747952		[learning rate: 0.0038203]
		[batch 20/20] avg loss: 0.17243479755978783		[learning rate: 0.0038158]
	Learning Rate: 0.00381584
	LOSS [training: 0.17197616968863366 | validation: 0.20033041423065043]
	TIME [epoch: 8.15 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20884999839434837		[learning rate: 0.0038113]
		[batch 20/20] avg loss: 0.21476196555087385		[learning rate: 0.0038068]
	Learning Rate: 0.00380684
	LOSS [training: 0.2118059819726111 | validation: 0.17095712279942926]
	TIME [epoch: 8.17 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20481525882917753		[learning rate: 0.0038023]
		[batch 20/20] avg loss: 0.21195993768320215		[learning rate: 0.0037979]
	Learning Rate: 0.00379786
	LOSS [training: 0.20838759825618985 | validation: 0.1096530035040435]
	TIME [epoch: 8.16 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1866886839895876		[learning rate: 0.0037934]
		[batch 20/20] avg loss: 1.3888403011553756		[learning rate: 0.0037889]
	Learning Rate: 0.0037889
	LOSS [training: 0.7877644925724816 | validation: 1.1298779474797]
	TIME [epoch: 8.15 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2786359280635544		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 4.036871616062951		[learning rate: 0.00378]
	Learning Rate: 0.00377996
	LOSS [training: 3.157753772063253 | validation: 3.552992409408726]
	TIME [epoch: 8.15 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5027200187006464		[learning rate: 0.0037755]
		[batch 20/20] avg loss: 1.8043833453162794		[learning rate: 0.003771]
	Learning Rate: 0.00377104
	LOSS [training: 2.6535516820084633 | validation: 0.3165156853446872]
	TIME [epoch: 8.16 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2388765114554202		[learning rate: 0.0037666]
		[batch 20/20] avg loss: 0.17939330906217205		[learning rate: 0.0037621]
	Learning Rate: 0.00376215
	LOSS [training: 0.20913491025879613 | validation: 0.13709237226765433]
	TIME [epoch: 8.16 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22472589431702028		[learning rate: 0.0037577]
		[batch 20/20] avg loss: 3.550495564030199		[learning rate: 0.0037533]
	Learning Rate: 0.00375327
	LOSS [training: 1.8876107291736097 | validation: 3.606163943010437]
	TIME [epoch: 8.14 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2371360305349137		[learning rate: 0.0037488]
		[batch 20/20] avg loss: 0.950320133228525		[learning rate: 0.0037444]
	Learning Rate: 0.00374442
	LOSS [training: 1.5937280818817199 | validation: 0.3887447625627474]
	TIME [epoch: 8.14 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.380032784893336		[learning rate: 0.00374]
		[batch 20/20] avg loss: 4.69571107782579		[learning rate: 0.0037356]
	Learning Rate: 0.00373559
	LOSS [training: 3.5378719313595623 | validation: 4.470491372489656]
	TIME [epoch: 8.15 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.929865892627066		[learning rate: 0.0037312]
		[batch 20/20] avg loss: 2.170782690626747		[learning rate: 0.0037268]
	Learning Rate: 0.00372678
	LOSS [training: 2.5503242916269064 | validation: 2.2345219820451296]
	TIME [epoch: 8.16 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1429145349774608		[learning rate: 0.0037224]
		[batch 20/20] avg loss: 2.035893170361448		[learning rate: 0.003718]
	Learning Rate: 0.00371799
	LOSS [training: 2.0894038526694545 | validation: 2.377886325804349]
	TIME [epoch: 8.15 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8686859526377877		[learning rate: 0.0037136]
		[batch 20/20] avg loss: 2.829160805918776		[learning rate: 0.0037092]
	Learning Rate: 0.00370922
	LOSS [training: 2.3489233792782818 | validation: 3.4402574228309555]
	TIME [epoch: 8.14 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.390184604341904		[learning rate: 0.0037048]
		[batch 20/20] avg loss: 2.588849139322046		[learning rate: 0.0037005]
	Learning Rate: 0.00370047
	LOSS [training: 2.989516871831976 | validation: 2.7607096332655794]
	TIME [epoch: 8.15 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4867607670717145		[learning rate: 0.0036961]
		[batch 20/20] avg loss: 1.2269971447526415		[learning rate: 0.0036917]
	Learning Rate: 0.00369174
	LOSS [training: 1.8568789559121774 | validation: 0.47002133085744313]
	TIME [epoch: 8.17 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41737664010928854		[learning rate: 0.0036874]
		[batch 20/20] avg loss: 0.3941880879381316		[learning rate: 0.003683]
	Learning Rate: 0.00368303
	LOSS [training: 0.40578236402371004 | validation: 0.6362133265171079]
	TIME [epoch: 8.14 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6052894354071677		[learning rate: 0.0036787]
		[batch 20/20] avg loss: 0.4173950515120611		[learning rate: 0.0036743]
	Learning Rate: 0.00367434
	LOSS [training: 0.5113422434596143 | validation: 0.2457700090247943]
	TIME [epoch: 8.14 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4171385916312963		[learning rate: 0.00367]
		[batch 20/20] avg loss: 0.3535432740805665		[learning rate: 0.0036657]
	Learning Rate: 0.00366567
	LOSS [training: 0.3853409328559314 | validation: 0.30526815526138695]
	TIME [epoch: 8.15 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6700816865819286		[learning rate: 0.0036613]
		[batch 20/20] avg loss: 2.8563153460198363		[learning rate: 0.003657]
	Learning Rate: 0.00365703
	LOSS [training: 1.7631985163008825 | validation: 1.9514330658883008]
	TIME [epoch: 8.17 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1456194093997374		[learning rate: 0.0036527]
		[batch 20/20] avg loss: 0.38121912236962985		[learning rate: 0.0036484]
	Learning Rate: 0.0036484
	LOSS [training: 0.7634192658846837 | validation: 0.31257274116817246]
	TIME [epoch: 8.15 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31202309147544954		[learning rate: 0.0036441]
		[batch 20/20] avg loss: 0.3730275708737386		[learning rate: 0.0036398]
	Learning Rate: 0.00363979
	LOSS [training: 0.3425253311745941 | validation: 0.41140403366794054]
	TIME [epoch: 8.15 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3759478613322812		[learning rate: 0.0036355]
		[batch 20/20] avg loss: 0.360527313844882		[learning rate: 0.0036312]
	Learning Rate: 0.00363121
	LOSS [training: 0.3682375875885815 | validation: 0.2910321431146246]
	TIME [epoch: 8.15 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.386483019389272		[learning rate: 0.0036269]
		[batch 20/20] avg loss: 0.3243833448186707		[learning rate: 0.0036226]
	Learning Rate: 0.00362264
	LOSS [training: 0.35543318210397123 | validation: 0.4044775425490283]
	TIME [epoch: 8.17 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.403946531426189		[learning rate: 0.0036184]
		[batch 20/20] avg loss: 0.3014825111898007		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.35271452130799485 | validation: 0.34480394132687187]
	TIME [epoch: 8.15 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3604311055081325		[learning rate: 0.0036098]
		[batch 20/20] avg loss: 0.3443924487731757		[learning rate: 0.0036056]
	Learning Rate: 0.00360557
	LOSS [training: 0.35241177714065414 | validation: 0.4694747966374032]
	TIME [epoch: 8.15 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6320737818326267		[learning rate: 0.0036013]
		[batch 20/20] avg loss: 0.4127466288769285		[learning rate: 0.0035971]
	Learning Rate: 0.00359707
	LOSS [training: 0.5224102053547776 | validation: 0.3087509755818688]
	TIME [epoch: 8.15 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40769435039582974		[learning rate: 0.0035928]
		[batch 20/20] avg loss: 0.46688918506237187		[learning rate: 0.0035886]
	Learning Rate: 0.00358858
	LOSS [training: 0.43729176772910083 | validation: 0.968184209832278]
	TIME [epoch: 8.18 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3886267300524315		[learning rate: 0.0035843]
		[batch 20/20] avg loss: 0.3012760818640539		[learning rate: 0.0035801]
	Learning Rate: 0.00358012
	LOSS [training: 0.34495140595824275 | validation: 0.20413574661981607]
	TIME [epoch: 8.15 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36227532829618314		[learning rate: 0.0035759]
		[batch 20/20] avg loss: 1.082600826552582		[learning rate: 0.0035717]
	Learning Rate: 0.00357167
	LOSS [training: 0.7224380774243826 | validation: 0.48054496920131107]
	TIME [epoch: 8.15 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41770025226519325		[learning rate: 0.0035675]
		[batch 20/20] avg loss: 1.850020410546294		[learning rate: 0.0035632]
	Learning Rate: 0.00356325
	LOSS [training: 1.1338603314057436 | validation: 2.861377483296711]
	TIME [epoch: 8.15 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.790594350357758		[learning rate: 0.003559]
		[batch 20/20] avg loss: 2.6974877316085353		[learning rate: 0.0035548]
	Learning Rate: 0.00355484
	LOSS [training: 2.7440410409831477 | validation: 2.7704988646195186]
	TIME [epoch: 8.17 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7326188079671203		[learning rate: 0.0035506]
		[batch 20/20] avg loss: 2.589269009219575		[learning rate: 0.0035465]
	Learning Rate: 0.00354646
	LOSS [training: 2.6609439085933477 | validation: 2.136739142752344]
	TIME [epoch: 8.15 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6891525726833283		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 2.5147770376631704		[learning rate: 0.0035381]
	Learning Rate: 0.00353809
	LOSS [training: 2.601964805173249 | validation: 2.458833202720642]
	TIME [epoch: 8.15 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.900458198413342		[learning rate: 0.0035339]
		[batch 20/20] avg loss: 2.3396453682337848		[learning rate: 0.0035297]
	Learning Rate: 0.00352975
	LOSS [training: 2.1200517833235635 | validation: 4.635310798074266]
	TIME [epoch: 8.15 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8690635098190227		[learning rate: 0.0035256]
		[batch 20/20] avg loss: 3.6327777066542444		[learning rate: 0.0035214]
	Learning Rate: 0.00352142
	LOSS [training: 3.750920608236634 | validation: 3.9703504478008407]
	TIME [epoch: 8.17 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.321884141456342		[learning rate: 0.0035173]
		[batch 20/20] avg loss: 3.3084512483760364		[learning rate: 0.0035131]
	Learning Rate: 0.00351311
	LOSS [training: 3.8151676949161897 | validation: 2.4531890189405594]
	TIME [epoch: 8.15 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9933190976550614		[learning rate: 0.003509]
		[batch 20/20] avg loss: 4.1700344455476		[learning rate: 0.0035048]
	Learning Rate: 0.00350483
	LOSS [training: 3.581676771601331 | validation: 4.351099456855229]
	TIME [epoch: 8.14 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.873815058305245		[learning rate: 0.0035007]
		[batch 20/20] avg loss: 4.324179004643053		[learning rate: 0.0034966]
	Learning Rate: 0.00349656
	LOSS [training: 4.098997031474149 | validation: 4.9366822711109055]
	TIME [epoch: 8.14 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.119953556013219		[learning rate: 0.0034924]
		[batch 20/20] avg loss: 3.7895363770107147		[learning rate: 0.0034883]
	Learning Rate: 0.00348831
	LOSS [training: 4.454744966511967 | validation: 2.8166885923669516]
	TIME [epoch: 8.17 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.1338050593292195		[learning rate: 0.0034842]
		[batch 20/20] avg loss: 3.405746663672736		[learning rate: 0.0034801]
	Learning Rate: 0.00348008
	LOSS [training: 3.769775861500978 | validation: 3.103853659042906]
	TIME [epoch: 8.15 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8787174818142134		[learning rate: 0.003476]
		[batch 20/20] avg loss: 2.364060835579805		[learning rate: 0.0034719]
	Learning Rate: 0.00347187
	LOSS [training: 2.621389158697009 | validation: 2.3321601599957584]
	TIME [epoch: 8.14 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4686428257829163		[learning rate: 0.0034678]
		[batch 20/20] avg loss: 2.168485057959617		[learning rate: 0.0034637]
	Learning Rate: 0.00346369
	LOSS [training: 2.318563941871266 | validation: 2.212982852608148]
	TIME [epoch: 8.14 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.071690560901488		[learning rate: 0.0034596]
		[batch 20/20] avg loss: 1.0332344910140905		[learning rate: 0.0034555]
	Learning Rate: 0.00345552
	LOSS [training: 1.5524625259577889 | validation: 0.35867384204939096]
	TIME [epoch: 8.17 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8445327275455622		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 1.1907935186959555		[learning rate: 0.0034474]
	Learning Rate: 0.00344736
	LOSS [training: 1.017663123120759 | validation: 0.7798411025344196]
	TIME [epoch: 8.15 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7685153132514857		[learning rate: 0.0034433]
		[batch 20/20] avg loss: 0.5003168452090598		[learning rate: 0.0034392]
	Learning Rate: 0.00343923
	LOSS [training: 0.6344160792302729 | validation: 0.27018742193959233]
	TIME [epoch: 8.15 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29205217097391883		[learning rate: 0.0034352]
		[batch 20/20] avg loss: 0.3583197521010732		[learning rate: 0.0034311]
	Learning Rate: 0.00343112
	LOSS [training: 0.325185961537496 | validation: 0.2697104636592307]
	TIME [epoch: 8.14 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3405964567568263		[learning rate: 0.0034271]
		[batch 20/20] avg loss: 0.33289646335534206		[learning rate: 0.003423]
	Learning Rate: 0.00342303
	LOSS [training: 0.33674646005608416 | validation: 0.23012172945042536]
	TIME [epoch: 8.17 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28998092156194843		[learning rate: 0.003419]
		[batch 20/20] avg loss: 0.3122931937895127		[learning rate: 0.003415]
	Learning Rate: 0.00341495
	LOSS [training: 0.30113705767573057 | validation: 0.30891484369820477]
	TIME [epoch: 8.15 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2647979228538292		[learning rate: 0.0034109]
		[batch 20/20] avg loss: 0.6589548465379782		[learning rate: 0.0034069]
	Learning Rate: 0.0034069
	LOSS [training: 0.46187638469590364 | validation: 1.4050566705030896]
	TIME [epoch: 8.15 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1305524446892496		[learning rate: 0.0034029]
		[batch 20/20] avg loss: 0.545025253596837		[learning rate: 0.0033989]
	Learning Rate: 0.00339886
	LOSS [training: 1.3377888491430434 | validation: 0.27626915774578903]
	TIME [epoch: 8.15 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2744616354748145		[learning rate: 0.0033948]
		[batch 20/20] avg loss: 0.208109994219219		[learning rate: 0.0033908]
	Learning Rate: 0.00339084
	LOSS [training: 0.24128581484701672 | validation: 0.27471667440595615]
	TIME [epoch: 8.16 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21084568636974582		[learning rate: 0.0033868]
		[batch 20/20] avg loss: 0.20642490785946638		[learning rate: 0.0033828]
	Learning Rate: 0.00338284
	LOSS [training: 0.20863529711460616 | validation: 0.13724205834413256]
	TIME [epoch: 8.16 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25116567423872593		[learning rate: 0.0033789]
		[batch 20/20] avg loss: 0.27116238010146787		[learning rate: 0.0033749]
	Learning Rate: 0.00337487
	LOSS [training: 0.26116402717009685 | validation: 0.2713945144588545]
	TIME [epoch: 8.14 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3050292978016194		[learning rate: 0.0033709]
		[batch 20/20] avg loss: 0.2942722715762434		[learning rate: 0.0033669]
	Learning Rate: 0.0033669
	LOSS [training: 0.29965078468893147 | validation: 0.11173216751506011]
	TIME [epoch: 8.14 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19774729942370622		[learning rate: 0.0033629]
		[batch 20/20] avg loss: 0.1885133572522288		[learning rate: 0.003359]
	Learning Rate: 0.00335896
	LOSS [training: 0.19313032833796753 | validation: 0.2209883576484773]
	TIME [epoch: 8.16 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21408970260735788		[learning rate: 0.003355]
		[batch 20/20] avg loss: 0.18916859762682145		[learning rate: 0.003351]
	Learning Rate: 0.00335104
	LOSS [training: 0.20162915011708965 | validation: 0.12217162061150806]
	TIME [epoch: 8.16 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23685547054816186		[learning rate: 0.0033471]
		[batch 20/20] avg loss: 0.201147621774794		[learning rate: 0.0033431]
	Learning Rate: 0.00334313
	LOSS [training: 0.21900154616147796 | validation: 0.19057381394672807]
	TIME [epoch: 8.15 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20643280813451623		[learning rate: 0.0033392]
		[batch 20/20] avg loss: 0.21483850277643746		[learning rate: 0.0033352]
	Learning Rate: 0.00333525
	LOSS [training: 0.21063565545547683 | validation: 0.15237156976805613]
	TIME [epoch: 8.15 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1740808223642644		[learning rate: 0.0033313]
		[batch 20/20] avg loss: 0.17882655577509604		[learning rate: 0.0033274]
	Learning Rate: 0.00332738
	LOSS [training: 0.17645368906968023 | validation: 0.2532126540864736]
	TIME [epoch: 8.16 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17293797799708546		[learning rate: 0.0033235]
		[batch 20/20] avg loss: 0.19172473453656985		[learning rate: 0.0033195]
	Learning Rate: 0.00331953
	LOSS [training: 0.1823313562668277 | validation: 0.20184654290045817]
	TIME [epoch: 8.16 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1968803364878397		[learning rate: 0.0033156]
		[batch 20/20] avg loss: 0.1750088364564612		[learning rate: 0.0033117]
	Learning Rate: 0.0033117
	LOSS [training: 0.18594458647215045 | validation: 0.21132551569580177]
	TIME [epoch: 8.15 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22629510902619895		[learning rate: 0.0033078]
		[batch 20/20] avg loss: 0.20707111273027765		[learning rate: 0.0033039]
	Learning Rate: 0.00330389
	LOSS [training: 0.21668311087823827 | validation: 0.16374961922871878]
	TIME [epoch: 8.15 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15539365623061957		[learning rate: 0.0033]
		[batch 20/20] avg loss: 0.1653738691222937		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.16038376267645663 | validation: 0.08877437516485923]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_520.pth
	Model improved!!!
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18429517690362288		[learning rate: 0.0032922]
		[batch 20/20] avg loss: 0.17267623811332802		[learning rate: 0.0032883]
	Learning Rate: 0.00328832
	LOSS [training: 0.17848570750847542 | validation: 0.13773883343188698]
	TIME [epoch: 8.15 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22876809506451906		[learning rate: 0.0032844]
		[batch 20/20] avg loss: 0.16070054672046905		[learning rate: 0.0032806]
	Learning Rate: 0.00328057
	LOSS [training: 0.19473432089249407 | validation: 0.15054406249854052]
	TIME [epoch: 8.14 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1858378409694667		[learning rate: 0.0032767]
		[batch 20/20] avg loss: 0.2434243649949109		[learning rate: 0.0032728]
	Learning Rate: 0.00327283
	LOSS [training: 0.2146311029821888 | validation: 0.17441082938598404]
	TIME [epoch: 8.14 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24818047873046595		[learning rate: 0.003269]
		[batch 20/20] avg loss: 0.17762270880896033		[learning rate: 0.0032651]
	Learning Rate: 0.00326511
	LOSS [training: 0.2129015937697131 | validation: 0.1796699930753896]
	TIME [epoch: 8.16 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18074090163608034		[learning rate: 0.0032613]
		[batch 20/20] avg loss: 0.1575995505381807		[learning rate: 0.0032574]
	Learning Rate: 0.00325741
	LOSS [training: 0.16917022608713053 | validation: 0.35173425617359777]
	TIME [epoch: 8.15 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20193449748931608		[learning rate: 0.0032536]
		[batch 20/20] avg loss: 0.18295908523239474		[learning rate: 0.0032497]
	Learning Rate: 0.00324972
	LOSS [training: 0.1924467913608554 | validation: 0.15264653576070258]
	TIME [epoch: 8.14 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16564487611248027		[learning rate: 0.0032459]
		[batch 20/20] avg loss: 0.15862455725436786		[learning rate: 0.0032421]
	Learning Rate: 0.00324206
	LOSS [training: 0.16213471668342408 | validation: 0.18781628298433423]
	TIME [epoch: 8.14 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19783370052036853		[learning rate: 0.0032382]
		[batch 20/20] avg loss: 0.17044199972431556		[learning rate: 0.0032344]
	Learning Rate: 0.00323441
	LOSS [training: 0.18413785012234202 | validation: 0.17710061469086202]
	TIME [epoch: 8.15 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24926546162963042		[learning rate: 0.0032306]
		[batch 20/20] avg loss: 0.20492656821147598		[learning rate: 0.0032268]
	Learning Rate: 0.00322678
	LOSS [training: 0.2270960149205532 | validation: 0.10135814090386865]
	TIME [epoch: 8.15 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1899577562653931		[learning rate: 0.003223]
		[batch 20/20] avg loss: 0.17986596875223584		[learning rate: 0.0032192]
	Learning Rate: 0.00321917
	LOSS [training: 0.1849118625088145 | validation: 0.2908485884498362]
	TIME [epoch: 8.14 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17344039585007076		[learning rate: 0.0032154]
		[batch 20/20] avg loss: 0.1821574686618899		[learning rate: 0.0032116]
	Learning Rate: 0.00321157
	LOSS [training: 0.1777989322559803 | validation: 0.16590148636848734]
	TIME [epoch: 8.14 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13514589510317107		[learning rate: 0.0032078]
		[batch 20/20] avg loss: 0.20185918964008204		[learning rate: 0.003204]
	Learning Rate: 0.003204
	LOSS [training: 0.16850254237162654 | validation: 0.14091418306950357]
	TIME [epoch: 8.14 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16819012509382542		[learning rate: 0.0032002]
		[batch 20/20] avg loss: 0.1543792469096848		[learning rate: 0.0031964]
	Learning Rate: 0.00319644
	LOSS [training: 0.16128468600175508 | validation: 0.09631371887582443]
	TIME [epoch: 8.17 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13200015322908062		[learning rate: 0.0031927]
		[batch 20/20] avg loss: 0.19968143993971627		[learning rate: 0.0031889]
	Learning Rate: 0.0031889
	LOSS [training: 0.16584079658439846 | validation: 0.1675312880476869]
	TIME [epoch: 8.14 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1675805529218294		[learning rate: 0.0031851]
		[batch 20/20] avg loss: 0.13424312519642595		[learning rate: 0.0031814]
	Learning Rate: 0.00318138
	LOSS [training: 0.15091183905912767 | validation: 0.1600692927302479]
	TIME [epoch: 8.15 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15191038886104957		[learning rate: 0.0031776]
		[batch 20/20] avg loss: 0.1776244176122288		[learning rate: 0.0031739]
	Learning Rate: 0.00317387
	LOSS [training: 0.1647674032366392 | validation: 0.1464470346146069]
	TIME [epoch: 8.15 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16386457430437415		[learning rate: 0.0031701]
		[batch 20/20] avg loss: 0.1682884320091623		[learning rate: 0.0031664]
	Learning Rate: 0.00316639
	LOSS [training: 0.16607650315676822 | validation: 0.3576331781899338]
	TIME [epoch: 8.17 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21893427572212118		[learning rate: 0.0031627]
		[batch 20/20] avg loss: 0.14952080070352125		[learning rate: 0.0031589]
	Learning Rate: 0.00315892
	LOSS [training: 0.18422753821282123 | validation: 0.12292731360405573]
	TIME [epoch: 8.15 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1832711186654773		[learning rate: 0.0031552]
		[batch 20/20] avg loss: 0.15284061384661154		[learning rate: 0.0031515]
	Learning Rate: 0.00315147
	LOSS [training: 0.16805586625604443 | validation: 0.36200456803576264]
	TIME [epoch: 8.14 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16979753253656926		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.13077559012747203		[learning rate: 0.003144]
	Learning Rate: 0.00314403
	LOSS [training: 0.15028656133202065 | validation: 0.17245709966703704]
	TIME [epoch: 8.14 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1905492088361067		[learning rate: 0.0031403]
		[batch 20/20] avg loss: 0.1544968948718051		[learning rate: 0.0031366]
	Learning Rate: 0.00313662
	LOSS [training: 0.17252305185395592 | validation: 0.1205055285296672]
	TIME [epoch: 8.18 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1528996659947697		[learning rate: 0.0031329]
		[batch 20/20] avg loss: 0.14077968480837083		[learning rate: 0.0031292]
	Learning Rate: 0.00312922
	LOSS [training: 0.14683967540157028 | validation: 0.15182348238021204]
	TIME [epoch: 8.15 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17352500411748578		[learning rate: 0.0031255]
		[batch 20/20] avg loss: 0.18518488488267362		[learning rate: 0.0031218]
	Learning Rate: 0.00312184
	LOSS [training: 0.17935494450007966 | validation: 0.2994973909314823]
	TIME [epoch: 8.14 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25325064572320216		[learning rate: 0.0031182]
		[batch 20/20] avg loss: 0.22925927823581235		[learning rate: 0.0031145]
	Learning Rate: 0.00311447
	LOSS [training: 0.24125496197950724 | validation: 0.19740139414406588]
	TIME [epoch: 8.15 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1908638233891168		[learning rate: 0.0031108]
		[batch 20/20] avg loss: 0.12952763291177458		[learning rate: 0.0031071]
	Learning Rate: 0.00310713
	LOSS [training: 0.16019572815044567 | validation: 0.1532766213867301]
	TIME [epoch: 8.18 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17154761259797469		[learning rate: 0.0031035]
		[batch 20/20] avg loss: 0.12898466461714153		[learning rate: 0.0030998]
	Learning Rate: 0.0030998
	LOSS [training: 0.1502661386075581 | validation: 0.0903824931241877]
	TIME [epoch: 8.15 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14206967937827997		[learning rate: 0.0030961]
		[batch 20/20] avg loss: 0.19296531675874798		[learning rate: 0.0030925]
	Learning Rate: 0.00309249
	LOSS [training: 0.16751749806851396 | validation: 0.12958499010026733]
	TIME [epoch: 8.15 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19769264439319315		[learning rate: 0.0030888]
		[batch 20/20] avg loss: 0.16394683011871095		[learning rate: 0.0030852]
	Learning Rate: 0.00308519
	LOSS [training: 0.18081973725595205 | validation: 0.21922959282347165]
	TIME [epoch: 8.15 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22781716014101444		[learning rate: 0.0030815]
		[batch 20/20] avg loss: 0.14401975326189004		[learning rate: 0.0030779]
	Learning Rate: 0.00307791
	LOSS [training: 0.18591845670145218 | validation: 0.1418050901337779]
	TIME [epoch: 8.17 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1656844463032851		[learning rate: 0.0030743]
		[batch 20/20] avg loss: 0.15386400442504403		[learning rate: 0.0030707]
	Learning Rate: 0.00307065
	LOSS [training: 0.15977422536416452 | validation: 0.23623020856226268]
	TIME [epoch: 8.15 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19289833169481482		[learning rate: 0.003067]
		[batch 20/20] avg loss: 0.15437794455577186		[learning rate: 0.0030634]
	Learning Rate: 0.00306341
	LOSS [training: 0.17363813812529336 | validation: 0.1589432376647171]
	TIME [epoch: 8.15 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12279812823950258		[learning rate: 0.0030598]
		[batch 20/20] avg loss: 0.15837894562376342		[learning rate: 0.0030562]
	Learning Rate: 0.00305618
	LOSS [training: 0.14058853693163298 | validation: 0.1301661293926785]
	TIME [epoch: 8.15 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14508176410988952		[learning rate: 0.0030526]
		[batch 20/20] avg loss: 0.14832917785827007		[learning rate: 0.003049]
	Learning Rate: 0.00304897
	LOSS [training: 0.14670547098407974 | validation: 0.08157917706292703]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13693642484100846		[learning rate: 0.0030454]
		[batch 20/20] avg loss: 0.1296656340419155		[learning rate: 0.0030418]
	Learning Rate: 0.00304178
	LOSS [training: 0.133301029441462 | validation: 0.12261729591745546]
	TIME [epoch: 8.15 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14368636189954218		[learning rate: 0.0030382]
		[batch 20/20] avg loss: 0.14467437806530833		[learning rate: 0.0030346]
	Learning Rate: 0.00303461
	LOSS [training: 0.14418036998242525 | validation: 0.24386328508856614]
	TIME [epoch: 8.15 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16060136482040485		[learning rate: 0.003031]
		[batch 20/20] avg loss: 0.18617515877089605		[learning rate: 0.0030274]
	Learning Rate: 0.00302745
	LOSS [training: 0.17338826179565042 | validation: 0.15539432927076752]
	TIME [epoch: 8.15 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12848089116367664		[learning rate: 0.0030239]
		[batch 20/20] avg loss: 0.18145685781920515		[learning rate: 0.0030203]
	Learning Rate: 0.00302031
	LOSS [training: 0.15496887449144092 | validation: 0.15810613624625963]
	TIME [epoch: 8.17 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21755444093411325		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.14423305651479418		[learning rate: 0.0030132]
	Learning Rate: 0.00301318
	LOSS [training: 0.18089374872445368 | validation: 0.28384474868578224]
	TIME [epoch: 8.15 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16118900204565276		[learning rate: 0.0030096]
		[batch 20/20] avg loss: 0.17289768836521588		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.16704334520543435 | validation: 0.14577734068105802]
	TIME [epoch: 8.14 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17663042161854142		[learning rate: 0.0030025]
		[batch 20/20] avg loss: 0.13702585866371322		[learning rate: 0.002999]
	Learning Rate: 0.00299899
	LOSS [training: 0.15682814014112734 | validation: 0.09208620206383998]
	TIME [epoch: 8.15 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12729028749621513		[learning rate: 0.0029954]
		[batch 20/20] avg loss: 0.1574401160675593		[learning rate: 0.0029919]
	Learning Rate: 0.00299191
	LOSS [training: 0.14236520178188725 | validation: 0.1392609003106856]
	TIME [epoch: 8.17 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2033834000178712		[learning rate: 0.0029884]
		[batch 20/20] avg loss: 0.1689931196243875		[learning rate: 0.0029849]
	Learning Rate: 0.00298485
	LOSS [training: 0.18618825982112935 | validation: 0.19020189093000975]
	TIME [epoch: 8.15 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1726991153875789		[learning rate: 0.0029813]
		[batch 20/20] avg loss: 0.15647012936025795		[learning rate: 0.0029778]
	Learning Rate: 0.00297781
	LOSS [training: 0.16458462237391844 | validation: 0.13810400977938103]
	TIME [epoch: 8.14 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11220948822024139		[learning rate: 0.0029743]
		[batch 20/20] avg loss: 0.17383738561296908		[learning rate: 0.0029708]
	Learning Rate: 0.00297079
	LOSS [training: 0.1430234369166053 | validation: 0.10081582109178365]
	TIME [epoch: 8.14 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14758209894258265		[learning rate: 0.0029673]
		[batch 20/20] avg loss: 0.17068224983506108		[learning rate: 0.0029638]
	Learning Rate: 0.00296378
	LOSS [training: 0.15913217438882188 | validation: 0.14502950254584]
	TIME [epoch: 8.17 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.171503256636172		[learning rate: 0.0029603]
		[batch 20/20] avg loss: 0.1700468235981749		[learning rate: 0.0029568]
	Learning Rate: 0.00295679
	LOSS [training: 0.17077504011717343 | validation: 0.13559092187291205]
	TIME [epoch: 8.15 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13252373448227933		[learning rate: 0.0029533]
		[batch 20/20] avg loss: 0.1513129300634058		[learning rate: 0.0029498]
	Learning Rate: 0.00294982
	LOSS [training: 0.1419183322728426 | validation: 0.17913800493571275]
	TIME [epoch: 8.15 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1883079863130575		[learning rate: 0.0029463]
		[batch 20/20] avg loss: 0.21019339430349024		[learning rate: 0.0029429]
	Learning Rate: 0.00294286
	LOSS [training: 0.19925069030827391 | validation: 0.14334908902576643]
	TIME [epoch: 8.15 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14715194445482416		[learning rate: 0.0029394]
		[batch 20/20] avg loss: 0.18489439621603354		[learning rate: 0.0029359]
	Learning Rate: 0.00293592
	LOSS [training: 0.16602317033542885 | validation: 0.280534452256612]
	TIME [epoch: 8.17 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16698634506556492		[learning rate: 0.0029325]
		[batch 20/20] avg loss: 0.17707351787165707		[learning rate: 0.002929]
	Learning Rate: 0.00292899
	LOSS [training: 0.172029931468611 | validation: 0.1896785904307843]
	TIME [epoch: 8.16 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1671882015881218		[learning rate: 0.0029255]
		[batch 20/20] avg loss: 0.18214231900905464		[learning rate: 0.0029221]
	Learning Rate: 0.00292208
	LOSS [training: 0.17466526029858825 | validation: 0.2584419551229124]
	TIME [epoch: 8.15 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19999024070001845		[learning rate: 0.0029186]
		[batch 20/20] avg loss: 0.1617834348022521		[learning rate: 0.0029152]
	Learning Rate: 0.00291519
	LOSS [training: 0.1808868377511353 | validation: 0.1764174137791804]
	TIME [epoch: 8.15 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18678037208788567		[learning rate: 0.0029117]
		[batch 20/20] avg loss: 0.1743729864168661		[learning rate: 0.0029083]
	Learning Rate: 0.00290831
	LOSS [training: 0.18057667925237594 | validation: 0.09835998947641796]
	TIME [epoch: 8.16 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21195563236375553		[learning rate: 0.0029049]
		[batch 20/20] avg loss: 0.15666731588118538		[learning rate: 0.0029015]
	Learning Rate: 0.00290145
	LOSS [training: 0.18431147412247043 | validation: 0.13943941626986298]
	TIME [epoch: 8.16 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16181041555060938		[learning rate: 0.002898]
		[batch 20/20] avg loss: 0.23792810610937248		[learning rate: 0.0028946]
	Learning Rate: 0.00289461
	LOSS [training: 0.19986926082999096 | validation: 0.18014247419045645]
	TIME [epoch: 8.16 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21956666450039092		[learning rate: 0.0028912]
		[batch 20/20] avg loss: 0.12856742982557204		[learning rate: 0.0028878]
	Learning Rate: 0.00288778
	LOSS [training: 0.17406704716298144 | validation: 0.1070193945046221]
	TIME [epoch: 8.15 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14475121399896412		[learning rate: 0.0028844]
		[batch 20/20] avg loss: 0.15613729868831513		[learning rate: 0.002881]
	Learning Rate: 0.00288097
	LOSS [training: 0.15044425634363964 | validation: 0.11783787508488844]
	TIME [epoch: 8.17 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13986377486190799		[learning rate: 0.0028776]
		[batch 20/20] avg loss: 0.14852110517418177		[learning rate: 0.0028742]
	Learning Rate: 0.00287417
	LOSS [training: 0.1441924400180449 | validation: 0.15033365654239825]
	TIME [epoch: 8.17 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16151704050279678		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.1387505885204774		[learning rate: 0.0028674]
	Learning Rate: 0.00286739
	LOSS [training: 0.1501338145116371 | validation: 0.1966021111270712]
	TIME [epoch: 8.15 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1744695581296989		[learning rate: 0.002864]
		[batch 20/20] avg loss: 0.16828807436721713		[learning rate: 0.0028606]
	Learning Rate: 0.00286063
	LOSS [training: 0.17137881624845802 | validation: 0.17296309762346984]
	TIME [epoch: 8.15 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15918741207465026		[learning rate: 0.0028573]
		[batch 20/20] avg loss: 0.1535995538409624		[learning rate: 0.0028539]
	Learning Rate: 0.00285388
	LOSS [training: 0.15639348295780633 | validation: 0.28399640446343294]
	TIME [epoch: 8.17 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18840542030128274		[learning rate: 0.0028505]
		[batch 20/20] avg loss: 0.1434908755907186		[learning rate: 0.0028471]
	Learning Rate: 0.00284715
	LOSS [training: 0.16594814794600066 | validation: 0.1330482066836073]
	TIME [epoch: 8.16 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19216970189294147		[learning rate: 0.0028438]
		[batch 20/20] avg loss: 0.16095316976142354		[learning rate: 0.0028404]
	Learning Rate: 0.00284043
	LOSS [training: 0.17656143582718253 | validation: 0.1684237743298476]
	TIME [epoch: 8.14 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15279754938442036		[learning rate: 0.0028371]
		[batch 20/20] avg loss: 0.1769231788484324		[learning rate: 0.0028337]
	Learning Rate: 0.00283373
	LOSS [training: 0.16486036411642635 | validation: 0.0748054986423098]
	TIME [epoch: 8.15 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r0_20240219_184940/states/model_tr_study1_584.pth
	Model improved!!!
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1679290169703344		[learning rate: 0.0028304]
		[batch 20/20] avg loss: 0.17314352998184418		[learning rate: 0.002827]
	Learning Rate: 0.00282705
	LOSS [training: 0.1705362734760893 | validation: 0.09162021696197634]
	TIME [epoch: 8.16 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13144344730252705		[learning rate: 0.0028237]
		[batch 20/20] avg loss: 0.18509405874881996		[learning rate: 0.0028204]
	Learning Rate: 0.00282038
	LOSS [training: 0.15826875302567353 | validation: 0.33181156325632116]
	TIME [epoch: 8.16 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21519391295454343		[learning rate: 0.0028171]
		[batch 20/20] avg loss: 0.137576215851064		[learning rate: 0.0028137]
	Learning Rate: 0.00281373
	LOSS [training: 0.17638506440280372 | validation: 0.23741842657469653]
	TIME [epoch: 8.15 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12787493495924013		[learning rate: 0.0028104]
		[batch 20/20] avg loss: 0.1692252653610882		[learning rate: 0.0028071]
	Learning Rate: 0.00280709
	LOSS [training: 0.1485501001601642 | validation: 0.23932363277943033]
	TIME [epoch: 8.15 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15120762240951918		[learning rate: 0.0028038]
		[batch 20/20] avg loss: 0.18228956194953283		[learning rate: 0.0028005]
	Learning Rate: 0.00280047
	LOSS [training: 0.16674859217952598 | validation: 0.13687779126488578]
	TIME [epoch: 8.16 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17404741184170197		[learning rate: 0.0027972]
		[batch 20/20] avg loss: 0.17062769745661313		[learning rate: 0.0027939]
	Learning Rate: 0.00279386
	LOSS [training: 0.17233755464915756 | validation: 0.17656530418103142]
	TIME [epoch: 8.16 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12374559415031856		[learning rate: 0.0027906]
		[batch 20/20] avg loss: 0.17110915123118145		[learning rate: 0.0027873]
	Learning Rate: 0.00278727
	LOSS [training: 0.14742737269075 | validation: 0.17880460642337656]
	TIME [epoch: 8.15 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21018544823154534		[learning rate: 0.002784]
		[batch 20/20] avg loss: 0.12935660636166763		[learning rate: 0.0027807]
	Learning Rate: 0.0027807
	LOSS [training: 0.16977102729660648 | validation: 0.09274452194590602]
	TIME [epoch: 8.14 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14304529001180372		[learning rate: 0.0027774]
		[batch 20/20] avg loss: 0.11952088338630844		[learning rate: 0.0027741]
	Learning Rate: 0.00277414
	LOSS [training: 0.13128308669905608 | validation: 0.23933766793605285]
	TIME [epoch: 8.15 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15633042934083663		[learning rate: 0.0027709]
		[batch 20/20] avg loss: 0.17323616558620042		[learning rate: 0.0027676]
	Learning Rate: 0.00276759
	LOSS [training: 0.16478329746351855 | validation: 0.16654287223197256]
	TIME [epoch: 8.16 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.220118182475386		[learning rate: 0.0027643]
		[batch 20/20] avg loss: 0.18540042477183466		[learning rate: 0.0027611]
	Learning Rate: 0.00276107
	LOSS [training: 0.2027593036236103 | validation: 0.13069457173220297]
	TIME [epoch: 8.14 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12302589163095054		[learning rate: 0.0027578]
		[batch 20/20] avg loss: 0.1799863667454172		[learning rate: 0.0027546]
	Learning Rate: 0.00275455
	LOSS [training: 0.15150612918818387 | validation: 0.3053628312086635]
	TIME [epoch: 8.14 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19019808651355624		[learning rate: 0.0027513]
		[batch 20/20] avg loss: 0.21355207714598312		[learning rate: 0.0027481]
	Learning Rate: 0.00274806
	LOSS [training: 0.20187508182976965 | validation: 0.10247651532203204]
	TIME [epoch: 8.14 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14075301590698577		[learning rate: 0.0027448]
		[batch 20/20] avg loss: 0.18020466238716332		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.16047883914707456 | validation: 0.1388762775282249]
	TIME [epoch: 8.17 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19916784870416152		[learning rate: 0.0027383]
		[batch 20/20] avg loss: 0.1613676980392335		[learning rate: 0.0027351]
	Learning Rate: 0.00273511
	LOSS [training: 0.1802677733716975 | validation: 0.11256916393782004]
	TIME [epoch: 8.15 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18759167019749962		[learning rate: 0.0027319]
		[batch 20/20] avg loss: 0.17067591020622686		[learning rate: 0.0027287]
	Learning Rate: 0.00272866
	LOSS [training: 0.17913379020186324 | validation: 0.1891105897347663]
	TIME [epoch: 8.14 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1613464522308846		[learning rate: 0.0027254]
		[batch 20/20] avg loss: 0.17182780241143847		[learning rate: 0.0027222]
	Learning Rate: 0.00272222
	LOSS [training: 0.16658712732116152 | validation: 0.2818129383725661]
	TIME [epoch: 8.14 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15869321845199796		[learning rate: 0.002719]
		[batch 20/20] avg loss: 0.12335626960149793		[learning rate: 0.0027158]
	Learning Rate: 0.0027158
	LOSS [training: 0.14102474402674794 | validation: 0.15371747554273466]
	TIME [epoch: 8.17 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12844562677403507		[learning rate: 0.0027126]
		[batch 20/20] avg loss: 0.17869153076813488		[learning rate: 0.0027094]
	Learning Rate: 0.00270939
	LOSS [training: 0.15356857877108498 | validation: 0.10622644698568966]
	TIME [epoch: 8.14 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16771453192833638		[learning rate: 0.0027062]
		[batch 20/20] avg loss: 0.14168405286475		[learning rate: 0.002703]
	Learning Rate: 0.002703
	LOSS [training: 0.15469929239654318 | validation: 0.12558226601745826]
	TIME [epoch: 8.14 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14314676507945825		[learning rate: 0.0026998]
		[batch 20/20] avg loss: 0.20657829005935774		[learning rate: 0.0026966]
	Learning Rate: 0.00269662
	LOSS [training: 0.17486252756940798 | validation: 0.12229225070313542]
	TIME [epoch: 8.15 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12115636034819414		[learning rate: 0.0026934]
		[batch 20/20] avg loss: 0.16787682549995275		[learning rate: 0.0026903]
	Learning Rate: 0.00269026
	LOSS [training: 0.14451659292407343 | validation: 0.1256246806139683]
	TIME [epoch: 8.17 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17053041560348686		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.17693678322173384		[learning rate: 0.0026839]
	Learning Rate: 0.00268392
	LOSS [training: 0.17373359941261035 | validation: 0.18127332429471144]
	TIME [epoch: 8.14 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14917904092351358		[learning rate: 0.0026808]
		[batch 20/20] avg loss: 0.15940656328775712		[learning rate: 0.0026776]
	Learning Rate: 0.00267759
	LOSS [training: 0.15429280210563537 | validation: 0.12001283097483825]
	TIME [epoch: 8.15 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13424138711546071		[learning rate: 0.0026744]
		[batch 20/20] avg loss: 0.1534722152644783		[learning rate: 0.0026713]
	Learning Rate: 0.00267127
	LOSS [training: 0.14385680118996952 | validation: 0.1784027101067082]
	TIME [epoch: 8.14 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13785480309205547		[learning rate: 0.0026681]
		[batch 20/20] avg loss: 0.1558422303705947		[learning rate: 0.002665]
	Learning Rate: 0.00266497
	LOSS [training: 0.1468485167313251 | validation: 0.17538205088671427]
	TIME [epoch: 8.17 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16595589662338345		[learning rate: 0.0026618]
		[batch 20/20] avg loss: 0.1525918812020312		[learning rate: 0.0026587]
	Learning Rate: 0.00265868
	LOSS [training: 0.15927388891270736 | validation: 0.10073467030758292]
	TIME [epoch: 8.14 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12451002228897712		[learning rate: 0.0026555]
		[batch 20/20] avg loss: 0.13379316781681608		[learning rate: 0.0026524]
	Learning Rate: 0.00265241
	LOSS [training: 0.1291515950528966 | validation: 0.12251673386800453]
	TIME [epoch: 8.14 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17238402953859308		[learning rate: 0.0026493]
		[batch 20/20] avg loss: 0.15990756582042143		[learning rate: 0.0026462]
	Learning Rate: 0.00264616
	LOSS [training: 0.16614579767950724 | validation: 0.13267303685560486]
	TIME [epoch: 8.15 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15075969535410155		[learning rate: 0.002643]
		[batch 20/20] avg loss: 0.11882992306908544		[learning rate: 0.0026399]
	Learning Rate: 0.00263991
	LOSS [training: 0.13479480921159354 | validation: 0.15381460035599037]
	TIME [epoch: 8.17 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17417749954438133		[learning rate: 0.0026368]
		[batch 20/20] avg loss: 0.17076932812016277		[learning rate: 0.0026337]
	Learning Rate: 0.00263369
	LOSS [training: 0.17247341383227205 | validation: 0.22262261032543312]
	TIME [epoch: 8.14 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13346493780392138		[learning rate: 0.0026306]
		[batch 20/20] avg loss: 0.21647675058112106		[learning rate: 0.0026275]
	Learning Rate: 0.00262747
	LOSS [training: 0.17497084419252124 | validation: 0.0841255308085509]
	TIME [epoch: 8.14 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1778304251627121		[learning rate: 0.0026244]
		[batch 20/20] avg loss: 0.12448554720817051		[learning rate: 0.0026213]
	Learning Rate: 0.00262128
	LOSS [training: 0.1511579861854413 | validation: 0.14014292886101629]
	TIME [epoch: 8.14 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17506446857854704		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.17141159302378978		[learning rate: 0.0026151]
	Learning Rate: 0.00261509
	LOSS [training: 0.1732380308011684 | validation: 0.09941811739745791]
	TIME [epoch: 8.17 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12904382264563727		[learning rate: 0.002612]
		[batch 20/20] avg loss: 0.1383462130388701		[learning rate: 0.0026089]
	Learning Rate: 0.00260892
	LOSS [training: 0.13369501784225366 | validation: 0.15361688011653482]
	TIME [epoch: 8.15 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19949748607459425		[learning rate: 0.0026058]
		[batch 20/20] avg loss: 0.14386900324174173		[learning rate: 0.0026028]
	Learning Rate: 0.00260277
	LOSS [training: 0.171683244658168 | validation: 0.1506291750351585]
	TIME [epoch: 8.14 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1350868722205296		[learning rate: 0.0025997]
		[batch 20/20] avg loss: 0.1547995112842536		[learning rate: 0.0025966]
	Learning Rate: 0.00259663
	LOSS [training: 0.14494319175239162 | validation: 0.21582664859279338]
	TIME [epoch: 8.14 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14171473857245295		[learning rate: 0.0025936]
		[batch 20/20] avg loss: 0.13669140426627846		[learning rate: 0.0025905]
	Learning Rate: 0.00259051
	LOSS [training: 0.13920307141936566 | validation: 0.07536259338044157]
	TIME [epoch: 8.17 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13047069937073405		[learning rate: 0.0025874]
		[batch 20/20] avg loss: 0.1626710463837658		[learning rate: 0.0025844]
	Learning Rate: 0.0025844
	LOSS [training: 0.14657087287724996 | validation: 0.3781755380271195]
	TIME [epoch: 8.15 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16761750941071601		[learning rate: 0.0025813]
		[batch 20/20] avg loss: 0.13085939502592278		[learning rate: 0.0025783]
	Learning Rate: 0.0025783
	LOSS [training: 0.1492384522183194 | validation: 0.10697224711387146]
	TIME [epoch: 8.14 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12896422471056934		[learning rate: 0.0025753]
		[batch 20/20] avg loss: 0.15439856994168638		[learning rate: 0.0025722]
	Learning Rate: 0.00257222
	LOSS [training: 0.14168139732612786 | validation: 0.14193524361852827]
	TIME [epoch: 8.14 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17173284436314368		[learning rate: 0.0025692]
		[batch 20/20] avg loss: 0.12775196830991903		[learning rate: 0.0025662]
	Learning Rate: 0.00256615
	LOSS [training: 0.14974240633653135 | validation: 0.17750683007703888]
	TIME [epoch: 8.16 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19963254102655464		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.14621893240955397		[learning rate: 0.0025601]
	Learning Rate: 0.0025601
	LOSS [training: 0.17292573671805428 | validation: 0.14067644843974222]
	TIME [epoch: 8.14 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11817440671885635		[learning rate: 0.0025571]
		[batch 20/20] avg loss: 0.1356861786099629		[learning rate: 0.0025541]
	Learning Rate: 0.00255406
	LOSS [training: 0.12693029266440964 | validation: 0.16667391293779282]
	TIME [epoch: 8.14 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16655530999921203		[learning rate: 0.002551]
		[batch 20/20] avg loss: 0.13912485932389146		[learning rate: 0.002548]
	Learning Rate: 0.00254803
	LOSS [training: 0.15284008466155177 | validation: 0.1729105535932942]
	TIME [epoch: 8.14 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16162963322064075		[learning rate: 0.002545]
		[batch 20/20] avg loss: 0.14661261717229018		[learning rate: 0.002542]
	Learning Rate: 0.00254202
	LOSS [training: 0.15412112519646548 | validation: 0.09782765849263876]
	TIME [epoch: 8.16 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14880482442092516		[learning rate: 0.002539]
		[batch 20/20] avg loss: 0.16485457234672246		[learning rate: 0.002536]
	Learning Rate: 0.00253603
	LOSS [training: 0.15682969838382382 | validation: 0.21869456010558355]
	TIME [epoch: 8.15 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18240723686082633		[learning rate: 0.002533]
		[batch 20/20] avg loss: 0.1351472683566985		[learning rate: 0.00253]
	Learning Rate: 0.00253004
	LOSS [training: 0.1587772526087624 | validation: 0.2689566345044196]
	TIME [epoch: 8.14 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19346809280079671		[learning rate: 0.0025271]
		[batch 20/20] avg loss: 0.12745905492539097		[learning rate: 0.0025241]
	Learning Rate: 0.00252408
	LOSS [training: 0.16046357386309384 | validation: 0.16191647157309105]
	TIME [epoch: 8.14 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1595642739563715		[learning rate: 0.0025211]
		[batch 20/20] avg loss: 0.13207293889258825		[learning rate: 0.0025181]
	Learning Rate: 0.00251812
	LOSS [training: 0.14581860642447986 | validation: 0.11711045984413156]
	TIME [epoch: 8.16 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13709456417265636		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.21733418817318015		[learning rate: 0.0025122]
	Learning Rate: 0.00251218
	LOSS [training: 0.17721437617291827 | validation: 0.15698209358503037]
	TIME [epoch: 8.15 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15115093470367308		[learning rate: 0.0025092]
		[batch 20/20] avg loss: 0.13323243441484325		[learning rate: 0.0025063]
	Learning Rate: 0.00250626
	LOSS [training: 0.14219168455925818 | validation: 0.1584811988691343]
	TIME [epoch: 8.14 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16682187067274934		[learning rate: 0.0025033]
		[batch 20/20] avg loss: 0.17087952937144252		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.16885070002209596 | validation: 0.11400498313206325]
	TIME [epoch: 8.14 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15667205881341012		[learning rate: 0.0024974]
		[batch 20/20] avg loss: 0.11914210260088601		[learning rate: 0.0024944]
	Learning Rate: 0.00249445
	LOSS [training: 0.13790708070714808 | validation: 0.1162859702692513]
	TIME [epoch: 8.16 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16941320253100747		[learning rate: 0.0024915]
		[batch 20/20] avg loss: 0.1388760210838486		[learning rate: 0.0024886]
	Learning Rate: 0.00248856
	LOSS [training: 0.15414461180742803 | validation: 0.12517695063594952]
	TIME [epoch: 8.15 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1355774751809097		[learning rate: 0.0024856]
		[batch 20/20] avg loss: 0.12573412979477291		[learning rate: 0.0024827]
	Learning Rate: 0.00248269
	LOSS [training: 0.1306558024878413 | validation: 0.09853311374108795]
	TIME [epoch: 8.14 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16764050601782574		[learning rate: 0.0024798]
		[batch 20/20] avg loss: 0.18648156878189354		[learning rate: 0.0024768]
	Learning Rate: 0.00247684
	LOSS [training: 0.17706103739985965 | validation: 0.11803751305773853]
	TIME [epoch: 8.13 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12266276583454783		[learning rate: 0.0024739]
		[batch 20/20] avg loss: 0.1500621091202845		[learning rate: 0.002471]
	Learning Rate: 0.00247099
	LOSS [training: 0.13636243747741614 | validation: 0.2445860909753176]
	TIME [epoch: 8.15 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15255171246006932		[learning rate: 0.0024681]
		[batch 20/20] avg loss: 0.12078902783315315		[learning rate: 0.0024652]
	Learning Rate: 0.00246517
	LOSS [training: 0.13667037014661126 | validation: 0.1450977930672766]
	TIME [epoch: 8.15 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1585258538421599		[learning rate: 0.0024623]
		[batch 20/20] avg loss: 0.1469558772585895		[learning rate: 0.0024594]
	Learning Rate: 0.00245935
	LOSS [training: 0.15274086555037467 | validation: 0.12692456723426232]
	TIME [epoch: 8.13 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1726335340548221		[learning rate: 0.0024564]
		[batch 20/20] avg loss: 0.15262597319129423		[learning rate: 0.0024535]
	Learning Rate: 0.00245355
	LOSS [training: 0.16262975362305818 | validation: 0.25853362626437626]
	TIME [epoch: 8.13 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1876111893204372		[learning rate: 0.0024507]
		[batch 20/20] avg loss: 0.13468530366261405		[learning rate: 0.0024478]
	Learning Rate: 0.00244776
	LOSS [training: 0.16114824649152565 | validation: 0.1410748069156329]
	TIME [epoch: 8.14 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13417220999129403		[learning rate: 0.0024449]
		[batch 20/20] avg loss: 0.12824605853586518		[learning rate: 0.002442]
	Learning Rate: 0.00244199
	LOSS [training: 0.13120913426357955 | validation: 0.11845573130064868]
	TIME [epoch: 8.15 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14564680808086394		[learning rate: 0.0024391]
		[batch 20/20] avg loss: 0.10529386600761503		[learning rate: 0.0024362]
	Learning Rate: 0.00243623
	LOSS [training: 0.12547033704423946 | validation: 0.12478114396705589]
	TIME [epoch: 8.14 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13376304078402718		[learning rate: 0.0024334]
		[batch 20/20] avg loss: 0.13111606104017354		[learning rate: 0.0024305]
	Learning Rate: 0.00243048
	LOSS [training: 0.13243955091210033 | validation: 0.11384135714218963]
	TIME [epoch: 8.14 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14796016011468222		[learning rate: 0.0024276]
		[batch 20/20] avg loss: 0.11396994044144233		[learning rate: 0.0024247]
	Learning Rate: 0.00242475
	LOSS [training: 0.13096505027806227 | validation: 0.1050437608546381]
	TIME [epoch: 8.15 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13571660466817564		[learning rate: 0.0024219]
		[batch 20/20] avg loss: 0.11828478370548157		[learning rate: 0.002419]
	Learning Rate: 0.00241903
	LOSS [training: 0.12700069418682863 | validation: 0.12826510892750106]
	TIME [epoch: 8.16 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12252512548099789		[learning rate: 0.0024162]
		[batch 20/20] avg loss: 0.15117264939685224		[learning rate: 0.0024133]
	Learning Rate: 0.00241332
	LOSS [training: 0.1368488874389251 | validation: 0.30560992456510966]
	TIME [epoch: 8.15 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14944144623130737		[learning rate: 0.0024105]
		[batch 20/20] avg loss: 0.1347407513108943		[learning rate: 0.0024076]
	Learning Rate: 0.00240763
	LOSS [training: 0.14209109877110085 | validation: 0.21221722051255804]
	TIME [epoch: 8.14 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15672248250583004		[learning rate: 0.0024048]
		[batch 20/20] avg loss: 0.14120019100646375		[learning rate: 0.002402]
	Learning Rate: 0.00240195
	LOSS [training: 0.1489613367561469 | validation: 0.20253545640973755]
	TIME [epoch: 8.15 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15220568762188177		[learning rate: 0.0023991]
		[batch 20/20] avg loss: 0.12236013068665152		[learning rate: 0.0023963]
	Learning Rate: 0.00239628
	LOSS [training: 0.13728290915426666 | validation: 0.19656450145382215]
	TIME [epoch: 8.18 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17838987419895241		[learning rate: 0.0023935]
		[batch 20/20] avg loss: 0.1655114078905374		[learning rate: 0.0023906]
	Learning Rate: 0.00239063
	LOSS [training: 0.17195064104474486 | validation: 0.16347327313239926]
	TIME [epoch: 8.14 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13022614603490998		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.13976583665191097		[learning rate: 0.002385]
	Learning Rate: 0.00238499
	LOSS [training: 0.1349959913434105 | validation: 0.20120920825082608]
	TIME [epoch: 8.15 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13857449038111874		[learning rate: 0.0023822]
		[batch 20/20] avg loss: 0.13423050260462246		[learning rate: 0.0023794]
	Learning Rate: 0.00237937
	LOSS [training: 0.1364024964928706 | validation: 0.10182086142103888]
	TIME [epoch: 8.15 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1904124540002979		[learning rate: 0.0023766]
		[batch 20/20] avg loss: 0.12192289221596139		[learning rate: 0.0023738]
	Learning Rate: 0.00237375
	LOSS [training: 0.15616767310812965 | validation: 0.12034007920444753]
	TIME [epoch: 8.17 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14170445669762713		[learning rate: 0.002371]
		[batch 20/20] avg loss: 0.18747066146843705		[learning rate: 0.0023682]
	Learning Rate: 0.00236816
	LOSS [training: 0.1645875590830321 | validation: 0.1505588799819062]
	TIME [epoch: 8.15 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15107999135282885		[learning rate: 0.0023654]
		[batch 20/20] avg loss: 0.15414251390280398		[learning rate: 0.0023626]
	Learning Rate: 0.00236257
	LOSS [training: 0.1526112526278164 | validation: 0.14970888963129264]
	TIME [epoch: 8.14 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.164769644685948		[learning rate: 0.0023598]
		[batch 20/20] avg loss: 0.11949265192545788		[learning rate: 0.002357]
	Learning Rate: 0.002357
	LOSS [training: 0.14213114830570295 | validation: 0.08107131345270813]
	TIME [epoch: 8.15 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12272070817662388		[learning rate: 0.0023542]
		[batch 20/20] avg loss: 0.1499060332342158		[learning rate: 0.0023514]
	Learning Rate: 0.00235144
	LOSS [training: 0.13631337070541982 | validation: 0.11018730856477829]
	TIME [epoch: 8.17 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12099180099733645		[learning rate: 0.0023487]
		[batch 20/20] avg loss: 0.13815019197571415		[learning rate: 0.0023459]
	Learning Rate: 0.00234589
	LOSS [training: 0.12957099648652526 | validation: 0.14862569649443869]
	TIME [epoch: 8.14 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19422335177236522		[learning rate: 0.0023431]
		[batch 20/20] avg loss: 0.10828401221924264		[learning rate: 0.0023404]
	Learning Rate: 0.00234036
	LOSS [training: 0.1512536819958039 | validation: 0.11036399371553304]
	TIME [epoch: 8.14 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14594035630343122		[learning rate: 0.0023376]
		[batch 20/20] avg loss: 0.11868781395845365		[learning rate: 0.0023348]
	Learning Rate: 0.00233484
	LOSS [training: 0.13231408513094242 | validation: 0.11582447221479084]
	TIME [epoch: 8.15 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11588327813131441		[learning rate: 0.0023321]
		[batch 20/20] avg loss: 0.12015597601426913		[learning rate: 0.0023293]
	Learning Rate: 0.00232933
	LOSS [training: 0.11801962707279175 | validation: 0.17404021679663198]
	TIME [epoch: 8.17 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1917347062283121		[learning rate: 0.0023266]
		[batch 20/20] avg loss: 3.6111511908487115		[learning rate: 0.0023238]
ERROR:
nan encountered in epoch 667 (validation loss).
