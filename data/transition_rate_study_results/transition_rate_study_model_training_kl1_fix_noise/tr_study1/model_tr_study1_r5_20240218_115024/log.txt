Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r5', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2433834076

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 10.677194809590812		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.681630412515753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.179412611053282 | validation: 8.334785789516408]
	TIME [epoch: 70.1 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 7.560458133334274		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.123402321446084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.341930227390177 | validation: 6.607649493871003]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.5945755158096535		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.861947664767049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.728261590288352 | validation: 9.069776299041663]
	TIME [epoch: 8.8 sec]
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 8.266951893056802		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.104469004724711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.6857104488907595 | validation: 6.57388129980425]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.932964196571469		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.336502828142508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.634733512356988 | validation: 5.775101743257785]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.282776466829256		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.002493704070785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.642635085450019 | validation: 5.257096890771439]
	TIME [epoch: 8.83 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_6.pth
	Model improved!!!
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.0688101570861885		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.495351631701522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2820808943938555 | validation: 5.5222612697585225]
	TIME [epoch: 8.81 sec]
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.0808814360273775		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.257612762661806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.169247099344593 | validation: 4.7628991368505105]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_8.pth
	Model improved!!!
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.356201185530333		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.007039805617924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.181620495574128 | validation: 4.604111119953821]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.17123677264482		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.622501356332262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.8968690644885395 | validation: 4.741322358057913]
	TIME [epoch: 8.8 sec]
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.703222269937934		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.944381526317939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.823801898127936 | validation: 4.9057585050358625]
	TIME [epoch: 8.81 sec]
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.817226361193375		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.720035223557399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.768630792375386 | validation: 4.897376593514724]
	TIME [epoch: 8.8 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.034039084794749		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.553399312176007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.293719198485377 | validation: 7.717244384136908]
	TIME [epoch: 8.79 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.353881874886386		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.646959176937616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.000420525912 | validation: 5.125318839099654]
	TIME [epoch: 8.79 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.362571644975857		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.034578520806358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.698575082891106 | validation: 5.057294477317752]
	TIME [epoch: 8.79 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.46552037274737		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.531115320181201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.498317846464285 | validation: 4.674165766314141]
	TIME [epoch: 8.8 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.342894711007586		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.362142422556263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.352518566781924 | validation: 4.177654392445638]
	TIME [epoch: 8.82 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.467080905672028		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.157055547278762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.312068226475395 | validation: 4.259316829300355]
	TIME [epoch: 8.79 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.2336590254118445		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.277931725997884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.255795375704864 | validation: 4.259370206059536]
	TIME [epoch: 8.79 sec]
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.104679405789244		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.453814652627697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.279247029208472 | validation: 4.46902119305038]
	TIME [epoch: 8.79 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.110479706888443		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.114112042772953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.112295874830698 | validation: 4.722235747463663]
	TIME [epoch: 8.79 sec]
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.06792199165872		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.178767492191179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.12334474192495 | validation: 4.212463910651383]
	TIME [epoch: 8.8 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.10522778855924		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.1157721121228334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.110499950341036 | validation: 4.442154334377197]
	TIME [epoch: 8.78 sec]
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.027626963098093		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.138337060036535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.082982011567314 | validation: 5.328844705290573]
	TIME [epoch: 8.79 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.226191897939521		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.933395263463154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.079793580701337 | validation: 3.8638991356004917]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.9020080320558685		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.785082455832389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.843545243944129 | validation: 5.52571785527512]
	TIME [epoch: 8.78 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.206546393332713		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.75778766757125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.982167030451981 | validation: 4.058010976977391]
	TIME [epoch: 8.81 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.973310978564893		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.627616700236946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.800463839400919 | validation: 3.861175699819585]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_28.pth
	Model improved!!!
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.7721086972430955		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.520602471781201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.646355584512148 | validation: 4.1099200757026955]
	TIME [epoch: 8.77 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.561296539672966		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.5444096993307825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.552853119501875 | validation: 3.730838225675444]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_30.pth
	Model improved!!!
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.359469268343384		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.574027462096641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.466748365220012 | validation: 3.6576882445064447]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_31.pth
	Model improved!!!
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.4394503966546335		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.194414863774069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.316932630214352 | validation: 3.594638538714583]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_32.pth
	Model improved!!!
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.238461803030578		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.187406164911012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.212933983970796 | validation: 3.249933527552858]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_33.pth
	Model improved!!!
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.368414706370643		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.8801001670041217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124257436687381 | validation: 3.204565752104556]
	TIME [epoch: 8.8 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_34.pth
	Model improved!!!
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.8032142683286096		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.158794284088889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9810042762087496 | validation: 3.4146081205576992]
	TIME [epoch: 8.79 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.794784460978653		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.639693094387101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7172387776828772 | validation: 3.2245037704059163]
	TIME [epoch: 8.79 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.6461010280496025		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7376546835076665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6918778557786345 | validation: 2.773293056273475]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_37.pth
	Model improved!!!
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.505147738743876		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.411709091349839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.458428415046858 | validation: 3.4365161172274172]
	TIME [epoch: 8.79 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.604250040713461		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.947034197187385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2756421189504237 | validation: 2.2509426893475246]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_39.pth
	Model improved!!!
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.279823649611441		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5965506080615448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9381871288364931 | validation: 1.1302446162785]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_40.pth
	Model improved!!!
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.407006333303046		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3230772825978123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3650418079504294 | validation: 1.7523204357558753]
	TIME [epoch: 8.78 sec]
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.407601221832795		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4373933172877975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4224972695602962 | validation: 1.534585556014889]
	TIME [epoch: 8.8 sec]
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.409421856880295		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1852177961226507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2973198265014727 | validation: 1.1254893982373155]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_43.pth
	Model improved!!!
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2796338949813464		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.340901656622322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3102677758018344 | validation: 1.0616039433922357]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6281585301727244		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2094677570531482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4188131436129363 | validation: 1.5540197647661607]
	TIME [epoch: 8.79 sec]
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2298248370566327		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.161024475383242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1954246562199373 | validation: 1.0225476935702131]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_46.pth
	Model improved!!!
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3250761900804928		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0363988214031603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1807375057418266 | validation: 0.8304494347428198]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1917816638224892		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.335508010645434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2636448372339617 | validation: 0.8261665140396014]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_48.pth
	Model improved!!!
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.118633030513491		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0764922810024689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0975626557579798 | validation: 1.1124681370875318]
	TIME [epoch: 8.78 sec]
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.102944005218587		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1040427499279373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1034933775732623 | validation: 0.949719599156671]
	TIME [epoch: 8.78 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1754212888992748		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 1.1170025319736467		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.146211910436461 | validation: 0.9761494748212678]
	TIME [epoch: 8.78 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9979405530448234		[learning rate: 0.00993]
		[batch 20/20] avg loss: 1.143771193771873		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 1.0708558734083484 | validation: 0.8989917672501563]
	TIME [epoch: 8.8 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0942174939141174		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 0.977977810222956		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 1.0360976520685368 | validation: 1.2872771076876122]
	TIME [epoch: 8.78 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.092010540513548		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 0.9679754898402935		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 1.0299930151769208 | validation: 0.7481827830480965]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_54.pth
	Model improved!!!
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9480003023680762		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 1.3932232673690716		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 1.170611784868574 | validation: 1.3986439591408923]
	TIME [epoch: 8.78 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.303225003010609		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 1.1394509239200938		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 1.2213379634653514 | validation: 1.651187233665958]
	TIME [epoch: 8.77 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2755660403044842		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 1.0152279639267996		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 1.145397002115642 | validation: 0.5528060048497455]
	TIME [epoch: 8.81 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_57.pth
	Model improved!!!
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0748865050749896		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 0.8557661696200307		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 0.96532633734751 | validation: 0.6667401093006996]
	TIME [epoch: 8.79 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9288173763781694		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 0.9857442798162405		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 0.9572808280972052 | validation: 0.7648934864359791]
	TIME [epoch: 8.78 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6952477661409032		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 0.8933690852698764		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 0.7943084257053898 | validation: 0.5994252401067502]
	TIME [epoch: 8.79 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7552667114496374		[learning rate: 0.009506]
		[batch 20/20] avg loss: 0.9241400161128446		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 0.8397033637812411 | validation: 0.6192858645924405]
	TIME [epoch: 8.77 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9037266340683839		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 0.8998868912057422		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 0.9018067626370627 | validation: 0.8476258241967328]
	TIME [epoch: 8.8 sec]
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7385314510300729		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 0.8985104196486542		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 0.8185209353393637 | validation: 0.7152285129389244]
	TIME [epoch: 8.78 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9072471459069484		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 0.7819714864042095		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 0.8446093161555794 | validation: 1.1641651116495642]
	TIME [epoch: 8.78 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9592102917724594		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 0.8202467422144311		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 0.8897285169934452 | validation: 0.4037755814688919]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_65.pth
	Model improved!!!
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0730578248355416		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 0.749414113505986		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 0.9112359691707634 | validation: 1.1563036030015348]
	TIME [epoch: 8.78 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.972215415172219		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 0.6209933740064162		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 0.7966043945893174 | validation: 0.44965510191058883]
	TIME [epoch: 8.79 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8040027667108676		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 0.726511508661197		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 0.7652571376860322 | validation: 0.9380021016176168]
	TIME [epoch: 8.79 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7657955185531586		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 0.7114856262878744		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 0.7386405724205163 | validation: 0.6113329454067269]
	TIME [epoch: 8.78 sec]
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7250780143636619		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 0.6514043182808071		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.6882411663222345 | validation: 0.8670616160857578]
	TIME [epoch: 8.78 sec]
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7983044783974613		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 0.8691091454700915		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 0.8337068119337765 | validation: 0.5399258203660741]
	TIME [epoch: 8.79 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7419322560633818		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 0.7036491725440015		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 0.7227907143036916 | validation: 1.4196994718859952]
	TIME [epoch: 8.79 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8807290789599884		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 0.6838217439956065		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 0.7822754114777976 | validation: 1.1288756328590777]
	TIME [epoch: 8.79 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6986483896695106		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 0.8078981360727482		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 0.7532732628711293 | validation: 0.5795071498427525]
	TIME [epoch: 8.78 sec]
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8017938354278511		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 0.6545049076525282		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 0.7281493715401897 | validation: 0.7120572798886446]
	TIME [epoch: 8.77 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6506312587152816		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 0.5741304466429566		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 0.6123808526791192 | validation: 0.43546648962612017]
	TIME [epoch: 8.77 sec]
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7668242777352137		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 0.7813261062025039		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 0.7740751919688587 | validation: 0.7218318050688377]
	TIME [epoch: 8.77 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6238157889667066		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 0.828631041568584		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 0.7262234152676452 | validation: 0.7224405799009697]
	TIME [epoch: 8.8 sec]
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8959454326399353		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 0.6177543909354323		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 0.7568499117876838 | validation: 1.0884257760880287]
	TIME [epoch: 8.78 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7001337180787012		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.5437294135109962		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 0.6219315657948488 | validation: 0.5561154636242198]
	TIME [epoch: 8.77 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.734253402734093		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 0.9442747344550579		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 0.8392640685945756 | validation: 0.45878671799355836]
	TIME [epoch: 8.78 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6341678764888117		[learning rate: 0.008586]
		[batch 20/20] avg loss: 0.6332809533466521		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 0.6337244149177319 | validation: 0.3149143933328895]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_82.pth
	Model improved!!!
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5709691867265508		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 0.5914863041702035		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 0.5812277454483771 | validation: 0.6997302189440898]
	TIME [epoch: 8.79 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6097011235523373		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 0.7360696067596281		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 0.6728853651559829 | validation: 0.44406205643626073]
	TIME [epoch: 8.78 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7120702311777832		[learning rate: 0.008462]
		[batch 20/20] avg loss: 0.5521359555063236		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 0.6321030933420533 | validation: 0.264859012001963]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_85.pth
	Model improved!!!
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5179153426838574		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 0.5965250243358515		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 0.5572201835098547 | validation: 0.45399090262400355]
	TIME [epoch: 8.78 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6229491716438383		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 0.5393925803293378		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 0.5811708759865881 | validation: 0.6685478877817066]
	TIME [epoch: 8.77 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6061256524578968		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 0.5812588917291862		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 0.5936922720935415 | validation: 0.4744279756322057]
	TIME [epoch: 8.79 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5708896078872596		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 0.7669906552265806		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.6689401315569199 | validation: 1.330655164138584]
	TIME [epoch: 8.78 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5931310242230567		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 0.5602042299433159		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 0.5766676270831862 | validation: 0.7248929459394989]
	TIME [epoch: 8.77 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5516124096807773		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 0.5472204524860917		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 0.5494164310834345 | validation: 0.4252879025012616]
	TIME [epoch: 8.78 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5418881258792253		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 0.8683385293869378		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 0.7051133276330817 | validation: 1.1335498219414875]
	TIME [epoch: 8.77 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7156515449228597		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 0.4771311682355585		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 0.596391356579209 | validation: 0.4465031842237162]
	TIME [epoch: 8.79 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49438350458910973		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 0.6189258561557283		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 0.5566546803724189 | validation: 0.38517727550302544]
	TIME [epoch: 8.78 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4649733591570798		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 0.48485918049464427		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 0.474916269825862 | validation: 0.4768455892283034]
	TIME [epoch: 8.76 sec]
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6554007076144976		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 0.5545735210555816		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 0.6049871143350395 | validation: 0.38732675463984373]
	TIME [epoch: 8.77 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5650026300339259		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 0.4807730329116743		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 0.5228878314728 | validation: 0.39741020602045574]
	TIME [epoch: 8.75 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6169860289567941		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 0.512569450419452		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 0.564777739688123 | validation: 0.8231138873718952]
	TIME [epoch: 8.78 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5986830372321472		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.6376783993227786		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 0.618180718277463 | validation: 0.8545657308111136]
	TIME [epoch: 8.79 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5108275103617579		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 0.5061755242190857		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 0.5085015172904217 | validation: 0.610067041560549]
	TIME [epoch: 8.76 sec]
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5517265303434538		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 0.6780155188214699		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 0.614871024582462 | validation: 1.1298245013519421]
	TIME [epoch: 8.77 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6204904207547874		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 0.46823679471472557		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 0.5443636077347566 | validation: 0.43099477553190224]
	TIME [epoch: 8.76 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6391025626921791		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 0.5569495134184994		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 0.5980260380553392 | validation: 0.7289875309937024]
	TIME [epoch: 8.77 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6166061946558072		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 0.5636679208724212		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 0.5901370577641143 | validation: 0.2697377314039352]
	TIME [epoch: 8.78 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7492541031389741		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 0.5793557713516354		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 0.6643049372453048 | validation: 0.5883328430371384]
	TIME [epoch: 8.76 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5805467254287869		[learning rate: 0.007643]
		[batch 20/20] avg loss: 0.5039909869899907		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 0.5422688562093888 | validation: 0.3359938916014028]
	TIME [epoch: 8.77 sec]
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5050525182486203		[learning rate: 0.007606]
		[batch 20/20] avg loss: 0.8052270533526646		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 0.6551397858006425 | validation: 0.5062715263129905]
	TIME [epoch: 8.76 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.623705631336871		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 0.3756441539474082		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.4996748926421395 | validation: 0.6907171284268198]
	TIME [epoch: 8.76 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5807237342551168		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 0.4483794342683437		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 0.5145515842617303 | validation: 0.6490056468154297]
	TIME [epoch: 8.79 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4642079469590398		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 0.7015062773757251		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 0.5828571121673825 | validation: 0.32448417679866043]
	TIME [epoch: 8.77 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5497753092631587		[learning rate: 0.00746]
		[batch 20/20] avg loss: 0.46595775453850896		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 0.5078665319008339 | validation: 0.36557243788613975]
	TIME [epoch: 8.77 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6073183155068576		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 0.551392886848366		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 0.5793556011776119 | validation: 0.398305542354087]
	TIME [epoch: 8.77 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5442807459162633		[learning rate: 0.007388]
		[batch 20/20] avg loss: 0.5633160731306016		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 0.5537984095234325 | validation: 1.0324866262037737]
	TIME [epoch: 8.77 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5351403393521825		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 0.6284539161965532		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 0.581797127774368 | validation: 1.1282584656964707]
	TIME [epoch: 8.79 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7848237107531993		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 0.8003690018290828		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 0.7925963562911412 | validation: 0.6399064686095564]
	TIME [epoch: 8.78 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5153376639600695		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 0.561197641470927		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 0.5382676527154983 | validation: 0.32506461795261843]
	TIME [epoch: 8.77 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6010330719205256		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 0.4644377613045747		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 0.5327354166125502 | validation: 0.3432580776365819]
	TIME [epoch: 8.77 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7919849591507451		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.5196687714463075		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 0.6558268652985263 | validation: 0.45150724876164516]
	TIME [epoch: 8.77 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5634954422027049		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 0.5573259553628767		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 0.5604106987827908 | validation: 0.35853184712735325]
	TIME [epoch: 8.79 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4387797932630435		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 0.47568277680664145		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 0.45723128503484256 | validation: 0.46951094614961575]
	TIME [epoch: 8.77 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.504193373832915		[learning rate: 0.007107]
		[batch 20/20] avg loss: 0.45775543331074015		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 0.48097440357182764 | validation: 0.3063388250599345]
	TIME [epoch: 8.76 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5003923927896393		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 0.48178212403821224		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 0.4910872584139259 | validation: 0.5074981672179356]
	TIME [epoch: 8.77 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5460724096968236		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 0.4202677955063624		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 0.48317010260159304 | validation: 1.1370495963203873]
	TIME [epoch: 8.78 sec]
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5722151844815834		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 0.4722338816578131		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 0.5222245330696983 | validation: 0.4299763075066152]
	TIME [epoch: 8.78 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4572135996268326		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 0.5170277789515284		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 0.4871206892891805 | validation: 0.5889345058244706]
	TIME [epoch: 8.81 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5664558050926088		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 0.6220178259324369		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 0.5942368155125229 | validation: 0.5310485741690654]
	TIME [epoch: 8.79 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6109467022388967		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 0.47786407017943666		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.5444053862091668 | validation: 0.4292939860094638]
	TIME [epoch: 8.76 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5186851392811727		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 0.5885552494370918		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 0.5536201943591323 | validation: 0.43863975480754414]
	TIME [epoch: 8.78 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5011178945834673		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 0.43417272768115256		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 0.46764531113230995 | validation: 0.3660390411407332]
	TIME [epoch: 8.79 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5017732918814816		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 0.5043538722858486		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 0.503063582083665 | validation: 0.8558766310200145]
	TIME [epoch: 8.8 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5116938922180468		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 0.41647535305869693		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 0.4640846226383718 | validation: 0.21724401657856893]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_131.pth
	Model improved!!!
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4800632887542635		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 0.5133241771987005		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 0.49669373297648195 | validation: 0.27488745035389667]
	TIME [epoch: 8.78 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.570803735985023		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 0.5323804697854254		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 0.5515921028852242 | validation: 0.26265291879578767]
	TIME [epoch: 8.77 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4651919192446964		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 0.4934408753442817		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 0.47931639729448905 | validation: 0.7110280420181827]
	TIME [epoch: 8.78 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5034987607628864		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 0.4695365522207685		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 0.4865176564918275 | validation: 0.5242883529484933]
	TIME [epoch: 8.8 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8642244145641911		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 0.582407390731146		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 0.7233159026476684 | validation: 0.4095121861896973]
	TIME [epoch: 8.77 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3805048734390032		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.6220556055232715		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 0.5012802394811373 | validation: 0.39082034400096954]
	TIME [epoch: 8.78 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5156236507930537		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 0.4860905249317248		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 0.5008570878623894 | validation: 1.4641328804047662]
	TIME [epoch: 8.78 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5891340986681668		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 0.3814096928722822		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 0.48527189577022456 | validation: 0.18772286893634021]
	TIME [epoch: 8.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_139.pth
	Model improved!!!
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5043854960206364		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 0.41874165738022573		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 0.461563576700431 | validation: 0.47114191189246774]
	TIME [epoch: 8.81 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4320912040745675		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 0.4674274812770881		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 0.4497593426758278 | validation: 0.46201673197436444]
	TIME [epoch: 8.77 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48084720368141315		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 0.374095145112591		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 0.42747117439700205 | validation: 0.6330632135870786]
	TIME [epoch: 8.78 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4480335612093004		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 0.4313108364218029		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 0.4396721988155517 | validation: 0.5768160768452872]
	TIME [epoch: 8.77 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43128940415628464		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 0.4186450639674377		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 0.4249672340618611 | validation: 0.20024420736980733]
	TIME [epoch: 8.78 sec]
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44023520199006666		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.3622968067391626		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 0.40126600436461474 | validation: 0.46894263450557505]
	TIME [epoch: 8.8 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3498537436812765		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.3969940864345921		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.37342391505793426 | validation: 0.2774156537024306]
	TIME [epoch: 8.77 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4640228922395405		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 0.43459617086940094		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 0.4493095315544708 | validation: 0.549354171646913]
	TIME [epoch: 8.77 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4592549189575985		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 0.380954075077705		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 0.42010449701765185 | validation: 0.31977653552722796]
	TIME [epoch: 8.78 sec]
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49249150903345973		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.5685113004696476		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 0.5305014047515536 | validation: 0.41692644450599947]
	TIME [epoch: 8.78 sec]
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4266749098373047		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.5469674391479671		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.48682117449263596 | validation: 0.2907099911930057]
	TIME [epoch: 8.8 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48296799421642367		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 0.5559501649652083		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 0.519459079590816 | validation: 0.3660701014099043]
	TIME [epoch: 8.78 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39675342554339205		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 0.37898173220838405		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 0.38786757887588796 | validation: 0.43834574775972435]
	TIME [epoch: 8.79 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40049987574147694		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 0.3866098944712436		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 0.3935548851063603 | validation: 0.21607656722928828]
	TIME [epoch: 8.79 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3586826035366218		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 0.33618524981055414		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 0.34743392667358797 | validation: 0.39372670529415027]
	TIME [epoch: 8.78 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43626565117598626		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 0.4210096832627892		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 0.4286376672193878 | validation: 0.18494884440104434]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_155.pth
	Model improved!!!
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3331974250400212		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.4411998527076362		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 0.38719863887382866 | validation: 0.34006982965056887]
	TIME [epoch: 8.81 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35548212015639535		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 0.4380134815167781		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 0.39674780083658673 | validation: 0.43960435770464573]
	TIME [epoch: 8.77 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4240332818817333		[learning rate: 0.00594]
		[batch 20/20] avg loss: 0.3955370810846102		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 0.4097851814831718 | validation: 0.759979129552345]
	TIME [epoch: 8.77 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46166681300165485		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 0.4130330950135323		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 0.4373499540075936 | validation: 0.17316913822759786]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_159.pth
	Model improved!!!
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4041770872873346		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 0.6086176614387822		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 0.5063973743630583 | validation: 0.2628450342526181]
	TIME [epoch: 8.78 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38413232286746457		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.39865852205370933		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.391395422460587 | validation: 0.6187162349315908]
	TIME [epoch: 8.8 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4599865968415635		[learning rate: 0.005826]
		[batch 20/20] avg loss: 0.46207406999425277		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 0.4610303334179081 | validation: 0.35110605013801877]
	TIME [epoch: 8.78 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37418308247619303		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 0.4195033067150911		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 0.396843194595642 | validation: 0.2883271463392499]
	TIME [epoch: 8.77 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3128263447283212		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 0.37413098202929856		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 0.34347866337880995 | validation: 0.22403054996206304]
	TIME [epoch: 8.78 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.48009939223743237		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 0.33290514918066694		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.40650227070904965 | validation: 0.2812779283324643]
	TIME [epoch: 8.77 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36855260655438393		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 0.42430793385939697		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 0.39643027020689037 | validation: 1.0994124367614375]
	TIME [epoch: 8.8 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42717246367845585		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 0.47414959271139623		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 0.450661028194926 | validation: 0.44859527281901984]
	TIME [epoch: 8.79 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44450888401360994		[learning rate: 0.005659]
		[batch 20/20] avg loss: 0.4295259161057565		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 0.43701740005968326 | validation: 0.47768138229961254]
	TIME [epoch: 8.78 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.493215519489009		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 0.4514420838934877		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.4723288016912484 | validation: 0.17791873856758475]
	TIME [epoch: 8.78 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.43695204547811883		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 0.3742221763623571		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 0.405587110920238 | validation: 0.5110067817913415]
	TIME [epoch: 8.77 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34259968145217823		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 0.37922451422818054		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 0.36091209784017936 | validation: 0.22823508506630738]
	TIME [epoch: 8.8 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4818875554322184		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 0.39863485690263		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 0.4402612061674242 | validation: 0.40426784804951943]
	TIME [epoch: 8.77 sec]
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42989102957082503		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 0.34933976440214776		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 0.3896153969864865 | validation: 0.399684955985806]
	TIME [epoch: 8.77 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4445394514179372		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 0.3977615788947336		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 0.4211505151563354 | validation: 0.6365641919027867]
	TIME [epoch: 8.78 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35181787624248606		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.448698702703635		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.4002582894730605 | validation: 0.414826369233362]
	TIME [epoch: 8.77 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38120167185755843		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.34338781305580063		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.3622947424566795 | validation: 0.4286800033455738]
	TIME [epoch: 8.8 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.35738607966359925		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.4297113256705806		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.39354870266708997 | validation: 0.24917311398561148]
	TIME [epoch: 8.77 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3876404203526784		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 0.2919321155973805		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.33978626797502953 | validation: 0.2969751160388768]
	TIME [epoch: 8.77 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27521412503594844		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.3463202665970776		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 0.310767195816513 | validation: 0.2016043718862165]
	TIME [epoch: 8.77 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3644977096187932		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 0.39155101262472797		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 0.3780243611217605 | validation: 0.6124379223290382]
	TIME [epoch: 8.78 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5917211514057914		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.35578006567441245		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.4737506085401019 | validation: 0.4921929773529493]
	TIME [epoch: 8.81 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34174173426127596		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 0.3788411226865331		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 0.36029142847390455 | validation: 0.6547888286042061]
	TIME [epoch: 8.78 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44317538205449925		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.3599693431005582		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.4015723625775288 | validation: 0.47476149498784803]
	TIME [epoch: 8.77 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.440541406456335		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 0.33250466709354204		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.3865230367749385 | validation: 0.25961747830933707]
	TIME [epoch: 8.78 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3062560292078117		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 0.3609723453065373		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 0.3336141872571745 | validation: 0.16455599576202956]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_185.pth
	Model improved!!!
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3695900451032016		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 0.3060359951383302		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 0.3378130201207658 | validation: 0.3471585979115322]
	TIME [epoch: 8.77 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39826315639123966		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.41413336326647243		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 0.40619825982885605 | validation: 0.5242677074249096]
	TIME [epoch: 8.79 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.338332327090487		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.34874747880693		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 0.3435399029487085 | validation: 0.22438994523192626]
	TIME [epoch: 8.76 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39481918056550275		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.2927759366356023		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.34379755860055244 | validation: 0.18457432132958818]
	TIME [epoch: 8.77 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.39683305783280465		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 0.3603848123572004		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 0.3786089350950026 | validation: 0.33204047477538]
	TIME [epoch: 8.77 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34069189396273497		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.355050605582591		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.3478712497726629 | validation: 0.546996645623031]
	TIME [epoch: 8.78 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.429901232297067		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 0.3927173056291015		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 0.4113092689630843 | validation: 0.2580250791605372]
	TIME [epoch: 8.79 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2972887361988589		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 0.30263445597347444		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 0.2999615960861667 | validation: 0.24223491017607757]
	TIME [epoch: 8.78 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4505754502337959		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.3268920375417815		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 0.38873374388778864 | validation: 0.44164808968179453]
	TIME [epoch: 8.78 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4389682308771007		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.38964889102815714		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.4143085609526289 | validation: 0.2986226646744429]
	TIME [epoch: 8.78 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3090246574510065		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.31043067700398697		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.3097276672274968 | validation: 0.3305420837043176]
	TIME [epoch: 8.78 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3783066066920645		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 0.31987211710874863		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 0.3490893619004065 | validation: 0.41650409937301597]
	TIME [epoch: 8.79 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3823722720153335		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.3338508928666749		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.3581115824410042 | validation: 0.28242390465820355]
	TIME [epoch: 8.77 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36424733847643853		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 0.3168172202633718		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.34053227936990516 | validation: 0.307292002497905]
	TIME [epoch: 8.76 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26293853031000947		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 0.32527154154761345		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 0.2941050359288115 | validation: 0.22236552717839803]
	TIME [epoch: 8.78 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3148439681019345		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.4157309384131729		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.36528745325755374 | validation: 0.3946905017453243]
	TIME [epoch: 8.77 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34078278547666113		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 0.30485860921702845		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 0.32282069734684476 | validation: 0.2256756874169888]
	TIME [epoch: 8.8 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31041522605312155		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.378269635152958		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.3443424306030397 | validation: 0.28546549971835355]
	TIME [epoch: 8.78 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4620289203063944		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.3543337079515302		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 0.4081813141289623 | validation: 0.19286536160078482]
	TIME [epoch: 8.79 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27548332292409883		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.3298498363607124		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.3026665796424056 | validation: 0.23857956677108236]
	TIME [epoch: 8.78 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2559387510682472		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.28576979914885997		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.2708542751085536 | validation: 0.3231244956099654]
	TIME [epoch: 8.78 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31309759259879805		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.2939276525568938		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.3035126225778459 | validation: 0.16622188852670053]
	TIME [epoch: 8.81 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28771944225548923		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 0.4026985701573107		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 0.3452090062064 | validation: 0.24945410347295693]
	TIME [epoch: 8.79 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36031631436693246		[learning rate: 0.004639]
		[batch 20/20] avg loss: 0.4016495441275348		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 0.38098292924723365 | validation: 0.2423714926165745]
	TIME [epoch: 8.78 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31534884835665367		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 0.28520458924531655		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 0.30027671880098517 | validation: 0.18276540988711937]
	TIME [epoch: 8.77 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29059543038753566		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.30691078934655375		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.2987531098670446 | validation: 0.1464010592192338]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_211.pth
	Model improved!!!
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31014613083802856		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.28493055806817297		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 0.2975383444531007 | validation: 0.30204128881047243]
	TIME [epoch: 8.8 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2993326959831537		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.31460486277759037		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.30696877938037204 | validation: 0.2622360715413785]
	TIME [epoch: 8.77 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33204933497425665		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 0.4635300116618225		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 0.3977896733180396 | validation: 0.5635445094899723]
	TIME [epoch: 8.77 sec]
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.366923211411047		[learning rate: 0.004506]
		[batch 20/20] avg loss: 0.29479640728074513		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 0.330859809345896 | validation: 0.23167243422311062]
	TIME [epoch: 8.78 sec]
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3269734703129982		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.3366600053632459		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.33181673783812193 | validation: 0.21481753746570642]
	TIME [epoch: 8.77 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3041679899021822		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 0.33429840928986326		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 0.3192331995960228 | validation: 0.14395195581403947]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_217.pth
	Model improved!!!
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2938389867102593		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 0.28447437639228007		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 0.28915668155126967 | validation: 0.3047462940234422]
	TIME [epoch: 8.79 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2921719670111462		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 0.3669366201729515		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 0.32955429359204885 | validation: 0.3792326734988373]
	TIME [epoch: 8.76 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29024022445804853		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 0.3745933961764313		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 0.3324168103172399 | validation: 0.3064886336423665]
	TIME [epoch: 8.77 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30083595077834663		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 0.34555293720150493		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 0.3231944439899258 | validation: 0.4820899561533726]
	TIME [epoch: 8.77 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31114287980872024		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 0.30357880306664325		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.3073608414376817 | validation: 0.20308713591489963]
	TIME [epoch: 8.77 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2609288122222152		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 0.3307143293670303		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 0.2958215707946228 | validation: 0.2702247158849051]
	TIME [epoch: 8.77 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31154686216062977		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 0.26312751608792867		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 0.2873371891242792 | validation: 0.27523279323910027]
	TIME [epoch: 8.78 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31118232253247086		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 0.29879152260337594		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.3049869225679233 | validation: 0.2536213190124441]
	TIME [epoch: 8.76 sec]
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29917472017249275		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.2909644492376704		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.2950695847050815 | validation: 0.10673736786618085]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_226.pth
	Model improved!!!
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27771654606960816		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 0.2959610775022335		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 0.28683881178592074 | validation: 0.5498744481169617]
	TIME [epoch: 8.77 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3380920955444613		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 0.28391271117174605		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 0.31100240335810364 | validation: 0.18659461111649442]
	TIME [epoch: 8.76 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2730010450566757		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 0.294293224993178		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 0.28364713502492683 | validation: 0.1733307680517806]
	TIME [epoch: 8.8 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2675264139354841		[learning rate: 0.00419]
		[batch 20/20] avg loss: 0.24662342322503067		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.2570749185802574 | validation: 0.17183677604573858]
	TIME [epoch: 8.77 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25596299518080134		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.342881844330298		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.2994224197555496 | validation: 0.14456993629214887]
	TIME [epoch: 8.77 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2895659668199199		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.35625307313257576		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.32290951997624784 | validation: 0.49336087317094174]
	TIME [epoch: 8.77 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2936178198436802		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.24494983728582737		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.26928382856475375 | validation: 0.34833760321295604]
	TIME [epoch: 8.77 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2961476621874489		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.26509876860110626		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.28062321539427765 | validation: 0.2118681278548793]
	TIME [epoch: 8.79 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3220940522061403		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.27482988232013206		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 0.2984619672631361 | validation: 0.2443257052520224]
	TIME [epoch: 8.77 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28699619673611987		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.3152282193513415		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.3011122080437306 | validation: 0.514161204162629]
	TIME [epoch: 8.77 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28248363991481507		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.2548840492009806		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.2686838445578979 | validation: 0.35125453269562534]
	TIME [epoch: 8.76 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3191283996807031		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 0.27445417664391464		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.29679128816230893 | validation: 0.29220208630412514]
	TIME [epoch: 8.76 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3023083410241386		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.24308689060546246		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.27269761581480056 | validation: 0.13288116621603765]
	TIME [epoch: 8.77 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2609440252186551		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.28402286176110636		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.27248344348988074 | validation: 0.21407095619663208]
	TIME [epoch: 8.79 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2625838693969915		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 0.3432819275362151		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.3029328984666033 | validation: 0.35494786577343695]
	TIME [epoch: 8.77 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25299179165084934		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.22631465312237214		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.23965322238661074 | validation: 0.26159064270714444]
	TIME [epoch: 8.77 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27757670899232995		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.26083066970350266		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.26920368934791633 | validation: 0.29597834594928357]
	TIME [epoch: 8.77 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3276131414152214		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.30082953657767836		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.31422133899644983 | validation: 0.1660927817915621]
	TIME [epoch: 8.77 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22715822407483127		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.2408194311038505		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.23398882758934086 | validation: 0.33855670400426874]
	TIME [epoch: 8.79 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24489817627925775		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.27089777815390004		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.25789797721657887 | validation: 0.14636311804889807]
	TIME [epoch: 8.77 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22639611162799783		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.2693294133617681		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.24786276249488298 | validation: 0.25459785566270193]
	TIME [epoch: 8.76 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2601524628143464		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.2765370146710425		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 0.2683447387426945 | validation: 0.17859165135617835]
	TIME [epoch: 8.77 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25492655265947745		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.2761145259789338		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.26552053931920555 | validation: 0.15541895121977403]
	TIME [epoch: 8.78 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21295423346231815		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.24143210841129897		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.22719317093680863 | validation: 0.2639782572235134]
	TIME [epoch: 8.79 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28023888149167375		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.24822122629409069		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.26423005389288223 | validation: 0.30151176876952596]
	TIME [epoch: 8.77 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2653598175160973		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 0.32612525399994274		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.29574253575802 | validation: 0.3373883227182826]
	TIME [epoch: 8.76 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2617143977366024		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.23350818814989557		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.24761129294324896 | validation: 0.27181940393148046]
	TIME [epoch: 8.77 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20543425972649754		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.2568416676880646		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.2311379637072811 | validation: 0.30144181337274084]
	TIME [epoch: 8.76 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22815063326461144		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.256347532352766		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.24224908280868868 | validation: 0.12309001302018553]
	TIME [epoch: 8.8 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25534666348950663		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.2201817945994975		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.23776422904450206 | validation: 0.3118488605518941]
	TIME [epoch: 8.79 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23903008162697362		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.24661603540637103		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.24282305851667235 | validation: 0.3864966414076589]
	TIME [epoch: 8.77 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27361920354605085		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.25624943910383413		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.2649343213249425 | validation: 0.25432950632100815]
	TIME [epoch: 8.77 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23734418619570657		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.26473740072938023		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.2510407934625434 | validation: 0.24025556589822464]
	TIME [epoch: 8.77 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24956008260605483		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.23359858462671826		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.2415793336163865 | validation: 0.19496487971701928]
	TIME [epoch: 8.78 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23804654334640504		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.25523354556712435		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.24664004445676477 | validation: 0.2741017704983765]
	TIME [epoch: 8.8 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23839364766295823		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.24506701070345507		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.2417303291832066 | validation: 0.28785048006938196]
	TIME [epoch: 8.79 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2519159481350892		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.2796938865850391		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.2658049173600642 | validation: 0.21739184478477558]
	TIME [epoch: 8.78 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2154745101466718		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.27830094135262773		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.2468877257496498 | validation: 0.20737849492497493]
	TIME [epoch: 8.77 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2740121400038783		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 0.23558240568329003		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.2547972728435842 | validation: 0.22155593275851948]
	TIME [epoch: 8.78 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2669022042696925		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.23390673911495002		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.2504044716923213 | validation: 0.31780442326999936]
	TIME [epoch: 8.79 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3095560090890213		[learning rate: 0.003502]
		[batch 20/20] avg loss: 0.4296086427248397		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.3695823259069305 | validation: 0.19166703034271676]
	TIME [epoch: 8.76 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2504627204242492		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 0.219581640822547		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.23502218062339814 | validation: 0.16930744415724971]
	TIME [epoch: 8.78 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2648766575975966		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.25755881450883206		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.2612177360532143 | validation: 0.25528148757880953]
	TIME [epoch: 8.77 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21951001068306714		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.2915681174691893		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.25553906407612814 | validation: 0.21849484246346765]
	TIME [epoch: 8.77 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20355967892429475		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.20628276164947654		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.20492122028688567 | validation: 0.29922009038173425]
	TIME [epoch: 8.8 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2627963451154384		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.26789935934230996		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.2653478522288742 | validation: 0.16870482429464878]
	TIME [epoch: 8.77 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2732492180607989		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.3003801651188581		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.2868146915898285 | validation: 0.2361508975778082]
	TIME [epoch: 8.77 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32863289711959526		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.23519851663503216		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.28191570687731365 | validation: 0.2142336878960693]
	TIME [epoch: 8.76 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22422721308619814		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.2472270489795366		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.23572713103286738 | validation: 0.14319883199335157]
	TIME [epoch: 8.78 sec]
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22734162109785658		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.22927932728351696		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.22831047419068679 | validation: 0.14554083417578614]
	TIME [epoch: 8.8 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19623261397408248		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.26062206686073963		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.22842734041741103 | validation: 0.23823613918537911]
	TIME [epoch: 8.77 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26485240835850904		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.24560495376508276		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.2552286810617959 | validation: 0.2333688209193465]
	TIME [epoch: 8.76 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2577407990144363		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.2657690359826258		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.26175491749853097 | validation: 0.20596250338500488]
	TIME [epoch: 8.76 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2485310725959251		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.27358454990757175		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.26105781125174843 | validation: 0.2648084483484143]
	TIME [epoch: 8.76 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23953995596261818		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.22949084779905088		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.2345154018808345 | validation: 0.2252891644440687]
	TIME [epoch: 8.78 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21596830902670566		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.20742341592574456		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.21169586247622513 | validation: 0.1720436918976626]
	TIME [epoch: 8.76 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2090538962792919		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.24738527169535765		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.22821958398732475 | validation: 0.1884402803872913]
	TIME [epoch: 8.76 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23788481624175986		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.17263060038067507		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.20525770831121748 | validation: 0.24252036657756165]
	TIME [epoch: 8.75 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2261928693277963		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 0.19619198767847892		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.21119242850313763 | validation: 0.3189630747331963]
	TIME [epoch: 8.76 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24763449938175003		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.23773664966778996		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.24268557452477002 | validation: 0.3195357871850074]
	TIME [epoch: 8.76 sec]
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2767186796197836		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.28586903945630016		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.2812938595380419 | validation: 0.173494099719254]
	TIME [epoch: 8.8 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2704974471262257		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.2146803506342157		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.24258889888022073 | validation: 0.17196184692885108]
	TIME [epoch: 8.75 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22693139240940505		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.2581790741636242		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.24255523328651454 | validation: 0.15552397018507483]
	TIME [epoch: 8.76 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21693371077013662		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 0.20807805066569535		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.21250588071791596 | validation: 0.24767756427622273]
	TIME [epoch: 8.77 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25863396996343435		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 0.27779129862848956		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.268212634295962 | validation: 0.253584063802451]
	TIME [epoch: 8.76 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24451985113640945		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.2519815574052906		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.24825070427085003 | validation: 0.15785111386557388]
	TIME [epoch: 8.79 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18511498661228004		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.24116451410735745		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.21313975035981875 | validation: 0.6265897241559846]
	TIME [epoch: 8.76 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26913943346506003		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 0.19160356695181602		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.23037150020843802 | validation: 0.25842350977126016]
	TIME [epoch: 8.76 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1963738164218361		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.24607477791789		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.22122429716986308 | validation: 0.25978625823784046]
	TIME [epoch: 8.76 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2114671172781291		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.27093219155648335		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.2411996544173062 | validation: 0.1853523091794385]
	TIME [epoch: 8.77 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23528584438323227		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.1951654592326954		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.21522565180796388 | validation: 0.1458193865149705]
	TIME [epoch: 8.79 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18274847421282472		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.18647813279063422		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.18461330350172947 | validation: 0.2820795084012588]
	TIME [epoch: 8.76 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20086416905079646		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.23408019241339045		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.21747218073209346 | validation: 0.1306693846758737]
	TIME [epoch: 8.77 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25374784978638637		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.20293871490526688		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.22834328234582663 | validation: 0.28058670695806753]
	TIME [epoch: 8.76 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22114946150002818		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.20494011785559768		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.21304478967781293 | validation: 0.28496551381783203]
	TIME [epoch: 8.76 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23238410339242085		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.17251837981550772		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.20245124160396424 | validation: 0.14642631867666517]
	TIME [epoch: 8.77 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2211783019672761		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.27517629430513774		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.24817729813620698 | validation: 0.15270650798941027]
	TIME [epoch: 8.79 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.210498825403363		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.294220186815478		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.2523595061094205 | validation: 0.14684748736733347]
	TIME [epoch: 8.78 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2167661378819584		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.19245296394726522		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.2046095509146118 | validation: 0.18371442783530006]
	TIME [epoch: 8.77 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18844207645749816		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.20656357915358017		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.19750282780553918 | validation: 0.16583411527227354]
	TIME [epoch: 8.77 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20619886542655447		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.1799984456435128		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.19309865553503366 | validation: 0.2626449508645088]
	TIME [epoch: 8.79 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18484875349557642		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.22435888735533913		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.20460382042545774 | validation: 0.15564448832852024]
	TIME [epoch: 8.78 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23166297120964735		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.395028336589728		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.31334565389968766 | validation: 0.2075741677529709]
	TIME [epoch: 8.76 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.235953482157985		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.2468221519600505		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.24138781705901774 | validation: 0.21101915016920153]
	TIME [epoch: 8.76 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19605670599829506		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.21077482255759947		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.20341576427794722 | validation: 0.17449209486700992]
	TIME [epoch: 8.76 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21311961724967426		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.2516713202187115		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.23239546873419292 | validation: 0.1824888878231613]
	TIME [epoch: 8.75 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21393092118127086		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.22248836012128267		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.2182096406512768 | validation: 0.14077518000206507]
	TIME [epoch: 8.79 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1840801345524288		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.17761351825748714		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.18084682640495794 | validation: 0.11207483250672348]
	TIME [epoch: 8.77 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18269196576161478		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.2027891776173752		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.192740571689495 | validation: 0.18671971241029972]
	TIME [epoch: 8.76 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19180087970329476		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.2043659247728113		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.198083402238053 | validation: 0.1378183726820538]
	TIME [epoch: 8.78 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17755956826278635		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.2600753480454526		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.21881745815411952 | validation: 0.26820884510423715]
	TIME [epoch: 8.77 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22054251479509315		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.21411133275569222		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.21732692377539267 | validation: 0.2000001936563785]
	TIME [epoch: 8.8 sec]
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1893497542113128		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.2276980678430654		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.2085239110271891 | validation: 0.23886902907856078]
	TIME [epoch: 8.77 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26839852538332404		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.20581357132151293		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.2371060483524185 | validation: 0.15311793154778144]
	TIME [epoch: 8.77 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2549260708077267		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.22869767397758073		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.24181187239265375 | validation: 0.17175525173175027]
	TIME [epoch: 8.78 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20418801728878236		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.2064081015345911		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.20529805941168672 | validation: 0.13805394990553863]
	TIME [epoch: 8.77 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20701898498123344		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.21124420788695372		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.20913159643409363 | validation: 0.14558445014105978]
	TIME [epoch: 8.8 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19241409219905603		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.2111537279488585		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.2017839100739572 | validation: 0.19381516411019625]
	TIME [epoch: 8.77 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16957527850979828		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.21483862479847385		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.192206951654136 | validation: 0.23701334028097382]
	TIME [epoch: 8.78 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1979103161252354		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.19857048507357183		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.1982404005994036 | validation: 0.2129052894050154]
	TIME [epoch: 8.77 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1781082577558931		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.18719585448035864		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.18265205611812588 | validation: 0.1686481444301288]
	TIME [epoch: 8.76 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24220993931713392		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.2561601997801252		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.2491850695486296 | validation: 0.1478733326040307]
	TIME [epoch: 8.79 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24136725813531318		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.19619252157112782		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.2187798898532205 | validation: 0.23662882892202952]
	TIME [epoch: 8.78 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19334136476364425		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.18584370903876685		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.18959253690120556 | validation: 0.24440883703453967]
	TIME [epoch: 8.77 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2195152053674339		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.18378437267549197		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.20164978902146294 | validation: 0.10969461344575097]
	TIME [epoch: 8.78 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21613536017134471		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.19667957367841457		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.20640746692487966 | validation: 0.1426317512506856]
	TIME [epoch: 8.76 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19478295482078653		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.18425636199626952		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.189519658408528 | validation: 0.22322337393220343]
	TIME [epoch: 8.78 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24224032031992024		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.18375728511162331		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.21299880271577173 | validation: 0.18924955999543142]
	TIME [epoch: 8.79 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20236135075152886		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.19266239323383869		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.19751187199268377 | validation: 0.18982581783192248]
	TIME [epoch: 8.78 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2963373249895169		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.21638178005469152		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.2563595525221043 | validation: 0.19279451243400353]
	TIME [epoch: 8.77 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19292423480286497		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.19099583399897213		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.1919600344009186 | validation: 0.1586368067127441]
	TIME [epoch: 8.77 sec]
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.263821525936371		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.2021485231629044		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.23298502454963774 | validation: 0.1400231653539426]
	TIME [epoch: 8.78 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19411563403780593		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.19976521871355618		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.19694042637568104 | validation: 0.2592498670907713]
	TIME [epoch: 8.78 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.205181565421292		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.19578404948969583		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.20048280745549393 | validation: 0.13045054797417005]
	TIME [epoch: 8.77 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1800142409634024		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.1634525489233692		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.1717333949433858 | validation: 0.14723964741827314]
	TIME [epoch: 8.77 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16866584158516687		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.20906298861318096		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.1888644150991739 | validation: 0.225149044668331]
	TIME [epoch: 8.77 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18554099573576316		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.16193474508297118		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.1737378704093672 | validation: 0.18858634223466275]
	TIME [epoch: 8.78 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18701798665367564		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.19280666067187896		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.18991232366277727 | validation: 0.14846329093433738]
	TIME [epoch: 8.81 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23378353245821754		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.178347138212084		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.20606533533515076 | validation: 0.1296685229256986]
	TIME [epoch: 8.78 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17077907581280702		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.19662551491315186		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.18370229536297944 | validation: 0.18231355958121023]
	TIME [epoch: 8.78 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23236335515316306		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.185791041927859		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.20907719854051102 | validation: 0.23359143504080918]
	TIME [epoch: 8.77 sec]
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21410767364455743		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.18049468950135591		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.19730118157295667 | validation: 0.2948104321578773]
	TIME [epoch: 8.77 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2021072829187382		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.19007041221951454		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.1960888475691263 | validation: 0.12224280528201084]
	TIME [epoch: 8.8 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18977835337272153		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.20215441914648494		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.19596638625960322 | validation: 0.18010368800884155]
	TIME [epoch: 8.77 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17697713362372477		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.15807314782167678		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.1675251407227008 | validation: 0.18732305318721187]
	TIME [epoch: 8.77 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20866914411742488		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.17605719138688533		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.1923631677521551 | validation: 0.16408176450930714]
	TIME [epoch: 8.77 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15366157411286752		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.2080261073115254		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.18084384071219645 | validation: 0.13947098010288295]
	TIME [epoch: 8.77 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20603534953407054		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.17788404747073963		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.19195969850240507 | validation: 0.1596625079257063]
	TIME [epoch: 8.8 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16236181688791176		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.22305816915005056		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.1927099930189812 | validation: 0.2082127774751171]
	TIME [epoch: 8.78 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19210113054959543		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.17369803331819683		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.18289958193389616 | validation: 0.2041165495099698]
	TIME [epoch: 8.78 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.152771776711143		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.19455839983316744		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.17366508827215524 | validation: 0.11914364686366873]
	TIME [epoch: 8.78 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18190093693316745		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.19454836836377987		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.18822465264847363 | validation: 0.31133977827572534]
	TIME [epoch: 8.78 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29109506324332235		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.20458835878623954		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.2478417110147809 | validation: 0.13725757409838157]
	TIME [epoch: 8.79 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18401543403707502		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.20399561361190507		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.19400552382449004 | validation: 0.21889120317158842]
	TIME [epoch: 8.8 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17812245740540292		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.1505494356103269		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.1643359465078649 | validation: 0.1593501320914581]
	TIME [epoch: 8.76 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2008569143762687		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.18981746103622515		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.19533718770624692 | validation: 0.17630924449800509]
	TIME [epoch: 8.77 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16661223404003983		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.13978129884127166		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.15319676644065572 | validation: 0.16776280312552214]
	TIME [epoch: 8.78 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17982786832304848		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.19043780157420775		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.18513283494862806 | validation: 0.13604590309262485]
	TIME [epoch: 8.79 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1602431542027218		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.20283399809391497		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.18153857614831836 | validation: 0.14010677005174663]
	TIME [epoch: 8.78 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18183494778523318		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.20879834296192504		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.1953166453735791 | validation: 0.17317274930840845]
	TIME [epoch: 8.77 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1680133307504313		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.1705167330692743		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.1692650319098528 | validation: 0.17854973190370987]
	TIME [epoch: 8.78 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15567120742590151		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.19014698310852826		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.17290909526721485 | validation: 0.1674084598277904]
	TIME [epoch: 8.77 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16094376319219558		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.15237206701250047		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.15665791510234803 | validation: 0.13979414972774706]
	TIME [epoch: 8.77 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14836920527347605		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.1775070925554904		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.16293814891448327 | validation: 0.1799826770653773]
	TIME [epoch: 8.8 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18735356778412485		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.15539545101234248		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.1713745093982337 | validation: 0.2023427255497999]
	TIME [epoch: 8.77 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1922783972125663		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.15751815400934654		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.1748982756109564 | validation: 0.11674298046382067]
	TIME [epoch: 8.78 sec]
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1534597324439487		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.17145987414944508		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.16245980329669688 | validation: 0.15238446541520378]
	TIME [epoch: 8.76 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1642084341460603		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.16368973580405394		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.1639490849750571 | validation: 0.17287867012409627]
	TIME [epoch: 8.77 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26714868229983896		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.19902249113963325		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.23308558671973612 | validation: 0.1810332698919922]
	TIME [epoch: 8.79 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1881219046947543		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.1814504639270706		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.18478618431091243 | validation: 0.1361834800344578]
	TIME [epoch: 8.76 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19572336582153974		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.1815114326308138		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.18861739922617676 | validation: 0.1748173540952767]
	TIME [epoch: 8.77 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1843202599068111		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.14150765678867996		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.16291395834774558 | validation: 0.15258607458593787]
	TIME [epoch: 8.77 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2008449383970806		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.15065039803389177		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.1757476682154862 | validation: 0.15798583202721783]
	TIME [epoch: 8.76 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15600067420415836		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.15971035976661957		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.15785551698538894 | validation: 0.16339072179620873]
	TIME [epoch: 8.78 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1508665633387436		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.20759459693978471		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.17923058013926418 | validation: 0.10870813652516305]
	TIME [epoch: 8.76 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14072564903860657		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.20046695797178785		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.1705963035051972 | validation: 0.14217355892848943]
	TIME [epoch: 8.76 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17116047616830277		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.17372451921950208		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.17244249769390244 | validation: 0.23861098009252324]
	TIME [epoch: 8.76 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18576763165845833		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.1637049894394394		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.17473631054894886 | validation: 0.21375213866030884]
	TIME [epoch: 8.76 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15087797855244778		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.16231439277333815		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.15659618566289296 | validation: 0.19857242193519717]
	TIME [epoch: 8.79 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2116393634912864		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.18339848853305357		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.19751892601216997 | validation: 0.13971534980188138]
	TIME [epoch: 8.78 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1555236644990065		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.2156011745768549		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.18556241953793073 | validation: 0.18591526371330508]
	TIME [epoch: 8.76 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17868273130980397		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.1748569083244771		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.17676981981714052 | validation: 0.15316510028452568]
	TIME [epoch: 8.76 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17312706199674152		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.17606414735726744		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.1745956046770045 | validation: 0.19370692834663178]
	TIME [epoch: 8.76 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15630363536575892		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.2468203981727966		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.20156201676927776 | validation: 0.13069444008029463]
	TIME [epoch: 8.77 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19922152936170218		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.18033634734778553		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.18977893835474385 | validation: 0.1751816060871548]
	TIME [epoch: 8.79 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15369128855784941		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.1934496824226718		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.1735704854902606 | validation: 0.17513737538305332]
	TIME [epoch: 8.76 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16386136941668528		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.15260914974911038		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.15823525958289786 | validation: 0.1652864537225776]
	TIME [epoch: 8.76 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1891862760979309		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.15223337376462115		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.170709824931276 | validation: 0.1366421990204305]
	TIME [epoch: 8.76 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16829877692453774		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.1535194427106027		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.1609091098175702 | validation: 0.1540377270678257]
	TIME [epoch: 8.77 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16939355960258418		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.1330770221878777		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.15123529089523094 | validation: 0.11531256897703454]
	TIME [epoch: 8.79 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1670230738554771		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.16102512885697012		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.16402410135622358 | validation: 0.14557308390745916]
	TIME [epoch: 8.77 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15397466124585305		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.15269569671053207		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.15333517897819257 | validation: 0.19284683684556722]
	TIME [epoch: 8.77 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17667323774188884		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.17017928655354933		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.17342626214771908 | validation: 0.11213110546865289]
	TIME [epoch: 8.77 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1545323334399624		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.17234020618728074		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.16343626981362155 | validation: 0.15715159072954887]
	TIME [epoch: 8.76 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15986566700018692		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.16901447563602998		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.16444007131810842 | validation: 0.1773584339883339]
	TIME [epoch: 8.8 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15728162451142016		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.1596475486819263		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.15846458659667323 | validation: 0.16307450342267749]
	TIME [epoch: 8.76 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1466116021071387		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.15337576661800884		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.14999368436257376 | validation: 0.1753584861204549]
	TIME [epoch: 8.76 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15096499862703117		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.14046109841299798		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.14571304852001457 | validation: 0.1319644553387066]
	TIME [epoch: 8.75 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16850482330501576		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.15064621062860373		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.15957551696680974 | validation: 0.09559265836034114]
	TIME [epoch: 8.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_405.pth
	Model improved!!!
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15919933929521926		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.15184691262244182		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.15552312595883055 | validation: 0.16855967697387408]
	TIME [epoch: 8.78 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16228677894338664		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.13720202953064173		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.14974440423701416 | validation: 0.16401209888698431]
	TIME [epoch: 8.76 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14599669352994782		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.159824598679516		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.1529106461047319 | validation: 0.135543384297808]
	TIME [epoch: 8.75 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19660939498495528		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.1601838279706414		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.1783966114777983 | validation: 0.1660430415164281]
	TIME [epoch: 8.75 sec]
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15678920869092403		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.20653944377966482		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.18166432623529435 | validation: 0.1324565797274943]
	TIME [epoch: 8.75 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17403776801686316		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.14920483270757967		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.1616213003622214 | validation: 0.13633614410275782]
	TIME [epoch: 8.75 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1538370959944481		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.14912663323079797		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.15148186461262303 | validation: 0.2688368837301015]
	TIME [epoch: 8.77 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18150081800159779		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.16883094140343383		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.1751658797025158 | validation: 0.12396918134104065]
	TIME [epoch: 8.75 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15506725151009898		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.1692468708469695		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.16215706117853423 | validation: 0.12296395949478081]
	TIME [epoch: 8.76 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14426915307007113		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.18170106307719386		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.1629851080736325 | validation: 0.21005790355171886]
	TIME [epoch: 8.76 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19772155502258232		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.1483005941979387		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.17301107461026052 | validation: 0.11578256347718913]
	TIME [epoch: 8.76 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12076271238034204		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 0.16505563673422505		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 0.14290917455728355 | validation: 0.21617407136920758]
	TIME [epoch: 8.79 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17151348354120255		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 0.13637110751501003		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 0.1539422955281063 | validation: 0.11554302736441294]
	TIME [epoch: 8.75 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1504146539160099		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 0.15083224713988536		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 0.15062345052794762 | validation: 0.13673193919257365]
	TIME [epoch: 8.74 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15765774872352573		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.16611483626686555		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.16188629249519565 | validation: 0.1720207589219907]
	TIME [epoch: 8.76 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15747859223745553		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.15783502185138487		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.1576568070444202 | validation: 0.156221089085881]
	TIME [epoch: 8.75 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16686386426324776		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.2097379644393172		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.18830091435128243 | validation: 0.1825777024468661]
	TIME [epoch: 8.77 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1924117438765646		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.1483305721426172		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.17037115800959088 | validation: 0.14910075649425053]
	TIME [epoch: 8.78 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16897784813761085		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.14726778474963287		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.15812281644362186 | validation: 0.19145654497466458]
	TIME [epoch: 8.75 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1684450858769717		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.14224850026447117		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.15534679307072144 | validation: 0.10775768987805084]
	TIME [epoch: 8.75 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14297285474750404		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.1764600818506613		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.1597164682990827 | validation: 0.12956929394697975]
	TIME [epoch: 8.75 sec]
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17031550307270749		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.14566745033972123		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.1579914767062143 | validation: 0.15528048419673707]
	TIME [epoch: 8.76 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1425886967255316		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.1535558143111671		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.14807225551834938 | validation: 0.10688811279161442]
	TIME [epoch: 8.77 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13974605315722963		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.17682377439671787		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.1582849137769738 | validation: 0.15810393389236715]
	TIME [epoch: 8.77 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15588494565602767		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.14092749257407836		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.148406219115053 | validation: 0.14492918867957436]
	TIME [epoch: 8.75 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15214626441683393		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.13031685554547415		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.14123155998115403 | validation: 0.10762615525881271]
	TIME [epoch: 8.75 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15787487978560244		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.1357108329983435		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.14679285639197298 | validation: 0.19859856441521861]
	TIME [epoch: 8.76 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17237298607937185		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.1272792212477546		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.14982610366356325 | validation: 0.1293137696113291]
	TIME [epoch: 8.75 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18387559850997867		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.13763725112756597		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.1607564248187723 | validation: 0.11474684033618718]
	TIME [epoch: 8.77 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1527091835969804		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.1368848888916733		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.14479703624432685 | validation: 0.1500440839529198]
	TIME [epoch: 8.75 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16074362446971785		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.1607454316048281		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.16074452803727302 | validation: 0.11377971314499224]
	TIME [epoch: 8.75 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14394579517388234		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.17921222287295563		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.16157900902341898 | validation: 0.12102841001344163]
	TIME [epoch: 8.75 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1429378673920614		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.13980060578130477		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.14136923658668307 | validation: 0.17315670038702616]
	TIME [epoch: 8.75 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14123631401748693		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.13278213499231725		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.1370092245049021 | validation: 0.11314506533848909]
	TIME [epoch: 8.76 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14690933540979084		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.15326235496140836		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.15008584518559961 | validation: 0.11718348293015055]
	TIME [epoch: 8.76 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450618904364457		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.15726895293984655		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.15116542168814612 | validation: 0.17137790599968397]
	TIME [epoch: 8.76 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14972339164384968		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.15523269145831609		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.15247804155108288 | validation: 0.13616577483195602]
	TIME [epoch: 8.76 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14226340778258512		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.17515099785328014		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.15870720281793266 | validation: 0.15650578411990584]
	TIME [epoch: 8.75 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16269403196347565		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.13861623359496122		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.1506551327792184 | validation: 0.15634888954155385]
	TIME [epoch: 8.76 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14427300172075447		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.1373447635315694		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.14080888262616192 | validation: 0.1403313582152046]
	TIME [epoch: 8.77 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17555338870816817		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.16081263924493672		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.1681830139765524 | validation: 0.12701099506107036]
	TIME [epoch: 8.77 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14033358912990126		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.14604986523769095		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.14319172718379608 | validation: 0.12036336502582504]
	TIME [epoch: 8.76 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13213286631363086		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.14970469454744822		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.14091878043053954 | validation: 0.11489688536589004]
	TIME [epoch: 8.76 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14182970686615096		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.1726092406082323		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.15721947373719164 | validation: 0.20002818735476377]
	TIME [epoch: 8.76 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14350005798107068		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.1561192756248496		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.14980966680296012 | validation: 0.20746788549148604]
	TIME [epoch: 8.75 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16681861670752102		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.12797942331805118		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.14739902001278607 | validation: 0.11867444837108647]
	TIME [epoch: 8.78 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13472615813495897		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.1513155729537608		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.1430208655443599 | validation: 0.14204768691272282]
	TIME [epoch: 8.76 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12606608395680424		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.1768612933748249		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.15146368866581458 | validation: 0.14852609382561988]
	TIME [epoch: 8.76 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14048882636063284		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.15283805416688553		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.14666344026375921 | validation: 0.15530375853241862]
	TIME [epoch: 8.77 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16443206271860084		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.14017913095769013		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.1523055968381455 | validation: 0.15267577578592206]
	TIME [epoch: 8.76 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15577022531823884		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.14224661715681722		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.14900842123752805 | validation: 0.1317742476607256]
	TIME [epoch: 8.77 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14232283505641302		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.182127795844202		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.16222531545030752 | validation: 0.21316016989981146]
	TIME [epoch: 8.78 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17204557973787857		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.1368952363961322		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.15447040806700538 | validation: 0.09702645308915842]
	TIME [epoch: 8.75 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12469073104091284		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.1520699905329409		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.1383803607869269 | validation: 0.12201935927962387]
	TIME [epoch: 8.75 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14745027498715835		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.1338891847271317		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.14066972985714504 | validation: 0.10514030754714043]
	TIME [epoch: 8.75 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14429414727248108		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.13003097580056736		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.13716256153652423 | validation: 0.10759733874096516]
	TIME [epoch: 8.76 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13887198399825829		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.1343380501355775		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.1366050170669179 | validation: 0.13152226676930415]
	TIME [epoch: 8.79 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12487681316886252		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.15717734132820066		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.14102707724853156 | validation: 0.2155936908665456]
	TIME [epoch: 8.77 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1275540416685388		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.12665031734928028		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.1271021795089095 | validation: 0.14065328187258785]
	TIME [epoch: 8.77 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14840150488987836		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.14446067855906056		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.14643109172446947 | validation: 0.17848340576455035]
	TIME [epoch: 8.76 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14065866594505858		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.17836482558691347		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.159511745765986 | validation: 0.1518463262676929]
	TIME [epoch: 8.77 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13988670196109293		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.13400804380403722		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.1369473728825651 | validation: 0.09398854528700108]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_467.pth
	Model improved!!!
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2082516874212879		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.14602971880624277		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.1771407031137653 | validation: 0.15238978007439244]
	TIME [epoch: 9.11 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15458499371936785		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.13116141123823827		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.14287320247880306 | validation: 0.16885185337497863]
	TIME [epoch: 8.78 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12165700651101016		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.13618667953545227		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.12892184302323123 | validation: 0.1221262691392852]
	TIME [epoch: 8.78 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12594424385994768		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.15423514030977953		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.1400896920848636 | validation: 0.10901168184804945]
	TIME [epoch: 8.78 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12377234347356147		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.1297362594788875		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.12675430147622446 | validation: 0.14375599689582425]
	TIME [epoch: 8.77 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1290759697355866		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.1244425846237934		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.12675927717969 | validation: 0.1388779054213496]
	TIME [epoch: 8.8 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14957639258612287		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.12662916259591944		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.1381027775910212 | validation: 0.10648281769206806]
	TIME [epoch: 8.78 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12364960986408349		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.13161906481821795		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.12763433734115073 | validation: 0.10774292336825389]
	TIME [epoch: 8.78 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14410099812409746		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.14131237464535193		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.1427066863847247 | validation: 0.13103235676605132]
	TIME [epoch: 8.77 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12595351263905114		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.11561930474396782		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.12078640869150944 | validation: 0.15302518948922567]
	TIME [epoch: 8.77 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12413175540294608		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.14131596087488008		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.13272385813891308 | validation: 0.1278444974680995]
	TIME [epoch: 8.77 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12931755871143888		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.1371629419337506		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.13324025032259473 | validation: 0.1226926419431625]
	TIME [epoch: 8.79 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14878681591619086		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.15129534829786725		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.15004108210702907 | validation: 0.11561141882122214]
	TIME [epoch: 8.78 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13600696113477903		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.14178787213814892		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.138897416636464 | validation: 0.11314637730975398]
	TIME [epoch: 8.78 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13415024422866567		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.13524610735428833		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.13469817579147697 | validation: 0.1654567004312245]
	TIME [epoch: 8.77 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14825961717438343		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.12947944935651842		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.1388695332654509 | validation: 0.10035021151055681]
	TIME [epoch: 8.77 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15191273301595629		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.13543219076436597		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.1436724618901611 | validation: 0.2075788874582757]
	TIME [epoch: 8.78 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1439577955430629		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 0.12331476398848629		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 0.13363627976577458 | validation: 0.10636512971097825]
	TIME [epoch: 8.79 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12058712779210885		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 0.15745996389035155		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 0.1390235458412302 | validation: 0.10113646606905048]
	TIME [epoch: 8.77 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11941990102742339		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.12684341514659042		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.12313165808700688 | validation: 0.10348777046428785]
	TIME [epoch: 8.78 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1452836360806172		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.12400125965834534		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.1346424478694813 | validation: 0.17667726689117427]
	TIME [epoch: 8.78 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1599273014524719		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.13133166797926293		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.1456294847158674 | validation: 0.13474059513096495]
	TIME [epoch: 8.77 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12570532246346514		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.10214932969295673		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.11392732607821095 | validation: 0.1254195908451529]
	TIME [epoch: 8.79 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11478437751706409		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.13785602628528165		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.12632020190117285 | validation: 0.17822242491848705]
	TIME [epoch: 8.77 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14639162591748983		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 0.14257170510660735		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 0.14448166551204858 | validation: 0.15040192153367082]
	TIME [epoch: 8.77 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11855874988888775		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 0.14889433949468195		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 0.13372654469178485 | validation: 0.09897256755456438]
	TIME [epoch: 8.77 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14412260450199504		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 0.16849772079070552		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 0.15631016264635028 | validation: 0.14271100357833683]
	TIME [epoch: 8.78 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12326279976941697		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 0.14605234807615558		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 0.13465757392278627 | validation: 0.2555600082721997]
	TIME [epoch: 8.79 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1567748118142926		[learning rate: 0.001154]
		[batch 20/20] avg loss: 0.13463964720827165		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 0.14570722951128212 | validation: 0.13443952582720756]
	TIME [epoch: 8.8 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12869314887581879		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 0.14665397056845159		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 0.13767355972213519 | validation: 0.13924950121930907]
	TIME [epoch: 8.78 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13211082951829528		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.14416246634289678		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 0.13813664793059602 | validation: 0.1396017534178174]
	TIME [epoch: 8.77 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12313900341786821		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.12444286029474101		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.1237909318563046 | validation: 0.15509237735694412]
	TIME [epoch: 8.78 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13346967693522932		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.1301493491370135		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.1318095130361214 | validation: 0.15434548610820853]
	TIME [epoch: 8.77 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1394466763068662		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.1219157366682605		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.13068120648756335 | validation: 0.126005323992977]
	TIME [epoch: 8.79 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13383595262078465		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.14014024206638082		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.13698809734358275 | validation: 0.10752077765030274]
	TIME [epoch: 8.79 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1276817541125778		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.16798052285138937		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.14783113848198356 | validation: 0.15450197947286148]
	TIME [epoch: 8.77 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14443171174015018		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.14184458928069707		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.14313815051042364 | validation: 0.09971155434712312]
	TIME [epoch: 8.78 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1443377017011302		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.11862020162109457		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.1314789516611124 | validation: 0.1287294259127464]
	TIME [epoch: 8.78 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1268458345108309		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.12742205386727928		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.1271339441890551 | validation: 0.12491432713837014]
	TIME [epoch: 8.77 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1265192772292454		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.15205309574354953		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.13928618648639746 | validation: 0.16719578846005603]
	TIME [epoch: 8.79 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17799778498686525		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.13439280646229115		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.1561952957245782 | validation: 0.12641170565638352]
	TIME [epoch: 8.78 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14308240424007884		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 0.15501266997834512		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 0.14904753710921198 | validation: 0.11266697785358737]
	TIME [epoch: 8.78 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13729071198946874		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 0.14085349850859186		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 0.1390721052490303 | validation: 0.1376586386467945]
	TIME [epoch: 8.77 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1661513159421053		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 0.15362600058350556		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 0.15988865826280543 | validation: 0.11917480597143522]
	TIME [epoch: 8.78 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13351488163659667		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 0.14148217522401674		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 0.13749852843030672 | validation: 0.14881755595385532]
	TIME [epoch: 8.79 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13669074549904675		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 0.12737682900199648		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 0.13203378725052162 | validation: 0.12278704502780405]
	TIME [epoch: 8.79 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1253437475178562		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.12846129125999667		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.12690251938892647 | validation: 0.13238461925381956]
	TIME [epoch: 8.78 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1450744940762371		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 0.11689761012957563		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.13098605210290637 | validation: 0.1150632929664743]
	TIME [epoch: 8.77 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1311847359559955		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 0.13488469732403094		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 0.1330347166400132 | validation: 0.1439521758052314]
	TIME [epoch: 8.78 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14038476482552165		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.11668934676868267		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 0.12853705579710212 | validation: 0.11149973306074563]
	TIME [epoch: 8.78 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12059265223068602		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.13834985495126723		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.1294712535909766 | validation: 0.10492299405853092]
	TIME [epoch: 8.82 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14272970187823125		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 0.14667189019858926		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 0.14470079603841027 | validation: 0.11314433536346033]
	TIME [epoch: 8.78 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10880063544294849		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 0.1571026597006398		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 0.13295164757179415 | validation: 0.1533236734220758]
	TIME [epoch: 8.77 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1296489094282838		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 0.15966106151087422		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 0.144654985469579 | validation: 0.11386076140207682]
	TIME [epoch: 8.78 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14464584170135916		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 0.11158596829975238		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 0.12811590500055578 | validation: 0.10515540326363758]
	TIME [epoch: 8.78 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11497693546218231		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 0.1212927826524689		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 0.1181348590573256 | validation: 0.17046470102248013]
	TIME [epoch: 8.78 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13709073719314724		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 0.11148129206760578		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 0.12428601463037652 | validation: 0.11025999015982316]
	TIME [epoch: 8.79 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1060828712865716		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 0.13872226975272553		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 0.12240257051964853 | validation: 0.08348166167367749]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_525.pth
	Model improved!!!
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11592319400388203		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 0.11697212091401477		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.1164476574589484 | validation: 0.1207003318024076]
	TIME [epoch: 8.78 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12936254075881404		[learning rate: 0.000993]
		[batch 20/20] avg loss: 0.11874525505118419		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 0.12405389790499907 | validation: 0.11844876783918605]
	TIME [epoch: 8.77 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11666770595763327		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 0.1282448518301677		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 0.1224562788939005 | validation: 0.11436841416257584]
	TIME [epoch: 8.77 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12390209570593343		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 0.13333360723436155		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 0.12861785147014748 | validation: 0.08439039445510774]
	TIME [epoch: 8.79 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11346525436409534		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 0.11339086418145763		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 0.11342805927277648 | validation: 0.12592024667844331]
	TIME [epoch: 8.78 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10757068142216053		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 0.12442047975291855		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 0.11599558058753952 | validation: 0.1045611778625671]
	TIME [epoch: 8.78 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11664365290965699		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 0.1386895062691688		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 0.12766657958941285 | validation: 0.22103025087830253]
	TIME [epoch: 8.77 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16877294872123763		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 0.10731276279685631		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 0.13804285575904698 | validation: 0.10259872857083545]
	TIME [epoch: 8.77 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1258695047574711		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 0.13269068990161434		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 0.12928009732954274 | validation: 0.11390569320635535]
	TIME [epoch: 8.77 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1233633587637835		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 0.11585126594457731		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 0.1196073123541804 | validation: 0.11622532332106295]
	TIME [epoch: 8.8 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10768676138457292		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.1283199908045169		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 0.11800337609454488 | validation: 0.11124507933096445]
	TIME [epoch: 8.78 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12130851154241425		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.10565572879105319		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 0.11348212016673373 | validation: 0.13147200167018305]
	TIME [epoch: 8.77 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16225566410638753		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.11584503673004971		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.13905035041821862 | validation: 0.10136917576070545]
	TIME [epoch: 8.77 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11255132184185954		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.11135921463763113		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.1119552682397453 | validation: 0.11037315524566643]
	TIME [epoch: 8.77 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11356172328740037		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.12155183707262349		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.11755678018001195 | validation: 0.10354826858917973]
	TIME [epoch: 8.78 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12471614117370551		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.126373122078936		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.12554463162632076 | validation: 0.1729914009551947]
	TIME [epoch: 8.78 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13541140961720335		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.13328110316960864		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.13434625639340597 | validation: 0.1254479896168464]
	TIME [epoch: 8.77 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12859210633906543		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.11725044225964303		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.12292127429935425 | validation: 0.10322382001191964]
	TIME [epoch: 8.77 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11878938805875236		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.13953868100623718		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.12916403453249475 | validation: 0.10250055983659878]
	TIME [epoch: 8.76 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1477855317091948		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.1296854371427983		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.13873548442599654 | validation: 0.12602166170354773]
	TIME [epoch: 8.77 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13544063741322843		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.13188608129047535		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.1336633593518519 | validation: 0.09923332936125048]
	TIME [epoch: 8.8 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12436359073076741		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 0.11406651759042105		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.11921505416059423 | validation: 0.13792091203283757]
	TIME [epoch: 8.76 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13016204934255945		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 0.14086559791492115		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 0.1355138236287403 | validation: 0.0946633584159698]
	TIME [epoch: 8.77 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12294720518972431		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 0.1332556324728013		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 0.12810141883126283 | validation: 0.09791125553544042]
	TIME [epoch: 8.77 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1197057618204701		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.11339573746308333		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.11655074964177672 | validation: 0.11958020367739865]
	TIME [epoch: 8.77 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1223753707673467		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.126553475096413		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.12446442293187984 | validation: 0.12234132703192813]
	TIME [epoch: 8.77 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1135284122660696		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.12409237461096656		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.11881039343851807 | validation: 0.11187378186468021]
	TIME [epoch: 8.79 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1063821347419368		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.12215593985606332		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.11426903729900006 | validation: 0.09505303192922286]
	TIME [epoch: 8.76 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11737828429774293		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.14168184916118207		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.1295300667294625 | validation: 0.12435623547168448]
	TIME [epoch: 8.76 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12269076602345974		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.11365896496265533		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.11817486549305752 | validation: 0.12453199240674251]
	TIME [epoch: 8.78 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14746706734597637		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.119204616595052		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.13333584197051418 | validation: 0.11748928086082623]
	TIME [epoch: 8.77 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11145681432820098		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.1482017503332363		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.12982928233071864 | validation: 0.13459989325797653]
	TIME [epoch: 8.78 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12110569875716112		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.12283351138852605		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.12196960507284357 | validation: 0.13018800697367985]
	TIME [epoch: 8.78 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13157580447169445		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.12402086505308925		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.12779833476239186 | validation: 0.13783735719688278]
	TIME [epoch: 8.77 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13150878895251858		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 0.14112324577422114		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 0.13631601736336987 | validation: 0.17506202429020135]
	TIME [epoch: 8.77 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12221186944751887		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 0.12959269152135233		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 0.12590228048443558 | validation: 0.1235259800176263]
	TIME [epoch: 8.77 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16686301104337622		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 0.11653754371709428		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 0.1417002773802353 | validation: 0.1344074529829546]
	TIME [epoch: 8.78 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11878108725369005		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 0.12354344733319542		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 0.12116226729344275 | validation: 0.08996337019990347]
	TIME [epoch: 8.8 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11153464692990026		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 0.11021171477193234		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 0.11087318085091628 | validation: 0.1325158843029164]
	TIME [epoch: 8.78 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1343627374945589		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 0.11739165209371501		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 0.12587719479413698 | validation: 0.08912490254509868]
	TIME [epoch: 8.77 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10950323340294059		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 0.13251603215531896		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 0.12100963277912977 | validation: 0.0952101666308269]
	TIME [epoch: 8.77 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11157581392352649		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 0.11074779964096917		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 0.11116180678224782 | validation: 0.11834505959538547]
	TIME [epoch: 8.78 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12236937993530501		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 0.12207077395864234		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 0.12222007694697368 | validation: 0.16890913181612352]
	TIME [epoch: 8.78 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1308595167422394		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 0.12091116856081159		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 0.12588534265152548 | validation: 0.12134819257068973]
	TIME [epoch: 8.79 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1217232218681189		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 0.10644212506858793		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 0.11408267346835343 | validation: 0.11188999577189766]
	TIME [epoch: 8.78 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11309572426020201		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 0.13843748942149875		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 0.12576660684085036 | validation: 0.15120309504089016]
	TIME [epoch: 8.78 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13092640952744508		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 0.11839116014702895		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 0.124658784837237 | validation: 0.10002952574154939]
	TIME [epoch: 8.78 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12437076148852373		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 0.1385013244659317		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 0.13143604297722772 | validation: 0.1323945949560921]
	TIME [epoch: 8.77 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1232449206285533		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 0.11240529468431779		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 0.11782510765643553 | validation: 0.13422585250431296]
	TIME [epoch: 8.8 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12562911779225608		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 0.12851932663714843		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 0.12707422221470227 | validation: 0.09981442165478363]
	TIME [epoch: 8.78 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11598063507464063		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 0.10681815310756955		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 0.11139939409110508 | validation: 0.10664863764585045]
	TIME [epoch: 8.78 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11674237629305965		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 0.1340781403197508		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 0.12541025830640523 | validation: 0.09214394888654928]
	TIME [epoch: 8.79 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11618136423770156		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 0.111498275204161		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 0.1138398197209313 | validation: 0.1267845929362399]
	TIME [epoch: 8.77 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11146745343252291		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 0.12334572333593981		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 0.11740658838423138 | validation: 0.09743397523080784]
	TIME [epoch: 8.77 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1153927726769745		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 0.11293815409631858		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 0.11416546338664654 | validation: 0.14986720520002472]
	TIME [epoch: 8.8 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1276898927132906		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 0.13858561884460752		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 0.13313775577894907 | validation: 0.10553442991186623]
	TIME [epoch: 8.78 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11863991678015892		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 0.11701198748911903		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 0.11782595213463898 | validation: 0.08757907746539123]
	TIME [epoch: 8.78 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11598309150370081		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 0.12096697759441433		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 0.11847503454905757 | validation: 0.10520731788477286]
	TIME [epoch: 8.78 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11852364647256422		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.10238712047835377		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 0.110455383475459 | validation: 0.13640073573930186]
	TIME [epoch: 8.78 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12869436235826498		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.1039541711634275		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.11632426676084626 | validation: 0.09884531713210877]
	TIME [epoch: 8.78 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11659171597450017		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.13752116973833456		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.12705644285641737 | validation: 0.13591211650704055]
	TIME [epoch: 8.79 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10880213837154613		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.11548058843173896		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.11214136340164256 | validation: 0.11385553656483183]
	TIME [epoch: 8.78 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10481138144573361		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.13413705352529132		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.11947421748551243 | validation: 0.13620474430397467]
	TIME [epoch: 8.77 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.108736068031556		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.09980903662511656		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.10427255232833628 | validation: 0.09437424022961165]
	TIME [epoch: 8.78 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10574400189249064		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.13044049348413528		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.11809224768831299 | validation: 0.15235075775113607]
	TIME [epoch: 8.78 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12960182013891064		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.11868032111497082		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.12414107062694076 | validation: 0.08902942679057098]
	TIME [epoch: 8.8 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12626970856598613		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.11432138077526326		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.12029554467062469 | validation: 0.10311243216212228]
	TIME [epoch: 8.78 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11437165412285732		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.12547847261224224		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.1199250633675498 | validation: 0.10894510250232622]
	TIME [epoch: 8.77 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1101431905524547		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.11914579087414998		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.11464449071330232 | validation: 0.09245082187857524]
	TIME [epoch: 8.77 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10218895675890918		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.11344121696585555		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.10781508686238235 | validation: 0.09467542735941797]
	TIME [epoch: 8.77 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10818553362052455		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.11996388423332918		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.11407470892692687 | validation: 0.08825245964683538]
	TIME [epoch: 8.78 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11757842546538885		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.11948793971503016		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.11853318259020951 | validation: 0.11934903981521064]
	TIME [epoch: 8.79 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11429898349422932		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.11046226238291235		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.11238062293857083 | validation: 0.10811537842721763]
	TIME [epoch: 8.77 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11286720296323405		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.12882433909736216		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.1208457710302981 | validation: 0.09830970796778679]
	TIME [epoch: 8.77 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11739909245892781		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.12079697500716738		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.11909803373304757 | validation: 0.11700450526264716]
	TIME [epoch: 8.77 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12526419236470315		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.11180059164103988		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.11853239200287154 | validation: 0.10072191454493765]
	TIME [epoch: 8.77 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13249928935954886		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.1325170308627662		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.13250816011115757 | validation: 0.1187473184658582]
	TIME [epoch: 8.79 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11462791604827487		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.13698392745120044		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.12580592174973765 | validation: 0.12392833761452018]
	TIME [epoch: 8.78 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11841316733763767		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.12527837975742645		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.12184577354753204 | validation: 0.13812567378898402]
	TIME [epoch: 8.78 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12474949915867328		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.12453688818950684		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.12464319367409007 | validation: 0.11283602974949823]
	TIME [epoch: 8.77 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12114697148516755		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.12116285604235093		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.12115491376375924 | validation: 0.11378913097246746]
	TIME [epoch: 8.78 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1515433106283512		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.11682126022469082		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.134182285426521 | validation: 0.09344837357509386]
	TIME [epoch: 8.77 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13207873304122986		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.11430521903953465		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.12319197604038223 | validation: 0.11617801392915508]
	TIME [epoch: 8.87 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11764833326430417		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.1140210326510162		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.1158346829576602 | validation: 0.12828485466699555]
	TIME [epoch: 8.77 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12431557035162581		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.12609299144591352		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.12520428089876964 | validation: 0.12198694923117234]
	TIME [epoch: 8.78 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12806989883473735		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.11721493178410222		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.1226424153094198 | validation: 0.10086633760497564]
	TIME [epoch: 8.77 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11524227519978944		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.12172159956036796		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.1184819373800787 | validation: 0.13447773976028654]
	TIME [epoch: 8.77 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11499796680848565		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.1248221967780044		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.119910081793245 | validation: 0.14808669869874294]
	TIME [epoch: 8.78 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1162820989271395		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.11140673526015457		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.11384441709364705 | validation: 0.10741128773966145]
	TIME [epoch: 8.79 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11652722907880228		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.11609171281568573		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.11630947094724402 | validation: 0.10031494037012928]
	TIME [epoch: 8.77 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11099680278651299		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.10596123924044605		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.10847902101347953 | validation: 0.11079572836508327]
	TIME [epoch: 8.78 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11252438985572351		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.10610581747129164		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.10931510366350757 | validation: 0.1253825775754668]
	TIME [epoch: 8.77 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12943729268198761		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.12206656315306001		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.1257519279175238 | validation: 0.10950146393507625]
	TIME [epoch: 8.77 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11180524873824257		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.1076052757207087		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.10970526222947563 | validation: 0.08960008763112415]
	TIME [epoch: 8.79 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1111595947283428		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.11675104952204578		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.11395532212519428 | validation: 0.1579631559675181]
	TIME [epoch: 8.76 sec]
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1677483048084384		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.12207103008610634		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.14490966744727235 | validation: 0.14086214197874322]
	TIME [epoch: 8.77 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14088476621401183		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.12009162133926518		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.1304881937766385 | validation: 0.11790946618647197]
	TIME [epoch: 8.76 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12603974260948436		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.1263483992915086		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.12619407095049648 | validation: 0.09459387501669751]
	TIME [epoch: 8.77 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12575577720407036		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.13073313550469579		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.12824445635438303 | validation: 0.14470629431404183]
	TIME [epoch: 8.77 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.123116403105705		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.13086949252064445		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.12699294781317472 | validation: 0.12052506342177507]
	TIME [epoch: 8.79 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1298563157933041		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.09918128273163537		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.11451879926246973 | validation: 0.12271357598665385]
	TIME [epoch: 8.77 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13956265640724877		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.10808334433324432		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.12382300037024652 | validation: 0.12694197892886963]
	TIME [epoch: 8.76 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12511403523138936		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.10419775452558437		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.11465589487848685 | validation: 0.09224804119771278]
	TIME [epoch: 8.77 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11492066152468217		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.11290847665010806		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.11391456908739513 | validation: 0.12226189404583188]
	TIME [epoch: 8.76 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11182953675356408		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.11231474057679802		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.11207213866518106 | validation: 0.11313038162814723]
	TIME [epoch: 8.8 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11347895900334967		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.10799066159948126		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.11073481030141545 | validation: 0.0901618040239922]
	TIME [epoch: 8.78 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1185393942031034		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.12582705274186168		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.12218322347248256 | validation: 0.0955956036680671]
	TIME [epoch: 8.77 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09952232917613402		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.10474948294433921		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.10213590606023659 | validation: 0.09146129199304473]
	TIME [epoch: 8.77 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12812095318127978		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.13976511426228772		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.13394303372178373 | validation: 0.09937083481945841]
	TIME [epoch: 8.76 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10500397353938044		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.11988538997356157		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.11244468175647102 | validation: 0.15791932207890994]
	TIME [epoch: 8.77 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10980205759882758		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.10492919113527151		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.10736562436704952 | validation: 0.11548040845159781]
	TIME [epoch: 8.79 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.094658727812965		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.1107351786265757		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.10269695321977035 | validation: 0.0815539712716542]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_637.pth
	Model improved!!!
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11187462694188025		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.14516607293917635		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.1285203499405283 | validation: 0.09285723300213836]
	TIME [epoch: 8.77 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12759887690196148		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.1276104264603233		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.1276046516811424 | validation: 0.12013292253783714]
	TIME [epoch: 8.77 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10617242807055823		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 0.10964631813908124		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.10790937310481974 | validation: 0.09188143387719444]
	TIME [epoch: 8.77 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1164418356545498		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 0.10732062161144924		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 0.11188122863299951 | validation: 0.09568165592900006]
	TIME [epoch: 8.78 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11434132950359673		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 0.12935549323784962		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 0.12184841137072318 | validation: 0.13557038664981985]
	TIME [epoch: 8.79 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11192914758960766		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 0.10829892257762781		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 0.11011403508361772 | validation: 0.11019699901671046]
	TIME [epoch: 8.78 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10668726037025866		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 0.10584787438973345		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 0.10626756737999606 | validation: 0.10705133924997288]
	TIME [epoch: 8.78 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1182868401328991		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 0.1154999050834891		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 0.11689337260819407 | validation: 0.09531736458608854]
	TIME [epoch: 8.78 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10479932869737354		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 0.10105149101588173		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 0.10292540985662761 | validation: 0.09764389493018732]
	TIME [epoch: 8.78 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10825743748848131		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 0.1069084217295043		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 0.10758292960899281 | validation: 0.12072971989196778]
	TIME [epoch: 8.8 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11186255659930566		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 0.09904026102606422		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 0.10545140881268493 | validation: 0.10944737085581924]
	TIME [epoch: 8.78 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10700464167212091		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 0.13801840354459066		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 0.12251152260835581 | validation: 0.10419178901627348]
	TIME [epoch: 8.77 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10595730474362137		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 0.10905494562099552		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 0.10750612518230847 | validation: 0.09840068887168282]
	TIME [epoch: 8.78 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09787944755363305		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 0.12690560503155793		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 0.11239252629259551 | validation: 0.08585172537788521]
	TIME [epoch: 8.77 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10145701581083777		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 0.11821285702083746		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 0.10983493641583761 | validation: 0.11016835907200549]
	TIME [epoch: 8.79 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11220012941819091		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 0.10665451094543732		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 0.10942732018181411 | validation: 0.10796433106412066]
	TIME [epoch: 8.79 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10325053273956666		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 0.10140946956056023		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 0.10233000115006344 | validation: 0.11666842795646948]
	TIME [epoch: 8.78 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09861250979397122		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 0.11301603677724874		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 0.10581427328561 | validation: 0.09537007715282161]
	TIME [epoch: 8.78 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.100743911236749		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 0.11654493050788661		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 0.10864442087231782 | validation: 0.08324877468443766]
	TIME [epoch: 8.77 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11333105207892218		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 0.10709749964413698		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 0.11021427586152957 | validation: 0.10697541516632741]
	TIME [epoch: 8.78 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11526544316483582		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 0.11399602465869962		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 0.11463073391176774 | validation: 0.09599840418687475]
	TIME [epoch: 8.8 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10409254346156274		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 0.1024429765041532		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 0.10326775998285798 | validation: 0.0764514866645686]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_659.pth
	Model improved!!!
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10877383921704634		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 0.10603410139730315		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 0.10740397030717472 | validation: 0.09939551275553812]
	TIME [epoch: 8.77 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.134502176051819		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 0.1071319744506108		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 0.12081707525121492 | validation: 0.0813963951338864]
	TIME [epoch: 8.75 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10284199501897254		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 0.10634167600672584		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 0.10459183551284919 | validation: 0.10872790014607862]
	TIME [epoch: 8.76 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1051989349005605		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 0.11101413059960256		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 0.10810653275008153 | validation: 0.08749105419429612]
	TIME [epoch: 8.75 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10822381684679686		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 0.10349522168129846		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 0.10585951926404766 | validation: 0.09635510853611277]
	TIME [epoch: 8.79 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10138679723031836		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 0.11336401472918511		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 0.10737540597975175 | validation: 0.10511370230069648]
	TIME [epoch: 8.77 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11516930477311778		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 0.11301544091472208		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 0.1140923728439199 | validation: 0.08733840662602607]
	TIME [epoch: 8.76 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10777481073843001		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 0.10476735667394518		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 0.10627108370618758 | validation: 0.11012114972583113]
	TIME [epoch: 8.76 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11611795343245032		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 0.10898206671481303		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 0.1125500100736317 | validation: 0.11214941725257693]
	TIME [epoch: 8.76 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11486480482153465		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 0.11592141401159602		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 0.1153931094165653 | validation: 0.09691138721818139]
	TIME [epoch: 8.78 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1036109737627402		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 0.09967957656547319		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 0.10164527516410671 | validation: 0.12205246831890076]
	TIME [epoch: 8.78 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11456415000691562		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 0.11724753359300258		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 0.11590584179995911 | validation: 0.10113870314672173]
	TIME [epoch: 8.77 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11029801030007398		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 0.1146946428684866		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 0.11249632658428028 | validation: 0.09579456624235651]
	TIME [epoch: 8.77 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11975058505703262		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 0.11198326804527445		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 0.11586692655115358 | validation: 0.09015911962060513]
	TIME [epoch: 8.76 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10521252299167339		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 0.10499672904872233		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 0.10510462602019785 | validation: 0.09263761023958784]
	TIME [epoch: 8.76 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12728211360311842		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 0.12822418548603545		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 0.12775314954457695 | validation: 0.10296584872372483]
	TIME [epoch: 8.79 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.099160285990631		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 0.10754721085780505		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 0.103353748424218 | validation: 0.11512202647781511]
	TIME [epoch: 8.76 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13349646365490325		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 0.11108847637701513		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 0.12229247001595919 | validation: 0.09053984814692974]
	TIME [epoch: 8.75 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10120348149463335		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 0.10219416142842339		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 0.10169882146152838 | validation: 0.08807940119947917]
	TIME [epoch: 8.76 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10634677310063836		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 0.1043535635759798		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 0.10535016833830906 | validation: 0.10339975562553408]
	TIME [epoch: 8.76 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10263553428827177		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 0.09831951293758043		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 0.1004775236129261 | validation: 0.09598401018692686]
	TIME [epoch: 8.77 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11772014201295333		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 0.0995445219459776		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 0.10863233197946547 | validation: 0.08839580376843227]
	TIME [epoch: 8.78 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10113842929221928		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 0.11044098252448412		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 0.10578970590835168 | validation: 0.11900081545357113]
	TIME [epoch: 8.76 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11099537294757862		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 0.10515981241244383		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 0.10807759268001124 | validation: 0.09029857747754877]
	TIME [epoch: 8.76 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10752360105211818		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 0.11176800866547369		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 0.10964580485879594 | validation: 0.09696822060212203]
	TIME [epoch: 8.76 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12108853896448132		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 0.120128276275233		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 0.12060840761985718 | validation: 0.1125620715300634]
	TIME [epoch: 8.76 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10864983369932217		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 0.10096411383917958		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 0.1048069737692509 | validation: 0.09281538487161366]
	TIME [epoch: 8.79 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10613234032044175		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 0.1107419709195036		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 0.10843715561997269 | validation: 0.10687213266816424]
	TIME [epoch: 8.77 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10772620995505833		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 0.107088519585669		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 0.10740736477036368 | validation: 0.11827175886505342]
	TIME [epoch: 8.76 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10264384181870409		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 0.10105973998948632		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 0.10185179090409521 | validation: 0.10275643944649945]
	TIME [epoch: 8.77 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11449868029425384		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 0.1015933174806936		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 0.10804599888747371 | validation: 0.0937443848017363]
	TIME [epoch: 8.76 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10295746566591353		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 0.10878466070674757		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 0.10587106318633058 | validation: 0.07748367026162542]
	TIME [epoch: 8.76 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10742325374251453		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 0.0991343038078725		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 0.10327877877519351 | validation: 0.09820833152771091]
	TIME [epoch: 8.78 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10504573455071128		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 0.09209942728656825		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 0.09857258091863977 | validation: 0.0871301043873328]
	TIME [epoch: 8.77 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09927332942136931		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 0.11468470862407484		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 0.10697901902272207 | validation: 0.09092767788742111]
	TIME [epoch: 8.76 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09606783972265581		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 0.11402968113190919		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 0.1050487604272825 | validation: 0.09275701266416253]
	TIME [epoch: 8.75 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09912508413959802		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 0.10473688062213778		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 0.10193098238086791 | validation: 0.0974417365470596]
	TIME [epoch: 8.76 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09687612568328652		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 0.11676676653968346		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 0.106821446111485 | validation: 0.10583469981268248]
	TIME [epoch: 8.76 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09899608160883752		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 0.10668344505017328		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 0.1028397633295054 | validation: 0.08954980103890327]
	TIME [epoch: 8.78 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10223213073884097		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.09918259837920007		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 0.10070736455902052 | validation: 0.08726300154878394]
	TIME [epoch: 8.77 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1025854282255259		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 0.10526119287530789		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 0.1039233105504169 | validation: 0.10007377012906757]
	TIME [epoch: 8.77 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1096980292585779		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 0.10990737155987926		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 0.1098027004092286 | validation: 0.08668689992743643]
	TIME [epoch: 8.77 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10048073980370498		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 0.10344651639998088		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 0.10196362810184292 | validation: 0.11389616944316257]
	TIME [epoch: 8.76 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10050223126071725		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 0.1050259546187905		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 0.10276409293975389 | validation: 0.09234406677411756]
	TIME [epoch: 8.79 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10317201712509086		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 0.11360357942901707		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 0.10838779827705398 | validation: 0.09501883652312261]
	TIME [epoch: 8.76 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1016597171558911		[learning rate: 0.000419]
		[batch 20/20] avg loss: 0.09633465917978791		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 0.0989971881678395 | validation: 0.10233690225419759]
	TIME [epoch: 8.76 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11128446486219727		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 0.10390655239178308		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 0.10759550862699019 | validation: 0.09216801878469101]
	TIME [epoch: 8.77 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10208257761098194		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 0.09954819980534697		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 0.10081538870816445 | validation: 0.08166767386978908]
	TIME [epoch: 8.76 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10649862894106135		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.09726924434496134		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.10188393664301135 | validation: 0.09929386730058196]
	TIME [epoch: 8.77 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10044418800136348		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 0.11048849080961073		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 0.10546633940548708 | validation: 0.0872805072549528]
	TIME [epoch: 8.78 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11332450023819889		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.11453216644696304		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.11392833334258094 | validation: 0.10287466111964996]
	TIME [epoch: 8.77 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0938627196486508		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 0.10222413960703051		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 0.09804342962784066 | validation: 0.10413693540869953]
	TIME [epoch: 8.76 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11129074977121559		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 0.10866279366569156		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 0.10997677171845358 | validation: 0.09997042060710777]
	TIME [epoch: 8.76 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11112485096021055		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.11090542527131615		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.11101513811576333 | validation: 0.11620807805295695]
	TIME [epoch: 8.77 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10885966086154589		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 0.12003655115278385		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 0.11444810600716485 | validation: 0.09935714842981844]
	TIME [epoch: 8.78 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11040747301710208		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.1022968685274936		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 0.10635217077229783 | validation: 0.0961305593730394]
	TIME [epoch: 8.77 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10136965829056517		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 0.1096160267551689		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 0.10549284252286703 | validation: 0.10953481563447547]
	TIME [epoch: 8.76 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12649648456695362		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.11580052335128406		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 0.12114850395911883 | validation: 0.0948948920776781]
	TIME [epoch: 8.76 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10040475208506858		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 0.09620685200449355		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 0.09830580204478107 | validation: 0.09832225434126439]
	TIME [epoch: 8.76 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10310489077549882		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 0.09989540394496346		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 0.10150014736023114 | validation: 0.09564748372841497]
	TIME [epoch: 8.76 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10491314384466732		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 0.09953058127928242		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 0.10222186256197488 | validation: 0.08665446255000256]
	TIME [epoch: 8.79 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0924780815400177		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.1033390329682438		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 0.09790855725413075 | validation: 0.111974242160117]
	TIME [epoch: 8.76 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12180387157937804		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.10523053063588583		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.1135172011076319 | validation: 0.07598711820327203]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_722.pth
	Model improved!!!
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10655785967206492		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.11113936026308031		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.10884860996757265 | validation: 0.08006572878867119]
	TIME [epoch: 8.77 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09981882929582878		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 0.11179038416787293		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.10580460673185084 | validation: 0.10153416645292916]
	TIME [epoch: 8.76 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09856774164488273		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 0.09751943694885079		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 0.09804358929686677 | validation: 0.09009036444733662]
	TIME [epoch: 8.78 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10244568176030058		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.09940875591631708		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 0.10092721883830884 | validation: 0.12621340938678155]
	TIME [epoch: 8.78 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1045178056779267		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.09598413063078316		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.10025096815435494 | validation: 0.0891482266425656]
	TIME [epoch: 8.77 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10797112626218586		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.10630954981607368		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.10714033803912977 | validation: 0.08170345495539576]
	TIME [epoch: 8.76 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10701186985905153		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 0.09915708576263693		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 0.10308447781084422 | validation: 0.13920699115554705]
	TIME [epoch: 8.76 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.104847912240372		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.10575417022617843		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 0.10530104123327519 | validation: 0.07906796186920512]
	TIME [epoch: 8.76 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10349270396983015		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.1154068726169658		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.10944978829339795 | validation: 0.09308007216290612]
	TIME [epoch: 8.78 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11599351127571375		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.10693307821577958		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.11146329474574668 | validation: 0.08072541147818038]
	TIME [epoch: 8.76 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09674969890313266		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.10457036563202682		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.10066003226757976 | validation: 0.07940228291972519]
	TIME [epoch: 8.75 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09934188329985005		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.10666862034260446		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.10300525182122724 | validation: 0.09725488465226276]
	TIME [epoch: 8.76 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0967162792252535		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.10027874177899095		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.09849751050212222 | validation: 0.09393868271150274]
	TIME [epoch: 8.76 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09634378766356815		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.09545392885648747		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.09589885826002781 | validation: 0.08094972264084836]
	TIME [epoch: 8.76 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10265106910769776		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.10310820440620036		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.10287963675694907 | validation: 0.08485075333082051]
	TIME [epoch: 8.79 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09775005449063948		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.09657492648959329		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.09716249049011637 | validation: 0.08715243624080773]
	TIME [epoch: 8.76 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10065723110873066		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.09726318468612638		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.09896020789742853 | validation: 0.09880450175196977]
	TIME [epoch: 8.77 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0939830943664079		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 0.10175874010817361		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 0.09787091723729076 | validation: 0.10704067128163468]
	TIME [epoch: 8.77 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10687220228920039		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 0.10140438209627098		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 0.10413829219273565 | validation: 0.10039674986277981]
	TIME [epoch: 8.76 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10297622059302372		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 0.1177560872689671		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 0.11036615393099539 | validation: 0.1298732681159656]
	TIME [epoch: 8.78 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12074685419019186		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 0.09282974221444595		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 0.1067882982023189 | validation: 0.08711584061561775]
	TIME [epoch: 8.76 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09743609249390119		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 0.09976202485256676		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 0.09859905867323396 | validation: 0.10052771562320295]
	TIME [epoch: 8.76 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10219104796607266		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 0.09515775664386955		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 0.09867440230497113 | validation: 0.10951753090613608]
	TIME [epoch: 8.76 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09792763564606108		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 0.10521673214772691		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 0.101572183896894 | validation: 0.10763211173086557]
	TIME [epoch: 8.76 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1080382882246271		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 0.09778599600062758		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 0.10291214211262734 | validation: 0.08685948028933924]
	TIME [epoch: 8.76 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08642321526854813		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 0.1097793175025861		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 0.09810126638556711 | validation: 0.09414552332250015]
	TIME [epoch: 8.79 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09994719385427198		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 0.09999125411086374		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 0.09996922398256784 | validation: 0.08826560503457795]
	TIME [epoch: 8.76 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09688258932743855		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 0.1175218721797813		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 0.1072022307536099 | validation: 0.12708003529228534]
	TIME [epoch: 8.76 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10279619880019383		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 0.09363359426449272		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 0.09821489653234329 | validation: 0.11167652782540391]
	TIME [epoch: 8.76 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10223976965299324		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 0.103941429777758		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 0.1030905997153756 | validation: 0.10668180061214713]
	TIME [epoch: 8.76 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10351009987522639		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 0.10012094886081666		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 0.10181552436802155 | validation: 0.0964426798364332]
	TIME [epoch: 8.77 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09822187392017533		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 0.09458657414348773		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 0.09640422403183152 | validation: 0.08698817306500577]
	TIME [epoch: 8.78 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09450336025632652		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 0.10167315161743533		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 0.09808825593688095 | validation: 0.08525264695784868]
	TIME [epoch: 8.76 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09814908583423522		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 0.09921580230396813		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 0.09868244406910168 | validation: 0.09124334655240239]
	TIME [epoch: 8.76 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09449640449548322		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 0.10490183871078818		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 0.09969912160313568 | validation: 0.1069717281764476]
	TIME [epoch: 8.76 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10393000667962704		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 0.09604754712243056		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 0.0999887769010288 | validation: 0.08430750060765863]
	TIME [epoch: 8.76 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10257888627367329		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 0.10576796242538392		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 0.10417342434952859 | validation: 0.09209216858232257]
	TIME [epoch: 8.78 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10608018151262359		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 0.10586296737912804		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 0.10597157444587582 | validation: 0.1120254213578384]
	TIME [epoch: 8.76 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11468962586596136		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 0.10166555169154826		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 0.1081775887787548 | validation: 0.10072792394709992]
	TIME [epoch: 8.76 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10123386047323431		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 0.1185028040371999		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 0.10986833225521711 | validation: 0.10413291308786951]
	TIME [epoch: 8.76 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1030384515157738		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 0.09927274439780306		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 0.10115559795678841 | validation: 0.0968947581618943]
	TIME [epoch: 8.76 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10258698888551424		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 0.10008782272410122		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 0.10133740580480773 | validation: 0.09380815070365169]
	TIME [epoch: 8.77 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11727960374119131		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 0.1127721319638448		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 0.11502586785251803 | validation: 0.09577245418205063]
	TIME [epoch: 8.78 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10730061115322431		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 0.10820065151110225		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 0.1077506313321633 | validation: 0.09025698835927011]
	TIME [epoch: 8.76 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10327069040650019		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 0.11660216800987662		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 0.1099364292081884 | validation: 0.09657467570767696]
	TIME [epoch: 8.77 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10375340490232396		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 0.09974617335923698		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 0.1017497891307805 | validation: 0.10388204276015264]
	TIME [epoch: 8.78 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10467501677130556		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.10342433385238484		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.10404967531184521 | validation: 0.09659906651138124]
	TIME [epoch: 8.77 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09228434010784865		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 0.09962425740353198		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 0.09595429875569032 | validation: 0.08726588852866198]
	TIME [epoch: 8.77 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09755540398763253		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 0.0997572279547004		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 0.09865631597116647 | validation: 0.10708877741179636]
	TIME [epoch: 8.78 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10792498611560064		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 0.10454837652270117		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 0.10623668131915091 | validation: 0.10160233772774528]
	TIME [epoch: 8.76 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1062233282552079		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 0.10485505248974544		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 0.10553919037247668 | validation: 0.09036590023802964]
	TIME [epoch: 8.77 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09685638547176066		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 0.09940191910778928		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 0.09812915228977498 | validation: 0.0893536879967591]
	TIME [epoch: 8.76 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10118447663122306		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 0.1020977302693491		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 0.10164110345028607 | validation: 0.08439032598906737]
	TIME [epoch: 8.76 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960084961797809		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 0.09735840620353668		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 0.0966834511916588 | validation: 0.08579712050750204]
	TIME [epoch: 8.79 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09555017351427983		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 0.10497152626612094		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 0.10026084989020037 | validation: 0.08470794711544037]
	TIME [epoch: 8.76 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09389869477397696		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 0.11085262918285407		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 0.1023756619784155 | validation: 0.0781789679691696]
	TIME [epoch: 8.76 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0922376015570558		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 0.10014587226649858		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 0.09619173691177718 | validation: 0.10590066713270196]
	TIME [epoch: 8.76 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09898173850004152		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.1067225545961636		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 0.10285214654810255 | validation: 0.11397911657992482]
	TIME [epoch: 8.77 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10487959417492283		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.10363033173735872		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.10425496295614081 | validation: 0.08902341812188307]
	TIME [epoch: 8.78 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09413170268097348		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.10072956899592256		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.09743063583844801 | validation: 0.09293276027019978]
	TIME [epoch: 8.78 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0985070923117009		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 0.10804281633585922		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 0.10327495432378006 | validation: 0.07589656373837145]
	TIME [epoch: 8.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_783.pth
	Model improved!!!
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09552381005394467		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.10386805595016882		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.09969593300205676 | validation: 0.1003593664166868]
	TIME [epoch: 8.76 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11066579416636788		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 0.1072992951667318		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 0.10898254466654986 | validation: 0.09575007119588841]
	TIME [epoch: 8.76 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09889596830116855		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 0.09689809866805717		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 0.09789703348461286 | validation: 0.08611808798306586]
	TIME [epoch: 8.75 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09342363575693885		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.10053657850540895		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.0969801071311739 | validation: 0.09830758626297038]
	TIME [epoch: 8.78 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09892994594547957		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.09677411499669801		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.09785203047108881 | validation: 0.0960922582611411]
	TIME [epoch: 8.77 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09833217869305323		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.10127078775386737		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.0998014832234603 | validation: 0.09009144780632312]
	TIME [epoch: 8.76 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10132018098214102		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.11053147421895038		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.1059258276005457 | validation: 0.09546124179468418]
	TIME [epoch: 8.76 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1005763870749163		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.0955372642893049		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.09805682568211062 | validation: 0.08560350992246857]
	TIME [epoch: 8.76 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09265446814336864		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.10097365017404605		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.09681405915870735 | validation: 0.10035747553709728]
	TIME [epoch: 8.77 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10508093055800709		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.09489693151101619		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.09998893103451165 | validation: 0.08201036483245694]
	TIME [epoch: 8.78 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10128880316117168		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.10548122671073726		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.10338501493595445 | validation: 0.08255206000621962]
	TIME [epoch: 8.77 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10551445091235201		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.0998501941240948		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.1026823225182234 | validation: 0.0848725370700997]
	TIME [epoch: 8.77 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10062857743035589		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.09884123692798917		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.09973490717917252 | validation: 0.08763294666679228]
	TIME [epoch: 8.76 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09702279460843515		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.09432978529575062		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.09567628995209287 | validation: 0.07824632360458483]
	TIME [epoch: 8.76 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10170266643635265		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.10111536202237585		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.10140901422936424 | validation: 0.0906248089891523]
	TIME [epoch: 8.76 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10356892133618487		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.10223148734993996		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.10290020434306242 | validation: 0.09920529041436768]
	TIME [epoch: 8.77 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11108792931457445		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.09327236536845206		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.10218014734151326 | validation: 0.08811144608695991]
	TIME [epoch: 8.76 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09633536577704506		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.10205976492607309		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.09919756535155907 | validation: 0.08332266371758587]
	TIME [epoch: 8.76 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09536652679508108		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.10045854742585669		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.09791253711046889 | validation: 0.07615130410772475]
	TIME [epoch: 8.77 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09390188302456953		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.09679820607581553		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.09535004455019254 | validation: 0.0869840596574123]
	TIME [epoch: 8.75 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09867988982464433		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.09522854145320118		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.09695421563892276 | validation: 0.08315053742178617]
	TIME [epoch: 8.78 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10030433482919114		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.1013143755587607		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.10080935519397591 | validation: 0.09217810163784237]
	TIME [epoch: 8.77 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10589506927329588		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.09460953930718512		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.1002523042902405 | validation: 0.08303187242827818]
	TIME [epoch: 8.75 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09854334034804288		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.10237649953609784		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.10045991994207037 | validation: 0.09257525583692829]
	TIME [epoch: 8.77 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09765154778224108		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.09463300436648991		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.09614227607436547 | validation: 0.08841022403479216]
	TIME [epoch: 8.77 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09818644196827139		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.0956841776193329		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.09693530979380213 | validation: 0.10018041687816091]
	TIME [epoch: 8.78 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09810132231283492		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.09964886615005507		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.09887509423144501 | validation: 0.07360985463938385]
	TIME [epoch: 8.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_810.pth
	Model improved!!!
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09015157780853565		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.09906539982904673		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.0946084888187912 | validation: 0.08502975262635548]
	TIME [epoch: 8.76 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10295810143880872		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.09340132216101976		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.09817971179991425 | validation: 0.09135818292417325]
	TIME [epoch: 8.76 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09755812324314748		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.09573987110706678		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.09664899717510714 | validation: 0.08883908352377802]
	TIME [epoch: 8.75 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09417118441536773		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.10606326248966942		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.10011722345251857 | validation: 0.11479142752450094]
	TIME [epoch: 8.77 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10560275602064009		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.09800485404372537		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.10180380503218274 | validation: 0.09244581776859291]
	TIME [epoch: 8.78 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10832794201070595		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.0932856656672815		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.10080680383899372 | validation: 0.08985055454881835]
	TIME [epoch: 8.75 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09036033082026423		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.10305885396299415		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.09670959239162921 | validation: 0.08245467248352145]
	TIME [epoch: 8.76 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09542758199551726		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.09211244199920672		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.09377001199736199 | validation: 0.09145837297688886]
	TIME [epoch: 8.76 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09370238241097326		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.09691310878692169		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.09530774559894747 | validation: 0.08660597193088063]
	TIME [epoch: 8.76 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09699692889386502		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.09872744322593457		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.0978621860598998 | validation: 0.09599933305518492]
	TIME [epoch: 8.75 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09730102814151649		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.097107205557714		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.09720411684961525 | validation: 0.07573420554667837]
	TIME [epoch: 8.78 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09482927629818179		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.09566768903875673		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.09524848266846928 | validation: 0.0893191484191507]
	TIME [epoch: 8.76 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08746045144356525		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.10713748384076856		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.09729896764216692 | validation: 0.0766313052463983]
	TIME [epoch: 8.76 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10111847589564751		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.08976406783380857		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.09544127186472805 | validation: 0.10349790661600455]
	TIME [epoch: 8.76 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09753827761233617		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.10045027183271311		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.09899427472252464 | validation: 0.08029798230605308]
	TIME [epoch: 8.75 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09661439054342326		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 0.09142187107666314		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.0940181308100432 | validation: 0.08635053239306037]
	TIME [epoch: 8.77 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09756924335520792		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.0951803190613073		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.09637478120825761 | validation: 0.09649554412359854]
	TIME [epoch: 8.77 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09894753645115309		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.09860400279322078		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.09877576962218693 | validation: 0.10072940874835033]
	TIME [epoch: 8.76 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0979866070847553		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.09715448387266278		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.09757054547870905 | validation: 0.08732176187542944]
	TIME [epoch: 8.76 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0952695549919245		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.10347084049005528		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.0993701977409899 | validation: 0.10823210887622198]
	TIME [epoch: 8.76 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09114853025082956		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.10583257233213125		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.09849055129148039 | validation: 0.07859579559042182]
	TIME [epoch: 8.76 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09602279877388945		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.0962614824818282		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.09614214062785884 | validation: 0.0865463811529561]
	TIME [epoch: 8.78 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09172656230888342		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.10472147388627698		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.0982240180975802 | validation: 0.10307813058062382]
	TIME [epoch: 8.76 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1041774140934372		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.09590957531448545		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.10004349470396134 | validation: 0.09332997260769957]
	TIME [epoch: 8.76 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09301152925368084		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.100754972132937		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.09688325069330891 | validation: 0.09033263459889841]
	TIME [epoch: 8.76 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09749791105565012		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.09337793084684329		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.09543792095124667 | validation: 0.08184067087719787]
	TIME [epoch: 8.77 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09734036159798297		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.09487727654059343		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.0961088190692882 | validation: 0.08712529323968597]
	TIME [epoch: 8.77 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09599593063419842		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.0913741565023837		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.09368504356829105 | validation: 0.09309873793857155]
	TIME [epoch: 8.77 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09507014590944454		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.09516260030184717		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.09511637310564587 | validation: 0.08405796462263788]
	TIME [epoch: 8.76 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0924295145911552		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.08700637010700873		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.08971794234908197 | validation: 0.08094853464882792]
	TIME [epoch: 8.75 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09300349287706985		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.09082778562928007		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.09191563925317497 | validation: 0.08417499086483116]
	TIME [epoch: 8.76 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09594373124658365		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.09447228747698025		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.09520800936178195 | validation: 0.09141861215424482]
	TIME [epoch: 8.76 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10325762137289123		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.10095973251867196		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.1021086769457816 | validation: 0.08536978869272799]
	TIME [epoch: 8.78 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09921652060027558		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.10013892314062187		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.09967772187044872 | validation: 0.09367897539271106]
	TIME [epoch: 8.76 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09625136915609003		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.09402049282002616		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.0951359309880581 | validation: 0.08191826122756517]
	TIME [epoch: 8.75 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09680148446780049		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.10426311770800485		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.10053230108790268 | validation: 0.09195296132240935]
	TIME [epoch: 8.76 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09708179107917367		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.09529569210146571		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.0961887415903197 | validation: 0.08438893165361504]
	TIME [epoch: 8.75 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09734369576028155		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.09542559238520897		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.09638464407274527 | validation: 0.078567100741007]
	TIME [epoch: 8.76 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09314348854223622		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.09245509063554803		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.09279928958889214 | validation: 0.0818905278206131]
	TIME [epoch: 8.78 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09530039898265369		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.09531518863019203		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.09530779380642287 | validation: 0.09611934393507826]
	TIME [epoch: 8.77 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09482372338080926		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.09944736376911353		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.09713554357496137 | validation: 0.09841984236435183]
	TIME [epoch: 8.75 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09627618856634615		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.09222609338324889		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.0942511409747975 | validation: 0.0848579151967862]
	TIME [epoch: 8.75 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09636843551783		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.09491659735229359		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.09564251643506179 | validation: 0.08280052205745266]
	TIME [epoch: 8.75 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09505556686989562		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.09710733949496238		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.09608145318242899 | validation: 0.07627044534962926]
	TIME [epoch: 8.76 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09612616743334333		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.1056168253257308		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.10087149637953705 | validation: 0.09938084989402353]
	TIME [epoch: 8.76 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09542546340257177		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.09459061558943829		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.09500803949600503 | validation: 0.07278695529257022]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_856.pth
	Model improved!!!
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09905740361144413		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.0973006725387417		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.09817903807509291 | validation: 0.08629594175162864]
	TIME [epoch: 8.76 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10139224421025113		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.10202047036145054		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.10170635728585083 | validation: 0.0929832872761816]
	TIME [epoch: 8.76 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1006230956403276		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.09429879822199767		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.09746094693116265 | validation: 0.08281720780045074]
	TIME [epoch: 8.75 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09389439905148347		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.09725442468838139		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.09557441186993244 | validation: 0.08682381076523402]
	TIME [epoch: 8.78 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10044695147355773		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.09703764905407757		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.09874230026381764 | validation: 0.08961355408133961]
	TIME [epoch: 8.77 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10231586506477883		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.09185209648208295		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.09708398077343088 | validation: 0.09199350964982173]
	TIME [epoch: 8.76 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10010761818413867		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.09007798286525164		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.09509280052469518 | validation: 0.08335093609792783]
	TIME [epoch: 8.77 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09774124596877255		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.09692722633428405		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.09733423615152832 | validation: 0.09305024421472495]
	TIME [epoch: 8.75 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09568402531490916		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.09130108442451478		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.09349255486971197 | validation: 0.08120749114031003]
	TIME [epoch: 8.76 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09939252053318223		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.08941926593684402		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.09440589323501311 | validation: 0.07675883920400511]
	TIME [epoch: 8.78 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09402255891780169		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.0973812243778743		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.09570189164783802 | validation: 0.08938837624711994]
	TIME [epoch: 8.76 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08865645761362229		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.09737903971017334		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.09301774866189781 | validation: 0.07762878353648756]
	TIME [epoch: 8.76 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09640904548577317		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.09953396505469442		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.09797150527023381 | validation: 0.08318916765313736]
	TIME [epoch: 8.76 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09409257373960159		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.09836952764494969		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.09623105069227564 | validation: 0.08258242389050854]
	TIME [epoch: 8.77 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09115739641588991		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.10397830957023961		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.09756785299306475 | validation: 0.08353261766798661]
	TIME [epoch: 8.78 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09166472923753932		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.09446133917293494		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.09306303420523714 | validation: 0.09229075488292832]
	TIME [epoch: 8.77 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09644768259945367		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.10884873548457681		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.10264820904201526 | validation: 0.08602573066563146]
	TIME [epoch: 8.76 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09282547141012473		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.1014468195336529		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.09713614547188881 | validation: 0.0820152715139587]
	TIME [epoch: 8.77 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09595198070402908		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.09800042474889895		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.09697620272646404 | validation: 0.09111406035888256]
	TIME [epoch: 8.77 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09878471239239074		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.09240576115468213		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.09559523677353646 | validation: 0.0785634596567274]
	TIME [epoch: 8.76 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09682542659956214		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.09734927825759566		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.0970873524285789 | validation: 0.08804976992838018]
	TIME [epoch: 8.8 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09182848622502941		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.09642126517339084		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.09412487569921013 | validation: 0.08400983842615378]
	TIME [epoch: 8.76 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09029480582172653		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.09321489361218023		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.09175484971695336 | validation: 0.08002982647109727]
	TIME [epoch: 8.76 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09289676775392108		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.09380456399326433		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.09335066587359273 | validation: 0.08361210682677381]
	TIME [epoch: 8.76 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09259725707206021		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.09641109052863826		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.09450417380034923 | validation: 0.0836026729121501]
	TIME [epoch: 8.75 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1013806313588502		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.09576698196642269		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.09857380666263646 | validation: 0.08685555979724142]
	TIME [epoch: 8.77 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10016023929533321		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.09485903889705677		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.09750963909619499 | validation: 0.0889904948160519]
	TIME [epoch: 8.77 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10671552574840408		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.09111447513612599		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.09891500044226503 | validation: 0.09335223406667253]
	TIME [epoch: 8.77 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09482754983119204		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.09715040913943403		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.09598897948531304 | validation: 0.09354489856878667]
	TIME [epoch: 8.76 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10289811872327906		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.10108915121499182		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.10199363496913542 | validation: 0.09216853207126832]
	TIME [epoch: 8.76 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1000139207108528		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.10018355055846193		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.10009873563465732 | validation: 0.09501627379193385]
	TIME [epoch: 8.77 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0963428002708993		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.09308121498580858		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.09471200762835391 | validation: 0.0849068195934539]
	TIME [epoch: 8.78 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09708114563373986		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 0.09396932231219528		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 0.09552523397296757 | validation: 0.09214009012108967]
	TIME [epoch: 8.77 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09680872543831116		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 0.09786084513752322		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 0.0973347852879172 | validation: 0.08745930626723851]
	TIME [epoch: 8.77 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09459245135371425		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.09772430187824718		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.0961583766159807 | validation: 0.08416210644804786]
	TIME [epoch: 8.77 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09025087776821415		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.09288874133959006		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.09156980955390209 | validation: 0.08611275134727864]
	TIME [epoch: 8.76 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09606121955284566		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.099904566890393		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.09798289322161932 | validation: 0.09638024786865478]
	TIME [epoch: 8.76 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10639023881468587		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.09835563842818894		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.1023729386214374 | validation: 0.08786210990547083]
	TIME [epoch: 8.79 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09449862171340223		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.09084311395348114		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.0926708678334417 | validation: 0.08246509337615084]
	TIME [epoch: 8.76 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09811814145078178		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.09185247581636641		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.09498530863357409 | validation: 0.10210729332308156]
	TIME [epoch: 8.76 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09927241660123928		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 0.0913134555508707		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 0.095292936076055 | validation: 0.0880202931399183]
	TIME [epoch: 8.77 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09400903305437011		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 0.10102436446781489		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 0.09751669876109251 | validation: 0.10643302163749992]
	TIME [epoch: 8.76 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.104211381632846		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 0.10141271557557618		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 0.10281204860421111 | validation: 0.10411566323090338]
	TIME [epoch: 8.77 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10553051390759143		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 0.09397647940559671		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 0.09975349665659407 | validation: 0.08657318370792416]
	TIME [epoch: 8.78 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09053943570731211		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 0.09893882238391308		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 0.09473912904561257 | validation: 0.08563776219365446]
	TIME [epoch: 8.76 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09801822300499756		[learning rate: 0.00016124]
		[batch 20/20] avg loss: 0.09443899172534236		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 0.09622860736516996 | validation: 0.08463967769332156]
	TIME [epoch: 8.74 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09745638373508461		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 0.09386553752967464		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 0.09566096063237961 | validation: 0.0876248815805613]
	TIME [epoch: 8.76 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09320594914017447		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 0.09540860495137948		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 0.09430727704577695 | validation: 0.0864611124472605]
	TIME [epoch: 8.77 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09289215908338085		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 0.09758918109359782		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 0.09524067008848933 | validation: 0.08601153518256023]
	TIME [epoch: 8.79 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10127580654508246		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 0.09623262501255764		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 0.09875421577882004 | validation: 0.10334530590281207]
	TIME [epoch: 8.76 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09380385408185045		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 0.09294530631887651		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 0.09337458020036349 | validation: 0.07648317276291898]
	TIME [epoch: 8.75 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09370673418370071		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 0.09540167942685078		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 0.09455420680527572 | validation: 0.08728879210251189]
	TIME [epoch: 8.75 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10016498722685141		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 0.09679266937058498		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 0.09847882829871821 | validation: 0.0817609206370174]
	TIME [epoch: 8.75 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.099891502819352		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 0.08982184278306013		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 0.09485667280120605 | validation: 0.08523742849393065]
	TIME [epoch: 8.75 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08906448113079062		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 0.097085714551116		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 0.09307509784095332 | validation: 0.08303155311432234]
	TIME [epoch: 8.77 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09674052446815032		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 0.08922400686333723		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 0.09298226566574377 | validation: 0.08366233375237285]
	TIME [epoch: 8.75 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09184017499765787		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 0.09810869823429892		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 0.09497443661597839 | validation: 0.0932521382002959]
	TIME [epoch: 8.76 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10008378294123228		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 0.08782747140619582		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 0.09395562717371404 | validation: 0.08418979322124423]
	TIME [epoch: 8.75 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0958707627946624		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 0.09853881875668387		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 0.09720479077567312 | validation: 0.08069015317909611]
	TIME [epoch: 8.75 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09389944972506484		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 0.0906735362673016		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 0.09228649299618319 | validation: 0.08467045805954453]
	TIME [epoch: 8.78 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960543604530383		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 0.09333139549182583		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 0.09469287797243207 | validation: 0.08168603374564395]
	TIME [epoch: 8.76 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09713073219410236		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 0.09654120846665012		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 0.09683597033037623 | validation: 0.08266013217683771]
	TIME [epoch: 8.8 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09831691386710131		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 0.0948978369970474		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 0.09660737543207434 | validation: 0.08133382363736907]
	TIME [epoch: 8.75 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08877088639747506		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 0.0929714908371843		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 0.09087118861732968 | validation: 0.07870914253370914]
	TIME [epoch: 8.75 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09609288995763397		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 0.0917488768912024		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 0.0939208834244182 | validation: 0.09445860257760111]
	TIME [epoch: 8.75 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08879719862052596		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 0.09921245533304716		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 0.09400482697678655 | validation: 0.08029976063683374]
	TIME [epoch: 8.78 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09481164584022891		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 0.09637766816920433		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 0.0955946570047166 | validation: 0.09572730097050386]
	TIME [epoch: 8.75 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08775936137597502		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 0.09543076933287004		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 0.09159506535442251 | validation: 0.0851025514589205]
	TIME [epoch: 8.75 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08972975613135585		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 0.09482548138432652		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 0.09227761875784116 | validation: 0.08420970281083308]
	TIME [epoch: 8.76 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09325417250637791		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 0.09646215313590407		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 0.09485816282114098 | validation: 0.08692287162197221]
	TIME [epoch: 8.75 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08856083373557347		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 0.0943092299461934		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 0.09143503184088342 | validation: 0.09955372385107905]
	TIME [epoch: 8.76 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09873974459058964		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 0.09863963150558966		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 0.09868968804808965 | validation: 0.08249951469819256]
	TIME [epoch: 8.78 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08984469035697909		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 0.09385894334745135		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 0.09185181685221519 | validation: 0.08564551302658771]
	TIME [epoch: 8.75 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0903737309943365		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 0.09766028349163018		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 0.09401700724298333 | validation: 0.07956080088155874]
	TIME [epoch: 8.75 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09253663650367103		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 0.09795481627924127		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 0.09524572639145615 | validation: 0.08775563621878185]
	TIME [epoch: 8.76 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09937034647981355		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 0.10053280757246001		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 0.09995157702613676 | validation: 0.0810232970698489]
	TIME [epoch: 8.76 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09605004402156277		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 0.08924440819194322		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 0.092647226106753 | validation: 0.08245642539815679]
	TIME [epoch: 8.79 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09381011181684347		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 0.08786123794591338		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 0.09083567488137842 | validation: 0.08929544827780338]
	TIME [epoch: 8.76 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09562309210541549		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 0.09702721959780823		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 0.09632515585161186 | validation: 0.1058267749043656]
	TIME [epoch: 8.76 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09650617172183029		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 0.09634025203728001		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 0.09642321187955516 | validation: 0.08920252778570169]
	TIME [epoch: 8.75 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08336768960672178		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 0.09831682759260589		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 0.09084225859966384 | validation: 0.07990221601681644]
	TIME [epoch: 8.76 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09197330794752048		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 0.09651543057530473		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 0.09424436926141261 | validation: 0.09059151312803326]
	TIME [epoch: 8.76 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.098439982941588		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 0.0912297797801438		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 0.0948348813608659 | validation: 0.08617038659166999]
	TIME [epoch: 8.77 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09962175532800062		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 0.08595406982734047		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 0.09278791257767052 | validation: 0.08219598266279318]
	TIME [epoch: 8.75 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09481329924801599		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 0.09754053573506705		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 0.09617691749154152 | validation: 0.08715150735586163]
	TIME [epoch: 8.74 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09838780214828731		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 0.09287688862521276		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 0.09563234538675006 | validation: 0.08011350267276071]
	TIME [epoch: 8.76 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09506921704831577		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 0.09775478067453965		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 0.09641199886142769 | validation: 0.08557119759480969]
	TIME [epoch: 8.75 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09546286870461998		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 0.0965311210808845		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 0.09599699489275225 | validation: 0.09411170232987727]
	TIME [epoch: 8.77 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10141864626764648		[learning rate: 0.0001309]
		[batch 20/20] avg loss: 0.09622625367885562		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 0.09882244997325103 | validation: 0.08620240372006302]
	TIME [epoch: 8.76 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10025834671061004		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 0.10206757020148671		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 0.10116295845604839 | validation: 0.0868443423413188]
	TIME [epoch: 8.75 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09190955308401655		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 0.09050883597095427		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 0.09120919452748541 | validation: 0.07903322078812622]
	TIME [epoch: 8.76 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09046538510457708		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 0.09639657502547692		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 0.093430980065027 | validation: 0.07487891986095024]
	TIME [epoch: 8.75 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09277427601760889		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 0.08951899030767206		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 0.09114663316264049 | validation: 0.0822119783403785]
	TIME [epoch: 8.76 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09290119084029082		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 0.09136535785761205		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 0.09213327434895142 | validation: 0.09656629125220015]
	TIME [epoch: 8.78 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09759940874750916		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 0.09518627262515347		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 0.09639284068633133 | validation: 0.07800934873983431]
	TIME [epoch: 8.76 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09115990283359879		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 0.09169477865718786		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 0.09142734074539333 | validation: 0.0813428299824715]
	TIME [epoch: 8.77 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09422056582392793		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 0.09121897782305713		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 0.09271977182349254 | validation: 0.07767335479807705]
	TIME [epoch: 8.76 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1007967702228835		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 0.09447281428871132		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 0.09763479225579738 | validation: 0.08540903123419785]
	TIME [epoch: 8.76 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09139556814190346		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 0.0984806860230933		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 0.09493812708249835 | validation: 0.08799868702946129]
	TIME [epoch: 8.76 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0960697142065858		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 0.0936378239201229		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 0.09485376906335434 | validation: 0.08442493207616891]
	TIME [epoch: 8.76 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09203671543601806		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 0.08918544494482603		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 0.09061108019042205 | validation: 0.09123460703026165]
	TIME [epoch: 8.75 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09426290708503549		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 0.09053516135013161		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 0.09239903421758354 | validation: 0.09553394490624353]
	TIME [epoch: 8.76 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09167726886381815		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 0.09150729188733686		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 0.09159228037557751 | validation: 0.09309909298924147]
	TIME [epoch: 8.78 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09552244415264612		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 0.09013500078374719		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 0.09282872246819665 | validation: 0.087455459790545]
	TIME [epoch: 8.75 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09158943136475745		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.09192990640937417		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.0917596688870658 | validation: 0.0901585691621325]
	TIME [epoch: 8.78 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09482454641097575		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.09074642243511691		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.09278548442304634 | validation: 0.08807435779398239]
	TIME [epoch: 8.76 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09072844684863493		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.08975231129738012		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.09024037907300751 | validation: 0.08723484331059024]
	TIME [epoch: 8.76 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09569836086947703		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.09672125014953734		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.09620980550950717 | validation: 0.07181399469878716]
	TIME [epoch: 8.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r5_20240218_115024/states/model_tr_study1_964.pth
	Model improved!!!
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09422615097880116		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.09140140254279176		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.09281377676079647 | validation: 0.08130407041155818]
	TIME [epoch: 8.75 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08752998694949368		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.09491702389747277		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.0912235054234832 | validation: 0.09839137853621065]
	TIME [epoch: 8.76 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09543343305633759		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.0912252130235818		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.0933293230399597 | validation: 0.09416910307860846]
	TIME [epoch: 8.77 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08803839845380042		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.08634402184789598		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.0871912101508482 | validation: 0.0857585956818297]
	TIME [epoch: 8.75 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09170743186176315		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.09598855885944105		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.09384799536060212 | validation: 0.08146032071893775]
	TIME [epoch: 8.75 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08970944500456905		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.09887787493226882		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.09429365996841894 | validation: 0.09792134081397519]
	TIME [epoch: 8.75 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09742870041891286		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.0955694931080047		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.09649909676345877 | validation: 0.08691775008375706]
	TIME [epoch: 8.75 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10514683686091049		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.09823952312995529		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.1016931799954329 | validation: 0.08063972770489247]
	TIME [epoch: 8.77 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10104524205292265		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.09941710395750324		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.10023117300521293 | validation: 0.08359210512106346]
	TIME [epoch: 8.77 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09632018972430959		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.0938652299309197		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.09509270982761464 | validation: 0.07903634657648922]
	TIME [epoch: 8.75 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09413037756331159		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.08908583778703805		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.09160810767517483 | validation: 0.08525408780352667]
	TIME [epoch: 8.74 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09172560325691144		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.09625316872606429		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.09398938599148787 | validation: 0.09637496780455934]
	TIME [epoch: 8.76 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09621782190764996		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.09916041075715834		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.09768911633240415 | validation: 0.09390640799856381]
	TIME [epoch: 8.74 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0925837052807809		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.102627336845731		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.09760552106325594 | validation: 0.10317133561682049]
	TIME [epoch: 8.78 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09450450317985043		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.10309046310735245		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.09879748314360146 | validation: 0.09915937785788956]
	TIME [epoch: 8.75 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09068217569278955		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.08731664607755082		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.08899941088517019 | validation: 0.08897200158739307]
	TIME [epoch: 8.74 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09014347792883681		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.09420127272220455		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.09217237532552067 | validation: 0.09095875402817295]
	TIME [epoch: 8.74 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09161214419094424		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.09772976087555592		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.09467095253325007 | validation: 0.07974324372383843]
	TIME [epoch: 8.74 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08538757983726539		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.09472145630434889		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.09005451807080714 | validation: 0.0762681403701335]
	TIME [epoch: 8.76 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09358413090989826		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.0929458438516302		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.09326498738076422 | validation: 0.08148134974630203]
	TIME [epoch: 8.77 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0927875980093111		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.09579899658092776		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.09429329729511943 | validation: 0.07715130724779692]
	TIME [epoch: 8.76 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.089888832678264		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.1019631354060961		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.09592598404218004 | validation: 0.08047090530243048]
	TIME [epoch: 8.76 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0858375830549935		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.09787317429185614		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.09185537867342483 | validation: 0.08457423957395366]
	TIME [epoch: 8.76 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09555135288869107		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 0.09049633704469116		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 0.09302384496669112 | validation: 0.08295252565624825]
	TIME [epoch: 8.76 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08984732795328891		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 0.09537228731143488		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 0.09260980763236189 | validation: 0.08068516284942778]
	TIME [epoch: 8.78 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09194321119359063		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 0.09827401854158838		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 0.09510861486758951 | validation: 0.08439647817850152]
	TIME [epoch: 8.75 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09189135316268414		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 0.09369672957422569		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 0.0927940413684549 | validation: 0.08268384442086651]
	TIME [epoch: 8.74 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09643957992400796		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 0.09498519355191638		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 0.09571238673796216 | validation: 0.08940551465253013]
	TIME [epoch: 8.75 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0953902255172872		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 0.097093975406315		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 0.0962421004618011 | validation: 0.08006793258408096]
	TIME [epoch: 8.75 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0954112884227071		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 0.09651737630307905		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 0.09596433236289306 | validation: 0.09246758723443625]
	TIME [epoch: 8.75 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09973013852107113		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 0.0968553114685958		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 0.09829272499483346 | validation: 0.08671474888048222]
	TIME [epoch: 8.78 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09635728900235359		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 0.09306869445132154		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 0.09471299172683756 | validation: 0.08937699621876866]
	TIME [epoch: 8.76 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09244928758707857		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 0.08984298706581348		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 0.09114613732644603 | validation: 0.09047495045667431]
	TIME [epoch: 8.76 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09491138603853798		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 0.08850507489821098		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 0.09170823046837448 | validation: 0.08763419270266652]
	TIME [epoch: 8.71 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0898640496529504		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 0.08925166220493515		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 0.08955785592894276 | validation: 0.08428883260606053]
	TIME [epoch: 8.68 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0941511782624678		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 0.09336961574331333		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 0.09376039700289056 | validation: 0.08274690563801143]
	TIME [epoch: 8.71 sec]
Finished training in 8869.369 seconds.
