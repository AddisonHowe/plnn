Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r3', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3068463815

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.18976686406793		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.059672589869509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.624719726968722 | validation: 7.48903582887132]
	TIME [epoch: 79.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.579334125267838		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.783343348134702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.181338736701269 | validation: 7.12450770080115]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.392787482875228		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.905528601317224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.149158042096225 | validation: 6.321543787340241]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.821410984211353		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.820966537691947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.8211887609516495 | validation: 6.165662869858532]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.752582396993643		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.707652035280209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.730117216136927 | validation: 6.272824194677362]
	TIME [epoch: 8.34 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.670966771417011		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.749113624697893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.710040198057451 | validation: 6.365323021242125]
	TIME [epoch: 8.33 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.846110891320419		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.578160027549449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.712135459434933 | validation: 6.366870765405533]
	TIME [epoch: 8.33 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.462771068313789		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.95441069961561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.708590883964699 | validation: 7.528621920392156]
	TIME [epoch: 8.32 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.626163541523008		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.634630241399954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.630396891461482 | validation: 6.1179978467179374]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.453922368313033		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.67229837355819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.563110370935611 | validation: 5.994420408994493]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.558907706501264		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.472869803685272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.515888755093269 | validation: 6.201082584169945]
	TIME [epoch: 8.32 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.593192147186741		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.640169490782432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6166808189845865 | validation: 6.061762738839635]
	TIME [epoch: 8.31 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.6040465610178005		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.5212179407751645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.562632250896482 | validation: 6.139788636698149]
	TIME [epoch: 8.35 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.399969558884407		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.535380155047394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.4676748569659015 | validation: 5.991769303333728]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.56392995050855		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.393124516937858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.478527233723203 | validation: 5.966347469104795]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.506437077708145		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.352456267731195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.429446672719671 | validation: 6.195499025725464]
	TIME [epoch: 8.33 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.231373930007921		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.575441842974607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.403407886491264 | validation: 6.293572533660772]
	TIME [epoch: 8.34 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.485981848603646		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.3036232150559295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.394802531829788 | validation: 5.984773036705007]
	TIME [epoch: 8.33 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.265483608043619		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.300931715569441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.2832076618065305 | validation: 5.721962067261659]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.274531715449649		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.668646535364584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.9715891254071165 | validation: 7.149396460214284]
	TIME [epoch: 8.32 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.6391781491447315		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.071967317519744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.355572733332238 | validation: 3.9268693834052772]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.351923458279171		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.3191804869626083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8355519726208898 | validation: 2.262025188995733]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.210008648518021		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6098247055836357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.909916677050828 | validation: 2.1499080617851885]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.116456711218958		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2928312703892058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2046439908040822 | validation: 2.130943069356631]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9680297831954927		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1255792022713473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0468044927334206 | validation: 1.7096500686273217]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0629487847345245		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5818338919807011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.822391338357613 | validation: 1.4033318373482981]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7064028292051685		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.892042210741355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7992225199732619 | validation: 1.2318432457928217]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7067499000927178		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.921733177085791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.814241538589254 | validation: 1.639119080504837]
	TIME [epoch: 8.31 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8207417655357054		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1195649432717696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9701533544037377 | validation: 1.405182029828366]
	TIME [epoch: 8.3 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.807841605961844		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7074485097765895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7576450578692167 | validation: 1.0988453339006612]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6485585895678732		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6214951971039298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6350268933359016 | validation: 1.4793941486201487]
	TIME [epoch: 8.31 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4633717314687977		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5664195386354929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.514895635052145 | validation: 1.0908055149935878]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4770332254708252		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5639056598548993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.520469442662862 | validation: 1.502838210651872]
	TIME [epoch: 8.31 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4212019236475717		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.246328487607259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3337652056274147 | validation: 1.3812349097134582]
	TIME [epoch: 8.33 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.432889455128855		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4342248518140313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.433557153471443 | validation: 1.4884229314032884]
	TIME [epoch: 8.31 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4589974372277577		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8800528827001233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6695251599639402 | validation: 1.289999696939364]
	TIME [epoch: 8.3 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5088957368928686		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2213859746443567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3651408557686127 | validation: 1.7017372041793948]
	TIME [epoch: 8.31 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.468043506913362		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4449274459231114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4564854764182367 | validation: 1.052790869985336]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.442952835565592		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4540077809966867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4484803082811393 | validation: 1.1968647623580222]
	TIME [epoch: 8.32 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1827612814968762		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.433788471025581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3082748762612286 | validation: 1.2051424965526292]
	TIME [epoch: 8.31 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4677130267336593		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7000739492399544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5838934879868072 | validation: 1.336269507196192]
	TIME [epoch: 8.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4451125966635483		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4017271661430104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4234198814032795 | validation: 1.22441122198101]
	TIME [epoch: 8.3 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2659687584591892		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2100458161959626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2380072873275758 | validation: 1.2178697151351612]
	TIME [epoch: 8.33 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2611685100344132		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3199718733559396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2905701916951764 | validation: 1.129947805031408]
	TIME [epoch: 8.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5493018711275295		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.374868699983119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4620852855553246 | validation: 1.5984329158784885]
	TIME [epoch: 8.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4316961444286909		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3148728313749793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.373284487901835 | validation: 1.5121492258013556]
	TIME [epoch: 8.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3710176273884147		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1184208620446083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2447192447165114 | validation: 0.8018200761738771]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0922152079214682		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.46504517347929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.278630190700379 | validation: 1.3208011936612944]
	TIME [epoch: 8.33 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2685064463419962		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.440991784247823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3547491152949098 | validation: 1.0170470691318398]
	TIME [epoch: 8.33 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.101921628477533		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1045077986468996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1032147135622166 | validation: 0.6838399708879381]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0988094381302007		[learning rate: 0.0099894]
		[batch 20/20] avg loss: 1.3157144636301508		[learning rate: 0.0099776]
	Learning Rate: 0.00997759
	LOSS [training: 1.207261950880176 | validation: 2.2703957855142094]
	TIME [epoch: 8.35 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.55339411156544		[learning rate: 0.0099658]
		[batch 20/20] avg loss: 1.5259177723949868		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.5396559419802132 | validation: 0.9138313213406881]
	TIME [epoch: 8.34 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5466121220463247		[learning rate: 0.0099423]
		[batch 20/20] avg loss: 1.3380528482478435		[learning rate: 0.0099306]
	Learning Rate: 0.00993057
	LOSS [training: 1.4423324851470842 | validation: 1.1976206359338792]
	TIME [epoch: 8.33 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8812979569412527		[learning rate: 0.0099189]
		[batch 20/20] avg loss: 1.1221547622428105		[learning rate: 0.0099071]
	Learning Rate: 0.00990715
	LOSS [training: 1.0017263595920314 | validation: 1.081546938834548]
	TIME [epoch: 8.33 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1257258653029392		[learning rate: 0.0098955]
		[batch 20/20] avg loss: 1.1087443406967528		[learning rate: 0.0098838]
	Learning Rate: 0.00988378
	LOSS [training: 1.1172351029998462 | validation: 0.8612424231096781]
	TIME [epoch: 8.33 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.008293599346975		[learning rate: 0.0098721]
		[batch 20/20] avg loss: 1.1975156201833674		[learning rate: 0.0098605]
	Learning Rate: 0.00986047
	LOSS [training: 1.102904609765171 | validation: 0.986060785721167]
	TIME [epoch: 8.35 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2068590029124853		[learning rate: 0.0098488]
		[batch 20/20] avg loss: 1.1027769091236952		[learning rate: 0.0098372]
	Learning Rate: 0.00983721
	LOSS [training: 1.1548179560180902 | validation: 1.2745382635603597]
	TIME [epoch: 8.33 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.027627346721398		[learning rate: 0.0098256]
		[batch 20/20] avg loss: 1.1752820948997666		[learning rate: 0.009814]
	Learning Rate: 0.009814
	LOSS [training: 1.1014547208105825 | validation: 0.9466474916226473]
	TIME [epoch: 8.32 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.089681715203031		[learning rate: 0.0098024]
		[batch 20/20] avg loss: 1.022429626932717		[learning rate: 0.0097909]
	Learning Rate: 0.00979085
	LOSS [training: 1.056055671067874 | validation: 1.8317554091392316]
	TIME [epoch: 8.32 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1913067311617391		[learning rate: 0.0097793]
		[batch 20/20] avg loss: 1.2873693087939735		[learning rate: 0.0097678]
	Learning Rate: 0.00976776
	LOSS [training: 1.2393380199778565 | validation: 1.06524240590368]
	TIME [epoch: 8.35 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2874846356284455		[learning rate: 0.0097562]
		[batch 20/20] avg loss: 1.1466275355818416		[learning rate: 0.0097447]
	Learning Rate: 0.00974472
	LOSS [training: 1.2170560856051433 | validation: 1.013907935331858]
	TIME [epoch: 8.33 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0653855396151166		[learning rate: 0.0097332]
		[batch 20/20] avg loss: 0.9505741518004662		[learning rate: 0.0097217]
	Learning Rate: 0.00972173
	LOSS [training: 1.0079798457077915 | validation: 1.6222721048378972]
	TIME [epoch: 8.33 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.21123331732099		[learning rate: 0.0097103]
		[batch 20/20] avg loss: 0.830900367685872		[learning rate: 0.0096988]
	Learning Rate: 0.0096988
	LOSS [training: 1.0210668425034308 | validation: 0.9723185766570452]
	TIME [epoch: 8.32 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8793667919739361		[learning rate: 0.0096874]
		[batch 20/20] avg loss: 1.0526673156486426		[learning rate: 0.0096759]
	Learning Rate: 0.00967592
	LOSS [training: 0.9660170538112895 | validation: 0.7880891457247533]
	TIME [epoch: 8.35 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1964072127150924		[learning rate: 0.0096645]
		[batch 20/20] avg loss: 0.9387528097486475		[learning rate: 0.0096531]
	Learning Rate: 0.0096531
	LOSS [training: 1.0675800112318699 | validation: 0.8097717686087079]
	TIME [epoch: 8.34 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8475189542979094		[learning rate: 0.0096417]
		[batch 20/20] avg loss: 0.8727928566132478		[learning rate: 0.0096303]
	Learning Rate: 0.00963033
	LOSS [training: 0.8601559054555785 | validation: 1.2972394207584035]
	TIME [epoch: 8.32 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9332394702549193		[learning rate: 0.009619]
		[batch 20/20] avg loss: 1.1952372307545058		[learning rate: 0.0096076]
	Learning Rate: 0.00960761
	LOSS [training: 1.0642383505047126 | validation: 0.4206582019981244]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.838510241113123		[learning rate: 0.0095963]
		[batch 20/20] avg loss: 0.9769495894137199		[learning rate: 0.0095849]
	Learning Rate: 0.00958495
	LOSS [training: 0.9077299152634216 | validation: 1.2472667659889176]
	TIME [epoch: 8.32 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9786562146695402		[learning rate: 0.0095736]
		[batch 20/20] avg loss: 0.9927676015451832		[learning rate: 0.0095623]
	Learning Rate: 0.00956234
	LOSS [training: 0.9857119081073616 | validation: 0.7389999042644194]
	TIME [epoch: 8.34 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0816147161180951		[learning rate: 0.0095511]
		[batch 20/20] avg loss: 0.9175999683290905		[learning rate: 0.0095398]
	Learning Rate: 0.00953978
	LOSS [training: 0.9996073422235927 | validation: 0.5657744098199238]
	TIME [epoch: 8.32 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8481367941168025		[learning rate: 0.0095285]
		[batch 20/20] avg loss: 1.0083246435979032		[learning rate: 0.0095173]
	Learning Rate: 0.00951728
	LOSS [training: 0.928230718857353 | validation: 1.246613412402255]
	TIME [epoch: 8.31 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.985371874138583		[learning rate: 0.009506]
		[batch 20/20] avg loss: 1.012949209792961		[learning rate: 0.0094948]
	Learning Rate: 0.00949483
	LOSS [training: 0.999160541965772 | validation: 0.7350838155554476]
	TIME [epoch: 8.31 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7598743168095976		[learning rate: 0.0094836]
		[batch 20/20] avg loss: 0.8652074465758137		[learning rate: 0.0094724]
	Learning Rate: 0.00947243
	LOSS [training: 0.8125408816927058 | validation: 0.79381755469074]
	TIME [epoch: 8.34 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9978498767410879		[learning rate: 0.0094613]
		[batch 20/20] avg loss: 0.6894615332292958		[learning rate: 0.0094501]
	Learning Rate: 0.00945009
	LOSS [training: 0.8436557049851918 | validation: 0.8859301538713829]
	TIME [epoch: 8.32 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.959826337517149		[learning rate: 0.0094389]
		[batch 20/20] avg loss: 0.8878571632183296		[learning rate: 0.0094278]
	Learning Rate: 0.0094278
	LOSS [training: 0.9238417503677393 | validation: 0.6957343976478243]
	TIME [epoch: 8.31 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8150042405244946		[learning rate: 0.0094167]
		[batch 20/20] avg loss: 0.7498730625357185		[learning rate: 0.0094056]
	Learning Rate: 0.00940556
	LOSS [training: 0.7824386515301065 | validation: 0.7578093516289266]
	TIME [epoch: 8.32 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6293088862401401		[learning rate: 0.0093945]
		[batch 20/20] avg loss: 0.9251557435662041		[learning rate: 0.0093834]
	Learning Rate: 0.00938337
	LOSS [training: 0.7772323149031723 | validation: 1.1710948228246043]
	TIME [epoch: 8.33 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1510515876596876		[learning rate: 0.0093723]
		[batch 20/20] avg loss: 0.8076766367147339		[learning rate: 0.0093612]
	Learning Rate: 0.00936124
	LOSS [training: 0.979364112187211 | validation: 1.2510484612324384]
	TIME [epoch: 8.33 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.125235944008406		[learning rate: 0.0093502]
		[batch 20/20] avg loss: 0.8507431094992199		[learning rate: 0.0093392]
	Learning Rate: 0.00933916
	LOSS [training: 0.9879895267538131 | validation: 0.6172600156846377]
	TIME [epoch: 8.32 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7415696537777376		[learning rate: 0.0093281]
		[batch 20/20] avg loss: 1.3438917701085615		[learning rate: 0.0093171]
	Learning Rate: 0.00931713
	LOSS [training: 1.0427307119431497 | validation: 0.9137834569322365]
	TIME [epoch: 8.32 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0455399208867682		[learning rate: 0.0093061]
		[batch 20/20] avg loss: 0.7756046368972844		[learning rate: 0.0092951]
	Learning Rate: 0.00929515
	LOSS [training: 0.9105722788920263 | validation: 0.6118566273356912]
	TIME [epoch: 8.32 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7624486541012796		[learning rate: 0.0092842]
		[batch 20/20] avg loss: 0.8415141180122264		[learning rate: 0.0092732]
	Learning Rate: 0.00927322
	LOSS [training: 0.8019813860567531 | validation: 0.6595688492483505]
	TIME [epoch: 8.34 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7483130815858898		[learning rate: 0.0092623]
		[batch 20/20] avg loss: 0.6885875593105555		[learning rate: 0.0092514]
	Learning Rate: 0.00925135
	LOSS [training: 0.7184503204482225 | validation: 0.379816539449689]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7098210475058307		[learning rate: 0.0092404]
		[batch 20/20] avg loss: 0.7373612871715837		[learning rate: 0.0092295]
	Learning Rate: 0.00922953
	LOSS [training: 0.7235911673387073 | validation: 1.0868602466330355]
	TIME [epoch: 8.32 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.722694649318672		[learning rate: 0.0092186]
		[batch 20/20] avg loss: 0.7622965835278067		[learning rate: 0.0092078]
	Learning Rate: 0.00920776
	LOSS [training: 0.7424956164232392 | validation: 0.5936364784220124]
	TIME [epoch: 8.31 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.721025119802523		[learning rate: 0.0091969]
		[batch 20/20] avg loss: 0.6582354815767134		[learning rate: 0.009186]
	Learning Rate: 0.00918604
	LOSS [training: 0.6896303006896184 | validation: 0.6534133824363888]
	TIME [epoch: 8.34 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7484966908956425		[learning rate: 0.0091752]
		[batch 20/20] avg loss: 0.5782503622789814		[learning rate: 0.0091644]
	Learning Rate: 0.00916437
	LOSS [training: 0.6633735265873121 | validation: 0.8611607280003697]
	TIME [epoch: 8.31 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7996675127506874		[learning rate: 0.0091536]
		[batch 20/20] avg loss: 0.6791684067337679		[learning rate: 0.0091428]
	Learning Rate: 0.00914275
	LOSS [training: 0.7394179597422276 | validation: 0.5512734658902687]
	TIME [epoch: 8.31 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7711590834714892		[learning rate: 0.009132]
		[batch 20/20] avg loss: 0.7645984792700043		[learning rate: 0.0091212]
	Learning Rate: 0.00912119
	LOSS [training: 0.7678787813707468 | validation: 0.8648685539689673]
	TIME [epoch: 8.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6978608961051828		[learning rate: 0.0091104]
		[batch 20/20] avg loss: 0.7049706871172166		[learning rate: 0.0090997]
	Learning Rate: 0.00909967
	LOSS [training: 0.7014157916111995 | validation: 0.6622976488855187]
	TIME [epoch: 8.32 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.741978701567175		[learning rate: 0.0090889]
		[batch 20/20] avg loss: 0.9263399776026402		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.8341593395849076 | validation: 0.603267806342337]
	TIME [epoch: 8.32 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7587692064684513		[learning rate: 0.0090675]
		[batch 20/20] avg loss: 0.6359422224299731		[learning rate: 0.0090568]
	Learning Rate: 0.00905679
	LOSS [training: 0.6973557144492122 | validation: 1.1075584489891692]
	TIME [epoch: 8.31 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7304296655957849		[learning rate: 0.0090461]
		[batch 20/20] avg loss: 0.6366718190549664		[learning rate: 0.0090354]
	Learning Rate: 0.00903543
	LOSS [training: 0.6835507423253757 | validation: 0.5787496081682922]
	TIME [epoch: 8.31 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7016845257568749		[learning rate: 0.0090248]
		[batch 20/20] avg loss: 0.8657445788617449		[learning rate: 0.0090141]
	Learning Rate: 0.00901411
	LOSS [training: 0.78371455230931 | validation: 0.41096913841502647]
	TIME [epoch: 8.31 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6211069206474269		[learning rate: 0.0090035]
		[batch 20/20] avg loss: 0.775039156651701		[learning rate: 0.0089929]
	Learning Rate: 0.00899285
	LOSS [training: 0.6980730386495638 | validation: 0.9642222949356258]
	TIME [epoch: 8.34 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7732389627381518		[learning rate: 0.0089822]
		[batch 20/20] avg loss: 0.7619980661393246		[learning rate: 0.0089716]
	Learning Rate: 0.00897164
	LOSS [training: 0.7676185144387382 | validation: 0.4098489394204723]
	TIME [epoch: 8.31 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.735240089121654		[learning rate: 0.0089611]
		[batch 20/20] avg loss: 0.6061385386622553		[learning rate: 0.0089505]
	Learning Rate: 0.00895048
	LOSS [training: 0.6706893138919546 | validation: 0.5300383267868708]
	TIME [epoch: 8.31 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.753515200138615		[learning rate: 0.0089399]
		[batch 20/20] avg loss: 0.6587980464567071		[learning rate: 0.0089294]
	Learning Rate: 0.00892936
	LOSS [training: 0.7061566232976609 | validation: 0.37436771087720044]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7206952738097273		[learning rate: 0.0089188]
		[batch 20/20] avg loss: 0.7877178895187754		[learning rate: 0.0089083]
	Learning Rate: 0.0089083
	LOSS [training: 0.7542065816642513 | validation: 0.7029272724481371]
	TIME [epoch: 8.34 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7189364797864732		[learning rate: 0.0088978]
		[batch 20/20] avg loss: 0.6165139513388642		[learning rate: 0.0088873]
	Learning Rate: 0.00888729
	LOSS [training: 0.6677252155626686 | validation: 0.7873194172189497]
	TIME [epoch: 8.31 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5829397389652637		[learning rate: 0.0088768]
		[batch 20/20] avg loss: 0.5606064602218617		[learning rate: 0.0088663]
	Learning Rate: 0.00886632
	LOSS [training: 0.5717730995935626 | validation: 0.8156044462022766]
	TIME [epoch: 8.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6490574507280648		[learning rate: 0.0088559]
		[batch 20/20] avg loss: 0.594558802303975		[learning rate: 0.0088454]
	Learning Rate: 0.00884541
	LOSS [training: 0.6218081265160198 | validation: 0.5728535317610497]
	TIME [epoch: 8.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5982369083593395		[learning rate: 0.008835]
		[batch 20/20] avg loss: 0.6007390409960779		[learning rate: 0.0088245]
	Learning Rate: 0.00882454
	LOSS [training: 0.5994879746777088 | validation: 0.9582662562328825]
	TIME [epoch: 8.32 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5597999856324295		[learning rate: 0.0088141]
		[batch 20/20] avg loss: 0.5262823573417378		[learning rate: 0.0088037]
	Learning Rate: 0.00880373
	LOSS [training: 0.5430411714870836 | validation: 0.5975397542322639]
	TIME [epoch: 8.32 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5657318927963393		[learning rate: 0.0087933]
		[batch 20/20] avg loss: 0.5754867765314848		[learning rate: 0.008783]
	Learning Rate: 0.00878296
	LOSS [training: 0.5706093346639121 | validation: 0.41290808140861646]
	TIME [epoch: 8.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5709018977294169		[learning rate: 0.0087726]
		[batch 20/20] avg loss: 0.5627660887485453		[learning rate: 0.0087622]
	Learning Rate: 0.00876225
	LOSS [training: 0.5668339932389812 | validation: 0.3562450734712179]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.575789507670579		[learning rate: 0.0087519]
		[batch 20/20] avg loss: 0.8334812329525176		[learning rate: 0.0087416]
	Learning Rate: 0.00874158
	LOSS [training: 0.7046353703115483 | validation: 0.46409113702473315]
	TIME [epoch: 8.31 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5370439151466775		[learning rate: 0.0087313]
		[batch 20/20] avg loss: 0.4984818293847423		[learning rate: 0.008721]
	Learning Rate: 0.00872096
	LOSS [training: 0.5177628722657099 | validation: 0.7710610336497907]
	TIME [epoch: 8.33 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6257718075689195		[learning rate: 0.0087107]
		[batch 20/20] avg loss: 0.5376583809305566		[learning rate: 0.0087004]
	Learning Rate: 0.00870038
	LOSS [training: 0.581715094249738 | validation: 0.7162588246574548]
	TIME [epoch: 8.31 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0347346720004436		[learning rate: 0.0086901]
		[batch 20/20] avg loss: 0.5040746107355767		[learning rate: 0.0086799]
	Learning Rate: 0.00867986
	LOSS [training: 0.7694046413680102 | validation: 0.4612808415461391]
	TIME [epoch: 8.31 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5369383805005592		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.5901469214047277		[learning rate: 0.0086594]
	Learning Rate: 0.00865939
	LOSS [training: 0.5635426509526436 | validation: 0.66052110741856]
	TIME [epoch: 8.31 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6139232246925472		[learning rate: 0.0086492]
		[batch 20/20] avg loss: 1.0310954092376388		[learning rate: 0.008639]
	Learning Rate: 0.00863896
	LOSS [training: 0.8225093169650929 | validation: 0.3123424557754303]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6757402092603865		[learning rate: 0.0086288]
		[batch 20/20] avg loss: 0.6821640835293075		[learning rate: 0.0086186]
	Learning Rate: 0.00861858
	LOSS [training: 0.6789521463948469 | validation: 0.9990748542109531]
	TIME [epoch: 8.33 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8187799423026633		[learning rate: 0.0086084]
		[batch 20/20] avg loss: 0.586868639920722		[learning rate: 0.0085983]
	Learning Rate: 0.00859825
	LOSS [training: 0.7028242911116926 | validation: 0.7449529016248682]
	TIME [epoch: 8.33 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7215662041581623		[learning rate: 0.0085881]
		[batch 20/20] avg loss: 0.5909927014778026		[learning rate: 0.008578]
	Learning Rate: 0.00857797
	LOSS [training: 0.6562794528179824 | validation: 0.45386314113137954]
	TIME [epoch: 8.32 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5083273741636692		[learning rate: 0.0085678]
		[batch 20/20] avg loss: 0.7520900682866964		[learning rate: 0.0085577]
	Learning Rate: 0.00855774
	LOSS [training: 0.6302087212251829 | validation: 0.5939098133615559]
	TIME [epoch: 8.34 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5653294999255754		[learning rate: 0.0085476]
		[batch 20/20] avg loss: 0.5701672293035811		[learning rate: 0.0085376]
	Learning Rate: 0.00853755
	LOSS [training: 0.5677483646145782 | validation: 0.45169891926404115]
	TIME [epoch: 8.32 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.551248584587937		[learning rate: 0.0085275]
		[batch 20/20] avg loss: 0.5441918582110534		[learning rate: 0.0085174]
	Learning Rate: 0.00851741
	LOSS [training: 0.5477202213994952 | validation: 0.5820273322417777]
	TIME [epoch: 8.32 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5480735142708222		[learning rate: 0.0085074]
		[batch 20/20] avg loss: 0.5869265804485897		[learning rate: 0.0084973]
	Learning Rate: 0.00849732
	LOSS [training: 0.5675000473597058 | validation: 0.9216857130267166]
	TIME [epoch: 8.32 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6723690017529739		[learning rate: 0.0084873]
		[batch 20/20] avg loss: 0.4364441969619315		[learning rate: 0.0084773]
	Learning Rate: 0.00847728
	LOSS [training: 0.5544065993574526 | validation: 0.5607592753574977]
	TIME [epoch: 8.32 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6063317170455965		[learning rate: 0.0084673]
		[batch 20/20] avg loss: 0.7376305198746176		[learning rate: 0.0084573]
	Learning Rate: 0.00845728
	LOSS [training: 0.671981118460107 | validation: 0.9501194098579728]
	TIME [epoch: 8.36 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5467412924838025		[learning rate: 0.0084473]
		[batch 20/20] avg loss: 0.6550451553784971		[learning rate: 0.0084373]
	Learning Rate: 0.00843733
	LOSS [training: 0.6008932239311497 | validation: 0.39828283383196583]
	TIME [epoch: 8.33 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5997559328951537		[learning rate: 0.0084274]
		[batch 20/20] avg loss: 0.567566756840238		[learning rate: 0.0084174]
	Learning Rate: 0.00841743
	LOSS [training: 0.5836613448676958 | validation: 0.37653487618461473]
	TIME [epoch: 8.32 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5722672044285406		[learning rate: 0.0084075]
		[batch 20/20] avg loss: 0.6217946167728811		[learning rate: 0.0083976]
	Learning Rate: 0.00839757
	LOSS [training: 0.5970309106007108 | validation: 0.31028180971362374]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.721628906706795		[learning rate: 0.0083877]
		[batch 20/20] avg loss: 0.5462712823184913		[learning rate: 0.0083778]
	Learning Rate: 0.00837777
	LOSS [training: 0.6339500945126432 | validation: 0.41268806876836817]
	TIME [epoch: 8.32 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5124136884180536		[learning rate: 0.0083679]
		[batch 20/20] avg loss: 0.5477864472023569		[learning rate: 0.008358]
	Learning Rate: 0.008358
	LOSS [training: 0.5301000678102052 | validation: 0.730414171931617]
	TIME [epoch: 8.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.460924635640967		[learning rate: 0.0083481]
		[batch 20/20] avg loss: 0.5788267169192733		[learning rate: 0.0083383]
	Learning Rate: 0.00833829
	LOSS [training: 0.5198756762801201 | validation: 0.8265650957795323]
	TIME [epoch: 8.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6212612123883752		[learning rate: 0.0083284]
		[batch 20/20] avg loss: 0.6085474152608787		[learning rate: 0.0083186]
	Learning Rate: 0.00831862
	LOSS [training: 0.614904313824627 | validation: 0.5031065332752883]
	TIME [epoch: 8.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6098682134677698		[learning rate: 0.0083088]
		[batch 20/20] avg loss: 0.5486025490138559		[learning rate: 0.008299]
	Learning Rate: 0.008299
	LOSS [training: 0.5792353812408129 | validation: 0.473829385405949]
	TIME [epoch: 8.36 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4383741056410287		[learning rate: 0.0082892]
		[batch 20/20] avg loss: 0.4947641691421004		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.46656913739156447 | validation: 0.47609908494232245]
	TIME [epoch: 8.35 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45601438387272675		[learning rate: 0.0082697]
		[batch 20/20] avg loss: 0.5859433042399937		[learning rate: 0.0082599]
	Learning Rate: 0.00825989
	LOSS [training: 0.5209788440563603 | validation: 0.39611204854772675]
	TIME [epoch: 8.31 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.445025551233767		[learning rate: 0.0082501]
		[batch 20/20] avg loss: 0.5430710058709282		[learning rate: 0.0082404]
	Learning Rate: 0.00824041
	LOSS [training: 0.49404827855234756 | validation: 0.777525008580437]
	TIME [epoch: 8.33 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6200937652195019		[learning rate: 0.0082307]
		[batch 20/20] avg loss: 0.47554074868122476		[learning rate: 0.008221]
	Learning Rate: 0.00822097
	LOSS [training: 0.5478172569503632 | validation: 0.6753538518035058]
	TIME [epoch: 8.32 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5714863686951277		[learning rate: 0.0082113]
		[batch 20/20] avg loss: 0.48999045848566264		[learning rate: 0.0082016]
	Learning Rate: 0.00820158
	LOSS [training: 0.5307384135903951 | validation: 0.4342621952683818]
	TIME [epoch: 8.33 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5356673018304567		[learning rate: 0.0081919]
		[batch 20/20] avg loss: 0.5437125997663211		[learning rate: 0.0081822]
	Learning Rate: 0.00818223
	LOSS [training: 0.539689950798389 | validation: 0.5613453453199058]
	TIME [epoch: 8.34 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5041108840384643		[learning rate: 0.0081726]
		[batch 20/20] avg loss: 0.4623444131637033		[learning rate: 0.0081629]
	Learning Rate: 0.00816293
	LOSS [training: 0.4832276486010838 | validation: 0.5107241919032172]
	TIME [epoch: 8.32 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5636460008481047		[learning rate: 0.0081533]
		[batch 20/20] avg loss: 0.47664394241886754		[learning rate: 0.0081437]
	Learning Rate: 0.00814368
	LOSS [training: 0.520144971633486 | validation: 0.45718954434097914]
	TIME [epoch: 8.32 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5426543352589531		[learning rate: 0.0081341]
		[batch 20/20] avg loss: 0.5657781328269387		[learning rate: 0.0081245]
	Learning Rate: 0.00812447
	LOSS [training: 0.554216234042946 | validation: 0.48490125495788855]
	TIME [epoch: 8.33 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6057996883813235		[learning rate: 0.0081149]
		[batch 20/20] avg loss: 0.5042970957814562		[learning rate: 0.0081053]
	Learning Rate: 0.0081053
	LOSS [training: 0.5550483920813898 | validation: 0.7293272408544547]
	TIME [epoch: 8.32 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5058977824863798		[learning rate: 0.0080957]
		[batch 20/20] avg loss: 0.46380765210393016		[learning rate: 0.0080862]
	Learning Rate: 0.00808618
	LOSS [training: 0.48485271729515506 | validation: 0.3466031880360613]
	TIME [epoch: 8.32 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5984997899990943		[learning rate: 0.0080766]
		[batch 20/20] avg loss: 0.6055737683123995		[learning rate: 0.0080671]
	Learning Rate: 0.00806711
	LOSS [training: 0.602036779155747 | validation: 0.821790413362341]
	TIME [epoch: 8.31 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48789853189518373		[learning rate: 0.0080576]
		[batch 20/20] avg loss: 0.5979438379078316		[learning rate: 0.0080481]
	Learning Rate: 0.00804808
	LOSS [training: 0.5429211849015075 | validation: 0.4494955822855498]
	TIME [epoch: 8.33 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5231463037103955		[learning rate: 0.0080386]
		[batch 20/20] avg loss: 0.41574478075926946		[learning rate: 0.0080291]
	Learning Rate: 0.0080291
	LOSS [training: 0.4694455422348325 | validation: 0.24433261000720527]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6173454431399409		[learning rate: 0.0080196]
		[batch 20/20] avg loss: 0.44908719189336227		[learning rate: 0.0080102]
	Learning Rate: 0.00801016
	LOSS [training: 0.5332163175166518 | validation: 0.33797630536016643]
	TIME [epoch: 8.32 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4992437246676375		[learning rate: 0.0080007]
		[batch 20/20] avg loss: 0.4764280657195491		[learning rate: 0.0079913]
	Learning Rate: 0.00799126
	LOSS [training: 0.4878358951935934 | validation: 0.7488948487180576]
	TIME [epoch: 8.31 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5687402473895956		[learning rate: 0.0079818]
		[batch 20/20] avg loss: 0.5368192544133163		[learning rate: 0.0079724]
	Learning Rate: 0.00797241
	LOSS [training: 0.5527797509014559 | validation: 0.37078091592893175]
	TIME [epoch: 8.31 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48909993426927956		[learning rate: 0.007963]
		[batch 20/20] avg loss: 0.5355551562449942		[learning rate: 0.0079536]
	Learning Rate: 0.00795361
	LOSS [training: 0.5123275452571369 | validation: 0.2632573256651924]
	TIME [epoch: 8.34 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4808427116242594		[learning rate: 0.0079442]
		[batch 20/20] avg loss: 0.5174781459000818		[learning rate: 0.0079348]
	Learning Rate: 0.00793485
	LOSS [training: 0.4991604287621706 | validation: 0.48524020766804765]
	TIME [epoch: 8.31 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4748584661527879		[learning rate: 0.0079255]
		[batch 20/20] avg loss: 0.639688386565407		[learning rate: 0.0079161]
	Learning Rate: 0.00791613
	LOSS [training: 0.5572734263590975 | validation: 0.5157729362559315]
	TIME [epoch: 8.3 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5754878702869587		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.6086204232788879		[learning rate: 0.0078975]
	Learning Rate: 0.00789746
	LOSS [training: 0.5920541467829233 | validation: 0.7429194085269761]
	TIME [epoch: 8.32 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5341458823057954		[learning rate: 0.0078881]
		[batch 20/20] avg loss: 0.47320834319721766		[learning rate: 0.0078788]
	Learning Rate: 0.00787883
	LOSS [training: 0.5036771127515065 | validation: 0.508421740464253]
	TIME [epoch: 8.33 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5153642428786885		[learning rate: 0.0078695]
		[batch 20/20] avg loss: 0.48411408328886035		[learning rate: 0.0078602]
	Learning Rate: 0.00786024
	LOSS [training: 0.4997391630837744 | validation: 0.3394208303319677]
	TIME [epoch: 8.31 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5621003200995489		[learning rate: 0.007851]
		[batch 20/20] avg loss: 0.5062184045827897		[learning rate: 0.0078417]
	Learning Rate: 0.0078417
	LOSS [training: 0.5341593623411692 | validation: 0.478983060847832]
	TIME [epoch: 8.31 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5233434576273241		[learning rate: 0.0078324]
		[batch 20/20] avg loss: 0.5900074549689805		[learning rate: 0.0078232]
	Learning Rate: 0.0078232
	LOSS [training: 0.5566754562981524 | validation: 0.712231588536971]
	TIME [epoch: 8.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5765368601340757		[learning rate: 0.007814]
		[batch 20/20] avg loss: 0.45813557401774413		[learning rate: 0.0078047]
	Learning Rate: 0.00780475
	LOSS [training: 0.5173362170759099 | validation: 0.41590611217629114]
	TIME [epoch: 8.31 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44269229215230865		[learning rate: 0.0077955]
		[batch 20/20] avg loss: 0.41112218082765695		[learning rate: 0.0077863]
	Learning Rate: 0.00778634
	LOSS [training: 0.42690723648998274 | validation: 0.8758921505477482]
	TIME [epoch: 8.32 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7268695569042998		[learning rate: 0.0077772]
		[batch 20/20] avg loss: 0.4533187329861647		[learning rate: 0.007768]
	Learning Rate: 0.00776797
	LOSS [training: 0.5900941449452322 | validation: 0.3314730900527184]
	TIME [epoch: 8.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5032373043807652		[learning rate: 0.0077588]
		[batch 20/20] avg loss: 0.4262102867354344		[learning rate: 0.0077496]
	Learning Rate: 0.00774965
	LOSS [training: 0.4647237955580998 | validation: 0.919094969602674]
	TIME [epoch: 8.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5036776637165546		[learning rate: 0.0077405]
		[batch 20/20] avg loss: 0.4972436214199508		[learning rate: 0.0077314]
	Learning Rate: 0.00773137
	LOSS [training: 0.5004606425682527 | validation: 0.3585912269119458]
	TIME [epoch: 8.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46726875180235006		[learning rate: 0.0077222]
		[batch 20/20] avg loss: 0.47452945448497286		[learning rate: 0.0077131]
	Learning Rate: 0.00771313
	LOSS [training: 0.47089910314366146 | validation: 0.4436436564292304]
	TIME [epoch: 8.32 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47826879011031964		[learning rate: 0.007704]
		[batch 20/20] avg loss: 0.3811706391350326		[learning rate: 0.0076949]
	Learning Rate: 0.00769494
	LOSS [training: 0.429719714622676 | validation: 0.8883839429265227]
	TIME [epoch: 8.31 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.506918616751586		[learning rate: 0.0076859]
		[batch 20/20] avg loss: 0.4805010116390619		[learning rate: 0.0076768]
	Learning Rate: 0.00767679
	LOSS [training: 0.49370981419532384 | validation: 0.2642046697582516]
	TIME [epoch: 8.31 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49001877169151536		[learning rate: 0.0076677]
		[batch 20/20] avg loss: 0.6104382737788819		[learning rate: 0.0076587]
	Learning Rate: 0.00765868
	LOSS [training: 0.5502285227351986 | validation: 0.4636002040726283]
	TIME [epoch: 8.31 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4466892220661169		[learning rate: 0.0076496]
		[batch 20/20] avg loss: 0.40343384808167776		[learning rate: 0.0076406]
	Learning Rate: 0.00764061
	LOSS [training: 0.4250615350738974 | validation: 0.6623407006068613]
	TIME [epoch: 8.33 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41613303276694225		[learning rate: 0.0076316]
		[batch 20/20] avg loss: 0.44044223132399063		[learning rate: 0.0076226]
	Learning Rate: 0.00762259
	LOSS [training: 0.42828763204546655 | validation: 0.4574806830590513]
	TIME [epoch: 8.31 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5967093447346908		[learning rate: 0.0076136]
		[batch 20/20] avg loss: 0.4362546331176206		[learning rate: 0.0076046]
	Learning Rate: 0.00760461
	LOSS [training: 0.5164819889261556 | validation: 0.34112092840825525]
	TIME [epoch: 8.31 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5402361818951992		[learning rate: 0.0075956]
		[batch 20/20] avg loss: 0.4155111241053313		[learning rate: 0.0075867]
	Learning Rate: 0.00758667
	LOSS [training: 0.4778736530002653 | validation: 0.5133111075844068]
	TIME [epoch: 8.31 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3977984645164437		[learning rate: 0.0075777]
		[batch 20/20] avg loss: 0.39370561205252064		[learning rate: 0.0075688]
	Learning Rate: 0.00756878
	LOSS [training: 0.39575203828448213 | validation: 0.36366260663611566]
	TIME [epoch: 8.31 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5111536586047684		[learning rate: 0.0075598]
		[batch 20/20] avg loss: 0.5630106666633244		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.5370821626340463 | validation: 0.2941718058908728]
	TIME [epoch: 8.33 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45778413003786345		[learning rate: 0.007542]
		[batch 20/20] avg loss: 0.43716059523557177		[learning rate: 0.0075331]
	Learning Rate: 0.00753311
	LOSS [training: 0.4474723626367177 | validation: 0.33679805755139997]
	TIME [epoch: 8.31 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4118451936338043		[learning rate: 0.0075242]
		[batch 20/20] avg loss: 0.4317307453395448		[learning rate: 0.0075153]
	Learning Rate: 0.00751534
	LOSS [training: 0.42178796948667446 | validation: 0.5226648097432716]
	TIME [epoch: 8.31 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38684137945852604		[learning rate: 0.0075065]
		[batch 20/20] avg loss: 0.40600530364188714		[learning rate: 0.0074976]
	Learning Rate: 0.00749761
	LOSS [training: 0.39642334155020653 | validation: 0.3170937325354967]
	TIME [epoch: 8.31 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3477933953442005		[learning rate: 0.0074888]
		[batch 20/20] avg loss: 0.40500652576131363		[learning rate: 0.0074799]
	Learning Rate: 0.00747993
	LOSS [training: 0.37639996055275704 | validation: 0.4539570236140958]
	TIME [epoch: 8.34 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41964104274752023		[learning rate: 0.0074711]
		[batch 20/20] avg loss: 0.4339733509076488		[learning rate: 0.0074623]
	Learning Rate: 0.00746228
	LOSS [training: 0.4268071968275845 | validation: 0.9248516192864209]
	TIME [epoch: 8.31 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6086085517401341		[learning rate: 0.0074535]
		[batch 20/20] avg loss: 0.40897690542998416		[learning rate: 0.0074447]
	Learning Rate: 0.00744468
	LOSS [training: 0.508792728585059 | validation: 0.5326418130600445]
	TIME [epoch: 8.31 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4831403060124658		[learning rate: 0.0074359]
		[batch 20/20] avg loss: 0.4738013985677285		[learning rate: 0.0074271]
	Learning Rate: 0.00742712
	LOSS [training: 0.4784708522900972 | validation: 0.7251629226485861]
	TIME [epoch: 8.31 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44786872181625126		[learning rate: 0.0074184]
		[batch 20/20] avg loss: 0.422873863276498		[learning rate: 0.0074096]
	Learning Rate: 0.0074096
	LOSS [training: 0.43537129254637463 | validation: 0.1845167469292467]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6222607316756953		[learning rate: 0.0074009]
		[batch 20/20] avg loss: 0.45568308401692886		[learning rate: 0.0073921]
	Learning Rate: 0.00739212
	LOSS [training: 0.538971907846312 | validation: 0.33902788361326913]
	TIME [epoch: 8.32 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37196290478817157		[learning rate: 0.0073834]
		[batch 20/20] avg loss: 0.4203105225239134		[learning rate: 0.0073747]
	Learning Rate: 0.00737469
	LOSS [training: 0.3961367136560424 | validation: 0.6834816083930096]
	TIME [epoch: 8.31 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43082530389098944		[learning rate: 0.007366]
		[batch 20/20] avg loss: 0.4369132935812572		[learning rate: 0.0073573]
	Learning Rate: 0.00735729
	LOSS [training: 0.4338692987361233 | validation: 0.25033391399038885]
	TIME [epoch: 8.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45911909478031054		[learning rate: 0.0073486]
		[batch 20/20] avg loss: 0.37241896348081677		[learning rate: 0.0073399]
	Learning Rate: 0.00733994
	LOSS [training: 0.4157690291305636 | validation: 0.6946669349660097]
	TIME [epoch: 8.31 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4792869578046853		[learning rate: 0.0073313]
		[batch 20/20] avg loss: 0.4948754589500707		[learning rate: 0.0073226]
	Learning Rate: 0.00732262
	LOSS [training: 0.48708120837737806 | validation: 0.40138348681689895]
	TIME [epoch: 8.33 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.489717884486605		[learning rate: 0.007314]
		[batch 20/20] avg loss: 0.35223390690118006		[learning rate: 0.0073053]
	Learning Rate: 0.00730535
	LOSS [training: 0.42097589569389243 | validation: 0.3176721918401043]
	TIME [epoch: 8.31 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3442532648301582		[learning rate: 0.0072967]
		[batch 20/20] avg loss: 0.43075283815718163		[learning rate: 0.0072881]
	Learning Rate: 0.00728812
	LOSS [training: 0.38750305149366987 | validation: 0.5876036153300424]
	TIME [epoch: 8.31 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45969376417165037		[learning rate: 0.0072795]
		[batch 20/20] avg loss: 0.47200768781040187		[learning rate: 0.0072709]
	Learning Rate: 0.00727093
	LOSS [training: 0.4658507259910262 | validation: 0.24517639024507393]
	TIME [epoch: 8.3 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41413315059992745		[learning rate: 0.0072623]
		[batch 20/20] avg loss: 0.3946875635942769		[learning rate: 0.0072538]
	Learning Rate: 0.00725377
	LOSS [training: 0.40441035709710216 | validation: 0.37538677731467646]
	TIME [epoch: 8.33 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37527960374671077		[learning rate: 0.0072452]
		[batch 20/20] avg loss: 0.4502299157599291		[learning rate: 0.0072367]
	Learning Rate: 0.00723666
	LOSS [training: 0.41275475975332 | validation: 0.34538660250006264]
	TIME [epoch: 8.31 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5427324414636931		[learning rate: 0.0072281]
		[batch 20/20] avg loss: 0.41456054398845354		[learning rate: 0.0072196]
	Learning Rate: 0.00721959
	LOSS [training: 0.47864649272607335 | validation: 0.23888957661808574]
	TIME [epoch: 8.31 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35374181573842983		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.43595358079755125		[learning rate: 0.0072026]
	Learning Rate: 0.00720256
	LOSS [training: 0.3948476982679905 | validation: 0.20328827026951315]
	TIME [epoch: 8.31 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33661323954930233		[learning rate: 0.0071941]
		[batch 20/20] avg loss: 0.4666732062193283		[learning rate: 0.0071856]
	Learning Rate: 0.00718558
	LOSS [training: 0.4016432228843153 | validation: 0.30954439718514276]
	TIME [epoch: 8.32 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41698349258860146		[learning rate: 0.0071771]
		[batch 20/20] avg loss: 0.4574039479443511		[learning rate: 0.0071686]
	Learning Rate: 0.00716863
	LOSS [training: 0.43719372026647624 | validation: 0.4748047722796548]
	TIME [epoch: 8.33 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33190903399012284		[learning rate: 0.0071602]
		[batch 20/20] avg loss: 0.536284741933738		[learning rate: 0.0071517]
	Learning Rate: 0.00715172
	LOSS [training: 0.43409688796193036 | validation: 0.44322074578358467]
	TIME [epoch: 8.31 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4694587861445664		[learning rate: 0.0071433]
		[batch 20/20] avg loss: 0.45404230681074126		[learning rate: 0.0071348]
	Learning Rate: 0.00713485
	LOSS [training: 0.46175054647765396 | validation: 0.32042423037742285]
	TIME [epoch: 8.31 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39257122610284406		[learning rate: 0.0071264]
		[batch 20/20] avg loss: 0.515893420145598		[learning rate: 0.007118]
	Learning Rate: 0.00711802
	LOSS [training: 0.45423232312422107 | validation: 0.33237577916596395]
	TIME [epoch: 8.31 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4379826491473814		[learning rate: 0.0071096]
		[batch 20/20] avg loss: 0.3517156801066563		[learning rate: 0.0071012]
	Learning Rate: 0.00710123
	LOSS [training: 0.39484916462701897 | validation: 0.24775101257082732]
	TIME [epoch: 8.33 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3833909349743366		[learning rate: 0.0070928]
		[batch 20/20] avg loss: 0.46004718155379865		[learning rate: 0.0070845]
	Learning Rate: 0.00708447
	LOSS [training: 0.4217190582640676 | validation: 0.39944544207812316]
	TIME [epoch: 8.31 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4204631572654753		[learning rate: 0.0070761]
		[batch 20/20] avg loss: 0.4033156615527407		[learning rate: 0.0070678]
	Learning Rate: 0.00706776
	LOSS [training: 0.411889409409108 | validation: 0.36784628900875393]
	TIME [epoch: 8.31 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.475038323560112		[learning rate: 0.0070594]
		[batch 20/20] avg loss: 0.3662274630562244		[learning rate: 0.0070511]
	Learning Rate: 0.00705109
	LOSS [training: 0.4206328933081682 | validation: 0.35029961639327467]
	TIME [epoch: 8.31 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34316184339702877		[learning rate: 0.0070428]
		[batch 20/20] avg loss: 0.40013188974849356		[learning rate: 0.0070345]
	Learning Rate: 0.00703446
	LOSS [training: 0.37164686657276114 | validation: 0.26198254760958206]
	TIME [epoch: 8.33 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3884257229471484		[learning rate: 0.0070262]
		[batch 20/20] avg loss: 0.39844996739042465		[learning rate: 0.0070179]
	Learning Rate: 0.00701787
	LOSS [training: 0.39343784516878655 | validation: 0.27046237411754787]
	TIME [epoch: 8.31 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4445860028712617		[learning rate: 0.0070096]
		[batch 20/20] avg loss: 0.39912229143575895		[learning rate: 0.0070013]
	Learning Rate: 0.00700131
	LOSS [training: 0.42185414715351033 | validation: 0.2500771899164288]
	TIME [epoch: 8.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3891013709465202		[learning rate: 0.006993]
		[batch 20/20] avg loss: 0.4263795970421766		[learning rate: 0.0069848]
	Learning Rate: 0.0069848
	LOSS [training: 0.4077404839943483 | validation: 0.42259779283720467]
	TIME [epoch: 8.31 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3893400519446857		[learning rate: 0.0069766]
		[batch 20/20] avg loss: 0.33262277152821407		[learning rate: 0.0069683]
	Learning Rate: 0.00696832
	LOSS [training: 0.36098141173645 | validation: 0.5435430777033154]
	TIME [epoch: 8.33 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4205815089002948		[learning rate: 0.0069601]
		[batch 20/20] avg loss: 0.4901076465949661		[learning rate: 0.0069519]
	Learning Rate: 0.00695188
	LOSS [training: 0.4553445777476305 | validation: 0.292886329569046]
	TIME [epoch: 8.32 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36885394314169695		[learning rate: 0.0069437]
		[batch 20/20] avg loss: 0.4513987748078863		[learning rate: 0.0069355]
	Learning Rate: 0.00693549
	LOSS [training: 0.4101263589747915 | validation: 0.5124560422078055]
	TIME [epoch: 8.31 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38066256771854656		[learning rate: 0.0069273]
		[batch 20/20] avg loss: 0.4391703692785024		[learning rate: 0.0069191]
	Learning Rate: 0.00691913
	LOSS [training: 0.40991646849852437 | validation: 0.22351217657329053]
	TIME [epoch: 8.32 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32465110472868175		[learning rate: 0.006911]
		[batch 20/20] avg loss: 0.35705820329639437		[learning rate: 0.0069028]
	Learning Rate: 0.00690281
	LOSS [training: 0.34085465401253806 | validation: 0.3391884064947375]
	TIME [epoch: 8.31 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48860657241089606		[learning rate: 0.0068947]
		[batch 20/20] avg loss: 0.5356283879509375		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.5121174801809169 | validation: 0.3977571931036153]
	TIME [epoch: 8.33 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42283014450681033		[learning rate: 0.0068784]
		[batch 20/20] avg loss: 0.3863037976426996		[learning rate: 0.0068703]
	Learning Rate: 0.00687028
	LOSS [training: 0.404566971074755 | validation: 0.39650444858231076]
	TIME [epoch: 8.31 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4604848513044226		[learning rate: 0.0068622]
		[batch 20/20] avg loss: 0.33858042442428815		[learning rate: 0.0068541]
	Learning Rate: 0.00685407
	LOSS [training: 0.39953263786435533 | validation: 0.6101871950101422]
	TIME [epoch: 8.31 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4953512436620171		[learning rate: 0.006846]
		[batch 20/20] avg loss: 0.35592122470129023		[learning rate: 0.0068379]
	Learning Rate: 0.0068379
	LOSS [training: 0.4256362341816537 | validation: 0.40092562905757984]
	TIME [epoch: 8.31 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3743486208608999		[learning rate: 0.0068298]
		[batch 20/20] avg loss: 0.3642837749793513		[learning rate: 0.0068218]
	Learning Rate: 0.00682178
	LOSS [training: 0.3693161979201256 | validation: 0.5475898286251104]
	TIME [epoch: 8.33 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3673953699360566		[learning rate: 0.0068137]
		[batch 20/20] avg loss: 0.3828802918237454		[learning rate: 0.0068057]
	Learning Rate: 0.00680568
	LOSS [training: 0.375137830879901 | validation: 0.5383112310073179]
	TIME [epoch: 8.31 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3739992887871625		[learning rate: 0.0067977]
		[batch 20/20] avg loss: 0.3925438228761263		[learning rate: 0.0067896]
	Learning Rate: 0.00678963
	LOSS [training: 0.3832715558316444 | validation: 0.47991831421831854]
	TIME [epoch: 8.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5539549051545681		[learning rate: 0.0067816]
		[batch 20/20] avg loss: 0.3935432569857092		[learning rate: 0.0067736]
	Learning Rate: 0.00677361
	LOSS [training: 0.47374908107013863 | validation: 0.5254319665286514]
	TIME [epoch: 8.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4415775876915805		[learning rate: 0.0067656]
		[batch 20/20] avg loss: 0.31392285221934246		[learning rate: 0.0067576]
	Learning Rate: 0.00675764
	LOSS [training: 0.3777502199554615 | validation: 0.4640912101620133]
	TIME [epoch: 8.31 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39324952230972476		[learning rate: 0.0067497]
		[batch 20/20] avg loss: 0.43071088150314446		[learning rate: 0.0067417]
	Learning Rate: 0.0067417
	LOSS [training: 0.41198020190643464 | validation: 0.421097003082748]
	TIME [epoch: 8.32 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.359001318173782		[learning rate: 0.0067337]
		[batch 20/20] avg loss: 0.4426678870835219		[learning rate: 0.0067258]
	Learning Rate: 0.00672579
	LOSS [training: 0.400834602628652 | validation: 0.2967636361502425]
	TIME [epoch: 8.31 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3982875798324855		[learning rate: 0.0067179]
		[batch 20/20] avg loss: 0.39908565191211476		[learning rate: 0.0067099]
	Learning Rate: 0.00670993
	LOSS [training: 0.39868661587230014 | validation: 0.5036444885570043]
	TIME [epoch: 8.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35948526931646607		[learning rate: 0.006702]
		[batch 20/20] avg loss: 0.3001488949580188		[learning rate: 0.0066941]
	Learning Rate: 0.0066941
	LOSS [training: 0.3298170821372425 | validation: 0.23594684504278168]
	TIME [epoch: 8.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4293535451265866		[learning rate: 0.0066862]
		[batch 20/20] avg loss: 0.3058397135182617		[learning rate: 0.0066783]
	Learning Rate: 0.00667831
	LOSS [training: 0.3675966293224242 | validation: 0.4087087003979917]
	TIME [epoch: 8.33 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4463701618203412		[learning rate: 0.0066704]
		[batch 20/20] avg loss: 0.3362503392184085		[learning rate: 0.0066626]
	Learning Rate: 0.00666256
	LOSS [training: 0.39131025051937485 | validation: 0.23149215148576624]
	TIME [epoch: 8.31 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3558964287927405		[learning rate: 0.0066547]
		[batch 20/20] avg loss: 0.40624166385315197		[learning rate: 0.0066468]
	Learning Rate: 0.00664684
	LOSS [training: 0.38106904632294625 | validation: 0.2716401710283488]
	TIME [epoch: 8.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40169127371413105		[learning rate: 0.006639]
		[batch 20/20] avg loss: 0.32216881987607854		[learning rate: 0.0066312]
	Learning Rate: 0.00663116
	LOSS [training: 0.3619300467951047 | validation: 0.2978886379241807]
	TIME [epoch: 8.31 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3855226682430263		[learning rate: 0.0066233]
		[batch 20/20] avg loss: 0.4356840035954628		[learning rate: 0.0066155]
	Learning Rate: 0.00661552
	LOSS [training: 0.41060333591924447 | validation: 0.2513591267964338]
	TIME [epoch: 8.33 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3492127100456803		[learning rate: 0.0066077]
		[batch 20/20] avg loss: 0.42597146898827665		[learning rate: 0.0065999]
	Learning Rate: 0.00659992
	LOSS [training: 0.38759208951697854 | validation: 0.2854168080903854]
	TIME [epoch: 8.31 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.320705481138451		[learning rate: 0.0065921]
		[batch 20/20] avg loss: 0.41873137211330114		[learning rate: 0.0065843]
	Learning Rate: 0.00658435
	LOSS [training: 0.36971842662587606 | validation: 0.41991798575002304]
	TIME [epoch: 8.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40095391110354067		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.4643663587250466		[learning rate: 0.0065688]
	Learning Rate: 0.00656882
	LOSS [training: 0.43266013491429367 | validation: 0.2488195821582757]
	TIME [epoch: 8.31 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3875833402068809		[learning rate: 0.0065611]
		[batch 20/20] avg loss: 0.3971360895099985		[learning rate: 0.0065533]
	Learning Rate: 0.00655332
	LOSS [training: 0.3923597148584396 | validation: 0.6602675778276151]
	TIME [epoch: 8.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4736788549318023		[learning rate: 0.0065456]
		[batch 20/20] avg loss: 0.4161115665718526		[learning rate: 0.0065379]
	Learning Rate: 0.00653786
	LOSS [training: 0.4448952107518275 | validation: 0.4689801102351261]
	TIME [epoch: 8.33 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3960725556535549		[learning rate: 0.0065301]
		[batch 20/20] avg loss: 0.44343774856705415		[learning rate: 0.0065224]
	Learning Rate: 0.00652244
	LOSS [training: 0.4197551521103045 | validation: 0.19259818094414055]
	TIME [epoch: 8.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5165806590419629		[learning rate: 0.0065147]
		[batch 20/20] avg loss: 0.41363930030793405		[learning rate: 0.0065071]
	Learning Rate: 0.00650706
	LOSS [training: 0.46510997967494855 | validation: 0.8890632293430425]
	TIME [epoch: 8.31 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4583673399289501		[learning rate: 0.0064994]
		[batch 20/20] avg loss: 0.36449964559110953		[learning rate: 0.0064917]
	Learning Rate: 0.00649171
	LOSS [training: 0.41143349276002983 | validation: 0.1522387912314669]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36237951266441737		[learning rate: 0.006484]
		[batch 20/20] avg loss: 0.38002599808062204		[learning rate: 0.0064764]
	Learning Rate: 0.00647639
	LOSS [training: 0.3712027553725198 | validation: 0.4058115110170679]
	TIME [epoch: 8.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31708344780571973		[learning rate: 0.0064688]
		[batch 20/20] avg loss: 0.37800502679069325		[learning rate: 0.0064611]
	Learning Rate: 0.00646112
	LOSS [training: 0.3475442372982066 | validation: 0.23982245261009294]
	TIME [epoch: 8.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3202952353184763		[learning rate: 0.0064535]
		[batch 20/20] avg loss: 0.36792695203126435		[learning rate: 0.0064459]
	Learning Rate: 0.00644588
	LOSS [training: 0.3441110936748704 | validation: 0.2004552576518394]
	TIME [epoch: 8.29 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34375220239317417		[learning rate: 0.0064383]
		[batch 20/20] avg loss: 0.3874575143727089		[learning rate: 0.0064307]
	Learning Rate: 0.00643067
	LOSS [training: 0.3656048583829416 | validation: 0.3424944772431504]
	TIME [epoch: 8.29 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4250044821781011		[learning rate: 0.0064231]
		[batch 20/20] avg loss: 0.3599714416995498		[learning rate: 0.0064155]
	Learning Rate: 0.0064155
	LOSS [training: 0.39248796193882546 | validation: 0.40699720738617645]
	TIME [epoch: 8.31 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37531680739207934		[learning rate: 0.0064079]
		[batch 20/20] avg loss: 0.3312128353756314		[learning rate: 0.0064004]
	Learning Rate: 0.00640037
	LOSS [training: 0.35326482138385545 | validation: 0.40701473006639505]
	TIME [epoch: 8.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37227262190956767		[learning rate: 0.0063928]
		[batch 20/20] avg loss: 0.35637871185243697		[learning rate: 0.0063853]
	Learning Rate: 0.00638527
	LOSS [training: 0.36432566688100226 | validation: 0.36659110828040237]
	TIME [epoch: 8.29 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32241729831018084		[learning rate: 0.0063777]
		[batch 20/20] avg loss: 0.36566378861985716		[learning rate: 0.0063702]
	Learning Rate: 0.00637021
	LOSS [training: 0.344040543465019 | validation: 0.36576621807113163]
	TIME [epoch: 8.29 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3743645401408936		[learning rate: 0.0063627]
		[batch 20/20] avg loss: 0.31396082016628984		[learning rate: 0.0063552]
	Learning Rate: 0.00635518
	LOSS [training: 0.34416268015359175 | validation: 0.3504347244298548]
	TIME [epoch: 8.3 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3429267871922151		[learning rate: 0.0063477]
		[batch 20/20] avg loss: 0.31777230193992323		[learning rate: 0.0063402]
	Learning Rate: 0.00634019
	LOSS [training: 0.33034954456606924 | validation: 0.27541859674968006]
	TIME [epoch: 8.32 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38604128662541776		[learning rate: 0.0063327]
		[batch 20/20] avg loss: 0.3011282575541906		[learning rate: 0.0063252]
	Learning Rate: 0.00632524
	LOSS [training: 0.34358477208980415 | validation: 0.20284373892126595]
	TIME [epoch: 8.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3357708351750306		[learning rate: 0.0063178]
		[batch 20/20] avg loss: 0.34501083060178706		[learning rate: 0.0063103]
	Learning Rate: 0.00631032
	LOSS [training: 0.3403908328884089 | validation: 0.27756573903894904]
	TIME [epoch: 8.29 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37008493705955303		[learning rate: 0.0063029]
		[batch 20/20] avg loss: 0.29030510177246077		[learning rate: 0.0062954]
	Learning Rate: 0.00629543
	LOSS [training: 0.33019501941600693 | validation: 0.4352379059623221]
	TIME [epoch: 8.29 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3836788036502732		[learning rate: 0.006288]
		[batch 20/20] avg loss: 0.35740162004340786		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.3705402118468405 | validation: 0.2535984249589739]
	TIME [epoch: 8.33 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3762688416489003		[learning rate: 0.0062732]
		[batch 20/20] avg loss: 0.3251099363673865		[learning rate: 0.0062658]
	Learning Rate: 0.00626577
	LOSS [training: 0.3506893890081434 | validation: 0.6063248018939806]
	TIME [epoch: 8.29 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34408751672531424		[learning rate: 0.0062584]
		[batch 20/20] avg loss: 0.4097492274411675		[learning rate: 0.006251]
	Learning Rate: 0.00625099
	LOSS [training: 0.37691837208324086 | validation: 0.20709203865470666]
	TIME [epoch: 8.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.345917417607938		[learning rate: 0.0062436]
		[batch 20/20] avg loss: 0.330316916592511		[learning rate: 0.0062362]
	Learning Rate: 0.00623624
	LOSS [training: 0.33811716710022455 | validation: 0.28584957326245575]
	TIME [epoch: 8.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37004815098324917		[learning rate: 0.0062289]
		[batch 20/20] avg loss: 0.3224959131920849		[learning rate: 0.0062215]
	Learning Rate: 0.00622153
	LOSS [training: 0.3462720320876671 | validation: 0.3258373504362666]
	TIME [epoch: 8.32 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3878929906196887		[learning rate: 0.0062142]
		[batch 20/20] avg loss: 0.3154183095750168		[learning rate: 0.0062069]
	Learning Rate: 0.00620686
	LOSS [training: 0.3516556500973528 | validation: 0.237142674352487]
	TIME [epoch: 8.31 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35244978526881654		[learning rate: 0.0061995]
		[batch 20/20] avg loss: 0.30447081183081265		[learning rate: 0.0061922]
	Learning Rate: 0.00619222
	LOSS [training: 0.3284602985498145 | validation: 0.35118754938860675]
	TIME [epoch: 8.31 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3556534771390259		[learning rate: 0.0061849]
		[batch 20/20] avg loss: 0.30439874594014327		[learning rate: 0.0061776]
	Learning Rate: 0.00617761
	LOSS [training: 0.3300261115395845 | validation: 0.31848264968489315]
	TIME [epoch: 8.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36904095001908727		[learning rate: 0.0061703]
		[batch 20/20] avg loss: 0.3264219316312527		[learning rate: 0.006163]
	Learning Rate: 0.00616304
	LOSS [training: 0.34773144082516994 | validation: 0.4778602765676478]
	TIME [epoch: 8.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40477990816309123		[learning rate: 0.0061558]
		[batch 20/20] avg loss: 0.27409355215459186		[learning rate: 0.0061485]
	Learning Rate: 0.0061485
	LOSS [training: 0.3394367301588416 | validation: 0.5396206081571829]
	TIME [epoch: 8.33 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4750302327724241		[learning rate: 0.0061412]
		[batch 20/20] avg loss: 0.37791988067699706		[learning rate: 0.006134]
	Learning Rate: 0.006134
	LOSS [training: 0.42647505672471053 | validation: 0.31827722987528473]
	TIME [epoch: 8.31 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.340326575639939		[learning rate: 0.0061268]
		[batch 20/20] avg loss: 0.35040840880610424		[learning rate: 0.0061195]
	Learning Rate: 0.00611953
	LOSS [training: 0.34536749222302165 | validation: 0.2854763448565214]
	TIME [epoch: 8.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3231998682843433		[learning rate: 0.0061123]
		[batch 20/20] avg loss: 0.3752979115797494		[learning rate: 0.0061051]
	Learning Rate: 0.00610509
	LOSS [training: 0.34924888993204634 | validation: 0.6674042263930041]
	TIME [epoch: 8.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3463267088445751		[learning rate: 0.0060979]
		[batch 20/20] avg loss: 0.3682855838925992		[learning rate: 0.0060907]
	Learning Rate: 0.00609069
	LOSS [training: 0.3573061463685871 | validation: 0.7030017512627694]
	TIME [epoch: 8.33 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38041914251042785		[learning rate: 0.0060835]
		[batch 20/20] avg loss: 0.3692350305898954		[learning rate: 0.0060763]
	Learning Rate: 0.00607633
	LOSS [training: 0.3748270865501616 | validation: 0.38878364753415207]
	TIME [epoch: 8.31 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33015684580421023		[learning rate: 0.0060692]
		[batch 20/20] avg loss: 0.35275974875363697		[learning rate: 0.006062]
	Learning Rate: 0.00606199
	LOSS [training: 0.3414582972789236 | validation: 0.27584537170041457]
	TIME [epoch: 8.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32404559748967027		[learning rate: 0.0060548]
		[batch 20/20] avg loss: 0.3575252967978736		[learning rate: 0.0060477]
	Learning Rate: 0.00604769
	LOSS [training: 0.34078544714377196 | validation: 0.4066452718799297]
	TIME [epoch: 8.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3917334929316618		[learning rate: 0.0060406]
		[batch 20/20] avg loss: 0.3622570560477968		[learning rate: 0.0060334]
	Learning Rate: 0.00603343
	LOSS [training: 0.37699527448972925 | validation: 0.26084054652162947]
	TIME [epoch: 8.32 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37054016717876753		[learning rate: 0.0060263]
		[batch 20/20] avg loss: 0.286770234426272		[learning rate: 0.0060192]
	Learning Rate: 0.0060192
	LOSS [training: 0.32865520080251975 | validation: 0.5017596076290478]
	TIME [epoch: 8.32 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35192421623522285		[learning rate: 0.0060121]
		[batch 20/20] avg loss: 0.3993976037608345		[learning rate: 0.006005]
	Learning Rate: 0.006005
	LOSS [training: 0.3756609099980287 | validation: 0.4155235312959856]
	TIME [epoch: 8.31 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3357291178682404		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.32483740803109035		[learning rate: 0.0059908]
	Learning Rate: 0.00599083
	LOSS [training: 0.33028326294966537 | validation: 0.3394006815010967]
	TIME [epoch: 8.31 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2758604872273294		[learning rate: 0.0059838]
		[batch 20/20] avg loss: 0.33811842351201915		[learning rate: 0.0059767]
	Learning Rate: 0.0059767
	LOSS [training: 0.30698945536967426 | validation: 0.38164430249728265]
	TIME [epoch: 8.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3695505748955433		[learning rate: 0.0059696]
		[batch 20/20] avg loss: 0.3350088942474521		[learning rate: 0.0059626]
	Learning Rate: 0.0059626
	LOSS [training: 0.35227973457149764 | validation: 0.20172192362190816]
	TIME [epoch: 8.34 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3157625030604128		[learning rate: 0.0059556]
		[batch 20/20] avg loss: 0.35068722198575336		[learning rate: 0.0059485]
	Learning Rate: 0.00594854
	LOSS [training: 0.33322486252308303 | validation: 0.4848890198304518]
	TIME [epoch: 8.32 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31623395582808334		[learning rate: 0.0059415]
		[batch 20/20] avg loss: 0.29871843278969423		[learning rate: 0.0059345]
	Learning Rate: 0.00593451
	LOSS [training: 0.30747619430888873 | validation: 0.4222679008797448]
	TIME [epoch: 8.32 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43617771619182044		[learning rate: 0.0059275]
		[batch 20/20] avg loss: 0.3529019583831138		[learning rate: 0.0059205]
	Learning Rate: 0.00592051
	LOSS [training: 0.394539837287467 | validation: 0.26844505367210014]
	TIME [epoch: 8.32 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37065868647092953		[learning rate: 0.0059135]
		[batch 20/20] avg loss: 0.3488160324553925		[learning rate: 0.0059065]
	Learning Rate: 0.00590654
	LOSS [training: 0.35973735946316104 | validation: 0.33639611069387865]
	TIME [epoch: 8.34 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3202595567903866		[learning rate: 0.0058996]
		[batch 20/20] avg loss: 0.39818442537487686		[learning rate: 0.0058926]
	Learning Rate: 0.00589261
	LOSS [training: 0.3592219910826318 | validation: 0.2866582440251709]
	TIME [epoch: 8.31 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34748396424830696		[learning rate: 0.0058857]
		[batch 20/20] avg loss: 0.3795923349162177		[learning rate: 0.0058787]
	Learning Rate: 0.00587871
	LOSS [training: 0.3635381495822623 | validation: 0.358522037651396]
	TIME [epoch: 8.31 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29361973931560265		[learning rate: 0.0058718]
		[batch 20/20] avg loss: 0.3950023946365314		[learning rate: 0.0058648]
	Learning Rate: 0.00586484
	LOSS [training: 0.34431106697606706 | validation: 0.4967745176900663]
	TIME [epoch: 8.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38611707635548403		[learning rate: 0.0058579]
		[batch 20/20] avg loss: 0.3384006774259205		[learning rate: 0.005851]
	Learning Rate: 0.00585101
	LOSS [training: 0.3622588768907023 | validation: 0.2465318457903158]
	TIME [epoch: 8.31 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4367979574311408		[learning rate: 0.0058441]
		[batch 20/20] avg loss: 0.2912183030365713		[learning rate: 0.0058372]
	Learning Rate: 0.00583721
	LOSS [training: 0.3640081302338561 | validation: 0.21710857316041593]
	TIME [epoch: 8.33 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2901232500290337		[learning rate: 0.0058303]
		[batch 20/20] avg loss: 0.2876720030085571		[learning rate: 0.0058234]
	Learning Rate: 0.00582344
	LOSS [training: 0.28889762651879547 | validation: 0.4846366056813639]
	TIME [epoch: 8.31 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2885150754711982		[learning rate: 0.0058166]
		[batch 20/20] avg loss: 0.283239434905241		[learning rate: 0.0058097]
	Learning Rate: 0.0058097
	LOSS [training: 0.2858772551882196 | validation: 0.43946760004692054]
	TIME [epoch: 8.31 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3260050868556608		[learning rate: 0.0058028]
		[batch 20/20] avg loss: 0.4575574226429774		[learning rate: 0.005796]
	Learning Rate: 0.005796
	LOSS [training: 0.3917812547493191 | validation: 0.47438508050189687]
	TIME [epoch: 8.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3419306834396335		[learning rate: 0.0057892]
		[batch 20/20] avg loss: 0.35835720291970835		[learning rate: 0.0057823]
	Learning Rate: 0.00578233
	LOSS [training: 0.35014394317967096 | validation: 0.40647261159702996]
	TIME [epoch: 8.33 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35078692550716983		[learning rate: 0.0057755]
		[batch 20/20] avg loss: 0.2940644243534093		[learning rate: 0.0057687]
	Learning Rate: 0.00576869
	LOSS [training: 0.32242567493028956 | validation: 0.2261821993375236]
	TIME [epoch: 8.31 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3488290915963575		[learning rate: 0.0057619]
		[batch 20/20] avg loss: 0.35846675904378955		[learning rate: 0.0057551]
	Learning Rate: 0.00575508
	LOSS [training: 0.35364792532007355 | validation: 0.29733763123627366]
	TIME [epoch: 8.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28605816730777883		[learning rate: 0.0057483]
		[batch 20/20] avg loss: 0.283490710463824		[learning rate: 0.0057415]
	Learning Rate: 0.0057415
	LOSS [training: 0.2847744388858014 | validation: 0.41489661850092]
	TIME [epoch: 8.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3011257027262787		[learning rate: 0.0057347]
		[batch 20/20] avg loss: 0.2672549446615081		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.28419032369389347 | validation: 0.18645301676961618]
	TIME [epoch: 8.32 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3543428340018715		[learning rate: 0.0057212]
		[batch 20/20] avg loss: 0.3152112989220226		[learning rate: 0.0057144]
	Learning Rate: 0.00571445
	LOSS [training: 0.33477706646194705 | validation: 0.20852100229336926]
	TIME [epoch: 8.31 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2854731996626199		[learning rate: 0.0057077]
		[batch 20/20] avg loss: 0.39924556058366356		[learning rate: 0.005701]
	Learning Rate: 0.00570097
	LOSS [training: 0.34235938012314165 | validation: 0.2868904990760576]
	TIME [epoch: 8.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32883420701215493		[learning rate: 0.0056942]
		[batch 20/20] avg loss: 0.33102534366995007		[learning rate: 0.0056875]
	Learning Rate: 0.00568752
	LOSS [training: 0.3299297753410525 | validation: 0.4478790144047859]
	TIME [epoch: 8.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2821570885508825		[learning rate: 0.0056808]
		[batch 20/20] avg loss: 0.36128991980434894		[learning rate: 0.0056741]
	Learning Rate: 0.00567411
	LOSS [training: 0.32172350417761575 | validation: 0.2758639107218084]
	TIME [epoch: 8.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2633543236827153		[learning rate: 0.0056674]
		[batch 20/20] avg loss: 0.3163372865154065		[learning rate: 0.0056607]
	Learning Rate: 0.00566072
	LOSS [training: 0.2898458050990609 | validation: 0.4452182806228659]
	TIME [epoch: 8.33 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3043309748113623		[learning rate: 0.005654]
		[batch 20/20] avg loss: 0.37477792507891555		[learning rate: 0.0056474]
	Learning Rate: 0.00564737
	LOSS [training: 0.33955444994513895 | validation: 0.3215744923907851]
	TIME [epoch: 8.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20770546330126133		[learning rate: 0.0056407]
		[batch 20/20] avg loss: 0.3193761649840508		[learning rate: 0.005634]
	Learning Rate: 0.00563405
	LOSS [training: 0.2635408141426561 | validation: 0.3407016992578875]
	TIME [epoch: 8.31 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3446688230276444		[learning rate: 0.0056274]
		[batch 20/20] avg loss: 1.8759417588069596		[learning rate: 0.0056208]
	Learning Rate: 0.00562076
	LOSS [training: 1.1103052909173017 | validation: 3.1213787424423067]
	TIME [epoch: 8.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0155625958960264		[learning rate: 0.0056141]
		[batch 20/20] avg loss: 2.037357610695541		[learning rate: 0.0056075]
	Learning Rate: 0.0056075
	LOSS [training: 2.5264601032957836 | validation: 1.7811831543412726]
	TIME [epoch: 8.34 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5372620133797357		[learning rate: 0.0056009]
		[batch 20/20] avg loss: 0.3791732366297258		[learning rate: 0.0055943]
	Learning Rate: 0.00559427
	LOSS [training: 0.9582176250047306 | validation: 0.36012813041405073]
	TIME [epoch: 8.32 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.348803140257623		[learning rate: 0.0055877]
		[batch 20/20] avg loss: 0.3058187749581701		[learning rate: 0.0055811]
	Learning Rate: 0.00558108
	LOSS [training: 0.32731095760789664 | validation: 0.39076192438322466]
	TIME [epoch: 8.31 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30039564625914983		[learning rate: 0.0055745]
		[batch 20/20] avg loss: 0.37881769640166574		[learning rate: 0.0055679]
	Learning Rate: 0.00556791
	LOSS [training: 0.33960667133040773 | validation: 0.25937721294623745]
	TIME [epoch: 8.32 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29036804281184614		[learning rate: 0.0055613]
		[batch 20/20] avg loss: 0.3086467350083993		[learning rate: 0.0055548]
	Learning Rate: 0.00555478
	LOSS [training: 0.2995073889101227 | validation: 0.2707601560828967]
	TIME [epoch: 8.33 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3737796253252736		[learning rate: 0.0055482]
		[batch 20/20] avg loss: 0.30109537100727085		[learning rate: 0.0055417]
	Learning Rate: 0.00554167
	LOSS [training: 0.3374374981662722 | validation: 0.37389960790778526]
	TIME [epoch: 8.32 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24949026572755315		[learning rate: 0.0055351]
		[batch 20/20] avg loss: 0.3861107962093275		[learning rate: 0.0055286]
	Learning Rate: 0.0055286
	LOSS [training: 0.3178005309684403 | validation: 0.3438681034521286]
	TIME [epoch: 8.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26413784600073364		[learning rate: 0.0055221]
		[batch 20/20] avg loss: 0.3313725460677793		[learning rate: 0.0055156]
	Learning Rate: 0.00551556
	LOSS [training: 0.29775519603425643 | validation: 0.2366686398075351]
	TIME [epoch: 8.32 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3087104897571247		[learning rate: 0.0055091]
		[batch 20/20] avg loss: 0.3001822492100055		[learning rate: 0.0055026]
	Learning Rate: 0.00550255
	LOSS [training: 0.30444636948356507 | validation: 0.36744714675816464]
	TIME [epoch: 8.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3039799588177317		[learning rate: 0.0054961]
		[batch 20/20] avg loss: 0.43505468883114773		[learning rate: 0.0054896]
	Learning Rate: 0.00548957
	LOSS [training: 0.36951732382443975 | validation: 0.33471898828664426]
	TIME [epoch: 8.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28241318771415486		[learning rate: 0.0054831]
		[batch 20/20] avg loss: 0.3375337316171483		[learning rate: 0.0054766]
	Learning Rate: 0.00547662
	LOSS [training: 0.30997345966565154 | validation: 0.3763854225157328]
	TIME [epoch: 8.31 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.375509017742219		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.30126845953237896		[learning rate: 0.0054637]
	Learning Rate: 0.0054637
	LOSS [training: 0.33838873863729896 | validation: 0.3058969640881303]
	TIME [epoch: 8.31 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3624461442077837		[learning rate: 0.0054573]
		[batch 20/20] avg loss: 0.39849033645034326		[learning rate: 0.0054508]
	Learning Rate: 0.00545082
	LOSS [training: 0.3804682403290635 | validation: 0.6314050650040577]
	TIME [epoch: 8.32 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.357624408722538		[learning rate: 0.0054444]
		[batch 20/20] avg loss: 0.27357018895490637		[learning rate: 0.005438]
	Learning Rate: 0.00543796
	LOSS [training: 0.3155972988387222 | validation: 0.27062674817889787]
	TIME [epoch: 8.34 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34241384156578686		[learning rate: 0.0054315]
		[batch 20/20] avg loss: 0.36008557143458286		[learning rate: 0.0054251]
	Learning Rate: 0.00542513
	LOSS [training: 0.35124970650018483 | validation: 0.30536645430227327]
	TIME [epoch: 8.32 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3124883891752166		[learning rate: 0.0054187]
		[batch 20/20] avg loss: 0.3310730853419359		[learning rate: 0.0054123]
	Learning Rate: 0.00541233
	LOSS [training: 0.32178073725857625 | validation: 0.23020481423046218]
	TIME [epoch: 8.31 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3167186076503454		[learning rate: 0.0054059]
		[batch 20/20] avg loss: 0.3258906022383994		[learning rate: 0.0053996]
	Learning Rate: 0.00539957
	LOSS [training: 0.3213046049443724 | validation: 0.28199882677673155]
	TIME [epoch: 8.31 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3088419242463726		[learning rate: 0.0053932]
		[batch 20/20] avg loss: 0.30118856051637016		[learning rate: 0.0053868]
	Learning Rate: 0.00538683
	LOSS [training: 0.3050152423813714 | validation: 0.33239030007763704]
	TIME [epoch: 8.33 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40846763284764426		[learning rate: 0.0053805]
		[batch 20/20] avg loss: 0.3939594795810252		[learning rate: 0.0053741]
	Learning Rate: 0.00537412
	LOSS [training: 0.4012135562143347 | validation: 0.29653745466026016]
	TIME [epoch: 8.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2670714247375589		[learning rate: 0.0053678]
		[batch 20/20] avg loss: 0.2882565315748405		[learning rate: 0.0053614]
	Learning Rate: 0.00536145
	LOSS [training: 0.2776639781561997 | validation: 0.5404460752138207]
	TIME [epoch: 8.31 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33275294096535657		[learning rate: 0.0053551]
		[batch 20/20] avg loss: 0.3065321356016979		[learning rate: 0.0053488]
	Learning Rate: 0.0053488
	LOSS [training: 0.3196425382835272 | validation: 0.22385169676527467]
	TIME [epoch: 8.31 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2960742685139263		[learning rate: 0.0053425]
		[batch 20/20] avg loss: 0.32839340413588514		[learning rate: 0.0053362]
	Learning Rate: 0.00533618
	LOSS [training: 0.3122338363249057 | validation: 0.4854433306346703]
	TIME [epoch: 8.32 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36472988612751417		[learning rate: 0.0053299]
		[batch 20/20] avg loss: 0.30440606301439543		[learning rate: 0.0053236]
	Learning Rate: 0.0053236
	LOSS [training: 0.3345679745709548 | validation: 0.30593394873075486]
	TIME [epoch: 8.34 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30184766322242823		[learning rate: 0.0053173]
		[batch 20/20] avg loss: 0.30416456466571573		[learning rate: 0.005311]
	Learning Rate: 0.00531104
	LOSS [training: 0.303006113944072 | validation: 0.21265598117129197]
	TIME [epoch: 8.32 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2805168569272213		[learning rate: 0.0053048]
		[batch 20/20] avg loss: 0.2465014069347326		[learning rate: 0.0052985]
	Learning Rate: 0.00529851
	LOSS [training: 0.26350913193097697 | validation: 0.16439433622168836]
	TIME [epoch: 8.31 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36170311688928203		[learning rate: 0.0052923]
		[batch 20/20] avg loss: 0.37005083764415836		[learning rate: 0.005286]
	Learning Rate: 0.00528601
	LOSS [training: 0.3658769772667202 | validation: 0.2149794444832278]
	TIME [epoch: 8.31 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2951411223970137		[learning rate: 0.0052798]
		[batch 20/20] avg loss: 0.3453026711465788		[learning rate: 0.0052735]
	Learning Rate: 0.00527354
	LOSS [training: 0.32022189677179624 | validation: 0.31026312686164853]
	TIME [epoch: 8.34 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.297607987420874		[learning rate: 0.0052673]
		[batch 20/20] avg loss: 0.40796068662628526		[learning rate: 0.0052611]
	Learning Rate: 0.0052611
	LOSS [training: 0.3527843370235797 | validation: 0.23116996906913975]
	TIME [epoch: 8.31 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2434501650073675		[learning rate: 0.0052549]
		[batch 20/20] avg loss: 0.27825315281993246		[learning rate: 0.0052487]
	Learning Rate: 0.00524869
	LOSS [training: 0.26085165891365003 | validation: 0.23611100004629884]
	TIME [epoch: 8.31 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2832522893782262		[learning rate: 0.0052425]
		[batch 20/20] avg loss: 0.4142800442227993		[learning rate: 0.0052363]
	Learning Rate: 0.00523631
	LOSS [training: 0.34876616680051287 | validation: 0.3027892802709083]
	TIME [epoch: 8.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27247723815479985		[learning rate: 0.0052301]
		[batch 20/20] avg loss: 0.2763451302841275		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.2744111842194637 | validation: 0.3017642365754682]
	TIME [epoch: 8.31 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.282346753361629		[learning rate: 0.0052178]
		[batch 20/20] avg loss: 0.3257310038642446		[learning rate: 0.0052116]
	Learning Rate: 0.00521164
	LOSS [training: 0.30403887861293677 | validation: 0.31710255868593173]
	TIME [epoch: 8.32 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24764248749654097		[learning rate: 0.0052055]
		[batch 20/20] avg loss: 0.26060364988936097		[learning rate: 0.0051993]
	Learning Rate: 0.00519935
	LOSS [training: 0.2541230686929509 | validation: 0.2966936590750088]
	TIME [epoch: 8.31 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37169475623558756		[learning rate: 0.0051932]
		[batch 20/20] avg loss: 0.23863004624752815		[learning rate: 0.0051871]
	Learning Rate: 0.00518708
	LOSS [training: 0.3051624012415578 | validation: 0.9563117889076453]
	TIME [epoch: 8.33 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42682195243626025		[learning rate: 0.005181]
		[batch 20/20] avg loss: 0.29420993432361847		[learning rate: 0.0051748]
	Learning Rate: 0.00517485
	LOSS [training: 0.36051594337993936 | validation: 0.3759481697602687]
	TIME [epoch: 8.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3401540229026138		[learning rate: 0.0051687]
		[batch 20/20] avg loss: 0.2807439745852258		[learning rate: 0.0051626]
	Learning Rate: 0.00516264
	LOSS [training: 0.31044899874391974 | validation: 0.20332354898972016]
	TIME [epoch: 8.35 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2724490318328867		[learning rate: 0.0051565]
		[batch 20/20] avg loss: 0.30859675474632986		[learning rate: 0.0051505]
	Learning Rate: 0.00515046
	LOSS [training: 0.2905228932896083 | validation: 0.23489429895492048]
	TIME [epoch: 8.33 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2954341691600833		[learning rate: 0.0051444]
		[batch 20/20] avg loss: 0.3123058177601386		[learning rate: 0.0051383]
	Learning Rate: 0.00513831
	LOSS [training: 0.30386999346011095 | validation: 0.20323888011991115]
	TIME [epoch: 8.34 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25845641295318567		[learning rate: 0.0051322]
		[batch 20/20] avg loss: 0.2727470583000534		[learning rate: 0.0051262]
	Learning Rate: 0.00512619
	LOSS [training: 0.26560173562661954 | validation: 0.39922406302949276]
	TIME [epoch: 8.33 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2930960589795214		[learning rate: 0.0051201]
		[batch 20/20] avg loss: 0.3649284197987048		[learning rate: 0.0051141]
	Learning Rate: 0.0051141
	LOSS [training: 0.32901223938911306 | validation: 0.3732151904051423]
	TIME [epoch: 8.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36305417510729365		[learning rate: 0.0051081]
		[batch 20/20] avg loss: 0.2943697516848549		[learning rate: 0.005102]
	Learning Rate: 0.00510204
	LOSS [training: 0.32871196339607434 | validation: 0.3227968180481059]
	TIME [epoch: 8.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21401447939616042		[learning rate: 0.005096]
		[batch 20/20] avg loss: 0.31031169210926113		[learning rate: 0.00509]
	Learning Rate: 0.00509
	LOSS [training: 0.26216308575271074 | validation: 0.35016091342654443]
	TIME [epoch: 8.32 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2701132812932131		[learning rate: 0.005084]
		[batch 20/20] avg loss: 0.2865483278385948		[learning rate: 0.005078]
	Learning Rate: 0.00507799
	LOSS [training: 0.278330804565904 | validation: 0.2512918747737609]
	TIME [epoch: 8.33 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24291061538489434		[learning rate: 0.005072]
		[batch 20/20] avg loss: 0.27860677048182253		[learning rate: 0.005066]
	Learning Rate: 0.00506602
	LOSS [training: 0.2607586929333584 | validation: 0.2207582458930487]
	TIME [epoch: 8.34 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21693154795805675		[learning rate: 0.00506]
		[batch 20/20] avg loss: 0.2461742206855575		[learning rate: 0.0050541]
	Learning Rate: 0.00505407
	LOSS [training: 0.2315528843218071 | validation: 0.47987734140188215]
	TIME [epoch: 8.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31387476422695315		[learning rate: 0.0050481]
		[batch 20/20] avg loss: 0.23901672781450878		[learning rate: 0.0050421]
	Learning Rate: 0.00504215
	LOSS [training: 0.2764457460207309 | validation: 0.3636417267042673]
	TIME [epoch: 8.33 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34056935078144035		[learning rate: 0.0050362]
		[batch 20/20] avg loss: 0.2141232112568381		[learning rate: 0.0050303]
	Learning Rate: 0.00503025
	LOSS [training: 0.2773462810191392 | validation: 0.3618684368360663]
	TIME [epoch: 8.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643823192876198		[learning rate: 0.0050243]
		[batch 20/20] avg loss: 0.2324155863690581		[learning rate: 0.0050184]
	Learning Rate: 0.00501839
	LOSS [training: 0.24839895282833896 | validation: 0.1384918561694058]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.278299105381596		[learning rate: 0.0050125]
		[batch 20/20] avg loss: 0.3358712344290781		[learning rate: 0.0050065]
	Learning Rate: 0.00500655
	LOSS [training: 0.3070851699053371 | validation: 0.4750770012168957]
	TIME [epoch: 8.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3275947098809648		[learning rate: 0.0050006]
		[batch 20/20] avg loss: 0.2071298752751177		[learning rate: 0.0049947]
	Learning Rate: 0.00499474
	LOSS [training: 0.26736229257804117 | validation: 0.3475070260356837]
	TIME [epoch: 8.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49431336644258234		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.27449920142140327		[learning rate: 0.004983]
	Learning Rate: 0.00498296
	LOSS [training: 0.3844062839319928 | validation: 0.34206915876082244]
	TIME [epoch: 8.34 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33479053856622304		[learning rate: 0.0049771]
		[batch 20/20] avg loss: 0.3025616685071003		[learning rate: 0.0049712]
	Learning Rate: 0.0049712
	LOSS [training: 0.31867610353666176 | validation: 0.3540894441313886]
	TIME [epoch: 8.33 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3010387645338075		[learning rate: 0.0049653]
		[batch 20/20] avg loss: 0.2704917989831034		[learning rate: 0.0049595]
	Learning Rate: 0.00495948
	LOSS [training: 0.28576528175845545 | validation: 0.16331662738178657]
	TIME [epoch: 8.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23711300164307647		[learning rate: 0.0049536]
		[batch 20/20] avg loss: 0.26299657708800306		[learning rate: 0.0049478]
	Learning Rate: 0.00494778
	LOSS [training: 0.25005478936553976 | validation: 0.2536283432178723]
	TIME [epoch: 8.34 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2905479976939279		[learning rate: 0.0049419]
		[batch 20/20] avg loss: 0.24286644170151606		[learning rate: 0.0049361]
	Learning Rate: 0.00493611
	LOSS [training: 0.266707219697722 | validation: 0.20555236569877267]
	TIME [epoch: 8.33 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2619936852259047		[learning rate: 0.0049303]
		[batch 20/20] avg loss: 0.2910238451542876		[learning rate: 0.0049245]
	Learning Rate: 0.00492446
	LOSS [training: 0.2765087651900961 | validation: 0.22739243712931329]
	TIME [epoch: 8.32 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30174296707031917		[learning rate: 0.0049187]
		[batch 20/20] avg loss: 0.3240720823359259		[learning rate: 0.0049128]
	Learning Rate: 0.00491285
	LOSS [training: 0.31290752470312255 | validation: 0.15866565541432504]
	TIME [epoch: 8.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2767866774231257		[learning rate: 0.0049071]
		[batch 20/20] avg loss: 0.2522698584784878		[learning rate: 0.0049013]
	Learning Rate: 0.00490126
	LOSS [training: 0.2645282679508068 | validation: 0.27163034116851953]
	TIME [epoch: 8.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30175747933940866		[learning rate: 0.0048955]
		[batch 20/20] avg loss: 0.2543083241475074		[learning rate: 0.0048897]
	Learning Rate: 0.0048897
	LOSS [training: 0.27803290174345807 | validation: 0.12242258889846779]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.281786317949946		[learning rate: 0.0048839]
		[batch 20/20] avg loss: 0.2864371807075857		[learning rate: 0.0048782]
	Learning Rate: 0.00487816
	LOSS [training: 0.2841117493287658 | validation: 0.17598115513182364]
	TIME [epoch: 8.33 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2735486157278785		[learning rate: 0.0048724]
		[batch 20/20] avg loss: 0.37429197499073374		[learning rate: 0.0048667]
	Learning Rate: 0.00486666
	LOSS [training: 0.32392029535930617 | validation: 0.2800434462988289]
	TIME [epoch: 8.33 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2908160908521224		[learning rate: 0.0048609]
		[batch 20/20] avg loss: 0.2596239453663768		[learning rate: 0.0048552]
	Learning Rate: 0.00485518
	LOSS [training: 0.2752200181092496 | validation: 0.3118799306014451]
	TIME [epoch: 8.35 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3008073739225171		[learning rate: 0.0048494]
		[batch 20/20] avg loss: 0.27375069526109574		[learning rate: 0.0048437]
	Learning Rate: 0.00484372
	LOSS [training: 0.2872790345918064 | validation: 0.17924038289021543]
	TIME [epoch: 8.33 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29034317526011577		[learning rate: 0.004838]
		[batch 20/20] avg loss: 0.24381269096486938		[learning rate: 0.0048323]
	Learning Rate: 0.0048323
	LOSS [training: 0.2670779331124925 | validation: 0.2714961735180439]
	TIME [epoch: 8.32 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30166687610205306		[learning rate: 0.0048266]
		[batch 20/20] avg loss: 0.31377377123919115		[learning rate: 0.0048209]
	Learning Rate: 0.0048209
	LOSS [training: 0.30772032367062213 | validation: 0.2115958495765364]
	TIME [epoch: 8.32 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2568355140791283		[learning rate: 0.0048152]
		[batch 20/20] avg loss: 0.29262395294519705		[learning rate: 0.0048095]
	Learning Rate: 0.00480953
	LOSS [training: 0.2747297335121627 | validation: 0.2675559745747349]
	TIME [epoch: 8.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.284683609484189		[learning rate: 0.0048039]
		[batch 20/20] avg loss: 0.32731667576878926		[learning rate: 0.0047982]
	Learning Rate: 0.00479818
	LOSS [training: 0.30600014262648917 | validation: 0.4033938446699038]
	TIME [epoch: 8.33 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2965438830332402		[learning rate: 0.0047925]
		[batch 20/20] avg loss: 0.277464812498551		[learning rate: 0.0047869]
	Learning Rate: 0.00478687
	LOSS [training: 0.28700434776589556 | validation: 0.41594527518272884]
	TIME [epoch: 8.33 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3368131101932589		[learning rate: 0.0047812]
		[batch 20/20] avg loss: 0.3158959432430285		[learning rate: 0.0047756]
	Learning Rate: 0.00477557
	LOSS [training: 0.3263545267181437 | validation: 0.27808184565330163]
	TIME [epoch: 8.32 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2919036223889008		[learning rate: 0.0047699]
		[batch 20/20] avg loss: 0.2719936846860972		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.281948653537499 | validation: 0.3003297138593005]
	TIME [epoch: 8.33 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2524777921652698		[learning rate: 0.0047587]
		[batch 20/20] avg loss: 0.26779251159946676		[learning rate: 0.0047531]
	Learning Rate: 0.00475307
	LOSS [training: 0.2601351518823683 | validation: 0.3003373715447313]
	TIME [epoch: 8.34 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26662173725664023		[learning rate: 0.0047475]
		[batch 20/20] avg loss: 0.3061976287331494		[learning rate: 0.0047419]
	Learning Rate: 0.00474186
	LOSS [training: 0.28640968299489483 | validation: 0.1975969413417076]
	TIME [epoch: 8.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3175888379752092		[learning rate: 0.0047363]
		[batch 20/20] avg loss: 0.24321396899790154		[learning rate: 0.0047307]
	Learning Rate: 0.00473067
	LOSS [training: 0.2804014034865554 | validation: 0.17920995188026906]
	TIME [epoch: 8.32 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2837610960223951		[learning rate: 0.0047251]
		[batch 20/20] avg loss: 0.20545915843456974		[learning rate: 0.0047195]
	Learning Rate: 0.00471952
	LOSS [training: 0.24461012722848247 | validation: 0.549955108130165]
	TIME [epoch: 8.33 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2869383086032037		[learning rate: 0.0047139]
		[batch 20/20] avg loss: 0.2664625961044903		[learning rate: 0.0047084]
	Learning Rate: 0.00470838
	LOSS [training: 0.276700452353847 | validation: 0.22878567002876057]
	TIME [epoch: 8.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23385783781290148		[learning rate: 0.0047028]
		[batch 20/20] avg loss: 0.2863723989977205		[learning rate: 0.0046973]
	Learning Rate: 0.00469728
	LOSS [training: 0.26011511840531104 | validation: 0.22296192379685217]
	TIME [epoch: 8.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24842255443861064		[learning rate: 0.0046917]
		[batch 20/20] avg loss: 0.2512894982000634		[learning rate: 0.0046862]
	Learning Rate: 0.0046862
	LOSS [training: 0.249856026319337 | validation: 0.23718013814093752]
	TIME [epoch: 8.32 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29093401791916607		[learning rate: 0.0046807]
		[batch 20/20] avg loss: 0.2857430382650239		[learning rate: 0.0046751]
	Learning Rate: 0.00467514
	LOSS [training: 0.2883385280920949 | validation: 0.1591774524146297]
	TIME [epoch: 8.32 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2766095703421757		[learning rate: 0.0046696]
		[batch 20/20] avg loss: 0.28190808610754936		[learning rate: 0.0046641]
	Learning Rate: 0.00466411
	LOSS [training: 0.27925882822486253 | validation: 0.15728774648571245]
	TIME [epoch: 8.34 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21976077319320417		[learning rate: 0.0046586]
		[batch 20/20] avg loss: 0.24597799720113872		[learning rate: 0.0046531]
	Learning Rate: 0.00465311
	LOSS [training: 0.23286938519717149 | validation: 0.2372083571396552]
	TIME [epoch: 8.33 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2514940626866659		[learning rate: 0.0046476]
		[batch 20/20] avg loss: 0.263383113769303		[learning rate: 0.0046421]
	Learning Rate: 0.00464214
	LOSS [training: 0.2574385882279845 | validation: 0.1480357294801546]
	TIME [epoch: 8.32 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3640555077391789		[learning rate: 0.0046367]
		[batch 20/20] avg loss: 0.23509276096155504		[learning rate: 0.0046312]
	Learning Rate: 0.00463119
	LOSS [training: 0.29957413435036695 | validation: 0.15604100186724054]
	TIME [epoch: 8.32 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2416189146539728		[learning rate: 0.0046257]
		[batch 20/20] avg loss: 0.2846993956680429		[learning rate: 0.0046203]
	Learning Rate: 0.00462026
	LOSS [training: 0.26315915516100785 | validation: 0.1635207233009933]
	TIME [epoch: 8.33 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3636960934973025		[learning rate: 0.0046148]
		[batch 20/20] avg loss: 0.24122524827232344		[learning rate: 0.0046094]
	Learning Rate: 0.00460936
	LOSS [training: 0.302460670884813 | validation: 0.26292219778635456]
	TIME [epoch: 8.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24499125750484332		[learning rate: 0.0046039]
		[batch 20/20] avg loss: 0.24186537194066576		[learning rate: 0.0045985]
	Learning Rate: 0.00459849
	LOSS [training: 0.24342831472275456 | validation: 0.3655214029880176]
	TIME [epoch: 8.32 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26184348576085636		[learning rate: 0.0045931]
		[batch 20/20] avg loss: 0.2728079602247423		[learning rate: 0.0045876]
	Learning Rate: 0.00458764
	LOSS [training: 0.26732572299279933 | validation: 0.20214779135045235]
	TIME [epoch: 8.32 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2271835737719669		[learning rate: 0.0045822]
		[batch 20/20] avg loss: 0.19933420938714683		[learning rate: 0.0045768]
	Learning Rate: 0.00457682
	LOSS [training: 0.21325889157955694 | validation: 0.44665353564449767]
	TIME [epoch: 8.32 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3082448111333075		[learning rate: 0.0045714]
		[batch 20/20] avg loss: 0.22769310440427648		[learning rate: 0.004566]
	Learning Rate: 0.00456603
	LOSS [training: 0.26796895776879204 | validation: 0.21707813311157945]
	TIME [epoch: 8.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21971061014980225		[learning rate: 0.0045606]
		[batch 20/20] avg loss: 0.2437526313726634		[learning rate: 0.0045553]
	Learning Rate: 0.00455526
	LOSS [training: 0.23173162076123285 | validation: 0.4531920070795926]
	TIME [epoch: 8.33 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2856687225512399		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.23202703470145414		[learning rate: 0.0045445]
	Learning Rate: 0.00454451
	LOSS [training: 0.258847878626347 | validation: 0.22757631934692474]
	TIME [epoch: 8.32 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24999124239151677		[learning rate: 0.0045391]
		[batch 20/20] avg loss: 0.26584165198519283		[learning rate: 0.0045338]
	Learning Rate: 0.00453379
	LOSS [training: 0.2579164471883547 | validation: 0.16811335112846437]
	TIME [epoch: 8.32 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27213735116259274		[learning rate: 0.0045284]
		[batch 20/20] avg loss: 0.23521022856545465		[learning rate: 0.0045231]
	Learning Rate: 0.0045231
	LOSS [training: 0.25367378986402367 | validation: 0.2295189246676476]
	TIME [epoch: 8.34 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24207184429805947		[learning rate: 0.0045178]
		[batch 20/20] avg loss: 0.28728716072773003		[learning rate: 0.0045124]
	Learning Rate: 0.00451243
	LOSS [training: 0.26467950251289474 | validation: 0.303464740387228]
	TIME [epoch: 8.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.280263789799159		[learning rate: 0.0045071]
		[batch 20/20] avg loss: 0.2715701992852298		[learning rate: 0.0045018]
	Learning Rate: 0.00450178
	LOSS [training: 0.2759169945421944 | validation: 0.2884745611183199]
	TIME [epoch: 8.32 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2782145772377584		[learning rate: 0.0044965]
		[batch 20/20] avg loss: 0.24705016798846083		[learning rate: 0.0044912]
	Learning Rate: 0.00449116
	LOSS [training: 0.2626323726131096 | validation: 0.28605380076605463]
	TIME [epoch: 8.32 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3486204928493898		[learning rate: 0.0044859]
		[batch 20/20] avg loss: 0.26562915067851833		[learning rate: 0.0044806]
	Learning Rate: 0.00448057
	LOSS [training: 0.30712482176395406 | validation: 0.39504471922489615]
	TIME [epoch: 8.32 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25712397790961444		[learning rate: 0.0044753]
		[batch 20/20] avg loss: 0.2592799208509826		[learning rate: 0.00447]
	Learning Rate: 0.00447
	LOSS [training: 0.2582019493802985 | validation: 0.19290230030397026]
	TIME [epoch: 8.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27670443200826955		[learning rate: 0.0044647]
		[batch 20/20] avg loss: 0.23070104994358073		[learning rate: 0.0044595]
	Learning Rate: 0.00445946
	LOSS [training: 0.2537027409759251 | validation: 0.23946737129745638]
	TIME [epoch: 8.32 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2651367604248147		[learning rate: 0.0044542]
		[batch 20/20] avg loss: 0.2747651278245856		[learning rate: 0.0044489]
	Learning Rate: 0.00444894
	LOSS [training: 0.26995094412470017 | validation: 0.37217977181368245]
	TIME [epoch: 8.32 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23040498437723445		[learning rate: 0.0044437]
		[batch 20/20] avg loss: 0.25229309520912274		[learning rate: 0.0044384]
	Learning Rate: 0.00443844
	LOSS [training: 0.24134903979317857 | validation: 0.23766468572485458]
	TIME [epoch: 8.32 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2872850094960226		[learning rate: 0.0044332]
		[batch 20/20] avg loss: 0.22727103829859777		[learning rate: 0.004428]
	Learning Rate: 0.00442797
	LOSS [training: 0.25727802389731014 | validation: 0.18044600754786005]
	TIME [epoch: 8.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1904025711849696		[learning rate: 0.0044227]
		[batch 20/20] avg loss: 0.3123730278222326		[learning rate: 0.0044175]
	Learning Rate: 0.00441753
	LOSS [training: 0.2513877995036011 | validation: 0.45549517620099333]
	TIME [epoch: 8.32 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30388803972682293		[learning rate: 0.0044123]
		[batch 20/20] avg loss: 0.2591228562074445		[learning rate: 0.0044071]
	Learning Rate: 0.00440711
	LOSS [training: 0.28150544796713367 | validation: 0.19135100779233058]
	TIME [epoch: 8.32 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24833962879373678		[learning rate: 0.0044019]
		[batch 20/20] avg loss: 0.256735188770852		[learning rate: 0.0043967]
	Learning Rate: 0.00439671
	LOSS [training: 0.2525374087822944 | validation: 0.23340547581099894]
	TIME [epoch: 8.32 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25872494280415304		[learning rate: 0.0043915]
		[batch 20/20] avg loss: 0.29831460677636473		[learning rate: 0.0043863]
	Learning Rate: 0.00438634
	LOSS [training: 0.2785197747902589 | validation: 0.23728585315345396]
	TIME [epoch: 8.34 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21374240516720971		[learning rate: 0.0043812]
		[batch 20/20] avg loss: 0.2649587294671212		[learning rate: 0.004376]
	Learning Rate: 0.004376
	LOSS [training: 0.23935056731716547 | validation: 0.19112063670428459]
	TIME [epoch: 8.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2946381486370766		[learning rate: 0.0043708]
		[batch 20/20] avg loss: 0.34334973986193285		[learning rate: 0.0043657]
	Learning Rate: 0.00436567
	LOSS [training: 0.3189939442495048 | validation: 0.33820384240768997]
	TIME [epoch: 8.32 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2673205254150096		[learning rate: 0.0043605]
		[batch 20/20] avg loss: 0.2759226154172234		[learning rate: 0.0043554]
	Learning Rate: 0.00435538
	LOSS [training: 0.2716215704161164 | validation: 0.2525383554870882]
	TIME [epoch: 8.32 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2379537781897319		[learning rate: 0.0043502]
		[batch 20/20] avg loss: 0.23106027029569937		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.23450702424271563 | validation: 0.15744725457170722]
	TIME [epoch: 8.32 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23461389004875682		[learning rate: 0.00434]
		[batch 20/20] avg loss: 0.247168676522251		[learning rate: 0.0043349]
	Learning Rate: 0.00433485
	LOSS [training: 0.24089128328550397 | validation: 0.22316303601909637]
	TIME [epoch: 8.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2558994219217748		[learning rate: 0.0043297]
		[batch 20/20] avg loss: 0.20617462960168575		[learning rate: 0.0043246]
	Learning Rate: 0.00432463
	LOSS [training: 0.23103702576173019 | validation: 0.2205728910453687]
	TIME [epoch: 8.32 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21741788324775455		[learning rate: 0.0043195]
		[batch 20/20] avg loss: 0.24250983692254705		[learning rate: 0.0043144]
	Learning Rate: 0.00431443
	LOSS [training: 0.2299638600851508 | validation: 0.16388273718911173]
	TIME [epoch: 8.32 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3648107088742166		[learning rate: 0.0043093]
		[batch 20/20] avg loss: 0.29551422694869006		[learning rate: 0.0043042]
	Learning Rate: 0.00430425
	LOSS [training: 0.3301624679114533 | validation: 0.2045359409061355]
	TIME [epoch: 8.32 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29463175432812905		[learning rate: 0.0042992]
		[batch 20/20] avg loss: 0.22608342386492417		[learning rate: 0.0042941]
	Learning Rate: 0.0042941
	LOSS [training: 0.2603575890965266 | validation: 0.15819165494207613]
	TIME [epoch: 8.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9453625585638598		[learning rate: 0.004289]
		[batch 20/20] avg loss: 0.6554695609017915		[learning rate: 0.004284]
	Learning Rate: 0.00428397
	LOSS [training: 0.8004160597328257 | validation: 0.17250270276237564]
	TIME [epoch: 8.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21496686778646676		[learning rate: 0.0042789]
		[batch 20/20] avg loss: 0.20502284183196173		[learning rate: 0.0042739]
	Learning Rate: 0.00427386
	LOSS [training: 0.20999485480921423 | validation: 0.1678166476029445]
	TIME [epoch: 8.32 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2175988663074273		[learning rate: 0.0042688]
		[batch 20/20] avg loss: 0.2470077359103277		[learning rate: 0.0042638]
	Learning Rate: 0.00426378
	LOSS [training: 0.23230330110887748 | validation: 0.29895224529398545]
	TIME [epoch: 8.32 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2344993977253865		[learning rate: 0.0042587]
		[batch 20/20] avg loss: 0.20936309135993966		[learning rate: 0.0042537]
	Learning Rate: 0.00425372
	LOSS [training: 0.22193124454266305 | validation: 0.17031194184468357]
	TIME [epoch: 8.34 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2667951637930744		[learning rate: 0.0042487]
		[batch 20/20] avg loss: 0.29027042816237414		[learning rate: 0.0042437]
	Learning Rate: 0.00424369
	LOSS [training: 0.2785327959777243 | validation: 0.14910446454110968]
	TIME [epoch: 8.34 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19537075274749477		[learning rate: 0.0042387]
		[batch 20/20] avg loss: 0.24575608392291515		[learning rate: 0.0042337]
	Learning Rate: 0.00423368
	LOSS [training: 0.22056341833520504 | validation: 0.45727690414371]
	TIME [epoch: 8.33 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30277894207395545		[learning rate: 0.0042287]
		[batch 20/20] avg loss: 0.27403847088572175		[learning rate: 0.0042237]
	Learning Rate: 0.00422369
	LOSS [training: 0.28840870647983863 | validation: 0.3041493327401106]
	TIME [epoch: 8.32 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27588400295375454		[learning rate: 0.0042187]
		[batch 20/20] avg loss: 0.3077992060886432		[learning rate: 0.0042137]
	Learning Rate: 0.00421373
	LOSS [training: 0.2918416045211989 | validation: 0.2623592138910371]
	TIME [epoch: 8.32 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2250757215794365		[learning rate: 0.0042088]
		[batch 20/20] avg loss: 0.22730735972752703		[learning rate: 0.0042038]
	Learning Rate: 0.00420379
	LOSS [training: 0.22619154065348174 | validation: 0.28104033054310584]
	TIME [epoch: 8.35 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2600061335820151		[learning rate: 0.0041988]
		[batch 20/20] avg loss: 0.22615487189863429		[learning rate: 0.0041939]
	Learning Rate: 0.00419387
	LOSS [training: 0.24308050274032467 | validation: 0.24159259789615106]
	TIME [epoch: 8.32 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2432225751562392		[learning rate: 0.0041889]
		[batch 20/20] avg loss: 0.26289488271269246		[learning rate: 0.004184]
	Learning Rate: 0.00418398
	LOSS [training: 0.2530587289344658 | validation: 0.16598759110006156]
	TIME [epoch: 8.32 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33812238753372875		[learning rate: 0.004179]
		[batch 20/20] avg loss: 0.2461178201278357		[learning rate: 0.0041741]
	Learning Rate: 0.00417411
	LOSS [training: 0.2921201038307822 | validation: 0.22721388393294345]
	TIME [epoch: 8.32 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3006276130475461		[learning rate: 0.0041692]
		[batch 20/20] avg loss: 0.2825957857573657		[learning rate: 0.0041643]
	Learning Rate: 0.00416427
	LOSS [training: 0.2916116994024559 | validation: 0.18821178971119473]
	TIME [epoch: 8.35 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26343225661193426		[learning rate: 0.0041594]
		[batch 20/20] avg loss: 0.24363923204737964		[learning rate: 0.0041544]
	Learning Rate: 0.00415444
	LOSS [training: 0.253535744329657 | validation: 0.26144745533068003]
	TIME [epoch: 8.32 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22362290854391292		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.22071895755729734		[learning rate: 0.0041446]
	Learning Rate: 0.00414464
	LOSS [training: 0.22217093305060515 | validation: 0.20216777724574914]
	TIME [epoch: 8.32 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25094047275895537		[learning rate: 0.0041398]
		[batch 20/20] avg loss: 0.23076223734073542		[learning rate: 0.0041349]
	Learning Rate: 0.00413487
	LOSS [training: 0.2408513550498454 | validation: 0.24513775236243246]
	TIME [epoch: 8.31 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22337660336615534		[learning rate: 0.00413]
		[batch 20/20] avg loss: 0.28613428936659957		[learning rate: 0.0041251]
	Learning Rate: 0.00412511
	LOSS [training: 0.25475544636637737 | validation: 0.16001041766918062]
	TIME [epoch: 8.33 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22450265324924928		[learning rate: 0.0041202]
		[batch 20/20] avg loss: 0.24194595957485668		[learning rate: 0.0041154]
	Learning Rate: 0.00411538
	LOSS [training: 0.23322430641205308 | validation: 0.4415264136332177]
	TIME [epoch: 8.33 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5810133202257161		[learning rate: 0.0041105]
		[batch 20/20] avg loss: 0.299343020146691		[learning rate: 0.0041057]
	Learning Rate: 0.00410568
	LOSS [training: 0.4401781701862036 | validation: 0.16901896978052797]
	TIME [epoch: 8.32 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22531913206519652		[learning rate: 0.0041008]
		[batch 20/20] avg loss: 0.23570234140253682		[learning rate: 0.004096]
	Learning Rate: 0.00409599
	LOSS [training: 0.23051073673386666 | validation: 0.17357808928766064]
	TIME [epoch: 8.31 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.220176797512738		[learning rate: 0.0040912]
		[batch 20/20] avg loss: 0.2501915047855353		[learning rate: 0.0040863]
	Learning Rate: 0.00408633
	LOSS [training: 0.23518415114913668 | validation: 0.4444891046628803]
	TIME [epoch: 8.32 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.210769651949754		[learning rate: 0.0040815]
		[batch 20/20] avg loss: 0.1762126562032125		[learning rate: 0.0040767]
	Learning Rate: 0.00407669
	LOSS [training: 0.19349115407648326 | validation: 0.2900682984817017]
	TIME [epoch: 8.34 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2388010246481967		[learning rate: 0.0040719]
		[batch 20/20] avg loss: 0.26001341762430774		[learning rate: 0.0040671]
	Learning Rate: 0.00406707
	LOSS [training: 0.2494072211362522 | validation: 0.27693514092403115]
	TIME [epoch: 8.32 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2772205443657082		[learning rate: 0.0040623]
		[batch 20/20] avg loss: 0.1987307276301084		[learning rate: 0.0040575]
	Learning Rate: 0.00405748
	LOSS [training: 0.23797563599790825 | validation: 0.1484289675949666]
	TIME [epoch: 8.32 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2295709613170059		[learning rate: 0.0040527]
		[batch 20/20] avg loss: 0.20656714742848856		[learning rate: 0.0040479]
	Learning Rate: 0.00404791
	LOSS [training: 0.2180690543727472 | validation: 0.1711085678707886]
	TIME [epoch: 8.31 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21981262581419939		[learning rate: 0.0040431]
		[batch 20/20] avg loss: 0.27331890200914116		[learning rate: 0.0040384]
	Learning Rate: 0.00403836
	LOSS [training: 0.2465657639116703 | validation: 0.15617898417177276]
	TIME [epoch: 8.34 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2742307413591803		[learning rate: 0.0040336]
		[batch 20/20] avg loss: 0.21671110966593696		[learning rate: 0.0040288]
	Learning Rate: 0.00402883
	LOSS [training: 0.2454709255125586 | validation: 0.20992500545619497]
	TIME [epoch: 8.32 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23208981134363102		[learning rate: 0.0040241]
		[batch 20/20] avg loss: 0.2491353817918863		[learning rate: 0.0040193]
	Learning Rate: 0.00401933
	LOSS [training: 0.2406125965677587 | validation: 0.18989190131559983]
	TIME [epoch: 8.31 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2494721003365732		[learning rate: 0.0040146]
		[batch 20/20] avg loss: 0.23732958332731352		[learning rate: 0.0040099]
	Learning Rate: 0.00400985
	LOSS [training: 0.24340084183194338 | validation: 0.2160292422141288]
	TIME [epoch: 8.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2719984822057338		[learning rate: 0.0040051]
		[batch 20/20] avg loss: 0.24470875557005806		[learning rate: 0.0040004]
	Learning Rate: 0.00400039
	LOSS [training: 0.2583536188878959 | validation: 0.2782573113388404]
	TIME [epoch: 8.32 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28461870557741725		[learning rate: 0.0039957]
		[batch 20/20] avg loss: 0.23057760823916004		[learning rate: 0.003991]
	Learning Rate: 0.00399096
	LOSS [training: 0.2575981569082886 | validation: 0.2082073030148995]
	TIME [epoch: 8.34 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21379275765786926		[learning rate: 0.0039862]
		[batch 20/20] avg loss: 0.2786809778238311		[learning rate: 0.0039815]
	Learning Rate: 0.00398154
	LOSS [training: 0.24623686774085013 | validation: 0.21934033130868702]
	TIME [epoch: 8.31 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2316163984010368		[learning rate: 0.0039768]
		[batch 20/20] avg loss: 0.22966962461087514		[learning rate: 0.0039721]
	Learning Rate: 0.00397215
	LOSS [training: 0.23064301150595595 | validation: 0.2506786728580728]
	TIME [epoch: 8.31 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2582949860388069		[learning rate: 0.0039675]
		[batch 20/20] avg loss: 0.2745518339613437		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.26642341000007536 | validation: 0.1742195462812764]
	TIME [epoch: 8.31 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22418027831668264		[learning rate: 0.0039581]
		[batch 20/20] avg loss: 0.25796518328485185		[learning rate: 0.0039534]
	Learning Rate: 0.00395343
	LOSS [training: 0.24107273080076724 | validation: 0.22836012753247803]
	TIME [epoch: 8.34 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2600838484026576		[learning rate: 0.0039488]
		[batch 20/20] avg loss: 0.20029176566658569		[learning rate: 0.0039441]
	Learning Rate: 0.00394411
	LOSS [training: 0.23018780703462166 | validation: 0.17790598488980883]
	TIME [epoch: 8.32 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27219694615833184		[learning rate: 0.0039395]
		[batch 20/20] avg loss: 0.2213441693607944		[learning rate: 0.0039348]
	Learning Rate: 0.0039348
	LOSS [training: 0.24677055775956305 | validation: 0.14202529693818844]
	TIME [epoch: 8.32 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20368514136305998		[learning rate: 0.0039302]
		[batch 20/20] avg loss: 0.2322923143631906		[learning rate: 0.0039255]
	Learning Rate: 0.00392552
	LOSS [training: 0.21798872786312526 | validation: 0.1942506019644724]
	TIME [epoch: 8.32 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20910978321431956		[learning rate: 0.0039209]
		[batch 20/20] avg loss: 0.30582227746693247		[learning rate: 0.0039163]
	Learning Rate: 0.00391626
	LOSS [training: 0.25746603034062604 | validation: 0.423262576883195]
	TIME [epoch: 8.33 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24338519598013947		[learning rate: 0.0039116]
		[batch 20/20] avg loss: 0.23958533611260896		[learning rate: 0.003907]
	Learning Rate: 0.00390702
	LOSS [training: 0.24148526604637427 | validation: 0.23212623706211188]
	TIME [epoch: 8.33 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26041204664828715		[learning rate: 0.0039024]
		[batch 20/20] avg loss: 0.2020857864710285		[learning rate: 0.0038978]
	Learning Rate: 0.00389781
	LOSS [training: 0.23124891655965785 | validation: 0.2274646248296879]
	TIME [epoch: 8.32 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2037419471477501		[learning rate: 0.0038932]
		[batch 20/20] avg loss: 0.26619446836810123		[learning rate: 0.0038886]
	Learning Rate: 0.00388861
	LOSS [training: 0.23496820775792568 | validation: 0.24560255858745877]
	TIME [epoch: 8.32 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24128393115512056		[learning rate: 0.003884]
		[batch 20/20] avg loss: 0.27633221830103344		[learning rate: 0.0038794]
	Learning Rate: 0.00387944
	LOSS [training: 0.258808074728077 | validation: 0.21303070231293575]
	TIME [epoch: 8.32 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22975843824365216		[learning rate: 0.0038749]
		[batch 20/20] avg loss: 0.2219959569118554		[learning rate: 0.0038703]
	Learning Rate: 0.00387029
	LOSS [training: 0.22587719757775376 | validation: 0.34835483195089023]
	TIME [epoch: 8.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26223098282133744		[learning rate: 0.0038657]
		[batch 20/20] avg loss: 0.3256340806779493		[learning rate: 0.0038612]
	Learning Rate: 0.00386116
	LOSS [training: 0.2939325317496433 | validation: 0.1432209100598092]
	TIME [epoch: 8.31 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27038389893294756		[learning rate: 0.0038566]
		[batch 20/20] avg loss: 0.23194552039780097		[learning rate: 0.0038521]
	Learning Rate: 0.00385205
	LOSS [training: 0.2511647096653743 | validation: 0.21954112456252942]
	TIME [epoch: 8.32 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22084633343718202		[learning rate: 0.0038475]
		[batch 20/20] avg loss: 0.30227209669493277		[learning rate: 0.003843]
	Learning Rate: 0.00384297
	LOSS [training: 0.2615592150660574 | validation: 0.32444301718895147]
	TIME [epoch: 8.32 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2817050116082939		[learning rate: 0.0038384]
		[batch 20/20] avg loss: 0.21368012368774125		[learning rate: 0.0038339]
	Learning Rate: 0.0038339
	LOSS [training: 0.24769256764801764 | validation: 0.14940965121211455]
	TIME [epoch: 8.35 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2031629550086834		[learning rate: 0.0038294]
		[batch 20/20] avg loss: 0.22248986746632773		[learning rate: 0.0038249]
	Learning Rate: 0.00382486
	LOSS [training: 0.21282641123750556 | validation: 0.382545205952309]
	TIME [epoch: 8.31 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25581464450573793		[learning rate: 0.0038203]
		[batch 20/20] avg loss: 0.18101498278538092		[learning rate: 0.0038158]
	Learning Rate: 0.00381584
	LOSS [training: 0.21841481364555942 | validation: 0.17666172216656303]
	TIME [epoch: 8.32 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.251725855074905		[learning rate: 0.0038113]
		[batch 20/20] avg loss: 0.3502071995621479		[learning rate: 0.0038068]
	Learning Rate: 0.00380684
	LOSS [training: 0.3009665273185266 | validation: 0.22972852718739178]
	TIME [epoch: 8.32 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2684669529013878		[learning rate: 0.0038023]
		[batch 20/20] avg loss: 0.21303768279954313		[learning rate: 0.0037979]
	Learning Rate: 0.00379786
	LOSS [training: 0.24075231785046544 | validation: 0.2903745170369347]
	TIME [epoch: 8.33 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2355582814509649		[learning rate: 0.0037934]
		[batch 20/20] avg loss: 0.2222284166517218		[learning rate: 0.0037889]
	Learning Rate: 0.0037889
	LOSS [training: 0.22889334905134331 | validation: 0.1871483772946544]
	TIME [epoch: 8.33 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22952673095363352		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.2600705549952943		[learning rate: 0.00378]
	Learning Rate: 0.00377996
	LOSS [training: 0.2447986429744639 | validation: 0.24091058642169175]
	TIME [epoch: 8.32 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27837684321893363		[learning rate: 0.0037755]
		[batch 20/20] avg loss: 0.21630678500972525		[learning rate: 0.003771]
	Learning Rate: 0.00377104
	LOSS [training: 0.24734181411432948 | validation: 0.18261703836650564]
	TIME [epoch: 8.32 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3211791473751089		[learning rate: 0.0037666]
		[batch 20/20] avg loss: 0.20774801393920034		[learning rate: 0.0037621]
	Learning Rate: 0.00376215
	LOSS [training: 0.2644635806571546 | validation: 0.11341266760522731]
	TIME [epoch: 8.32 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2427743246489229		[learning rate: 0.0037577]
		[batch 20/20] avg loss: 0.3452821837236999		[learning rate: 0.0037533]
	Learning Rate: 0.00375327
	LOSS [training: 0.2940282541863114 | validation: 0.1410313961789109]
	TIME [epoch: 8.34 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25296876553386255		[learning rate: 0.0037488]
		[batch 20/20] avg loss: 0.26589837299470803		[learning rate: 0.0037444]
	Learning Rate: 0.00374442
	LOSS [training: 0.2594335692642853 | validation: 0.23878520433634592]
	TIME [epoch: 8.31 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2474864860540522		[learning rate: 0.00374]
		[batch 20/20] avg loss: 0.2854766378516326		[learning rate: 0.0037356]
	Learning Rate: 0.00373559
	LOSS [training: 0.2664815619528424 | validation: 0.46365216075059534]
	TIME [epoch: 8.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34868422019625483		[learning rate: 0.0037312]
		[batch 20/20] avg loss: 0.20771393836086652		[learning rate: 0.0037268]
	Learning Rate: 0.00372678
	LOSS [training: 0.2781990792785607 | validation: 0.15821569669315166]
	TIME [epoch: 8.31 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28906498863488594		[learning rate: 0.0037224]
		[batch 20/20] avg loss: 0.24867113330735996		[learning rate: 0.003718]
	Learning Rate: 0.00371799
	LOSS [training: 0.268868060971123 | validation: 0.2781768360922641]
	TIME [epoch: 8.34 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5125517049696142		[learning rate: 0.0037136]
		[batch 20/20] avg loss: 3.143184228161986		[learning rate: 0.0037092]
	Learning Rate: 0.00370922
	LOSS [training: 1.8278679665658 | validation: 2.9396321385691913]
	TIME [epoch: 8.32 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.781033255687942		[learning rate: 0.0037048]
		[batch 20/20] avg loss: 1.8442905092116084		[learning rate: 0.0037005]
	Learning Rate: 0.00370047
	LOSS [training: 2.8126618824497753 | validation: 0.7820306537320016]
	TIME [epoch: 8.32 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.878807162982955		[learning rate: 0.0036961]
		[batch 20/20] avg loss: 2.7761035249373216		[learning rate: 0.0036917]
	Learning Rate: 0.00369174
	LOSS [training: 1.8274553439601384 | validation: 3.2986070674836054]
	TIME [epoch: 8.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6704712747286723		[learning rate: 0.0036874]
		[batch 20/20] avg loss: 3.1887221922372104		[learning rate: 0.003683]
	Learning Rate: 0.00368303
	LOSS [training: 2.929596733482941 | validation: 4.122082700788713]
	TIME [epoch: 8.33 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.477489651939701		[learning rate: 0.0036787]
		[batch 20/20] avg loss: 3.5999395764234365		[learning rate: 0.0036743]
	Learning Rate: 0.00367434
	LOSS [training: 4.0387146141815675 | validation: 3.839183788843395]
	TIME [epoch: 8.33 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.639539357463074		[learning rate: 0.00367]
		[batch 20/20] avg loss: 4.695990469380842		[learning rate: 0.0036657]
	Learning Rate: 0.00366567
	LOSS [training: 4.6677649134219585 | validation: 5.321821079005262]
	TIME [epoch: 8.31 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.657282696193643		[learning rate: 0.0036613]
		[batch 20/20] avg loss: 5.5254449839510595		[learning rate: 0.003657]
	Learning Rate: 0.00365703
	LOSS [training: 5.5913638400723515 | validation: 5.846005731947907]
	TIME [epoch: 8.31 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.7255552898771		[learning rate: 0.0036527]
		[batch 20/20] avg loss: 7.465194336259482		[learning rate: 0.0036484]
	Learning Rate: 0.0036484
	LOSS [training: 7.095374813068288 | validation: 7.475177873051717]
	TIME [epoch: 8.31 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.922257962177346		[learning rate: 0.0036441]
		[batch 20/20] avg loss: 8.817436695770258		[learning rate: 0.0036398]
	Learning Rate: 0.00363979
	LOSS [training: 8.369847328973803 | validation: 8.427814533985943]
	TIME [epoch: 8.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.510933694098826		[learning rate: 0.0036355]
		[batch 20/20] avg loss: 7.632736558652143		[learning rate: 0.0036312]
	Learning Rate: 0.00363121
	LOSS [training: 8.071835126375484 | validation: 6.9341746400216255]
	TIME [epoch: 8.31 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.01047566570107		[learning rate: 0.0036269]
		[batch 20/20] avg loss: 7.210934972971556		[learning rate: 0.0036226]
	Learning Rate: 0.00362264
	LOSS [training: 7.110705319336314 | validation: 6.759180284290052]
	TIME [epoch: 8.31 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.320949544247631		[learning rate: 0.0036184]
		[batch 20/20] avg loss: 7.383171715929008		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 7.35206063008832 | validation: 6.917360856500823]
	TIME [epoch: 8.31 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.006697918673886		[learning rate: 0.0036098]
		[batch 20/20] avg loss: 8.365651733514692		[learning rate: 0.0036056]
	Learning Rate: 0.00360557
	LOSS [training: 8.186174826094287 | validation: 7.870511654489674]
	TIME [epoch: 8.33 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.875739052561916		[learning rate: 0.0036013]
		[batch 20/20] avg loss: 6.960106265776707		[learning rate: 0.0035971]
	Learning Rate: 0.00359707
	LOSS [training: 7.417922659169312 | validation: 6.769193544271822]
	TIME [epoch: 8.32 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.314169360534166		[learning rate: 0.0035928]
		[batch 20/20] avg loss: 8.174241613664785		[learning rate: 0.0035886]
	Learning Rate: 0.00358858
	LOSS [training: 7.744205487099475 | validation: 8.43622606627128]
	TIME [epoch: 8.31 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.81850044237949		[learning rate: 0.0035843]
		[batch 20/20] avg loss: 9.27744724851408		[learning rate: 0.0035801]
	Learning Rate: 0.00358012
	LOSS [training: 9.047973845446787 | validation: 9.029170108575354]
	TIME [epoch: 8.32 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.219434781376037		[learning rate: 0.0035759]
		[batch 20/20] avg loss: 8.859169098625085		[learning rate: 0.0035717]
	Learning Rate: 0.00357167
	LOSS [training: 9.039301940000561 | validation: 8.413088419515923]
	TIME [epoch: 8.33 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.76286414280227		[learning rate: 0.0035675]
		[batch 20/20] avg loss: 8.615737987556322		[learning rate: 0.0035632]
	Learning Rate: 0.00356325
	LOSS [training: 8.689301065179297 | validation: 8.143330913801632]
	TIME [epoch: 8.32 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.653307976369305		[learning rate: 0.003559]
		[batch 20/20] avg loss: 8.40532975016276		[learning rate: 0.0035548]
	Learning Rate: 0.00355484
	LOSS [training: 8.529318863266033 | validation: 7.625712568088121]
	TIME [epoch: 8.31 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.143705453546831		[learning rate: 0.0035506]
		[batch 20/20] avg loss: 7.9984378131454745		[learning rate: 0.0035465]
	Learning Rate: 0.00354646
	LOSS [training: 8.071071633346152 | validation: 7.187663572737469]
	TIME [epoch: 8.31 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.584652779903953		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 8.083142518572762		[learning rate: 0.0035381]
	Learning Rate: 0.00353809
	LOSS [training: 7.833897649238358 | validation: 7.730713042953381]
	TIME [epoch: 8.31 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.061655656565305		[learning rate: 0.0035339]
		[batch 20/20] avg loss: 8.061264424963605		[learning rate: 0.0035297]
	Learning Rate: 0.00352975
	LOSS [training: 8.061460040764455 | validation: 7.5535715081855095]
	TIME [epoch: 8.34 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.885441600720723		[learning rate: 0.0035256]
		[batch 20/20] avg loss: 5.812050814514636		[learning rate: 0.0035214]
	Learning Rate: 0.00352142
	LOSS [training: 6.84874620761768 | validation: 4.205255115506819]
	TIME [epoch: 8.31 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6815431065605018		[learning rate: 0.0035173]
		[batch 20/20] avg loss: 2.938755397677105		[learning rate: 0.0035131]
	Learning Rate: 0.00351311
	LOSS [training: 3.3101492521188036 | validation: 2.721292790363374]
	TIME [epoch: 8.31 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.757171472996405		[learning rate: 0.003509]
		[batch 20/20] avg loss: 2.5781841588249015		[learning rate: 0.0035048]
	Learning Rate: 0.00350483
	LOSS [training: 2.667677815910653 | validation: 2.2535099568158943]
	TIME [epoch: 8.31 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.164363747703964		[learning rate: 0.0035007]
		[batch 20/20] avg loss: 1.1572935193020364		[learning rate: 0.0034966]
	Learning Rate: 0.00349656
	LOSS [training: 1.6608286335030005 | validation: 0.6817769839409822]
	TIME [epoch: 8.33 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.644253557954358		[learning rate: 0.0034924]
		[batch 20/20] avg loss: 0.5968974619881116		[learning rate: 0.0034883]
	Learning Rate: 0.00348831
	LOSS [training: 0.6205755099712348 | validation: 0.7788561529949765]
	TIME [epoch: 8.31 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.548172182043536		[learning rate: 0.0034842]
		[batch 20/20] avg loss: 0.5439090018802141		[learning rate: 0.0034801]
	Learning Rate: 0.00348008
	LOSS [training: 0.546040591961875 | validation: 0.4809838346292237]
	TIME [epoch: 8.31 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5070958741931095		[learning rate: 0.003476]
		[batch 20/20] avg loss: 0.47005630934366793		[learning rate: 0.0034719]
	Learning Rate: 0.00347187
	LOSS [training: 0.48857609176838884 | validation: 0.4249479528741898]
	TIME [epoch: 8.31 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.511803803924787		[learning rate: 0.0034678]
		[batch 20/20] avg loss: 0.4398052732190904		[learning rate: 0.0034637]
	Learning Rate: 0.00346369
	LOSS [training: 0.4758045385719386 | validation: 0.42616581248055885]
	TIME [epoch: 8.33 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47101134726576		[learning rate: 0.0034596]
		[batch 20/20] avg loss: 0.45493057629866396		[learning rate: 0.0034555]
	Learning Rate: 0.00345552
	LOSS [training: 0.46297096178221187 | validation: 0.355122941521438]
	TIME [epoch: 8.32 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4308058916993688		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.3950691273332052		[learning rate: 0.0034474]
	Learning Rate: 0.00344736
	LOSS [training: 0.412937509516287 | validation: 0.35224014505698575]
	TIME [epoch: 8.31 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4891830145407413		[learning rate: 0.0034433]
		[batch 20/20] avg loss: 0.45999174107705193		[learning rate: 0.0034392]
	Learning Rate: 0.00343923
	LOSS [training: 0.4745873778088966 | validation: 0.49709215082711433]
	TIME [epoch: 8.31 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43291541427696545		[learning rate: 0.0034352]
		[batch 20/20] avg loss: 0.38875130317130857		[learning rate: 0.0034311]
	Learning Rate: 0.00343112
	LOSS [training: 0.410833358724137 | validation: 0.2774939088474421]
	TIME [epoch: 8.31 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3523455509536669		[learning rate: 0.0034271]
		[batch 20/20] avg loss: 0.3821650032473284		[learning rate: 0.003423]
	Learning Rate: 0.00342303
	LOSS [training: 0.36725527710049766 | validation: 0.3440488604209059]
	TIME [epoch: 8.33 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3713214453466275		[learning rate: 0.003419]
		[batch 20/20] avg loss: 0.38450678365290647		[learning rate: 0.003415]
	Learning Rate: 0.00341495
	LOSS [training: 0.377914114499767 | validation: 0.3466826238538589]
	TIME [epoch: 8.31 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38257832752461274		[learning rate: 0.0034109]
		[batch 20/20] avg loss: 0.4117944733636679		[learning rate: 0.0034069]
	Learning Rate: 0.0034069
	LOSS [training: 0.3971864004441404 | validation: 0.4636621824581555]
	TIME [epoch: 8.31 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4632754551008221		[learning rate: 0.0034029]
		[batch 20/20] avg loss: 0.3525169544608384		[learning rate: 0.0033989]
	Learning Rate: 0.00339886
	LOSS [training: 0.4078962047808301 | validation: 0.23370488627984098]
	TIME [epoch: 8.31 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45158976480504337		[learning rate: 0.0033948]
		[batch 20/20] avg loss: 0.38485556385245434		[learning rate: 0.0033908]
	Learning Rate: 0.00339084
	LOSS [training: 0.4182226643287488 | validation: 0.2408505462563602]
	TIME [epoch: 8.33 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3673474124552255		[learning rate: 0.0033868]
		[batch 20/20] avg loss: 0.29232887953303155		[learning rate: 0.0033828]
	Learning Rate: 0.00338284
	LOSS [training: 0.32983814599412853 | validation: 0.2854574593679424]
	TIME [epoch: 8.31 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3256900869831764		[learning rate: 0.0033789]
		[batch 20/20] avg loss: 0.34608166848609867		[learning rate: 0.0033749]
	Learning Rate: 0.00337487
	LOSS [training: 0.3358858777346375 | validation: 0.4209526070917837]
	TIME [epoch: 8.31 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3741801994428919		[learning rate: 0.0033709]
		[batch 20/20] avg loss: 0.4740295764367368		[learning rate: 0.0033669]
	Learning Rate: 0.0033669
	LOSS [training: 0.4241048879398145 | validation: 0.2604348650675241]
	TIME [epoch: 8.31 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3258935690199294		[learning rate: 0.0033629]
		[batch 20/20] avg loss: 0.3243331196057916		[learning rate: 0.003359]
	Learning Rate: 0.00335896
	LOSS [training: 0.3251133443128605 | validation: 0.5185775837432609]
	TIME [epoch: 8.31 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3826117985073537		[learning rate: 0.003355]
		[batch 20/20] avg loss: 0.3726265914308084		[learning rate: 0.003351]
	Learning Rate: 0.00335104
	LOSS [training: 0.3776191949690811 | validation: 0.29421936305509533]
	TIME [epoch: 8.33 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2848801023338103		[learning rate: 0.0033471]
		[batch 20/20] avg loss: 0.3892992027699495		[learning rate: 0.0033431]
	Learning Rate: 0.00334313
	LOSS [training: 0.33708965255187995 | validation: 0.5403003054440564]
	TIME [epoch: 8.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43932188288430296		[learning rate: 0.0033392]
		[batch 20/20] avg loss: 0.34797311790276125		[learning rate: 0.0033352]
	Learning Rate: 0.00333525
	LOSS [training: 0.39364750039353213 | validation: 0.26581925646419285]
	TIME [epoch: 8.31 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3122080958928433		[learning rate: 0.0033313]
		[batch 20/20] avg loss: 0.3430952705156592		[learning rate: 0.0033274]
	Learning Rate: 0.00332738
	LOSS [training: 0.3276516832042512 | validation: 0.32784354487643985]
	TIME [epoch: 8.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3141074672311394		[learning rate: 0.0033235]
		[batch 20/20] avg loss: 0.38427428737869923		[learning rate: 0.0033195]
	Learning Rate: 0.00331953
	LOSS [training: 0.3491908773049193 | validation: 0.5943888835075275]
	TIME [epoch: 8.33 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3376733946507876		[learning rate: 0.0033156]
		[batch 20/20] avg loss: 0.3233896604229044		[learning rate: 0.0033117]
	Learning Rate: 0.0033117
	LOSS [training: 0.330531527536846 | validation: 0.21158308493176498]
	TIME [epoch: 8.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3552560444374112		[learning rate: 0.0033078]
		[batch 20/20] avg loss: 0.29207629401824525		[learning rate: 0.0033039]
	Learning Rate: 0.00330389
	LOSS [training: 0.32366616922782826 | validation: 0.2739389456433788]
	TIME [epoch: 8.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30437941758505455		[learning rate: 0.0033]
		[batch 20/20] avg loss: 0.3491063497387149		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.32674288366188475 | validation: 0.4252303173028694]
	TIME [epoch: 8.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32696105064113945		[learning rate: 0.0032922]
		[batch 20/20] avg loss: 0.3340532920370478		[learning rate: 0.0032883]
	Learning Rate: 0.00328832
	LOSS [training: 0.3305071713390936 | validation: 0.33015368629534514]
	TIME [epoch: 8.31 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3234268749715999		[learning rate: 0.0032844]
		[batch 20/20] avg loss: 0.28147186574341293		[learning rate: 0.0032806]
	Learning Rate: 0.00328057
	LOSS [training: 0.30244937035750635 | validation: 0.3302561873754735]
	TIME [epoch: 8.31 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31256216514712115		[learning rate: 0.0032767]
		[batch 20/20] avg loss: 0.3254254978233314		[learning rate: 0.0032728]
	Learning Rate: 0.00327283
	LOSS [training: 0.31899383148522636 | validation: 0.32797116141471405]
	TIME [epoch: 8.31 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.406720994371253		[learning rate: 0.003269]
		[batch 20/20] avg loss: 0.3083856343730634		[learning rate: 0.0032651]
	Learning Rate: 0.00326511
	LOSS [training: 0.35755331437215815 | validation: 0.4205636227626455]
	TIME [epoch: 8.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3362575655947616		[learning rate: 0.0032613]
		[batch 20/20] avg loss: 0.33353325662494904		[learning rate: 0.0032574]
	Learning Rate: 0.00325741
	LOSS [training: 0.3348954111098553 | validation: 0.7381643036886807]
	TIME [epoch: 8.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39925028812657576		[learning rate: 0.0032536]
		[batch 20/20] avg loss: 0.3014223826283914		[learning rate: 0.0032497]
	Learning Rate: 0.00324972
	LOSS [training: 0.3503363353774836 | validation: 0.5049816279134955]
	TIME [epoch: 8.33 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37898791399669723		[learning rate: 0.0032459]
		[batch 20/20] avg loss: 0.2794683894312037		[learning rate: 0.0032421]
	Learning Rate: 0.00324206
	LOSS [training: 0.3292281517139505 | validation: 0.2958316340690806]
	TIME [epoch: 8.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3415577802297055		[learning rate: 0.0032382]
		[batch 20/20] avg loss: 0.3412194412303615		[learning rate: 0.0032344]
	Learning Rate: 0.00323441
	LOSS [training: 0.3413886107300335 | validation: 0.2526678840171994]
	TIME [epoch: 8.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37261631056787325		[learning rate: 0.0032306]
		[batch 20/20] avg loss: 0.29326219706817086		[learning rate: 0.0032268]
	Learning Rate: 0.00322678
	LOSS [training: 0.3329392538180221 | validation: 0.35956428350264624]
	TIME [epoch: 8.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32168756647091595		[learning rate: 0.003223]
		[batch 20/20] avg loss: 0.28916548623579985		[learning rate: 0.0032192]
	Learning Rate: 0.00321917
	LOSS [training: 0.30542652635335793 | validation: 0.3156039047930731]
	TIME [epoch: 8.33 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3114011775557298		[learning rate: 0.0032154]
		[batch 20/20] avg loss: 0.3357073698100472		[learning rate: 0.0032116]
	Learning Rate: 0.00321157
	LOSS [training: 0.32355427368288847 | validation: 0.4659553250945366]
	TIME [epoch: 8.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3262891670284483		[learning rate: 0.0032078]
		[batch 20/20] avg loss: 0.3322770609029551		[learning rate: 0.003204]
	Learning Rate: 0.003204
	LOSS [training: 0.32928311396570165 | validation: 0.469128227568512]
	TIME [epoch: 8.31 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3473288846343658		[learning rate: 0.0032002]
		[batch 20/20] avg loss: 0.29476415710341763		[learning rate: 0.0031964]
	Learning Rate: 0.00319644
	LOSS [training: 0.3210465208688918 | validation: 0.23331501897826212]
	TIME [epoch: 8.31 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29026455866012674		[learning rate: 0.0031927]
		[batch 20/20] avg loss: 0.30297728479783237		[learning rate: 0.0031889]
	Learning Rate: 0.0031889
	LOSS [training: 0.2966209217289796 | validation: 0.31111250225160614]
	TIME [epoch: 8.32 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31002747727965757		[learning rate: 0.0031851]
		[batch 20/20] avg loss: 0.4028021705457753		[learning rate: 0.0031814]
	Learning Rate: 0.00318138
	LOSS [training: 0.3564148239127164 | validation: 0.35083774153311087]
	TIME [epoch: 8.31 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36037196654726594		[learning rate: 0.0031776]
		[batch 20/20] avg loss: 0.3984570740248113		[learning rate: 0.0031739]
	Learning Rate: 0.00317387
	LOSS [training: 0.37941452028603867 | validation: 0.4693361008495793]
	TIME [epoch: 8.31 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44990127792200996		[learning rate: 0.0031701]
		[batch 20/20] avg loss: 0.3560844322530198		[learning rate: 0.0031664]
	Learning Rate: 0.00316639
	LOSS [training: 0.4029928550875149 | validation: 0.259111923820516]
	TIME [epoch: 8.31 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28338598140931154		[learning rate: 0.0031627]
		[batch 20/20] avg loss: 0.2777514958763889		[learning rate: 0.0031589]
	Learning Rate: 0.00315892
	LOSS [training: 0.28056873864285026 | validation: 0.2689419112115982]
	TIME [epoch: 8.31 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27996133793865835		[learning rate: 0.0031552]
		[batch 20/20] avg loss: 0.31553102216877277		[learning rate: 0.0031515]
	Learning Rate: 0.00315147
	LOSS [training: 0.2977461800537156 | validation: 0.40288741631349434]
	TIME [epoch: 8.33 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2721510157582595		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.28345006746823914		[learning rate: 0.003144]
	Learning Rate: 0.00314403
	LOSS [training: 0.27780054161324935 | validation: 0.27109332341002146]
	TIME [epoch: 8.31 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3186678295681125		[learning rate: 0.0031403]
		[batch 20/20] avg loss: 0.31332211228461354		[learning rate: 0.0031366]
	Learning Rate: 0.00313662
	LOSS [training: 0.315994970926363 | validation: 0.37214724434935487]
	TIME [epoch: 8.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32372167233180393		[learning rate: 0.0031329]
		[batch 20/20] avg loss: 0.36551396037876116		[learning rate: 0.0031292]
	Learning Rate: 0.00312922
	LOSS [training: 0.3446178163552825 | validation: 0.40195267023936987]
	TIME [epoch: 8.31 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35548762276637713		[learning rate: 0.0031255]
		[batch 20/20] avg loss: 0.325609169177744		[learning rate: 0.0031218]
	Learning Rate: 0.00312184
	LOSS [training: 0.3405483959720606 | validation: 0.34557263958153445]
	TIME [epoch: 8.33 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28375969762741304		[learning rate: 0.0031182]
		[batch 20/20] avg loss: 0.2632855721739018		[learning rate: 0.0031145]
	Learning Rate: 0.00311447
	LOSS [training: 0.2735226349006575 | validation: 0.3118792447863802]
	TIME [epoch: 8.32 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32470891489063475		[learning rate: 0.0031108]
		[batch 20/20] avg loss: 0.28452170107452895		[learning rate: 0.0031071]
	Learning Rate: 0.00310713
	LOSS [training: 0.30461530798258185 | validation: 0.26940157118497615]
	TIME [epoch: 8.31 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31466143190078316		[learning rate: 0.0031035]
		[batch 20/20] avg loss: 0.27079773155212117		[learning rate: 0.0030998]
	Learning Rate: 0.0030998
	LOSS [training: 0.2927295817264521 | validation: 0.2493469163428046]
	TIME [epoch: 8.31 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2551706525553632		[learning rate: 0.0030961]
		[batch 20/20] avg loss: 0.3012187533243869		[learning rate: 0.0030925]
	Learning Rate: 0.00309249
	LOSS [training: 0.2781947029398751 | validation: 0.25089140353586625]
	TIME [epoch: 8.32 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.323202249873535		[learning rate: 0.0030888]
		[batch 20/20] avg loss: 0.2809043025514132		[learning rate: 0.0030852]
	Learning Rate: 0.00308519
	LOSS [training: 0.30205327621247413 | validation: 0.26281146475324346]
	TIME [epoch: 8.32 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24848356149393253		[learning rate: 0.0030815]
		[batch 20/20] avg loss: 0.3824361827299321		[learning rate: 0.0030779]
	Learning Rate: 0.00307791
	LOSS [training: 0.31545987211193227 | validation: 0.18955704766759993]
	TIME [epoch: 8.31 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3005186267793632		[learning rate: 0.0030743]
		[batch 20/20] avg loss: 0.2780388359228022		[learning rate: 0.0030707]
	Learning Rate: 0.00307065
	LOSS [training: 0.2892787313510827 | validation: 0.3222041623600332]
	TIME [epoch: 8.31 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29785010655473104		[learning rate: 0.003067]
		[batch 20/20] avg loss: 0.2595713184592755		[learning rate: 0.0030634]
	Learning Rate: 0.00306341
	LOSS [training: 0.2787107125070032 | validation: 0.24595981692213384]
	TIME [epoch: 8.31 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.277147269448473		[learning rate: 0.0030598]
		[batch 20/20] avg loss: 0.2536121206681695		[learning rate: 0.0030562]
	Learning Rate: 0.00305618
	LOSS [training: 0.26537969505832126 | validation: 0.3078915878653732]
	TIME [epoch: 8.33 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28186108869937715		[learning rate: 0.0030526]
		[batch 20/20] avg loss: 0.32781793012906063		[learning rate: 0.003049]
	Learning Rate: 0.00304897
	LOSS [training: 0.3048395094142188 | validation: 0.22188895424852567]
	TIME [epoch: 8.31 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2789970118746205		[learning rate: 0.0030454]
		[batch 20/20] avg loss: 0.2837071117813629		[learning rate: 0.0030418]
	Learning Rate: 0.00304178
	LOSS [training: 0.2813520618279917 | validation: 0.2608029680830692]
	TIME [epoch: 8.31 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.278769594881916		[learning rate: 0.0030382]
		[batch 20/20] avg loss: 0.4277691013621947		[learning rate: 0.0030346]
	Learning Rate: 0.00303461
	LOSS [training: 0.35326934812205535 | validation: 0.22168846062032463]
	TIME [epoch: 8.31 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26577688452834775		[learning rate: 0.003031]
		[batch 20/20] avg loss: 0.2776977598547708		[learning rate: 0.0030274]
	Learning Rate: 0.00302745
	LOSS [training: 0.27173732219155927 | validation: 0.22221025052940668]
	TIME [epoch: 8.33 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29267869046122974		[learning rate: 0.0030239]
		[batch 20/20] avg loss: 0.3448750426618279		[learning rate: 0.0030203]
	Learning Rate: 0.00302031
	LOSS [training: 0.3187768665615288 | validation: 0.2524960841545063]
	TIME [epoch: 8.31 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3478767559908092		[learning rate: 0.0030167]
		[batch 20/20] avg loss: 0.36569127520665196		[learning rate: 0.0030132]
	Learning Rate: 0.00301318
	LOSS [training: 0.35678401559873063 | validation: 0.20743566250325446]
	TIME [epoch: 8.31 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.290846284335689		[learning rate: 0.0030096]
		[batch 20/20] avg loss: 0.2613960983920806		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.27612119136388474 | validation: 0.22127491941740782]
	TIME [epoch: 8.31 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2836621724723954		[learning rate: 0.0030025]
		[batch 20/20] avg loss: 0.27871739509809756		[learning rate: 0.002999]
	Learning Rate: 0.00299899
	LOSS [training: 0.2811897837852465 | validation: 0.33405746068544695]
	TIME [epoch: 8.32 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24341285858896375		[learning rate: 0.0029954]
		[batch 20/20] avg loss: 0.24568870347641875		[learning rate: 0.0029919]
	Learning Rate: 0.00299191
	LOSS [training: 0.24455078103269123 | validation: 0.27108689741801034]
	TIME [epoch: 8.32 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2512419852531477		[learning rate: 0.0029884]
		[batch 20/20] avg loss: 0.2922506961669951		[learning rate: 0.0029849]
	Learning Rate: 0.00298485
	LOSS [training: 0.2717463407100715 | validation: 0.3072308874434262]
	TIME [epoch: 8.31 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.242952080008843		[learning rate: 0.0029813]
		[batch 20/20] avg loss: 0.24198764411036247		[learning rate: 0.0029778]
	Learning Rate: 0.00297781
	LOSS [training: 0.24246986205960272 | validation: 0.20567093004232379]
	TIME [epoch: 8.31 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26692170229385265		[learning rate: 0.0029743]
		[batch 20/20] avg loss: 0.2933179862632906		[learning rate: 0.0029708]
	Learning Rate: 0.00297079
	LOSS [training: 0.2801198442785716 | validation: 0.19902728228653016]
	TIME [epoch: 8.31 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25433836237545593		[learning rate: 0.0029673]
		[batch 20/20] avg loss: 0.21850419391286474		[learning rate: 0.0029638]
	Learning Rate: 0.00296378
	LOSS [training: 0.23642127814416028 | validation: 0.16530599143519847]
	TIME [epoch: 8.34 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2619723941322054		[learning rate: 0.0029603]
		[batch 20/20] avg loss: 0.27668508444205997		[learning rate: 0.0029568]
	Learning Rate: 0.00295679
	LOSS [training: 0.26932873928713263 | validation: 0.3451291272820365]
	TIME [epoch: 8.31 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25633332394556596		[learning rate: 0.0029533]
		[batch 20/20] avg loss: 0.2186117538166934		[learning rate: 0.0029498]
	Learning Rate: 0.00294982
	LOSS [training: 0.23747253888112968 | validation: 0.33064882490489267]
	TIME [epoch: 8.31 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3113952844184757		[learning rate: 0.0029463]
		[batch 20/20] avg loss: 0.3515313637884513		[learning rate: 0.0029429]
	Learning Rate: 0.00294286
	LOSS [training: 0.3314633241034634 | validation: 0.40599632239375494]
	TIME [epoch: 8.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27770332288388827		[learning rate: 0.0029394]
		[batch 20/20] avg loss: 0.23764385347281544		[learning rate: 0.0029359]
	Learning Rate: 0.00293592
	LOSS [training: 0.2576735881783519 | validation: 0.2974602062570795]
	TIME [epoch: 8.33 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2762381886836803		[learning rate: 0.0029325]
		[batch 20/20] avg loss: 0.26137893781732846		[learning rate: 0.002929]
	Learning Rate: 0.00292899
	LOSS [training: 0.26880856325050445 | validation: 0.30080017907438117]
	TIME [epoch: 8.31 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2653596666944074		[learning rate: 0.0029255]
		[batch 20/20] avg loss: 0.3046207306150123		[learning rate: 0.0029221]
	Learning Rate: 0.00292208
	LOSS [training: 0.2849901986547098 | validation: 0.28810073234844247]
	TIME [epoch: 8.31 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3252142463839675		[learning rate: 0.0029186]
		[batch 20/20] avg loss: 0.22993424958616954		[learning rate: 0.0029152]
	Learning Rate: 0.00291519
	LOSS [training: 0.2775742479850685 | validation: 0.39480451769985225]
	TIME [epoch: 8.31 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29930097913652526		[learning rate: 0.0029117]
		[batch 20/20] avg loss: 0.3157501529160093		[learning rate: 0.0029083]
	Learning Rate: 0.00290831
	LOSS [training: 0.3075255660262673 | validation: 0.2941751915028731]
	TIME [epoch: 8.31 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26404244633809476		[learning rate: 0.0029049]
		[batch 20/20] avg loss: 0.26373296475511565		[learning rate: 0.0029015]
	Learning Rate: 0.00290145
	LOSS [training: 0.2638877055466051 | validation: 0.33477172183987836]
	TIME [epoch: 8.33 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24707776826305583		[learning rate: 0.002898]
		[batch 20/20] avg loss: 0.2638880935302513		[learning rate: 0.0028946]
	Learning Rate: 0.00289461
	LOSS [training: 0.25548293089665347 | validation: 0.36016882963816244]
	TIME [epoch: 8.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22987332558172752		[learning rate: 0.0028912]
		[batch 20/20] avg loss: 0.2093698811530731		[learning rate: 0.0028878]
	Learning Rate: 0.00288778
	LOSS [training: 0.21962160336740028 | validation: 0.3063013984794132]
	TIME [epoch: 8.31 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2340414084987658		[learning rate: 0.0028844]
		[batch 20/20] avg loss: 0.24918476580684704		[learning rate: 0.002881]
	Learning Rate: 0.00288097
	LOSS [training: 0.24161308715280638 | validation: 0.21716181508220633]
	TIME [epoch: 8.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22716961528459795		[learning rate: 0.0028776]
		[batch 20/20] avg loss: 0.2711646427860392		[learning rate: 0.0028742]
	Learning Rate: 0.00287417
	LOSS [training: 0.2491671290353185 | validation: 0.2578058756464413]
	TIME [epoch: 8.33 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22791356113093347		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.2436214242594053		[learning rate: 0.0028674]
	Learning Rate: 0.00286739
	LOSS [training: 0.23576749269516933 | validation: 0.19050938222313274]
	TIME [epoch: 8.31 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2875753106548587		[learning rate: 0.002864]
		[batch 20/20] avg loss: 0.24766792618302064		[learning rate: 0.0028606]
	Learning Rate: 0.00286063
	LOSS [training: 0.26762161841893967 | validation: 0.3590247530154654]
	TIME [epoch: 8.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3228740692514118		[learning rate: 0.0028573]
		[batch 20/20] avg loss: 0.2636398258932996		[learning rate: 0.0028539]
	Learning Rate: 0.00285388
	LOSS [training: 0.2932569475723557 | validation: 0.26643247465204245]
	TIME [epoch: 8.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2616385217702482		[learning rate: 0.0028505]
		[batch 20/20] avg loss: 0.28122920554815817		[learning rate: 0.0028471]
	Learning Rate: 0.00284715
	LOSS [training: 0.2714338636592032 | validation: 0.2641501434834276]
	TIME [epoch: 8.32 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2579816097817058		[learning rate: 0.0028438]
		[batch 20/20] avg loss: 0.2602879523719711		[learning rate: 0.0028404]
	Learning Rate: 0.00284043
	LOSS [training: 0.2591347810768384 | validation: 0.2748393257485734]
	TIME [epoch: 8.31 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24232823387271435		[learning rate: 0.0028371]
		[batch 20/20] avg loss: 0.252329116073893		[learning rate: 0.0028337]
	Learning Rate: 0.00283373
	LOSS [training: 0.24732867497330369 | validation: 0.19354611256042875]
	TIME [epoch: 8.31 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31198001606776704		[learning rate: 0.0028304]
		[batch 20/20] avg loss: 0.2952703906256525		[learning rate: 0.002827]
	Learning Rate: 0.00282705
	LOSS [training: 0.3036252033467098 | validation: 0.3288535951705578]
	TIME [epoch: 8.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28403465143684503		[learning rate: 0.0028237]
		[batch 20/20] avg loss: 0.25254765986523		[learning rate: 0.0028204]
	Learning Rate: 0.00282038
	LOSS [training: 0.2682911556510375 | validation: 0.185277042425616]
	TIME [epoch: 8.31 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2921091581160036		[learning rate: 0.0028171]
		[batch 20/20] avg loss: 0.22111598929128867		[learning rate: 0.0028137]
	Learning Rate: 0.00281373
	LOSS [training: 0.2566125737036461 | validation: 0.23320238372258195]
	TIME [epoch: 8.33 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21416852661131763		[learning rate: 0.0028104]
		[batch 20/20] avg loss: 0.22751593268740686		[learning rate: 0.0028071]
	Learning Rate: 0.00280709
	LOSS [training: 0.22084222964936223 | validation: 0.21536986557091214]
	TIME [epoch: 8.31 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26523573958388097		[learning rate: 0.0028038]
		[batch 20/20] avg loss: 0.22457089868836982		[learning rate: 0.0028005]
	Learning Rate: 0.00280047
	LOSS [training: 0.2449033191361253 | validation: 0.26580265552968]
	TIME [epoch: 8.31 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2390204937785259		[learning rate: 0.0027972]
		[batch 20/20] avg loss: 0.21976987158306419		[learning rate: 0.0027939]
	Learning Rate: 0.00279386
	LOSS [training: 0.229395182680795 | validation: 0.29858664519364936]
	TIME [epoch: 8.31 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23226716106350187		[learning rate: 0.0027906]
		[batch 20/20] avg loss: 0.25844785008610743		[learning rate: 0.0027873]
	Learning Rate: 0.00278727
	LOSS [training: 0.2453575055748046 | validation: 0.2143721889181661]
	TIME [epoch: 8.33 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22787128243942772		[learning rate: 0.002784]
		[batch 20/20] avg loss: 0.2307243080789015		[learning rate: 0.0027807]
	Learning Rate: 0.0027807
	LOSS [training: 0.22929779525916466 | validation: 0.2077314287181421]
	TIME [epoch: 8.31 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2514269986116981		[learning rate: 0.0027774]
		[batch 20/20] avg loss: 0.23866438314919827		[learning rate: 0.0027741]
	Learning Rate: 0.00277414
	LOSS [training: 0.24504569088044822 | validation: 0.156135954897766]
	TIME [epoch: 8.31 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2451148701815719		[learning rate: 0.0027709]
		[batch 20/20] avg loss: 0.19827026130155362		[learning rate: 0.0027676]
	Learning Rate: 0.00276759
	LOSS [training: 0.22169256574156276 | validation: 0.1918462299693276]
	TIME [epoch: 8.31 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23092873114999984		[learning rate: 0.0027643]
		[batch 20/20] avg loss: 0.24259853934451536		[learning rate: 0.0027611]
	Learning Rate: 0.00276107
	LOSS [training: 0.23676363524725758 | validation: 0.17134296387334289]
	TIME [epoch: 8.32 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21422639717231018		[learning rate: 0.0027578]
		[batch 20/20] avg loss: 0.23999903684425122		[learning rate: 0.0027546]
	Learning Rate: 0.00275455
	LOSS [training: 0.22711271700828067 | validation: 0.20430013996228885]
	TIME [epoch: 8.32 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35963128265739985		[learning rate: 0.0027513]
		[batch 20/20] avg loss: 0.2348355847900269		[learning rate: 0.0027481]
	Learning Rate: 0.00274806
	LOSS [training: 0.29723343372371336 | validation: 0.2873338416343043]
	TIME [epoch: 8.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24207543577754215		[learning rate: 0.0027448]
		[batch 20/20] avg loss: 0.25542071931912547		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.24874807754833372 | validation: 0.19279354411297758]
	TIME [epoch: 8.31 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2472990114146052		[learning rate: 0.0027383]
		[batch 20/20] avg loss: 0.2310744833939299		[learning rate: 0.0027351]
	Learning Rate: 0.00273511
	LOSS [training: 0.23918674740426754 | validation: 0.19914629499990358]
	TIME [epoch: 8.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22101901742901794		[learning rate: 0.0027319]
		[batch 20/20] avg loss: 0.1941730756541851		[learning rate: 0.0027287]
	Learning Rate: 0.00272866
	LOSS [training: 0.2075960465416015 | validation: 0.16971817245604548]
	TIME [epoch: 8.33 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29374013785651654		[learning rate: 0.0027254]
		[batch 20/20] avg loss: 0.2325459467180202		[learning rate: 0.0027222]
	Learning Rate: 0.00272222
	LOSS [training: 0.26314304228726837 | validation: 0.2709400762560599]
	TIME [epoch: 8.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25156012859991683		[learning rate: 0.002719]
		[batch 20/20] avg loss: 0.21248273786906657		[learning rate: 0.0027158]
	Learning Rate: 0.0027158
	LOSS [training: 0.23202143323449173 | validation: 0.1999474599276096]
	TIME [epoch: 8.31 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22156840606868725		[learning rate: 0.0027126]
		[batch 20/20] avg loss: 0.19133441062083914		[learning rate: 0.0027094]
	Learning Rate: 0.00270939
	LOSS [training: 0.2064514083447632 | validation: 0.24011468538724406]
	TIME [epoch: 8.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20188170364705002		[learning rate: 0.0027062]
		[batch 20/20] avg loss: 0.3093579832775645		[learning rate: 0.002703]
	Learning Rate: 0.002703
	LOSS [training: 0.25561984346230726 | validation: 0.24891537627381252]
	TIME [epoch: 8.33 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24669965084187856		[learning rate: 0.0026998]
		[batch 20/20] avg loss: 0.198217306425099		[learning rate: 0.0026966]
	Learning Rate: 0.00269662
	LOSS [training: 0.22245847863348872 | validation: 0.2653635210944149]
	TIME [epoch: 8.32 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23771820554610165		[learning rate: 0.0026934]
		[batch 20/20] avg loss: 0.22486226080279784		[learning rate: 0.0026903]
	Learning Rate: 0.00269026
	LOSS [training: 0.23129023317444974 | validation: 0.18293157881044403]
	TIME [epoch: 8.31 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23001261180176508		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.23556399235027126		[learning rate: 0.0026839]
	Learning Rate: 0.00268392
	LOSS [training: 0.23278830207601814 | validation: 0.1883389202599633]
	TIME [epoch: 8.31 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21694273061118724		[learning rate: 0.0026808]
		[batch 20/20] avg loss: 0.21055411204838198		[learning rate: 0.0026776]
	Learning Rate: 0.00267759
	LOSS [training: 0.2137484213297846 | validation: 0.2535242768938879]
	TIME [epoch: 8.32 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24480865108293734		[learning rate: 0.0026744]
		[batch 20/20] avg loss: 0.23152976635658348		[learning rate: 0.0026713]
	Learning Rate: 0.00267127
	LOSS [training: 0.2381692087197605 | validation: 0.20319902751784769]
	TIME [epoch: 8.31 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2264713873807		[learning rate: 0.0026681]
		[batch 20/20] avg loss: 0.2825582261954671		[learning rate: 0.002665]
	Learning Rate: 0.00266497
	LOSS [training: 0.2545148067880835 | validation: 0.24179359114337406]
	TIME [epoch: 8.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19952604548380032		[learning rate: 0.0026618]
		[batch 20/20] avg loss: 0.31415298512027634		[learning rate: 0.0026587]
	Learning Rate: 0.00265868
	LOSS [training: 0.2568395153020383 | validation: 0.25790307169414406]
	TIME [epoch: 8.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2427646775407642		[learning rate: 0.0026555]
		[batch 20/20] avg loss: 0.21345678763638737		[learning rate: 0.0026524]
	Learning Rate: 0.00265241
	LOSS [training: 0.22811073258857575 | validation: 0.27958145703414383]
	TIME [epoch: 8.31 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27042149927163406		[learning rate: 0.0026493]
		[batch 20/20] avg loss: 0.22641570387227694		[learning rate: 0.0026462]
	Learning Rate: 0.00264616
	LOSS [training: 0.24841860157195547 | validation: 0.16513613062418148]
	TIME [epoch: 8.33 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3158253684876654		[learning rate: 0.002643]
		[batch 20/20] avg loss: 0.22498210442984812		[learning rate: 0.0026399]
	Learning Rate: 0.00263991
	LOSS [training: 0.27040373645875676 | validation: 0.23476176093267956]
	TIME [epoch: 8.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2159015620703221		[learning rate: 0.0026368]
		[batch 20/20] avg loss: 0.20935170740583456		[learning rate: 0.0026337]
	Learning Rate: 0.00263369
	LOSS [training: 0.2126266347380783 | validation: 0.29617727911858444]
	TIME [epoch: 8.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23929245458587572		[learning rate: 0.0026306]
		[batch 20/20] avg loss: 0.2563712741947855		[learning rate: 0.0026275]
	Learning Rate: 0.00262747
	LOSS [training: 0.24783186439033056 | validation: 0.2934928765217335]
	TIME [epoch: 8.31 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22408001128347427		[learning rate: 0.0026244]
		[batch 20/20] avg loss: 0.17290370640044375		[learning rate: 0.0026213]
	Learning Rate: 0.00262128
	LOSS [training: 0.19849185884195902 | validation: 0.15234974583507674]
	TIME [epoch: 8.33 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1874228416977402		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.23969941988400945		[learning rate: 0.0026151]
	Learning Rate: 0.00261509
	LOSS [training: 0.21356113079087483 | validation: 0.30244551247114837]
	TIME [epoch: 8.31 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19945014483704165		[learning rate: 0.002612]
		[batch 20/20] avg loss: 0.2458995998710809		[learning rate: 0.0026089]
	Learning Rate: 0.00260892
	LOSS [training: 0.22267487235406133 | validation: 0.4092194305705529]
	TIME [epoch: 8.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26814166225139624		[learning rate: 0.0026058]
		[batch 20/20] avg loss: 0.2641243893584579		[learning rate: 0.0026028]
	Learning Rate: 0.00260277
	LOSS [training: 0.2661330258049271 | validation: 0.2515911030754711]
	TIME [epoch: 8.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2350977796209035		[learning rate: 0.0025997]
		[batch 20/20] avg loss: 0.21219533238664212		[learning rate: 0.0025966]
	Learning Rate: 0.00259663
	LOSS [training: 0.22364655600377276 | validation: 0.2271360624413133]
	TIME [epoch: 8.31 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22096119382951027		[learning rate: 0.0025936]
		[batch 20/20] avg loss: 0.22702220605189466		[learning rate: 0.0025905]
	Learning Rate: 0.00259051
	LOSS [training: 0.22399169994070248 | validation: 0.34601355429795383]
	TIME [epoch: 8.33 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2256181403199713		[learning rate: 0.0025874]
		[batch 20/20] avg loss: 0.21668355360648922		[learning rate: 0.0025844]
	Learning Rate: 0.0025844
	LOSS [training: 0.22115084696323026 | validation: 0.21830000127354313]
	TIME [epoch: 8.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18895541477117325		[learning rate: 0.0025813]
		[batch 20/20] avg loss: 0.19204697267804147		[learning rate: 0.0025783]
	Learning Rate: 0.0025783
	LOSS [training: 0.19050119372460733 | validation: 0.17929693608691044]
	TIME [epoch: 8.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23077316097828077		[learning rate: 0.0025753]
		[batch 20/20] avg loss: 0.2502328747012504		[learning rate: 0.0025722]
	Learning Rate: 0.00257222
	LOSS [training: 0.24050301783976563 | validation: 0.297026212724971]
	TIME [epoch: 8.31 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21497716454349552		[learning rate: 0.0025692]
		[batch 20/20] avg loss: 0.24190740579290346		[learning rate: 0.0025662]
	Learning Rate: 0.00256615
	LOSS [training: 0.22844228516819948 | validation: 0.16957036950001037]
	TIME [epoch: 8.32 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2926556250960836		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.24413928774374388		[learning rate: 0.0025601]
	Learning Rate: 0.0025601
	LOSS [training: 0.2683974564199138 | validation: 0.19028239164497912]
	TIME [epoch: 8.31 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2341797851909407		[learning rate: 0.0025571]
		[batch 20/20] avg loss: 0.2186461442464994		[learning rate: 0.0025541]
	Learning Rate: 0.00255406
	LOSS [training: 0.22641296471872002 | validation: 0.16399408919816225]
	TIME [epoch: 8.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2257525980737456		[learning rate: 0.002551]
		[batch 20/20] avg loss: 0.23408408937525343		[learning rate: 0.002548]
	Learning Rate: 0.00254803
	LOSS [training: 0.2299183437244995 | validation: 0.2011598868930606]
	TIME [epoch: 8.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2526591937399414		[learning rate: 0.002545]
		[batch 20/20] avg loss: 0.24330493349372642		[learning rate: 0.002542]
	Learning Rate: 0.00254202
	LOSS [training: 0.24798206361683386 | validation: 0.22204244563660713]
	TIME [epoch: 8.32 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20568587576003713		[learning rate: 0.002539]
		[batch 20/20] avg loss: 0.23081261805941272		[learning rate: 0.002536]
	Learning Rate: 0.00253603
	LOSS [training: 0.2182492469097249 | validation: 0.16278492985113713]
	TIME [epoch: 8.31 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23071019883866958		[learning rate: 0.002533]
		[batch 20/20] avg loss: 0.2238296793954889		[learning rate: 0.00253]
	Learning Rate: 0.00253004
	LOSS [training: 0.22726993911707924 | validation: 0.20506390006499065]
	TIME [epoch: 8.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21579435589526982		[learning rate: 0.0025271]
		[batch 20/20] avg loss: 0.2883369642029		[learning rate: 0.0025241]
	Learning Rate: 0.00252408
	LOSS [training: 0.2520656600490848 | validation: 0.1384603701730644]
	TIME [epoch: 8.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16139200994355732		[learning rate: 0.0025211]
		[batch 20/20] avg loss: 0.2575780709793329		[learning rate: 0.0025181]
	Learning Rate: 0.00251812
	LOSS [training: 0.20948504046144514 | validation: 0.15463941326075414]
	TIME [epoch: 8.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2109225269494968		[learning rate: 0.0025152]
		[batch 20/20] avg loss: 0.25291812582781703		[learning rate: 0.0025122]
	Learning Rate: 0.00251218
	LOSS [training: 0.2319203263886569 | validation: 0.2799796539312144]
	TIME [epoch: 8.33 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2615810935491238		[learning rate: 0.0025092]
		[batch 20/20] avg loss: 0.25851114517406537		[learning rate: 0.0025063]
	Learning Rate: 0.00250626
	LOSS [training: 0.26004611936159455 | validation: 0.26042022470703113]
	TIME [epoch: 8.31 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20016746732742682		[learning rate: 0.0025033]
		[batch 20/20] avg loss: 0.2304032295411743		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.2152853484343006 | validation: 0.22167865997219022]
	TIME [epoch: 8.31 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2629146126042525		[learning rate: 0.0024974]
		[batch 20/20] avg loss: 0.2179546620158177		[learning rate: 0.0024944]
	Learning Rate: 0.00249445
	LOSS [training: 0.24043463731003506 | validation: 0.1687296133152925]
	TIME [epoch: 8.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20363138330018388		[learning rate: 0.0024915]
		[batch 20/20] avg loss: 0.23009760994223877		[learning rate: 0.0024886]
	Learning Rate: 0.00248856
	LOSS [training: 0.2168644966212113 | validation: 0.13096974942444672]
	TIME [epoch: 8.33 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2804871230371965		[learning rate: 0.0024856]
		[batch 20/20] avg loss: 0.3070421688388534		[learning rate: 0.0024827]
	Learning Rate: 0.00248269
	LOSS [training: 0.2937646459380249 | validation: 0.24719309956041854]
	TIME [epoch: 8.31 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3056693991287996		[learning rate: 0.0024798]
		[batch 20/20] avg loss: 0.17830055992655494		[learning rate: 0.0024768]
	Learning Rate: 0.00247684
	LOSS [training: 0.24198497952767722 | validation: 0.3417994525920205]
	TIME [epoch: 8.31 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1852419221845502		[learning rate: 0.0024739]
		[batch 20/20] avg loss: 0.23604178353578226		[learning rate: 0.002471]
	Learning Rate: 0.00247099
	LOSS [training: 0.21064185286016626 | validation: 0.2143965734393581]
	TIME [epoch: 8.31 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17092752077753412		[learning rate: 0.0024681]
		[batch 20/20] avg loss: 0.19719304362746654		[learning rate: 0.0024652]
	Learning Rate: 0.00246517
	LOSS [training: 0.1840602822025003 | validation: 0.1683683715089414]
	TIME [epoch: 8.32 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2065573662368842		[learning rate: 0.0024623]
		[batch 20/20] avg loss: 0.2133001098362036		[learning rate: 0.0024594]
	Learning Rate: 0.00245935
	LOSS [training: 0.20992873803654385 | validation: 0.23771201661904406]
	TIME [epoch: 8.31 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21211832847218498		[learning rate: 0.0024564]
		[batch 20/20] avg loss: 0.20480477390149301		[learning rate: 0.0024535]
	Learning Rate: 0.00245355
	LOSS [training: 0.208461551186839 | validation: 0.1496470524938852]
	TIME [epoch: 8.31 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23558223968033465		[learning rate: 0.0024507]
		[batch 20/20] avg loss: 0.2986594368674317		[learning rate: 0.0024478]
	Learning Rate: 0.00244776
	LOSS [training: 0.2671208382738831 | validation: 0.27233184333724275]
	TIME [epoch: 8.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21038765319594604		[learning rate: 0.0024449]
		[batch 20/20] avg loss: 0.2079541234328882		[learning rate: 0.002442]
	Learning Rate: 0.00244199
	LOSS [training: 0.20917088831441713 | validation: 0.20274629571744807]
	TIME [epoch: 8.31 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27409072872735657		[learning rate: 0.0024391]
		[batch 20/20] avg loss: 0.20627775431265225		[learning rate: 0.0024362]
	Learning Rate: 0.00243623
	LOSS [training: 0.2401842415200044 | validation: 0.20386495497111307]
	TIME [epoch: 8.33 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17426407888254433		[learning rate: 0.0024334]
		[batch 20/20] avg loss: 0.2502752056354961		[learning rate: 0.0024305]
	Learning Rate: 0.00243048
	LOSS [training: 0.21226964225902017 | validation: 0.21096508268612377]
	TIME [epoch: 8.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20400102704486672		[learning rate: 0.0024276]
		[batch 20/20] avg loss: 0.17621163971841397		[learning rate: 0.0024247]
	Learning Rate: 0.00242475
	LOSS [training: 0.19010633338164035 | validation: 0.27001281237577357]
	TIME [epoch: 8.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1875478419273934		[learning rate: 0.0024219]
		[batch 20/20] avg loss: 0.19522521449534783		[learning rate: 0.002419]
	Learning Rate: 0.00241903
	LOSS [training: 0.19138652821137064 | validation: 0.16952541268828172]
	TIME [epoch: 8.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22821680937573316		[learning rate: 0.0024162]
		[batch 20/20] avg loss: 0.2046378768244058		[learning rate: 0.0024133]
	Learning Rate: 0.00241332
	LOSS [training: 0.21642734310006945 | validation: 0.17472149272006596]
	TIME [epoch: 8.33 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22095078260808537		[learning rate: 0.0024105]
		[batch 20/20] avg loss: 0.19300746304368493		[learning rate: 0.0024076]
	Learning Rate: 0.00240763
	LOSS [training: 0.2069791228258852 | validation: 0.16800724642023934]
	TIME [epoch: 8.31 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21919534777386573		[learning rate: 0.0024048]
		[batch 20/20] avg loss: 0.24484644846977455		[learning rate: 0.002402]
	Learning Rate: 0.00240195
	LOSS [training: 0.2320208981218202 | validation: 0.2077765609014975]
	TIME [epoch: 8.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18208281634278467		[learning rate: 0.0023991]
		[batch 20/20] avg loss: 0.2061109662275323		[learning rate: 0.0023963]
	Learning Rate: 0.00239628
	LOSS [training: 0.1940968912851585 | validation: 0.23703519450160676]
	TIME [epoch: 8.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18890479040123376		[learning rate: 0.0023935]
		[batch 20/20] avg loss: 0.21782398677223255		[learning rate: 0.0023906]
	Learning Rate: 0.00239063
	LOSS [training: 0.2033643885867332 | validation: 0.5127587050729892]
	TIME [epoch: 8.32 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2318019926645559		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.20786598043315713		[learning rate: 0.002385]
	Learning Rate: 0.00238499
	LOSS [training: 0.2198339865488564 | validation: 0.3169085376696262]
	TIME [epoch: 8.32 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21000976760739043		[learning rate: 0.0023822]
		[batch 20/20] avg loss: 0.18787162156122603		[learning rate: 0.0023794]
	Learning Rate: 0.00237937
	LOSS [training: 0.19894069458430824 | validation: 0.22047125811797613]
	TIME [epoch: 8.29 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21235967414382356		[learning rate: 0.0023766]
		[batch 20/20] avg loss: 0.22940375920964504		[learning rate: 0.0023738]
	Learning Rate: 0.00237375
	LOSS [training: 0.22088171667673434 | validation: 0.24951299517014044]
	TIME [epoch: 8.29 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22113962165280907		[learning rate: 0.002371]
		[batch 20/20] avg loss: 0.19522918658066649		[learning rate: 0.0023682]
	Learning Rate: 0.00236816
	LOSS [training: 0.20818440411673778 | validation: 0.16106237902716944]
	TIME [epoch: 8.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24608588022994296		[learning rate: 0.0023654]
		[batch 20/20] avg loss: 0.193865820345695		[learning rate: 0.0023626]
	Learning Rate: 0.00236257
	LOSS [training: 0.21997585028781894 | validation: 0.13316215365559592]
	TIME [epoch: 8.36 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19274551784280114		[learning rate: 0.0023598]
		[batch 20/20] avg loss: 0.20630767532789668		[learning rate: 0.002357]
	Learning Rate: 0.002357
	LOSS [training: 0.1995265965853489 | validation: 0.16064833556995206]
	TIME [epoch: 8.34 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16586355337759276		[learning rate: 0.0023542]
		[batch 20/20] avg loss: 0.20554047079977272		[learning rate: 0.0023514]
	Learning Rate: 0.00235144
	LOSS [training: 0.1857020120886827 | validation: 0.2240449053601597]
	TIME [epoch: 8.32 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21872670960756527		[learning rate: 0.0023487]
		[batch 20/20] avg loss: 0.2080657555107352		[learning rate: 0.0023459]
	Learning Rate: 0.00234589
	LOSS [training: 0.21339623255915022 | validation: 0.12521916055977775]
	TIME [epoch: 8.33 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21691872104234497		[learning rate: 0.0023431]
		[batch 20/20] avg loss: 0.2058429181650685		[learning rate: 0.0023404]
	Learning Rate: 0.00234036
	LOSS [training: 0.21138081960370672 | validation: 0.16243382159495315]
	TIME [epoch: 8.34 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18991090678572692		[learning rate: 0.0023376]
		[batch 20/20] avg loss: 0.17169383909053829		[learning rate: 0.0023348]
	Learning Rate: 0.00233484
	LOSS [training: 0.1808023729381326 | validation: 0.210453175672157]
	TIME [epoch: 8.31 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23872557410169862		[learning rate: 0.0023321]
		[batch 20/20] avg loss: 0.24738640618016156		[learning rate: 0.0023293]
	Learning Rate: 0.00232933
	LOSS [training: 0.24305599014093016 | validation: 0.2682931962658383]
	TIME [epoch: 8.33 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17296598222700815		[learning rate: 0.0023266]
		[batch 20/20] avg loss: 0.20503329903755124		[learning rate: 0.0023238]
	Learning Rate: 0.00232383
	LOSS [training: 0.1889996406322797 | validation: 0.18611647077390736]
	TIME [epoch: 8.33 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18067323611492792		[learning rate: 0.0023211]
		[batch 20/20] avg loss: 0.1916970181191988		[learning rate: 0.0023184]
	Learning Rate: 0.00231835
	LOSS [training: 0.18618512711706334 | validation: 0.44560733925952434]
	TIME [epoch: 8.32 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2659753293180174		[learning rate: 0.0023156]
		[batch 20/20] avg loss: 0.1976342289330697		[learning rate: 0.0023129]
	Learning Rate: 0.00231288
	LOSS [training: 0.23180477912554354 | validation: 0.1898381318023005]
	TIME [epoch: 8.32 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17897591003375274		[learning rate: 0.0023102]
		[batch 20/20] avg loss: 0.22909478154839893		[learning rate: 0.0023074]
	Learning Rate: 0.00230743
	LOSS [training: 0.2040353457910758 | validation: 0.14404528753375972]
	TIME [epoch: 8.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2281883977241082		[learning rate: 0.0023047]
		[batch 20/20] avg loss: 0.2038751282152634		[learning rate: 0.002302]
	Learning Rate: 0.00230199
	LOSS [training: 0.21603176296968582 | validation: 0.282021199710706]
	TIME [epoch: 8.31 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2267176767305037		[learning rate: 0.0022993]
		[batch 20/20] avg loss: 0.20527781448258325		[learning rate: 0.0022966]
	Learning Rate: 0.00229656
	LOSS [training: 0.21599774560654345 | validation: 0.14847164165109822]
	TIME [epoch: 8.31 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1748221738829384		[learning rate: 0.0022938]
		[batch 20/20] avg loss: 0.17827883270977352		[learning rate: 0.0022911]
	Learning Rate: 0.00229114
	LOSS [training: 0.17655050329635597 | validation: 0.22716228565713376]
	TIME [epoch: 8.33 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20556379200458497		[learning rate: 0.0022884]
		[batch 20/20] avg loss: 0.21795425265195076		[learning rate: 0.0022857]
	Learning Rate: 0.00228573
	LOSS [training: 0.21175902232826793 | validation: 0.15012390614526325]
	TIME [epoch: 8.31 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21636495486277424		[learning rate: 0.002283]
		[batch 20/20] avg loss: 0.211163822047833		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.2137643884553036 | validation: 0.17515185034105085]
	TIME [epoch: 8.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20716987243165982		[learning rate: 0.0022777]
		[batch 20/20] avg loss: 0.19446748457903446		[learning rate: 0.002275]
	Learning Rate: 0.00227496
	LOSS [training: 0.20081867850534713 | validation: 0.14379259862126925]
	TIME [epoch: 8.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19963157417864405		[learning rate: 0.0022723]
		[batch 20/20] avg loss: 0.20513742431586285		[learning rate: 0.0022696]
	Learning Rate: 0.0022696
	LOSS [training: 0.20238449924725344 | validation: 0.16006093133510216]
	TIME [epoch: 8.32 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18147755190045486		[learning rate: 0.0022669]
		[batch 20/20] avg loss: 0.21384306633006595		[learning rate: 0.0022642]
	Learning Rate: 0.00226424
	LOSS [training: 0.19766030911526042 | validation: 0.1695654238494298]
	TIME [epoch: 8.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21937635520780746		[learning rate: 0.0022616]
		[batch 20/20] avg loss: 0.2321182797779771		[learning rate: 0.0022589]
	Learning Rate: 0.0022589
	LOSS [training: 0.22574731749289226 | validation: 0.19239079318670177]
	TIME [epoch: 8.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28862086290909666		[learning rate: 0.0022562]
		[batch 20/20] avg loss: 0.22513471101395194		[learning rate: 0.0022536]
	Learning Rate: 0.00225357
	LOSS [training: 0.2568777869615243 | validation: 0.20832412674737316]
	TIME [epoch: 8.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20741915408517322		[learning rate: 0.0022509]
		[batch 20/20] avg loss: 0.2476605771905484		[learning rate: 0.0022483]
	Learning Rate: 0.00224826
	LOSS [training: 0.2275398656378608 | validation: 0.22828216646043986]
	TIME [epoch: 8.31 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1947804159084741		[learning rate: 0.0022456]
		[batch 20/20] avg loss: 0.18988770209547062		[learning rate: 0.002243]
	Learning Rate: 0.00224295
	LOSS [training: 0.19233405900197237 | validation: 0.30924736123148944]
	TIME [epoch: 8.32 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21261399974621176		[learning rate: 0.0022403]
		[batch 20/20] avg loss: 0.19591752951891978		[learning rate: 0.0022377]
	Learning Rate: 0.00223766
	LOSS [training: 0.20426576463256577 | validation: 0.20185708096895516]
	TIME [epoch: 8.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21735304252296647		[learning rate: 0.002235]
		[batch 20/20] avg loss: 0.23652196222051222		[learning rate: 0.0022324]
	Learning Rate: 0.00223239
	LOSS [training: 0.22693750237173935 | validation: 0.14435459579892013]
	TIME [epoch: 8.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16552743333381317		[learning rate: 0.0022298]
		[batch 20/20] avg loss: 0.15853116236767845		[learning rate: 0.0022271]
	Learning Rate: 0.00222712
	LOSS [training: 0.16202929785074577 | validation: 0.22335405739723932]
	TIME [epoch: 8.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2034441900254607		[learning rate: 0.0022245]
		[batch 20/20] avg loss: 0.21488074200888857		[learning rate: 0.0022219]
	Learning Rate: 0.00222187
	LOSS [training: 0.20916246601717464 | validation: 0.13376173305586517]
	TIME [epoch: 8.32 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17851460585914042		[learning rate: 0.0022192]
		[batch 20/20] avg loss: 0.24580392065586637		[learning rate: 0.0022166]
	Learning Rate: 0.00221663
	LOSS [training: 0.21215926325750342 | validation: 0.12055027731153131]
	TIME [epoch: 8.31 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1832677841467482		[learning rate: 0.002214]
		[batch 20/20] avg loss: 0.24261590607195305		[learning rate: 0.0022114]
	Learning Rate: 0.0022114
	LOSS [training: 0.21294184510935063 | validation: 0.36002699423426077]
	TIME [epoch: 8.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2007075560158315		[learning rate: 0.0022088]
		[batch 20/20] avg loss: 0.22051549074491056		[learning rate: 0.0022062]
	Learning Rate: 0.00220618
	LOSS [training: 0.210611523380371 | validation: 0.27709677002662425]
	TIME [epoch: 8.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19966739839797856		[learning rate: 0.0022036]
		[batch 20/20] avg loss: 0.21398586448365994		[learning rate: 0.002201]
	Learning Rate: 0.00220098
	LOSS [training: 0.20682663144081928 | validation: 0.15890720795079016]
	TIME [epoch: 8.31 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19232407587705216		[learning rate: 0.0021984]
		[batch 20/20] avg loss: 0.19024003479768997		[learning rate: 0.0021958]
	Learning Rate: 0.00219578
	LOSS [training: 0.19128205533737108 | validation: 0.18849080688264397]
	TIME [epoch: 8.31 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18167278999740713		[learning rate: 0.0021932]
		[batch 20/20] avg loss: 0.16294754630347946		[learning rate: 0.0021906]
	Learning Rate: 0.0021906
	LOSS [training: 0.1723101681504433 | validation: 0.12828664506923504]
	TIME [epoch: 8.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17357320586351788		[learning rate: 0.002188]
		[batch 20/20] avg loss: 0.20230074277895566		[learning rate: 0.0021854]
	Learning Rate: 0.00218544
	LOSS [training: 0.18793697432123677 | validation: 0.1389899741891817]
	TIME [epoch: 8.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18542030919903502		[learning rate: 0.0021829]
		[batch 20/20] avg loss: 0.19130324647536318		[learning rate: 0.0021803]
	Learning Rate: 0.00218028
	LOSS [training: 0.1883617778371991 | validation: 0.22814919485237664]
	TIME [epoch: 8.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24616659697777538		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.3220852276982733		[learning rate: 0.0021751]
	Learning Rate: 0.00217514
	LOSS [training: 0.2841259123380243 | validation: 0.220884460707712]
	TIME [epoch: 8.32 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2384863915770799		[learning rate: 0.0021726]
		[batch 20/20] avg loss: 0.22717031052988257		[learning rate: 0.00217]
	Learning Rate: 0.00217001
	LOSS [training: 0.2328283510534812 | validation: 0.1834877501798593]
	TIME [epoch: 8.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22961770076499924		[learning rate: 0.0021674]
		[batch 20/20] avg loss: 0.19811294979459923		[learning rate: 0.0021649]
	Learning Rate: 0.00216489
	LOSS [training: 0.21386532527979923 | validation: 0.17891649699491752]
	TIME [epoch: 8.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20811357727539964		[learning rate: 0.0021623]
		[batch 20/20] avg loss: 0.18029969305656787		[learning rate: 0.0021598]
	Learning Rate: 0.00215978
	LOSS [training: 0.19420663516598374 | validation: 0.163729513492876]
	TIME [epoch: 8.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17818364123740338		[learning rate: 0.0021572]
		[batch 20/20] avg loss: 0.18771940084179592		[learning rate: 0.0021547]
	Learning Rate: 0.00215469
	LOSS [training: 0.18295152103959966 | validation: 0.1495395315562453]
	TIME [epoch: 8.32 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.203118195522135		[learning rate: 0.0021521]
		[batch 20/20] avg loss: 0.20396109403242293		[learning rate: 0.0021496]
	Learning Rate: 0.00214961
	LOSS [training: 0.20353964477727898 | validation: 0.2991838567210519]
	TIME [epoch: 8.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1957177381317569		[learning rate: 0.0021471]
		[batch 20/20] avg loss: 0.189095291984506		[learning rate: 0.0021445]
	Learning Rate: 0.00214454
	LOSS [training: 0.19240651505813147 | validation: 0.1325487608060641]
	TIME [epoch: 8.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1962574999287266		[learning rate: 0.002142]
		[batch 20/20] avg loss: 0.1989658590004868		[learning rate: 0.0021395]
	Learning Rate: 0.00213948
	LOSS [training: 0.1976116794646067 | validation: 0.10897943969259052]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_703.pth
	Model improved!!!
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15544313951723135		[learning rate: 0.002137]
		[batch 20/20] avg loss: 0.1892708888453328		[learning rate: 0.0021344]
	Learning Rate: 0.00213443
	LOSS [training: 0.17235701418128205 | validation: 0.18702063330439783]
	TIME [epoch: 8.32 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1906734621050147		[learning rate: 0.0021319]
		[batch 20/20] avg loss: 0.20961005847711162		[learning rate: 0.0021294]
	Learning Rate: 0.0021294
	LOSS [training: 0.20014176029106318 | validation: 0.19134137815612365]
	TIME [epoch: 8.31 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27934489628333314		[learning rate: 0.0021269]
		[batch 20/20] avg loss: 0.2290614280203053		[learning rate: 0.0021244]
	Learning Rate: 0.00212437
	LOSS [training: 0.25420316215181915 | validation: 0.2199541237499773]
	TIME [epoch: 8.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17694853795743282		[learning rate: 0.0021219]
		[batch 20/20] avg loss: 0.22033952661868725		[learning rate: 0.0021194]
	Learning Rate: 0.00211936
	LOSS [training: 0.19864403228806 | validation: 0.1693780519716174]
	TIME [epoch: 8.29 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21388076302250977		[learning rate: 0.0021169]
		[batch 20/20] avg loss: 0.19561644646504406		[learning rate: 0.0021144]
	Learning Rate: 0.00211436
	LOSS [training: 0.20474860474377693 | validation: 0.16629839063982677]
	TIME [epoch: 8.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16817298707456194		[learning rate: 0.0021119]
		[batch 20/20] avg loss: 0.19741291697872684		[learning rate: 0.0021094]
	Learning Rate: 0.00210938
	LOSS [training: 0.1827929520266444 | validation: 0.20296166142320854]
	TIME [epoch: 8.32 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16700921251783346		[learning rate: 0.0021069]
		[batch 20/20] avg loss: 0.21142533847256545		[learning rate: 0.0021044]
	Learning Rate: 0.0021044
	LOSS [training: 0.18921727549519946 | validation: 0.1312029136691484]
	TIME [epoch: 8.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23164498498871086		[learning rate: 0.0021019]
		[batch 20/20] avg loss: 0.1433558470122484		[learning rate: 0.0020994]
	Learning Rate: 0.00209944
	LOSS [training: 0.18750041600047967 | validation: 0.11973655680937828]
	TIME [epoch: 8.29 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15138924793295042		[learning rate: 0.002097]
		[batch 20/20] avg loss: 0.24472899312361754		[learning rate: 0.0020945]
	Learning Rate: 0.00209448
	LOSS [training: 0.19805912052828395 | validation: 0.2878459476123262]
	TIME [epoch: 8.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20126558953153192		[learning rate: 0.002092]
		[batch 20/20] avg loss: 0.18218643982925847		[learning rate: 0.0020895]
	Learning Rate: 0.00208954
	LOSS [training: 0.19172601468039524 | validation: 0.2639315019885114]
	TIME [epoch: 8.33 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25672803042782977		[learning rate: 0.0020871]
		[batch 20/20] avg loss: 0.22342310304017796		[learning rate: 0.0020846]
	Learning Rate: 0.00208461
	LOSS [training: 0.24007556673400382 | validation: 0.25548356228083907]
	TIME [epoch: 8.29 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1979233320502232		[learning rate: 0.0020822]
		[batch 20/20] avg loss: 0.17969939971845345		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.18881136588433836 | validation: 0.21848307102956696]
	TIME [epoch: 8.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17892662331655793		[learning rate: 0.0020772]
		[batch 20/20] avg loss: 0.21294317042088612		[learning rate: 0.0020748]
	Learning Rate: 0.00207479
	LOSS [training: 0.19593489686872206 | validation: 0.17038754592834843]
	TIME [epoch: 8.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17565273732352188		[learning rate: 0.0020723]
		[batch 20/20] avg loss: 0.21889908768213098		[learning rate: 0.0020699]
	Learning Rate: 0.0020699
	LOSS [training: 0.1972759125028264 | validation: 0.16918847536032744]
	TIME [epoch: 8.32 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19458346047714598		[learning rate: 0.0020675]
		[batch 20/20] avg loss: 0.1968035698585268		[learning rate: 0.002065]
	Learning Rate: 0.00206501
	LOSS [training: 0.19569351516783634 | validation: 0.33096792086614235]
	TIME [epoch: 8.31 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18355842725323976		[learning rate: 0.0020626]
		[batch 20/20] avg loss: 0.17023610410197176		[learning rate: 0.0020601]
	Learning Rate: 0.00206014
	LOSS [training: 0.17689726567760578 | validation: 0.2287989839873724]
	TIME [epoch: 8.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1918406636542665		[learning rate: 0.0020577]
		[batch 20/20] avg loss: 0.14654811236975127		[learning rate: 0.0020553]
	Learning Rate: 0.00205528
	LOSS [training: 0.16919438801200887 | validation: 0.272425350740536]
	TIME [epoch: 8.29 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2389187859804855		[learning rate: 0.0020529]
		[batch 20/20] avg loss: 0.19425627033053527		[learning rate: 0.0020504]
	Learning Rate: 0.00205044
	LOSS [training: 0.21658752815551038 | validation: 0.17978843279053047]
	TIME [epoch: 8.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17480773628487284		[learning rate: 0.002048]
		[batch 20/20] avg loss: 0.1727933682135999		[learning rate: 0.0020456]
	Learning Rate: 0.0020456
	LOSS [training: 0.17380055224923635 | validation: 0.17346006695996435]
	TIME [epoch: 8.32 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20192483598357266		[learning rate: 0.0020432]
		[batch 20/20] avg loss: 0.15667037155149305		[learning rate: 0.0020408]
	Learning Rate: 0.00204077
	LOSS [training: 0.17929760376753287 | validation: 0.23373950799829715]
	TIME [epoch: 8.29 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18794169946923864		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.28823765031158555		[learning rate: 0.002036]
	Learning Rate: 0.00203596
	LOSS [training: 0.23808967489041208 | validation: 0.15679159632522247]
	TIME [epoch: 8.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17006210568098842		[learning rate: 0.0020336]
		[batch 20/20] avg loss: 0.19440996907365213		[learning rate: 0.0020312]
	Learning Rate: 0.00203116
	LOSS [training: 0.18223603737732028 | validation: 0.17938718996888817]
	TIME [epoch: 8.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18332777282199966		[learning rate: 0.0020288]
		[batch 20/20] avg loss: 0.17027026125951525		[learning rate: 0.0020264]
	Learning Rate: 0.00202637
	LOSS [training: 0.17679901704075743 | validation: 0.18491567244150148]
	TIME [epoch: 8.32 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1653477921220206		[learning rate: 0.002024]
		[batch 20/20] avg loss: 0.16403648569964657		[learning rate: 0.0020216]
	Learning Rate: 0.00202159
	LOSS [training: 0.16469213891083362 | validation: 0.14162425217856267]
	TIME [epoch: 8.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1941237758359929		[learning rate: 0.0020192]
		[batch 20/20] avg loss: 0.18082076041011227		[learning rate: 0.0020168]
	Learning Rate: 0.00201682
	LOSS [training: 0.18747226812305262 | validation: 0.19624647499212525]
	TIME [epoch: 8.29 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2211546318282779		[learning rate: 0.0020144]
		[batch 20/20] avg loss: 0.22135234225169742		[learning rate: 0.0020121]
	Learning Rate: 0.00201206
	LOSS [training: 0.22125348703998765 | validation: 0.33530632580324626]
	TIME [epoch: 8.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23827037345100593		[learning rate: 0.0020097]
		[batch 20/20] avg loss: 0.18904073935522703		[learning rate: 0.0020073]
	Learning Rate: 0.00200731
	LOSS [training: 0.21365555640311645 | validation: 0.1633636924907877]
	TIME [epoch: 8.31 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27099451278814785		[learning rate: 0.0020049]
		[batch 20/20] avg loss: 0.20189424496608432		[learning rate: 0.0020026]
	Learning Rate: 0.00200258
	LOSS [training: 0.23644437887711606 | validation: 0.2073694716935275]
	TIME [epoch: 8.31 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1712180784946214		[learning rate: 0.0020002]
		[batch 20/20] avg loss: 0.21606681972200717		[learning rate: 0.0019979]
	Learning Rate: 0.00199786
	LOSS [training: 0.19364244910831432 | validation: 0.14613255837352185]
	TIME [epoch: 8.29 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2279206465233441		[learning rate: 0.0019955]
		[batch 20/20] avg loss: 0.2253024365830764		[learning rate: 0.0019931]
	Learning Rate: 0.00199314
	LOSS [training: 0.22661154155321026 | validation: 0.17561236321840884]
	TIME [epoch: 8.29 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18747125111235607		[learning rate: 0.0019908]
		[batch 20/20] avg loss: 0.19437196528089146		[learning rate: 0.0019884]
	Learning Rate: 0.00198844
	LOSS [training: 0.19092160819662377 | validation: 0.15999198625248812]
	TIME [epoch: 8.29 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19339883864326993		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.19242073377939015		[learning rate: 0.0019838]
	Learning Rate: 0.00198375
	LOSS [training: 0.19290978621133004 | validation: 0.21617356586715908]
	TIME [epoch: 8.31 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23616106356994476		[learning rate: 0.0019814]
		[batch 20/20] avg loss: 0.2486122691759915		[learning rate: 0.0019791]
	Learning Rate: 0.00197907
	LOSS [training: 0.24238666637296813 | validation: 0.1917073244790583]
	TIME [epoch: 8.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20592680523298695		[learning rate: 0.0019767]
		[batch 20/20] avg loss: 0.19138669168657793		[learning rate: 0.0019744]
	Learning Rate: 0.0019744
	LOSS [training: 0.19865674845978246 | validation: 0.13852646473892055]
	TIME [epoch: 8.29 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14601705927201503		[learning rate: 0.0019721]
		[batch 20/20] avg loss: 0.305486781278572		[learning rate: 0.0019697]
	Learning Rate: 0.00196975
	LOSS [training: 0.22575192027529348 | validation: 0.28874733201536623]
	TIME [epoch: 8.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22393798877431953		[learning rate: 0.0019674]
		[batch 20/20] avg loss: 0.18868797867302617		[learning rate: 0.0019651]
	Learning Rate: 0.0019651
	LOSS [training: 0.20631298372367288 | validation: 0.14867114756041616]
	TIME [epoch: 8.32 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20441259413365537		[learning rate: 0.0019628]
		[batch 20/20] avg loss: 0.15856132366810238		[learning rate: 0.0019605]
	Learning Rate: 0.00196046
	LOSS [training: 0.18148695890087888 | validation: 0.1274633972172149]
	TIME [epoch: 8.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19547532849996013		[learning rate: 0.0019582]
		[batch 20/20] avg loss: 0.1603305083390743		[learning rate: 0.0019558]
	Learning Rate: 0.00195584
	LOSS [training: 0.1779029184195172 | validation: 0.1609278250064321]
	TIME [epoch: 8.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18886740553989653		[learning rate: 0.0019535]
		[batch 20/20] avg loss: 0.2073271893648858		[learning rate: 0.0019512]
	Learning Rate: 0.00195123
	LOSS [training: 0.19809729745239119 | validation: 0.19488550335950877]
	TIME [epoch: 8.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16726443577448216		[learning rate: 0.0019489]
		[batch 20/20] avg loss: 0.15629176261204464		[learning rate: 0.0019466]
	Learning Rate: 0.00194662
	LOSS [training: 0.16177809919326339 | validation: 0.14840390471342374]
	TIME [epoch: 8.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17881754490836044		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.1726252168091139		[learning rate: 0.001942]
	Learning Rate: 0.00194203
	LOSS [training: 0.1757213808587372 | validation: 0.20056211680292155]
	TIME [epoch: 8.32 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17125788649273835		[learning rate: 0.0019397]
		[batch 20/20] avg loss: 0.15772396808945816		[learning rate: 0.0019375]
	Learning Rate: 0.00193745
	LOSS [training: 0.16449092729109824 | validation: 0.1772290161625008]
	TIME [epoch: 8.29 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19135970916480388		[learning rate: 0.0019352]
		[batch 20/20] avg loss: 0.21521986970570667		[learning rate: 0.0019329]
	Learning Rate: 0.00193288
	LOSS [training: 0.20328978943525527 | validation: 0.10250600819404009]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_746.pth
	Model improved!!!
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13824736332265788		[learning rate: 0.0019306]
		[batch 20/20] avg loss: 0.197892025804503		[learning rate: 0.0019283]
	Learning Rate: 0.00192832
	LOSS [training: 0.16806969456358045 | validation: 0.15002591792669015]
	TIME [epoch: 8.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21276447041708155		[learning rate: 0.001926]
		[batch 20/20] avg loss: 0.16034651098451158		[learning rate: 0.0019238]
	Learning Rate: 0.00192377
	LOSS [training: 0.18655549070079655 | validation: 0.284469601127402]
	TIME [epoch: 8.32 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1755893467449367		[learning rate: 0.0019215]
		[batch 20/20] avg loss: 0.17388639620013968		[learning rate: 0.0019192]
	Learning Rate: 0.00191924
	LOSS [training: 0.17473787147253822 | validation: 0.10230932084817078]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.167759184767223		[learning rate: 0.001917]
		[batch 20/20] avg loss: 0.15253937355007732		[learning rate: 0.0019147]
	Learning Rate: 0.00191471
	LOSS [training: 0.16014927915865018 | validation: 0.35941842262659046]
	TIME [epoch: 8.28 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20670394492189406		[learning rate: 0.0019124]
		[batch 20/20] avg loss: 0.1863188506965841		[learning rate: 0.0019102]
	Learning Rate: 0.00191019
	LOSS [training: 0.19651139780923904 | validation: 0.23473338836095067]
	TIME [epoch: 8.29 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23704232145309753		[learning rate: 0.0019079]
		[batch 20/20] avg loss: 0.18262537612128069		[learning rate: 0.0019057]
	Learning Rate: 0.00190569
	LOSS [training: 0.20983384878718914 | validation: 0.12317134117925344]
	TIME [epoch: 8.31 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18286119933864248		[learning rate: 0.0019034]
		[batch 20/20] avg loss: 0.19188875489567375		[learning rate: 0.0019012]
	Learning Rate: 0.00190119
	LOSS [training: 0.18737497711715811 | validation: 0.10639743744365754]
	TIME [epoch: 8.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1518465008631568		[learning rate: 0.0018989]
		[batch 20/20] avg loss: 0.19720164168113805		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.1745240712721474 | validation: 0.10534315622080218]
	TIME [epoch: 8.29 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14968562515221534		[learning rate: 0.0018945]
		[batch 20/20] avg loss: 0.16251265436680265		[learning rate: 0.0018922]
	Learning Rate: 0.00189223
	LOSS [training: 0.15609913975950898 | validation: 0.09778874285185973]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13356953921321296		[learning rate: 0.00189]
		[batch 20/20] avg loss: 0.21204432608912813		[learning rate: 0.0018878]
	Learning Rate: 0.00188777
	LOSS [training: 0.17280693265117056 | validation: 0.2276533821940132]
	TIME [epoch: 8.31 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1543746100172311		[learning rate: 0.0018855]
		[batch 20/20] avg loss: 0.17003523916734545		[learning rate: 0.0018833]
	Learning Rate: 0.00188332
	LOSS [training: 0.16220492459228827 | validation: 0.19476516404412544]
	TIME [epoch: 8.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16438340806757856		[learning rate: 0.0018811]
		[batch 20/20] avg loss: 0.16561305342794647		[learning rate: 0.0018789]
	Learning Rate: 0.00187887
	LOSS [training: 0.16499823074776251 | validation: 0.1271523900491443]
	TIME [epoch: 8.29 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18124322966650225		[learning rate: 0.0018767]
		[batch 20/20] avg loss: 0.17664784496189073		[learning rate: 0.0018744]
	Learning Rate: 0.00187444
	LOSS [training: 0.17894553731419646 | validation: 0.21043613590117688]
	TIME [epoch: 8.29 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1703139620957727		[learning rate: 0.0018722]
		[batch 20/20] avg loss: 0.1624190859734199		[learning rate: 0.00187]
	Learning Rate: 0.00187002
	LOSS [training: 0.16636652403459629 | validation: 0.11176618195214799]
	TIME [epoch: 8.29 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1391465217812038		[learning rate: 0.0018678]
		[batch 20/20] avg loss: 0.1538088842943856		[learning rate: 0.0018656]
	Learning Rate: 0.00186561
	LOSS [training: 0.14647770303779467 | validation: 0.2296098316149138]
	TIME [epoch: 8.32 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13717425992572674		[learning rate: 0.0018634]
		[batch 20/20] avg loss: 0.17271941142495023		[learning rate: 0.0018612]
	Learning Rate: 0.00186121
	LOSS [training: 0.1549468356753385 | validation: 0.2132908070879302]
	TIME [epoch: 8.29 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17312600694422678		[learning rate: 0.001859]
		[batch 20/20] avg loss: 0.15190603839472666		[learning rate: 0.0018568]
	Learning Rate: 0.00185682
	LOSS [training: 0.16251602266947668 | validation: 0.2084960578871619]
	TIME [epoch: 8.29 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19285851374189883		[learning rate: 0.0018546]
		[batch 20/20] avg loss: 0.16745522825720796		[learning rate: 0.0018524]
	Learning Rate: 0.00185244
	LOSS [training: 0.1801568709995534 | validation: 0.1134348601129822]
	TIME [epoch: 8.29 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17431684855632415		[learning rate: 0.0018503]
		[batch 20/20] avg loss: 0.15907214001387446		[learning rate: 0.0018481]
	Learning Rate: 0.00184807
	LOSS [training: 0.16669449428509936 | validation: 0.13111982785961462]
	TIME [epoch: 8.32 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13642343341399824		[learning rate: 0.0018459]
		[batch 20/20] avg loss: 0.15954742281238746		[learning rate: 0.0018437]
	Learning Rate: 0.00184371
	LOSS [training: 0.14798542811319282 | validation: 0.1441709312392766]
	TIME [epoch: 8.29 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18420397660855042		[learning rate: 0.0018415]
		[batch 20/20] avg loss: 0.1860917360167901		[learning rate: 0.0018394]
	Learning Rate: 0.00183936
	LOSS [training: 0.18514785631267028 | validation: 0.12142785525916626]
	TIME [epoch: 8.29 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2440860390580378		[learning rate: 0.0018372]
		[batch 20/20] avg loss: 0.1474852922333761		[learning rate: 0.001835]
	Learning Rate: 0.00183502
	LOSS [training: 0.19578566564570696 | validation: 0.1682353646171108]
	TIME [epoch: 8.28 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17241922257918418		[learning rate: 0.0018329]
		[batch 20/20] avg loss: 0.16719732420492806		[learning rate: 0.0018307]
	Learning Rate: 0.00183069
	LOSS [training: 0.16980827339205612 | validation: 0.13211358932358003]
	TIME [epoch: 8.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15826142623892167		[learning rate: 0.0018285]
		[batch 20/20] avg loss: 0.14932959960101105		[learning rate: 0.0018264]
	Learning Rate: 0.00182637
	LOSS [training: 0.1537955129199664 | validation: 0.19239174182092839]
	TIME [epoch: 8.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20245362615331858		[learning rate: 0.0018242]
		[batch 20/20] avg loss: 0.17490764761237373		[learning rate: 0.0018221]
	Learning Rate: 0.00182207
	LOSS [training: 0.1886806368828462 | validation: 0.16399478762402253]
	TIME [epoch: 8.28 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1441440018481608		[learning rate: 0.0018199]
		[batch 20/20] avg loss: 0.17104831611384602		[learning rate: 0.0018178]
	Learning Rate: 0.00181777
	LOSS [training: 0.1575961589810034 | validation: 0.23857641158170204]
	TIME [epoch: 8.28 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17244405076347752		[learning rate: 0.0018156]
		[batch 20/20] avg loss: 0.1366069287566482		[learning rate: 0.0018135]
	Learning Rate: 0.00181348
	LOSS [training: 0.15452548976006283 | validation: 0.10457347248786338]
	TIME [epoch: 8.29 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17542072789313537		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.15829635431719308		[learning rate: 0.0018092]
	Learning Rate: 0.0018092
	LOSS [training: 0.16685854110516418 | validation: 0.15457026279934838]
	TIME [epoch: 8.31 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17387683437041837		[learning rate: 0.0018071]
		[batch 20/20] avg loss: 0.15672900421171243		[learning rate: 0.0018049]
	Learning Rate: 0.00180493
	LOSS [training: 0.1653029192910654 | validation: 0.09736413973148339]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_775.pth
	Model improved!!!
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14795322092863641		[learning rate: 0.0018028]
		[batch 20/20] avg loss: 0.14698371702568153		[learning rate: 0.0018007]
	Learning Rate: 0.00180068
	LOSS [training: 0.14746846897715896 | validation: 0.14365517346766]
	TIME [epoch: 8.29 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1862037104126635		[learning rate: 0.0017986]
		[batch 20/20] avg loss: 0.2480802736619776		[learning rate: 0.0017964]
	Learning Rate: 0.00179643
	LOSS [training: 0.21714199203732054 | validation: 0.14790160428121785]
	TIME [epoch: 8.28 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1810729933680825		[learning rate: 0.0017943]
		[batch 20/20] avg loss: 0.15314680692789145		[learning rate: 0.0017922]
	Learning Rate: 0.00179219
	LOSS [training: 0.16710990014798696 | validation: 0.1388572160241335]
	TIME [epoch: 8.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1673136857761343		[learning rate: 0.0017901]
		[batch 20/20] avg loss: 0.15467884583350056		[learning rate: 0.001788]
	Learning Rate: 0.00178796
	LOSS [training: 0.16099626580481746 | validation: 0.16333424632922647]
	TIME [epoch: 8.29 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15410961202576337		[learning rate: 0.0017859]
		[batch 20/20] avg loss: 0.15697790615805646		[learning rate: 0.0017837]
	Learning Rate: 0.00178375
	LOSS [training: 0.15554375909190993 | validation: 0.13222506464657555]
	TIME [epoch: 8.29 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1687105679422432		[learning rate: 0.0017816]
		[batch 20/20] avg loss: 0.21679828240880972		[learning rate: 0.0017795]
	Learning Rate: 0.00177954
	LOSS [training: 0.1927544251755265 | validation: 0.13658278042082642]
	TIME [epoch: 8.28 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17983195733860757		[learning rate: 0.0017774]
		[batch 20/20] avg loss: 0.17598643046713902		[learning rate: 0.0017753]
	Learning Rate: 0.00177534
	LOSS [training: 0.17790919390287332 | validation: 0.1132992833258121]
	TIME [epoch: 8.28 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13646768244556182		[learning rate: 0.0017732]
		[batch 20/20] avg loss: 0.15464154140607145		[learning rate: 0.0017712]
	Learning Rate: 0.00177115
	LOSS [training: 0.14555461192581662 | validation: 0.17007983371740826]
	TIME [epoch: 8.31 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18145414078468924		[learning rate: 0.0017691]
		[batch 20/20] avg loss: 0.13990948583412804		[learning rate: 0.001767]
	Learning Rate: 0.00176698
	LOSS [training: 0.16068181330940862 | validation: 0.23207821991676855]
	TIME [epoch: 8.28 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16249261581174773		[learning rate: 0.0017649]
		[batch 20/20] avg loss: 0.14606791674673603		[learning rate: 0.0017628]
	Learning Rate: 0.00176281
	LOSS [training: 0.15428026627924185 | validation: 0.09573181109793755]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31561794737047033		[learning rate: 0.0017607]
		[batch 20/20] avg loss: 0.16839814487202362		[learning rate: 0.0017587]
	Learning Rate: 0.00175865
	LOSS [training: 0.242008046121247 | validation: 0.18458992392003823]
	TIME [epoch: 8.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17498867740964344		[learning rate: 0.0017566]
		[batch 20/20] avg loss: 0.19151001064164763		[learning rate: 0.0017545]
	Learning Rate: 0.0017545
	LOSS [training: 0.1832493440256455 | validation: 0.10408729466263092]
	TIME [epoch: 8.32 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2006810822205388		[learning rate: 0.0017524]
		[batch 20/20] avg loss: 0.15585381174998775		[learning rate: 0.0017504]
	Learning Rate: 0.00175036
	LOSS [training: 0.17826744698526326 | validation: 0.2160585123174758]
	TIME [epoch: 8.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2117651305966824		[learning rate: 0.0017483]
		[batch 20/20] avg loss: 0.1609697448850862		[learning rate: 0.0017462]
	Learning Rate: 0.00174623
	LOSS [training: 0.1863674377408843 | validation: 0.206851043238136]
	TIME [epoch: 8.29 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1961080682378567		[learning rate: 0.0017442]
		[batch 20/20] avg loss: 0.17690472577756225		[learning rate: 0.0017421]
	Learning Rate: 0.00174212
	LOSS [training: 0.18650639700770946 | validation: 0.12202220865359198]
	TIME [epoch: 8.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19074236207445622		[learning rate: 0.0017401]
		[batch 20/20] avg loss: 0.14970463670218964		[learning rate: 0.001738]
	Learning Rate: 0.00173801
	LOSS [training: 0.17022349938832299 | validation: 0.18013974908099575]
	TIME [epoch: 8.31 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16972412220134378		[learning rate: 0.001736]
		[batch 20/20] avg loss: 0.19080455125819087		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.18026433672976733 | validation: 0.2743170108583355]
	TIME [epoch: 8.31 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2205902124414154		[learning rate: 0.0017319]
		[batch 20/20] avg loss: 0.21321516475104102		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.2169026885962282 | validation: 0.16340875010294284]
	TIME [epoch: 8.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18165397148706616		[learning rate: 0.0017278]
		[batch 20/20] avg loss: 0.2035358805779456		[learning rate: 0.0017257]
	Learning Rate: 0.00172574
	LOSS [training: 0.19259492603250594 | validation: 0.13890483649224747]
	TIME [epoch: 8.29 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19533543604862444		[learning rate: 0.0017237]
		[batch 20/20] avg loss: 0.14546903719160867		[learning rate: 0.0017217]
	Learning Rate: 0.00172167
	LOSS [training: 0.17040223662011655 | validation: 0.15228152767369552]
	TIME [epoch: 8.29 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16398727749777953		[learning rate: 0.0017196]
		[batch 20/20] avg loss: 0.13062901207939276		[learning rate: 0.0017176]
	Learning Rate: 0.0017176
	LOSS [training: 0.14730814478858617 | validation: 0.13450738807788956]
	TIME [epoch: 8.32 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16830371195511262		[learning rate: 0.0017156]
		[batch 20/20] avg loss: 0.17290520876679571		[learning rate: 0.0017136]
	Learning Rate: 0.00171355
	LOSS [training: 0.1706044603609542 | validation: 0.20063687663283503]
	TIME [epoch: 8.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19621384451750773		[learning rate: 0.0017115]
		[batch 20/20] avg loss: 0.19007911977465722		[learning rate: 0.0017095]
	Learning Rate: 0.00170951
	LOSS [training: 0.19314648214608252 | validation: 0.1470246203557567]
	TIME [epoch: 8.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15724360955090338		[learning rate: 0.0017075]
		[batch 20/20] avg loss: 0.16755467614823483		[learning rate: 0.0017055]
	Learning Rate: 0.00170548
	LOSS [training: 0.16239914284956908 | validation: 0.1602048319780367]
	TIME [epoch: 8.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18838539661825576		[learning rate: 0.0017035]
		[batch 20/20] avg loss: 0.16735895243237373		[learning rate: 0.0017015]
	Learning Rate: 0.00170146
	LOSS [training: 0.17787217452531479 | validation: 0.11911120220756376]
	TIME [epoch: 8.31 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19089568649051566		[learning rate: 0.0016994]
		[batch 20/20] avg loss: 0.14325298840576464		[learning rate: 0.0016974]
	Learning Rate: 0.00169744
	LOSS [training: 0.16707433744814018 | validation: 0.11900710188276237]
	TIME [epoch: 8.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.164110484707083		[learning rate: 0.0016954]
		[batch 20/20] avg loss: 0.17769635069273812		[learning rate: 0.0016934]
	Learning Rate: 0.00169344
	LOSS [training: 0.17090341769991055 | validation: 0.14428172043691084]
	TIME [epoch: 8.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1537922712674053		[learning rate: 0.0016914]
		[batch 20/20] avg loss: 0.18568571926215646		[learning rate: 0.0016894]
	Learning Rate: 0.00168944
	LOSS [training: 0.1697389952647809 | validation: 0.10434510219712255]
	TIME [epoch: 8.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16702727848402182		[learning rate: 0.0016874]
		[batch 20/20] avg loss: 0.1717779155733603		[learning rate: 0.0016855]
	Learning Rate: 0.00168546
	LOSS [training: 0.1694025970286911 | validation: 0.12874289436140118]
	TIME [epoch: 8.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16064040609664507		[learning rate: 0.0016835]
		[batch 20/20] avg loss: 0.1647561457150028		[learning rate: 0.0016815]
	Learning Rate: 0.00168148
	LOSS [training: 0.16269827590582392 | validation: 0.22454925635392553]
	TIME [epoch: 8.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21372258606533134		[learning rate: 0.0016795]
		[batch 20/20] avg loss: 0.16338672346613634		[learning rate: 0.0016775]
	Learning Rate: 0.00167752
	LOSS [training: 0.18855465476573388 | validation: 0.13382527875531147]
	TIME [epoch: 8.29 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15408662610238413		[learning rate: 0.0016755]
		[batch 20/20] avg loss: 0.16936730489634708		[learning rate: 0.0016736]
	Learning Rate: 0.00167356
	LOSS [training: 0.16172696549936558 | validation: 0.12195109072881052]
	TIME [epoch: 8.29 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12012655699582168		[learning rate: 0.0016716]
		[batch 20/20] avg loss: 0.12191759018482037		[learning rate: 0.0016696]
	Learning Rate: 0.00166961
	LOSS [training: 0.12102207359032105 | validation: 0.09937272445622666]
	TIME [epoch: 8.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13256267448641074		[learning rate: 0.0016676]
		[batch 20/20] avg loss: 0.17979374456037547		[learning rate: 0.0016657]
	Learning Rate: 0.00166567
	LOSS [training: 0.1561782095233931 | validation: 0.1540668676375454]
	TIME [epoch: 8.32 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16927351841950974		[learning rate: 0.0016637]
		[batch 20/20] avg loss: 0.1912802213589299		[learning rate: 0.0016617]
	Learning Rate: 0.00166174
	LOSS [training: 0.18027686988921982 | validation: 0.20518974175801458]
	TIME [epoch: 8.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17335632128110456		[learning rate: 0.0016598]
		[batch 20/20] avg loss: 0.1473215799932575		[learning rate: 0.0016578]
	Learning Rate: 0.00165782
	LOSS [training: 0.16033895063718104 | validation: 0.11370671646367217]
	TIME [epoch: 8.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17142555542999569		[learning rate: 0.0016559]
		[batch 20/20] avg loss: 0.1819074855549645		[learning rate: 0.0016539]
	Learning Rate: 0.00165391
	LOSS [training: 0.17666652049248005 | validation: 0.16060128219833691]
	TIME [epoch: 8.28 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16222888395057913		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.18556178215206462		[learning rate: 0.00165]
	Learning Rate: 0.00165001
	LOSS [training: 0.17389533305132185 | validation: 0.2345500839289128]
	TIME [epoch: 8.32 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2477855843367697		[learning rate: 0.0016481]
		[batch 20/20] avg loss: 0.1592630651112486		[learning rate: 0.0016461]
	Learning Rate: 0.00164612
	LOSS [training: 0.2035243247240092 | validation: 0.13547811668300688]
	TIME [epoch: 8.29 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23219969083453024		[learning rate: 0.0016442]
		[batch 20/20] avg loss: 0.19081912460746803		[learning rate: 0.0016422]
	Learning Rate: 0.00164224
	LOSS [training: 0.2115094077209992 | validation: 0.2087289390874675]
	TIME [epoch: 8.29 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1681036920315957		[learning rate: 0.0016403]
		[batch 20/20] avg loss: 0.1550200368785067		[learning rate: 0.0016384]
	Learning Rate: 0.00163836
	LOSS [training: 0.16156186445505122 | validation: 0.10383573312558794]
	TIME [epoch: 8.28 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16999357562828948		[learning rate: 0.0016364]
		[batch 20/20] avg loss: 0.16339181977804906		[learning rate: 0.0016345]
	Learning Rate: 0.0016345
	LOSS [training: 0.16669269770316927 | validation: 0.16235811225776747]
	TIME [epoch: 8.31 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16382472097256112		[learning rate: 0.0016326]
		[batch 20/20] avg loss: 0.1707992808824123		[learning rate: 0.0016306]
	Learning Rate: 0.00163064
	LOSS [training: 0.16731200092748671 | validation: 0.10394227027016967]
	TIME [epoch: 8.28 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1359108914408907		[learning rate: 0.0016287]
		[batch 20/20] avg loss: 0.17673734728087334		[learning rate: 0.0016268]
	Learning Rate: 0.0016268
	LOSS [training: 0.15632411936088203 | validation: 0.285176426680534]
	TIME [epoch: 8.29 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4776005214450839		[learning rate: 0.0016249]
		[batch 20/20] avg loss: 2.093525993487243		[learning rate: 0.001623]
	Learning Rate: 0.00162296
	LOSS [training: 1.2855632574661633 | validation: 2.1438638636269656]
	TIME [epoch: 8.28 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5967752859093058		[learning rate: 0.001621]
		[batch 20/20] avg loss: 0.8408980238017325		[learning rate: 0.0016191]
	Learning Rate: 0.00161913
	LOSS [training: 1.2188366548555194 | validation: 0.15395165469171246]
	TIME [epoch: 8.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16202895199240347		[learning rate: 0.0016172]
		[batch 20/20] avg loss: 0.18301043066792216		[learning rate: 0.0016153]
	Learning Rate: 0.00161531
	LOSS [training: 0.1725196913301628 | validation: 0.14851981113566082]
	TIME [epoch: 8.32 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15242469358809943		[learning rate: 0.0016134]
		[batch 20/20] avg loss: 0.14202736154055118		[learning rate: 0.0016115]
	Learning Rate: 0.0016115
	LOSS [training: 0.1472260275643253 | validation: 0.12080712961869305]
	TIME [epoch: 8.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14095928955993417		[learning rate: 0.0016096]
		[batch 20/20] avg loss: 0.1309753936237982		[learning rate: 0.0016077]
	Learning Rate: 0.0016077
	LOSS [training: 0.13596734159186616 | validation: 0.15254221110418914]
	TIME [epoch: 8.29 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1796045887620271		[learning rate: 0.0016058]
		[batch 20/20] avg loss: 0.11404102573252146		[learning rate: 0.0016039]
	Learning Rate: 0.00160391
	LOSS [training: 0.14682280724727428 | validation: 0.1929108180215664]
	TIME [epoch: 8.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14428513098434462		[learning rate: 0.001602]
		[batch 20/20] avg loss: 0.13260641337822016		[learning rate: 0.0016001]
	Learning Rate: 0.00160012
	LOSS [training: 0.13844577218128234 | validation: 0.1416602602317507]
	TIME [epoch: 8.32 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14399842223732784		[learning rate: 0.0015982]
		[batch 20/20] avg loss: 0.14534297761481701		[learning rate: 0.0015964]
	Learning Rate: 0.00159635
	LOSS [training: 0.14467069992607245 | validation: 0.10897010965550188]
	TIME [epoch: 8.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1629272547153676		[learning rate: 0.0015945]
		[batch 20/20] avg loss: 0.158957632673989		[learning rate: 0.0015926]
	Learning Rate: 0.00159258
	LOSS [training: 0.16094244369467833 | validation: 0.22303647181213512]
	TIME [epoch: 8.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14053619805572554		[learning rate: 0.0015907]
		[batch 20/20] avg loss: 0.20053034550607157		[learning rate: 0.0015888]
	Learning Rate: 0.00158883
	LOSS [training: 0.17053327178089855 | validation: 0.18884312737126702]
	TIME [epoch: 8.29 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14041073687549688		[learning rate: 0.001587]
		[batch 20/20] avg loss: 0.14077177080548148		[learning rate: 0.0015851]
	Learning Rate: 0.00158508
	LOSS [training: 0.1405912538404892 | validation: 0.1644171186098685]
	TIME [epoch: 8.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15936283292088155		[learning rate: 0.0015832]
		[batch 20/20] avg loss: 0.1518782239069878		[learning rate: 0.0015813]
	Learning Rate: 0.00158134
	LOSS [training: 0.15562052841393467 | validation: 0.1495887458101245]
	TIME [epoch: 8.32 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16046260172506102		[learning rate: 0.0015795]
		[batch 20/20] avg loss: 0.1807885444288298		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.17062557307694542 | validation: 0.1386779709950436]
	TIME [epoch: 8.29 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15545590805448312		[learning rate: 0.0015757]
		[batch 20/20] avg loss: 0.12980066233669224		[learning rate: 0.0015739]
	Learning Rate: 0.00157389
	LOSS [training: 0.1426282851955877 | validation: 0.15515228422727262]
	TIME [epoch: 8.28 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15200392704154136		[learning rate: 0.001572]
		[batch 20/20] avg loss: 0.15551434823262716		[learning rate: 0.0015702]
	Learning Rate: 0.00157018
	LOSS [training: 0.15375913763708426 | validation: 0.14774193358682455]
	TIME [epoch: 8.29 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17783540405666085		[learning rate: 0.0015683]
		[batch 20/20] avg loss: 0.1243240277982675		[learning rate: 0.0015665]
	Learning Rate: 0.00156647
	LOSS [training: 0.1510797159274642 | validation: 0.11042846025912853]
	TIME [epoch: 8.31 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15586761728345297		[learning rate: 0.0015646]
		[batch 20/20] avg loss: 0.14145196851042036		[learning rate: 0.0015628]
	Learning Rate: 0.00156278
	LOSS [training: 0.1486597928969367 | validation: 0.16486725902188168]
	TIME [epoch: 8.29 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14946709829200894		[learning rate: 0.0015609]
		[batch 20/20] avg loss: 0.15130289703498492		[learning rate: 0.0015591]
	Learning Rate: 0.00155909
	LOSS [training: 0.15038499766349692 | validation: 0.21459022325128752]
	TIME [epoch: 8.28 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14378870503409152		[learning rate: 0.0015573]
		[batch 20/20] avg loss: 0.14044879958159023		[learning rate: 0.0015554]
	Learning Rate: 0.00155541
	LOSS [training: 0.1421187523078409 | validation: 0.1914302572914271]
	TIME [epoch: 8.29 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1493582311553059		[learning rate: 0.0015536]
		[batch 20/20] avg loss: 0.15044243500223176		[learning rate: 0.0015517]
	Learning Rate: 0.00155175
	LOSS [training: 0.14990033307876882 | validation: 0.1289476869640399]
	TIME [epoch: 8.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23511281761120423		[learning rate: 0.0015499]
		[batch 20/20] avg loss: 0.1776324630373803		[learning rate: 0.0015481]
	Learning Rate: 0.00154809
	LOSS [training: 0.2063726403242923 | validation: 0.11020463316015985]
	TIME [epoch: 8.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13013599496850164		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.18150441446150065		[learning rate: 0.0015444]
	Learning Rate: 0.00154443
	LOSS [training: 0.15582020471500116 | validation: 0.3003497508590649]
	TIME [epoch: 8.29 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17362288566585132		[learning rate: 0.0015426]
		[batch 20/20] avg loss: 0.17687392805401772		[learning rate: 0.0015408]
	Learning Rate: 0.00154079
	LOSS [training: 0.17524840685993448 | validation: 0.23356030207605877]
	TIME [epoch: 8.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17264338202617074		[learning rate: 0.001539]
		[batch 20/20] avg loss: 0.15731732134239546		[learning rate: 0.0015372]
	Learning Rate: 0.00153716
	LOSS [training: 0.1649803516842831 | validation: 0.13441801136379472]
	TIME [epoch: 8.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19458058838703918		[learning rate: 0.0015353]
		[batch 20/20] avg loss: 0.15736221108661233		[learning rate: 0.0015335]
	Learning Rate: 0.00153353
	LOSS [training: 0.17597139973682574 | validation: 0.20689565222165682]
	TIME [epoch: 8.32 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1461079753810291		[learning rate: 0.0015317]
		[batch 20/20] avg loss: 0.13090730388508404		[learning rate: 0.0015299]
	Learning Rate: 0.00152991
	LOSS [training: 0.13850763963305654 | validation: 0.0989997602558444]
	TIME [epoch: 8.28 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14892630438666737		[learning rate: 0.0015281]
		[batch 20/20] avg loss: 0.1435066201255531		[learning rate: 0.0015263]
	Learning Rate: 0.0015263
	LOSS [training: 0.14621646225611024 | validation: 0.14598853968614708]
	TIME [epoch: 8.28 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16233827826290803		[learning rate: 0.0015245]
		[batch 20/20] avg loss: 0.14327917418921696		[learning rate: 0.0015227]
	Learning Rate: 0.0015227
	LOSS [training: 0.15280872622606248 | validation: 0.10383229543789202]
	TIME [epoch: 8.28 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1573520832765871		[learning rate: 0.0015209]
		[batch 20/20] avg loss: 0.13468970675204756		[learning rate: 0.0015191]
	Learning Rate: 0.00151911
	LOSS [training: 0.1460208950143173 | validation: 0.13839110555694545]
	TIME [epoch: 8.31 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15371546162454694		[learning rate: 0.0015173]
		[batch 20/20] avg loss: 0.1658251247676105		[learning rate: 0.0015155]
	Learning Rate: 0.00151553
	LOSS [training: 0.1597702931960787 | validation: 0.12222856589520432]
	TIME [epoch: 8.29 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14869800821170578		[learning rate: 0.0015137]
		[batch 20/20] avg loss: 0.14663298794965096		[learning rate: 0.001512]
	Learning Rate: 0.00151195
	LOSS [training: 0.14766549808067836 | validation: 0.12196342239322495]
	TIME [epoch: 8.29 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15645634935801483		[learning rate: 0.0015102]
		[batch 20/20] avg loss: 0.1749212849555019		[learning rate: 0.0015084]
	Learning Rate: 0.00150839
	LOSS [training: 0.16568881715675843 | validation: 0.21330073143054346]
	TIME [epoch: 8.29 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1778674406420159		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.15462190774225415		[learning rate: 0.0015048]
	Learning Rate: 0.00150483
	LOSS [training: 0.166244674192135 | validation: 0.13604725738502854]
	TIME [epoch: 8.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2576086963634046		[learning rate: 0.0015031]
		[batch 20/20] avg loss: 0.1432464899962939		[learning rate: 0.0015013]
	Learning Rate: 0.00150128
	LOSS [training: 0.2004275931798493 | validation: 0.11685899261509276]
	TIME [epoch: 8.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17335743617706126		[learning rate: 0.0014995]
		[batch 20/20] avg loss: 0.17640066023724063		[learning rate: 0.0014977]
	Learning Rate: 0.00149774
	LOSS [training: 0.174879048207151 | validation: 0.13383876211918294]
	TIME [epoch: 8.29 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13952055905840433		[learning rate: 0.001496]
		[batch 20/20] avg loss: 0.13581160985987734		[learning rate: 0.0014942]
	Learning Rate: 0.0014942
	LOSS [training: 0.1376660844591408 | validation: 0.11944917373618263]
	TIME [epoch: 8.29 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1600110399999008		[learning rate: 0.0014924]
		[batch 20/20] avg loss: 0.18238880379200023		[learning rate: 0.0014907]
	Learning Rate: 0.00149068
	LOSS [training: 0.1711999218959505 | validation: 0.1186488839946809]
	TIME [epoch: 8.29 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14354049931580498		[learning rate: 0.0014889]
		[batch 20/20] avg loss: 0.19128074399291475		[learning rate: 0.0014872]
	Learning Rate: 0.00148716
	LOSS [training: 0.16741062165435988 | validation: 0.19314751491758386]
	TIME [epoch: 8.31 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15214880386038793		[learning rate: 0.0014854]
		[batch 20/20] avg loss: 0.16907974904873996		[learning rate: 0.0014837]
	Learning Rate: 0.00148366
	LOSS [training: 0.16061427645456394 | validation: 0.12845140602288635]
	TIME [epoch: 8.29 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1714973162281302		[learning rate: 0.0014819]
		[batch 20/20] avg loss: 0.14213120955518574		[learning rate: 0.0014802]
	Learning Rate: 0.00148016
	LOSS [training: 0.15681426289165795 | validation: 0.1322522407062358]
	TIME [epoch: 8.27 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18300658359267583		[learning rate: 0.0014784]
		[batch 20/20] avg loss: 0.16483387853559653		[learning rate: 0.0014767]
	Learning Rate: 0.00147667
	LOSS [training: 0.17392023106413615 | validation: 0.16391001389935098]
	TIME [epoch: 8.28 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1549840106378142		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.156878897450432		[learning rate: 0.0014732]
	Learning Rate: 0.00147318
	LOSS [training: 0.1559314540441231 | validation: 0.22600225520355377]
	TIME [epoch: 8.31 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.167724263394226		[learning rate: 0.0014714]
		[batch 20/20] avg loss: 0.16691513728331428		[learning rate: 0.0014697]
	Learning Rate: 0.00146971
	LOSS [training: 0.16731970033877014 | validation: 0.1836066393822619]
	TIME [epoch: 8.28 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1819702747757118		[learning rate: 0.001468]
		[batch 20/20] avg loss: 0.14368796643118034		[learning rate: 0.0014662]
	Learning Rate: 0.00146624
	LOSS [training: 0.16282912060344606 | validation: 0.12129907280534612]
	TIME [epoch: 8.28 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15059635629605123		[learning rate: 0.0014645]
		[batch 20/20] avg loss: 0.13734337923029832		[learning rate: 0.0014628]
	Learning Rate: 0.00146278
	LOSS [training: 0.1439698677631748 | validation: 0.1302042900002064]
	TIME [epoch: 8.28 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13542709465581598		[learning rate: 0.0014611]
		[batch 20/20] avg loss: 0.18364983967565576		[learning rate: 0.0014593]
	Learning Rate: 0.00145933
	LOSS [training: 0.1595384671657359 | validation: 0.15102126779131358]
	TIME [epoch: 8.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17282853744585636		[learning rate: 0.0014576]
		[batch 20/20] avg loss: 0.1229442396592892		[learning rate: 0.0014559]
	Learning Rate: 0.00145589
	LOSS [training: 0.14788638855257277 | validation: 0.16418574161252314]
	TIME [epoch: 8.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13530906028832707		[learning rate: 0.0014542]
		[batch 20/20] avg loss: 0.14058573835987975		[learning rate: 0.0014525]
	Learning Rate: 0.00145245
	LOSS [training: 0.13794739932410344 | validation: 0.1441364772701889]
	TIME [epoch: 8.28 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13673416054704654		[learning rate: 0.0014507]
		[batch 20/20] avg loss: 0.1559039932100053		[learning rate: 0.001449]
	Learning Rate: 0.00144903
	LOSS [training: 0.14631907687852597 | validation: 0.1971306648437441]
	TIME [epoch: 8.29 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18917589468681423		[learning rate: 0.0014473]
		[batch 20/20] avg loss: 0.13866009787036		[learning rate: 0.0014456]
	Learning Rate: 0.00144561
	LOSS [training: 0.1639179962785871 | validation: 0.24708592721882594]
	TIME [epoch: 8.28 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1920651582049971		[learning rate: 0.0014439]
		[batch 20/20] avg loss: 0.16410876812135608		[learning rate: 0.0014422]
	Learning Rate: 0.0014422
	LOSS [training: 0.1780869631631766 | validation: 0.1923722025732885]
	TIME [epoch: 8.32 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14228050944738957		[learning rate: 0.0014405]
		[batch 20/20] avg loss: 0.13262353679246067		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.13745202311992513 | validation: 0.17471717911355913]
	TIME [epoch: 8.29 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14218858846052218		[learning rate: 0.0014371]
		[batch 20/20] avg loss: 0.1646826681247736		[learning rate: 0.0014354]
	Learning Rate: 0.0014354
	LOSS [training: 0.15343562829264787 | validation: 0.10744574025127757]
	TIME [epoch: 8.29 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13640147863520063		[learning rate: 0.0014337]
		[batch 20/20] avg loss: 0.13445033518675567		[learning rate: 0.001432]
	Learning Rate: 0.00143202
	LOSS [training: 0.13542590691097817 | validation: 0.140799764966552]
	TIME [epoch: 8.3 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13427937420205888		[learning rate: 0.0014303]
		[batch 20/20] avg loss: 0.16213545341167862		[learning rate: 0.0014286]
	Learning Rate: 0.00142864
	LOSS [training: 0.14820741380686878 | validation: 0.13002280841271938]
	TIME [epoch: 8.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15953865820117547		[learning rate: 0.001427]
		[batch 20/20] avg loss: 0.1266276301872607		[learning rate: 0.0014253]
	Learning Rate: 0.00142527
	LOSS [training: 0.14308314419421808 | validation: 0.16801439480991498]
	TIME [epoch: 8.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17353959211213937		[learning rate: 0.0014236]
		[batch 20/20] avg loss: 0.16844873736498966		[learning rate: 0.0014219]
	Learning Rate: 0.00142191
	LOSS [training: 0.1709941647385645 | validation: 0.13412218772745813]
	TIME [epoch: 8.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12650465224245877		[learning rate: 0.0014202]
		[batch 20/20] avg loss: 0.11967773013921468		[learning rate: 0.0014186]
	Learning Rate: 0.00141855
	LOSS [training: 0.12309119119083675 | validation: 0.15385797739578072]
	TIME [epoch: 8.29 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13704816616911028		[learning rate: 0.0014169]
		[batch 20/20] avg loss: 0.16366770441410003		[learning rate: 0.0014152]
	Learning Rate: 0.00141521
	LOSS [training: 0.15035793529160515 | validation: 0.18406747569344342]
	TIME [epoch: 8.29 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14778897985495215		[learning rate: 0.0014135]
		[batch 20/20] avg loss: 0.21368096796240818		[learning rate: 0.0014119]
	Learning Rate: 0.00141187
	LOSS [training: 0.1807349739086802 | validation: 0.12397357752076582]
	TIME [epoch: 8.32 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397771359361859		[learning rate: 0.0014102]
		[batch 20/20] avg loss: 0.1424162379576222		[learning rate: 0.0014085]
	Learning Rate: 0.00140854
	LOSS [training: 0.14109668694690405 | validation: 0.18993851013010735]
	TIME [epoch: 8.29 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16396249121974135		[learning rate: 0.0014069]
		[batch 20/20] avg loss: 0.15411567453794212		[learning rate: 0.0014052]
	Learning Rate: 0.00140522
	LOSS [training: 0.15903908287884175 | validation: 0.09617090345338326]
	TIME [epoch: 8.29 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1414258080913663		[learning rate: 0.0014036]
		[batch 20/20] avg loss: 0.15306694445422858		[learning rate: 0.0014019]
	Learning Rate: 0.0014019
	LOSS [training: 0.14724637627279744 | validation: 0.1858890109882638]
	TIME [epoch: 8.29 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1395814821571919		[learning rate: 0.0014002]
		[batch 20/20] avg loss: 0.18182994164802652		[learning rate: 0.0013986]
	Learning Rate: 0.0013986
	LOSS [training: 0.1607057119026092 | validation: 0.14395394095246467]
	TIME [epoch: 8.31 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12154064460702454		[learning rate: 0.0013969]
		[batch 20/20] avg loss: 0.13630931929353823		[learning rate: 0.0013953]
	Learning Rate: 0.0013953
	LOSS [training: 0.1289249819502814 | validation: 0.11878771706512878]
	TIME [epoch: 8.29 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12935116990885914		[learning rate: 0.0013937]
		[batch 20/20] avg loss: 0.1565811799145823		[learning rate: 0.001392]
	Learning Rate: 0.00139201
	LOSS [training: 0.14296617491172076 | validation: 0.13926310917244308]
	TIME [epoch: 8.29 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14102001745092466		[learning rate: 0.0013904]
		[batch 20/20] avg loss: 0.11084104676803704		[learning rate: 0.0013887]
	Learning Rate: 0.00138872
	LOSS [training: 0.12593053210948085 | validation: 0.10480628116072492]
	TIME [epoch: 8.29 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13139773409028294		[learning rate: 0.0013871]
		[batch 20/20] avg loss: 0.1527950857515753		[learning rate: 0.0013854]
	Learning Rate: 0.00138545
	LOSS [training: 0.14209640992092915 | validation: 0.12739037592323907]
	TIME [epoch: 8.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13135688998221356		[learning rate: 0.0013838]
		[batch 20/20] avg loss: 0.12650967780186745		[learning rate: 0.0013822]
	Learning Rate: 0.00138218
	LOSS [training: 0.1289332838920405 | validation: 0.1680268040666343]
	TIME [epoch: 8.29 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1436403313149905		[learning rate: 0.0013805]
		[batch 20/20] avg loss: 0.14281204940504305		[learning rate: 0.0013789]
	Learning Rate: 0.00137892
	LOSS [training: 0.14322619036001677 | validation: 0.11107699943126068]
	TIME [epoch: 8.28 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12085236312053982		[learning rate: 0.0013773]
		[batch 20/20] avg loss: 0.1552410525767877		[learning rate: 0.0013757]
	Learning Rate: 0.00137567
	LOSS [training: 0.13804670784866374 | validation: 0.11460974346046653]
	TIME [epoch: 8.28 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13430738240943899		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.149922591375819		[learning rate: 0.0013724]
	Learning Rate: 0.00137242
	LOSS [training: 0.142114986892629 | validation: 0.2168317939311018]
	TIME [epoch: 8.29 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1483631940468011		[learning rate: 0.0013708]
		[batch 20/20] avg loss: 0.11190696664033332		[learning rate: 0.0013692]
	Learning Rate: 0.00136918
	LOSS [training: 0.1301350803435672 | validation: 0.18605607819698547]
	TIME [epoch: 8.31 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16389823233884465		[learning rate: 0.0013676]
		[batch 20/20] avg loss: 0.1510950270644121		[learning rate: 0.001366]
	Learning Rate: 0.00136595
	LOSS [training: 0.15749662970162837 | validation: 0.13077592738015048]
	TIME [epoch: 8.29 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14925249845723854		[learning rate: 0.0013643]
		[batch 20/20] avg loss: 0.14812423061481078		[learning rate: 0.0013627]
	Learning Rate: 0.00136273
	LOSS [training: 0.14868836453602466 | validation: 0.19924431582857238]
	TIME [epoch: 8.28 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14282076125210577		[learning rate: 0.0013611]
		[batch 20/20] avg loss: 0.14627487345780146		[learning rate: 0.0013595]
	Learning Rate: 0.00135952
	LOSS [training: 0.1445478173549536 | validation: 0.1537905890909567]
	TIME [epoch: 8.28 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16144932927926886		[learning rate: 0.0013579]
		[batch 20/20] avg loss: 0.14625884019402438		[learning rate: 0.0013563]
	Learning Rate: 0.00135631
	LOSS [training: 0.15385408473664663 | validation: 0.12276822985771738]
	TIME [epoch: 8.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11297978114004474		[learning rate: 0.0013547]
		[batch 20/20] avg loss: 0.14356394602143624		[learning rate: 0.0013531]
	Learning Rate: 0.00135311
	LOSS [training: 0.1282718635807405 | validation: 0.176286111792127]
	TIME [epoch: 8.29 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12585332226546064		[learning rate: 0.0013515]
		[batch 20/20] avg loss: 0.15605300755059204		[learning rate: 0.0013499]
	Learning Rate: 0.00134992
	LOSS [training: 0.14095316490802634 | validation: 0.14840696426418445]
	TIME [epoch: 8.29 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16003320603902704		[learning rate: 0.0013483]
		[batch 20/20] avg loss: 0.14917274831017577		[learning rate: 0.0013467]
	Learning Rate: 0.00134673
	LOSS [training: 0.15460297717460142 | validation: 0.16969058389963584]
	TIME [epoch: 8.29 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14314850895839912		[learning rate: 0.0013451]
		[batch 20/20] avg loss: 0.16906292190090685		[learning rate: 0.0013436]
	Learning Rate: 0.00134356
	LOSS [training: 0.15610571542965299 | validation: 0.2377321067494501]
	TIME [epoch: 8.31 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14278011621981185		[learning rate: 0.001342]
		[batch 20/20] avg loss: 0.14520120286365573		[learning rate: 0.0013404]
	Learning Rate: 0.00134039
	LOSS [training: 0.1439906595417338 | validation: 0.11487473292100134]
	TIME [epoch: 8.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1276456511165252		[learning rate: 0.0013388]
		[batch 20/20] avg loss: 0.1198238747513066		[learning rate: 0.0013372]
	Learning Rate: 0.00133723
	LOSS [training: 0.12373476293391591 | validation: 0.09920557673381473]
	TIME [epoch: 8.28 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.137606287080834		[learning rate: 0.0013356]
		[batch 20/20] avg loss: 0.10568146304125428		[learning rate: 0.0013341]
	Learning Rate: 0.00133407
	LOSS [training: 0.12164387506104417 | validation: 0.10309214349006252]
	TIME [epoch: 8.28 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12825760794699873		[learning rate: 0.0013325]
		[batch 20/20] avg loss: 0.14791859124006473		[learning rate: 0.0013309]
	Learning Rate: 0.00133093
	LOSS [training: 0.13808809959353172 | validation: 0.17762386956668375]
	TIME [epoch: 8.29 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12791451198282672		[learning rate: 0.0013294]
		[batch 20/20] avg loss: 0.14332800658888223		[learning rate: 0.0013278]
	Learning Rate: 0.00132779
	LOSS [training: 0.1356212592858545 | validation: 0.10269122684298412]
	TIME [epoch: 8.32 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13908640810495132		[learning rate: 0.0013262]
		[batch 20/20] avg loss: 0.11486982751271406		[learning rate: 0.0013247]
	Learning Rate: 0.00132465
	LOSS [training: 0.1269781178088327 | validation: 0.1057336063973539]
	TIME [epoch: 8.29 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12796429977168472		[learning rate: 0.0013231]
		[batch 20/20] avg loss: 0.13709796947782477		[learning rate: 0.0013215]
	Learning Rate: 0.00132153
	LOSS [training: 0.13253113462475472 | validation: 0.12422330828782739]
	TIME [epoch: 8.29 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11042974402587862		[learning rate: 0.00132]
		[batch 20/20] avg loss: 0.13452020364713155		[learning rate: 0.0013184]
	Learning Rate: 0.00131841
	LOSS [training: 0.12247497383650507 | validation: 0.16922034289850596]
	TIME [epoch: 8.28 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1521458037210956		[learning rate: 0.0013169]
		[batch 20/20] avg loss: 0.12347606512140943		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.13781093442125253 | validation: 0.1491919257896738]
	TIME [epoch: 8.31 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14440052282531468		[learning rate: 0.0013138]
		[batch 20/20] avg loss: 0.12847799823389955		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.13643926052960711 | validation: 0.09474384940339357]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_910.pth
	Model improved!!!
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12297982156616187		[learning rate: 0.0013107]
		[batch 20/20] avg loss: 0.12865807948070876		[learning rate: 0.0013091]
	Learning Rate: 0.0013091
	LOSS [training: 0.1258189505234353 | validation: 0.0925113931474047]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_911.pth
	Model improved!!!
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15720624574896114		[learning rate: 0.0013076]
		[batch 20/20] avg loss: 0.11259826884985749		[learning rate: 0.001306]
	Learning Rate: 0.00130602
	LOSS [training: 0.13490225729940927 | validation: 0.10057837268232311]
	TIME [epoch: 8.29 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12115820545508378		[learning rate: 0.0013045]
		[batch 20/20] avg loss: 0.14004952681677987		[learning rate: 0.0013029]
	Learning Rate: 0.00130294
	LOSS [training: 0.13060386613593186 | validation: 0.09203099141500917]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_913.pth
	Model improved!!!
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14319750079404514		[learning rate: 0.0013014]
		[batch 20/20] avg loss: 0.14162463054974145		[learning rate: 0.0012999]
	Learning Rate: 0.00129986
	LOSS [training: 0.14241106567189327 | validation: 0.10056437782920641]
	TIME [epoch: 8.31 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14937649220692867		[learning rate: 0.0012983]
		[batch 20/20] avg loss: 0.15056914214292152		[learning rate: 0.0012968]
	Learning Rate: 0.0012968
	LOSS [training: 0.14997281717492505 | validation: 0.116440208043182]
	TIME [epoch: 8.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.132042834115439		[learning rate: 0.0012953]
		[batch 20/20] avg loss: 0.13645223612420257		[learning rate: 0.0012937]
	Learning Rate: 0.00129374
	LOSS [training: 0.13424753511982077 | validation: 0.19849127629910515]
	TIME [epoch: 8.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15768012786989002		[learning rate: 0.0012922]
		[batch 20/20] avg loss: 0.1447696393224723		[learning rate: 0.0012907]
	Learning Rate: 0.00129069
	LOSS [training: 0.15122488359618116 | validation: 0.14314679441851358]
	TIME [epoch: 8.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16784510344564002		[learning rate: 0.0012892]
		[batch 20/20] avg loss: 0.16780573375593857		[learning rate: 0.0012876]
	Learning Rate: 0.00128764
	LOSS [training: 0.1678254186007893 | validation: 0.21050178627294594]
	TIME [epoch: 8.33 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14399836871188187		[learning rate: 0.0012861]
		[batch 20/20] avg loss: 0.15975070874257907		[learning rate: 0.0012846]
	Learning Rate: 0.0012846
	LOSS [training: 0.15187453872723047 | validation: 0.1255406857985819]
	TIME [epoch: 8.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14487210961384053		[learning rate: 0.0012831]
		[batch 20/20] avg loss: 0.17046662608534296		[learning rate: 0.0012816]
	Learning Rate: 0.00128157
	LOSS [training: 0.15766936784959176 | validation: 0.13653590731118934]
	TIME [epoch: 8.3 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1350927285547517		[learning rate: 0.0012801]
		[batch 20/20] avg loss: 0.14954997029783826		[learning rate: 0.0012786]
	Learning Rate: 0.00127855
	LOSS [training: 0.14232134942629499 | validation: 0.13972900925927778]
	TIME [epoch: 8.3 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13344854227022232		[learning rate: 0.001277]
		[batch 20/20] avg loss: 0.11015380449712635		[learning rate: 0.0012755]
	Learning Rate: 0.00127553
	LOSS [training: 0.12180117338367429 | validation: 0.11269931231036115]
	TIME [epoch: 8.32 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12924185055913315		[learning rate: 0.001274]
		[batch 20/20] avg loss: 0.14340384593974037		[learning rate: 0.0012725]
	Learning Rate: 0.00127253
	LOSS [training: 0.13632284824943672 | validation: 0.23750158935851592]
	TIME [epoch: 8.31 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15563635711342494		[learning rate: 0.001271]
		[batch 20/20] avg loss: 0.13878299034148286		[learning rate: 0.0012695]
	Learning Rate: 0.00126952
	LOSS [training: 0.1472096737274539 | validation: 0.1573318876949258]
	TIME [epoch: 8.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1411597548800776		[learning rate: 0.001268]
		[batch 20/20] avg loss: 0.1323632439239264		[learning rate: 0.0012665]
	Learning Rate: 0.00126653
	LOSS [training: 0.13676149940200202 | validation: 0.12903120422817208]
	TIME [epoch: 8.31 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14221132560255081		[learning rate: 0.001265]
		[batch 20/20] avg loss: 0.12627838209542736		[learning rate: 0.0012635]
	Learning Rate: 0.00126354
	LOSS [training: 0.1342448538489891 | validation: 0.11739890631828215]
	TIME [epoch: 8.32 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12341024195904775		[learning rate: 0.0012621]
		[batch 20/20] avg loss: 0.13580939026130337		[learning rate: 0.0012606]
	Learning Rate: 0.00126056
	LOSS [training: 0.12960981611017558 | validation: 0.10554283528419073]
	TIME [epoch: 8.32 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1530460474792204		[learning rate: 0.0012591]
		[batch 20/20] avg loss: 0.1500589023613298		[learning rate: 0.0012576]
	Learning Rate: 0.00125759
	LOSS [training: 0.1515524749202751 | validation: 0.13853747627216884]
	TIME [epoch: 8.31 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11873224411168978		[learning rate: 0.0012561]
		[batch 20/20] avg loss: 0.11664420788373334		[learning rate: 0.0012546]
	Learning Rate: 0.00125462
	LOSS [training: 0.11768822599771153 | validation: 0.17643173018344316]
	TIME [epoch: 8.3 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18398528674170936		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.1495620734887336		[learning rate: 0.0012517]
	Learning Rate: 0.00125166
	LOSS [training: 0.16677368011522145 | validation: 0.13152946192051973]
	TIME [epoch: 8.31 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12798958911208463		[learning rate: 0.0012502]
		[batch 20/20] avg loss: 0.14367349997921647		[learning rate: 0.0012487]
	Learning Rate: 0.00124871
	LOSS [training: 0.13583154454565055 | validation: 0.14820580642215508]
	TIME [epoch: 8.33 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13573718862801926		[learning rate: 0.0012472]
		[batch 20/20] avg loss: 0.9116426704825032		[learning rate: 0.0012458]
	Learning Rate: 0.00124576
	LOSS [training: 0.5236899295552613 | validation: 2.5120466780049604]
	TIME [epoch: 8.31 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.073173564579289		[learning rate: 0.0012443]
		[batch 20/20] avg loss: 3.1728810836350694		[learning rate: 0.0012428]
	Learning Rate: 0.00124283
	LOSS [training: 3.123027324107179 | validation: 3.18852509124784]
	TIME [epoch: 8.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5806214960048357		[learning rate: 0.0012414]
		[batch 20/20] avg loss: 3.9020932915439714		[learning rate: 0.0012399]
	Learning Rate: 0.00123989
	LOSS [training: 3.741357393774404 | validation: 3.623744974657878]
	TIME [epoch: 8.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.925799244921012		[learning rate: 0.0012384]
		[batch 20/20] avg loss: 3.96064758757018		[learning rate: 0.001237]
	Learning Rate: 0.00123697
	LOSS [training: 3.9432234162455964 | validation: 3.390446514697574]
	TIME [epoch: 8.33 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.824756762745418		[learning rate: 0.0012355]
		[batch 20/20] avg loss: 3.9797581322185005		[learning rate: 0.0012341]
	Learning Rate: 0.00123405
	LOSS [training: 3.902257447481959 | validation: 3.320019113879979]
	TIME [epoch: 8.31 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.005970081928693		[learning rate: 0.0012326]
		[batch 20/20] avg loss: 4.061036499693917		[learning rate: 0.0012311]
	Learning Rate: 0.00123114
	LOSS [training: 4.033503290811305 | validation: 3.7498615583883854]
	TIME [epoch: 8.31 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.292792788343989		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 4.549884555199016		[learning rate: 0.0012282]
	Learning Rate: 0.00122824
	LOSS [training: 4.421338671771503 | validation: 4.297081605100684]
	TIME [epoch: 8.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.144752494630581		[learning rate: 0.0012268]
		[batch 20/20] avg loss: 3.7278150128516927		[learning rate: 0.0012253]
	Learning Rate: 0.00122534
	LOSS [training: 3.936283753741138 | validation: 3.4816666238012313]
	TIME [epoch: 8.32 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7666873048040563		[learning rate: 0.0012239]
		[batch 20/20] avg loss: 3.9333601297579848		[learning rate: 0.0012224]
	Learning Rate: 0.00122245
	LOSS [training: 3.8500237172810197 | validation: 3.2417081060224153]
	TIME [epoch: 8.32 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.570285467644215		[learning rate: 0.001221]
		[batch 20/20] avg loss: 3.805437427184713		[learning rate: 0.0012196]
	Learning Rate: 0.00121957
	LOSS [training: 3.6878614474144635 | validation: 3.1043384670997325]
	TIME [epoch: 8.31 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1180648046688		[learning rate: 0.0012181]
		[batch 20/20] avg loss: 3.5650244625244163		[learning rate: 0.0012167]
	Learning Rate: 0.00121669
	LOSS [training: 3.341544633596608 | validation: 3.5084403974110154]
	TIME [epoch: 8.3 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.771816438883872		[learning rate: 0.0012153]
		[batch 20/20] avg loss: 3.669659086844301		[learning rate: 0.0012138]
	Learning Rate: 0.00121382
	LOSS [training: 3.7207377628640863 | validation: 3.3181462866890676]
	TIME [epoch: 8.31 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.261463279083393		[learning rate: 0.0012124]
		[batch 20/20] avg loss: 3.5771597498884673		[learning rate: 0.001211]
	Learning Rate: 0.00121096
	LOSS [training: 3.4193115144859307 | validation: 3.388188617731337]
	TIME [epoch: 8.33 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.171187856061386		[learning rate: 0.0012095]
		[batch 20/20] avg loss: 4.731326085568908		[learning rate: 0.0012081]
	Learning Rate: 0.0012081
	LOSS [training: 4.451256970815146 | validation: 4.408638876695914]
	TIME [epoch: 8.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.962864291873293		[learning rate: 0.0012067]
		[batch 20/20] avg loss: 5.064733775575964		[learning rate: 0.0012052]
	Learning Rate: 0.00120525
	LOSS [training: 5.013799033724629 | validation: 4.479679321082258]
	TIME [epoch: 8.31 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.35357103043462		[learning rate: 0.0012038]
		[batch 20/20] avg loss: 4.26979738684916		[learning rate: 0.0012024]
	Learning Rate: 0.00120241
	LOSS [training: 4.311684208641891 | validation: 3.3643185814768084]
	TIME [epoch: 8.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.038527122606725		[learning rate: 0.001201]
		[batch 20/20] avg loss: 4.006688671097808		[learning rate: 0.0011996]
	Learning Rate: 0.00119957
	LOSS [training: 4.022607896852266 | validation: 3.36586274874036]
	TIME [epoch: 8.32 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.9279020778254448		[learning rate: 0.0011982]
		[batch 20/20] avg loss: 3.452279971915896		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 3.69009102487067 | validation: 3.050416709233003]
	TIME [epoch: 8.31 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2174771422752295		[learning rate: 0.0011953]
		[batch 20/20] avg loss: 2.844181411643625		[learning rate: 0.0011939]
	Learning Rate: 0.00119392
	LOSS [training: 3.030829276959428 | validation: 2.7407758032637544]
	TIME [epoch: 8.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3541585847284736		[learning rate: 0.0011925]
		[batch 20/20] avg loss: 3.7136896148298133		[learning rate: 0.0011911]
	Learning Rate: 0.0011911
	LOSS [training: 3.5339240997791435 | validation: 3.420108979782239]
	TIME [epoch: 8.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3505138960582728		[learning rate: 0.0011897]
		[batch 20/20] avg loss: 3.442686272314778		[learning rate: 0.0011883]
	Learning Rate: 0.00118829
	LOSS [training: 3.396600084186525 | validation: 2.7177643836167342]
	TIME [epoch: 8.31 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6935847013131062		[learning rate: 0.0011869]
		[batch 20/20] avg loss: 2.648497431591828		[learning rate: 0.0011855]
	Learning Rate: 0.00118549
	LOSS [training: 2.6710410664524664 | validation: 2.101994924224743]
	TIME [epoch: 8.32 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.793187633020822		[learning rate: 0.0011841]
		[batch 20/20] avg loss: 2.5213270409821265		[learning rate: 0.0011827]
	Learning Rate: 0.00118269
	LOSS [training: 2.6572573370014743 | validation: 1.6961875350124755]
	TIME [epoch: 8.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.96181331274173		[learning rate: 0.0011813]
		[batch 20/20] avg loss: 0.6007720677836804		[learning rate: 0.0011799]
	Learning Rate: 0.0011799
	LOSS [training: 1.2812926902627053 | validation: 0.17977649795752926]
	TIME [epoch: 8.29 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4689399767657331		[learning rate: 0.0011785]
		[batch 20/20] avg loss: 2.212617134415764		[learning rate: 0.0011771]
	Learning Rate: 0.00117712
	LOSS [training: 1.3407785555907483 | validation: 2.716567490324264]
	TIME [epoch: 8.3 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2240348075325884		[learning rate: 0.0011757]
		[batch 20/20] avg loss: 2.446764697220556		[learning rate: 0.0011743]
	Learning Rate: 0.00117434
	LOSS [training: 2.835399752376573 | validation: 2.732661592276006]
	TIME [epoch: 8.32 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.728537201765764		[learning rate: 0.001173]
		[batch 20/20] avg loss: 3.606432715188695		[learning rate: 0.0011716]
	Learning Rate: 0.00117157
	LOSS [training: 3.1674849584772296 | validation: 4.1452485938840455]
	TIME [epoch: 8.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8408045040147174		[learning rate: 0.0011702]
		[batch 20/20] avg loss: 4.325559286936735		[learning rate: 0.0011688]
	Learning Rate: 0.00116881
	LOSS [training: 4.083181895475725 | validation: 3.4853759911899003]
	TIME [epoch: 8.3 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4133298678792627		[learning rate: 0.0011674]
		[batch 20/20] avg loss: 2.9532450024496546		[learning rate: 0.0011661]
	Learning Rate: 0.00116605
	LOSS [training: 3.1832874351644582 | validation: 2.8212495317141997]
	TIME [epoch: 8.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.239676608991775		[learning rate: 0.0011647]
		[batch 20/20] avg loss: 1.8840014327844117		[learning rate: 0.0011633]
	Learning Rate: 0.0011633
	LOSS [training: 2.061839020888094 | validation: 1.5638050306047566]
	TIME [epoch: 8.32 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3512411757465332		[learning rate: 0.0011619]
		[batch 20/20] avg loss: 1.402357403638361		[learning rate: 0.0011606]
	Learning Rate: 0.00116056
	LOSS [training: 1.3767992896924472 | validation: 1.029576088570113]
	TIME [epoch: 8.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1594671763141133		[learning rate: 0.0011592]
		[batch 20/20] avg loss: 1.0376598660820355		[learning rate: 0.0011578]
	Learning Rate: 0.00115782
	LOSS [training: 1.0985635211980742 | validation: 0.9340435589833052]
	TIME [epoch: 8.29 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8946559727118946		[learning rate: 0.0011565]
		[batch 20/20] avg loss: 0.979597418534819		[learning rate: 0.0011551]
	Learning Rate: 0.00115509
	LOSS [training: 0.9371266956233566 | validation: 0.8995303648672548]
	TIME [epoch: 8.29 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8291872511634697		[learning rate: 0.0011537]
		[batch 20/20] avg loss: 0.8342092596806434		[learning rate: 0.0011524]
	Learning Rate: 0.00115236
	LOSS [training: 0.8316982554220566 | validation: 0.7508843776354439]
	TIME [epoch: 8.29 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6006023806981731		[learning rate: 0.001151]
		[batch 20/20] avg loss: 0.3186622745588557		[learning rate: 0.0011496]
	Learning Rate: 0.00114965
	LOSS [training: 0.4596323276285144 | validation: 0.23219308019299345]
	TIME [epoch: 8.32 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21283636236790254		[learning rate: 0.0011483]
		[batch 20/20] avg loss: 0.19169128126591523		[learning rate: 0.0011469]
	Learning Rate: 0.00114693
	LOSS [training: 0.20226382181690888 | validation: 0.2189235130914891]
	TIME [epoch: 8.29 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1606424807796199		[learning rate: 0.0011456]
		[batch 20/20] avg loss: 0.1553103082952151		[learning rate: 0.0011442]
	Learning Rate: 0.00114423
	LOSS [training: 0.15797639453741752 | validation: 0.13404997094593582]
	TIME [epoch: 8.29 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13754814004497734		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 0.15968495034741845		[learning rate: 0.0011415]
	Learning Rate: 0.00114153
	LOSS [training: 0.1486165451961979 | validation: 0.12639061240896107]
	TIME [epoch: 8.29 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12558276259713147		[learning rate: 0.0011402]
		[batch 20/20] avg loss: 0.17351800519513427		[learning rate: 0.0011388]
	Learning Rate: 0.00113884
	LOSS [training: 0.1495503838961329 | validation: 0.17523794023847605]
	TIME [epoch: 8.32 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14695115031481576		[learning rate: 0.0011375]
		[batch 20/20] avg loss: 0.12146127759377137		[learning rate: 0.0011362]
	Learning Rate: 0.00113615
	LOSS [training: 0.13420621395429358 | validation: 0.10757037053080226]
	TIME [epoch: 8.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9708967129365547		[learning rate: 0.0011348]
		[batch 20/20] avg loss: 2.0479580969662345		[learning rate: 0.0011335]
	Learning Rate: 0.00113347
	LOSS [training: 1.5094274049513945 | validation: 2.4166165932591683]
	TIME [epoch: 8.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5007582987806376		[learning rate: 0.0011321]
		[batch 20/20] avg loss: 2.460461147699857		[learning rate: 0.0011308]
	Learning Rate: 0.0011308
	LOSS [training: 1.980609723240247 | validation: 3.528993405560273]
	TIME [epoch: 8.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.282671401505128		[learning rate: 0.0011295]
		[batch 20/20] avg loss: 4.35735680110684		[learning rate: 0.0011281]
	Learning Rate: 0.00112813
	LOSS [training: 4.3200141013059845 | validation: 2.5956090047278635]
	TIME [epoch: 8.32 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.697864976782563		[learning rate: 0.0011268]
		[batch 20/20] avg loss: 1.9270641513626046		[learning rate: 0.0011255]
	Learning Rate: 0.00112547
	LOSS [training: 2.3124645640725836 | validation: 1.6674394348544315]
	TIME [epoch: 8.31 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.780604391070576		[learning rate: 0.0011241]
		[batch 20/20] avg loss: 1.8165251753061935		[learning rate: 0.0011228]
	Learning Rate: 0.00112281
	LOSS [training: 1.7985647831883853 | validation: 1.4694064238059914]
	TIME [epoch: 8.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.158362065005081		[learning rate: 0.0011215]
		[batch 20/20] avg loss: 0.16985209758353154		[learning rate: 0.0011202]
	Learning Rate: 0.00112017
	LOSS [training: 0.6641070812943063 | validation: 0.1410261480699108]
	TIME [epoch: 8.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2142373681208793		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.212696840593431		[learning rate: 0.0011175]
	Learning Rate: 0.00111752
	LOSS [training: 0.21346710435715513 | validation: 0.11388204644445499]
	TIME [epoch: 8.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13901446372793774		[learning rate: 0.0011162]
		[batch 20/20] avg loss: 0.1343585545580292		[learning rate: 0.0011149]
	Learning Rate: 0.00111489
	LOSS [training: 0.1366865091429835 | validation: 0.1887634240694081]
	TIME [epoch: 8.32 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14315202021146295		[learning rate: 0.0011136]
		[batch 20/20] avg loss: 0.1168430117796386		[learning rate: 0.0011123]
	Learning Rate: 0.00111226
	LOSS [training: 0.12999751599555082 | validation: 0.14564718006628394]
	TIME [epoch: 8.29 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1584839342003867		[learning rate: 0.0011109]
		[batch 20/20] avg loss: 0.15070155850294215		[learning rate: 0.0011096]
	Learning Rate: 0.00110963
	LOSS [training: 0.15459274635166442 | validation: 0.14674869649536507]
	TIME [epoch: 8.29 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1154968826583597		[learning rate: 0.0011083]
		[batch 20/20] avg loss: 0.14316484506639054		[learning rate: 0.001107]
	Learning Rate: 0.00110702
	LOSS [training: 0.12933086386237513 | validation: 0.1045169373676965]
	TIME [epoch: 8.29 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19921500182410493		[learning rate: 0.0011057]
		[batch 20/20] avg loss: 0.12657958808862257		[learning rate: 0.0011044]
	Learning Rate: 0.0011044
	LOSS [training: 0.16289729495636374 | validation: 0.11955953233563077]
	TIME [epoch: 8.32 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15292861435607907		[learning rate: 0.0011031]
		[batch 20/20] avg loss: 0.1402727426072808		[learning rate: 0.0011018]
	Learning Rate: 0.0011018
	LOSS [training: 0.1466006784816799 | validation: 0.10707867386276414]
	TIME [epoch: 8.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12859573492595883		[learning rate: 0.0011005]
		[batch 20/20] avg loss: 0.12754997079130395		[learning rate: 0.0010992]
	Learning Rate: 0.0010992
	LOSS [training: 0.12807285285863138 | validation: 0.09342616896350303]
	TIME [epoch: 8.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13051334820132188		[learning rate: 0.0010979]
		[batch 20/20] avg loss: 0.150594922663584		[learning rate: 0.0010966]
	Learning Rate: 0.00109661
	LOSS [training: 0.14055413543245293 | validation: 0.14294650810401133]
	TIME [epoch: 8.29 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11971598866770114		[learning rate: 0.0010953]
		[batch 20/20] avg loss: 0.12677133413466923		[learning rate: 0.001094]
	Learning Rate: 0.00109402
	LOSS [training: 0.12324366140118521 | validation: 0.11621112888648374]
	TIME [epoch: 8.32 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13474495683792434		[learning rate: 0.0010927]
		[batch 20/20] avg loss: 0.143364866960058		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.13905491189899116 | validation: 0.08854086695268878]
	TIME [epoch: 8.31 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_988.pth
	Model improved!!!
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13357318233979715		[learning rate: 0.0010902]
		[batch 20/20] avg loss: 0.12478330456313666		[learning rate: 0.0010889]
	Learning Rate: 0.00108887
	LOSS [training: 0.12917824345146695 | validation: 0.12076667337292586]
	TIME [epoch: 8.29 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09902496053897687		[learning rate: 0.0010876]
		[batch 20/20] avg loss: 0.1221469722992159		[learning rate: 0.0010863]
	Learning Rate: 0.0010863
	LOSS [training: 0.11058596641909638 | validation: 0.09826439054672223]
	TIME [epoch: 8.28 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12535780965989654		[learning rate: 0.001085]
		[batch 20/20] avg loss: 0.13330654189672275		[learning rate: 0.0010837]
	Learning Rate: 0.00108373
	LOSS [training: 0.12933217577830963 | validation: 0.12558012912438415]
	TIME [epoch: 8.28 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11206504453219886		[learning rate: 0.0010825]
		[batch 20/20] avg loss: 0.14907396916380536		[learning rate: 0.0010812]
	Learning Rate: 0.00108118
	LOSS [training: 0.13056950684800211 | validation: 0.16208962694263318]
	TIME [epoch: 8.31 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1329140511609434		[learning rate: 0.0010799]
		[batch 20/20] avg loss: 0.15414177212443161		[learning rate: 0.0010786]
	Learning Rate: 0.00107863
	LOSS [training: 0.1435279116426875 | validation: 0.11513550250305614]
	TIME [epoch: 8.28 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13913302011042167		[learning rate: 0.0010774]
		[batch 20/20] avg loss: 0.11308940179135037		[learning rate: 0.0010761]
	Learning Rate: 0.00107608
	LOSS [training: 0.126111210950886 | validation: 0.11363821389852839]
	TIME [epoch: 8.28 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10763203964333541		[learning rate: 0.0010748]
		[batch 20/20] avg loss: 0.12420626940747112		[learning rate: 0.0010735]
	Learning Rate: 0.00107355
	LOSS [training: 0.11591915452540327 | validation: 0.095767606414419]
	TIME [epoch: 8.28 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11013421461098234		[learning rate: 0.0010723]
		[batch 20/20] avg loss: 0.13309688776438508		[learning rate: 0.001071]
	Learning Rate: 0.00107101
	LOSS [training: 0.12161555118768372 | validation: 0.14355033075037787]
	TIME [epoch: 8.31 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13684469461209087		[learning rate: 0.0010697]
		[batch 20/20] avg loss: 0.12001880142293608		[learning rate: 0.0010685]
	Learning Rate: 0.00106849
	LOSS [training: 0.1284317480175135 | validation: 0.1407491389036085]
	TIME [epoch: 8.28 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10954811406465167		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.1543059174633127		[learning rate: 0.001066]
	Learning Rate: 0.00106597
	LOSS [training: 0.1319270157639822 | validation: 0.10006386327515585]
	TIME [epoch: 8.28 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15415125952090286		[learning rate: 0.0010647]
		[batch 20/20] avg loss: 0.10638062376484521		[learning rate: 0.0010635]
	Learning Rate: 0.00106345
	LOSS [training: 0.13026594164287408 | validation: 0.14277680876439156]
	TIME [epoch: 8.28 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11261453037421629		[learning rate: 0.0010622]
		[batch 20/20] avg loss: 0.15036742795558863		[learning rate: 0.0010609]
	Learning Rate: 0.00106094
	LOSS [training: 0.13149097916490246 | validation: 0.18178046227136005]
	TIME [epoch: 8.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1338260755190393		[learning rate: 0.0010597]
		[batch 20/20] avg loss: 0.12269798164952934		[learning rate: 0.0010584]
	Learning Rate: 0.00105844
	LOSS [training: 0.12826202858428432 | validation: 0.12722863620767327]
	TIME [epoch: 8.29 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1091520914504384		[learning rate: 0.0010572]
		[batch 20/20] avg loss: 0.11931991460711297		[learning rate: 0.0010559]
	Learning Rate: 0.00105594
	LOSS [training: 0.1142360030287757 | validation: 0.14590283523704356]
	TIME [epoch: 8.28 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13399107913766986		[learning rate: 0.0010547]
		[batch 20/20] avg loss: 0.11125458317742005		[learning rate: 0.0010535]
	Learning Rate: 0.00105345
	LOSS [training: 0.12262283115754495 | validation: 0.10667433959225632]
	TIME [epoch: 8.28 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11051091133287234		[learning rate: 0.0010522]
		[batch 20/20] avg loss: 0.1356564000579315		[learning rate: 0.001051]
	Learning Rate: 0.00105097
	LOSS [training: 0.12308365569540192 | validation: 0.15670960480583937]
	TIME [epoch: 8.28 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1327326308165679		[learning rate: 0.0010497]
		[batch 20/20] avg loss: 0.11093245905632002		[learning rate: 0.0010485]
	Learning Rate: 0.00104849
	LOSS [training: 0.12183254493644394 | validation: 0.10737949533292962]
	TIME [epoch: 8.31 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1134690541830727		[learning rate: 0.0010473]
		[batch 20/20] avg loss: 0.11791339920018165		[learning rate: 0.001046]
	Learning Rate: 0.00104602
	LOSS [training: 0.1156912266916272 | validation: 0.10414120870334373]
	TIME [epoch: 8.29 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12741822391318894		[learning rate: 0.0010448]
		[batch 20/20] avg loss: 0.1329906314099712		[learning rate: 0.0010435]
	Learning Rate: 0.00104355
	LOSS [training: 0.13020442766158005 | validation: 0.12219761412911785]
	TIME [epoch: 8.28 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10597324575658015		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 0.10790265220714687		[learning rate: 0.0010411]
	Learning Rate: 0.00104109
	LOSS [training: 0.10693794898186353 | validation: 0.11445674185857886]
	TIME [epoch: 8.28 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10465080587366138		[learning rate: 0.0010399]
		[batch 20/20] avg loss: 0.10881828293809652		[learning rate: 0.0010386]
	Learning Rate: 0.00103863
	LOSS [training: 0.10673454440587896 | validation: 0.11166031033873858]
	TIME [epoch: 8.31 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1667269848130843		[learning rate: 0.0010374]
		[batch 20/20] avg loss: 0.1293134471438012		[learning rate: 0.0010362]
	Learning Rate: 0.00103618
	LOSS [training: 0.14802021597844278 | validation: 0.10973401992140819]
	TIME [epoch: 8.29 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11588559733280587		[learning rate: 0.001035]
		[batch 20/20] avg loss: 0.14326968996914158		[learning rate: 0.0010337]
	Learning Rate: 0.00103374
	LOSS [training: 0.12957764365097374 | validation: 0.11203156399725855]
	TIME [epoch: 8.29 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1400418969655594		[learning rate: 0.0010325]
		[batch 20/20] avg loss: 0.12250391225788375		[learning rate: 0.0010313]
	Learning Rate: 0.0010313
	LOSS [training: 0.13127290461172159 | validation: 0.1554651334632553]
	TIME [epoch: 8.29 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12116639377236212		[learning rate: 0.0010301]
		[batch 20/20] avg loss: 0.13477332681684476		[learning rate: 0.0010289]
	Learning Rate: 0.00102887
	LOSS [training: 0.12796986029460344 | validation: 0.11445198463242628]
	TIME [epoch: 8.29 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11934326989727047		[learning rate: 0.0010277]
		[batch 20/20] avg loss: 0.11185309765407599		[learning rate: 0.0010264]
	Learning Rate: 0.00102644
	LOSS [training: 0.11559818377567324 | validation: 0.09731535349657547]
	TIME [epoch: 8.31 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11943892244556395		[learning rate: 0.0010252]
		[batch 20/20] avg loss: 0.13948038632498463		[learning rate: 0.001024]
	Learning Rate: 0.00102402
	LOSS [training: 0.12945965438527426 | validation: 0.30692963854797145]
	TIME [epoch: 8.29 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19736938248265487		[learning rate: 0.0010228]
		[batch 20/20] avg loss: 0.10974294421892294		[learning rate: 0.0010216]
	Learning Rate: 0.0010216
	LOSS [training: 0.15355616335078892 | validation: 0.09771312155338521]
	TIME [epoch: 8.29 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11261813159391713		[learning rate: 0.0010204]
		[batch 20/20] avg loss: 0.12599760034681257		[learning rate: 0.0010192]
	Learning Rate: 0.00101919
	LOSS [training: 0.11930786597036487 | validation: 0.15151565222284974]
	TIME [epoch: 8.29 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12048090740033612		[learning rate: 0.001018]
		[batch 20/20] avg loss: 0.18056166761910936		[learning rate: 0.0010168]
	Learning Rate: 0.00101679
	LOSS [training: 0.15052128750972277 | validation: 0.19697740400519415]
	TIME [epoch: 8.31 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12526541476533715		[learning rate: 0.0010156]
		[batch 20/20] avg loss: 0.1429531330663818		[learning rate: 0.0010144]
	Learning Rate: 0.00101439
	LOSS [training: 0.13410927391585944 | validation: 0.10915768722186517]
	TIME [epoch: 8.29 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12766163924479026		[learning rate: 0.0010132]
		[batch 20/20] avg loss: 0.13556215554846351		[learning rate: 0.001012]
	Learning Rate: 0.001012
	LOSS [training: 0.1316118973966269 | validation: 0.10395182193430567]
	TIME [epoch: 8.29 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13505304255620798		[learning rate: 0.0010108]
		[batch 20/20] avg loss: 0.11306515056664175		[learning rate: 0.0010096]
	Learning Rate: 0.00100961
	LOSS [training: 0.12405909656142484 | validation: 0.14331991487060597]
	TIME [epoch: 8.29 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11881916980145904		[learning rate: 0.0010084]
		[batch 20/20] avg loss: 0.10968563490263472		[learning rate: 0.0010072]
	Learning Rate: 0.00100723
	LOSS [training: 0.11425240235204685 | validation: 0.11744853769989097]
	TIME [epoch: 8.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13813237618665944		[learning rate: 0.001006]
		[batch 20/20] avg loss: 0.1273413309854518		[learning rate: 0.0010049]
	Learning Rate: 0.00100485
	LOSS [training: 0.1327368535860556 | validation: 0.2230261987107115]
	TIME [epoch: 8.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14095824371358479		[learning rate: 0.0010037]
		[batch 20/20] avg loss: 0.13902001859812108		[learning rate: 0.0010025]
	Learning Rate: 0.00100248
	LOSS [training: 0.13998913115585293 | validation: 0.15725285337137052]
	TIME [epoch: 8.29 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16832218475120742		[learning rate: 0.0010013]
		[batch 20/20] avg loss: 0.11919318577677493		[learning rate: 0.0010001]
	Learning Rate: 0.00100012
	LOSS [training: 0.14375768526399121 | validation: 0.10752367079757717]
	TIME [epoch: 8.29 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13586553048425148		[learning rate: 0.00099894]
		[batch 20/20] avg loss: 0.11937155314947337		[learning rate: 0.00099776]
	Learning Rate: 0.000997759
	LOSS [training: 0.1276185418168624 | validation: 0.10560097232433009]
	TIME [epoch: 8.29 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12646362774478676		[learning rate: 0.00099658]
		[batch 20/20] avg loss: 0.12784722297266968		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 0.12715542535872823 | validation: 0.12640630859351637]
	TIME [epoch: 8.31 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12321466529512223		[learning rate: 0.00099423]
		[batch 20/20] avg loss: 0.16217151146234818		[learning rate: 0.00099306]
	Learning Rate: 0.000993057
	LOSS [training: 0.14269308837873523 | validation: 0.1651010533316627]
	TIME [epoch: 8.29 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1483906793910963		[learning rate: 0.00099189]
		[batch 20/20] avg loss: 0.11612585043811312		[learning rate: 0.00099071]
	Learning Rate: 0.000990715
	LOSS [training: 0.1322582649146047 | validation: 0.1142490569414157]
	TIME [epoch: 8.29 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11218438458681881		[learning rate: 0.00098955]
		[batch 20/20] avg loss: 0.10643297473492785		[learning rate: 0.00098838]
	Learning Rate: 0.000988378
	LOSS [training: 0.10930867966087335 | validation: 0.13849942967381923]
	TIME [epoch: 8.29 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1077559683779575		[learning rate: 0.00098721]
		[batch 20/20] avg loss: 0.12207688019796503		[learning rate: 0.00098605]
	Learning Rate: 0.000986047
	LOSS [training: 0.1149164242879613 | validation: 0.10610677303847094]
	TIME [epoch: 8.31 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10269728538170328		[learning rate: 0.00098488]
		[batch 20/20] avg loss: 0.11626019201307716		[learning rate: 0.00098372]
	Learning Rate: 0.000983721
	LOSS [training: 0.10947873869739022 | validation: 0.1150091815200637]
	TIME [epoch: 8.29 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1435056403624226		[learning rate: 0.00098256]
		[batch 20/20] avg loss: 0.13457209288453248		[learning rate: 0.0009814]
	Learning Rate: 0.0009814
	LOSS [training: 0.13903886662347753 | validation: 0.11497448136721909]
	TIME [epoch: 8.29 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13968350805021804		[learning rate: 0.00098024]
		[batch 20/20] avg loss: 0.12498491747571412		[learning rate: 0.00097909]
	Learning Rate: 0.000979085
	LOSS [training: 0.1323342127629661 | validation: 0.11942617781026249]
	TIME [epoch: 8.29 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14470349290076026		[learning rate: 0.00097793]
		[batch 20/20] avg loss: 0.12660011686205036		[learning rate: 0.00097678]
	Learning Rate: 0.000976776
	LOSS [training: 0.13565180488140532 | validation: 0.09829207495324349]
	TIME [epoch: 8.31 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1119284712432604		[learning rate: 0.00097562]
		[batch 20/20] avg loss: 0.13279994812797166		[learning rate: 0.00097447]
	Learning Rate: 0.000974472
	LOSS [training: 0.12236420968561604 | validation: 0.11482560643586248]
	TIME [epoch: 8.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11935044743727954		[learning rate: 0.00097332]
		[batch 20/20] avg loss: 0.11923691905981566		[learning rate: 0.00097217]
	Learning Rate: 0.000972173
	LOSS [training: 0.11929368324854764 | validation: 0.17525398872582032]
	TIME [epoch: 8.29 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13957168266163694		[learning rate: 0.00097103]
		[batch 20/20] avg loss: 0.1119784168224651		[learning rate: 0.00096988]
	Learning Rate: 0.00096988
	LOSS [training: 0.125775049742051 | validation: 0.1009306760013372]
	TIME [epoch: 8.29 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12274331122372757		[learning rate: 0.00096874]
		[batch 20/20] avg loss: 0.11945549412209637		[learning rate: 0.00096759]
	Learning Rate: 0.000967592
	LOSS [training: 0.12109940267291194 | validation: 0.14106801251092974]
	TIME [epoch: 8.29 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13801297292490644		[learning rate: 0.00096645]
		[batch 20/20] avg loss: 0.13382754641611014		[learning rate: 0.00096531]
	Learning Rate: 0.00096531
	LOSS [training: 0.13592025967050828 | validation: 0.10770892663135348]
	TIME [epoch: 8.32 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11600898474884506		[learning rate: 0.00096417]
		[batch 20/20] avg loss: 0.11085586364331583		[learning rate: 0.00096303]
	Learning Rate: 0.000963033
	LOSS [training: 0.11343242419608046 | validation: 0.18704924655111355]
	TIME [epoch: 8.29 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11303467571061651		[learning rate: 0.0009619]
		[batch 20/20] avg loss: 0.11558891606799146		[learning rate: 0.00096076]
	Learning Rate: 0.000960761
	LOSS [training: 0.11431179588930403 | validation: 0.13640352435644457]
	TIME [epoch: 8.29 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11031340819033172		[learning rate: 0.00095963]
		[batch 20/20] avg loss: 0.10766215287410116		[learning rate: 0.00095849]
	Learning Rate: 0.000958495
	LOSS [training: 0.10898778053221644 | validation: 0.09975989061687597]
	TIME [epoch: 8.29 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10573464248480062		[learning rate: 0.00095736]
		[batch 20/20] avg loss: 0.13786634467590902		[learning rate: 0.00095623]
	Learning Rate: 0.000956234
	LOSS [training: 0.12180049358035483 | validation: 0.1563675787712228]
	TIME [epoch: 8.32 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1159456856067308		[learning rate: 0.00095511]
		[batch 20/20] avg loss: 0.14670810103505347		[learning rate: 0.00095398]
	Learning Rate: 0.000953978
	LOSS [training: 0.1313268933208921 | validation: 0.1601346314312329]
	TIME [epoch: 8.29 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1243480787829833		[learning rate: 0.00095285]
		[batch 20/20] avg loss: 0.14251378669016793		[learning rate: 0.00095173]
	Learning Rate: 0.000951728
	LOSS [training: 0.13343093273657564 | validation: 0.12971518627909318]
	TIME [epoch: 8.29 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11617477295140315		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 0.11480639316131032		[learning rate: 0.00094948]
	Learning Rate: 0.000949483
	LOSS [training: 0.11549058305635676 | validation: 0.1243436788494574]
	TIME [epoch: 8.29 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12231868795441933		[learning rate: 0.00094836]
		[batch 20/20] avg loss: 0.11952189768860892		[learning rate: 0.00094724]
	Learning Rate: 0.000947243
	LOSS [training: 0.12092029282151415 | validation: 0.10421592638377976]
	TIME [epoch: 8.32 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14495927572618333		[learning rate: 0.00094613]
		[batch 20/20] avg loss: 0.10011355741285954		[learning rate: 0.00094501]
	Learning Rate: 0.000945009
	LOSS [training: 0.12253641656952141 | validation: 0.08655303030657421]
	TIME [epoch: 8.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r3_20240219_184940/states/model_tr_study1_1049.pth
	Model improved!!!
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1158250359072592		[learning rate: 0.00094389]
		[batch 20/20] avg loss: 0.9859093072258858		[learning rate: 0.00094278]
	Learning Rate: 0.00094278
	LOSS [training: 0.5508671715665725 | validation: 2.5244440197901397]
	TIME [epoch: 8.29 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0068722487342745		[learning rate: 0.00094167]
		[batch 20/20] avg loss: 3.4802740610616447		[learning rate: 0.00094056]
	Learning Rate: 0.000940556
	LOSS [training: 3.24357315489796 | validation: 3.374240677420626]
	TIME [epoch: 8.28 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.037855423323869		[learning rate: 0.00093945]
		[batch 20/20] avg loss: 4.846056127927225		[learning rate: 0.00093834]
	Learning Rate: 0.000938337
	LOSS [training: 4.441955775625547 | validation: 4.638228925122149]
	TIME [epoch: 8.29 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.819319971118214		[learning rate: 0.00093723]
		[batch 20/20] avg loss: 4.632917939488357		[learning rate: 0.00093612]
	Learning Rate: 0.000936124
	LOSS [training: 4.726118955303286 | validation: 4.969793810263588]
	TIME [epoch: 8.32 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.534668576675601		[learning rate: 0.00093502]
		[batch 20/20] avg loss: 5.990785998006145		[learning rate: 0.00093392]
	Learning Rate: 0.000933916
	LOSS [training: 5.762727287340874 | validation: 5.300890968259114]
	TIME [epoch: 8.29 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.354656259122065		[learning rate: 0.00093281]
		[batch 20/20] avg loss: 5.235889007857462		[learning rate: 0.00093171]
	Learning Rate: 0.000931713
	LOSS [training: 5.2952726334897635 | validation: 5.128736546923493]
	TIME [epoch: 8.29 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.406502077934988		[learning rate: 0.00093061]
		[batch 20/20] avg loss: 5.251067785445985		[learning rate: 0.00092951]
	Learning Rate: 0.000929515
	LOSS [training: 5.328784931690486 | validation: 4.658991406841482]
	TIME [epoch: 8.28 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.728753669667163		[learning rate: 0.00092842]
		[batch 20/20] avg loss: 4.900213017031132		[learning rate: 0.00092732]
	Learning Rate: 0.000927322
	LOSS [training: 4.814483343349147 | validation: 4.688111363993525]
	TIME [epoch: 8.31 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.6579707200771745		[learning rate: 0.00092623]
		[batch 20/20] avg loss: 4.566825655376371		[learning rate: 0.00092513]
	Learning Rate: 0.000925135
	LOSS [training: 4.612398187726772 | validation: 4.255494529452225]
	TIME [epoch: 8.29 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.589936109833762		[learning rate: 0.00092404]
		[batch 20/20] avg loss: 4.087655670225765		[learning rate: 0.00092295]
	Learning Rate: 0.000922953
	LOSS [training: 4.338795890029764 | validation: 3.8359977327803607]
	TIME [epoch: 8.29 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.434933649617013		[learning rate: 0.00092186]
		[batch 20/20] avg loss: 4.724000302027525		[learning rate: 0.00092078]
	Learning Rate: 0.000920776
	LOSS [training: 4.579466975822268 | validation: 4.657679723256733]
	TIME [epoch: 8.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.8905056413314885		[learning rate: 0.00091969]
		[batch 20/20] avg loss: 4.785739400476719		[learning rate: 0.0009186]
	Learning Rate: 0.000918604
	LOSS [training: 4.838122520904104 | validation: 4.69759870912313]
	TIME [epoch: 8.31 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.843994892227188		[learning rate: 0.00091752]
		[batch 20/20] avg loss: 4.72415790133784		[learning rate: 0.00091644]
	Learning Rate: 0.000916437
	LOSS [training: 4.784076396782514 | validation: 4.3941385793727]
	TIME [epoch: 8.31 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.520140382134257		[learning rate: 0.00091536]
		[batch 20/20] avg loss: 4.607781408600229		[learning rate: 0.00091428]
	Learning Rate: 0.000914275
	LOSS [training: 4.563960895367242 | validation: 4.367866349888558]
	TIME [epoch: 8.29 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.769421679494135		[learning rate: 0.0009132]
		[batch 20/20] avg loss: 4.373421958604912		[learning rate: 0.00091212]
	Learning Rate: 0.000912119
	LOSS [training: 4.571421819049524 | validation: 4.231743577183126]
	TIME [epoch: 8.29 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.366398937404471		[learning rate: 0.00091104]
		[batch 20/20] avg loss: 4.402207123839009		[learning rate: 0.00090997]
	Learning Rate: 0.000909967
	LOSS [training: 4.38430303062174 | validation: 4.266967178692041]
	TIME [epoch: 8.29 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.449268477447609		[learning rate: 0.00090889]
		[batch 20/20] avg loss: 4.52338807197914		[learning rate: 0.00090782]
	Learning Rate: 0.00090782
	LOSS [training: 4.486328274713374 | validation: 4.257779591198265]
	TIME [epoch: 8.31 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.442091829925733		[learning rate: 0.00090675]
		[batch 20/20] avg loss: 4.9055220224625975		[learning rate: 0.00090568]
	Learning Rate: 0.000905679
	LOSS [training: 4.673806926194165 | validation: 4.56888594696906]
	TIME [epoch: 8.29 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.792603438527875		[learning rate: 0.00090461]
		[batch 20/20] avg loss: 5.085241792113297		[learning rate: 0.00090354]
	Learning Rate: 0.000903543
	LOSS [training: 4.938922615320585 | validation: 5.143267699750688]
	TIME [epoch: 8.29 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.354146387627248		[learning rate: 0.00090248]
		[batch 20/20] avg loss: 5.567705773173533		[learning rate: 0.00090141]
	Learning Rate: 0.000901411
	LOSS [training: 5.460926080400391 | validation: 5.1656685778769695]
	TIME [epoch: 8.29 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.261477657018279		[learning rate: 0.00090035]
		[batch 20/20] avg loss: 5.490603187020054		[learning rate: 0.00089929]
	Learning Rate: 0.000899285
	LOSS [training: 5.376040422019167 | validation: 5.470378649199324]
	TIME [epoch: 8.31 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.863667906084041		[learning rate: 0.00089822]
		[batch 20/20] avg loss: 6.167278521723141		[learning rate: 0.00089716]
	Learning Rate: 0.000897164
	LOSS [training: 6.015473213903592 | validation: 5.678137784689293]
	TIME [epoch: 8.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.468494716218046		[learning rate: 0.00089611]
		[batch 20/20] avg loss: 6.1424734768377345		[learning rate: 0.00089505]
	Learning Rate: 0.000895048
	LOSS [training: 6.3054840965278895 | validation: 5.856166868802825]
	TIME [epoch: 8.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.658387935668472		[learning rate: 0.00089399]
		[batch 20/20] avg loss: 5.200338561678562		[learning rate: 0.00089294]
	Learning Rate: 0.000892936
	LOSS [training: 5.429363248673517 | validation: 4.741971998634191]
	TIME [epoch: 8.29 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.813665428671673		[learning rate: 0.00089188]
		[batch 20/20] avg loss: 5.054682443291854		[learning rate: 0.00089083]
	Learning Rate: 0.00089083
	LOSS [training: 4.934173935981763 | validation: 4.973564830121072]
	TIME [epoch: 8.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.997832342233915		[learning rate: 0.00088978]
		[batch 20/20] avg loss: 5.0847834021765745		[learning rate: 0.00088873]
	Learning Rate: 0.000888729
	LOSS [training: 5.0413078722052465 | validation: 5.053844035433563]
	TIME [epoch: 8.32 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.21129068591181		[learning rate: 0.00088768]
		[batch 20/20] avg loss: 5.27492017342858		[learning rate: 0.00088663]
	Learning Rate: 0.000886632
	LOSS [training: 5.243105429670195 | validation: 4.614178097994148]
	TIME [epoch: 8.29 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.800660373032743		[learning rate: 0.00088559]
		[batch 20/20] avg loss: 4.88183497121035		[learning rate: 0.00088454]
	Learning Rate: 0.000884541
	LOSS [training: 4.841247672121546 | validation: 4.909318158310272]
	TIME [epoch: 8.29 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.83891448055353		[learning rate: 0.0008835]
		[batch 20/20] avg loss: 5.384515531811875		[learning rate: 0.00088245]
	Learning Rate: 0.000882454
	LOSS [training: 5.111715006182702 | validation: 4.881225428039373]
	TIME [epoch: 8.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.152066328446811		[learning rate: 0.00088141]
		[batch 20/20] avg loss: 5.024436902631668		[learning rate: 0.00088037]
	Learning Rate: 0.000880373
	LOSS [training: 5.0882516155392405 | validation: 4.7611335618745105]
	TIME [epoch: 8.32 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.856165937521087		[learning rate: 0.00087933]
		[batch 20/20] avg loss: 5.340448808918398		[learning rate: 0.0008783]
	Learning Rate: 0.000878296
	LOSS [training: 5.098307373219743 | validation: 5.368088279252141]
	TIME [epoch: 8.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.518917672134542		[learning rate: 0.00087726]
		[batch 20/20] avg loss: 5.445308200107446		[learning rate: 0.00087622]
	Learning Rate: 0.000876224
	LOSS [training: 5.482112936120993 | validation: 5.366608848038578]
	TIME [epoch: 8.29 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.173006139743242		[learning rate: 0.00087519]
		[batch 20/20] avg loss: 5.207966102020892		[learning rate: 0.00087416]
	Learning Rate: 0.000874157
	LOSS [training: 5.190486120882067 | validation: 4.917515236788229]
	TIME [epoch: 8.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.271753932936602		[learning rate: 0.00087313]
		[batch 20/20] avg loss: 5.579493466543232		[learning rate: 0.0008721]
	Learning Rate: 0.000872096
	LOSS [training: 5.425623699739917 | validation: 5.3980081923984224]
	TIME [epoch: 8.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.506860963890686		[learning rate: 0.00087107]
		[batch 20/20] avg loss: 6.087191445872071		[learning rate: 0.00087004]
	Learning Rate: 0.000870038
	LOSS [training: 5.797026204881377 | validation: 6.113202628491573]
	TIME [epoch: 8.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.208501194146791		[learning rate: 0.00086901]
		[batch 20/20] avg loss: 6.157019632374612		[learning rate: 0.00086799]
	Learning Rate: 0.000867986
	LOSS [training: 6.1827604132607 | validation: 5.857670848197385]
	TIME [epoch: 8.29 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.161930368731364		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 6.111257434509843		[learning rate: 0.00086594]
	Learning Rate: 0.000865939
	LOSS [training: 6.136593901620602 | validation: 5.690136829640948]
	TIME [epoch: 8.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.075601178606638		[learning rate: 0.00086492]
		[batch 20/20] avg loss: 5.903223113182333		[learning rate: 0.0008639]
	Learning Rate: 0.000863896
	LOSS [training: 5.989412145894486 | validation: 5.556206782934597]
	TIME [epoch: 8.29 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.901811342771953		[learning rate: 0.00086288]
		[batch 20/20] avg loss: 6.134526570933128		[learning rate: 0.00086186]
	Learning Rate: 0.000861858
	LOSS [training: 6.0181689568525405 | validation: 5.6753689645712795]
	TIME [epoch: 8.32 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.68745852917761		[learning rate: 0.00086084]
		[batch 20/20] avg loss: 4.746826029749707		[learning rate: 0.00085983]
	Learning Rate: 0.000859825
	LOSS [training: 5.217142279463659 | validation: 4.037631225952048]
	TIME [epoch: 8.29 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.820382994640692		[learning rate: 0.00085881]
		[batch 20/20] avg loss: 3.825269595377854		[learning rate: 0.0008578]
	Learning Rate: 0.000857797
	LOSS [training: 3.8228262950092726 | validation: 4.293018919608272]
	TIME [epoch: 8.29 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.804800899111586		[learning rate: 0.00085678]
		[batch 20/20] avg loss: 4.6931511935914765		[learning rate: 0.00085577]
	Learning Rate: 0.000855774
	LOSS [training: 4.748976046351531 | validation: 3.95874569004999]
	TIME [epoch: 8.28 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.067773020933705		[learning rate: 0.00085476]
		[batch 20/20] avg loss: 4.411759165356524		[learning rate: 0.00085376]
	Learning Rate: 0.000853755
	LOSS [training: 4.239766093145116 | validation: 3.933217548691828]
	TIME [epoch: 8.32 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.390071506025144		[learning rate: 0.00085275]
		[batch 20/20] avg loss: 5.068097349517672		[learning rate: 0.00085174]
	Learning Rate: 0.000851741
	LOSS [training: 4.729084427771408 | validation: 4.902482970372774]
	TIME [epoch: 8.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.468611586413665		[learning rate: 0.00085074]
		[batch 20/20] avg loss: 5.467567372580282		[learning rate: 0.00084973]
	Learning Rate: 0.000849732
	LOSS [training: 5.468089479496973 | validation: 5.093708675933355]
	TIME [epoch: 8.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.340211264237877		[learning rate: 0.00084873]
		[batch 20/20] avg loss: 5.6818239487312745		[learning rate: 0.00084773]
	Learning Rate: 0.000847728
	LOSS [training: 5.511017606484576 | validation: 5.2580871888326834]
	TIME [epoch: 8.28 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.617475380601987		[learning rate: 0.00084673]
		[batch 20/20] avg loss: 5.778728927212591		[learning rate: 0.00084573]
	Learning Rate: 0.000845728
	LOSS [training: 5.698102153907288 | validation: 5.120591105949746]
	TIME [epoch: 8.31 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.620334435560958		[learning rate: 0.00084473]
		[batch 20/20] avg loss: 5.705577371047309		[learning rate: 0.00084373]
	Learning Rate: 0.000843733
	LOSS [training: 5.662955903304133 | validation: 5.487990362173271]
	TIME [epoch: 8.29 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.934071319389767		[learning rate: 0.00084274]
		[batch 20/20] avg loss: 6.0394742966402575		[learning rate: 0.00084174]
	Learning Rate: 0.000841743
	LOSS [training: 5.986772808015011 | validation: 5.623206755692688]
	TIME [epoch: 8.29 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.024351663557165		[learning rate: 0.00084075]
		[batch 20/20] avg loss: 5.936990221623262		[learning rate: 0.00083976]
	Learning Rate: 0.000839757
	LOSS [training: 5.980670942590215 | validation: 5.5752349326867305]
	TIME [epoch: 8.29 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.913699914042668		[learning rate: 0.00083877]
		[batch 20/20] avg loss: 5.616084524150223		[learning rate: 0.00083778]
	Learning Rate: 0.000837777
	LOSS [training: 5.764892219096444 | validation: 5.163086927656837]
	TIME [epoch: 8.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.587282423142001		[learning rate: 0.00083679]
		[batch 20/20] avg loss: 5.462164898377127		[learning rate: 0.0008358]
	Learning Rate: 0.0008358
	LOSS [training: 5.524723660759563 | validation: 4.996792151435065]
	TIME [epoch: 8.31 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.233353252482601		[learning rate: 0.00083481]
		[batch 20/20] avg loss: 5.3002971301490645		[learning rate: 0.00083383]
	Learning Rate: 0.000833829
	LOSS [training: 5.266825191315833 | validation: 4.862971974448092]
	TIME [epoch: 8.29 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.216154724209386		[learning rate: 0.00083284]
		[batch 20/20] avg loss: 5.648676389683977		[learning rate: 0.00083186]
	Learning Rate: 0.000831862
	LOSS [training: 5.432415556946681 | validation: 5.0858892898046415]
	TIME [epoch: 8.29 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.658655802099769		[learning rate: 0.00083088]
		[batch 20/20] avg loss: 5.8247856223750745		[learning rate: 0.0008299]
	Learning Rate: 0.0008299
	LOSS [training: 5.741720712237422 | validation: 5.513353562403648]
	TIME [epoch: 8.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.014951635998183		[learning rate: 0.00082892]
		[batch 20/20] avg loss: 6.2557000358085535		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 6.135325835903367 | validation: 5.932376414559065]
	TIME [epoch: 8.31 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.2029094337833275		[learning rate: 0.00082697]
		[batch 20/20] avg loss: 6.354413970312699		[learning rate: 0.00082599]
	Learning Rate: 0.000825989
	LOSS [training: 6.278661702048014 | validation: 6.0926742989066724]
	TIME [epoch: 8.29 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.304229861772455		[learning rate: 0.00082501]
		[batch 20/20] avg loss: 6.44393842398073		[learning rate: 0.00082404]
	Learning Rate: 0.000824041
	LOSS [training: 6.374084142876592 | validation: 6.386051794571468]
	TIME [epoch: 8.28 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.555542032897168		[learning rate: 0.00082307]
		[batch 20/20] avg loss: 6.721939511504802		[learning rate: 0.0008221]
	Learning Rate: 0.000822097
	LOSS [training: 6.638740772200985 | validation: 6.647099900613456]
	TIME [epoch: 8.29 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.824815917550248		[learning rate: 0.00082113]
		[batch 20/20] avg loss: 6.830867730845531		[learning rate: 0.00082016]
	Learning Rate: 0.000820158
	LOSS [training: 6.827841824197888 | validation: 6.728044198617088]
	TIME [epoch: 8.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.830027346870537		[learning rate: 0.00081919]
		[batch 20/20] avg loss: 6.718028256468793		[learning rate: 0.00081822]
	Learning Rate: 0.000818223
	LOSS [training: 6.774027801669665 | validation: 6.667857372984651]
	TIME [epoch: 8.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.806260890134912		[learning rate: 0.00081726]
		[batch 20/20] avg loss: 6.683384603495962		[learning rate: 0.00081629]
	Learning Rate: 0.000816293
	LOSS [training: 6.744822746815437 | validation: 6.834546281854149]
	TIME [epoch: 8.29 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.991976629895499		[learning rate: 0.00081533]
		[batch 20/20] avg loss: 7.08334001806102		[learning rate: 0.00081437]
	Learning Rate: 0.000814368
	LOSS [training: 7.037658323978259 | validation: 7.012896792472471]
	TIME [epoch: 8.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.012849583514933		[learning rate: 0.00081341]
		[batch 20/20] avg loss: 7.088694024176505		[learning rate: 0.00081245]
	Learning Rate: 0.000812447
	LOSS [training: 7.050771803845718 | validation: 6.922172750838723]
	TIME [epoch: 8.29 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.155342700250341		[learning rate: 0.00081149]
		[batch 20/20] avg loss: 6.933883880842572		[learning rate: 0.00081053]
	Learning Rate: 0.00081053
	LOSS [training: 7.044613290546456 | validation: 6.845908210295879]
	TIME [epoch: 8.32 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.029542484465966		[learning rate: 0.00080957]
		[batch 20/20] avg loss: 7.0368408451115245		[learning rate: 0.00080862]
	Learning Rate: 0.000808618
	LOSS [training: 7.033191664788744 | validation: 6.944216112735253]
	TIME [epoch: 8.29 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.031277355803698		[learning rate: 0.00080766]
		[batch 20/20] avg loss: 7.077058981571769		[learning rate: 0.00080671]
	Learning Rate: 0.000806711
	LOSS [training: 7.054168168687733 | validation: 6.881549293136872]
	TIME [epoch: 8.29 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.944899871528752		[learning rate: 0.00080576]
		[batch 20/20] avg loss: 7.036357261777276		[learning rate: 0.00080481]
	Learning Rate: 0.000804808
	LOSS [training: 6.9906285666530135 | validation: 6.767605817575266]
	TIME [epoch: 8.29 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.85544468904128		[learning rate: 0.00080386]
		[batch 20/20] avg loss: 6.859964990623288		[learning rate: 0.00080291]
	Learning Rate: 0.00080291
	LOSS [training: 6.857704839832285 | validation: 6.652046337941693]
	TIME [epoch: 8.31 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.700540537191225		[learning rate: 0.00080196]
		[batch 20/20] avg loss: 6.862496407265326		[learning rate: 0.00080102]
	Learning Rate: 0.000801016
	LOSS [training: 6.781518472228275 | validation: 6.746861558906396]
	TIME [epoch: 8.29 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.9465742396689265		[learning rate: 0.00080007]
		[batch 20/20] avg loss: 6.972367054124452		[learning rate: 0.00079913]
	Learning Rate: 0.000799126
	LOSS [training: 6.9594706468966905 | validation: 6.820798264846586]
	TIME [epoch: 8.29 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.239561608312508		[learning rate: 0.00079818]
		[batch 20/20] avg loss: 6.89819125452494		[learning rate: 0.00079724]
	Learning Rate: 0.000797241
	LOSS [training: 7.068876431418725 | validation: 6.851847436963798]
	TIME [epoch: 8.28 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.117015425003993		[learning rate: 0.0007963]
		[batch 20/20] avg loss: 7.090171302686589		[learning rate: 0.00079536]
	Learning Rate: 0.000795361
	LOSS [training: 7.103593363845289 | validation: 6.77778705488065]
	TIME [epoch: 8.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.848534453040001		[learning rate: 0.00079442]
		[batch 20/20] avg loss: 7.017237954765731		[learning rate: 0.00079348]
	Learning Rate: 0.000793484
	LOSS [training: 6.932886203902866 | validation: 6.820723849304374]
	TIME [epoch: 8.31 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.896181802793114		[learning rate: 0.00079255]
		[batch 20/20] avg loss: 7.069078970118829		[learning rate: 0.00079161]
	Learning Rate: 0.000791613
	LOSS [training: 6.982630386455972 | validation: 7.0393966875126575]
	TIME [epoch: 8.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.092550168558523		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 7.263667662616728		[learning rate: 0.00078975]
	Learning Rate: 0.000789745
	LOSS [training: 7.178108915587626 | validation: 7.095076024274226]
	TIME [epoch: 8.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.211311876412758		[learning rate: 0.00078881]
		[batch 20/20] avg loss: 6.859080127563369		[learning rate: 0.00078788]
	Learning Rate: 0.000787882
	LOSS [training: 7.035196001988065 | validation: 6.803731800392505]
	TIME [epoch: 8.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.950225581162225		[learning rate: 0.00078695]
		[batch 20/20] avg loss: 7.039740909630647		[learning rate: 0.00078602]
	Learning Rate: 0.000786024
	LOSS [training: 6.994983245396435 | validation: 6.9481944077125934]
	TIME [epoch: 8.32 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.030623134840901		[learning rate: 0.0007851]
		[batch 20/20] avg loss: 6.846885202219282		[learning rate: 0.00078417]
	Learning Rate: 0.00078417
	LOSS [training: 6.938754168530091 | validation: 6.757682661920317]
	TIME [epoch: 8.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.611952105531534		[learning rate: 0.00078324]
		[batch 20/20] avg loss: 6.6259627471996385		[learning rate: 0.00078232]
	Learning Rate: 0.00078232
	LOSS [training: 6.6189574263655855 | validation: 6.432237537765356]
	TIME [epoch: 8.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.634511279848458		[learning rate: 0.0007814]
		[batch 20/20] avg loss: 6.528086619452648		[learning rate: 0.00078047]
	Learning Rate: 0.000780475
	LOSS [training: 6.581298949650552 | validation: 6.6174629445719955]
	TIME [epoch: 8.29 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.863931117267823		[learning rate: 0.00077955]
		[batch 20/20] avg loss: 6.888245019281499		[learning rate: 0.00077863]
	Learning Rate: 0.000778634
	LOSS [training: 6.876088068274662 | validation: 6.814576840458203]
	TIME [epoch: 8.31 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.027628811629543		[learning rate: 0.00077772]
		[batch 20/20] avg loss: 6.603807586032073		[learning rate: 0.0007768]
	Learning Rate: 0.000776797
	LOSS [training: 6.815718198830808 | validation: 6.7221176878523226]
	TIME [epoch: 8.31 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.927018425571565		[learning rate: 0.00077588]
		[batch 20/20] avg loss: 6.675149604429403		[learning rate: 0.00077496]
	Learning Rate: 0.000774965
	LOSS [training: 6.801084015000486 | validation: 6.629864231136296]
	TIME [epoch: 8.29 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.671698358784596		[learning rate: 0.00077405]
		[batch 20/20] avg loss: 6.655241442592429		[learning rate: 0.00077314]
	Learning Rate: 0.000773137
	LOSS [training: 6.663469900688511 | validation: 6.430724895269685]
	TIME [epoch: 8.29 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.69983113406038		[learning rate: 0.00077222]
		[batch 20/20] avg loss: 6.264401734294287		[learning rate: 0.00077131]
	Learning Rate: 0.000771313
	LOSS [training: 6.482116434177334 | validation: 6.25464689083129]
	TIME [epoch: 8.29 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.284800543747658		[learning rate: 0.0007704]
		[batch 20/20] avg loss: 6.290311029721463		[learning rate: 0.00076949]
	Learning Rate: 0.000769494
	LOSS [training: 6.2875557867345595 | validation: 5.8726359194803806]
	TIME [epoch: 8.32 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.3213130801141215		[learning rate: 0.00076859]
		[batch 20/20] avg loss: 6.511422334925824		[learning rate: 0.00076768]
	Learning Rate: 0.000767679
	LOSS [training: 6.416367707519974 | validation: 6.600665733799401]
	TIME [epoch: 8.29 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.980854664461779		[learning rate: 0.00076677]
		[batch 20/20] avg loss: 6.87478599115593		[learning rate: 0.00076587]
	Learning Rate: 0.000765868
	LOSS [training: 6.927820327808854 | validation: 6.7931011046150624]
	TIME [epoch: 8.31 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.1336367473021784		[learning rate: 0.00076496]
		[batch 20/20] avg loss: 6.987439576733093		[learning rate: 0.00076406]
	Learning Rate: 0.000764061
	LOSS [training: 7.060538162017634 | validation: 6.816953289209389]
	TIME [epoch: 8.29 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.227307671854602		[learning rate: 0.00076316]
		[batch 20/20] avg loss: 7.024829319359879		[learning rate: 0.00076226]
	Learning Rate: 0.000762259
	LOSS [training: 7.126068495607241 | validation: 6.677075311474729]
	TIME [epoch: 8.32 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.929779853872299		[learning rate: 0.00076136]
		[batch 20/20] avg loss: 7.151807114020133		[learning rate: 0.00076046]
	Learning Rate: 0.000760461
	LOSS [training: 7.0407934839462145 | validation: 6.859994272966546]
	TIME [epoch: 8.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.149747954499055		[learning rate: 0.00075956]
		[batch 20/20] avg loss: 7.30087500190348		[learning rate: 0.00075867]
	Learning Rate: 0.000758667
	LOSS [training: 7.2253114782012675 | validation: 7.074170280313803]
	TIME [epoch: 8.29 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.171280259280141		[learning rate: 0.00075777]
		[batch 20/20] avg loss: 7.309579055729034		[learning rate: 0.00075688]
	Learning Rate: 0.000756877
	LOSS [training: 7.2404296575045874 | validation: 7.027988332974761]
	TIME [epoch: 8.29 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.425561130664475		[learning rate: 0.00075598]
		[batch 20/20] avg loss: 7.332928346605091		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 7.379244738634782 | validation: 7.242268344580191]
	TIME [epoch: 8.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.690907220671322		[learning rate: 0.0007542]
		[batch 20/20] avg loss: 7.372084312408181		[learning rate: 0.00075331]
	Learning Rate: 0.000753311
	LOSS [training: 7.531495766539753 | validation: 7.099858195047925]
	TIME [epoch: 8.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.496508826500495		[learning rate: 0.00075242]
		[batch 20/20] avg loss: 7.442429771742384		[learning rate: 0.00075153]
	Learning Rate: 0.000751534
	LOSS [training: 7.46946929912144 | validation: 7.244909316852621]
	TIME [epoch: 8.29 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.402348095263557		[learning rate: 0.00075065]
		[batch 20/20] avg loss: 7.709676310568658		[learning rate: 0.00074976]
	Learning Rate: 0.000749761
	LOSS [training: 7.556012202916108 | validation: 7.325554780268047]
	TIME [epoch: 8.29 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.593158181835354		[learning rate: 0.00074888]
		[batch 20/20] avg loss: 7.5721360304239855		[learning rate: 0.00074799]
	Learning Rate: 0.000747993
	LOSS [training: 7.58264710612967 | validation: 7.311170161362339]
	TIME [epoch: 8.29 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.429405863222937		[learning rate: 0.00074711]
		[batch 20/20] avg loss: 7.335567876869591		[learning rate: 0.00074623]
	Learning Rate: 0.000746228
	LOSS [training: 7.382486870046263 | validation: 7.095637155013575]
	TIME [epoch: 8.32 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.270441256980322		[learning rate: 0.00074535]
		[batch 20/20] avg loss: 7.382002285528739		[learning rate: 0.00074447]
	Learning Rate: 0.000744468
	LOSS [training: 7.326221771254529 | validation: 7.192190636139039]
	TIME [epoch: 8.29 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.4266515129829855		[learning rate: 0.00074359]
		[batch 20/20] avg loss: 7.713679497706821		[learning rate: 0.00074271]
	Learning Rate: 0.000742712
	LOSS [training: 7.570165505344903 | validation: 7.447257998434273]
	TIME [epoch: 8.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.605183635439102		[learning rate: 0.00074184]
		[batch 20/20] avg loss: 7.259752900983872		[learning rate: 0.00074096]
	Learning Rate: 0.00074096
	LOSS [training: 7.432468268211485 | validation: 7.016408951036228]
	TIME [epoch: 8.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.291799439097716		[learning rate: 0.00074009]
		[batch 20/20] avg loss: 7.155755342711632		[learning rate: 0.00073921]
	Learning Rate: 0.000739212
	LOSS [training: 7.223777390904675 | validation: 7.054281474721499]
	TIME [epoch: 8.32 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.419922796396267		[learning rate: 0.00073834]
		[batch 20/20] avg loss: 7.315284609783392		[learning rate: 0.00073747]
	Learning Rate: 0.000737469
	LOSS [training: 7.36760370308983 | validation: 7.289941349187858]
	TIME [epoch: 8.29 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.410084086792999		[learning rate: 0.0007366]
		[batch 20/20] avg loss: 7.443535782107037		[learning rate: 0.00073573]
	Learning Rate: 0.000735729
	LOSS [training: 7.426809934450018 | validation: 7.306766883328674]
	TIME [epoch: 8.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.432871229011059		[learning rate: 0.00073486]
		[batch 20/20] avg loss: 7.453643544375042		[learning rate: 0.00073399]
	Learning Rate: 0.000733994
	LOSS [training: 7.44325738669305 | validation: 7.261300630534815]
	TIME [epoch: 8.29 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.336397748229777		[learning rate: 0.00073313]
		[batch 20/20] avg loss: 7.338143650368768		[learning rate: 0.00073226]
	Learning Rate: 0.000732262
	LOSS [training: 7.337270699299275 | validation: 7.307100033677855]
	TIME [epoch: 8.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.449215059850284		[learning rate: 0.0007314]
		[batch 20/20] avg loss: 7.506345176953045		[learning rate: 0.00073054]
	Learning Rate: 0.000730535
	LOSS [training: 7.4777801184016655 | validation: 7.392458153042353]
	TIME [epoch: 8.31 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.520220614364239		[learning rate: 0.00072967]
		[batch 20/20] avg loss: 7.233637215028608		[learning rate: 0.00072881]
	Learning Rate: 0.000728812
	LOSS [training: 7.376928914696424 | validation: 7.1321102092048285]
	TIME [epoch: 8.29 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.124259074371567		[learning rate: 0.00072795]
		[batch 20/20] avg loss: 7.304231628134704		[learning rate: 0.00072709]
	Learning Rate: 0.000727093
	LOSS [training: 7.214245351253136 | validation: 7.160827854088038]
	TIME [epoch: 8.29 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.219556609292877		[learning rate: 0.00072623]
		[batch 20/20] avg loss: 7.396043120106175		[learning rate: 0.00072538]
	Learning Rate: 0.000725378
	LOSS [training: 7.307799864699527 | validation: 7.293765789834016]
	TIME [epoch: 8.29 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.481318605058306		[learning rate: 0.00072452]
		[batch 20/20] avg loss: 7.255600249261228		[learning rate: 0.00072367]
	Learning Rate: 0.000723666
	LOSS [training: 7.368459427159768 | validation: 7.330490787684517]
	TIME [epoch: 8.31 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.285728207929848		[learning rate: 0.00072281]
		[batch 20/20] avg loss: 7.409709439008067		[learning rate: 0.00072196]
	Learning Rate: 0.000721959
	LOSS [training: 7.347718823468957 | validation: 7.17831670269368]
	TIME [epoch: 8.28 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.2152970134427745		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 7.281472359226302		[learning rate: 0.00072026]
	Learning Rate: 0.000720257
	LOSS [training: 7.248384686334537 | validation: 7.074098596076055]
	TIME [epoch: 8.28 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.272097141805467		[learning rate: 0.00071941]
		[batch 20/20] avg loss: 7.146876765871615		[learning rate: 0.00071856]
	Learning Rate: 0.000718557
	LOSS [training: 7.20948695383854 | validation: 7.1605995693662425]
	TIME [epoch: 8.28 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.202768867872061		[learning rate: 0.00071771]
		[batch 20/20] avg loss: 7.127789523614915		[learning rate: 0.00071686]
	Learning Rate: 0.000716863
	LOSS [training: 7.165279195743487 | validation: 6.937346703661144]
	TIME [epoch: 8.31 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.010743002523796		[learning rate: 0.00071602]
		[batch 20/20] avg loss: 6.971336601056994		[learning rate: 0.00071517]
	Learning Rate: 0.000715172
	LOSS [training: 6.991039801790395 | validation: 6.884232774033361]
	TIME [epoch: 8.29 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.823268430739214		[learning rate: 0.00071433]
		[batch 20/20] avg loss: 7.0195299201897665		[learning rate: 0.00071348]
	Learning Rate: 0.000713485
	LOSS [training: 6.921399175464491 | validation: 6.862876723057626]
	TIME [epoch: 8.29 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.992294073595454		[learning rate: 0.00071264]
		[batch 20/20] avg loss: 6.988323410966269		[learning rate: 0.0007118]
	Learning Rate: 0.000711802
	LOSS [training: 6.990308742280862 | validation: 6.8029392393530586]
	TIME [epoch: 8.28 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.937550290000066		[learning rate: 0.00071096]
		[batch 20/20] avg loss: 6.863884327650995		[learning rate: 0.00071012]
	Learning Rate: 0.000710123
	LOSS [training: 6.90071730882553 | validation: 6.740565820155782]
	TIME [epoch: 8.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.694649812383746		[learning rate: 0.00070928]
		[batch 20/20] avg loss: 6.8415958512155415		[learning rate: 0.00070845]
	Learning Rate: 0.000708448
	LOSS [training: 6.768122831799644 | validation: 6.544950604091682]
	TIME [epoch: 8.31 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.690592309952497		[learning rate: 0.00070761]
		[batch 20/20] avg loss: 6.463809209676754		[learning rate: 0.00070678]
	Learning Rate: 0.000706776
	LOSS [training: 6.577200759814625 | validation: 6.386352293529924]
	TIME [epoch: 8.29 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.34593966618304		[learning rate: 0.00070594]
		[batch 20/20] avg loss: 6.071096126907663		[learning rate: 0.00070511]
	Learning Rate: 0.000705109
	LOSS [training: 6.208517896545352 | validation: 5.9100430189784445]
	TIME [epoch: 8.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.2425502144838205		[learning rate: 0.00070428]
		[batch 20/20] avg loss: 6.077952269380104		[learning rate: 0.00070345]
	Learning Rate: 0.000703446
	LOSS [training: 6.160251241931964 | validation: 5.924002877334311]
	TIME [epoch: 8.29 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.332732695441783		[learning rate: 0.00070262]
		[batch 20/20] avg loss: 6.279656271458982		[learning rate: 0.00070179]
	Learning Rate: 0.000701787
	LOSS [training: 6.306194483450383 | validation: 5.994335787100868]
	TIME [epoch: 8.32 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.529156911265035		[learning rate: 0.00070096]
		[batch 20/20] avg loss: 6.383895425649908		[learning rate: 0.00070013]
	Learning Rate: 0.000700131
	LOSS [training: 6.45652616845747 | validation: 6.190138432898567]
	TIME [epoch: 8.29 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.57109314824528		[learning rate: 0.00069931]
		[batch 20/20] avg loss: 6.370810583983202		[learning rate: 0.00069848]
	Learning Rate: 0.00069848
	LOSS [training: 6.470951866114243 | validation: 6.068494255458489]
	TIME [epoch: 8.29 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.355195996735822		[learning rate: 0.00069766]
		[batch 20/20] avg loss: 6.7564406035345055		[learning rate: 0.00069683]
	Learning Rate: 0.000696832
	LOSS [training: 6.555818300135162 | validation: 6.166072911540352]
	TIME [epoch: 8.29 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.636156396889892		[learning rate: 0.00069601]
		[batch 20/20] avg loss: 6.406398016936104		[learning rate: 0.00069519]
	Learning Rate: 0.000695188
	LOSS [training: 6.521277206912998 | validation: 6.1113618166597234]
	TIME [epoch: 8.31 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.243522571452604		[learning rate: 0.00069437]
		[batch 20/20] avg loss: 6.652616500858215		[learning rate: 0.00069355]
	Learning Rate: 0.000693549
	LOSS [training: 6.44806953615541 | validation: 6.14506255347443]
	TIME [epoch: 8.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.517083826549178		[learning rate: 0.00069273]
		[batch 20/20] avg loss: 6.564001215091087		[learning rate: 0.00069191]
	Learning Rate: 0.000691913
	LOSS [training: 6.540542520820132 | validation: 6.3555225252707785]
	TIME [epoch: 8.29 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.613058368041787		[learning rate: 0.0006911]
		[batch 20/20] avg loss: 6.839571818547182		[learning rate: 0.00069028]
	Learning Rate: 0.00069028
	LOSS [training: 6.7263150932944855 | validation: 6.340886805161871]
	TIME [epoch: 8.29 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.789612019061255		[learning rate: 0.00068947]
		[batch 20/20] avg loss: 6.744699681240593		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 6.767155850150924 | validation: 6.38410777659221]
	TIME [epoch: 8.29 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.903497477778773		[learning rate: 0.00068784]
		[batch 20/20] avg loss: 6.642914609779851		[learning rate: 0.00068703]
	Learning Rate: 0.000687028
	LOSS [training: 6.773206043779313 | validation: 6.477067508437509]
	TIME [epoch: 8.31 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.745476284637415		[learning rate: 0.00068622]
		[batch 20/20] avg loss: 7.066812233957433		[learning rate: 0.00068541]
	Learning Rate: 0.000685407
	LOSS [training: 6.906144259297425 | validation: 6.676314156696484]
	TIME [epoch: 8.29 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.991287843051294		[learning rate: 0.0006846]
		[batch 20/20] avg loss: 7.0186897884791675		[learning rate: 0.00068379]
	Learning Rate: 0.00068379
	LOSS [training: 7.004988815765232 | validation: 6.772199038248698]
	TIME [epoch: 8.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.077856353865746		[learning rate: 0.00068298]
		[batch 20/20] avg loss: 6.867229507635749		[learning rate: 0.00068218]
	Learning Rate: 0.000682178
	LOSS [training: 6.972542930750748 | validation: 6.611362515607849]
	TIME [epoch: 8.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.899008566970484		[learning rate: 0.00068137]
		[batch 20/20] avg loss: 6.812073856134087		[learning rate: 0.00068057]
	Learning Rate: 0.000680568
	LOSS [training: 6.855541211552287 | validation: 6.4378675144061255]
	TIME [epoch: 8.32 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.8780168235958765		[learning rate: 0.00067977]
		[batch 20/20] avg loss: 6.5844795721814275		[learning rate: 0.00067896]
	Learning Rate: 0.000678963
	LOSS [training: 6.731248197888651 | validation: 6.374810827650473]
	TIME [epoch: 8.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.431099299328123		[learning rate: 0.00067816]
		[batch 20/20] avg loss: 6.389138400533207		[learning rate: 0.00067736]
	Learning Rate: 0.000677362
	LOSS [training: 6.410118849930664 | validation: 5.992975612880386]
	TIME [epoch: 8.29 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.319381733442039		[learning rate: 0.00067656]
		[batch 20/20] avg loss: 6.29327673345513		[learning rate: 0.00067576]
	Learning Rate: 0.000675764
	LOSS [training: 6.306329233448586 | validation: 6.148845948260812]
	TIME [epoch: 8.29 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.336626017857434		[learning rate: 0.00067497]
		[batch 20/20] avg loss: 6.362594320320055		[learning rate: 0.00067417]
	Learning Rate: 0.00067417
	LOSS [training: 6.349610169088744 | validation: 6.0238450570274304]
	TIME [epoch: 8.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.432789279509724		[learning rate: 0.00067337]
		[batch 20/20] avg loss: 6.346907089706117		[learning rate: 0.00067258]
	Learning Rate: 0.000672579
	LOSS [training: 6.389848184607921 | validation: 6.144579177208142]
	TIME [epoch: 8.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.371980619669364		[learning rate: 0.00067179]
		[batch 20/20] avg loss: 6.309942964085611		[learning rate: 0.00067099]
	Learning Rate: 0.000670993
	LOSS [training: 6.340961791877489 | validation: 6.270812091610232]
	TIME [epoch: 8.28 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.546792330044077		[learning rate: 0.0006702]
		[batch 20/20] avg loss: 6.6257443941575245		[learning rate: 0.00066941]
	Learning Rate: 0.00066941
	LOSS [training: 6.5862683621008005 | validation: 6.387933994128941]
	TIME [epoch: 8.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.7647798773078325		[learning rate: 0.00066862]
		[batch 20/20] avg loss: 6.692720614576461		[learning rate: 0.00066783]
	Learning Rate: 0.000667831
	LOSS [training: 6.728750245942146 | validation: 6.479475354411299]
	TIME [epoch: 8.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.792709814659993		[learning rate: 0.00066704]
		[batch 20/20] avg loss: 6.749823464096826		[learning rate: 0.00066626]
	Learning Rate: 0.000666256
	LOSS [training: 6.771266639378409 | validation: 6.235874647225064]
	TIME [epoch: 8.32 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.562139930457226		[learning rate: 0.00066547]
		[batch 20/20] avg loss: 6.611925663419235		[learning rate: 0.00066468]
	Learning Rate: 0.000664684
	LOSS [training: 6.58703279693823 | validation: 6.306231189039412]
	TIME [epoch: 8.29 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.674380065718571		[learning rate: 0.0006639]
		[batch 20/20] avg loss: 6.521147228122857		[learning rate: 0.00066312]
	Learning Rate: 0.000663116
	LOSS [training: 6.597763646920714 | validation: 6.316472401403896]
	TIME [epoch: 8.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.592937114275148		[learning rate: 0.00066233]
		[batch 20/20] avg loss: 6.370556738218342		[learning rate: 0.00066155]
	Learning Rate: 0.000661552
	LOSS [training: 6.481746926246745 | validation: 6.124217760622949]
	TIME [epoch: 8.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.363492043420298		[learning rate: 0.00066077]
		[batch 20/20] avg loss: 6.083690632551475		[learning rate: 0.00065999]
	Learning Rate: 0.000659992
	LOSS [training: 6.223591337985886 | validation: 5.870731796239841]
	TIME [epoch: 8.32 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.079337342096929		[learning rate: 0.00065921]
		[batch 20/20] avg loss: 6.0682280523897365		[learning rate: 0.00065843]
	Learning Rate: 0.000658435
	LOSS [training: 6.073782697243333 | validation: 6.0143996841867695]
	TIME [epoch: 8.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.0696008022751915		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 6.197162514885698		[learning rate: 0.00065688]
	Learning Rate: 0.000656882
	LOSS [training: 6.1333816585804435 | validation: 5.956152828412773]
	TIME [epoch: 8.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.076805896056943		[learning rate: 0.00065611]
		[batch 20/20] avg loss: 6.27542762197262		[learning rate: 0.00065533]
	Learning Rate: 0.000655332
	LOSS [training: 6.17611675901478 | validation: 6.241176183532488]
	TIME [epoch: 8.29 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.248493196594943		[learning rate: 0.00065456]
		[batch 20/20] avg loss: 6.253913530968488		[learning rate: 0.00065379]
	Learning Rate: 0.000653786
	LOSS [training: 6.251203363781714 | validation: 6.152566465622146]
	TIME [epoch: 8.31 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.225274580186574		[learning rate: 0.00065301]
		[batch 20/20] avg loss: 6.2665666147291414		[learning rate: 0.00065224]
	Learning Rate: 0.000652244
	LOSS [training: 6.245920597457858 | validation: 6.3062987515353806]
	TIME [epoch: 8.31 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.475141530898398		[learning rate: 0.00065147]
		[batch 20/20] avg loss: 6.589006920311047		[learning rate: 0.00065071]
	Learning Rate: 0.000650706
	LOSS [training: 6.532074225604722 | validation: 6.508138290856578]
	TIME [epoch: 8.29 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.738421228411289		[learning rate: 0.00064994]
		[batch 20/20] avg loss: 6.36132463156983		[learning rate: 0.00064917]
	Learning Rate: 0.000649171
	LOSS [training: 6.549872929990561 | validation: 6.461365766545107]
	TIME [epoch: 8.29 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.490562688882615		[learning rate: 0.0006484]
		[batch 20/20] avg loss: 6.5859190304660205		[learning rate: 0.00064764]
	Learning Rate: 0.000647639
	LOSS [training: 6.538240859674318 | validation: 6.498472416619412]
	TIME [epoch: 8.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.468329480566363		[learning rate: 0.00064688]
		[batch 20/20] avg loss: 6.648391726518163		[learning rate: 0.00064611]
	Learning Rate: 0.000646112
	LOSS [training: 6.558360603542264 | validation: 6.457759048279509]
	TIME [epoch: 8.32 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.448362567223181		[learning rate: 0.00064535]
		[batch 20/20] avg loss: 6.586510449991977		[learning rate: 0.00064459]
	Learning Rate: 0.000644588
	LOSS [training: 6.517436508607581 | validation: 6.461381327540781]
	TIME [epoch: 8.29 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.665055107521707		[learning rate: 0.00064383]
		[batch 20/20] avg loss: 6.593377246345287		[learning rate: 0.00064307]
	Learning Rate: 0.000643067
	LOSS [training: 6.629216176933497 | validation: 6.546450001409726]
	TIME [epoch: 8.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.618723506275924		[learning rate: 0.00064231]
		[batch 20/20] avg loss: 6.741497340850465		[learning rate: 0.00064155]
	Learning Rate: 0.00064155
	LOSS [training: 6.680110423563195 | validation: 6.597631126402219]
	TIME [epoch: 8.28 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.699420034268826		[learning rate: 0.00064079]
		[batch 20/20] avg loss: 6.769995368232951		[learning rate: 0.00064004]
	Learning Rate: 0.000640037
	LOSS [training: 6.734707701250888 | validation: 6.769956035280909]
	TIME [epoch: 8.31 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.8653706333487605		[learning rate: 0.00063928]
		[batch 20/20] avg loss: 6.919688046457251		[learning rate: 0.00063853]
	Learning Rate: 0.000638527
	LOSS [training: 6.892529339903007 | validation: 6.889011469486878]
	TIME [epoch: 8.29 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.023101075821931		[learning rate: 0.00063777]
		[batch 20/20] avg loss: 7.214639072273573		[learning rate: 0.00063702]
	Learning Rate: 0.000637021
	LOSS [training: 7.118870074047751 | validation: 7.1265573277165135]
	TIME [epoch: 8.29 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.211304041934781		[learning rate: 0.00063627]
		[batch 20/20] avg loss: 7.376561405646117		[learning rate: 0.00063552]
	Learning Rate: 0.000635519
	LOSS [training: 7.29393272379045 | validation: 7.244752163297953]
	TIME [epoch: 8.29 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.327542928661671		[learning rate: 0.00063477]
		[batch 20/20] avg loss: 7.407575298622744		[learning rate: 0.00063402]
	Learning Rate: 0.000634019
	LOSS [training: 7.367559113642207 | validation: 7.376273716632991]
	TIME [epoch: 8.31 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.492853513642899		[learning rate: 0.00063327]
		[batch 20/20] avg loss: 7.487414795410447		[learning rate: 0.00063252]
	Learning Rate: 0.000632524
	LOSS [training: 7.490134154526674 | validation: 7.473195842758161]
	TIME [epoch: 8.31 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.625099091298151		[learning rate: 0.00063178]
		[batch 20/20] avg loss: 7.511931610282227		[learning rate: 0.00063103]
	Learning Rate: 0.000631032
	LOSS [training: 7.56851535079019 | validation: 7.529904035658733]
	TIME [epoch: 8.29 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.553041697730632		[learning rate: 0.00063029]
		[batch 20/20] avg loss: 7.524338392486698		[learning rate: 0.00062954]
	Learning Rate: 0.000629543
	LOSS [training: 7.538690045108666 | validation: 7.454497525588527]
	TIME [epoch: 8.29 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.485608886660333		[learning rate: 0.0006288]
		[batch 20/20] avg loss: 7.357059767753787		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 7.42133432720706 | validation: 7.283931570278648]
	TIME [epoch: 8.29 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.292393216850213		[learning rate: 0.00062732]
		[batch 20/20] avg loss: 7.348546957977116		[learning rate: 0.00062658]
	Learning Rate: 0.000626577
	LOSS [training: 7.320470087413665 | validation: 7.188799740154721]
	TIME [epoch: 8.31 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.281732000602242		[learning rate: 0.00062584]
		[batch 20/20] avg loss: 7.442651607440565		[learning rate: 0.0006251]
	Learning Rate: 0.000625099
	LOSS [training: 7.362191804021404 | validation: 7.233137695605677]
	TIME [epoch: 8.28 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.258692286029439		[learning rate: 0.00062436]
		[batch 20/20] avg loss: 7.407499637765369		[learning rate: 0.00062362]
	Learning Rate: 0.000623624
	LOSS [training: 7.333095961897404 | validation: 7.199515949130307]
	TIME [epoch: 8.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.303952923407136		[learning rate: 0.00062289]
		[batch 20/20] avg loss: 7.273855968583862		[learning rate: 0.00062215]
	Learning Rate: 0.000622153
	LOSS [training: 7.288904445995499 | validation: 7.111577068199933]
	TIME [epoch: 8.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.311114585832311		[learning rate: 0.00062142]
		[batch 20/20] avg loss: 7.1674577187591755		[learning rate: 0.00062069]
	Learning Rate: 0.000620686
	LOSS [training: 7.239286152295745 | validation: 7.192187697559967]
	TIME [epoch: 8.32 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.2738550343517065		[learning rate: 0.00061995]
		[batch 20/20] avg loss: 7.335309363720735		[learning rate: 0.00061922]
	Learning Rate: 0.000619222
	LOSS [training: 7.304582199036221 | validation: 7.296806524992184]
	TIME [epoch: 8.29 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.412927312555195		[learning rate: 0.00061849]
		[batch 20/20] avg loss: 7.416756413486309		[learning rate: 0.00061776]
	Learning Rate: 0.000617761
	LOSS [training: 7.414841863020753 | validation: 7.395183739185162]
	TIME [epoch: 8.29 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.315007634628797		[learning rate: 0.00061703]
		[batch 20/20] avg loss: 7.504934809843974		[learning rate: 0.0006163]
	Learning Rate: 0.000616304
	LOSS [training: 7.409971222236386 | validation: 7.390939466000184]
	TIME [epoch: 8.29 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.427993968436572		[learning rate: 0.00061558]
		[batch 20/20] avg loss: 7.366019586344109		[learning rate: 0.00061485]
	Learning Rate: 0.00061485
	LOSS [training: 7.397006777390341 | validation: 7.300168586464302]
	TIME [epoch: 8.29 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.500708458758076		[learning rate: 0.00061412]
		[batch 20/20] avg loss: 7.546240658410669		[learning rate: 0.0006134]
	Learning Rate: 0.0006134
	LOSS [training: 7.523474558584375 | validation: 7.577425414577796]
	TIME [epoch: 8.31 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.594333608939898		[learning rate: 0.00061268]
		[batch 20/20] avg loss: 7.482751897420583		[learning rate: 0.00061195]
	Learning Rate: 0.000611953
	LOSS [training: 7.538542753180242 | validation: 7.3576824733693424]
	TIME [epoch: 8.28 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.595388416337381		[learning rate: 0.00061123]
		[batch 20/20] avg loss: 7.403063631390043		[learning rate: 0.00061051]
	Learning Rate: 0.000610509
	LOSS [training: 7.499226023863711 | validation: 7.536826748519412]
	TIME [epoch: 8.29 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.675350123066723		[learning rate: 0.00060979]
		[batch 20/20] avg loss: 7.668679430759838		[learning rate: 0.00060907]
	Learning Rate: 0.000609069
	LOSS [training: 7.672014776913279 | validation: 7.642346871909593]
	TIME [epoch: 8.28 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.781066352696731		[learning rate: 0.00060835]
		[batch 20/20] avg loss: 7.802499074467707		[learning rate: 0.00060763]
	Learning Rate: 0.000607632
	LOSS [training: 7.791782713582218 | validation: 7.516580617426385]
	TIME [epoch: 8.31 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.664766882793009		[learning rate: 0.00060692]
		[batch 20/20] avg loss: 7.687711545761988		[learning rate: 0.0006062]
	Learning Rate: 0.000606199
	LOSS [training: 7.676239214277497 | validation: 7.610516928520097]
	TIME [epoch: 8.29 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6389464784958365		[learning rate: 0.00060548]
		[batch 20/20] avg loss: 7.593222209330233		[learning rate: 0.00060477]
	Learning Rate: 0.000604769
	LOSS [training: 7.616084343913035 | validation: 7.590106502793165]
	TIME [epoch: 8.28 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.747302910095442		[learning rate: 0.00060406]
		[batch 20/20] avg loss: 7.741634571451717		[learning rate: 0.00060334]
	Learning Rate: 0.000603343
	LOSS [training: 7.744468740773579 | validation: 7.519310835980131]
	TIME [epoch: 8.29 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.639011163191884		[learning rate: 0.00060263]
		[batch 20/20] avg loss: 7.64474638459194		[learning rate: 0.00060192]
	Learning Rate: 0.00060192
	LOSS [training: 7.6418787738919125 | validation: 7.497724350297798]
	TIME [epoch: 8.31 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.574590658930336		[learning rate: 0.00060121]
		[batch 20/20] avg loss: 7.738574771411672		[learning rate: 0.0006005]
	Learning Rate: 0.0006005
	LOSS [training: 7.6565827151710035 | validation: 7.705937383233419]
	TIME [epoch: 8.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7965421273771085		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 7.937990421834293		[learning rate: 0.00059908]
	Learning Rate: 0.000599083
	LOSS [training: 7.867266274605702 | validation: 7.791747998711461]
	TIME [epoch: 8.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.904902298746845		[learning rate: 0.00059838]
		[batch 20/20] avg loss: 7.798036155123978		[learning rate: 0.00059767]
	Learning Rate: 0.00059767
	LOSS [training: 7.8514692269354125 | validation: 7.8108359148112765]
	TIME [epoch: 8.29 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.838911379692002		[learning rate: 0.00059696]
		[batch 20/20] avg loss: 7.939900116932904		[learning rate: 0.00059626]
	Learning Rate: 0.00059626
	LOSS [training: 7.889405748312451 | validation: 7.9061437428310155]
	TIME [epoch: 8.28 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.056213021201263		[learning rate: 0.00059556]
		[batch 20/20] avg loss: 8.092515241871167		[learning rate: 0.00059485]
	Learning Rate: 0.000594854
	LOSS [training: 8.074364131536214 | validation: 8.054380696920648]
	TIME [epoch: 8.31 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.112309040765183		[learning rate: 0.00059415]
		[batch 20/20] avg loss: 8.192375881336421		[learning rate: 0.00059345]
	Learning Rate: 0.000593451
	LOSS [training: 8.152342461050802 | validation: 8.091482603761051]
	TIME [epoch: 8.29 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.212956268649211		[learning rate: 0.00059275]
		[batch 20/20] avg loss: 8.251528905963605		[learning rate: 0.00059205]
	Learning Rate: 0.000592051
	LOSS [training: 8.232242587306411 | validation: 8.086335710664425]
	TIME [epoch: 8.29 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.187244080821591		[learning rate: 0.00059135]
		[batch 20/20] avg loss: 8.102864230469704		[learning rate: 0.00059065]
	Learning Rate: 0.000590654
	LOSS [training: 8.14505415564565 | validation: 7.8182410969672285]
	TIME [epoch: 8.28 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.893673511472028		[learning rate: 0.00058996]
		[batch 20/20] avg loss: 7.719914740191598		[learning rate: 0.00058926]
	Learning Rate: 0.000589261
	LOSS [training: 7.806794125831814 | validation: 7.796413676047877]
	TIME [epoch: 8.31 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.995269413241172		[learning rate: 0.00058857]
		[batch 20/20] avg loss: 7.973819161247849		[learning rate: 0.00058787]
	Learning Rate: 0.000587871
	LOSS [training: 7.984544287244512 | validation: 7.830466883072092]
	TIME [epoch: 8.29 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.683965306134181		[learning rate: 0.00058718]
		[batch 20/20] avg loss: 7.6457081487941165		[learning rate: 0.00058648]
	Learning Rate: 0.000586484
	LOSS [training: 7.664836727464149 | validation: 7.552449783697778]
	TIME [epoch: 8.28 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.627904172775482		[learning rate: 0.00058579]
		[batch 20/20] avg loss: 7.596148632733909		[learning rate: 0.0005851]
	Learning Rate: 0.000585101
	LOSS [training: 7.612026402754694 | validation: 7.386102030592145]
	TIME [epoch: 8.28 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.505559687656432		[learning rate: 0.00058441]
		[batch 20/20] avg loss: 7.5188068566853605		[learning rate: 0.00058372]
	Learning Rate: 0.000583721
	LOSS [training: 7.512183272170897 | validation: 7.29516527195103]
	TIME [epoch: 8.31 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.422771589304864		[learning rate: 0.00058303]
		[batch 20/20] avg loss: 7.542065635775801		[learning rate: 0.00058234]
	Learning Rate: 0.000582344
	LOSS [training: 7.482418612540333 | validation: 7.424979338445245]
	TIME [epoch: 8.29 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.542295264010065		[learning rate: 0.00058166]
		[batch 20/20] avg loss: 7.492061900677712		[learning rate: 0.00058097]
	Learning Rate: 0.00058097
	LOSS [training: 7.517178582343886 | validation: 7.364641967603265]
	TIME [epoch: 8.29 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.5104173517375		[learning rate: 0.00058028]
		[batch 20/20] avg loss: 7.4066139727131795		[learning rate: 0.0005796]
	Learning Rate: 0.0005796
	LOSS [training: 7.45851566222534 | validation: 7.307696323099748]
	TIME [epoch: 8.28 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.32489723154472		[learning rate: 0.00057892]
		[batch 20/20] avg loss: 7.533279849576482		[learning rate: 0.00057823]
	Learning Rate: 0.000578233
	LOSS [training: 7.429088540560601 | validation: 7.335002436450985]
	TIME [epoch: 8.28 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.421133189398297		[learning rate: 0.00057755]
		[batch 20/20] avg loss: 7.627934267671162		[learning rate: 0.00057687]
	Learning Rate: 0.000576869
	LOSS [training: 7.52453372853473 | validation: 7.480010797145396]
	TIME [epoch: 8.31 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6165468372961715		[learning rate: 0.00057619]
		[batch 20/20] avg loss: 7.873450995549395		[learning rate: 0.00057551]
	Learning Rate: 0.000575508
	LOSS [training: 7.744998916422785 | validation: 7.840976113848197]
	TIME [epoch: 8.29 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.032635713746263		[learning rate: 0.00057483]
		[batch 20/20] avg loss: 8.045723820830736		[learning rate: 0.00057415]
	Learning Rate: 0.00057415
	LOSS [training: 8.0391797672885 | validation: 7.997446988131854]
	TIME [epoch: 8.28 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.148494956110213		[learning rate: 0.00057347]
		[batch 20/20] avg loss: 8.467388929949925		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 8.30794194303007 | validation: 8.289669674695595]
	TIME [epoch: 8.28 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.471624941302386		[learning rate: 0.00057212]
		[batch 20/20] avg loss: 8.503946091736177		[learning rate: 0.00057144]
	Learning Rate: 0.000571445
	LOSS [training: 8.487785516519283 | validation: 8.447030811211468]
	TIME [epoch: 8.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.590311016664645		[learning rate: 0.00057077]
		[batch 20/20] avg loss: 8.522382703503382		[learning rate: 0.0005701]
	Learning Rate: 0.000570097
	LOSS [training: 8.556346860084012 | validation: 8.317096149364197]
	TIME [epoch: 8.29 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.569306979609355		[learning rate: 0.00056942]
		[batch 20/20] avg loss: 8.611993407036902		[learning rate: 0.00056875]
	Learning Rate: 0.000568752
	LOSS [training: 8.59065019332313 | validation: 8.4349250059279]
	TIME [epoch: 8.28 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.661419330643863		[learning rate: 0.00056808]
		[batch 20/20] avg loss: 8.685242039595398		[learning rate: 0.00056741]
	Learning Rate: 0.000567411
	LOSS [training: 8.673330685119629 | validation: 8.513636566570643]
	TIME [epoch: 8.29 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.73882183767309		[learning rate: 0.00056674]
		[batch 20/20] avg loss: 8.71072124823658		[learning rate: 0.00056607]
	Learning Rate: 0.000566072
	LOSS [training: 8.724771542954834 | validation: 8.503017505503873]
	TIME [epoch: 8.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.629660077140493		[learning rate: 0.0005654]
		[batch 20/20] avg loss: 8.662296337336498		[learning rate: 0.00056474]
	Learning Rate: 0.000564737
	LOSS [training: 8.645978207238496 | validation: 8.365891713482599]
	TIME [epoch: 8.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.63309293158601		[learning rate: 0.00056407]
		[batch 20/20] avg loss: 8.600762166062129		[learning rate: 0.0005634]
	Learning Rate: 0.000563405
	LOSS [training: 8.61692754882407 | validation: 8.419743569497353]
	TIME [epoch: 8.29 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.694939600781831		[learning rate: 0.00056274]
		[batch 20/20] avg loss: 8.78299380751128		[learning rate: 0.00056208]
	Learning Rate: 0.000562076
	LOSS [training: 8.738966704146558 | validation: 8.656363167807383]
	TIME [epoch: 8.29 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.786001378720668		[learning rate: 0.00056141]
		[batch 20/20] avg loss: 8.85392186988186		[learning rate: 0.00056075]
	Learning Rate: 0.00056075
	LOSS [training: 8.819961624301262 | validation: 8.577533193298848]
	TIME [epoch: 8.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.753033172105		[learning rate: 0.00056009]
		[batch 20/20] avg loss: 8.807106980286573		[learning rate: 0.00055943]
	Learning Rate: 0.000559427
	LOSS [training: 8.780070076195788 | validation: 8.614909058866829]
	TIME [epoch: 8.31 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.873459120009619		[learning rate: 0.00055877]
		[batch 20/20] avg loss: 8.80352045777521		[learning rate: 0.00055811]
	Learning Rate: 0.000558108
	LOSS [training: 8.838489788892415 | validation: 8.750534479089659]
	TIME [epoch: 8.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.907319958384623		[learning rate: 0.00055745]
		[batch 20/20] avg loss: 8.756590960399802		[learning rate: 0.00055679]
	Learning Rate: 0.000556791
	LOSS [training: 8.831955459392214 | validation: 8.640119528697053]
	TIME [epoch: 8.29 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.867881371872173		[learning rate: 0.00055613]
		[batch 20/20] avg loss: 8.752445093100633		[learning rate: 0.00055548]
	Learning Rate: 0.000555478
	LOSS [training: 8.810163232486403 | validation: 8.59311804242279]
	TIME [epoch: 8.29 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.739540261999641		[learning rate: 0.00055482]
		[batch 20/20] avg loss: 8.699855622256454		[learning rate: 0.00055417]
	Learning Rate: 0.000554167
	LOSS [training: 8.719697942128048 | validation: 8.432157873710622]
	TIME [epoch: 8.32 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.698539802661534		[learning rate: 0.00055351]
		[batch 20/20] avg loss: 8.671046672906012		[learning rate: 0.00055286]
	Learning Rate: 0.00055286
	LOSS [training: 8.684793237783772 | validation: 8.465360298902617]
	TIME [epoch: 8.29 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.715345098286011		[learning rate: 0.00055221]
		[batch 20/20] avg loss: 8.708344564992785		[learning rate: 0.00055156]
	Learning Rate: 0.000551556
	LOSS [training: 8.711844831639398 | validation: 8.588651470248596]
	TIME [epoch: 8.29 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.732322889484756		[learning rate: 0.00055091]
		[batch 20/20] avg loss: 8.713991267142447		[learning rate: 0.00055026]
	Learning Rate: 0.000550255
	LOSS [training: 8.723157078313601 | validation: 8.578563677893886]
	TIME [epoch: 8.29 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.695756544514609		[learning rate: 0.00054961]
		[batch 20/20] avg loss: 8.648187523771542		[learning rate: 0.00054896]
	Learning Rate: 0.000548957
	LOSS [training: 8.671972034143076 | validation: 8.510914716927617]
	TIME [epoch: 8.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.65246118518265		[learning rate: 0.00054831]
		[batch 20/20] avg loss: 8.651002978644943		[learning rate: 0.00054766]
	Learning Rate: 0.000547662
	LOSS [training: 8.651732081913796 | validation: 8.466896871981838]
	TIME [epoch: 8.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.64018653660995		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 8.691331852312896		[learning rate: 0.00054637]
	Learning Rate: 0.00054637
	LOSS [training: 8.665759194461423 | validation: 8.514366443012268]
	TIME [epoch: 8.29 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.704956656422272		[learning rate: 0.00054573]
		[batch 20/20] avg loss: 8.710530031900554		[learning rate: 0.00054508]
	Learning Rate: 0.000545082
	LOSS [training: 8.707743344161411 | validation: 8.561824866698124]
	TIME [epoch: 8.29 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.64417674707349		[learning rate: 0.00054444]
		[batch 20/20] avg loss: 8.637682253363142		[learning rate: 0.0005438]
	Learning Rate: 0.000543796
	LOSS [training: 8.640929500218313 | validation: 8.4653598320271]
	TIME [epoch: 8.29 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.588434083376098		[learning rate: 0.00054315]
		[batch 20/20] avg loss: 8.636556436038992		[learning rate: 0.00054251]
	Learning Rate: 0.000542513
	LOSS [training: 8.612495259707547 | validation: 8.451214181255509]
	TIME [epoch: 8.32 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.586694732600742		[learning rate: 0.00054187]
		[batch 20/20] avg loss: 8.659444086984319		[learning rate: 0.00054123]
	Learning Rate: 0.000541233
	LOSS [training: 8.623069409792532 | validation: 8.505725866523598]
	TIME [epoch: 8.29 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.701613991286475		[learning rate: 0.00054059]
		[batch 20/20] avg loss: 8.745338637779492		[learning rate: 0.00053996]
	Learning Rate: 0.000539957
	LOSS [training: 8.723476314532983 | validation: 8.652214990779827]
	TIME [epoch: 8.29 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.958703973114975		[learning rate: 0.00053932]
		[batch 20/20] avg loss: 8.85293251746349		[learning rate: 0.00053868]
	Learning Rate: 0.000538683
	LOSS [training: 8.905818245289232 | validation: 8.72693497513071]
	TIME [epoch: 8.29 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.862630745134986		[learning rate: 0.00053805]
		[batch 20/20] avg loss: 8.833906873287635		[learning rate: 0.00053741]
	Learning Rate: 0.000537412
	LOSS [training: 8.848268809211309 | validation: 8.638190073108266]
	TIME [epoch: 8.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.781982824500096		[learning rate: 0.00053678]
		[batch 20/20] avg loss: 8.722013859353938		[learning rate: 0.00053614]
	Learning Rate: 0.000536145
	LOSS [training: 8.751998341927017 | validation: 8.462071255618811]
	TIME [epoch: 8.3 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.59003803093257		[learning rate: 0.00053551]
		[batch 20/20] avg loss: 8.731651771407995		[learning rate: 0.00053488]
	Learning Rate: 0.00053488
	LOSS [training: 8.66084490117028 | validation: 8.469411317534572]
	TIME [epoch: 8.29 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.61465545151987		[learning rate: 0.00053425]
		[batch 20/20] avg loss: 8.584562508981874		[learning rate: 0.00053362]
	Learning Rate: 0.000533618
	LOSS [training: 8.599608980250874 | validation: 8.301683462723098]
	TIME [epoch: 8.29 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.510655350325463		[learning rate: 0.00053299]
		[batch 20/20] avg loss: 8.500370515460672		[learning rate: 0.00053236]
	Learning Rate: 0.00053236
	LOSS [training: 8.505512932893065 | validation: 8.327483002787677]
	TIME [epoch: 8.28 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.52812475381172		[learning rate: 0.00053173]
		[batch 20/20] avg loss: 8.546408145453967		[learning rate: 0.0005311]
	Learning Rate: 0.000531104
	LOSS [training: 8.537266449632844 | validation: 8.329835406009144]
	TIME [epoch: 8.31 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.577380615737727		[learning rate: 0.00053048]
		[batch 20/20] avg loss: 8.655846657843274		[learning rate: 0.00052985]
	Learning Rate: 0.000529851
	LOSS [training: 8.616613636790502 | validation: 8.434082391313463]
	TIME [epoch: 8.28 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.615540043581666		[learning rate: 0.00052923]
		[batch 20/20] avg loss: 8.529734481759991		[learning rate: 0.0005286]
	Learning Rate: 0.000528601
	LOSS [training: 8.57263726267083 | validation: 8.35526989243367]
	TIME [epoch: 8.27 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.537170026592314		[learning rate: 0.00052798]
		[batch 20/20] avg loss: 8.586029837355659		[learning rate: 0.00052735]
	Learning Rate: 0.000527354
	LOSS [training: 8.561599931973987 | validation: 8.35558720860775]
	TIME [epoch: 8.28 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.491925414388907		[learning rate: 0.00052673]
		[batch 20/20] avg loss: 8.666039649532056		[learning rate: 0.00052611]
	Learning Rate: 0.00052611
	LOSS [training: 8.57898253196048 | validation: 8.358051511022891]
	TIME [epoch: 8.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.593893873433519		[learning rate: 0.00052549]
		[batch 20/20] avg loss: 8.541070623438328		[learning rate: 0.00052487]
	Learning Rate: 0.000524869
	LOSS [training: 8.567482248435926 | validation: 8.2565321228299]
	TIME [epoch: 8.29 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.517406946285167		[learning rate: 0.00052425]
		[batch 20/20] avg loss: 8.388617467443577		[learning rate: 0.00052363]
	Learning Rate: 0.000523631
	LOSS [training: 8.45301220686437 | validation: 8.106910754651643]
	TIME [epoch: 8.29 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.318972882393025		[learning rate: 0.00052301]
		[batch 20/20] avg loss: 8.4007973458132		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 8.359885114103113 | validation: 8.186784245981485]
	TIME [epoch: 8.29 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.438063387492425		[learning rate: 0.00052178]
		[batch 20/20] avg loss: 8.32889021460672		[learning rate: 0.00052116]
	Learning Rate: 0.000521164
	LOSS [training: 8.383476801049573 | validation: 8.16661295491256]
	TIME [epoch: 8.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.409829465934443		[learning rate: 0.00052055]
		[batch 20/20] avg loss: 8.331100556367957		[learning rate: 0.00051993]
	Learning Rate: 0.000519935
	LOSS [training: 8.3704650111512 | validation: 8.115243777640778]
	TIME [epoch: 8.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.382104678373043		[learning rate: 0.00051932]
		[batch 20/20] avg loss: 8.232098130063374		[learning rate: 0.00051871]
	Learning Rate: 0.000518708
	LOSS [training: 8.30710140421821 | validation: 8.067213353255191]
	TIME [epoch: 8.29 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.331837368575211		[learning rate: 0.0005181]
		[batch 20/20] avg loss: 8.203491627756996		[learning rate: 0.00051748]
	Learning Rate: 0.000517485
	LOSS [training: 8.267664498166104 | validation: 8.032206074097967]
	TIME [epoch: 8.29 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.309915056318298		[learning rate: 0.00051687]
		[batch 20/20] avg loss: 8.231947630171685		[learning rate: 0.00051626]
	Learning Rate: 0.000516264
	LOSS [training: 8.270931343244992 | validation: 8.07984374539532]
	TIME [epoch: 8.28 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.272807527527412		[learning rate: 0.00051565]
		[batch 20/20] avg loss: 8.313856913931023		[learning rate: 0.00051505]
	Learning Rate: 0.000515046
	LOSS [training: 8.293332220729216 | validation: 8.113644969594617]
	TIME [epoch: 8.31 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.270063686299622		[learning rate: 0.00051444]
		[batch 20/20] avg loss: 8.363349800981165		[learning rate: 0.00051383]
	Learning Rate: 0.000513831
	LOSS [training: 8.316706743640392 | validation: 8.165974374904597]
	TIME [epoch: 8.29 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.281628032840544		[learning rate: 0.00051322]
		[batch 20/20] avg loss: 8.37757123723616		[learning rate: 0.00051262]
	Learning Rate: 0.000512619
	LOSS [training: 8.329599635038353 | validation: 8.108315753007004]
	TIME [epoch: 8.28 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.357923180771708		[learning rate: 0.00051201]
		[batch 20/20] avg loss: 8.247114326198147		[learning rate: 0.00051141]
	Learning Rate: 0.00051141
	LOSS [training: 8.302518753484929 | validation: 8.065012106871048]
	TIME [epoch: 8.29 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.297155795896904		[learning rate: 0.00051081]
		[batch 20/20] avg loss: 8.328592619132872		[learning rate: 0.0005102]
	Learning Rate: 0.000510204
	LOSS [training: 8.312874207514888 | validation: 8.099691619704608]
	TIME [epoch: 8.31 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.383445588023354		[learning rate: 0.0005096]
		[batch 20/20] avg loss: 8.260135152220489		[learning rate: 0.000509]
	Learning Rate: 0.000509
	LOSS [training: 8.32179037012192 | validation: 8.10634774872845]
	TIME [epoch: 8.29 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.303139463610517		[learning rate: 0.0005084]
		[batch 20/20] avg loss: 8.248960125152397		[learning rate: 0.0005078]
	Learning Rate: 0.0005078
	LOSS [training: 8.276049794381459 | validation: 8.047767847174883]
	TIME [epoch: 8.29 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.178473492452643		[learning rate: 0.0005072]
		[batch 20/20] avg loss: 8.3028544232091		[learning rate: 0.0005066]
	Learning Rate: 0.000506602
	LOSS [training: 8.240663957830872 | validation: 8.027245595231783]
	TIME [epoch: 8.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.186060392719886		[learning rate: 0.000506]
		[batch 20/20] avg loss: 8.088153839612037		[learning rate: 0.00050541]
	Learning Rate: 0.000505407
	LOSS [training: 8.137107116165962 | validation: 7.888415927852426]
	TIME [epoch: 8.31 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.933301308305045		[learning rate: 0.00050481]
		[batch 20/20] avg loss: 8.070411298421835		[learning rate: 0.00050421]
	Learning Rate: 0.000504215
	LOSS [training: 8.001856303363443 | validation: 7.755802294609792]
	TIME [epoch: 8.29 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.980521244800871		[learning rate: 0.00050362]
		[batch 20/20] avg loss: 7.955167920923863		[learning rate: 0.00050303]
	Learning Rate: 0.000503025
	LOSS [training: 7.967844582862367 | validation: 7.837028399373177]
	TIME [epoch: 8.29 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.033801776635439		[learning rate: 0.00050243]
		[batch 20/20] avg loss: 8.008674211867973		[learning rate: 0.00050184]
	Learning Rate: 0.000501839
	LOSS [training: 8.021237994251708 | validation: 7.824811315885549]
	TIME [epoch: 8.29 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.893729242262543		[learning rate: 0.00050125]
		[batch 20/20] avg loss: 7.99835126750215		[learning rate: 0.00050065]
	Learning Rate: 0.000500655
	LOSS [training: 7.946040254882346 | validation: 7.757610580279112]
	TIME [epoch: 8.29 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.757113151325544		[learning rate: 0.00050006]
		[batch 20/20] avg loss: 7.985584404603834		[learning rate: 0.00049947]
	Learning Rate: 0.000499474
	LOSS [training: 7.8713487779646885 | validation: 7.719691819321614]
	TIME [epoch: 8.31 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.955242792585773		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 7.888931197047074		[learning rate: 0.0004983]
	Learning Rate: 0.000498296
	LOSS [training: 7.922086994816423 | validation: 7.795558062544578]
	TIME [epoch: 8.28 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.022719535601015		[learning rate: 0.00049771]
		[batch 20/20] avg loss: 7.844872935803311		[learning rate: 0.00049712]
	Learning Rate: 0.00049712
	LOSS [training: 7.933796235702163 | validation: 7.757961240075289]
	TIME [epoch: 8.29 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.962173826388306		[learning rate: 0.00049653]
		[batch 20/20] avg loss: 7.839639856404818		[learning rate: 0.00049595]
	Learning Rate: 0.000495948
	LOSS [training: 7.900906841396564 | validation: 7.7318053471482]
	TIME [epoch: 8.28 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.002404780442909		[learning rate: 0.00049536]
		[batch 20/20] avg loss: 7.875125466434599		[learning rate: 0.00049478]
	Learning Rate: 0.000494778
	LOSS [training: 7.9387651234387535 | validation: 7.733809440557732]
	TIME [epoch: 8.31 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.869141934846601		[learning rate: 0.00049419]
		[batch 20/20] avg loss: 7.849405014918273		[learning rate: 0.00049361]
	Learning Rate: 0.000493611
	LOSS [training: 7.8592734748824356 | validation: 7.543221963610003]
	TIME [epoch: 8.28 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.746046937443571		[learning rate: 0.00049303]
		[batch 20/20] avg loss: 7.700110647099997		[learning rate: 0.00049245]
	Learning Rate: 0.000492446
	LOSS [training: 7.723078792271783 | validation: 7.505399142006987]
	TIME [epoch: 8.29 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8416596483548755		[learning rate: 0.00049187]
		[batch 20/20] avg loss: 7.541713182145481		[learning rate: 0.00049128]
	Learning Rate: 0.000491285
	LOSS [training: 7.691686415250177 | validation: 7.459318921403918]
	TIME [epoch: 8.29 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6347828286847115		[learning rate: 0.0004907]
		[batch 20/20] avg loss: 7.694690678221841		[learning rate: 0.00049013]
	Learning Rate: 0.000490126
	LOSS [training: 7.664736753453276 | validation: 7.491446877906441]
	TIME [epoch: 8.29 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7718390572307054		[learning rate: 0.00048955]
		[batch 20/20] avg loss: 7.641374222367887		[learning rate: 0.00048897]
	Learning Rate: 0.00048897
	LOSS [training: 7.706606639799297 | validation: 7.487914189961053]
	TIME [epoch: 8.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.73856599735755		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 7.551582283860088		[learning rate: 0.00048782]
	Learning Rate: 0.000487816
	LOSS [training: 7.64507414060882 | validation: 7.462804513529864]
	TIME [epoch: 8.28 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.581679871263107		[learning rate: 0.00048724]
		[batch 20/20] avg loss: 7.658556816850459		[learning rate: 0.00048667]
	Learning Rate: 0.000486666
	LOSS [training: 7.620118344056783 | validation: 7.460686852862295]
	TIME [epoch: 8.28 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.694815120089015		[learning rate: 0.00048609]
		[batch 20/20] avg loss: 7.687502495232765		[learning rate: 0.00048552]
	Learning Rate: 0.000485518
	LOSS [training: 7.691158807660889 | validation: 7.540939502265165]
	TIME [epoch: 8.29 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.745920733895689		[learning rate: 0.00048494]
		[batch 20/20] avg loss: 7.903866132828557		[learning rate: 0.00048437]
	Learning Rate: 0.000484372
	LOSS [training: 7.8248934333621225 | validation: 7.686431559265424]
	TIME [epoch: 8.32 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.883803022144694		[learning rate: 0.0004838]
		[batch 20/20] avg loss: 7.854052449924579		[learning rate: 0.00048323]
	Learning Rate: 0.00048323
	LOSS [training: 7.868927736034638 | validation: 7.58947085077231]
	TIME [epoch: 8.29 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.744215006899634		[learning rate: 0.00048266]
		[batch 20/20] avg loss: 7.769977330555848		[learning rate: 0.00048209]
	Learning Rate: 0.00048209
	LOSS [training: 7.757096168727742 | validation: 7.514597849031768]
	TIME [epoch: 8.29 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.690229217134066		[learning rate: 0.00048152]
		[batch 20/20] avg loss: 7.565137372364863		[learning rate: 0.00048095]
	Learning Rate: 0.000480953
	LOSS [training: 7.627683294749465 | validation: 7.43034507807442]
	TIME [epoch: 8.28 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.591285558542902		[learning rate: 0.00048039]
		[batch 20/20] avg loss: 7.67802686049002		[learning rate: 0.00047982]
	Learning Rate: 0.000479818
	LOSS [training: 7.63465620951646 | validation: 7.461081334124089]
	TIME [epoch: 8.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.638356449990937		[learning rate: 0.00047925]
		[batch 20/20] avg loss: 7.747959017899824		[learning rate: 0.00047869]
	Learning Rate: 0.000478687
	LOSS [training: 7.693157733945381 | validation: 7.525603819313171]
	TIME [epoch: 8.29 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6763409494206964		[learning rate: 0.00047812]
		[batch 20/20] avg loss: 7.854027538891242		[learning rate: 0.00047756]
	Learning Rate: 0.000477557
	LOSS [training: 7.765184244155971 | validation: 7.59046386545121]
	TIME [epoch: 8.29 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8435696010274025		[learning rate: 0.00047699]
		[batch 20/20] avg loss: 7.709679521864662		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 7.77662456144603 | validation: 7.574622889736339]
	TIME [epoch: 8.27 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8215615510996646		[learning rate: 0.00047587]
		[batch 20/20] avg loss: 7.814469005810835		[learning rate: 0.00047531]
	Learning Rate: 0.000475307
	LOSS [training: 7.818015278455249 | validation: 7.639443315820019]
	TIME [epoch: 8.29 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.909813314146914		[learning rate: 0.00047475]
		[batch 20/20] avg loss: 7.795242128970592		[learning rate: 0.00047419]
	Learning Rate: 0.000474186
	LOSS [training: 7.8525277215587534 | validation: 7.635822418148388]
	TIME [epoch: 8.31 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.809406712450628		[learning rate: 0.00047363]
		[batch 20/20] avg loss: 7.999714091405077		[learning rate: 0.00047307]
	Learning Rate: 0.000473067
	LOSS [training: 7.904560401927853 | validation: 7.688390969384532]
	TIME [epoch: 8.28 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.843511442247655		[learning rate: 0.00047251]
		[batch 20/20] avg loss: 7.838909434802711		[learning rate: 0.00047195]
	Learning Rate: 0.000471951
	LOSS [training: 7.841210438525181 | validation: 7.568803427207654]
	TIME [epoch: 8.29 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.76301422034963		[learning rate: 0.00047139]
		[batch 20/20] avg loss: 7.8071897743282745		[learning rate: 0.00047084]
	Learning Rate: 0.000470838
	LOSS [training: 7.785101997338954 | validation: 7.608670816148557]
	TIME [epoch: 8.28 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.969400000219494		[learning rate: 0.00047028]
		[batch 20/20] avg loss: 7.732951324934656		[learning rate: 0.00046973]
	Learning Rate: 0.000469728
	LOSS [training: 7.851175662577073 | validation: 7.6298227951448565]
	TIME [epoch: 8.31 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.889808656590032		[learning rate: 0.00046917]
		[batch 20/20] avg loss: 7.837539365853464		[learning rate: 0.00046862]
	Learning Rate: 0.00046862
	LOSS [training: 7.863674011221745 | validation: 7.664709840949133]
	TIME [epoch: 8.27 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.877949633959448		[learning rate: 0.00046807]
		[batch 20/20] avg loss: 7.844539364240383		[learning rate: 0.00046751]
	Learning Rate: 0.000467514
	LOSS [training: 7.861244499099915 | validation: 7.642572036156609]
	TIME [epoch: 8.28 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.876377804309408		[learning rate: 0.00046696]
		[batch 20/20] avg loss: 7.731698707636541		[learning rate: 0.00046641]
	Learning Rate: 0.000466411
	LOSS [training: 7.804038255972975 | validation: 7.593800034345063]
	TIME [epoch: 8.28 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.790258313476222		[learning rate: 0.00046586]
		[batch 20/20] avg loss: 7.8513975831209395		[learning rate: 0.00046531]
	Learning Rate: 0.000465311
	LOSS [training: 7.8208279482985805 | validation: 7.687906878300707]
	TIME [epoch: 8.31 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9370797964060085		[learning rate: 0.00046476]
		[batch 20/20] avg loss: 7.835269044938633		[learning rate: 0.00046421]
	Learning Rate: 0.000464214
	LOSS [training: 7.8861744206723206 | validation: 7.637767111383671]
	TIME [epoch: 8.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.776777541950738		[learning rate: 0.00046367]
		[batch 20/20] avg loss: 7.862602701622299		[learning rate: 0.00046312]
	Learning Rate: 0.000463119
	LOSS [training: 7.819690121786519 | validation: 7.5753703677259425]
	TIME [epoch: 8.29 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.759866181892972		[learning rate: 0.00046257]
		[batch 20/20] avg loss: 7.791594540317223		[learning rate: 0.00046203]
	Learning Rate: 0.000462026
	LOSS [training: 7.775730361105095 | validation: 7.537345548997806]
	TIME [epoch: 8.29 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.757483754124884		[learning rate: 0.00046148]
		[batch 20/20] avg loss: 7.759667379590712		[learning rate: 0.00046094]
	Learning Rate: 0.000460936
	LOSS [training: 7.7585755668577985 | validation: 7.530296615489297]
	TIME [epoch: 8.29 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7656742380285575		[learning rate: 0.00046039]
		[batch 20/20] avg loss: 7.760712512127171		[learning rate: 0.00045985]
	Learning Rate: 0.000459849
	LOSS [training: 7.763193375077866 | validation: 7.558628882355489]
	TIME [epoch: 8.31 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.723359974456268		[learning rate: 0.00045931]
		[batch 20/20] avg loss: 7.809948137865126		[learning rate: 0.00045876]
	Learning Rate: 0.000458764
	LOSS [training: 7.766654056160695 | validation: 7.567058551134029]
	TIME [epoch: 8.28 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.873680033538898		[learning rate: 0.00045822]
		[batch 20/20] avg loss: 7.6428428582346495		[learning rate: 0.00045768]
	Learning Rate: 0.000457682
	LOSS [training: 7.758261445886774 | validation: 7.5145298326445635]
	TIME [epoch: 8.28 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.743821690894944		[learning rate: 0.00045714]
		[batch 20/20] avg loss: 7.760312184064098		[learning rate: 0.0004566]
	Learning Rate: 0.000456603
	LOSS [training: 7.752066937479522 | validation: 7.6425497625237]
	TIME [epoch: 8.28 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.784669125199239		[learning rate: 0.00045606]
		[batch 20/20] avg loss: 7.860415261645185		[learning rate: 0.00045553]
	Learning Rate: 0.000455526
	LOSS [training: 7.822542193422213 | validation: 7.637747176523817]
	TIME [epoch: 8.31 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.813543178915509		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 7.7812958961711844		[learning rate: 0.00045445]
	Learning Rate: 0.000454451
	LOSS [training: 7.797419537543348 | validation: 7.55773254408712]
	TIME [epoch: 8.29 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.803764664404388		[learning rate: 0.00045391]
		[batch 20/20] avg loss: 7.625028480667853		[learning rate: 0.00045338]
	Learning Rate: 0.000453379
	LOSS [training: 7.71439657253612 | validation: 7.431523290393619]
	TIME [epoch: 8.28 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.604388023169851		[learning rate: 0.00045284]
		[batch 20/20] avg loss: 7.650500462297782		[learning rate: 0.00045231]
	Learning Rate: 0.00045231
	LOSS [training: 7.627444242733816 | validation: 7.45787216120924]
	TIME [epoch: 8.28 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.694666065303539		[learning rate: 0.00045178]
		[batch 20/20] avg loss: 7.617190430755445		[learning rate: 0.00045124]
	Learning Rate: 0.000451243
	LOSS [training: 7.655928248029492 | validation: 7.458914281074432]
	TIME [epoch: 8.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.597921943501443		[learning rate: 0.00045071]
		[batch 20/20] avg loss: 7.665433252394919		[learning rate: 0.00045018]
	Learning Rate: 0.000450178
	LOSS [training: 7.631677597948181 | validation: 7.390681207315502]
	TIME [epoch: 8.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.537510455094538		[learning rate: 0.00044965]
		[batch 20/20] avg loss: 7.607806349939487		[learning rate: 0.00044912]
	Learning Rate: 0.000449116
	LOSS [training: 7.572658402517011 | validation: 7.42214793279522]
	TIME [epoch: 8.28 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.5806468588323455		[learning rate: 0.00044859]
		[batch 20/20] avg loss: 7.567738615577656		[learning rate: 0.00044806]
	Learning Rate: 0.000448057
	LOSS [training: 7.574192737205001 | validation: 7.41208571067877]
	TIME [epoch: 8.29 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.550565952395017		[learning rate: 0.00044753]
		[batch 20/20] avg loss: 7.6550225193910055		[learning rate: 0.000447]
	Learning Rate: 0.000447
	LOSS [training: 7.602794235893012 | validation: 7.375382784294696]
	TIME [epoch: 8.28 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.638173402001937		[learning rate: 0.00044647]
		[batch 20/20] avg loss: 7.48225921931053		[learning rate: 0.00044595]
	Learning Rate: 0.000445946
	LOSS [training: 7.560216310656235 | validation: 7.35532197580153]
	TIME [epoch: 8.31 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.515393498671395		[learning rate: 0.00044542]
		[batch 20/20] avg loss: 7.555478683913843		[learning rate: 0.00044489]
	Learning Rate: 0.000444894
	LOSS [training: 7.535436091292619 | validation: 7.267968403423467]
	TIME [epoch: 8.28 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.449800168965095		[learning rate: 0.00044437]
		[batch 20/20] avg loss: 7.452273080612343		[learning rate: 0.00044384]
	Learning Rate: 0.000443844
	LOSS [training: 7.451036624788719 | validation: 7.284744372788526]
	TIME [epoch: 8.29 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.431758123114667		[learning rate: 0.00044332]
		[batch 20/20] avg loss: 7.408473449081765		[learning rate: 0.0004428]
	Learning Rate: 0.000442797
	LOSS [training: 7.420115786098218 | validation: 7.2216695519702885]
	TIME [epoch: 8.29 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.445267996817535		[learning rate: 0.00044227]
		[batch 20/20] avg loss: 7.432740222643569		[learning rate: 0.00044175]
	Learning Rate: 0.000441753
	LOSS [training: 7.4390041097305515 | validation: 7.260830252402313]
	TIME [epoch: 8.31 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.430143996528753		[learning rate: 0.00044123]
		[batch 20/20] avg loss: 7.49030412728151		[learning rate: 0.00044071]
	Learning Rate: 0.000440711
	LOSS [training: 7.460224061905132 | validation: 7.276853381353613]
	TIME [epoch: 8.29 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.604780608705866		[learning rate: 0.00044019]
		[batch 20/20] avg loss: 7.249616815903627		[learning rate: 0.00043967]
	Learning Rate: 0.000439671
	LOSS [training: 7.427198712304748 | validation: 7.27404454535519]
	TIME [epoch: 8.28 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.32955476900918		[learning rate: 0.00043915]
		[batch 20/20] avg loss: 7.488606731566074		[learning rate: 0.00043863]
	Learning Rate: 0.000438634
	LOSS [training: 7.409080750287627 | validation: 7.317095770967511]
	TIME [epoch: 8.29 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.536709140507552		[learning rate: 0.00043812]
		[batch 20/20] avg loss: 7.402583403494466		[learning rate: 0.0004376]
	Learning Rate: 0.0004376
	LOSS [training: 7.46964627200101 | validation: 7.306314252085421]
	TIME [epoch: 8.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.502638451310124		[learning rate: 0.00043708]
		[batch 20/20] avg loss: 7.506492718776016		[learning rate: 0.00043657]
	Learning Rate: 0.000436567
	LOSS [training: 7.504565585043068 | validation: 7.307583671500628]
	TIME [epoch: 8.3 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.501358118798059		[learning rate: 0.00043605]
		[batch 20/20] avg loss: 7.4825525582549774		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 7.491955338526519 | validation: 7.312979521452788]
	TIME [epoch: 8.28 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.51852035957797		[learning rate: 0.00043502]
		[batch 20/20] avg loss: 7.44961185346757		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 7.48406610652277 | validation: 7.297016876680502]
	TIME [epoch: 8.28 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.47898043667214		[learning rate: 0.000434]
		[batch 20/20] avg loss: 7.452703469128069		[learning rate: 0.00043349]
	Learning Rate: 0.000433485
	LOSS [training: 7.465841952900102 | validation: 7.291346127105031]
	TIME [epoch: 8.28 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.451690823018204		[learning rate: 0.00043297]
		[batch 20/20] avg loss: 7.444540500895978		[learning rate: 0.00043246]
	Learning Rate: 0.000432463
	LOSS [training: 7.44811566195709 | validation: 7.292049316334184]
	TIME [epoch: 8.31 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.465724571434014		[learning rate: 0.00043195]
		[batch 20/20] avg loss: 7.492538569166372		[learning rate: 0.00043144]
	Learning Rate: 0.000431443
	LOSS [training: 7.479131570300194 | validation: 7.288303936110894]
	TIME [epoch: 8.29 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.583823695112315		[learning rate: 0.00043093]
		[batch 20/20] avg loss: 7.357804459086258		[learning rate: 0.00043042]
	Learning Rate: 0.000430425
	LOSS [training: 7.470814077099286 | validation: 7.287173232063068]
	TIME [epoch: 8.29 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.398763470685267		[learning rate: 0.00042992]
		[batch 20/20] avg loss: 7.477200637880507		[learning rate: 0.00042941]
	Learning Rate: 0.00042941
	LOSS [training: 7.437982054282888 | validation: 7.354631680133817]
	TIME [epoch: 8.28 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.42824954310819		[learning rate: 0.0004289]
		[batch 20/20] avg loss: 7.443753923511375		[learning rate: 0.0004284]
	Learning Rate: 0.000428397
	LOSS [training: 7.436001733309783 | validation: 7.251986114331982]
	TIME [epoch: 8.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.343023782295324		[learning rate: 0.00042789]
		[batch 20/20] avg loss: 7.477890141391493		[learning rate: 0.00042739]
	Learning Rate: 0.000427386
	LOSS [training: 7.410456961843408 | validation: 7.319443235765574]
	TIME [epoch: 8.29 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.494734078986698		[learning rate: 0.00042688]
		[batch 20/20] avg loss: 7.336834442852068		[learning rate: 0.00042638]
	Learning Rate: 0.000426378
	LOSS [training: 7.4157842609193825 | validation: 7.289391674656202]
	TIME [epoch: 8.29 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.404447101152516		[learning rate: 0.00042587]
		[batch 20/20] avg loss: 7.567698398146353		[learning rate: 0.00042537]
	Learning Rate: 0.000425372
	LOSS [training: 7.486072749649435 | validation: 7.381407866930032]
	TIME [epoch: 8.29 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.576501412044415		[learning rate: 0.00042487]
		[batch 20/20] avg loss: 7.541014524754215		[learning rate: 0.00042437]
	Learning Rate: 0.000424369
	LOSS [training: 7.558757968399315 | validation: 7.399825447016683]
	TIME [epoch: 8.29 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.542090083277129		[learning rate: 0.00042387]
		[batch 20/20] avg loss: 7.606118702168061		[learning rate: 0.00042337]
	Learning Rate: 0.000423368
	LOSS [training: 7.574104392722596 | validation: 7.406493817237371]
	TIME [epoch: 8.31 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.682508098442159		[learning rate: 0.00042287]
		[batch 20/20] avg loss: 7.542573923656525		[learning rate: 0.00042237]
	Learning Rate: 0.000422369
	LOSS [training: 7.6125410110493394 | validation: 7.433944540512333]
	TIME [epoch: 8.29 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.709972141249862		[learning rate: 0.00042187]
		[batch 20/20] avg loss: 7.4522573894647675		[learning rate: 0.00042137]
	Learning Rate: 0.000421373
	LOSS [training: 7.581114765357316 | validation: 7.3407256244740005]
	TIME [epoch: 8.28 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.482621531804073		[learning rate: 0.00042088]
		[batch 20/20] avg loss: 7.563801094532789		[learning rate: 0.00042038]
	Learning Rate: 0.000420379
	LOSS [training: 7.52321131316843 | validation: 7.429763707216226]
	TIME [epoch: 8.28 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.603145595778895		[learning rate: 0.00041988]
		[batch 20/20] avg loss: 7.67760750904999		[learning rate: 0.00041939]
	Learning Rate: 0.000419387
	LOSS [training: 7.640376552414445 | validation: 7.463488886117451]
	TIME [epoch: 8.31 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.710061372270529		[learning rate: 0.00041889]
		[batch 20/20] avg loss: 7.657056961214844		[learning rate: 0.0004184]
	Learning Rate: 0.000418398
	LOSS [training: 7.683559166742685 | validation: 7.458886272802992]
	TIME [epoch: 8.29 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.592152411365895		[learning rate: 0.0004179]
		[batch 20/20] avg loss: 7.7044179992533275		[learning rate: 0.00041741]
	Learning Rate: 0.000417411
	LOSS [training: 7.648285205309611 | validation: 7.479657740150405]
	TIME [epoch: 8.28 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.546165062348652		[learning rate: 0.00041692]
		[batch 20/20] avg loss: 7.778586527258275		[learning rate: 0.00041643]
	Learning Rate: 0.000416427
	LOSS [training: 7.662375794803464 | validation: 7.523109911226252]
	TIME [epoch: 8.28 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.741069871518365		[learning rate: 0.00041594]
		[batch 20/20] avg loss: 7.690361556899424		[learning rate: 0.00041544]
	Learning Rate: 0.000415444
	LOSS [training: 7.715715714208895 | validation: 7.527828489773385]
	TIME [epoch: 8.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.829961320762749		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 7.593520841180525		[learning rate: 0.00041446]
	Learning Rate: 0.000414464
	LOSS [training: 7.7117410809716365 | validation: 7.5409539007256505]
	TIME [epoch: 8.29 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.755314988767691		[learning rate: 0.00041398]
		[batch 20/20] avg loss: 7.744953038944184		[learning rate: 0.00041349]
	Learning Rate: 0.000413487
	LOSS [training: 7.750134013855937 | validation: 7.607956185260007]
	TIME [epoch: 8.29 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.788422293962702		[learning rate: 0.000413]
		[batch 20/20] avg loss: 7.750034368888426		[learning rate: 0.00041251]
	Learning Rate: 0.000412511
	LOSS [training: 7.7692283314255635 | validation: 7.573768707683666]
	TIME [epoch: 8.29 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.838045538842745		[learning rate: 0.00041202]
		[batch 20/20] avg loss: 7.6779231611004946		[learning rate: 0.00041154]
	Learning Rate: 0.000411538
	LOSS [training: 7.75798434997162 | validation: 7.539948084113927]
	TIME [epoch: 8.28 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.777397717124137		[learning rate: 0.00041105]
		[batch 20/20] avg loss: 7.816688313990687		[learning rate: 0.00041057]
	Learning Rate: 0.000410568
	LOSS [training: 7.797043015557415 | validation: 7.609849113075372]
	TIME [epoch: 8.31 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.889552626618723		[learning rate: 0.00041008]
		[batch 20/20] avg loss: 7.636822399951829		[learning rate: 0.0004096]
	Learning Rate: 0.000409599
	LOSS [training: 7.763187513285276 | validation: 7.524926099125359]
	TIME [epoch: 8.29 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.74602454276274		[learning rate: 0.00040912]
		[batch 20/20] avg loss: 7.607501753915523		[learning rate: 0.00040863]
	Learning Rate: 0.000408633
	LOSS [training: 7.67676314833913 | validation: 7.505310085062513]
	TIME [epoch: 8.29 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.67515257146673		[learning rate: 0.00040815]
		[batch 20/20] avg loss: 7.549123654573931		[learning rate: 0.00040767]
	Learning Rate: 0.000407669
	LOSS [training: 7.612138113020329 | validation: 7.4705313214117695]
	TIME [epoch: 8.28 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.634460015651653		[learning rate: 0.00040719]
		[batch 20/20] avg loss: 7.707574352016961		[learning rate: 0.00040671]
	Learning Rate: 0.000406707
	LOSS [training: 7.671017183834307 | validation: 7.533961705915374]
	TIME [epoch: 8.31 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.658513829259654		[learning rate: 0.00040623]
		[batch 20/20] avg loss: 7.653306209101741		[learning rate: 0.00040575]
	Learning Rate: 0.000405748
	LOSS [training: 7.655910019180699 | validation: 7.515120854286295]
	TIME [epoch: 8.29 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.598821190986492		[learning rate: 0.00040527]
		[batch 20/20] avg loss: 7.637071550985039		[learning rate: 0.00040479]
	Learning Rate: 0.000404791
	LOSS [training: 7.6179463709857655 | validation: 7.481440878354456]
	TIME [epoch: 8.28 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.53784684920092		[learning rate: 0.00040431]
		[batch 20/20] avg loss: 7.683635632277517		[learning rate: 0.00040384]
	Learning Rate: 0.000403836
	LOSS [training: 7.610741240739219 | validation: 7.481397521762925]
	TIME [epoch: 8.28 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7796919335418435		[learning rate: 0.00040336]
		[batch 20/20] avg loss: 7.631041320100673		[learning rate: 0.00040288]
	Learning Rate: 0.000402883
	LOSS [training: 7.705366626821258 | validation: 7.489080333587953]
	TIME [epoch: 8.3 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.771335009308105		[learning rate: 0.00040241]
		[batch 20/20] avg loss: 7.52311565944103		[learning rate: 0.00040193]
	Learning Rate: 0.000401933
	LOSS [training: 7.647225334374568 | validation: 7.4298384146271035]
	TIME [epoch: 8.28 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.609017772694571		[learning rate: 0.00040146]
		[batch 20/20] avg loss: 7.620373976987682		[learning rate: 0.00040099]
	Learning Rate: 0.000400985
	LOSS [training: 7.614695874841125 | validation: 7.432346538740893]
	TIME [epoch: 8.28 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.633369219610378		[learning rate: 0.00040051]
		[batch 20/20] avg loss: 7.559108863986465		[learning rate: 0.00040004]
	Learning Rate: 0.000400039
	LOSS [training: 7.596239041798421 | validation: 7.4355082564953054]
	TIME [epoch: 8.28 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.641841804180925		[learning rate: 0.00039957]
		[batch 20/20] avg loss: 7.47696666974314		[learning rate: 0.0003991]
	Learning Rate: 0.000399096
	LOSS [training: 7.559404236962033 | validation: 7.41573013167506]
	TIME [epoch: 8.29 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.560668296119568		[learning rate: 0.00039862]
		[batch 20/20] avg loss: 7.581931278948476		[learning rate: 0.00039815]
	Learning Rate: 0.000398154
	LOSS [training: 7.571299787534022 | validation: 7.42532835948352]
	TIME [epoch: 8.31 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.599115049755881		[learning rate: 0.00039768]
		[batch 20/20] avg loss: 7.604347008622699		[learning rate: 0.00039721]
	Learning Rate: 0.000397215
	LOSS [training: 7.60173102918929 | validation: 7.490666210240427]
	TIME [epoch: 8.29 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.664309313755398		[learning rate: 0.00039675]
		[batch 20/20] avg loss: 7.670764622477037		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 7.667536968116218 | validation: 7.524037492945727]
	TIME [epoch: 8.28 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6175255431652955		[learning rate: 0.00039581]
		[batch 20/20] avg loss: 7.858254778420187		[learning rate: 0.00039534]
	Learning Rate: 0.000395343
	LOSS [training: 7.737890160792742 | validation: 7.499470536114327]
	TIME [epoch: 8.28 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.731531798538123		[learning rate: 0.00039488]
		[batch 20/20] avg loss: 7.808259752921299		[learning rate: 0.00039441]
	Learning Rate: 0.000394411
	LOSS [training: 7.7698957757297125 | validation: 7.522144613487293]
	TIME [epoch: 8.3 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.771656151093052		[learning rate: 0.00039395]
		[batch 20/20] avg loss: 7.860355553989824		[learning rate: 0.00039348]
	Learning Rate: 0.00039348
	LOSS [training: 7.816005852541437 | validation: 7.492203558174442]
	TIME [epoch: 8.29 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.752524830078746		[learning rate: 0.00039302]
		[batch 20/20] avg loss: 7.716785196872162		[learning rate: 0.00039255]
	Learning Rate: 0.000392552
	LOSS [training: 7.734655013475452 | validation: 7.433619660450987]
	TIME [epoch: 8.29 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.739814601596433		[learning rate: 0.00039209]
		[batch 20/20] avg loss: 7.756243473894666		[learning rate: 0.00039163]
	Learning Rate: 0.000391626
	LOSS [training: 7.748029037745549 | validation: 7.541426693922957]
	TIME [epoch: 8.29 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.812363783202748		[learning rate: 0.00039116]
		[batch 20/20] avg loss: 7.967816408063105		[learning rate: 0.0003907]
	Learning Rate: 0.000390702
	LOSS [training: 7.890090095632925 | validation: 7.530199935005578]
	TIME [epoch: 8.28 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.688000807244002		[learning rate: 0.00039024]
		[batch 20/20] avg loss: 7.537079121565597		[learning rate: 0.00038978]
	Learning Rate: 0.000389781
	LOSS [training: 7.6125399644048 | validation: 6.98177946138715]
	TIME [epoch: 8.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.3931400288812625		[learning rate: 0.00038932]
		[batch 20/20] avg loss: 7.178781155395944		[learning rate: 0.00038886]
	Learning Rate: 0.000388861
	LOSS [training: 7.285960592138602 | validation: 6.668925997468381]
	TIME [epoch: 8.28 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.795341464250406		[learning rate: 0.0003884]
		[batch 20/20] avg loss: 6.586917324100976		[learning rate: 0.00038794]
	Learning Rate: 0.000387944
	LOSS [training: 6.6911293941756895 | validation: 6.111643561422024]
	TIME [epoch: 8.29 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.276241526987479		[learning rate: 0.00038749]
		[batch 20/20] avg loss: 6.300720358460572		[learning rate: 0.00038703]
	Learning Rate: 0.000387029
	LOSS [training: 6.288480942724026 | validation: 6.075795189888999]
	TIME [epoch: 8.28 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.199592994787722		[learning rate: 0.00038657]
		[batch 20/20] avg loss: 6.0714151517842		[learning rate: 0.00038612]
	Learning Rate: 0.000386116
	LOSS [training: 6.135504073285961 | validation: 5.836101034042034]
	TIME [epoch: 8.31 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.936698282052071		[learning rate: 0.00038566]
		[batch 20/20] avg loss: 5.959490895021844		[learning rate: 0.00038521]
	Learning Rate: 0.000385205
	LOSS [training: 5.948094588536957 | validation: 5.816482135318762]
	TIME [epoch: 8.28 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.803095031143119		[learning rate: 0.00038475]
		[batch 20/20] avg loss: 5.960017173565204		[learning rate: 0.0003843]
	Learning Rate: 0.000384297
	LOSS [training: 5.881556102354162 | validation: 5.7312316838223545]
	TIME [epoch: 8.29 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.912858025746215		[learning rate: 0.00038384]
		[batch 20/20] avg loss: 5.822180898955944		[learning rate: 0.00038339]
	Learning Rate: 0.00038339
	LOSS [training: 5.867519462351078 | validation: 5.707841536846417]
	TIME [epoch: 8.28 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.612538880720648		[learning rate: 0.00038294]
		[batch 20/20] avg loss: 6.122293032648328		[learning rate: 0.00038249]
	Learning Rate: 0.000382486
	LOSS [training: 5.867415956684488 | validation: 5.731199607030967]
	TIME [epoch: 8.3 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.791811250058286		[learning rate: 0.00038203]
		[batch 20/20] avg loss: 5.965196472667847		[learning rate: 0.00038158]
	Learning Rate: 0.000381584
	LOSS [training: 5.878503861363067 | validation: 5.7569573929693805]
	TIME [epoch: 8.29 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.965020381751741		[learning rate: 0.00038113]
		[batch 20/20] avg loss: 5.731244789751976		[learning rate: 0.00038068]
	Learning Rate: 0.000380684
	LOSS [training: 5.848132585751858 | validation: 5.761399341641857]
	TIME [epoch: 8.29 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.76120135310254		[learning rate: 0.00038023]
		[batch 20/20] avg loss: 5.885546882389404		[learning rate: 0.00037979]
	Learning Rate: 0.000379786
	LOSS [training: 5.823374117745972 | validation: 5.772944770497804]
	TIME [epoch: 8.29 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.890127481729473		[learning rate: 0.00037934]
		[batch 20/20] avg loss: 5.742743767749674		[learning rate: 0.00037889]
	Learning Rate: 0.00037889
	LOSS [training: 5.816435624739573 | validation: 5.72109383199655]
	TIME [epoch: 8.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.708969057286171		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 5.910164636524782		[learning rate: 0.000378]
	Learning Rate: 0.000377996
	LOSS [training: 5.809566846905475 | validation: 5.702951584555011]
	TIME [epoch: 8.31 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.002511588405876		[learning rate: 0.00037755]
		[batch 20/20] avg loss: 5.6891809379071026		[learning rate: 0.0003771]
	Learning Rate: 0.000377104
	LOSS [training: 5.8458462631564885 | validation: 5.726977201665051]
	TIME [epoch: 8.28 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.92773586397765		[learning rate: 0.00037666]
		[batch 20/20] avg loss: 5.77450696067571		[learning rate: 0.00037621]
	Learning Rate: 0.000376215
	LOSS [training: 5.85112141232668 | validation: 5.73822792024612]
	TIME [epoch: 8.29 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.972720249712836		[learning rate: 0.00037577]
		[batch 20/20] avg loss: 5.751207900988693		[learning rate: 0.00037533]
	Learning Rate: 0.000375327
	LOSS [training: 5.861964075350764 | validation: 5.7274108708628155]
	TIME [epoch: 8.29 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.737737797524687		[learning rate: 0.00037488]
		[batch 20/20] avg loss: 5.998370027177235		[learning rate: 0.00037444]
	Learning Rate: 0.000374442
	LOSS [training: 5.868053912350961 | validation: 5.678971590982411]
	TIME [epoch: 8.31 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.8513159749365995		[learning rate: 0.000374]
		[batch 20/20] avg loss: 5.869495579782955		[learning rate: 0.00037356]
	Learning Rate: 0.000373559
	LOSS [training: 5.860405777359776 | validation: 5.643309106387234]
	TIME [epoch: 8.28 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.006081853976336		[learning rate: 0.00037312]
		[batch 20/20] avg loss: 5.709264691518879		[learning rate: 0.00037268]
	Learning Rate: 0.000372678
	LOSS [training: 5.857673272747607 | validation: 5.621432304027066]
	TIME [epoch: 8.29 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.8525685406458985		[learning rate: 0.00037224]
		[batch 20/20] avg loss: 5.906371576385003		[learning rate: 0.0003718]
	Learning Rate: 0.000371799
	LOSS [training: 5.87947005851545 | validation: 5.622344967774684]
	TIME [epoch: 8.28 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.933486640906394		[learning rate: 0.00037136]
		[batch 20/20] avg loss: 5.680952094561521		[learning rate: 0.00037092]
	Learning Rate: 0.000370922
	LOSS [training: 5.807219367733959 | validation: 5.642567251951648]
	TIME [epoch: 8.29 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.689619628496496		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 5.845755033502355		[learning rate: 0.00037005]
	Learning Rate: 0.000370047
	LOSS [training: 5.767687330999426 | validation: 5.554177863851934]
	TIME [epoch: 8.29 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.819872601224483		[learning rate: 0.00036961]
		[batch 20/20] avg loss: 5.627813293308598		[learning rate: 0.00036917]
	Learning Rate: 0.000369174
	LOSS [training: 5.72384294726654 | validation: 5.562115067674265]
	TIME [epoch: 8.28 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.727161075147391		[learning rate: 0.00036874]
		[batch 20/20] avg loss: 5.667044970866302		[learning rate: 0.0003683]
	Learning Rate: 0.000368303
	LOSS [training: 5.6971030230068465 | validation: 5.522489716552707]
	TIME [epoch: 8.29 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.67635235960036		[learning rate: 0.00036787]
		[batch 20/20] avg loss: 5.648131535957823		[learning rate: 0.00036743]
	Learning Rate: 0.000367434
	LOSS [training: 5.6622419477790915 | validation: 5.465582262429914]
	TIME [epoch: 8.28 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.719964456416838		[learning rate: 0.000367]
		[batch 20/20] avg loss: 5.4878743919080835		[learning rate: 0.00036657]
	Learning Rate: 0.000366567
	LOSS [training: 5.60391942416246 | validation: 5.457971868509611]
	TIME [epoch: 8.31 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.475578801252658		[learning rate: 0.00036613]
		[batch 20/20] avg loss: 5.717639295843602		[learning rate: 0.0003657]
	Learning Rate: 0.000365703
	LOSS [training: 5.59660904854813 | validation: 5.461071288346425]
	TIME [epoch: 8.29 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.771531626033175		[learning rate: 0.00036527]
		[batch 20/20] avg loss: 5.382426232480739		[learning rate: 0.00036484]
	Learning Rate: 0.00036484
	LOSS [training: 5.576978929256957 | validation: 5.440244437970195]
	TIME [epoch: 8.28 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.613990010220382		[learning rate: 0.00036441]
		[batch 20/20] avg loss: 5.531475553177498		[learning rate: 0.00036398]
	Learning Rate: 0.000363979
	LOSS [training: 5.5727327816989405 | validation: 5.418989472984645]
	TIME [epoch: 8.28 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.785013194911627		[learning rate: 0.00036355]
		[batch 20/20] avg loss: 5.343955031819356		[learning rate: 0.00036312]
	Learning Rate: 0.000363121
	LOSS [training: 5.564484113365491 | validation: 5.413625695300055]
	TIME [epoch: 8.31 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.709624854736566		[learning rate: 0.00036269]
		[batch 20/20] avg loss: 5.4246072608468925		[learning rate: 0.00036226]
	Learning Rate: 0.000362264
	LOSS [training: 5.567116057791728 | validation: 5.430324411323852]
	TIME [epoch: 8.28 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.5631155007120485		[learning rate: 0.00036184]
		[batch 20/20] avg loss: 5.579387754800768		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 5.571251627756409 | validation: 5.4259599163501]
	TIME [epoch: 8.28 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.563344893304079		[learning rate: 0.00036098]
		[batch 20/20] avg loss: 5.569733854568419		[learning rate: 0.00036056]
	Learning Rate: 0.000360557
	LOSS [training: 5.56653937393625 | validation: 5.435883661923043]
	TIME [epoch: 8.27 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.580779677221554		[learning rate: 0.00036013]
		[batch 20/20] avg loss: 5.6045006721418185		[learning rate: 0.00035971]
	Learning Rate: 0.000359707
	LOSS [training: 5.592640174681686 | validation: 5.488587089982181]
	TIME [epoch: 8.3 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.659612209253226		[learning rate: 0.00035928]
		[batch 20/20] avg loss: 5.568630242891826		[learning rate: 0.00035886]
	Learning Rate: 0.000358858
	LOSS [training: 5.614121226072525 | validation: 5.485714231023124]
	TIME [epoch: 8.29 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.6849474335076025		[learning rate: 0.00035843]
		[batch 20/20] avg loss: 5.541357475870179		[learning rate: 0.00035801]
	Learning Rate: 0.000358012
	LOSS [training: 5.613152454688892 | validation: 5.484662867401186]
	TIME [epoch: 8.28 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.609387502880787		[learning rate: 0.00035759]
		[batch 20/20] avg loss: 5.610844501673444		[learning rate: 0.00035717]
	Learning Rate: 0.000357167
	LOSS [training: 5.610116002277115 | validation: 5.486431492594942]
	TIME [epoch: 8.28 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.696837413514774		[learning rate: 0.00035675]
		[batch 20/20] avg loss: 5.541676246758578		[learning rate: 0.00035632]
	Learning Rate: 0.000356325
	LOSS [training: 5.619256830136676 | validation: 5.521850237707451]
	TIME [epoch: 8.28 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.544960574435188		[learning rate: 0.0003559]
		[batch 20/20] avg loss: 5.731533418757642		[learning rate: 0.00035548]
	Learning Rate: 0.000355484
	LOSS [training: 5.638246996596414 | validation: 5.513159340806498]
	TIME [epoch: 8.31 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.503038355673182		[learning rate: 0.00035506]
		[batch 20/20] avg loss: 5.759957465194605		[learning rate: 0.00035465]
	Learning Rate: 0.000354646
	LOSS [training: 5.631497910433894 | validation: 5.519895330117322]
	TIME [epoch: 8.28 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.540249194827771		[learning rate: 0.00035423]
		[batch 20/20] avg loss: 5.750680994773516		[learning rate: 0.00035381]
	Learning Rate: 0.000353809
	LOSS [training: 5.6454650948006435 | validation: 5.512296471141244]
	TIME [epoch: 8.28 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.592223541338655		[learning rate: 0.00035339]
		[batch 20/20] avg loss: 5.6382520080950345		[learning rate: 0.00035297]
	Learning Rate: 0.000352975
	LOSS [training: 5.615237774716844 | validation: 5.449195187875102]
	TIME [epoch: 8.29 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.334183067700083		[learning rate: 0.00035256]
		[batch 20/20] avg loss: 5.838073934912613		[learning rate: 0.00035214]
	Learning Rate: 0.000352142
	LOSS [training: 5.586128501306347 | validation: 5.440646545450127]
	TIME [epoch: 8.31 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.3555356909782255		[learning rate: 0.00035173]
		[batch 20/20] avg loss: 5.82976061685343		[learning rate: 0.00035131]
	Learning Rate: 0.000351311
	LOSS [training: 5.592648153915827 | validation: 5.446854016603606]
	TIME [epoch: 8.29 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.616801888771355		[learning rate: 0.0003509]
		[batch 20/20] avg loss: 5.632790177616421		[learning rate: 0.00035048]
	Learning Rate: 0.000350483
	LOSS [training: 5.6247960331938875 | validation: 5.450139199840445]
	TIME [epoch: 8.29 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.5425070590643495		[learning rate: 0.00035007]
		[batch 20/20] avg loss: 5.655079659174676		[learning rate: 0.00034966]
	Learning Rate: 0.000349656
	LOSS [training: 5.598793359119513 | validation: 5.4474686116873565]
	TIME [epoch: 8.28 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.598478914373965		[learning rate: 0.00034924]
		[batch 20/20] avg loss: 5.602069499896858		[learning rate: 0.00034883]
	Learning Rate: 0.000348831
	LOSS [training: 5.600274207135412 | validation: 5.4569462345223485]
	TIME [epoch: 8.3 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.693461645576593		[learning rate: 0.00034842]
		[batch 20/20] avg loss: 5.52111735271585		[learning rate: 0.00034801]
	Learning Rate: 0.000348008
	LOSS [training: 5.607289499146221 | validation: 5.488767492837096]
	TIME [epoch: 8.29 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.571070636475977		[learning rate: 0.0003476]
		[batch 20/20] avg loss: 5.629503713326768		[learning rate: 0.00034719]
	Learning Rate: 0.000347187
	LOSS [training: 5.600287174901373 | validation: 5.43967303092765]
	TIME [epoch: 8.28 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.788775291650197		[learning rate: 0.00034678]
		[batch 20/20] avg loss: 5.349108938721438		[learning rate: 0.00034637]
	Learning Rate: 0.000346369
	LOSS [training: 5.568942115185817 | validation: 5.4193447464535]
	TIME [epoch: 8.29 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.498330357131592		[learning rate: 0.00034596]
		[batch 20/20] avg loss: 5.618231002115745		[learning rate: 0.00034555]
	Learning Rate: 0.000345552
	LOSS [training: 5.5582806796236675 | validation: 5.414837261497345]
	TIME [epoch: 8.29 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.591755811209689		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 5.531359306625457		[learning rate: 0.00034474]
	Learning Rate: 0.000344736
	LOSS [training: 5.561557558917572 | validation: 5.4031822146278]
	TIME [epoch: 8.31 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.479471977740106		[learning rate: 0.00034433]
		[batch 20/20] avg loss: 5.650779034798193		[learning rate: 0.00034392]
	Learning Rate: 0.000343923
	LOSS [training: 5.565125506269149 | validation: 5.4328727339898535]
	TIME [epoch: 8.28 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.362148074263642		[learning rate: 0.00034352]
		[batch 20/20] avg loss: 5.868239149935034		[learning rate: 0.00034311]
	Learning Rate: 0.000343112
	LOSS [training: 5.615193612099338 | validation: 5.482512097656719]
	TIME [epoch: 8.3 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.869561956220059		[learning rate: 0.00034271]
		[batch 20/20] avg loss: 5.438018230706763		[learning rate: 0.0003423]
	Learning Rate: 0.000342303
	LOSS [training: 5.653790093463409 | validation: 5.492910619731555]
	TIME [epoch: 8.27 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.824752667285565		[learning rate: 0.0003419]
		[batch 20/20] avg loss: 5.399316663243131		[learning rate: 0.0003415]
	Learning Rate: 0.000341495
	LOSS [training: 5.612034665264349 | validation: 5.449555935714478]
	TIME [epoch: 8.31 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.74405095426919		[learning rate: 0.00034109]
		[batch 20/20] avg loss: 5.541906042448393		[learning rate: 0.00034069]
	Learning Rate: 0.00034069
	LOSS [training: 5.642978498358792 | validation: 5.479743722530224]
	TIME [epoch: 8.28 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.82057916305091		[learning rate: 0.00034029]
		[batch 20/20] avg loss: 5.520749262646466		[learning rate: 0.00033989]
	Learning Rate: 0.000339886
	LOSS [training: 5.670664212848688 | validation: 5.479011859961682]
	TIME [epoch: 8.28 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.604953559664177		[learning rate: 0.00033948]
		[batch 20/20] avg loss: 5.731088936804431		[learning rate: 0.00033908]
	Learning Rate: 0.000339084
	LOSS [training: 5.6680212482343055 | validation: 5.4716092289328575]
	TIME [epoch: 8.29 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.757813763094799		[learning rate: 0.00033868]
		[batch 20/20] avg loss: 5.706255565815118		[learning rate: 0.00033828]
	Learning Rate: 0.000338284
	LOSS [training: 5.732034664454959 | validation: 5.4690242489629295]
	TIME [epoch: 8.29 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.883686129870285		[learning rate: 0.00033789]
		[batch 20/20] avg loss: 5.580263782204864		[learning rate: 0.00033749]
	Learning Rate: 0.000337487
	LOSS [training: 5.731974956037574 | validation: 5.464770788025833]
	TIME [epoch: 8.29 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.98611986460288		[learning rate: 0.00033709]
		[batch 20/20] avg loss: 5.45981642791425		[learning rate: 0.00033669]
	Learning Rate: 0.00033669
	LOSS [training: 5.722968146258565 | validation: 5.460011329996887]
	TIME [epoch: 8.3 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.72190060497949		[learning rate: 0.00033629]
		[batch 20/20] avg loss: 5.610788085850592		[learning rate: 0.0003359]
	Learning Rate: 0.000335896
	LOSS [training: 5.66634434541504 | validation: 5.452792984397474]
	TIME [epoch: 8.29 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.687969355213322		[learning rate: 0.0003355]
		[batch 20/20] avg loss: 5.664266186290084		[learning rate: 0.0003351]
	Learning Rate: 0.000335104
	LOSS [training: 5.676117770751702 | validation: 5.469006314889901]
	TIME [epoch: 8.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.740594253452261		[learning rate: 0.00033471]
		[batch 20/20] avg loss: 5.680521897491942		[learning rate: 0.00033431]
	Learning Rate: 0.000334313
	LOSS [training: 5.710558075472102 | validation: 5.52321515667409]
	TIME [epoch: 8.31 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.804572996882982		[learning rate: 0.00033392]
		[batch 20/20] avg loss: 5.629419672236274		[learning rate: 0.00033352]
	Learning Rate: 0.000333525
	LOSS [training: 5.716996334559629 | validation: 5.522989312739085]
	TIME [epoch: 8.29 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.897982054189498		[learning rate: 0.00033313]
		[batch 20/20] avg loss: 5.3653259512128155		[learning rate: 0.00033274]
	Learning Rate: 0.000332738
	LOSS [training: 5.631654002701156 | validation: 5.486993665628811]
	TIME [epoch: 8.29 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.71957061382456		[learning rate: 0.00033235]
		[batch 20/20] avg loss: 5.468690769003858		[learning rate: 0.00033195]
	Learning Rate: 0.000331953
	LOSS [training: 5.594130691414209 | validation: 5.453327910400954]
	TIME [epoch: 8.29 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.4061675082905705		[learning rate: 0.00033156]
		[batch 20/20] avg loss: 5.756766498016912		[learning rate: 0.00033117]
	Learning Rate: 0.00033117
	LOSS [training: 5.58146700315374 | validation: 5.516282964031129]
	TIME [epoch: 8.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.52947336278927		[learning rate: 0.00033078]
		[batch 20/20] avg loss: 5.728427661579864		[learning rate: 0.00033039]
	Learning Rate: 0.000330389
	LOSS [training: 5.628950512184568 | validation: 5.499182890403139]
	TIME [epoch: 8.29 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.347521462911761		[learning rate: 0.00033]
		[batch 20/20] avg loss: 5.943679271174082		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 5.645600367042922 | validation: 5.535351095376007]
	TIME [epoch: 8.29 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.5251950972468		[learning rate: 0.00032922]
		[batch 20/20] avg loss: 5.7258141471262185		[learning rate: 0.00032883]
	Learning Rate: 0.000328832
	LOSS [training: 5.625504622186509 | validation: 5.540856303630241]
	TIME [epoch: 8.28 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.46859872009496		[learning rate: 0.00032844]
		[batch 20/20] avg loss: 5.75626634980024		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 5.612432534947599 | validation: 5.473808358685273]
	TIME [epoch: 8.29 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.467982092043746		[learning rate: 0.00032767]
		[batch 20/20] avg loss: 5.714012665964979		[learning rate: 0.00032728]
	Learning Rate: 0.000327283
	LOSS [training: 5.590997379004362 | validation: 5.528044785203208]
	TIME [epoch: 8.32 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.620041356838985		[learning rate: 0.0003269]
		[batch 20/20] avg loss: 5.69580278853576		[learning rate: 0.00032651]
	Learning Rate: 0.000326511
	LOSS [training: 5.657922072687372 | validation: 5.566975000536644]
	TIME [epoch: 8.29 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.812821867017803		[learning rate: 0.00032613]
		[batch 20/20] avg loss: 5.566434459610702		[learning rate: 0.00032574]
	Learning Rate: 0.00032574
	LOSS [training: 5.689628163314252 | validation: 5.591641308865039]
	TIME [epoch: 8.29 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.692642167765624		[learning rate: 0.00032536]
		[batch 20/20] avg loss: 5.670008902707561		[learning rate: 0.00032497]
	Learning Rate: 0.000324972
	LOSS [training: 5.681325535236593 | validation: 5.559458881851663]
	TIME [epoch: 8.29 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.80275222166101		[learning rate: 0.00032459]
		[batch 20/20] avg loss: 5.633570407918342		[learning rate: 0.00032421]
	Learning Rate: 0.000324206
	LOSS [training: 5.718161314789676 | validation: 5.628684420957981]
	TIME [epoch: 8.31 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.799729712805468		[learning rate: 0.00032382]
		[batch 20/20] avg loss: 5.637698298983961		[learning rate: 0.00032344]
	Learning Rate: 0.000323441
	LOSS [training: 5.7187140058947135 | validation: 5.609992663188225]
	TIME [epoch: 8.29 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.717112064676737		[learning rate: 0.00032306]
		[batch 20/20] avg loss: 5.796213679931446		[learning rate: 0.00032268]
	Learning Rate: 0.000322678
	LOSS [training: 5.756662872304091 | validation: 5.611573096837951]
	TIME [epoch: 8.29 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.541685230208552		[learning rate: 0.0003223]
		[batch 20/20] avg loss: 5.9013501885241055		[learning rate: 0.00032192]
	Learning Rate: 0.000321917
	LOSS [training: 5.721517709366329 | validation: 5.598216099409054]
	TIME [epoch: 8.28 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.799007874233043		[learning rate: 0.00032154]
		[batch 20/20] avg loss: 5.539452980123648		[learning rate: 0.00032116]
	Learning Rate: 0.000321157
	LOSS [training: 5.669230427178347 | validation: 5.485191768767167]
	TIME [epoch: 8.3 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.538898931700716		[learning rate: 0.00032078]
		[batch 20/20] avg loss: 5.712343744980348		[learning rate: 0.0003204]
	Learning Rate: 0.0003204
	LOSS [training: 5.625621338340533 | validation: 5.500564770427103]
	TIME [epoch: 8.29 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.535549119905509		[learning rate: 0.00032002]
		[batch 20/20] avg loss: 5.6557504236038625		[learning rate: 0.00031964]
	Learning Rate: 0.000319644
	LOSS [training: 5.595649771754686 | validation: 5.457232066225509]
	TIME [epoch: 8.29 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.584541435822187		[learning rate: 0.00031927]
		[batch 20/20] avg loss: 5.653771660429238		[learning rate: 0.00031889]
	Learning Rate: 0.00031889
	LOSS [training: 5.619156548125713 | validation: 5.5377393535814825]
	TIME [epoch: 8.28 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.61749010157587		[learning rate: 0.00031851]
		[batch 20/20] avg loss: 5.627530524671004		[learning rate: 0.00031814]
	Learning Rate: 0.000318138
	LOSS [training: 5.622510313123437 | validation: 5.502507464873407]
	TIME [epoch: 8.28 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.826833639222075		[learning rate: 0.00031776]
		[batch 20/20] avg loss: 5.441035986224244		[learning rate: 0.00031739]
	Learning Rate: 0.000317387
	LOSS [training: 5.63393481272316 | validation: 5.548729348959389]
	TIME [epoch: 8.3 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.761855264567474		[learning rate: 0.00031701]
		[batch 20/20] avg loss: 5.641651420255139		[learning rate: 0.00031664]
	Learning Rate: 0.000316639
	LOSS [training: 5.7017533424113065 | validation: 5.617786609766607]
	TIME [epoch: 8.29 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.049404419595103		[learning rate: 0.00031627]
		[batch 20/20] avg loss: 5.506160660980472		[learning rate: 0.00031589]
	Learning Rate: 0.000315892
	LOSS [training: 5.777782540287786 | validation: 5.611825868183283]
	TIME [epoch: 8.27 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.873309406901766		[learning rate: 0.00031552]
		[batch 20/20] avg loss: 5.637558089152151		[learning rate: 0.00031515]
	Learning Rate: 0.000315147
	LOSS [training: 5.755433748026958 | validation: 5.591314803899729]
	TIME [epoch: 8.28 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.734939486017735		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 5.729420528954073		[learning rate: 0.0003144]
	Learning Rate: 0.000314403
	LOSS [training: 5.732180007485903 | validation: 5.624262389423133]
	TIME [epoch: 8.3 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.725273205269042		[learning rate: 0.00031403]
		[batch 20/20] avg loss: 5.771475403131733		[learning rate: 0.00031366]
	Learning Rate: 0.000313662
	LOSS [training: 5.7483743042003885 | validation: 5.604318413352426]
	TIME [epoch: 8.28 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.67360615199872		[learning rate: 0.00031329]
		[batch 20/20] avg loss: 5.792223080763255		[learning rate: 0.00031292]
	Learning Rate: 0.000312922
	LOSS [training: 5.732914616380987 | validation: 5.584228573340974]
	TIME [epoch: 8.28 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.709243464998378		[learning rate: 0.00031255]
		[batch 20/20] avg loss: 5.708613203181703		[learning rate: 0.00031218]
	Learning Rate: 0.000312184
	LOSS [training: 5.70892833409004 | validation: 5.540461072403642]
	TIME [epoch: 8.28 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.649167258175696		[learning rate: 0.00031182]
		[batch 20/20] avg loss: 5.89498267650862		[learning rate: 0.00031145]
	Learning Rate: 0.000311447
	LOSS [training: 5.772074967342159 | validation: 5.58584594657866]
	TIME [epoch: 8.3 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.6691268811923985		[learning rate: 0.00031108]
		[batch 20/20] avg loss: 6.03274356958529		[learning rate: 0.00031071]
	Learning Rate: 0.000310713
	LOSS [training: 5.850935225388843 | validation: 5.611180300684508]
	TIME [epoch: 8.29 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.7433165960780395		[learning rate: 0.00031035]
		[batch 20/20] avg loss: 5.897855810085294		[learning rate: 0.00030998]
	Learning Rate: 0.00030998
	LOSS [training: 5.8205862030816675 | validation: 5.536055845130437]
	TIME [epoch: 8.28 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.759724730675244		[learning rate: 0.00030961]
		[batch 20/20] avg loss: 5.867491400698481		[learning rate: 0.00030925]
	Learning Rate: 0.000309249
	LOSS [training: 5.8136080656868625 | validation: 5.5643368643406355]
	TIME [epoch: 8.29 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.920914475248256		[learning rate: 0.00030888]
		[batch 20/20] avg loss: 5.6918728104906915		[learning rate: 0.00030852]
	Learning Rate: 0.000308519
	LOSS [training: 5.806393642869475 | validation: 5.582304159984807]
	TIME [epoch: 8.28 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.993860767913485		[learning rate: 0.00030815]
		[batch 20/20] avg loss: 5.727506329984623		[learning rate: 0.00030779]
	Learning Rate: 0.000307791
	LOSS [training: 5.860683548949054 | validation: 5.595541445004733]
	TIME [epoch: 8.3 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.86903826388618		[learning rate: 0.00030743]
		[batch 20/20] avg loss: 5.847794822669599		[learning rate: 0.00030707]
	Learning Rate: 0.000307065
	LOSS [training: 5.8584165432778885 | validation: 5.568023252671499]
	TIME [epoch: 8.28 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.777888787950856		[learning rate: 0.0003067]
		[batch 20/20] avg loss: 5.7083203330496755		[learning rate: 0.00030634]
	Learning Rate: 0.000306341
	LOSS [training: 5.743104560500265 | validation: 5.508419933304032]
	TIME [epoch: 8.28 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.634403937769326		[learning rate: 0.00030598]
		[batch 20/20] avg loss: 5.789604960071449		[learning rate: 0.00030562]
	Learning Rate: 0.000305618
	LOSS [training: 5.712004448920387 | validation: 5.5135342142892565]
	TIME [epoch: 8.28 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.66805394622837		[learning rate: 0.00030526]
		[batch 20/20] avg loss: 5.794518009524568		[learning rate: 0.0003049]
	Learning Rate: 0.000304897
	LOSS [training: 5.731285977876469 | validation: 5.546340839570581]
	TIME [epoch: 8.31 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.677595723292997		[learning rate: 0.00030454]
		[batch 20/20] avg loss: 5.884848629391935		[learning rate: 0.00030418]
	Learning Rate: 0.000304178
	LOSS [training: 5.781222176342466 | validation: 5.559576354376929]
	TIME [epoch: 8.28 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.832964995018202		[learning rate: 0.00030382]
		[batch 20/20] avg loss: 5.751550591938062		[learning rate: 0.00030346]
	Learning Rate: 0.000303461
	LOSS [training: 5.792257793478132 | validation: 5.566791338066476]
	TIME [epoch: 8.29 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.688730132022878		[learning rate: 0.0003031]
		[batch 20/20] avg loss: 5.882353787373836		[learning rate: 0.00030274]
	Learning Rate: 0.000302745
	LOSS [training: 5.785541959698358 | validation: 5.56077324898032]
	TIME [epoch: 8.28 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.787769652913042		[learning rate: 0.00030239]
		[batch 20/20] avg loss: 5.869051315270157		[learning rate: 0.00030203]
	Learning Rate: 0.000302031
	LOSS [training: 5.8284104840916 | validation: 5.574461585885402]
	TIME [epoch: 8.29 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.957486351355219		[learning rate: 0.00030167]
		[batch 20/20] avg loss: 5.963656262934576		[learning rate: 0.00030132]
	Learning Rate: 0.000301318
	LOSS [training: 5.960571307144898 | validation: 5.63500377986193]
	TIME [epoch: 8.3 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.910491338250546		[learning rate: 0.00030096]
		[batch 20/20] avg loss: 5.966199699160229		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 5.938345518705386 | validation: 5.592922556963618]
	TIME [epoch: 8.28 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.870187264540562		[learning rate: 0.00030025]
		[batch 20/20] avg loss: 5.832816588407648		[learning rate: 0.0002999]
	Learning Rate: 0.000299899
	LOSS [training: 5.851501926474105 | validation: 5.582813298400344]
	TIME [epoch: 8.28 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.796010799321104		[learning rate: 0.00029954]
		[batch 20/20] avg loss: 5.917887761276935		[learning rate: 0.00029919]
	Learning Rate: 0.000299191
	LOSS [training: 5.856949280299019 | validation: 5.6246947523303366]
	TIME [epoch: 8.28 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.673314472024249		[learning rate: 0.00029884]
		[batch 20/20] avg loss: 6.161614947977542		[learning rate: 0.00029849]
	Learning Rate: 0.000298485
	LOSS [training: 5.917464710000895 | validation: 5.720698117284494]
	TIME [epoch: 8.31 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.9160144469338		[learning rate: 0.00029813]
		[batch 20/20] avg loss: 6.131399184555607		[learning rate: 0.00029778]
	Learning Rate: 0.000297781
	LOSS [training: 6.023706815744703 | validation: 5.858105421847862]
	TIME [epoch: 8.29 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.126234321300977		[learning rate: 0.00029743]
		[batch 20/20] avg loss: 6.1665389972504245		[learning rate: 0.00029708]
	Learning Rate: 0.000297079
	LOSS [training: 6.1463866592757 | validation: 5.890986748580531]
	TIME [epoch: 8.28 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.997049176659754		[learning rate: 0.00029673]
		[batch 20/20] avg loss: 6.266598837605663		[learning rate: 0.00029638]
	Learning Rate: 0.000296378
	LOSS [training: 6.131824007132709 | validation: 5.870016972141819]
	TIME [epoch: 8.29 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.9529726671593775		[learning rate: 0.00029603]
		[batch 20/20] avg loss: 6.248832582488947		[learning rate: 0.00029568]
	Learning Rate: 0.000295679
	LOSS [training: 6.100902624824162 | validation: 5.819552903085557]
	TIME [epoch: 8.3 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.241454289691278		[learning rate: 0.00029533]
		[batch 20/20] avg loss: 5.972951454766514		[learning rate: 0.00029498]
	Learning Rate: 0.000294982
	LOSS [training: 6.107202872228897 | validation: 5.759380391879067]
	TIME [epoch: 8.29 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.178468778267784		[learning rate: 0.00029463]
		[batch 20/20] avg loss: 6.046872374009963		[learning rate: 0.00029429]
	Learning Rate: 0.000294286
	LOSS [training: 6.112670576138873 | validation: 5.7679627373694995]
	TIME [epoch: 8.29 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.971120116487409		[learning rate: 0.00029394]
		[batch 20/20] avg loss: 6.019757476437813		[learning rate: 0.00029359]
	Learning Rate: 0.000293592
	LOSS [training: 5.995438796462612 | validation: 5.652210117166527]
	TIME [epoch: 8.28 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.985645222168419		[learning rate: 0.00029325]
		[batch 20/20] avg loss: 5.938399959240339		[learning rate: 0.0002929]
	Learning Rate: 0.000292899
	LOSS [training: 5.962022590704378 | validation: 5.72009667726763]
	TIME [epoch: 8.28 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.872983920699888		[learning rate: 0.00029255]
		[batch 20/20] avg loss: 6.130261022058111		[learning rate: 0.00029221]
	Learning Rate: 0.000292208
	LOSS [training: 6.001622471378999 | validation: 5.6781488853669435]
	TIME [epoch: 8.31 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.00555629990715		[learning rate: 0.00029186]
		[batch 20/20] avg loss: 5.852448748954986		[learning rate: 0.00029152]
	Learning Rate: 0.000291519
	LOSS [training: 5.929002524431068 | validation: 5.710087259009045]
	TIME [epoch: 8.28 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.865193546357495		[learning rate: 0.00029117]
		[batch 20/20] avg loss: 6.102959123655473		[learning rate: 0.00029083]
	Learning Rate: 0.000290831
	LOSS [training: 5.984076335006483 | validation: 5.683537273249014]
	TIME [epoch: 8.29 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.847819437721498		[learning rate: 0.00029049]
		[batch 20/20] avg loss: 6.148793025903582		[learning rate: 0.00029015]
	Learning Rate: 0.000290145
	LOSS [training: 5.998306231812539 | validation: 5.705534998179685]
	TIME [epoch: 8.29 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.934852069098225		[learning rate: 0.0002898]
		[batch 20/20] avg loss: 6.08314640237166		[learning rate: 0.00028946]
	Learning Rate: 0.000289461
	LOSS [training: 6.008999235734942 | validation: 5.715013787439764]
	TIME [epoch: 8.31 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.043707474402872		[learning rate: 0.00028912]
		[batch 20/20] avg loss: 6.063179324728143		[learning rate: 0.00028878]
	Learning Rate: 0.000288778
	LOSS [training: 6.053443399565508 | validation: 5.795053390226962]
	TIME [epoch: 8.28 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.034052665294832		[learning rate: 0.00028844]
		[batch 20/20] avg loss: 6.22295389413626		[learning rate: 0.0002881]
	Learning Rate: 0.000288097
	LOSS [training: 6.128503279715545 | validation: 5.870907448381553]
	TIME [epoch: 8.29 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.15044245379709		[learning rate: 0.00028776]
		[batch 20/20] avg loss: 6.175604060148179		[learning rate: 0.00028742]
	Learning Rate: 0.000287417
	LOSS [training: 6.163023256972634 | validation: 5.795622590941024]
	TIME [epoch: 8.28 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.968981246847312		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 5.975792421045876		[learning rate: 0.00028674]
	Learning Rate: 0.000286739
	LOSS [training: 5.972386833946594 | validation: 5.647766042619599]
	TIME [epoch: 8.3 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.984191063763904		[learning rate: 0.0002864]
		[batch 20/20] avg loss: 5.759276714371374		[learning rate: 0.00028606]
	Learning Rate: 0.000286063
	LOSS [training: 5.87173388906764 | validation: 5.60423045011663]
	TIME [epoch: 8.3 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.843637803896154		[learning rate: 0.00028573]
		[batch 20/20] avg loss: 5.772156445201542		[learning rate: 0.00028539]
	Learning Rate: 0.000285388
	LOSS [training: 5.807897124548848 | validation: 5.565189289258757]
	TIME [epoch: 8.29 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.621630576725616		[learning rate: 0.00028505]
		[batch 20/20] avg loss: 6.005075222973501		[learning rate: 0.00028471]
	Learning Rate: 0.000284715
	LOSS [training: 5.813352899849558 | validation: 5.599593866029247]
	TIME [epoch: 8.29 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.892050714887541		[learning rate: 0.00028438]
		[batch 20/20] avg loss: 5.833161145321943		[learning rate: 0.00028404]
	Learning Rate: 0.000284043
	LOSS [training: 5.862605930104742 | validation: 5.615026483745205]
	TIME [epoch: 8.29 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.706021952826051		[learning rate: 0.00028371]
		[batch 20/20] avg loss: 6.05013557831779		[learning rate: 0.00028337]
	Learning Rate: 0.000283373
	LOSS [training: 5.878078765571919 | validation: 5.6111016058293295]
	TIME [epoch: 8.31 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.023073363395884		[learning rate: 0.00028304]
		[batch 20/20] avg loss: 5.8195101040694865		[learning rate: 0.0002827]
	Learning Rate: 0.000282705
	LOSS [training: 5.921291733732686 | validation: 5.679553246230176]
	TIME [epoch: 8.29 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.8697602302646015		[learning rate: 0.00028237]
		[batch 20/20] avg loss: 5.86377813156657		[learning rate: 0.00028204]
	Learning Rate: 0.000282038
	LOSS [training: 5.866769180915587 | validation: 5.610464053078295]
	TIME [epoch: 8.29 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.966203498070331		[learning rate: 0.00028171]
		[batch 20/20] avg loss: 5.612485291061098		[learning rate: 0.00028137]
	Learning Rate: 0.000281373
	LOSS [training: 5.789344394565714 | validation: 5.60285269786403]
	TIME [epoch: 8.29 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.760018233661991		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 5.781389689865285		[learning rate: 0.00028071]
	Learning Rate: 0.000280709
	LOSS [training: 5.770703961763638 | validation: 5.614946376827497]
	TIME [epoch: 8.31 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.878279300025507		[learning rate: 0.00028038]
		[batch 20/20] avg loss: 5.774495594994869		[learning rate: 0.00028005]
	Learning Rate: 0.000280047
	LOSS [training: 5.8263874475101884 | validation: 5.647324060646361]
	TIME [epoch: 8.29 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.80054629459495		[learning rate: 0.00027972]
		[batch 20/20] avg loss: 5.956033905462349		[learning rate: 0.00027939]
	Learning Rate: 0.000279386
	LOSS [training: 5.87829010002865 | validation: 5.689450205982187]
	TIME [epoch: 8.29 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.889658829998528		[learning rate: 0.00027906]
		[batch 20/20] avg loss: 6.104615923341575		[learning rate: 0.00027873]
	Learning Rate: 0.000278727
	LOSS [training: 5.997137376670052 | validation: 5.869635868340389]
	TIME [epoch: 8.28 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.0104520602559095		[learning rate: 0.0002784]
		[batch 20/20] avg loss: 6.109476911781146		[learning rate: 0.00027807]
	Learning Rate: 0.00027807
	LOSS [training: 6.0599644860185276 | validation: 5.8500151605973]
	TIME [epoch: 8.3 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.141350480831246		[learning rate: 0.00027774]
		[batch 20/20] avg loss: 5.930213162667945		[learning rate: 0.00027741]
	Learning Rate: 0.000277414
	LOSS [training: 6.035781821749595 | validation: 5.859164592745541]
	TIME [epoch: 8.29 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.183980746547943		[learning rate: 0.00027709]
		[batch 20/20] avg loss: 6.119296692968829		[learning rate: 0.00027676]
	Learning Rate: 0.000276759
	LOSS [training: 6.1516387197583855 | validation: 5.943185382135702]
	TIME [epoch: 8.28 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.34730516267226		[learning rate: 0.00027643]
		[batch 20/20] avg loss: 6.26926813035723		[learning rate: 0.00027611]
	Learning Rate: 0.000276107
	LOSS [training: 6.308286646514745 | validation: 6.080272669051595]
	TIME [epoch: 8.28 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.509592759134096		[learning rate: 0.00027578]
		[batch 20/20] avg loss: 6.305762937245587		[learning rate: 0.00027546]
	Learning Rate: 0.000275455
	LOSS [training: 6.40767784818984 | validation: 6.152957876813557]
	TIME [epoch: 8.29 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.367954315667054		[learning rate: 0.00027513]
		[batch 20/20] avg loss: 6.589990466924644		[learning rate: 0.00027481]
	Learning Rate: 0.000274806
	LOSS [training: 6.478972391295848 | validation: 6.211490650997433]
	TIME [epoch: 8.31 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.544294862917444		[learning rate: 0.00027448]
		[batch 20/20] avg loss: 6.611248274018868		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 6.577771568468156 | validation: 6.275651724157723]
	TIME [epoch: 8.28 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.524749352012312		[learning rate: 0.00027383]
		[batch 20/20] avg loss: 6.735823267909227		[learning rate: 0.00027351]
	Learning Rate: 0.000273511
	LOSS [training: 6.630286309960769 | validation: 6.3000677993893195]
	TIME [epoch: 8.29 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.715695668802654		[learning rate: 0.00027319]
		[batch 20/20] avg loss: 6.651683032959204		[learning rate: 0.00027287]
	Learning Rate: 0.000272866
	LOSS [training: 6.683689350880928 | validation: 6.347444158510106]
	TIME [epoch: 8.29 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.604708578922347		[learning rate: 0.00027254]
		[batch 20/20] avg loss: 6.702031599656377		[learning rate: 0.00027222]
	Learning Rate: 0.000272222
	LOSS [training: 6.653370089289362 | validation: 6.372773038186814]
	TIME [epoch: 8.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.572254945066706		[learning rate: 0.0002719]
		[batch 20/20] avg loss: 6.829359356969983		[learning rate: 0.00027158]
	Learning Rate: 0.00027158
	LOSS [training: 6.700807151018343 | validation: 6.395939485148002]
	TIME [epoch: 8.29 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.703520448719321		[learning rate: 0.00027126]
		[batch 20/20] avg loss: 6.802403759864741		[learning rate: 0.00027094]
	Learning Rate: 0.000270939
	LOSS [training: 6.752962104292031 | validation: 6.451764069482497]
	TIME [epoch: 8.29 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.824882252813867		[learning rate: 0.00027062]
		[batch 20/20] avg loss: 6.73258873480179		[learning rate: 0.0002703]
	Learning Rate: 0.0002703
	LOSS [training: 6.7787354938078295 | validation: 6.476599998101275]
	TIME [epoch: 8.29 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.8651994211482075		[learning rate: 0.00026998]
		[batch 20/20] avg loss: 6.891524829772902		[learning rate: 0.00026966]
	Learning Rate: 0.000269662
	LOSS [training: 6.878362125460555 | validation: 6.562854252162709]
	TIME [epoch: 8.29 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.796367831746478		[learning rate: 0.00026934]
		[batch 20/20] avg loss: 7.038368576608958		[learning rate: 0.00026903]
	Learning Rate: 0.000269026
	LOSS [training: 6.917368204177718 | validation: 6.644722839231577]
	TIME [epoch: 8.3 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.077758608158329		[learning rate: 0.00026871]
		[batch 20/20] avg loss: 6.879869059351924		[learning rate: 0.00026839]
	Learning Rate: 0.000268392
	LOSS [training: 6.978813833755128 | validation: 6.649832880514151]
	TIME [epoch: 8.29 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.937260681115847		[learning rate: 0.00026808]
		[batch 20/20] avg loss: 7.090067166982405		[learning rate: 0.00026776]
	Learning Rate: 0.000267759
	LOSS [training: 7.0136639240491245 | validation: 6.612603690693556]
	TIME [epoch: 8.29 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.920789944095953		[learning rate: 0.00026744]
		[batch 20/20] avg loss: 7.155209485772673		[learning rate: 0.00026713]
	Learning Rate: 0.000267127
	LOSS [training: 7.037999714934313 | validation: 6.646762973917473]
	TIME [epoch: 8.28 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.055499464529238		[learning rate: 0.00026681]
		[batch 20/20] avg loss: 7.0402598090169475		[learning rate: 0.0002665]
	Learning Rate: 0.000266497
	LOSS [training: 7.047879636773095 | validation: 6.63070836161838]
	TIME [epoch: 8.31 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.023175393841396		[learning rate: 0.00026618]
		[batch 20/20] avg loss: 7.056487590194914		[learning rate: 0.00026587]
	Learning Rate: 0.000265868
	LOSS [training: 7.039831492018156 | validation: 6.671566375841918]
	TIME [epoch: 8.28 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.116477810470192		[learning rate: 0.00026555]
		[batch 20/20] avg loss: 6.994284142810696		[learning rate: 0.00026524]
	Learning Rate: 0.000265241
	LOSS [training: 7.0553809766404445 | validation: 6.656969568966474]
	TIME [epoch: 8.28 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.0593005119681465		[learning rate: 0.00026493]
		[batch 20/20] avg loss: 6.9894774848140795		[learning rate: 0.00026462]
	Learning Rate: 0.000264616
	LOSS [training: 7.024388998391113 | validation: 6.65094438083382]
	TIME [epoch: 8.28 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.157889719265109		[learning rate: 0.0002643]
		[batch 20/20] avg loss: 6.813111891916354		[learning rate: 0.00026399]
	Learning Rate: 0.000263991
	LOSS [training: 6.98550080559073 | validation: 6.632852999478134]
	TIME [epoch: 8.3 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.904081033999577		[learning rate: 0.00026368]
		[batch 20/20] avg loss: 7.109318650143185		[learning rate: 0.00026337]
	Learning Rate: 0.000263369
	LOSS [training: 7.00669984207138 | validation: 6.678860411698231]
	TIME [epoch: 8.29 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.120998405861348		[learning rate: 0.00026306]
		[batch 20/20] avg loss: 7.054764777301868		[learning rate: 0.00026275]
	Learning Rate: 0.000262747
	LOSS [training: 7.087881591581608 | validation: 6.7240687112367405]
	TIME [epoch: 8.29 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.980787053377523		[learning rate: 0.00026244]
		[batch 20/20] avg loss: 7.20873251461182		[learning rate: 0.00026213]
	Learning Rate: 0.000262128
	LOSS [training: 7.094759783994671 | validation: 6.688634532140333]
	TIME [epoch: 8.29 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.09274792562141		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 7.201010824901227		[learning rate: 0.00026151]
	Learning Rate: 0.000261509
	LOSS [training: 7.146879375261318 | validation: 6.71929179452989]
	TIME [epoch: 8.29 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.169151112191047		[learning rate: 0.0002612]
		[batch 20/20] avg loss: 7.06812192539129		[learning rate: 0.00026089]
	Learning Rate: 0.000260892
	LOSS [training: 7.118636518791168 | validation: 6.6708873365006625]
	TIME [epoch: 8.3 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.989584378425654		[learning rate: 0.00026058]
		[batch 20/20] avg loss: 7.101397492986427		[learning rate: 0.00026028]
	Learning Rate: 0.000260277
	LOSS [training: 7.04549093570604 | validation: 6.571985599700456]
	TIME [epoch: 8.28 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.057226352629709		[learning rate: 0.00025997]
		[batch 20/20] avg loss: 6.841532872817287		[learning rate: 0.00025966]
	Learning Rate: 0.000259663
	LOSS [training: 6.949379612723497 | validation: 6.538184015523187]
	TIME [epoch: 8.28 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.975072878364221		[learning rate: 0.00025936]
		[batch 20/20] avg loss: 6.923148720292818		[learning rate: 0.00025905]
	Learning Rate: 0.000259051
	LOSS [training: 6.94911079932852 | validation: 6.605712860846908]
	TIME [epoch: 8.29 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.072056242775361		[learning rate: 0.00025874]
		[batch 20/20] avg loss: 6.9208765085333805		[learning rate: 0.00025844]
	Learning Rate: 0.00025844
	LOSS [training: 6.99646637565437 | validation: 6.580875310029133]
	TIME [epoch: 8.31 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.9498289216427995		[learning rate: 0.00025813]
		[batch 20/20] avg loss: 7.069220460717365		[learning rate: 0.00025783]
	Learning Rate: 0.00025783
	LOSS [training: 7.009524691180081 | validation: 6.6129932798252575]
	TIME [epoch: 8.29 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.839871356417771		[learning rate: 0.00025753]
		[batch 20/20] avg loss: 7.084931783151608		[learning rate: 0.00025722]
	Learning Rate: 0.000257222
	LOSS [training: 6.962401569784689 | validation: 6.5634845565707565]
	TIME [epoch: 8.28 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.004215171476718		[learning rate: 0.00025692]
		[batch 20/20] avg loss: 7.027638250089805		[learning rate: 0.00025662]
	Learning Rate: 0.000256615
	LOSS [training: 7.015926710783264 | validation: 6.641264540982874]
	TIME [epoch: 8.29 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.9878373409150285		[learning rate: 0.00025631]
		[batch 20/20] avg loss: 7.153218431475912		[learning rate: 0.00025601]
	Learning Rate: 0.00025601
	LOSS [training: 7.070527886195471 | validation: 6.668589679443419]
	TIME [epoch: 8.3 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.9475972475959775		[learning rate: 0.00025571]
		[batch 20/20] avg loss: 7.157178745580109		[learning rate: 0.00025541]
	Learning Rate: 0.000255406
	LOSS [training: 7.052387996588044 | validation: 6.695035369819642]
	TIME [epoch: 8.3 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.064798172947062		[learning rate: 0.0002551]
		[batch 20/20] avg loss: 7.100276970369784		[learning rate: 0.0002548]
	Learning Rate: 0.000254803
	LOSS [training: 7.082537571658422 | validation: 6.714296839353312]
	TIME [epoch: 8.28 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.109249203067537		[learning rate: 0.0002545]
		[batch 20/20] avg loss: 7.070981084939338		[learning rate: 0.0002542]
	Learning Rate: 0.000254202
	LOSS [training: 7.090115144003437 | validation: 6.704887549579922]
	TIME [epoch: 8.28 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.167842218339598		[learning rate: 0.0002539]
		[batch 20/20] avg loss: 7.198814910894095		[learning rate: 0.0002536]
	Learning Rate: 0.000253603
	LOSS [training: 7.183328564616845 | validation: 6.783366269229428]
	TIME [epoch: 8.29 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.245361838191307		[learning rate: 0.0002533]
		[batch 20/20] avg loss: 7.165721330865024		[learning rate: 0.000253]
	Learning Rate: 0.000253004
	LOSS [training: 7.205541584528166 | validation: 6.815418054753239]
	TIME [epoch: 8.31 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.183205906710316		[learning rate: 0.00025271]
		[batch 20/20] avg loss: 7.137383933591186		[learning rate: 0.00025241]
	Learning Rate: 0.000252408
	LOSS [training: 7.1602949201507515 | validation: 6.713511959719619]
	TIME [epoch: 8.28 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.965777084628196		[learning rate: 0.00025211]
		[batch 20/20] avg loss: 7.220322446658541		[learning rate: 0.00025181]
	Learning Rate: 0.000251812
	LOSS [training: 7.093049765643367 | validation: 6.697642552931109]
	TIME [epoch: 8.28 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.024319180053119		[learning rate: 0.00025152]
		[batch 20/20] avg loss: 7.118429585521002		[learning rate: 0.00025122]
	Learning Rate: 0.000251218
	LOSS [training: 7.071374382787061 | validation: 6.749710063900866]
	TIME [epoch: 8.28 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.135518855138687		[learning rate: 0.00025092]
		[batch 20/20] avg loss: 7.123271572950472		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 7.12939521404458 | validation: 6.742301277474793]
	TIME [epoch: 8.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.989243754290342		[learning rate: 0.00025033]
		[batch 20/20] avg loss: 7.186825019431341		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 7.088034386860841 | validation: 6.730907650366044]
	TIME [epoch: 8.28 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.17026816567647		[learning rate: 0.00024974]
		[batch 20/20] avg loss: 6.88878661010674		[learning rate: 0.00024944]
	Learning Rate: 0.000249445
	LOSS [training: 7.029527387891605 | validation: 6.738616467308859]
	TIME [epoch: 8.28 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.991640654830446		[learning rate: 0.00024915]
		[batch 20/20] avg loss: 7.028712711130129		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 7.010176682980287 | validation: 6.644691481585727]
	TIME [epoch: 8.29 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.943049500246988		[learning rate: 0.00024856]
		[batch 20/20] avg loss: 6.985899115210127		[learning rate: 0.00024827]
	Learning Rate: 0.000248269
	LOSS [training: 6.9644743077285565 | validation: 6.637951747281624]
	TIME [epoch: 8.3 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.9654767368304125		[learning rate: 0.00024798]
		[batch 20/20] avg loss: 6.961096033207676		[learning rate: 0.00024768]
	Learning Rate: 0.000247684
	LOSS [training: 6.963286385019046 | validation: 6.679484652922405]
	TIME [epoch: 8.29 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.916437549407472		[learning rate: 0.00024739]
		[batch 20/20] avg loss: 7.005292392138666		[learning rate: 0.0002471]
	Learning Rate: 0.000247099
	LOSS [training: 6.960864970773068 | validation: 6.698132796689092]
	TIME [epoch: 8.28 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.8144426645349565		[learning rate: 0.00024681]
		[batch 20/20] avg loss: 7.192444926490976		[learning rate: 0.00024652]
	Learning Rate: 0.000246517
	LOSS [training: 7.003443795512967 | validation: 6.787886788150413]
	TIME [epoch: 8.29 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.018897459825935		[learning rate: 0.00024623]
		[batch 20/20] avg loss: 7.092946167199592		[learning rate: 0.00024594]
	Learning Rate: 0.000245935
	LOSS [training: 7.055921813512763 | validation: 6.793302811924608]
	TIME [epoch: 8.29 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.00131972833703		[learning rate: 0.00024564]
		[batch 20/20] avg loss: 6.979980800418585		[learning rate: 0.00024535]
	Learning Rate: 0.000245355
	LOSS [training: 6.990650264377807 | validation: 6.707323659407942]
	TIME [epoch: 8.31 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.038023435642343		[learning rate: 0.00024507]
		[batch 20/20] avg loss: 6.777262934160154		[learning rate: 0.00024478]
	Learning Rate: 0.000244776
	LOSS [training: 6.907643184901249 | validation: 6.67754133611819]
	TIME [epoch: 8.28 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.786640921977025		[learning rate: 0.00024449]
		[batch 20/20] avg loss: 7.059193651311287		[learning rate: 0.0002442]
	Learning Rate: 0.000244199
	LOSS [training: 6.922917286644155 | validation: 6.7505668999165644]
	TIME [epoch: 8.28 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.97404835841367		[learning rate: 0.00024391]
		[batch 20/20] avg loss: 6.93322112270327		[learning rate: 0.00024362]
	Learning Rate: 0.000243623
	LOSS [training: 6.953634740558469 | validation: 6.695754160440657]
	TIME [epoch: 8.29 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.849899617916303		[learning rate: 0.00024334]
		[batch 20/20] avg loss: 7.127714941990163		[learning rate: 0.00024305]
	Learning Rate: 0.000243048
	LOSS [training: 6.988807279953233 | validation: 6.786449545854761]
	TIME [epoch: 8.31 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.900480768121095		[learning rate: 0.00024276]
		[batch 20/20] avg loss: 7.162144771289094		[learning rate: 0.00024247]
	Learning Rate: 0.000242475
	LOSS [training: 7.031312769705096 | validation: 6.732948826407495]
	TIME [epoch: 8.28 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.876576836048784		[learning rate: 0.00024219]
		[batch 20/20] avg loss: 7.083444161706351		[learning rate: 0.0002419]
	Learning Rate: 0.000241903
	LOSS [training: 6.980010498877569 | validation: 6.7011281385396675]
	TIME [epoch: 8.27 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.003447727163298		[learning rate: 0.00024162]
		[batch 20/20] avg loss: 6.8982688748211505		[learning rate: 0.00024133]
	Learning Rate: 0.000241332
	LOSS [training: 6.950858300992223 | validation: 6.77987599286177]
	TIME [epoch: 8.28 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.015500256441234		[learning rate: 0.00024105]
		[batch 20/20] avg loss: 7.154921031659953		[learning rate: 0.00024076]
	Learning Rate: 0.000240763
	LOSS [training: 7.085210644050593 | validation: 6.889478816105794]
	TIME [epoch: 8.3 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.172277323534395		[learning rate: 0.00024048]
		[batch 20/20] avg loss: 7.1942600635302725		[learning rate: 0.0002402]
	Learning Rate: 0.000240195
	LOSS [training: 7.1832686935323355 | validation: 6.99700968717303]
	TIME [epoch: 8.29 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.1679436098830935		[learning rate: 0.00023991]
		[batch 20/20] avg loss: 7.208847583375089		[learning rate: 0.00023963]
	Learning Rate: 0.000239628
	LOSS [training: 7.188395596629091 | validation: 6.910655246810496]
	TIME [epoch: 8.28 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.147100792112293		[learning rate: 0.00023935]
		[batch 20/20] avg loss: 7.114482005026071		[learning rate: 0.00023906]
	Learning Rate: 0.000239063
	LOSS [training: 7.1307913985691815 | validation: 6.915791122983768]
	TIME [epoch: 8.28 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.151150199557449		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 7.13636446474313		[learning rate: 0.0002385]
	Learning Rate: 0.000238499
	LOSS [training: 7.1437573321502885 | validation: 6.947926031825428]
	TIME [epoch: 8.28 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.115843356014679		[learning rate: 0.00023822]
		[batch 20/20] avg loss: 7.295954186574354		[learning rate: 0.00023794]
	Learning Rate: 0.000237937
	LOSS [training: 7.205898771294517 | validation: 7.078362439443276]
	TIME [epoch: 8.31 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.229000966872183		[learning rate: 0.00023766]
		[batch 20/20] avg loss: 7.383814070732095		[learning rate: 0.00023738]
	Learning Rate: 0.000237375
	LOSS [training: 7.306407518802137 | validation: 7.084553528938789]
	TIME [epoch: 8.28 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.189142339324616		[learning rate: 0.0002371]
		[batch 20/20] avg loss: 7.456284251286961		[learning rate: 0.00023682]
	Learning Rate: 0.000236816
	LOSS [training: 7.322713295305789 | validation: 7.10790684637048]
	TIME [epoch: 8.28 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.250252469354484		[learning rate: 0.00023654]
		[batch 20/20] avg loss: 7.485987034271818		[learning rate: 0.00023626]
	Learning Rate: 0.000236257
	LOSS [training: 7.36811975181315 | validation: 7.226287077853804]
	TIME [epoch: 8.27 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.358909699944107		[learning rate: 0.00023598]
		[batch 20/20] avg loss: 7.600450956527704		[learning rate: 0.0002357]
	Learning Rate: 0.0002357
	LOSS [training: 7.479680328235905 | validation: 7.300706246446228]
	TIME [epoch: 8.3 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.537342882114115		[learning rate: 0.00023542]
		[batch 20/20] avg loss: 7.631506961412086		[learning rate: 0.00023514]
	Learning Rate: 0.000235144
	LOSS [training: 7.584424921763099 | validation: 7.375194323838149]
	TIME [epoch: 8.29 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.64483082154464		[learning rate: 0.00023487]
		[batch 20/20] avg loss: 7.590843216055052		[learning rate: 0.00023459]
	Learning Rate: 0.000234589
	LOSS [training: 7.617837018799845 | validation: 7.415612624756441]
	TIME [epoch: 8.28 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.615261320150793		[learning rate: 0.00023431]
		[batch 20/20] avg loss: 7.816874529881124		[learning rate: 0.00023404]
	Learning Rate: 0.000234036
	LOSS [training: 7.7160679250159605 | validation: 7.5222996381104705]
	TIME [epoch: 8.29 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.694814040622289		[learning rate: 0.00023376]
		[batch 20/20] avg loss: 7.8625340143872124		[learning rate: 0.00023348]
	Learning Rate: 0.000233484
	LOSS [training: 7.778674027504751 | validation: 7.570604621609313]
	TIME [epoch: 8.29 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.803666377556534		[learning rate: 0.00023321]
		[batch 20/20] avg loss: 7.814782233581239		[learning rate: 0.00023293]
	Learning Rate: 0.000232933
	LOSS [training: 7.809224305568888 | validation: 7.54266948914931]
	TIME [epoch: 8.31 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.773880015791349		[learning rate: 0.00023266]
		[batch 20/20] avg loss: 7.851785525197805		[learning rate: 0.00023238]
	Learning Rate: 0.000232383
	LOSS [training: 7.812832770494578 | validation: 7.576078447223144]
	TIME [epoch: 8.28 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8295358838982505		[learning rate: 0.00023211]
		[batch 20/20] avg loss: 7.865293953036594		[learning rate: 0.00023184]
	Learning Rate: 0.000231835
	LOSS [training: 7.84741491846742 | validation: 7.594138116144348]
	TIME [epoch: 8.28 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.754164426384245		[learning rate: 0.00023156]
		[batch 20/20] avg loss: 7.962447124708935		[learning rate: 0.00023129]
	Learning Rate: 0.000231288
	LOSS [training: 7.8583057755465875 | validation: 7.593687395397978]
	TIME [epoch: 8.29 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.764611387949839		[learning rate: 0.00023102]
		[batch 20/20] avg loss: 7.99052344764848		[learning rate: 0.00023074]
	Learning Rate: 0.000230743
	LOSS [training: 7.877567417799161 | validation: 7.640558184835429]
	TIME [epoch: 8.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.778918619778933		[learning rate: 0.00023047]
		[batch 20/20] avg loss: 8.055315699376127		[learning rate: 0.0002302]
	Learning Rate: 0.000230198
	LOSS [training: 7.917117159577529 | validation: 7.682132574591354]
	TIME [epoch: 8.28 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.877857025538042		[learning rate: 0.00022993]
		[batch 20/20] avg loss: 7.999238814552841		[learning rate: 0.00022966]
	Learning Rate: 0.000229656
	LOSS [training: 7.938547920045441 | validation: 7.683927573214119]
	TIME [epoch: 8.28 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.923717478600193		[learning rate: 0.00022938]
		[batch 20/20] avg loss: 7.94799351007252		[learning rate: 0.00022911]
	Learning Rate: 0.000229114
	LOSS [training: 7.935855494336356 | validation: 7.70245130198668]
	TIME [epoch: 8.28 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.970557632397801		[learning rate: 0.00022884]
		[batch 20/20] avg loss: 7.981779479387631		[learning rate: 0.00022857]
	Learning Rate: 0.000228573
	LOSS [training: 7.976168555892717 | validation: 7.762371022966004]
	TIME [epoch: 8.29 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.015962439169432		[learning rate: 0.0002283]
		[batch 20/20] avg loss: 7.892863695586589		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 7.954413067378011 | validation: 7.747219723666349]
	TIME [epoch: 8.29 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.893901943513695		[learning rate: 0.00022777]
		[batch 20/20] avg loss: 8.062361401676611		[learning rate: 0.0002275]
	Learning Rate: 0.000227496
	LOSS [training: 7.978131672595152 | validation: 7.835957831945828]
	TIME [epoch: 8.28 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.09085405158447		[learning rate: 0.00022723]
		[batch 20/20] avg loss: 7.891996835114097		[learning rate: 0.00022696]
	Learning Rate: 0.00022696
	LOSS [training: 7.991425443349284 | validation: 7.724933262351418]
	TIME [epoch: 8.28 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.829518178247352		[learning rate: 0.00022669]
		[batch 20/20] avg loss: 8.074019241337647		[learning rate: 0.00022642]
	Learning Rate: 0.000226424
	LOSS [training: 7.951768709792501 | validation: 7.728806616158321]
	TIME [epoch: 8.29 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.960229187760793		[learning rate: 0.00022616]
		[batch 20/20] avg loss: 8.007395639023457		[learning rate: 0.00022589]
	Learning Rate: 0.00022589
	LOSS [training: 7.983812413392124 | validation: 7.791004571863724]
	TIME [epoch: 8.32 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.100354280967846		[learning rate: 0.00022562]
		[batch 20/20] avg loss: 7.880353668776209		[learning rate: 0.00022536]
	Learning Rate: 0.000225357
	LOSS [training: 7.990353974872027 | validation: 7.736467975446368]
	TIME [epoch: 8.29 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.903072155684559		[learning rate: 0.00022509]
		[batch 20/20] avg loss: 8.019424125809193		[learning rate: 0.00022483]
	Learning Rate: 0.000224826
	LOSS [training: 7.961248140746878 | validation: 7.758711850268841]
	TIME [epoch: 8.29 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.982645574397344		[learning rate: 0.00022456]
		[batch 20/20] avg loss: 7.936148854152449		[learning rate: 0.0002243]
	Learning Rate: 0.000224295
	LOSS [training: 7.9593972142748965 | validation: 7.766255793245644]
	TIME [epoch: 8.29 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.989306195074667		[learning rate: 0.00022403]
		[batch 20/20] avg loss: 7.992193510072591		[learning rate: 0.00022377]
	Learning Rate: 0.000223766
	LOSS [training: 7.990749852573629 | validation: 7.823797680805863]
	TIME [epoch: 8.31 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.033205709866106		[learning rate: 0.0002235]
		[batch 20/20] avg loss: 8.025820348070917		[learning rate: 0.00022324]
	Learning Rate: 0.000223239
	LOSS [training: 8.029513028968513 | validation: 7.824289479980051]
	TIME [epoch: 8.29 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.11672255313562		[learning rate: 0.00022298]
		[batch 20/20] avg loss: 7.923076932484671		[learning rate: 0.00022271]
	Learning Rate: 0.000222712
	LOSS [training: 8.019899742810145 | validation: 7.839075217127044]
	TIME [epoch: 8.28 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.035616361213046		[learning rate: 0.00022245]
		[batch 20/20] avg loss: 8.084390991239962		[learning rate: 0.00022219]
	Learning Rate: 0.000222187
	LOSS [training: 8.060003676226506 | validation: 7.894767082694011]
	TIME [epoch: 8.29 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.07540132145275		[learning rate: 0.00022192]
		[batch 20/20] avg loss: 8.062726971161247		[learning rate: 0.00022166]
	Learning Rate: 0.000221663
	LOSS [training: 8.069064146307 | validation: 7.839680862530177]
	TIME [epoch: 8.3 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.016423416797384		[learning rate: 0.0002214]
		[batch 20/20] avg loss: 8.051639955246593		[learning rate: 0.00022114]
	Learning Rate: 0.00022114
	LOSS [training: 8.034031686021988 | validation: 7.806816191817558]
	TIME [epoch: 8.29 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.976893585167123		[learning rate: 0.00022088]
		[batch 20/20] avg loss: 8.058735690795313		[learning rate: 0.00022062]
	Learning Rate: 0.000220618
	LOSS [training: 8.01781463798122 | validation: 7.788964634101242]
	TIME [epoch: 8.28 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.989651059245427		[learning rate: 0.00022036]
		[batch 20/20] avg loss: 7.998114826331863		[learning rate: 0.0002201]
	Learning Rate: 0.000220098
	LOSS [training: 7.993882942788645 | validation: 7.687854645294601]
	TIME [epoch: 8.28 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.003587653113192		[learning rate: 0.00021984]
		[batch 20/20] avg loss: 7.944697871957786		[learning rate: 0.00021958]
	Learning Rate: 0.000219578
	LOSS [training: 7.974142762535488 | validation: 7.678309938744126]
	TIME [epoch: 8.29 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.947750874615077		[learning rate: 0.00021932]
		[batch 20/20] avg loss: 7.912797395533595		[learning rate: 0.00021906]
	Learning Rate: 0.000219061
	LOSS [training: 7.930274135074336 | validation: 7.669720192743279]
	TIME [epoch: 8.33 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.83260693192873		[learning rate: 0.0002188]
		[batch 20/20] avg loss: 8.03824328283016		[learning rate: 0.00021854]
	Learning Rate: 0.000218544
	LOSS [training: 7.935425107379444 | validation: 7.675084853667912]
	TIME [epoch: 8.29 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.832376528016272		[learning rate: 0.00021829]
		[batch 20/20] avg loss: 8.035294191987983		[learning rate: 0.00021803]
	Learning Rate: 0.000218028
	LOSS [training: 7.933835360002128 | validation: 7.676327186468838]
	TIME [epoch: 8.28 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.910480317760819		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 7.937128664751033		[learning rate: 0.00021751]
	Learning Rate: 0.000217514
	LOSS [training: 7.9238044912559245 | validation: 7.689564060768291]
	TIME [epoch: 8.29 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8416329596570815		[learning rate: 0.00021726]
		[batch 20/20] avg loss: 8.015861960167046		[learning rate: 0.000217]
	Learning Rate: 0.000217001
	LOSS [training: 7.928747459912063 | validation: 7.677359750161061]
	TIME [epoch: 8.3 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9982254331971685		[learning rate: 0.00021674]
		[batch 20/20] avg loss: 7.93031351965085		[learning rate: 0.00021649]
	Learning Rate: 0.000216489
	LOSS [training: 7.96426947642401 | validation: 7.720427405483017]
	TIME [epoch: 8.28 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.01677172014191		[learning rate: 0.00021623]
		[batch 20/20] avg loss: 7.852045667577134		[learning rate: 0.00021598]
	Learning Rate: 0.000215978
	LOSS [training: 7.934408693859521 | validation: 7.6681785499791975]
	TIME [epoch: 8.28 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.913009893337369		[learning rate: 0.00021572]
		[batch 20/20] avg loss: 7.863403510214101		[learning rate: 0.00021547]
	Learning Rate: 0.000215469
	LOSS [training: 7.888206701775735 | validation: 7.596921573699847]
	TIME [epoch: 8.29 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.835352498564438		[learning rate: 0.00021521]
		[batch 20/20] avg loss: 7.912100629207534		[learning rate: 0.00021496]
	Learning Rate: 0.000214961
	LOSS [training: 7.873726563885986 | validation: 7.605646372116286]
	TIME [epoch: 8.3 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.812844360621884		[learning rate: 0.00021471]
		[batch 20/20] avg loss: 7.939550493747605		[learning rate: 0.00021445]
	Learning Rate: 0.000214454
	LOSS [training: 7.876197427184745 | validation: 7.638649744160016]
	TIME [epoch: 8.3 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.904929116827708		[learning rate: 0.0002142]
		[batch 20/20] avg loss: 7.862653225519994		[learning rate: 0.00021395]
	Learning Rate: 0.000213948
	LOSS [training: 7.8837911711738515 | validation: 7.648679053041493]
	TIME [epoch: 8.28 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.892217305040125		[learning rate: 0.0002137]
		[batch 20/20] avg loss: 7.971485671407919		[learning rate: 0.00021344]
	Learning Rate: 0.000213443
	LOSS [training: 7.931851488224024 | validation: 7.726412996684582]
	TIME [epoch: 8.29 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.054840829893745		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 7.970669937458247		[learning rate: 0.00021294]
	Learning Rate: 0.00021294
	LOSS [training: 8.012755383675998 | validation: 7.788750877691415]
	TIME [epoch: 8.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.097465403128176		[learning rate: 0.00021269]
		[batch 20/20] avg loss: 7.9790491139436055		[learning rate: 0.00021244]
	Learning Rate: 0.000212437
	LOSS [training: 8.038257258535893 | validation: 7.8244855013311625]
	TIME [epoch: 8.31 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.124587810923298		[learning rate: 0.00021219]
		[batch 20/20] avg loss: 8.024714118588372		[learning rate: 0.00021194]
	Learning Rate: 0.000211936
	LOSS [training: 8.074650964755836 | validation: 7.851023414232251]
	TIME [epoch: 8.29 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.044537289342951		[learning rate: 0.00021169]
		[batch 20/20] avg loss: 8.106194634318888		[learning rate: 0.00021144]
	Learning Rate: 0.000211436
	LOSS [training: 8.07536596183092 | validation: 7.893857156665624]
	TIME [epoch: 8.28 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.074863997559929		[learning rate: 0.00021119]
		[batch 20/20] avg loss: 8.091074086641882		[learning rate: 0.00021094]
	Learning Rate: 0.000210937
	LOSS [training: 8.082969042100904 | validation: 7.875509025297835]
	TIME [epoch: 8.29 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.088303810977482		[learning rate: 0.00021069]
		[batch 20/20] avg loss: 8.119734702345193		[learning rate: 0.00021044]
	Learning Rate: 0.00021044
	LOSS [training: 8.104019256661338 | validation: 7.9096150939257805]
	TIME [epoch: 8.3 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.173327991550442		[learning rate: 0.00021019]
		[batch 20/20] avg loss: 8.026251415903563		[learning rate: 0.00020994]
	Learning Rate: 0.000209944
	LOSS [training: 8.099789703727001 | validation: 7.907479308819655]
	TIME [epoch: 8.28 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.142758713781172		[learning rate: 0.0002097]
		[batch 20/20] avg loss: 8.081842422596768		[learning rate: 0.00020945]
	Learning Rate: 0.000209448
	LOSS [training: 8.11230056818897 | validation: 7.906793124760825]
	TIME [epoch: 8.28 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.012406685641675		[learning rate: 0.0002092]
		[batch 20/20] avg loss: 8.17603810351492		[learning rate: 0.00020895]
	Learning Rate: 0.000208954
	LOSS [training: 8.0942223945783 | validation: 7.893294001771851]
	TIME [epoch: 8.27 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.073325630252672		[learning rate: 0.00020871]
		[batch 20/20] avg loss: 8.177370940531969		[learning rate: 0.00020846]
	Learning Rate: 0.000208461
	LOSS [training: 8.125348285392322 | validation: 7.924310003805881]
	TIME [epoch: 8.28 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.026683636806467		[learning rate: 0.00020822]
		[batch 20/20] avg loss: 8.195321316188206		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 8.111002476497337 | validation: 7.864962633793431]
	TIME [epoch: 8.29 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.140110507875566		[learning rate: 0.00020772]
		[batch 20/20] avg loss: 8.071007347175769		[learning rate: 0.00020748]
	Learning Rate: 0.000207479
	LOSS [training: 8.105558927525667 | validation: 7.845688731989845]
	TIME [epoch: 8.28 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.18161427828213		[learning rate: 0.00020723]
		[batch 20/20] avg loss: 8.000348962835126		[learning rate: 0.00020699]
	Learning Rate: 0.00020699
	LOSS [training: 8.090981620558626 | validation: 7.9207161220969535]
	TIME [epoch: 8.28 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.089282191029564		[learning rate: 0.00020675]
		[batch 20/20] avg loss: 8.14474969828468		[learning rate: 0.0002065]
	Learning Rate: 0.000206501
	LOSS [training: 8.117015944657123 | validation: 7.894834715667594]
	TIME [epoch: 8.29 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.079398689530876		[learning rate: 0.00020626]
		[batch 20/20] avg loss: 8.132300483756188		[learning rate: 0.00020601]
	Learning Rate: 0.000206014
	LOSS [training: 8.105849586643533 | validation: 7.882356488381047]
	TIME [epoch: 8.3 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.164634847436561		[learning rate: 0.00020577]
		[batch 20/20] avg loss: 8.034935801234248		[learning rate: 0.00020553]
	Learning Rate: 0.000205528
	LOSS [training: 8.099785324335404 | validation: 7.873418078703704]
	TIME [epoch: 8.28 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.085391454098623		[learning rate: 0.00020529]
		[batch 20/20] avg loss: 8.09857848490792		[learning rate: 0.00020504]
	Learning Rate: 0.000205044
	LOSS [training: 8.091984969503269 | validation: 7.824144711987464]
	TIME [epoch: 8.28 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.003231415255488		[learning rate: 0.0002048]
		[batch 20/20] avg loss: 8.109616784657902		[learning rate: 0.00020456]
	Learning Rate: 0.00020456
	LOSS [training: 8.056424099956693 | validation: 7.779097894609843]
	TIME [epoch: 8.28 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.013941367973477		[learning rate: 0.00020432]
		[batch 20/20] avg loss: 8.005358128037008		[learning rate: 0.00020408]
	Learning Rate: 0.000204077
	LOSS [training: 8.009649748005241 | validation: 7.692961713956575]
	TIME [epoch: 8.29 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.018800047363989		[learning rate: 0.00020384]
		[batch 20/20] avg loss: 7.884134460561067		[learning rate: 0.0002036]
	Learning Rate: 0.000203596
	LOSS [training: 7.951467253962528 | validation: 7.695146315307217]
	TIME [epoch: 8.29 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9635137180570625		[learning rate: 0.00020336]
		[batch 20/20] avg loss: 7.992801745049922		[learning rate: 0.00020312]
	Learning Rate: 0.000203116
	LOSS [training: 7.978157731553492 | validation: 7.777692694305021]
	TIME [epoch: 8.28 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.017317807972857		[learning rate: 0.00020288]
		[batch 20/20] avg loss: 8.040770899514797		[learning rate: 0.00020264]
	Learning Rate: 0.000202637
	LOSS [training: 8.029044353743831 | validation: 7.767570352417953]
	TIME [epoch: 8.28 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.13043389042294		[learning rate: 0.0002024]
		[batch 20/20] avg loss: 7.947839483702012		[learning rate: 0.00020216]
	Learning Rate: 0.000202159
	LOSS [training: 8.039136687062475 | validation: 7.784717066427005]
	TIME [epoch: 8.28 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.978974271481718		[learning rate: 0.00020192]
		[batch 20/20] avg loss: 8.097341916578154		[learning rate: 0.00020168]
	Learning Rate: 0.000201682
	LOSS [training: 8.038158094029935 | validation: 7.854385781234929]
	TIME [epoch: 8.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.138231181239416		[learning rate: 0.00020144]
		[batch 20/20] avg loss: 8.060787390776161		[learning rate: 0.00020121]
	Learning Rate: 0.000201206
	LOSS [training: 8.09950928600779 | validation: 7.831273377517961]
	TIME [epoch: 8.28 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.093226175602467		[learning rate: 0.00020097]
		[batch 20/20] avg loss: 8.080478121210026		[learning rate: 0.00020073]
	Learning Rate: 0.000200731
	LOSS [training: 8.086852148406246 | validation: 7.83034135761117]
	TIME [epoch: 8.28 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.010842764754711		[learning rate: 0.00020049]
		[batch 20/20] avg loss: 8.113577377112742		[learning rate: 0.00020026]
	Learning Rate: 0.000200258
	LOSS [training: 8.062210070933727 | validation: 7.808866442499984]
	TIME [epoch: 8.28 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.002771063544206		[learning rate: 0.00020002]
		[batch 20/20] avg loss: 8.088461796153888		[learning rate: 0.00019979]
	Learning Rate: 0.000199786
	LOSS [training: 8.045616429849044 | validation: 7.793951422905348]
	TIME [epoch: 8.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.086320383043226		[learning rate: 0.00019955]
		[batch 20/20] avg loss: 7.988447181559268		[learning rate: 0.00019931]
	Learning Rate: 0.000199314
	LOSS [training: 8.037383782301244 | validation: 7.780109301699117]
	TIME [epoch: 8.29 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.026011974120536		[learning rate: 0.00019908]
		[batch 20/20] avg loss: 7.893355310463255		[learning rate: 0.00019884]
	Learning Rate: 0.000198844
	LOSS [training: 7.959683642291897 | validation: 7.722391826452482]
	TIME [epoch: 8.29 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.847874150999253		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 8.058335387646215		[learning rate: 0.00019838]
	Learning Rate: 0.000198375
	LOSS [training: 7.953104769322732 | validation: 7.686578110800764]
	TIME [epoch: 8.28 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.934322440346366		[learning rate: 0.00019814]
		[batch 20/20] avg loss: 7.940928893389222		[learning rate: 0.00019791]
	Learning Rate: 0.000197907
	LOSS [training: 7.937625666867793 | validation: 7.654901139492409]
	TIME [epoch: 8.3 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.905061634731483		[learning rate: 0.00019767]
		[batch 20/20] avg loss: 7.951493127525542		[learning rate: 0.00019744]
	Learning Rate: 0.00019744
	LOSS [training: 7.928277381128514 | validation: 7.708912607515982]
	TIME [epoch: 8.29 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.058069086182133		[learning rate: 0.00019721]
		[batch 20/20] avg loss: 7.916452670356213		[learning rate: 0.00019697]
	Learning Rate: 0.000196975
	LOSS [training: 7.987260878269173 | validation: 7.810586296130006]
	TIME [epoch: 8.28 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.965770079480531		[learning rate: 0.00019674]
		[batch 20/20] avg loss: 8.118140021616432		[learning rate: 0.00019651]
	Learning Rate: 0.00019651
	LOSS [training: 8.04195505054848 | validation: 7.825078226273555]
	TIME [epoch: 8.28 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.039245734201181		[learning rate: 0.00019628]
		[batch 20/20] avg loss: 8.024121539393601		[learning rate: 0.00019605]
	Learning Rate: 0.000196046
	LOSS [training: 8.031683636797393 | validation: 7.752713776199164]
	TIME [epoch: 8.28 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.014387246163539		[learning rate: 0.00019582]
		[batch 20/20] avg loss: 8.048545318758134		[learning rate: 0.00019558]
	Learning Rate: 0.000195584
	LOSS [training: 8.031466282460837 | validation: 7.820701787832727]
	TIME [epoch: 8.31 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.05519781585257		[learning rate: 0.00019535]
		[batch 20/20] avg loss: 8.01644603093468		[learning rate: 0.00019512]
	Learning Rate: 0.000195123
	LOSS [training: 8.035821923393623 | validation: 7.803665121158352]
	TIME [epoch: 8.28 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.02553107158349		[learning rate: 0.00019489]
		[batch 20/20] avg loss: 8.05422124489194		[learning rate: 0.00019466]
	Learning Rate: 0.000194662
	LOSS [training: 8.039876158237714 | validation: 7.8304549922539]
	TIME [epoch: 8.28 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.161622525808662		[learning rate: 0.00019443]
		[batch 20/20] avg loss: 7.897646062073737		[learning rate: 0.0001942]
	Learning Rate: 0.000194203
	LOSS [training: 8.0296342939412 | validation: 7.8309395470993675]
	TIME [epoch: 8.28 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.980758238142921		[learning rate: 0.00019397]
		[batch 20/20] avg loss: 8.091081589050903		[learning rate: 0.00019375]
	Learning Rate: 0.000193745
	LOSS [training: 8.035919913596912 | validation: 7.873789357793512]
	TIME [epoch: 8.3 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.957873228891621		[learning rate: 0.00019352]
		[batch 20/20] avg loss: 8.147704515267437		[learning rate: 0.00019329]
	Learning Rate: 0.000193288
	LOSS [training: 8.052788872079528 | validation: 7.838596013363543]
	TIME [epoch: 8.29 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.900566926865169		[learning rate: 0.00019306]
		[batch 20/20] avg loss: 8.147855204641733		[learning rate: 0.00019283]
	Learning Rate: 0.000192832
	LOSS [training: 8.024211065753452 | validation: 7.828629328814596]
	TIME [epoch: 8.29 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9657648552396365		[learning rate: 0.0001926]
		[batch 20/20] avg loss: 8.052961058313977		[learning rate: 0.00019238]
	Learning Rate: 0.000192377
	LOSS [training: 8.009362956776807 | validation: 7.754240126173909]
	TIME [epoch: 8.29 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.989660672664232		[learning rate: 0.00019215]
		[batch 20/20] avg loss: 7.938645975135131		[learning rate: 0.00019192]
	Learning Rate: 0.000191923
	LOSS [training: 7.964153323899682 | validation: 7.697449597010669]
	TIME [epoch: 8.3 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.978458322972976		[learning rate: 0.0001917]
		[batch 20/20] avg loss: 7.90573988701709		[learning rate: 0.00019147]
	Learning Rate: 0.000191471
	LOSS [training: 7.942099104995035 | validation: 7.6870732918001785]
	TIME [epoch: 8.3 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.918895932105395		[learning rate: 0.00019124]
		[batch 20/20] avg loss: 7.993265268288491		[learning rate: 0.00019102]
	Learning Rate: 0.000191019
	LOSS [training: 7.956080600196944 | validation: 7.712487637137706]
	TIME [epoch: 8.28 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.884009449120077		[learning rate: 0.00019079]
		[batch 20/20] avg loss: 8.069246336716956		[learning rate: 0.00019057]
	Learning Rate: 0.000190569
	LOSS [training: 7.976627892918517 | validation: 7.750838721420822]
	TIME [epoch: 8.29 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.096359965317623		[learning rate: 0.00019034]
		[batch 20/20] avg loss: 7.867337866999884		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 7.981848916158755 | validation: 7.70898028778362]
	TIME [epoch: 8.29 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.940158151287223		[learning rate: 0.00018989]
		[batch 20/20] avg loss: 8.009471027529154		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 7.974814589408187 | validation: 7.788433394984542]
	TIME [epoch: 8.32 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.989751127368159		[learning rate: 0.00018945]
		[batch 20/20] avg loss: 7.982879704138673		[learning rate: 0.00018922]
	Learning Rate: 0.000189223
	LOSS [training: 7.986315415753417 | validation: 7.765361131591192]
	TIME [epoch: 8.29 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.013768284247984		[learning rate: 0.000189]
		[batch 20/20] avg loss: 7.960100371733593		[learning rate: 0.00018878]
	Learning Rate: 0.000188777
	LOSS [training: 7.986934327990788 | validation: 7.780663055421052]
	TIME [epoch: 8.29 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.040257703702025		[learning rate: 0.00018855]
		[batch 20/20] avg loss: 7.932493431965698		[learning rate: 0.00018833]
	Learning Rate: 0.000188332
	LOSS [training: 7.986375567833861 | validation: 7.787770510498094]
	TIME [epoch: 8.29 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.982244313550147		[learning rate: 0.00018811]
		[batch 20/20] avg loss: 8.016519085487975		[learning rate: 0.00018789]
	Learning Rate: 0.000187887
	LOSS [training: 7.999381699519063 | validation: 7.728288145786573]
	TIME [epoch: 8.31 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.007457416544536		[learning rate: 0.00018767]
		[batch 20/20] avg loss: 7.897752025005936		[learning rate: 0.00018744]
	Learning Rate: 0.000187444
	LOSS [training: 7.952604720775237 | validation: 7.680647242108652]
	TIME [epoch: 8.3 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.040491498993763		[learning rate: 0.00018722]
		[batch 20/20] avg loss: 7.877923825007967		[learning rate: 0.000187]
	Learning Rate: 0.000187002
	LOSS [training: 7.959207662000867 | validation: 7.703887637548345]
	TIME [epoch: 8.29 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.983452248302347		[learning rate: 0.00018678]
		[batch 20/20] avg loss: 7.887991350967935		[learning rate: 0.00018656]
	Learning Rate: 0.000186561
	LOSS [training: 7.9357217996351395 | validation: 7.656741695220761]
	TIME [epoch: 8.3 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.859222284736388		[learning rate: 0.00018634]
		[batch 20/20] avg loss: 8.05615450487303		[learning rate: 0.00018612]
	Learning Rate: 0.000186121
	LOSS [training: 7.95768839480471 | validation: 7.744997670071717]
	TIME [epoch: 8.31 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.925024160111711		[learning rate: 0.0001859]
		[batch 20/20] avg loss: 8.032395998232278		[learning rate: 0.00018568]
	Learning Rate: 0.000185682
	LOSS [training: 7.978710079171995 | validation: 7.773575301990181]
	TIME [epoch: 8.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.949805844215154		[learning rate: 0.00018546]
		[batch 20/20] avg loss: 8.047057694634287		[learning rate: 0.00018524]
	Learning Rate: 0.000185244
	LOSS [training: 7.99843176942472 | validation: 7.802270943371988]
	TIME [epoch: 8.29 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.930644333168329		[learning rate: 0.00018503]
		[batch 20/20] avg loss: 8.030437759388452		[learning rate: 0.00018481]
	Learning Rate: 0.000184807
	LOSS [training: 7.9805410462783914 | validation: 7.793894423203388]
	TIME [epoch: 8.3 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.019723069775646		[learning rate: 0.00018459]
		[batch 20/20] avg loss: 8.000953386319312		[learning rate: 0.00018437]
	Learning Rate: 0.000184371
	LOSS [training: 8.010338228047479 | validation: 7.8029628094993155]
	TIME [epoch: 8.29 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.08488092615263		[learning rate: 0.00018415]
		[batch 20/20] avg loss: 7.9738590812693015		[learning rate: 0.00018394]
	Learning Rate: 0.000183936
	LOSS [training: 8.029370003710968 | validation: 7.859702190656476]
	TIME [epoch: 8.31 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.1128691815759		[learning rate: 0.00018372]
		[batch 20/20] avg loss: 7.997910829190525		[learning rate: 0.0001835]
	Learning Rate: 0.000183502
	LOSS [training: 8.055390005383215 | validation: 7.877685336593]
	TIME [epoch: 8.29 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.042458879756243		[learning rate: 0.00018329]
		[batch 20/20] avg loss: 8.074866166688988		[learning rate: 0.00018307]
	Learning Rate: 0.000183069
	LOSS [training: 8.058662523222615 | validation: 7.866517243043838]
	TIME [epoch: 8.29 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.02787664241004		[learning rate: 0.00018285]
		[batch 20/20] avg loss: 8.083727510108583		[learning rate: 0.00018264]
	Learning Rate: 0.000182637
	LOSS [training: 8.055802076259308 | validation: 7.855939812848842]
	TIME [epoch: 8.3 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.015697473204634		[learning rate: 0.00018242]
		[batch 20/20] avg loss: 8.032927995620824		[learning rate: 0.00018221]
	Learning Rate: 0.000182207
	LOSS [training: 8.024312734412728 | validation: 7.8053475928544165]
	TIME [epoch: 8.31 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.970734027231666		[learning rate: 0.00018199]
		[batch 20/20] avg loss: 8.052244381219232		[learning rate: 0.00018178]
	Learning Rate: 0.000181777
	LOSS [training: 8.011489204225452 | validation: 7.818415138608782]
	TIME [epoch: 8.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.992264936382822		[learning rate: 0.00018156]
		[batch 20/20] avg loss: 8.00000320267743		[learning rate: 0.00018135]
	Learning Rate: 0.000181348
	LOSS [training: 7.996134069530127 | validation: 7.795874198148957]
	TIME [epoch: 8.29 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.104052380405497		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 7.875468012178509		[learning rate: 0.00018092]
	Learning Rate: 0.00018092
	LOSS [training: 7.989760196292002 | validation: 7.772920911334771]
	TIME [epoch: 8.29 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.03796265082163		[learning rate: 0.00018071]
		[batch 20/20] avg loss: 7.941059190764527		[learning rate: 0.00018049]
	Learning Rate: 0.000180493
	LOSS [training: 7.989510920793078 | validation: 7.8723415335970435]
	TIME [epoch: 8.29 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.009554582353964		[learning rate: 0.00018028]
		[batch 20/20] avg loss: 8.088971762138119		[learning rate: 0.00018007]
	Learning Rate: 0.000180068
	LOSS [training: 8.04926317224604 | validation: 7.850235346999342]
	TIME [epoch: 8.31 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.091809919789396		[learning rate: 0.00017986]
		[batch 20/20] avg loss: 8.017761983332772		[learning rate: 0.00017964]
	Learning Rate: 0.000179643
	LOSS [training: 8.054785951561083 | validation: 7.902148159686552]
	TIME [epoch: 8.29 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.073634483399577		[learning rate: 0.00017943]
		[batch 20/20] avg loss: 8.00298741250014		[learning rate: 0.00017922]
	Learning Rate: 0.000179219
	LOSS [training: 8.038310947949858 | validation: 7.887256197839733]
	TIME [epoch: 8.29 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.051920040191693		[learning rate: 0.00017901]
		[batch 20/20] avg loss: 8.025689821662297		[learning rate: 0.0001788]
	Learning Rate: 0.000178796
	LOSS [training: 8.038804930926993 | validation: 7.8295804269108356]
	TIME [epoch: 8.29 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.002294942371446		[learning rate: 0.00017859]
		[batch 20/20] avg loss: 8.065378471427133		[learning rate: 0.00017837]
	Learning Rate: 0.000178375
	LOSS [training: 8.03383670689929 | validation: 7.865538701085681]
	TIME [epoch: 8.31 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.07525155374731		[learning rate: 0.00017816]
		[batch 20/20] avg loss: 7.990813758245342		[learning rate: 0.00017795]
	Learning Rate: 0.000177954
	LOSS [training: 8.033032655996326 | validation: 7.841233623979487]
	TIME [epoch: 8.29 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.148132091198557		[learning rate: 0.00017774]
		[batch 20/20] avg loss: 7.903200608315745		[learning rate: 0.00017753]
	Learning Rate: 0.000177534
	LOSS [training: 8.02566634975715 | validation: 7.814858250555929]
	TIME [epoch: 8.29 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.976547909308671		[learning rate: 0.00017732]
		[batch 20/20] avg loss: 8.041547878038076		[learning rate: 0.00017712]
	Learning Rate: 0.000177115
	LOSS [training: 8.009047893673372 | validation: 7.762723083572807]
	TIME [epoch: 8.29 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.00003228604714		[learning rate: 0.00017691]
		[batch 20/20] avg loss: 7.953910290433898		[learning rate: 0.0001767]
	Learning Rate: 0.000176698
	LOSS [training: 7.976971288240518 | validation: 7.7834948786869855]
	TIME [epoch: 8.31 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.980963652473591		[learning rate: 0.00017649]
		[batch 20/20] avg loss: 7.974839075406152		[learning rate: 0.00017628]
	Learning Rate: 0.000176281
	LOSS [training: 7.977901363939871 | validation: 7.831371013923428]
	TIME [epoch: 8.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9970728775269535		[learning rate: 0.00017607]
		[batch 20/20] avg loss: 7.9834849455438475		[learning rate: 0.00017587]
	Learning Rate: 0.000175865
	LOSS [training: 7.9902789115354 | validation: 7.776828327855872]
	TIME [epoch: 8.29 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.945112394671907		[learning rate: 0.00017566]
		[batch 20/20] avg loss: 8.042370524296278		[learning rate: 0.00017545]
	Learning Rate: 0.00017545
	LOSS [training: 7.993741459484092 | validation: 7.8653294140950525]
	TIME [epoch: 8.29 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9093862734798766		[learning rate: 0.00017524]
		[batch 20/20] avg loss: 8.106093883191173		[learning rate: 0.00017504]
	Learning Rate: 0.000175036
	LOSS [training: 8.007740078335527 | validation: 7.859490163214402]
	TIME [epoch: 8.29 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.959041104539135		[learning rate: 0.00017483]
		[batch 20/20] avg loss: 8.1166032461603		[learning rate: 0.00017462]
	Learning Rate: 0.000174623
	LOSS [training: 8.037822175349717 | validation: 7.884834703293648]
	TIME [epoch: 8.31 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.045617025522024		[learning rate: 0.00017442]
		[batch 20/20] avg loss: 8.023085020608205		[learning rate: 0.00017421]
	Learning Rate: 0.000174212
	LOSS [training: 8.034351023065113 | validation: 7.848232745470505]
	TIME [epoch: 8.29 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9943092059185545		[learning rate: 0.00017401]
		[batch 20/20] avg loss: 7.995769338684838		[learning rate: 0.0001738]
	Learning Rate: 0.000173801
	LOSS [training: 7.995039272301696 | validation: 7.807535013919256]
	TIME [epoch: 8.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.0672025475765		[learning rate: 0.0001736]
		[batch 20/20] avg loss: 7.98263388240193		[learning rate: 0.00017339]
	Learning Rate: 0.000173391
	LOSS [training: 8.024918214989214 | validation: 7.882554157850652]
	TIME [epoch: 8.29 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.085750863367583		[learning rate: 0.00017319]
		[batch 20/20] avg loss: 8.02806344405973		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 8.056907153713656 | validation: 7.897475217203343]
	TIME [epoch: 8.31 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.10038403528421		[learning rate: 0.00017278]
		[batch 20/20] avg loss: 7.990323122325718		[learning rate: 0.00017257]
	Learning Rate: 0.000172574
	LOSS [training: 8.045353578804963 | validation: 7.85531218672711]
	TIME [epoch: 8.29 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.07320433592677		[learning rate: 0.00017237]
		[batch 20/20] avg loss: 7.9825148022855075		[learning rate: 0.00017217]
	Learning Rate: 0.000172167
	LOSS [training: 8.027859569106138 | validation: 7.821963393568862]
	TIME [epoch: 8.29 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.984456716472705		[learning rate: 0.00017196]
		[batch 20/20] avg loss: 7.9825414653950855		[learning rate: 0.00017176]
	Learning Rate: 0.00017176
	LOSS [training: 7.983499090933894 | validation: 7.755057552692919]
	TIME [epoch: 8.29 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.918203647513691		[learning rate: 0.00017156]
		[batch 20/20] avg loss: 7.986805984929591		[learning rate: 0.00017136]
	Learning Rate: 0.000171355
	LOSS [training: 7.952504816221641 | validation: 7.75939535322169]
	TIME [epoch: 8.31 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.965434012138418		[learning rate: 0.00017115]
		[batch 20/20] avg loss: 7.992039430927958		[learning rate: 0.00017095]
	Learning Rate: 0.000170951
	LOSS [training: 7.978736721533187 | validation: 7.778419843724002]
	TIME [epoch: 8.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9661162289257605		[learning rate: 0.00017075]
		[batch 20/20] avg loss: 7.97319937043456		[learning rate: 0.00017055]
	Learning Rate: 0.000170548
	LOSS [training: 7.969657799680161 | validation: 7.728711063746459]
	TIME [epoch: 8.3 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.99186804011584		[learning rate: 0.00017035]
		[batch 20/20] avg loss: 7.91896889739256		[learning rate: 0.00017015]
	Learning Rate: 0.000170146
	LOSS [training: 7.955418468754199 | validation: 7.679600660215552]
	TIME [epoch: 8.29 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.017861131851502		[learning rate: 0.00016994]
		[batch 20/20] avg loss: 7.8354526516231555		[learning rate: 0.00016974]
	Learning Rate: 0.000169744
	LOSS [training: 7.926656891737331 | validation: 7.6823942997063135]
	TIME [epoch: 8.29 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.898303764336224		[learning rate: 0.00016954]
		[batch 20/20] avg loss: 7.915879789857103		[learning rate: 0.00016934]
	Learning Rate: 0.000169344
	LOSS [training: 7.907091777096663 | validation: 7.634697800325872]
	TIME [epoch: 8.32 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.927992347423093		[learning rate: 0.00016914]
		[batch 20/20] avg loss: 7.843521529296629		[learning rate: 0.00016894]
	Learning Rate: 0.000168944
	LOSS [training: 7.88575693835986 | validation: 7.647892135947038]
	TIME [epoch: 8.29 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.863984592020303		[learning rate: 0.00016874]
		[batch 20/20] avg loss: 7.965740275687049		[learning rate: 0.00016855]
	Learning Rate: 0.000168546
	LOSS [training: 7.914862433853673 | validation: 7.619264007326345]
	TIME [epoch: 8.29 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.739831554475311		[learning rate: 0.00016835]
		[batch 20/20] avg loss: 8.026527909824406		[learning rate: 0.00016815]
	Learning Rate: 0.000168148
	LOSS [training: 7.883179732149858 | validation: 7.613978316458823]
	TIME [epoch: 8.29 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.825269826375282		[learning rate: 0.00016795]
		[batch 20/20] avg loss: 7.90482018491784		[learning rate: 0.00016775]
	Learning Rate: 0.000167752
	LOSS [training: 7.865045005646559 | validation: 7.619882115445119]
	TIME [epoch: 8.31 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.82961730725884		[learning rate: 0.00016755]
		[batch 20/20] avg loss: 7.886226390305202		[learning rate: 0.00016736]
	Learning Rate: 0.000167356
	LOSS [training: 7.857921848782022 | validation: 7.609349106688421]
	TIME [epoch: 8.29 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.888062956697581		[learning rate: 0.00016716]
		[batch 20/20] avg loss: 7.765701335105227		[learning rate: 0.00016696]
	Learning Rate: 0.000166961
	LOSS [training: 7.826882145901405 | validation: 7.561822842129167]
	TIME [epoch: 8.29 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.853526817993577		[learning rate: 0.00016676]
		[batch 20/20] avg loss: 7.793904031847434		[learning rate: 0.00016657]
	Learning Rate: 0.000166567
	LOSS [training: 7.823715424920505 | validation: 7.553266311959204]
	TIME [epoch: 8.29 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.826390696982706		[learning rate: 0.00016637]
		[batch 20/20] avg loss: 7.825641248236226		[learning rate: 0.00016617]
	Learning Rate: 0.000166174
	LOSS [training: 7.826015972609466 | validation: 7.557365074904619]
	TIME [epoch: 8.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.765059776301149		[learning rate: 0.00016598]
		[batch 20/20] avg loss: 7.794661934360244		[learning rate: 0.00016578]
	Learning Rate: 0.000165782
	LOSS [training: 7.779860855330698 | validation: 7.528034703780699]
	TIME [epoch: 8.3 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.699925979073127		[learning rate: 0.00016559]
		[batch 20/20] avg loss: 7.895753861820138		[learning rate: 0.00016539]
	Learning Rate: 0.000165391
	LOSS [training: 7.797839920446632 | validation: 7.520727699936496]
	TIME [epoch: 8.29 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.802299085072214		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 7.754308243531365		[learning rate: 0.000165]
	Learning Rate: 0.000165001
	LOSS [training: 7.778303664301788 | validation: 7.497569818838301]
	TIME [epoch: 8.29 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.736415013747222		[learning rate: 0.00016481]
		[batch 20/20] avg loss: 7.742522996735818		[learning rate: 0.00016461]
	Learning Rate: 0.000164612
	LOSS [training: 7.73946900524152 | validation: 7.50343276731456]
	TIME [epoch: 8.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.764422162763308		[learning rate: 0.00016442]
		[batch 20/20] avg loss: 7.718188943587681		[learning rate: 0.00016422]
	Learning Rate: 0.000164224
	LOSS [training: 7.741305553175496 | validation: 7.443683890527389]
	TIME [epoch: 8.32 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.765496781281316		[learning rate: 0.00016403]
		[batch 20/20] avg loss: 7.673336820138692		[learning rate: 0.00016384]
	Learning Rate: 0.000163836
	LOSS [training: 7.719416800710005 | validation: 7.466192030622816]
	TIME [epoch: 8.29 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.748822283159723		[learning rate: 0.00016364]
		[batch 20/20] avg loss: 7.7049557350181415		[learning rate: 0.00016345]
	Learning Rate: 0.00016345
	LOSS [training: 7.726889009088931 | validation: 7.477269210371944]
	TIME [epoch: 8.29 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.699988365426115		[learning rate: 0.00016326]
		[batch 20/20] avg loss: 7.786977738883664		[learning rate: 0.00016306]
	Learning Rate: 0.000163064
	LOSS [training: 7.743483052154889 | validation: 7.466405133656739]
	TIME [epoch: 8.29 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.80736499661383		[learning rate: 0.00016287]
		[batch 20/20] avg loss: 7.653019171075755		[learning rate: 0.00016268]
	Learning Rate: 0.00016268
	LOSS [training: 7.730192083844793 | validation: 7.471321571239996]
	TIME [epoch: 8.31 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.628950234348667		[learning rate: 0.00016249]
		[batch 20/20] avg loss: 7.806965062922034		[learning rate: 0.0001623]
	Learning Rate: 0.000162296
	LOSS [training: 7.71795764863535 | validation: 7.473100022009207]
	TIME [epoch: 8.29 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7671239539636		[learning rate: 0.0001621]
		[batch 20/20] avg loss: 7.670527116764083		[learning rate: 0.00016191]
	Learning Rate: 0.000161913
	LOSS [training: 7.718825535363843 | validation: 7.4328561864987375]
	TIME [epoch: 8.29 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8331146144835415		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 7.590883398368239		[learning rate: 0.00016153]
	Learning Rate: 0.000161531
	LOSS [training: 7.71199900642589 | validation: 7.420377045060451]
	TIME [epoch: 8.29 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.703000078078776		[learning rate: 0.00016134]
		[batch 20/20] avg loss: 7.70693141250333		[learning rate: 0.00016115]
	Learning Rate: 0.00016115
	LOSS [training: 7.704965745291053 | validation: 7.4298204696739765]
	TIME [epoch: 8.31 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.661166697245257		[learning rate: 0.00016096]
		[batch 20/20] avg loss: 7.748905631611924		[learning rate: 0.00016077]
	Learning Rate: 0.00016077
	LOSS [training: 7.70503616442859 | validation: 7.4133029952961875]
	TIME [epoch: 8.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.754331647961395		[learning rate: 0.00016058]
		[batch 20/20] avg loss: 7.602967903443634		[learning rate: 0.00016039]
	Learning Rate: 0.000160391
	LOSS [training: 7.678649775702515 | validation: 7.432873512549692]
	TIME [epoch: 8.29 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.617709696078445		[learning rate: 0.0001602]
		[batch 20/20] avg loss: 7.756913141463005		[learning rate: 0.00016001]
	Learning Rate: 0.000160012
	LOSS [training: 7.687311418770723 | validation: 7.428189197917911]
	TIME [epoch: 8.29 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.828396797297458		[learning rate: 0.00015982]
		[batch 20/20] avg loss: 7.587255421539835		[learning rate: 0.00015964]
	Learning Rate: 0.000159635
	LOSS [training: 7.707826109418647 | validation: 7.463663738971142]
	TIME [epoch: 8.29 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.650598175707588		[learning rate: 0.00015945]
		[batch 20/20] avg loss: 7.8106624472169965		[learning rate: 0.00015926]
	Learning Rate: 0.000159258
	LOSS [training: 7.730630311462292 | validation: 7.446455119647612]
	TIME [epoch: 8.32 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.61628138341086		[learning rate: 0.00015907]
		[batch 20/20] avg loss: 7.83729270114806		[learning rate: 0.00015888]
	Learning Rate: 0.000158883
	LOSS [training: 7.72678704227946 | validation: 7.440563539244514]
	TIME [epoch: 8.29 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6847348654801		[learning rate: 0.0001587]
		[batch 20/20] avg loss: 7.7424221647829725		[learning rate: 0.00015851]
	Learning Rate: 0.000158508
	LOSS [training: 7.713578515131536 | validation: 7.471389672681015]
	TIME [epoch: 8.29 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.752590579182693		[learning rate: 0.00015832]
		[batch 20/20] avg loss: 7.743561037385635		[learning rate: 0.00015813]
	Learning Rate: 0.000158134
	LOSS [training: 7.748075808284163 | validation: 7.460375456752808]
	TIME [epoch: 8.29 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.662241224161531		[learning rate: 0.00015795]
		[batch 20/20] avg loss: 7.762038065067593		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 7.712139644614561 | validation: 7.464021132060978]
	TIME [epoch: 8.32 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.736398054684701		[learning rate: 0.00015757]
		[batch 20/20] avg loss: 7.707113264215373		[learning rate: 0.00015739]
	Learning Rate: 0.000157389
	LOSS [training: 7.721755659450037 | validation: 7.4246459174248685]
	TIME [epoch: 8.3 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.692922137732088		[learning rate: 0.0001572]
		[batch 20/20] avg loss: 7.750549869580462		[learning rate: 0.00015702]
	Learning Rate: 0.000157018
	LOSS [training: 7.721736003656277 | validation: 7.434419272599751]
	TIME [epoch: 8.29 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.712501703013475		[learning rate: 0.00015683]
		[batch 20/20] avg loss: 7.804154576959786		[learning rate: 0.00015665]
	Learning Rate: 0.000156647
	LOSS [training: 7.758328139986631 | validation: 7.4930353178377125]
	TIME [epoch: 8.29 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.869370169415895		[learning rate: 0.00015646]
		[batch 20/20] avg loss: 7.616977505279317		[learning rate: 0.00015628]
	Learning Rate: 0.000156278
	LOSS [training: 7.743173837347607 | validation: 7.438366142949319]
	TIME [epoch: 8.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.69645805332641		[learning rate: 0.00015609]
		[batch 20/20] avg loss: 7.704721131792238		[learning rate: 0.00015591]
	Learning Rate: 0.000155909
	LOSS [training: 7.700589592559323 | validation: 7.41377842143409]
	TIME [epoch: 8.31 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.780881921213188		[learning rate: 0.00015573]
		[batch 20/20] avg loss: 7.634420348641217		[learning rate: 0.00015554]
	Learning Rate: 0.000155541
	LOSS [training: 7.707651134927202 | validation: 7.4339723309179355]
	TIME [epoch: 8.29 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.828965193204345		[learning rate: 0.00015536]
		[batch 20/20] avg loss: 7.624292603995967		[learning rate: 0.00015517]
	Learning Rate: 0.000155175
	LOSS [training: 7.726628898600157 | validation: 7.437491386129499]
	TIME [epoch: 8.29 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.637198931223689		[learning rate: 0.00015499]
		[batch 20/20] avg loss: 7.790515924064506		[learning rate: 0.00015481]
	Learning Rate: 0.000154809
	LOSS [training: 7.713857427644098 | validation: 7.414038829822234]
	TIME [epoch: 8.29 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.657770325216591		[learning rate: 0.00015463]
		[batch 20/20] avg loss: 7.803042081931328		[learning rate: 0.00015444]
	Learning Rate: 0.000154443
	LOSS [training: 7.730406203573958 | validation: 7.43637260748193]
	TIME [epoch: 8.31 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.699750996405383		[learning rate: 0.00015426]
		[batch 20/20] avg loss: 7.728745145644382		[learning rate: 0.00015408]
	Learning Rate: 0.000154079
	LOSS [training: 7.7142480710248815 | validation: 7.455639973527184]
	TIME [epoch: 8.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.875941708340866		[learning rate: 0.0001539]
		[batch 20/20] avg loss: 7.585048831371336		[learning rate: 0.00015372]
	Learning Rate: 0.000153716
	LOSS [training: 7.7304952698561 | validation: 7.443181029196376]
	TIME [epoch: 8.29 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.827936577374109		[learning rate: 0.00015353]
		[batch 20/20] avg loss: 7.694572758142608		[learning rate: 0.00015335]
	Learning Rate: 0.000153353
	LOSS [training: 7.761254667758356 | validation: 7.488281050185652]
	TIME [epoch: 8.29 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.751389126733497		[learning rate: 0.00015317]
		[batch 20/20] avg loss: 7.7410257572902355		[learning rate: 0.00015299]
	Learning Rate: 0.000152991
	LOSS [training: 7.746207442011865 | validation: 7.473987454973396]
	TIME [epoch: 8.3 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.711332511926559		[learning rate: 0.00015281]
		[batch 20/20] avg loss: 7.7229516106939995		[learning rate: 0.00015263]
	Learning Rate: 0.00015263
	LOSS [training: 7.717142061310281 | validation: 7.42448898690539]
	TIME [epoch: 8.29 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.684098813937209		[learning rate: 0.00015245]
		[batch 20/20] avg loss: 7.734997622145878		[learning rate: 0.00015227]
	Learning Rate: 0.00015227
	LOSS [training: 7.709548218041542 | validation: 7.436013003554457]
	TIME [epoch: 8.29 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.786063714461292		[learning rate: 0.00015209]
		[batch 20/20] avg loss: 7.6931981625069685		[learning rate: 0.00015191]
	Learning Rate: 0.000151911
	LOSS [training: 7.73963093848413 | validation: 7.472382608466704]
	TIME [epoch: 8.29 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.826572329207947		[learning rate: 0.00015173]
		[batch 20/20] avg loss: 7.735118248242332		[learning rate: 0.00015155]
	Learning Rate: 0.000151553
	LOSS [training: 7.780845288725141 | validation: 7.493341175514957]
	TIME [epoch: 8.29 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.833015661584443		[learning rate: 0.00015137]
		[batch 20/20] avg loss: 7.702826372125841		[learning rate: 0.0001512]
	Learning Rate: 0.000151195
	LOSS [training: 7.7679210168551425 | validation: 7.488625657505764]
	TIME [epoch: 8.31 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.635647998713525		[learning rate: 0.00015102]
		[batch 20/20] avg loss: 7.824115894092875		[learning rate: 0.00015084]
	Learning Rate: 0.000150839
	LOSS [training: 7.729881946403198 | validation: 7.480081484751388]
	TIME [epoch: 8.29 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.742862837841213		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 7.728759914905366		[learning rate: 0.00015048]
	Learning Rate: 0.000150483
	LOSS [training: 7.735811376373292 | validation: 7.50332917889544]
	TIME [epoch: 8.29 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.688075105257491		[learning rate: 0.00015031]
		[batch 20/20] avg loss: 7.808220932470053		[learning rate: 0.00015013]
	Learning Rate: 0.000150128
	LOSS [training: 7.7481480188637715 | validation: 7.50700308795742]
	TIME [epoch: 8.29 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.653062188634907		[learning rate: 0.00014995]
		[batch 20/20] avg loss: 7.787940259217713		[learning rate: 0.00014977]
	Learning Rate: 0.000149774
	LOSS [training: 7.720501223926311 | validation: 7.408419928560765]
	TIME [epoch: 8.31 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.639171766140737		[learning rate: 0.0001496]
		[batch 20/20] avg loss: 7.727057383521796		[learning rate: 0.00014942]
	Learning Rate: 0.000149421
	LOSS [training: 7.683114574831267 | validation: 7.432843672810685]
	TIME [epoch: 8.29 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.636216639973599		[learning rate: 0.00014924]
		[batch 20/20] avg loss: 7.781084048746683		[learning rate: 0.00014907]
	Learning Rate: 0.000149068
	LOSS [training: 7.70865034436014 | validation: 7.42798956263562]
	TIME [epoch: 8.29 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.756724356277518		[learning rate: 0.00014889]
		[batch 20/20] avg loss: 7.586717579799192		[learning rate: 0.00014872]
	Learning Rate: 0.000148716
	LOSS [training: 7.671720968038355 | validation: 7.418545862752468]
	TIME [epoch: 8.29 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.650758301448683		[learning rate: 0.00014854]
		[batch 20/20] avg loss: 7.6918806752732065		[learning rate: 0.00014837]
	Learning Rate: 0.000148366
	LOSS [training: 7.671319488360943 | validation: 7.395716262335128]
	TIME [epoch: 8.31 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.763779301337422		[learning rate: 0.00014819]
		[batch 20/20] avg loss: 7.5563611110933335		[learning rate: 0.00014802]
	Learning Rate: 0.000148016
	LOSS [training: 7.660070206215378 | validation: 7.368723435978703]
	TIME [epoch: 8.3 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.696327209341838		[learning rate: 0.00014784]
		[batch 20/20] avg loss: 7.555472726871882		[learning rate: 0.00014767]
	Learning Rate: 0.000147667
	LOSS [training: 7.625899968106862 | validation: 7.389527857419684]
	TIME [epoch: 8.3 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.777618957128972		[learning rate: 0.00014749]
		[batch 20/20] avg loss: 7.5506761537585705		[learning rate: 0.00014732]
	Learning Rate: 0.000147318
	LOSS [training: 7.664147555443772 | validation: 7.428336571556035]
	TIME [epoch: 8.3 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.69110141767853		[learning rate: 0.00014714]
		[batch 20/20] avg loss: 7.643639717227299		[learning rate: 0.00014697]
	Learning Rate: 0.000146971
	LOSS [training: 7.667370567452915 | validation: 7.411691482897309]
	TIME [epoch: 8.29 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.740017535353975		[learning rate: 0.0001468]
		[batch 20/20] avg loss: 7.610033221101051		[learning rate: 0.00014662]
	Learning Rate: 0.000146624
	LOSS [training: 7.675025378227514 | validation: 7.394234165731814]
	TIME [epoch: 8.32 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.679676663135469		[learning rate: 0.00014645]
		[batch 20/20] avg loss: 7.653659407061525		[learning rate: 0.00014628]
	Learning Rate: 0.000146278
	LOSS [training: 7.666668035098496 | validation: 7.35862666447182]
	TIME [epoch: 8.29 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.638878277298824		[learning rate: 0.00014611]
		[batch 20/20] avg loss: 7.658968938895535		[learning rate: 0.00014593]
	Learning Rate: 0.000145933
	LOSS [training: 7.64892360809718 | validation: 7.372243087291443]
	TIME [epoch: 8.3 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.650663289249435		[learning rate: 0.00014576]
		[batch 20/20] avg loss: 7.717517388134709		[learning rate: 0.00014559]
	Learning Rate: 0.000145589
	LOSS [training: 7.684090338692073 | validation: 7.389241506324036]
	TIME [epoch: 8.3 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.693215228295109		[learning rate: 0.00014542]
		[batch 20/20] avg loss: 7.600556280342758		[learning rate: 0.00014525]
	Learning Rate: 0.000145245
	LOSS [training: 7.6468857543189355 | validation: 7.412787903664956]
	TIME [epoch: 8.32 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.716854291595219		[learning rate: 0.00014507]
		[batch 20/20] avg loss: 7.55708897749616		[learning rate: 0.0001449]
	Learning Rate: 0.000144903
	LOSS [training: 7.63697163454569 | validation: 7.3867179730583965]
	TIME [epoch: 8.29 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.51588705953755		[learning rate: 0.00014473]
		[batch 20/20] avg loss: 7.71646996323822		[learning rate: 0.00014456]
	Learning Rate: 0.000144561
	LOSS [training: 7.616178511387885 | validation: 7.341058058869341]
	TIME [epoch: 8.3 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6153568995142304		[learning rate: 0.00014439]
		[batch 20/20] avg loss: 7.671889385989962		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 7.643623142752096 | validation: 7.361215781700276]
	TIME [epoch: 8.3 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.749962530840979		[learning rate: 0.00014405]
		[batch 20/20] avg loss: 7.6174761801316295		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 7.683719355486305 | validation: 7.399783547563217]
	TIME [epoch: 8.31 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6865366515185		[learning rate: 0.00014371]
		[batch 20/20] avg loss: 7.665277573950313		[learning rate: 0.00014354]
	Learning Rate: 0.00014354
	LOSS [training: 7.675907112734407 | validation: 7.406813397987172]
	TIME [epoch: 8.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.661973733174529		[learning rate: 0.00014337]
		[batch 20/20] avg loss: 7.657188534125955		[learning rate: 0.0001432]
	Learning Rate: 0.000143202
	LOSS [training: 7.659581133650242 | validation: 7.390648931977826]
	TIME [epoch: 8.3 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.636947281936716		[learning rate: 0.00014303]
		[batch 20/20] avg loss: 7.651568501223399		[learning rate: 0.00014286]
	Learning Rate: 0.000142864
	LOSS [training: 7.6442578915800565 | validation: 7.373649965276871]
	TIME [epoch: 8.3 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6562846847618236		[learning rate: 0.0001427]
		[batch 20/20] avg loss: 7.556292466201407		[learning rate: 0.00014253]
	Learning Rate: 0.000142527
	LOSS [training: 7.606288575481615 | validation: 7.32703529535916]
	TIME [epoch: 8.29 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.73729012148064		[learning rate: 0.00014236]
		[batch 20/20] avg loss: 7.4026272353071105		[learning rate: 0.00014219]
	Learning Rate: 0.000142191
	LOSS [training: 7.569958678393874 | validation: 7.296023195591842]
	TIME [epoch: 8.32 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.628877727709119		[learning rate: 0.00014202]
		[batch 20/20] avg loss: 7.503133618519314		[learning rate: 0.00014186]
	Learning Rate: 0.000141855
	LOSS [training: 7.5660056731142165 | validation: 7.310943309008671]
	TIME [epoch: 8.29 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.380322631557928		[learning rate: 0.00014169]
		[batch 20/20] avg loss: 7.775288988212461		[learning rate: 0.00014152]
	Learning Rate: 0.000141521
	LOSS [training: 7.577805809885193 | validation: 7.351641105951813]
	TIME [epoch: 8.29 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.715108873468355		[learning rate: 0.00014135]
		[batch 20/20] avg loss: 7.4583749706208895		[learning rate: 0.00014119]
	Learning Rate: 0.000141187
	LOSS [training: 7.586741922044624 | validation: 7.322990460098506]
	TIME [epoch: 8.29 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.732665197563527		[learning rate: 0.00014102]
		[batch 20/20] avg loss: 7.50114042428397		[learning rate: 0.00014085]
	Learning Rate: 0.000140854
	LOSS [training: 7.616902810923749 | validation: 7.334156125287026]
	TIME [epoch: 8.32 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.675658865851153		[learning rate: 0.00014069]
		[batch 20/20] avg loss: 7.586120295291444		[learning rate: 0.00014052]
	Learning Rate: 0.000140522
	LOSS [training: 7.630889580571301 | validation: 7.369547814781249]
	TIME [epoch: 8.29 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.632391575644723		[learning rate: 0.00014036]
		[batch 20/20] avg loss: 7.610999380731519		[learning rate: 0.00014019]
	Learning Rate: 0.00014019
	LOSS [training: 7.621695478188123 | validation: 7.337121875322442]
	TIME [epoch: 8.29 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.736454471020338		[learning rate: 0.00014002]
		[batch 20/20] avg loss: 7.504564469721456		[learning rate: 0.00013986]
	Learning Rate: 0.00013986
	LOSS [training: 7.620509470370898 | validation: 7.354527121031172]
	TIME [epoch: 8.3 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.605919885032492		[learning rate: 0.00013969]
		[batch 20/20] avg loss: 7.696170856792553		[learning rate: 0.00013953]
	Learning Rate: 0.00013953
	LOSS [training: 7.651045370912523 | validation: 7.350600094080477]
	TIME [epoch: 8.31 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.678488035262487		[learning rate: 0.00013937]
		[batch 20/20] avg loss: 7.601487862875452		[learning rate: 0.0001392]
	Learning Rate: 0.000139201
	LOSS [training: 7.639987949068967 | validation: 7.364696504815659]
	TIME [epoch: 8.3 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.779830399286648		[learning rate: 0.00013904]
		[batch 20/20] avg loss: 7.50636065396346		[learning rate: 0.00013887]
	Learning Rate: 0.000138872
	LOSS [training: 7.643095526625052 | validation: 7.371195365391463]
	TIME [epoch: 8.29 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.63443503538177		[learning rate: 0.00013871]
		[batch 20/20] avg loss: 7.651940514556081		[learning rate: 0.00013854]
	Learning Rate: 0.000138545
	LOSS [training: 7.643187774968925 | validation: 7.3963213674863955]
	TIME [epoch: 8.29 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.678436208920017		[learning rate: 0.00013838]
		[batch 20/20] avg loss: 7.730165975882369		[learning rate: 0.00013822]
	Learning Rate: 0.000138218
	LOSS [training: 7.7043010924011925 | validation: 7.4050984116675895]
	TIME [epoch: 8.29 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.614247213412805		[learning rate: 0.00013805]
		[batch 20/20] avg loss: 7.744318747474367		[learning rate: 0.00013789]
	Learning Rate: 0.000137892
	LOSS [training: 7.679282980443586 | validation: 7.414809836316582]
	TIME [epoch: 8.32 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.684105760073969		[learning rate: 0.00013773]
		[batch 20/20] avg loss: 7.688282280449809		[learning rate: 0.00013757]
	Learning Rate: 0.000137567
	LOSS [training: 7.6861940202618895 | validation: 7.420341420322501]
	TIME [epoch: 8.29 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7406069628413245		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 7.552153136659925		[learning rate: 0.00013724]
	Learning Rate: 0.000137242
	LOSS [training: 7.646380049750626 | validation: 7.369757756053101]
	TIME [epoch: 8.29 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.57821765140704		[learning rate: 0.00013708]
		[batch 20/20] avg loss: 7.682830997918494		[learning rate: 0.00013692]
	Learning Rate: 0.000136918
	LOSS [training: 7.630524324662767 | validation: 7.349899271405118]
	TIME [epoch: 8.29 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.652645135189454		[learning rate: 0.00013676]
		[batch 20/20] avg loss: 7.586761679638153		[learning rate: 0.0001366]
	Learning Rate: 0.000136595
	LOSS [training: 7.6197034074138035 | validation: 7.365983436900574]
	TIME [epoch: 8.32 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.5081471273862075		[learning rate: 0.00013643]
		[batch 20/20] avg loss: 7.795696841177597		[learning rate: 0.00013627]
	Learning Rate: 0.000136273
	LOSS [training: 7.651921984281904 | validation: 7.422748484811736]
	TIME [epoch: 8.3 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.762194338161825		[learning rate: 0.00013611]
		[batch 20/20] avg loss: 7.627506785842499		[learning rate: 0.00013595]
	Learning Rate: 0.000135952
	LOSS [training: 7.694850562002162 | validation: 7.424497906009073]
	TIME [epoch: 8.29 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.644202464952636		[learning rate: 0.00013579]
		[batch 20/20] avg loss: 7.6733587306117155		[learning rate: 0.00013563]
	Learning Rate: 0.000135631
	LOSS [training: 7.658780597782178 | validation: 7.422249597187936]
	TIME [epoch: 8.3 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.785070755086568		[learning rate: 0.00013547]
		[batch 20/20] avg loss: 7.617061678252706		[learning rate: 0.00013531]
	Learning Rate: 0.000135311
	LOSS [training: 7.701066216669636 | validation: 7.4371326826926865]
	TIME [epoch: 8.3 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.654236219507223		[learning rate: 0.00013515]
		[batch 20/20] avg loss: 7.774765124857382		[learning rate: 0.00013499]
	Learning Rate: 0.000134992
	LOSS [training: 7.714500672182302 | validation: 7.454048448279927]
	TIME [epoch: 8.3 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.692002456555832		[learning rate: 0.00013483]
		[batch 20/20] avg loss: 7.724173467967502		[learning rate: 0.00013467]
	Learning Rate: 0.000134673
	LOSS [training: 7.708087962261667 | validation: 7.423567989605977]
	TIME [epoch: 8.29 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.678963280601988		[learning rate: 0.00013451]
		[batch 20/20] avg loss: 7.689990370312924		[learning rate: 0.00013436]
	Learning Rate: 0.000134356
	LOSS [training: 7.684476825457456 | validation: 7.439047842774623]
	TIME [epoch: 8.29 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.69722458196444		[learning rate: 0.0001342]
		[batch 20/20] avg loss: 7.75489747831457		[learning rate: 0.00013404]
	Learning Rate: 0.000134039
	LOSS [training: 7.7260610301395065 | validation: 7.478314192408186]
	TIME [epoch: 8.29 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.772746123419904		[learning rate: 0.00013388]
		[batch 20/20] avg loss: 7.730525767996996		[learning rate: 0.00013372]
	Learning Rate: 0.000133723
	LOSS [training: 7.751635945708449 | validation: 7.464234665142598]
	TIME [epoch: 8.32 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.74050250532382		[learning rate: 0.00013356]
		[batch 20/20] avg loss: 7.712510920276818		[learning rate: 0.00013341]
	Learning Rate: 0.000133407
	LOSS [training: 7.72650671280032 | validation: 7.477793334703936]
	TIME [epoch: 8.29 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7694281846080475		[learning rate: 0.00013325]
		[batch 20/20] avg loss: 7.737853015914735		[learning rate: 0.00013309]
	Learning Rate: 0.000133093
	LOSS [training: 7.753640600261393 | validation: 7.447428446223231]
	TIME [epoch: 8.29 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.750346688324166		[learning rate: 0.00013294]
		[batch 20/20] avg loss: 7.727207595965517		[learning rate: 0.00013278]
	Learning Rate: 0.000132779
	LOSS [training: 7.738777142144841 | validation: 7.461808147919565]
	TIME [epoch: 8.29 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.670385489528597		[learning rate: 0.00013262]
		[batch 20/20] avg loss: 7.783442206284984		[learning rate: 0.00013247]
	Learning Rate: 0.000132465
	LOSS [training: 7.726913847906791 | validation: 7.478547647257352]
	TIME [epoch: 8.31 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.702881281563282		[learning rate: 0.00013231]
		[batch 20/20] avg loss: 7.727616621877604		[learning rate: 0.00013215]
	Learning Rate: 0.000132153
	LOSS [training: 7.715248951720443 | validation: 7.445580872102908]
	TIME [epoch: 8.3 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6590759497631336		[learning rate: 0.000132]
		[batch 20/20] avg loss: 7.757798818715284		[learning rate: 0.00013184]
	Learning Rate: 0.000131841
	LOSS [training: 7.708437384239209 | validation: 7.456107868440586]
	TIME [epoch: 8.29 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.747293673739345		[learning rate: 0.00013169]
		[batch 20/20] avg loss: 7.722887186529069		[learning rate: 0.00013153]
	Learning Rate: 0.00013153
	LOSS [training: 7.735090430134209 | validation: 7.496266657082161]
	TIME [epoch: 8.29 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.703464405884084		[learning rate: 0.00013138]
		[batch 20/20] avg loss: 7.809674155290826		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 7.7565692805874535 | validation: 7.486928286537366]
	TIME [epoch: 8.3 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.700927846139654		[learning rate: 0.00013107]
		[batch 20/20] avg loss: 7.7227246316216265		[learning rate: 0.00013091]
	Learning Rate: 0.00013091
	LOSS [training: 7.711826238880642 | validation: 7.419150075710219]
	TIME [epoch: 8.31 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.717683631424146		[learning rate: 0.00013076]
		[batch 20/20] avg loss: 7.731114623922221		[learning rate: 0.0001306]
	Learning Rate: 0.000130602
	LOSS [training: 7.724399127673183 | validation: 7.434197010892625]
	TIME [epoch: 8.3 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.751738761712384		[learning rate: 0.00013045]
		[batch 20/20] avg loss: 7.673024310732217		[learning rate: 0.00013029]
	Learning Rate: 0.000130294
	LOSS [training: 7.7123815362223 | validation: 7.468141914421507]
	TIME [epoch: 8.29 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.777134034690896		[learning rate: 0.00013014]
		[batch 20/20] avg loss: 7.6971129807582965		[learning rate: 0.00012999]
	Learning Rate: 0.000129986
	LOSS [training: 7.737123507724595 | validation: 7.491670169915173]
	TIME [epoch: 8.29 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.704047382859322		[learning rate: 0.00012983]
		[batch 20/20] avg loss: 7.789442653051407		[learning rate: 0.00012968]
	Learning Rate: 0.00012968
	LOSS [training: 7.746745017955365 | validation: 7.514420715219724]
	TIME [epoch: 8.31 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7407613338605925		[learning rate: 0.00012953]
		[batch 20/20] avg loss: 7.749023982957112		[learning rate: 0.00012937]
	Learning Rate: 0.000129374
	LOSS [training: 7.744892658408853 | validation: 7.487141230952067]
	TIME [epoch: 8.29 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7149724440072305		[learning rate: 0.00012922]
		[batch 20/20] avg loss: 7.798990408096327		[learning rate: 0.00012907]
	Learning Rate: 0.000129069
	LOSS [training: 7.7569814260517775 | validation: 7.501456892613109]
	TIME [epoch: 8.29 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.765956289259115		[learning rate: 0.00012892]
		[batch 20/20] avg loss: 7.790621357389794		[learning rate: 0.00012876]
	Learning Rate: 0.000128764
	LOSS [training: 7.778288823324455 | validation: 7.52853247624107]
	TIME [epoch: 8.29 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.817109011916065		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 7.723211113971265		[learning rate: 0.00012846]
	Learning Rate: 0.00012846
	LOSS [training: 7.770160062943664 | validation: 7.5163741493303755]
	TIME [epoch: 8.31 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.860278340722419		[learning rate: 0.00012831]
		[batch 20/20] avg loss: 7.6748219708067555		[learning rate: 0.00012816]
	Learning Rate: 0.000128157
	LOSS [training: 7.767550155764589 | validation: 7.506692982546033]
	TIME [epoch: 8.3 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7465961344254834		[learning rate: 0.00012801]
		[batch 20/20] avg loss: 7.766990975056831		[learning rate: 0.00012786]
	Learning Rate: 0.000127855
	LOSS [training: 7.756793554741161 | validation: 7.519872112513392]
	TIME [epoch: 8.29 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.724670037825132		[learning rate: 0.0001277]
		[batch 20/20] avg loss: 7.830668309014347		[learning rate: 0.00012755]
	Learning Rate: 0.000127553
	LOSS [training: 7.77766917341974 | validation: 7.531674179894891]
	TIME [epoch: 8.29 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.798301426853921		[learning rate: 0.0001274]
		[batch 20/20] avg loss: 7.853732236027852		[learning rate: 0.00012725]
	Learning Rate: 0.000127253
	LOSS [training: 7.826016831440886 | validation: 7.56967977574203]
	TIME [epoch: 8.3 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.901454050567702		[learning rate: 0.0001271]
		[batch 20/20] avg loss: 7.779757490426787		[learning rate: 0.00012695]
	Learning Rate: 0.000126952
	LOSS [training: 7.840605770497246 | validation: 7.634012033773711]
	TIME [epoch: 8.31 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.913992471042091		[learning rate: 0.0001268]
		[batch 20/20] avg loss: 7.788336972676329		[learning rate: 0.00012665]
	Learning Rate: 0.000126653
	LOSS [training: 7.851164721859209 | validation: 7.578054393222201]
	TIME [epoch: 8.3 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8685070735425455		[learning rate: 0.0001265]
		[batch 20/20] avg loss: 7.820151877421307		[learning rate: 0.00012635]
	Learning Rate: 0.000126354
	LOSS [training: 7.8443294754819295 | validation: 7.612364450943646]
	TIME [epoch: 8.29 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.903907100247876		[learning rate: 0.00012621]
		[batch 20/20] avg loss: 7.766386719678745		[learning rate: 0.00012606]
	Learning Rate: 0.000126056
	LOSS [training: 7.835146909963309 | validation: 7.569620484865612]
	TIME [epoch: 8.29 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.788925433974955		[learning rate: 0.00012591]
		[batch 20/20] avg loss: 7.807407408319118		[learning rate: 0.00012576]
	Learning Rate: 0.000125759
	LOSS [training: 7.798166421147036 | validation: 7.595926356317196]
	TIME [epoch: 8.32 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.810688907336248		[learning rate: 0.00012561]
		[batch 20/20] avg loss: 7.863676596532724		[learning rate: 0.00012546]
	Learning Rate: 0.000125462
	LOSS [training: 7.8371827519344865 | validation: 7.57977197790076]
	TIME [epoch: 8.29 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.737926324424452		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 7.845631554783123		[learning rate: 0.00012517]
	Learning Rate: 0.000125166
	LOSS [training: 7.791778939603786 | validation: 7.541593084537252]
	TIME [epoch: 8.29 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.906444481858154		[learning rate: 0.00012502]
		[batch 20/20] avg loss: 7.664915179126721		[learning rate: 0.00012487]
	Learning Rate: 0.000124871
	LOSS [training: 7.785679830492438 | validation: 7.5278933666818455]
	TIME [epoch: 8.29 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.869258985181871		[learning rate: 0.00012472]
		[batch 20/20] avg loss: 7.674726575997875		[learning rate: 0.00012458]
	Learning Rate: 0.000124576
	LOSS [training: 7.771992780589871 | validation: 7.521964605865039]
	TIME [epoch: 8.31 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.755052357729346		[learning rate: 0.00012443]
		[batch 20/20] avg loss: 7.764998369680995		[learning rate: 0.00012428]
	Learning Rate: 0.000124283
	LOSS [training: 7.760025363705171 | validation: 7.484823888954601]
	TIME [epoch: 8.3 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.697259858233496		[learning rate: 0.00012414]
		[batch 20/20] avg loss: 7.753638616338478		[learning rate: 0.00012399]
	Learning Rate: 0.000123989
	LOSS [training: 7.725449237285986 | validation: 7.486882330968295]
	TIME [epoch: 8.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.750882547433617		[learning rate: 0.00012384]
		[batch 20/20] avg loss: 7.7076682853329945		[learning rate: 0.0001237]
	Learning Rate: 0.000123697
	LOSS [training: 7.729275416383307 | validation: 7.465216832339424]
	TIME [epoch: 8.29 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.740333277930875		[learning rate: 0.00012355]
		[batch 20/20] avg loss: 7.79787736343034		[learning rate: 0.00012341]
	Learning Rate: 0.000123405
	LOSS [training: 7.769105320680606 | validation: 7.536107440743708]
	TIME [epoch: 8.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.80742047756325		[learning rate: 0.00012326]
		[batch 20/20] avg loss: 7.751072542340853		[learning rate: 0.00012311]
	Learning Rate: 0.000123114
	LOSS [training: 7.779246509952051 | validation: 7.527873928781785]
	TIME [epoch: 8.32 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.805810512665633		[learning rate: 0.00012297]
		[batch 20/20] avg loss: 7.7661056982324626		[learning rate: 0.00012282]
	Learning Rate: 0.000122824
	LOSS [training: 7.785958105449048 | validation: 7.526046807971343]
	TIME [epoch: 8.29 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.74464986993255		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 7.835338595782656		[learning rate: 0.00012253]
	Learning Rate: 0.000122534
	LOSS [training: 7.7899942328576035 | validation: 7.519468332094307]
	TIME [epoch: 8.29 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.687757461847768		[learning rate: 0.00012239]
		[batch 20/20] avg loss: 7.886752566749746		[learning rate: 0.00012224]
	Learning Rate: 0.000122245
	LOSS [training: 7.7872550142987595 | validation: 7.539295211376162]
	TIME [epoch: 8.29 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.69892413472305		[learning rate: 0.0001221]
		[batch 20/20] avg loss: 7.839599659638594		[learning rate: 0.00012196]
	Learning Rate: 0.000121957
	LOSS [training: 7.769261897180823 | validation: 7.508058005932863]
	TIME [epoch: 8.32 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.666239867719416		[learning rate: 0.00012181]
		[batch 20/20] avg loss: 7.839961803959648		[learning rate: 0.00012167]
	Learning Rate: 0.000121669
	LOSS [training: 7.753100835839531 | validation: 7.494124728184138]
	TIME [epoch: 8.29 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.733318524674313		[learning rate: 0.00012153]
		[batch 20/20] avg loss: 7.752481348330038		[learning rate: 0.00012138]
	Learning Rate: 0.000121382
	LOSS [training: 7.742899936502175 | validation: 7.527287173648091]
	TIME [epoch: 8.3 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.695336302026357		[learning rate: 0.00012124]
		[batch 20/20] avg loss: 7.827760939187172		[learning rate: 0.0001211]
	Learning Rate: 0.000121096
	LOSS [training: 7.7615486206067645 | validation: 7.524585970361548]
	TIME [epoch: 8.29 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.809636237067991		[learning rate: 0.00012095]
		[batch 20/20] avg loss: 7.734313046588942		[learning rate: 0.00012081]
	Learning Rate: 0.00012081
	LOSS [training: 7.771974641828467 | validation: 7.503484039626803]
	TIME [epoch: 8.31 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.691386803654661		[learning rate: 0.00012067]
		[batch 20/20] avg loss: 7.836375040378383		[learning rate: 0.00012052]
	Learning Rate: 0.000120525
	LOSS [training: 7.76388092201652 | validation: 7.5398606702875215]
	TIME [epoch: 8.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.805128282191008		[learning rate: 0.00012038]
		[batch 20/20] avg loss: 7.759404806336289		[learning rate: 0.00012024]
	Learning Rate: 0.000120241
	LOSS [training: 7.782266544263651 | validation: 7.543339105646719]
	TIME [epoch: 8.3 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.772523107147168		[learning rate: 0.0001201]
		[batch 20/20] avg loss: 7.818876016366343		[learning rate: 0.00011996]
	Learning Rate: 0.000119957
	LOSS [training: 7.795699561756756 | validation: 7.536786303303565]
	TIME [epoch: 8.3 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.735414734925406		[learning rate: 0.00011982]
		[batch 20/20] avg loss: 7.813987907141815		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 7.774701321033611 | validation: 7.561476464262258]
	TIME [epoch: 8.29 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.811805771149442		[learning rate: 0.00011953]
		[batch 20/20] avg loss: 7.8096625850589305		[learning rate: 0.00011939]
	Learning Rate: 0.000119392
	LOSS [training: 7.810734178104188 | validation: 7.556537484245182]
	TIME [epoch: 8.32 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.828831465179609		[learning rate: 0.00011925]
		[batch 20/20] avg loss: 7.822654482892817		[learning rate: 0.00011911]
	Learning Rate: 0.00011911
	LOSS [training: 7.8257429740362126 | validation: 7.58195944300403]
	TIME [epoch: 8.3 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.922051713702453		[learning rate: 0.00011897]
		[batch 20/20] avg loss: 7.761852570947537		[learning rate: 0.00011883]
	Learning Rate: 0.000118829
	LOSS [training: 7.8419521423249945 | validation: 7.575889017538238]
	TIME [epoch: 8.3 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7044707879961205		[learning rate: 0.00011869]
		[batch 20/20] avg loss: 7.922286032392023		[learning rate: 0.00011855]
	Learning Rate: 0.000118549
	LOSS [training: 7.8133784101940735 | validation: 7.54771992116353]
	TIME [epoch: 8.29 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.789649480484462		[learning rate: 0.00011841]
		[batch 20/20] avg loss: 7.791411205461332		[learning rate: 0.00011827]
	Learning Rate: 0.000118269
	LOSS [training: 7.790530342972896 | validation: 7.493799179074189]
	TIME [epoch: 8.31 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.819641998206009		[learning rate: 0.00011813]
		[batch 20/20] avg loss: 7.749185035470634		[learning rate: 0.00011799]
	Learning Rate: 0.00011799
	LOSS [training: 7.784413516838322 | validation: 7.5204018852010375]
	TIME [epoch: 8.3 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.76150955358968		[learning rate: 0.00011785]
		[batch 20/20] avg loss: 7.782506751430729		[learning rate: 0.00011771]
	Learning Rate: 0.000117712
	LOSS [training: 7.772008152510206 | validation: 7.528555409113051]
	TIME [epoch: 8.29 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.712426424276647		[learning rate: 0.00011757]
		[batch 20/20] avg loss: 7.8413488813400125		[learning rate: 0.00011743]
	Learning Rate: 0.000117434
	LOSS [training: 7.7768876528083295 | validation: 7.537180854870352]
	TIME [epoch: 8.29 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.745777872557008		[learning rate: 0.0001173]
		[batch 20/20] avg loss: 7.850870348737082		[learning rate: 0.00011716]
	Learning Rate: 0.000117157
	LOSS [training: 7.798324110647043 | validation: 7.524689437725552]
	TIME [epoch: 8.31 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.711793710721395		[learning rate: 0.00011702]
		[batch 20/20] avg loss: 7.859975315146293		[learning rate: 0.00011688]
	Learning Rate: 0.000116881
	LOSS [training: 7.7858845129338405 | validation: 7.537641627657024]
	TIME [epoch: 8.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.886965996765551		[learning rate: 0.00011674]
		[batch 20/20] avg loss: 7.736301354488181		[learning rate: 0.00011661]
	Learning Rate: 0.000116605
	LOSS [training: 7.811633675626867 | validation: 7.548989370688346]
	TIME [epoch: 8.29 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.869914515621645		[learning rate: 0.00011647]
		[batch 20/20] avg loss: 7.724726731643267		[learning rate: 0.00011633]
	Learning Rate: 0.00011633
	LOSS [training: 7.797320623632457 | validation: 7.4979483538163585]
	TIME [epoch: 8.29 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.753145089669582		[learning rate: 0.00011619]
		[batch 20/20] avg loss: 7.8320741485238115		[learning rate: 0.00011606]
	Learning Rate: 0.000116056
	LOSS [training: 7.792609619096696 | validation: 7.5423597481700835]
	TIME [epoch: 8.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.870212518202754		[learning rate: 0.00011592]
		[batch 20/20] avg loss: 7.7109545541072695		[learning rate: 0.00011578]
	Learning Rate: 0.000115782
	LOSS [training: 7.7905835361550135 | validation: 7.564314999822436]
	TIME [epoch: 8.32 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.79640641446416		[learning rate: 0.00011565]
		[batch 20/20] avg loss: 7.815521499960258		[learning rate: 0.00011551]
	Learning Rate: 0.000115509
	LOSS [training: 7.805963957212208 | validation: 7.5546284389624265]
	TIME [epoch: 8.3 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.687408603270792		[learning rate: 0.00011537]
		[batch 20/20] avg loss: 7.888350029974556		[learning rate: 0.00011524]
	Learning Rate: 0.000115236
	LOSS [training: 7.787879316622676 | validation: 7.517666174699226]
	TIME [epoch: 8.29 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.79184850700185		[learning rate: 0.0001151]
		[batch 20/20] avg loss: 7.791531889943505		[learning rate: 0.00011496]
	Learning Rate: 0.000114965
	LOSS [training: 7.791690198472677 | validation: 7.531382773472044]
	TIME [epoch: 8.29 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7724367987077185		[learning rate: 0.00011483]
		[batch 20/20] avg loss: 7.776666119093576		[learning rate: 0.00011469]
	Learning Rate: 0.000114693
	LOSS [training: 7.774551458900648 | validation: 7.521563060411389]
	TIME [epoch: 8.32 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.841607903873299		[learning rate: 0.00011456]
		[batch 20/20] avg loss: 7.715699772495365		[learning rate: 0.00011442]
	Learning Rate: 0.000114423
	LOSS [training: 7.778653838184333 | validation: 7.561513056110047]
	TIME [epoch: 8.29 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.705434852075723		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 7.88357030614174		[learning rate: 0.00011415]
	Learning Rate: 0.000114153
	LOSS [training: 7.794502579108732 | validation: 7.520517444375416]
	TIME [epoch: 8.29 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.832502203150445		[learning rate: 0.00011402]
		[batch 20/20] avg loss: 7.730110423331887		[learning rate: 0.00011388]
	Learning Rate: 0.000113884
	LOSS [training: 7.781306313241167 | validation: 7.540494353145804]
	TIME [epoch: 8.29 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.750326946059606		[learning rate: 0.00011375]
		[batch 20/20] avg loss: 7.850219568807107		[learning rate: 0.00011362]
	Learning Rate: 0.000113615
	LOSS [training: 7.800273257433356 | validation: 7.552829358045327]
	TIME [epoch: 8.31 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.872858134616114		[learning rate: 0.00011348]
		[batch 20/20] avg loss: 7.744263011573311		[learning rate: 0.00011335]
	Learning Rate: 0.000113347
	LOSS [training: 7.808560573094711 | validation: 7.574428090754347]
	TIME [epoch: 8.3 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.832872534145119		[learning rate: 0.00011321]
		[batch 20/20] avg loss: 7.776264849801312		[learning rate: 0.00011308]
	Learning Rate: 0.00011308
	LOSS [training: 7.804568691973216 | validation: 7.590340754307596]
	TIME [epoch: 8.29 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.854454430532269		[learning rate: 0.00011295]
		[batch 20/20] avg loss: 7.8026809385240075		[learning rate: 0.00011281]
	Learning Rate: 0.000112813
	LOSS [training: 7.828567684528137 | validation: 7.599048653435346]
	TIME [epoch: 8.29 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.792559072064011		[learning rate: 0.00011268]
		[batch 20/20] avg loss: 7.825419741071523		[learning rate: 0.00011255]
	Learning Rate: 0.000112547
	LOSS [training: 7.808989406567768 | validation: 7.575130087033607]
	TIME [epoch: 8.29 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.85978260910201		[learning rate: 0.00011241]
		[batch 20/20] avg loss: 7.804504730983737		[learning rate: 0.00011228]
	Learning Rate: 0.000112281
	LOSS [training: 7.832143670042875 | validation: 7.587307211080104]
	TIME [epoch: 8.32 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.895342655463698		[learning rate: 0.00011215]
		[batch 20/20] avg loss: 7.773437049811447		[learning rate: 0.00011202]
	Learning Rate: 0.000112017
	LOSS [training: 7.834389852637573 | validation: 7.567305607328167]
	TIME [epoch: 8.29 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.843411956347319		[learning rate: 0.00011188]
		[batch 20/20] avg loss: 7.874554999778626		[learning rate: 0.00011175]
	Learning Rate: 0.000111752
	LOSS [training: 7.858983478062973 | validation: 7.624399092986744]
	TIME [epoch: 8.29 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.844283871967001		[learning rate: 0.00011162]
		[batch 20/20] avg loss: 7.905106135870791		[learning rate: 0.00011149]
	Learning Rate: 0.000111489
	LOSS [training: 7.874695003918896 | validation: 7.63872027819584]
	TIME [epoch: 8.29 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.811055705243812		[learning rate: 0.00011136]
		[batch 20/20] avg loss: 7.916844133979893		[learning rate: 0.00011123]
	Learning Rate: 0.000111226
	LOSS [training: 7.863949919611853 | validation: 7.626646449029385]
	TIME [epoch: 8.32 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.897201318436272		[learning rate: 0.00011109]
		[batch 20/20] avg loss: 7.807928069150731		[learning rate: 0.00011096]
	Learning Rate: 0.000110963
	LOSS [training: 7.852564693793501 | validation: 7.654308090759785]
	TIME [epoch: 8.3 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.842972867041416		[learning rate: 0.00011083]
		[batch 20/20] avg loss: 7.902640677178202		[learning rate: 0.0001107]
	Learning Rate: 0.000110702
	LOSS [training: 7.872806772109809 | validation: 7.631509382312206]
	TIME [epoch: 8.3 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.890457599723677		[learning rate: 0.00011057]
		[batch 20/20] avg loss: 7.8470559395865624		[learning rate: 0.00011044]
	Learning Rate: 0.00011044
	LOSS [training: 7.868756769655119 | validation: 7.622928840302464]
	TIME [epoch: 8.29 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.89139535296489		[learning rate: 0.00011031]
		[batch 20/20] avg loss: 7.810799408988094		[learning rate: 0.00011018]
	Learning Rate: 0.00011018
	LOSS [training: 7.851097380976492 | validation: 7.627273495624224]
	TIME [epoch: 8.3 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.928785273907614		[learning rate: 0.00011005]
		[batch 20/20] avg loss: 7.814584036591571		[learning rate: 0.00010992]
	Learning Rate: 0.00010992
	LOSS [training: 7.871684655249593 | validation: 7.633764812851316]
	TIME [epoch: 8.31 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.840526942720546		[learning rate: 0.00010979]
		[batch 20/20] avg loss: 7.887201787589575		[learning rate: 0.00010966]
	Learning Rate: 0.000109661
	LOSS [training: 7.863864365155061 | validation: 7.59942534292786]
	TIME [epoch: 8.29 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.889439123359598		[learning rate: 0.00010953]
		[batch 20/20] avg loss: 7.791275143557932		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 7.840357133458765 | validation: 7.57172457573837]
	TIME [epoch: 8.29 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.970520770365144		[learning rate: 0.00010927]
		[batch 20/20] avg loss: 7.728175478632423		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 7.84934812449878 | validation: 7.616801852573011]
	TIME [epoch: 8.29 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.893824027754645		[learning rate: 0.00010902]
		[batch 20/20] avg loss: 7.806631133402415		[learning rate: 0.00010889]
	Learning Rate: 0.000108887
	LOSS [training: 7.85022758057853 | validation: 7.567616198479807]
	TIME [epoch: 8.31 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.754332032884006		[learning rate: 0.00010876]
		[batch 20/20] avg loss: 7.964096543743277		[learning rate: 0.00010863]
	Learning Rate: 0.00010863
	LOSS [training: 7.859214288313642 | validation: 7.604822828858459]
	TIME [epoch: 8.29 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.900659354330107		[learning rate: 0.0001085]
		[batch 20/20] avg loss: 7.792887552365237		[learning rate: 0.00010837]
	Learning Rate: 0.000108373
	LOSS [training: 7.846773453347673 | validation: 7.569312234192274]
	TIME [epoch: 8.29 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.68106746229906		[learning rate: 0.00010825]
		[batch 20/20] avg loss: 7.985449705636282		[learning rate: 0.00010812]
	Learning Rate: 0.000108118
	LOSS [training: 7.833258583967671 | validation: 7.565257350970798]
	TIME [epoch: 8.29 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.829000869522557		[learning rate: 0.00010799]
		[batch 20/20] avg loss: 7.823244222281249		[learning rate: 0.00010786]
	Learning Rate: 0.000107863
	LOSS [training: 7.826122545901903 | validation: 7.556105352621013]
	TIME [epoch: 8.31 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.669120060032927		[learning rate: 0.00010774]
		[batch 20/20] avg loss: 7.961120291621327		[learning rate: 0.00010761]
	Learning Rate: 0.000107608
	LOSS [training: 7.81512017582713 | validation: 7.560719369400647]
	TIME [epoch: 8.3 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.883561508802499		[learning rate: 0.00010748]
		[batch 20/20] avg loss: 7.795294851370609		[learning rate: 0.00010735]
	Learning Rate: 0.000107355
	LOSS [training: 7.839428180086554 | validation: 7.6088804926547535]
	TIME [epoch: 8.29 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.897221346749044		[learning rate: 0.00010723]
		[batch 20/20] avg loss: 7.846331068189414		[learning rate: 0.0001071]
	Learning Rate: 0.000107101
	LOSS [training: 7.871776207469229 | validation: 7.637868307283742]
	TIME [epoch: 8.29 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.920903760437693		[learning rate: 0.00010697]
		[batch 20/20] avg loss: 7.839688252262367		[learning rate: 0.00010685]
	Learning Rate: 0.000106849
	LOSS [training: 7.880296006350031 | validation: 7.634570484219452]
	TIME [epoch: 8.3 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.855279436198702		[learning rate: 0.00010672]
		[batch 20/20] avg loss: 7.902924931111151		[learning rate: 0.0001066]
	Learning Rate: 0.000106597
	LOSS [training: 7.879102183654926 | validation: 7.688593451330071]
	TIME [epoch: 8.31 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.768709073899205		[learning rate: 0.00010647]
		[batch 20/20] avg loss: 7.974816462542546		[learning rate: 0.00010635]
	Learning Rate: 0.000106345
	LOSS [training: 7.871762768220876 | validation: 7.688431732429434]
	TIME [epoch: 8.29 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.925361849855011		[learning rate: 0.00010622]
		[batch 20/20] avg loss: 7.840198993471861		[learning rate: 0.00010609]
	Learning Rate: 0.000106094
	LOSS [training: 7.882780421663438 | validation: 7.671224039198671]
	TIME [epoch: 8.29 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9197167294729125		[learning rate: 0.00010597]
		[batch 20/20] avg loss: 7.878665047462642		[learning rate: 0.00010584]
	Learning Rate: 0.000105844
	LOSS [training: 7.899190888467777 | validation: 7.6846724085794715]
	TIME [epoch: 8.29 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.841854764483182		[learning rate: 0.00010572]
		[batch 20/20] avg loss: 7.9625179931998105		[learning rate: 0.00010559]
	Learning Rate: 0.000105594
	LOSS [training: 7.902186378841497 | validation: 7.710406035885046]
	TIME [epoch: 8.31 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.905950500232159		[learning rate: 0.00010547]
		[batch 20/20] avg loss: 7.905908470453052		[learning rate: 0.00010535]
	Learning Rate: 0.000105345
	LOSS [training: 7.9059294853426065 | validation: 7.718706681379068]
	TIME [epoch: 8.29 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9344736378141905		[learning rate: 0.00010522]
		[batch 20/20] avg loss: 7.869729806721101		[learning rate: 0.0001051]
	Learning Rate: 0.000105097
	LOSS [training: 7.902101722267645 | validation: 7.704616976868391]
	TIME [epoch: 8.29 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.973852049693926		[learning rate: 0.00010497]
		[batch 20/20] avg loss: 7.826137939000536		[learning rate: 0.00010485]
	Learning Rate: 0.000104849
	LOSS [training: 7.899994994347232 | validation: 7.735667384795265]
	TIME [epoch: 8.29 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.895101887245502		[learning rate: 0.00010473]
		[batch 20/20] avg loss: 7.903182307810181		[learning rate: 0.0001046]
	Learning Rate: 0.000104602
	LOSS [training: 7.899142097527841 | validation: 7.668778026662272]
	TIME [epoch: 8.31 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.796128438489644		[learning rate: 0.00010448]
		[batch 20/20] avg loss: 7.970685882090369		[learning rate: 0.00010435]
	Learning Rate: 0.000104355
	LOSS [training: 7.883407160290005 | validation: 7.691518015003223]
	TIME [epoch: 8.3 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.917587538868981		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 7.8318197020557845		[learning rate: 0.00010411]
	Learning Rate: 0.000104109
	LOSS [training: 7.874703620462384 | validation: 7.667033484993676]
	TIME [epoch: 8.3 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8370853477936375		[learning rate: 0.00010399]
		[batch 20/20] avg loss: 7.900955117904303		[learning rate: 0.00010386]
	Learning Rate: 0.000103863
	LOSS [training: 7.869020232848969 | validation: 7.682403568642778]
	TIME [epoch: 8.29 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.839023042987807		[learning rate: 0.00010374]
		[batch 20/20] avg loss: 7.938127201982843		[learning rate: 0.00010362]
	Learning Rate: 0.000103618
	LOSS [training: 7.888575122485325 | validation: 7.746363244484192]
	TIME [epoch: 8.29 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.920452955253938		[learning rate: 0.0001035]
		[batch 20/20] avg loss: 7.902482697982883		[learning rate: 0.00010337]
	Learning Rate: 0.000103374
	LOSS [training: 7.911467826618411 | validation: 7.708364719161662]
	TIME [epoch: 8.32 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.87130677635629		[learning rate: 0.00010325]
		[batch 20/20] avg loss: 7.940141604180385		[learning rate: 0.00010313]
	Learning Rate: 0.00010313
	LOSS [training: 7.905724190268337 | validation: 7.738202336034798]
	TIME [epoch: 8.29 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.892999397214543		[learning rate: 0.00010301]
		[batch 20/20] avg loss: 7.933540577820537		[learning rate: 0.00010289]
	Learning Rate: 0.000102887
	LOSS [training: 7.913269987517542 | validation: 7.745918526057721]
	TIME [epoch: 8.29 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.991173169089326		[learning rate: 0.00010277]
		[batch 20/20] avg loss: 7.845337437498953		[learning rate: 0.00010264]
	Learning Rate: 0.000102644
	LOSS [training: 7.9182553032941385 | validation: 7.717599401187329]
	TIME [epoch: 8.29 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9529920967274474		[learning rate: 0.00010252]
		[batch 20/20] avg loss: 7.87414168205613		[learning rate: 0.0001024]
	Learning Rate: 0.000102402
	LOSS [training: 7.91356688939179 | validation: 7.756700973596418]
	TIME [epoch: 8.31 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9613990007555815		[learning rate: 0.00010228]
		[batch 20/20] avg loss: 7.886662005392042		[learning rate: 0.00010216]
	Learning Rate: 0.00010216
	LOSS [training: 7.924030503073811 | validation: 7.7232309625039814]
	TIME [epoch: 8.29 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.849371169065185		[learning rate: 0.00010204]
		[batch 20/20] avg loss: 8.004442164412513		[learning rate: 0.00010192]
	Learning Rate: 0.000101919
	LOSS [training: 7.926906666738849 | validation: 7.746896739424592]
	TIME [epoch: 8.29 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.916727801073188		[learning rate: 0.0001018]
		[batch 20/20] avg loss: 7.930010224776069		[learning rate: 0.00010168]
	Learning Rate: 0.000101679
	LOSS [training: 7.9233690129246295 | validation: 7.73338850941731]
	TIME [epoch: 8.29 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.839602776317776		[learning rate: 0.00010156]
		[batch 20/20] avg loss: 7.9874493712628665		[learning rate: 0.00010144]
	Learning Rate: 0.000101439
	LOSS [training: 7.913526073790321 | validation: 7.735096722726997]
	TIME [epoch: 8.31 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.974373774328302		[learning rate: 0.00010132]
		[batch 20/20] avg loss: 7.892909544220148		[learning rate: 0.0001012]
	Learning Rate: 0.0001012
	LOSS [training: 7.933641659274225 | validation: 7.783400652560054]
	TIME [epoch: 8.3 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.99844064859772		[learning rate: 0.00010108]
		[batch 20/20] avg loss: 7.850591336190611		[learning rate: 0.00010096]
	Learning Rate: 0.000100961
	LOSS [training: 7.924515992394167 | validation: 7.764413217788361]
	TIME [epoch: 8.29 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.9621907082220575		[learning rate: 0.00010084]
		[batch 20/20] avg loss: 7.9307220140056955		[learning rate: 0.00010072]
	Learning Rate: 0.000100723
	LOSS [training: 7.946456361113876 | validation: 7.778303453624735]
	TIME [epoch: 8.29 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.980419313388724		[learning rate: 0.0001006]
		[batch 20/20] avg loss: 7.859921027681706		[learning rate: 0.00010049]
	Learning Rate: 0.000100485
	LOSS [training: 7.920170170535215 | validation: 7.769756553796895]
	TIME [epoch: 8.29 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.839711256281257		[learning rate: 0.00010037]
		[batch 20/20] avg loss: 8.00517711904946		[learning rate: 0.00010025]
	Learning Rate: 0.000100248
	LOSS [training: 7.922444187665358 | validation: 7.76589655624835]
	TIME [epoch: 8.32 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.890787705942797		[learning rate: 0.00010013]
		[batch 20/20] avg loss: 7.972197122960381		[learning rate: 0.00010001]
	Learning Rate: 0.000100012
	LOSS [training: 7.931492414451587 | validation: 7.763159798065352]
	TIME [epoch: 8.29 sec]
Finished training in 16751.632 seconds.
