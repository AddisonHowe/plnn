Args:
Namespace(name='model_tr_study1', outdir='out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1', training_data='data/transition_rate_studies/tr_study1/tr_study1_training/r1', validation_data='data/transition_rate_studies/tr_study1/tr_study1_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=1000, batch_size=100, report_every=10, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=True, sigma=0.1, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.0001, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 521020511

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_0.pth
EPOCH 1/1000:
	Training over batches...
		[batch 10/20] avg loss: 9.816873798080039		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.281593049530537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.549233423805289 | validation: 9.215135456585742]
	TIME [epoch: 80 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_1.pth
	Model improved!!!
EPOCH 2/1000:
	Training over batches...
		[batch 10/20] avg loss: 8.672828606272326		[learning rate: 0.01]
		[batch 20/20] avg loss: 8.051061100951294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.36194485361181 | validation: 8.241102010453151]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_2.pth
	Model improved!!!
EPOCH 3/1000:
	Training over batches...
		[batch 10/20] avg loss: 7.277151935810233		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.929776887890336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.1034644118502825 | validation: 6.871727124187244]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_3.pth
	Model improved!!!
EPOCH 4/1000:
	Training over batches...
		[batch 10/20] avg loss: 6.479869268590574		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.853366877573444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.166618073082008 | validation: 5.9032790998031786]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_4.pth
	Model improved!!!
EPOCH 5/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.576438257247849		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.146026800485743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.361232528866795 | validation: 4.875632213498234]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_5.pth
	Model improved!!!
EPOCH 6/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.991944344974583		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.818126582014021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.905035463494302 | validation: 6.110144246131349]
	TIME [epoch: 8.44 sec]
EPOCH 7/1000:
	Training over batches...
		[batch 10/20] avg loss: 5.085612929547212		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.721493734003422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.903553331775317 | validation: 4.786184697322165]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_7.pth
	Model improved!!!
EPOCH 8/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.665633766384405		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.510304997282763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.587969381833584 | validation: 4.969515981200861]
	TIME [epoch: 8.42 sec]
EPOCH 9/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.26557527821014		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.320584076499996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.293079677355069 | validation: 5.483205647417256]
	TIME [epoch: 8.45 sec]
EPOCH 10/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.6057938813187995		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.600628879015674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.603211380167236 | validation: 5.411851568326657]
	TIME [epoch: 8.44 sec]
EPOCH 11/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.5769300839688105		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.0990660144195505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.337998049194181 | validation: 4.113303455196974]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_11.pth
	Model improved!!!
EPOCH 12/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.914596880470178		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.1919068529017345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.053251866685956 | validation: 4.363613181629155]
	TIME [epoch: 8.43 sec]
EPOCH 13/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.646938684485538		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.391410755133359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.519174719809448 | validation: 4.264846533438559]
	TIME [epoch: 8.4 sec]
EPOCH 14/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.9733596886844347		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.831095515092348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9022276018883923 | validation: 4.473495343546183]
	TIME [epoch: 8.43 sec]
EPOCH 15/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.731284092004556		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.0918896023223095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.911586847163433 | validation: 4.213808248101929]
	TIME [epoch: 8.42 sec]
EPOCH 16/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.67267020346708		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4496572899873734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5611637467272272 | validation: 4.571646235898063]
	TIME [epoch: 8.41 sec]
EPOCH 17/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.546344252678999		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.4923102180594077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5193272353692024 | validation: 3.810190120614128]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_17.pth
	Model improved!!!
EPOCH 18/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.7329516042627837		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5761313149327827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.654541459597784 | validation: 4.080818916499243]
	TIME [epoch: 8.45 sec]
EPOCH 19/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.6239569576031085		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2532345703007928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4385957639519504 | validation: 3.7091166463488503]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_19.pth
	Model improved!!!
EPOCH 20/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.196972339673297		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.38797218674425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2924722632087735 | validation: 3.709344925339144]
	TIME [epoch: 8.4 sec]
EPOCH 21/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.47474018307095		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2721140157613164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3734270994161335 | validation: 3.6861601855768473]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_21.pth
	Model improved!!!
EPOCH 22/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.524358936667525		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.380131736969789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.452245336818657 | validation: 3.8674747989032743]
	TIME [epoch: 8.43 sec]
EPOCH 23/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.36195694909854		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.2699492425191368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.315953095808838 | validation: 3.6634858298141335]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_23.pth
	Model improved!!!
EPOCH 24/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.408012320430129		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.971766764164536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1898895422973323 | validation: 3.9973287475665527]
	TIME [epoch: 8.42 sec]
EPOCH 25/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3162412487112696		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.100318145520638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.208279697115954 | validation: 1.5375250284264201]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_25.pth
	Model improved!!!
EPOCH 26/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8830474648058135		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0614792033906078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.972263334098211 | validation: 2.8514162868650335]
	TIME [epoch: 8.42 sec]
EPOCH 27/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.0938324428653243		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0032468979525526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.048539670408938 | validation: 1.7576898942102603]
	TIME [epoch: 8.43 sec]
EPOCH 28/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8753518025758147		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7720676532130473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8237097278944312 | validation: 2.003542398218035]
	TIME [epoch: 8.41 sec]
EPOCH 29/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.780286227705999		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7884360694124368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7843611485592177 | validation: 1.789552702088091]
	TIME [epoch: 8.43 sec]
EPOCH 30/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.954785209460797		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8728821423858109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.913833675923304 | validation: 1.5522029481626098]
	TIME [epoch: 8.4 sec]
EPOCH 31/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.779056071081055		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0363331137159024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9076945923984785 | validation: 1.7435278940607333]
	TIME [epoch: 8.45 sec]
EPOCH 32/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7091718362066874		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7295829034441428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7193773698254151 | validation: 2.0625688657854546]
	TIME [epoch: 8.41 sec]
EPOCH 33/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9185329502570254		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8391342282073413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8788335892321837 | validation: 1.701405483271412]
	TIME [epoch: 8.4 sec]
EPOCH 34/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5887570037357466		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8311880581859046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7099725309608254 | validation: 2.408809340261499]
	TIME [epoch: 8.42 sec]
EPOCH 35/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8664205718136817		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7187337387385182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7925771552760996 | validation: 1.7336206057693295]
	TIME [epoch: 8.43 sec]
EPOCH 36/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.746485580507887		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6648011831586451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7056433818332657 | validation: 1.5753875548960339]
	TIME [epoch: 8.42 sec]
EPOCH 37/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5851628084463927		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6027256718698168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5939442401581048 | validation: 2.3389321343931764]
	TIME [epoch: 8.4 sec]
EPOCH 38/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7466025050189284		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4653049684897403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6059537367543342 | validation: 1.8733165243431171]
	TIME [epoch: 8.4 sec]
EPOCH 39/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8156464769068552		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4717657337242893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6437061053155724 | validation: 1.8841248993992152]
	TIME [epoch: 8.44 sec]
EPOCH 40/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5014720598544868		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4494219073854784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.475446983619983 | validation: 1.7054875752403253]
	TIME [epoch: 8.42 sec]
EPOCH 41/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6517691205867127		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.585712228557099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.618740674571906 | validation: 1.3500061389974816]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_41.pth
	Model improved!!!
EPOCH 42/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4134787776949829		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6406780134755343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5270783955852585 | validation: 1.2484044754654866]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_42.pth
	Model improved!!!
EPOCH 43/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3363456462481065		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5664555545371335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4514006003926199 | validation: 1.4063728717159298]
	TIME [epoch: 8.44 sec]
EPOCH 44/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3345835117972276		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3665294412816873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3505564765394573 | validation: 1.1906901797591565]
	TIME [epoch: 8.47 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_44.pth
	Model improved!!!
EPOCH 45/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4002331416529663		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3843697934787316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3923014675658487 | validation: 1.116370845434868]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_45.pth
	Model improved!!!
EPOCH 46/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4287905292832233		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2998654695714735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3643279994273487 | validation: 1.393463144733649]
	TIME [epoch: 8.43 sec]
EPOCH 47/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.570084858580342		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2512669038260475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4106758812031948 | validation: 1.0943642695347708]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_47.pth
	Model improved!!!
EPOCH 48/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2413526199133258		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3668447474848964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.304098683699111 | validation: 1.2516870611673552]
	TIME [epoch: 8.46 sec]
EPOCH 49/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3222105311379937		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2894460874043954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3058283092711942 | validation: 1.0562057661186064]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_49.pth
	Model improved!!!
EPOCH 50/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3590350040834853		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3102130160782852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3346240100808853 | validation: 1.384422033529213]
	TIME [epoch: 8.44 sec]
EPOCH 51/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.459198784219017		[learning rate: 0.0099782]
		[batch 20/20] avg loss: 1.2839890040476398		[learning rate: 0.0099541]
	Learning Rate: 0.00995405
	LOSS [training: 1.3715938941333286 | validation: 1.2328752307069926]
	TIME [epoch: 8.41 sec]
EPOCH 52/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4419514553709445		[learning rate: 0.00993]
		[batch 20/20] avg loss: 1.3244396483056242		[learning rate: 0.0099059]
	Learning Rate: 0.00990592
	LOSS [training: 1.3831955518382846 | validation: 1.3060366657693396]
	TIME [epoch: 8.46 sec]
EPOCH 53/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4217937812802712		[learning rate: 0.0098819]
		[batch 20/20] avg loss: 1.4023917633267347		[learning rate: 0.009858]
	Learning Rate: 0.00985801
	LOSS [training: 1.4120927723035028 | validation: 1.3422124042281895]
	TIME [epoch: 8.42 sec]
EPOCH 54/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4956440548296253		[learning rate: 0.0098341]
		[batch 20/20] avg loss: 1.4524984886118244		[learning rate: 0.0098103]
	Learning Rate: 0.00981034
	LOSS [training: 1.474071271720725 | validation: 1.2271477091845149]
	TIME [epoch: 8.4 sec]
EPOCH 55/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.442656630459489		[learning rate: 0.0097866]
		[batch 20/20] avg loss: 1.3066889571710143		[learning rate: 0.0097629]
	Learning Rate: 0.0097629
	LOSS [training: 1.3746727938152516 | validation: 1.2190561754537341]
	TIME [epoch: 8.43 sec]
EPOCH 56/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.339531598403992		[learning rate: 0.0097393]
		[batch 20/20] avg loss: 1.4148751987682264		[learning rate: 0.0097157]
	Learning Rate: 0.00971569
	LOSS [training: 1.3772033985861092 | validation: 1.1195126515637646]
	TIME [epoch: 8.42 sec]
EPOCH 57/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3424736373489297		[learning rate: 0.0096922]
		[batch 20/20] avg loss: 1.3601873104216047		[learning rate: 0.0096687]
	Learning Rate: 0.00966871
	LOSS [training: 1.3513304738852672 | validation: 1.2692453237389998]
	TIME [epoch: 8.41 sec]
EPOCH 58/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4317318903866885		[learning rate: 0.0096453]
		[batch 20/20] avg loss: 1.2358797357978042		[learning rate: 0.009622]
	Learning Rate: 0.00962195
	LOSS [training: 1.3338058130922466 | validation: 1.118268712502601]
	TIME [epoch: 8.42 sec]
EPOCH 59/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3174842254061676		[learning rate: 0.0095987]
		[batch 20/20] avg loss: 1.3156780696169945		[learning rate: 0.0095754]
	Learning Rate: 0.00957542
	LOSS [training: 1.3165811475115812 | validation: 1.1397552445850483]
	TIME [epoch: 8.4 sec]
EPOCH 60/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.348478312770704		[learning rate: 0.0095522]
		[batch 20/20] avg loss: 1.1934931773800423		[learning rate: 0.0095291]
	Learning Rate: 0.00952912
	LOSS [training: 1.270985745075373 | validation: 1.4330240725669252]
	TIME [epoch: 8.4 sec]
EPOCH 61/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2588815156258015		[learning rate: 0.009506]
		[batch 20/20] avg loss: 1.2818744312087904		[learning rate: 0.009483]
	Learning Rate: 0.00948303
	LOSS [training: 1.270377973417296 | validation: 1.2017093609911151]
	TIME [epoch: 8.46 sec]
EPOCH 62/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.227077607519592		[learning rate: 0.0094601]
		[batch 20/20] avg loss: 2.009996854556581		[learning rate: 0.0094372]
	Learning Rate: 0.00943718
	LOSS [training: 1.6185372310380868 | validation: 1.0238748535506577]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_62.pth
	Model improved!!!
EPOCH 63/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1243203993933253		[learning rate: 0.0094143]
		[batch 20/20] avg loss: 1.1598968772901903		[learning rate: 0.0093915]
	Learning Rate: 0.00939154
	LOSS [training: 1.142108638341758 | validation: 1.3758228059245823]
	TIME [epoch: 8.41 sec]
EPOCH 64/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2587702083139551		[learning rate: 0.0093688]
		[batch 20/20] avg loss: 1.1420434638064119		[learning rate: 0.0093461]
	Learning Rate: 0.00934612
	LOSS [training: 1.2004068360601834 | validation: 1.108165271486898]
	TIME [epoch: 8.43 sec]
EPOCH 65/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2972176902816366		[learning rate: 0.0093235]
		[batch 20/20] avg loss: 1.2237646396255144		[learning rate: 0.0093009]
	Learning Rate: 0.00930093
	LOSS [training: 1.2604911649535755 | validation: 1.2159009829430694]
	TIME [epoch: 8.43 sec]
EPOCH 66/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0932728401875664		[learning rate: 0.0092784]
		[batch 20/20] avg loss: 1.1981785540305476		[learning rate: 0.009256]
	Learning Rate: 0.00925595
	LOSS [training: 1.1457256971090573 | validation: 1.1063341335835886]
	TIME [epoch: 8.41 sec]
EPOCH 67/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.259496461070696		[learning rate: 0.0092335]
		[batch 20/20] avg loss: 1.2946907508049161		[learning rate: 0.0092112]
	Learning Rate: 0.00921119
	LOSS [training: 1.2770936059378064 | validation: 1.2289419173648177]
	TIME [epoch: 8.42 sec]
EPOCH 68/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1992721309830088		[learning rate: 0.0091889]
		[batch 20/20] avg loss: 1.1021837121051188		[learning rate: 0.0091666]
	Learning Rate: 0.00916665
	LOSS [training: 1.150727921544064 | validation: 1.0680339912995787]
	TIME [epoch: 8.41 sec]
EPOCH 69/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.045589636316793		[learning rate: 0.0091445]
		[batch 20/20] avg loss: 1.1562812320914166		[learning rate: 0.0091223]
	Learning Rate: 0.00912232
	LOSS [training: 1.1009354342041047 | validation: 0.973359870793058]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_69.pth
	Model improved!!!
EPOCH 70/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0041763358579432		[learning rate: 0.0091002]
		[batch 20/20] avg loss: 0.988094338648404		[learning rate: 0.0090782]
	Learning Rate: 0.0090782
	LOSS [training: 0.9961353372531736 | validation: 0.9235025810113651]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_70.pth
	Model improved!!!
EPOCH 71/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1099611677778314		[learning rate: 0.0090562]
		[batch 20/20] avg loss: 1.1479332592944296		[learning rate: 0.0090343]
	Learning Rate: 0.0090343
	LOSS [training: 1.1289472135361307 | validation: 0.9624128286533318]
	TIME [epoch: 8.42 sec]
EPOCH 72/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1338679535489664		[learning rate: 0.0090124]
		[batch 20/20] avg loss: 1.0569598907625202		[learning rate: 0.0089906]
	Learning Rate: 0.00899062
	LOSS [training: 1.095413922155743 | validation: 1.0389135338503086]
	TIME [epoch: 8.43 sec]
EPOCH 73/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0070895289324244		[learning rate: 0.0089689]
		[batch 20/20] avg loss: 1.1413955852141549		[learning rate: 0.0089471]
	Learning Rate: 0.00894714
	LOSS [training: 1.0742425570732899 | validation: 0.9317405834780257]
	TIME [epoch: 8.43 sec]
EPOCH 74/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0376609608647032		[learning rate: 0.0089255]
		[batch 20/20] avg loss: 0.9651021144214417		[learning rate: 0.0089039]
	Learning Rate: 0.00890387
	LOSS [training: 1.0013815376430726 | validation: 0.8172192277889934]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_74.pth
	Model improved!!!
EPOCH 75/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9667301933081418		[learning rate: 0.0088823]
		[batch 20/20] avg loss: 1.2045063100565216		[learning rate: 0.0088608]
	Learning Rate: 0.00886082
	LOSS [training: 1.0856182516823318 | validation: 1.1653295785507436]
	TIME [epoch: 8.42 sec]
EPOCH 76/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0531607655313162		[learning rate: 0.0088394]
		[batch 20/20] avg loss: 0.9177358629907448		[learning rate: 0.008818]
	Learning Rate: 0.00881797
	LOSS [training: 0.9854483142610304 | validation: 0.7894863529535413]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_76.pth
	Model improved!!!
EPOCH 77/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1023037732805974		[learning rate: 0.0087966]
		[batch 20/20] avg loss: 0.8516911219909101		[learning rate: 0.0087753]
	Learning Rate: 0.00877532
	LOSS [training: 0.9769974476357538 | validation: 1.0115124891928182]
	TIME [epoch: 8.42 sec]
EPOCH 78/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8999196394223894		[learning rate: 0.0087541]
		[batch 20/20] avg loss: 0.9264966948779552		[learning rate: 0.0087329]
	Learning Rate: 0.00873289
	LOSS [training: 0.9132081671501723 | validation: 0.7116069658445263]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_78.pth
	Model improved!!!
EPOCH 79/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9593264269127975		[learning rate: 0.0087117]
		[batch 20/20] avg loss: 0.9107189384607505		[learning rate: 0.0086907]
	Learning Rate: 0.00869066
	LOSS [training: 0.935022682686774 | validation: 0.7132848873824689]
	TIME [epoch: 8.43 sec]
EPOCH 80/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8756748279116836		[learning rate: 0.0086696]
		[batch 20/20] avg loss: 0.9151073069177617		[learning rate: 0.0086486]
	Learning Rate: 0.00864863
	LOSS [training: 0.8953910674147227 | validation: 0.9153466379249933]
	TIME [epoch: 8.41 sec]
EPOCH 81/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9337787946289374		[learning rate: 0.0086277]
		[batch 20/20] avg loss: 0.963458327789213		[learning rate: 0.0086068]
	Learning Rate: 0.00860681
	LOSS [training: 0.9486185612090751 | validation: 0.895244468522763]
	TIME [epoch: 8.42 sec]
EPOCH 82/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0873340498146808		[learning rate: 0.008586]
		[batch 20/20] avg loss: 0.9759649634460045		[learning rate: 0.0085652]
	Learning Rate: 0.00856519
	LOSS [training: 1.031649506630343 | validation: 0.6735463410829707]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_82.pth
	Model improved!!!
EPOCH 83/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.937265060633302		[learning rate: 0.0085445]
		[batch 20/20] avg loss: 0.8451792083916964		[learning rate: 0.0085238]
	Learning Rate: 0.00852377
	LOSS [training: 0.8912221345124992 | validation: 0.7721025027894797]
	TIME [epoch: 8.42 sec]
EPOCH 84/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8667201518484943		[learning rate: 0.0085031]
		[batch 20/20] avg loss: 0.9705571704057604		[learning rate: 0.0084825]
	Learning Rate: 0.00848255
	LOSS [training: 0.9186386611271272 | validation: 0.9526107858045668]
	TIME [epoch: 8.42 sec]
EPOCH 85/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.041426366417293		[learning rate: 0.008462]
		[batch 20/20] avg loss: 0.9081645266657793		[learning rate: 0.0084415]
	Learning Rate: 0.00844153
	LOSS [training: 0.9747954465415359 | validation: 0.9106246656797531]
	TIME [epoch: 8.42 sec]
EPOCH 86/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9007311076464409		[learning rate: 0.0084211]
		[batch 20/20] avg loss: 0.8534260400672649		[learning rate: 0.0084007]
	Learning Rate: 0.00840071
	LOSS [training: 0.877078573856853 | validation: 0.6811630965663907]
	TIME [epoch: 8.45 sec]
EPOCH 87/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7571880857388432		[learning rate: 0.0083804]
		[batch 20/20] avg loss: 0.6479546680661674		[learning rate: 0.0083601]
	Learning Rate: 0.00836008
	LOSS [training: 0.7025713769025052 | validation: 0.757748976098248]
	TIME [epoch: 8.42 sec]
EPOCH 88/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9028766019050375		[learning rate: 0.0083398]
		[batch 20/20] avg loss: 0.9737769837774731		[learning rate: 0.0083197]
	Learning Rate: 0.00831965
	LOSS [training: 0.9383267928412555 | validation: 0.7154365842302604]
	TIME [epoch: 8.42 sec]
EPOCH 89/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7489871147605474		[learning rate: 0.0082995]
		[batch 20/20] avg loss: 0.8782747341874282		[learning rate: 0.0082794]
	Learning Rate: 0.00827942
	LOSS [training: 0.8136309244739879 | validation: 0.7190614377812703]
	TIME [epoch: 8.42 sec]
EPOCH 90/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9108968874651213		[learning rate: 0.0082594]
		[batch 20/20] avg loss: 0.7543532972963675		[learning rate: 0.0082394]
	Learning Rate: 0.00823938
	LOSS [training: 0.8326250923807443 | validation: 1.0015100953479619]
	TIME [epoch: 8.4 sec]
EPOCH 91/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8173210312712564		[learning rate: 0.0082194]
		[batch 20/20] avg loss: 0.8650715379490347		[learning rate: 0.0081995]
	Learning Rate: 0.00819954
	LOSS [training: 0.8411962846101456 | validation: 0.9342739399639284]
	TIME [epoch: 8.45 sec]
EPOCH 92/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9898376490291231		[learning rate: 0.0081797]
		[batch 20/20] avg loss: 0.7875867757051263		[learning rate: 0.0081599]
	Learning Rate: 0.00815989
	LOSS [training: 0.8887122123671247 | validation: 0.7277350031080728]
	TIME [epoch: 8.41 sec]
EPOCH 93/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9806258534315611		[learning rate: 0.0081401]
		[batch 20/20] avg loss: 0.9392600303021045		[learning rate: 0.0081204]
	Learning Rate: 0.00812043
	LOSS [training: 0.9599429418668327 | validation: 1.091249680655144]
	TIME [epoch: 8.4 sec]
EPOCH 94/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8129402732153869		[learning rate: 0.0081008]
		[batch 20/20] avg loss: 0.729947371035021		[learning rate: 0.0080812]
	Learning Rate: 0.00808116
	LOSS [training: 0.7714438221252038 | validation: 0.7749186320056389]
	TIME [epoch: 8.43 sec]
EPOCH 95/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.654109830554498		[learning rate: 0.0080616]
		[batch 20/20] avg loss: 0.6739517576230882		[learning rate: 0.0080421]
	Learning Rate: 0.00804208
	LOSS [training: 0.6640307940887931 | validation: 0.5682749017284197]
	TIME [epoch: 8.44 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_95.pth
	Model improved!!!
EPOCH 96/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.767435329895604		[learning rate: 0.0080226]
		[batch 20/20] avg loss: 0.7171229627955162		[learning rate: 0.0080032]
	Learning Rate: 0.00800319
	LOSS [training: 0.7422791463455602 | validation: 0.6800194736094799]
	TIME [epoch: 8.41 sec]
EPOCH 97/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6582835038819608		[learning rate: 0.0079838]
		[batch 20/20] avg loss: 0.7485232738354668		[learning rate: 0.0079645]
	Learning Rate: 0.00796449
	LOSS [training: 0.7034033888587137 | validation: 0.7727227545288808]
	TIME [epoch: 8.43 sec]
EPOCH 98/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7800410948841913		[learning rate: 0.0079452]
		[batch 20/20] avg loss: 0.7658676109467324		[learning rate: 0.007926]
	Learning Rate: 0.00792597
	LOSS [training: 0.7729543529154618 | validation: 0.786867936911096]
	TIME [epoch: 8.42 sec]
EPOCH 99/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9169642083270302		[learning rate: 0.0079068]
		[batch 20/20] avg loss: 0.6606987011537981		[learning rate: 0.0078876]
	Learning Rate: 0.00788765
	LOSS [training: 0.7888314547404143 | validation: 0.7526270040330896]
	TIME [epoch: 8.43 sec]
EPOCH 100/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7019918995529668		[learning rate: 0.0078685]
		[batch 20/20] avg loss: 0.6119036460194565		[learning rate: 0.0078495]
	Learning Rate: 0.0078495
	LOSS [training: 0.6569477727862116 | validation: 0.4512380588632179]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_100.pth
	Model improved!!!
EPOCH 101/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7817137937862395		[learning rate: 0.0078305]
		[batch 20/20] avg loss: 0.7854381109775037		[learning rate: 0.0078115]
	Learning Rate: 0.00781154
	LOSS [training: 0.7835759523818715 | validation: 2.0340657720135678]
	TIME [epoch: 8.42 sec]
EPOCH 102/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9186519381712162		[learning rate: 0.0077926]
		[batch 20/20] avg loss: 0.5612338105957881		[learning rate: 0.0077738]
	Learning Rate: 0.00777377
	LOSS [training: 0.7399428743835019 | validation: 0.5234485565081097]
	TIME [epoch: 8.4 sec]
EPOCH 103/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6351149331838422		[learning rate: 0.0077549]
		[batch 20/20] avg loss: 0.5766002244529684		[learning rate: 0.0077362]
	Learning Rate: 0.00773618
	LOSS [training: 0.6058575788184053 | validation: 0.5460826151109546]
	TIME [epoch: 8.45 sec]
EPOCH 104/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9411942667609006		[learning rate: 0.0077174]
		[batch 20/20] avg loss: 0.591023129593893		[learning rate: 0.0076988]
	Learning Rate: 0.00769876
	LOSS [training: 0.7661086981773967 | validation: 0.717060328074042]
	TIME [epoch: 8.43 sec]
EPOCH 105/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6337165557032935		[learning rate: 0.0076801]
		[batch 20/20] avg loss: 0.49663339590466027		[learning rate: 0.0076615]
	Learning Rate: 0.00766153
	LOSS [training: 0.5651749758039769 | validation: 0.7572526254517258]
	TIME [epoch: 8.41 sec]
EPOCH 106/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.558322604278682		[learning rate: 0.007643]
		[batch 20/20] avg loss: 0.5769265136758328		[learning rate: 0.0076245]
	Learning Rate: 0.00762449
	LOSS [training: 0.5676245589772574 | validation: 0.33284727592790114]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_106.pth
	Model improved!!!
EPOCH 107/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5002744194365629		[learning rate: 0.007606]
		[batch 20/20] avg loss: 0.6494465458617151		[learning rate: 0.0075876]
	Learning Rate: 0.00758761
	LOSS [training: 0.5748604826491388 | validation: 0.4595773278780891]
	TIME [epoch: 8.4 sec]
EPOCH 108/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.507687378452881		[learning rate: 0.0075692]
		[batch 20/20] avg loss: 0.6110818965633447		[learning rate: 0.0075509]
	Learning Rate: 0.00755092
	LOSS [training: 0.5593846375081127 | validation: 0.6706003651432088]
	TIME [epoch: 8.45 sec]
EPOCH 109/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7404328041643784		[learning rate: 0.0075326]
		[batch 20/20] avg loss: 0.5932627844850711		[learning rate: 0.0075144]
	Learning Rate: 0.00751441
	LOSS [training: 0.6668477943247246 | validation: 0.6678245487801997]
	TIME [epoch: 8.42 sec]
EPOCH 110/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5531405296962765		[learning rate: 0.0074962]
		[batch 20/20] avg loss: 0.6318699076858456		[learning rate: 0.0074781]
	Learning Rate: 0.00747807
	LOSS [training: 0.592505218691061 | validation: 0.8779000408517843]
	TIME [epoch: 8.39 sec]
EPOCH 111/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7376537593999678		[learning rate: 0.00746]
		[batch 20/20] avg loss: 0.7314823662978509		[learning rate: 0.0074419]
	Learning Rate: 0.00744191
	LOSS [training: 0.7345680628489093 | validation: 0.9167766780725904]
	TIME [epoch: 8.41 sec]
EPOCH 112/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.560794597401784		[learning rate: 0.0074239]
		[batch 20/20] avg loss: 0.5671994120819863		[learning rate: 0.0074059]
	Learning Rate: 0.00740592
	LOSS [training: 0.5639970047418853 | validation: 0.5174676366795535]
	TIME [epoch: 8.46 sec]
EPOCH 113/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42575719999233774		[learning rate: 0.007388]
		[batch 20/20] avg loss: 0.574932868743378		[learning rate: 0.0073701]
	Learning Rate: 0.00737011
	LOSS [training: 0.5003450343678578 | validation: 0.589852982291462]
	TIME [epoch: 8.41 sec]
EPOCH 114/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5998088178161133		[learning rate: 0.0073523]
		[batch 20/20] avg loss: 0.7043037899530176		[learning rate: 0.0073345]
	Learning Rate: 0.00733446
	LOSS [training: 0.6520563038845656 | validation: 0.8038837995485383]
	TIME [epoch: 8.42 sec]
EPOCH 115/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6683096107239899		[learning rate: 0.0073167]
		[batch 20/20] avg loss: 0.6254494837111506		[learning rate: 0.007299]
	Learning Rate: 0.007299
	LOSS [training: 0.6468795472175701 | validation: 0.5971806087674311]
	TIME [epoch: 8.43 sec]
EPOCH 116/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6731565933241027		[learning rate: 0.0072813]
		[batch 20/20] avg loss: 0.7623217088317235		[learning rate: 0.0072637]
	Learning Rate: 0.0072637
	LOSS [training: 0.7177391510779129 | validation: 0.6782166660173914]
	TIME [epoch: 8.42 sec]
EPOCH 117/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5623080148548285		[learning rate: 0.0072461]
		[batch 20/20] avg loss: 0.5858142682226848		[learning rate: 0.0072286]
	Learning Rate: 0.00722857
	LOSS [training: 0.5740611415387569 | validation: 0.4384606014458273]
	TIME [epoch: 8.44 sec]
EPOCH 118/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4611062035844055		[learning rate: 0.0072111]
		[batch 20/20] avg loss: 0.6103322780997148		[learning rate: 0.0071936]
	Learning Rate: 0.00719362
	LOSS [training: 0.5357192408420601 | validation: 0.681752504504675]
	TIME [epoch: 8.42 sec]
EPOCH 119/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.626206192882192		[learning rate: 0.0071762]
		[batch 20/20] avg loss: 0.5168597998566973		[learning rate: 0.0071588]
	Learning Rate: 0.00715883
	LOSS [training: 0.5715329963694448 | validation: 0.47320971424161945]
	TIME [epoch: 8.4 sec]
EPOCH 120/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5135871047738184		[learning rate: 0.0071415]
		[batch 20/20] avg loss: 0.5501470012515318		[learning rate: 0.0071242]
	Learning Rate: 0.00712421
	LOSS [training: 0.5318670530126751 | validation: 0.3740200680582248]
	TIME [epoch: 8.42 sec]
EPOCH 121/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5230390649945799		[learning rate: 0.007107]
		[batch 20/20] avg loss: 0.48641944357686545		[learning rate: 0.0070898]
	Learning Rate: 0.00708976
	LOSS [training: 0.5047292542857227 | validation: 0.38443057217116006]
	TIME [epoch: 8.46 sec]
EPOCH 122/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5734091838531012		[learning rate: 0.0070726]
		[batch 20/20] avg loss: 0.5246029938132667		[learning rate: 0.0070555]
	Learning Rate: 0.00705548
	LOSS [training: 0.5490060888331839 | validation: 0.772096412903176]
	TIME [epoch: 8.4 sec]
EPOCH 123/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5392301738630669		[learning rate: 0.0070384]
		[batch 20/20] avg loss: 0.48107786942533604		[learning rate: 0.0070214]
	Learning Rate: 0.00702136
	LOSS [training: 0.5101540216442016 | validation: 0.268455856033505]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_123.pth
	Model improved!!!
EPOCH 124/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4763680366677764		[learning rate: 0.0070044]
		[batch 20/20] avg loss: 0.4615214604590115		[learning rate: 0.0069874]
	Learning Rate: 0.0069874
	LOSS [training: 0.46894474856339396 | validation: 0.34733501773596376]
	TIME [epoch: 8.43 sec]
EPOCH 125/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4814472342286886		[learning rate: 0.0069705]
		[batch 20/20] avg loss: 0.516879571212709		[learning rate: 0.0069536]
	Learning Rate: 0.00695361
	LOSS [training: 0.4991634027206987 | validation: 0.4669293007068672]
	TIME [epoch: 8.43 sec]
EPOCH 126/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45663101679057616		[learning rate: 0.0069368]
		[batch 20/20] avg loss: 0.3944861846558968		[learning rate: 0.00692]
	Learning Rate: 0.00691999
	LOSS [training: 0.4255586007232365 | validation: 0.28614437799518483]
	TIME [epoch: 8.4 sec]
EPOCH 127/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4779985683434461		[learning rate: 0.0069032]
		[batch 20/20] avg loss: 0.4107145237768643		[learning rate: 0.0068865]
	Learning Rate: 0.00688652
	LOSS [training: 0.4443565460601552 | validation: 0.4171275319963431]
	TIME [epoch: 8.42 sec]
EPOCH 128/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4286940147633823		[learning rate: 0.0068699]
		[batch 20/20] avg loss: 0.48288411960763666		[learning rate: 0.0068532]
	Learning Rate: 0.00685322
	LOSS [training: 0.4557890671855095 | validation: 0.4690220406809434]
	TIME [epoch: 8.42 sec]
EPOCH 129/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45698222810121364		[learning rate: 0.0068366]
		[batch 20/20] avg loss: 0.5084120518777995		[learning rate: 0.0068201]
	Learning Rate: 0.00682008
	LOSS [training: 0.48269713998950675 | validation: 0.2867387713304106]
	TIME [epoch: 8.43 sec]
EPOCH 130/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5605399348717455		[learning rate: 0.0068036]
		[batch 20/20] avg loss: 0.5789597050100588		[learning rate: 0.0067871]
	Learning Rate: 0.0067871
	LOSS [training: 0.5697498199409021 | validation: 0.502715623249608]
	TIME [epoch: 8.41 sec]
EPOCH 131/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5082592548479644		[learning rate: 0.0067707]
		[batch 20/20] avg loss: 0.5248748789852082		[learning rate: 0.0067543]
	Learning Rate: 0.00675428
	LOSS [training: 0.5165670669165864 | validation: 0.5764334501982786]
	TIME [epoch: 8.43 sec]
EPOCH 132/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4572587022034183		[learning rate: 0.0067379]
		[batch 20/20] avg loss: 0.4105387538776927		[learning rate: 0.0067216]
	Learning Rate: 0.00672162
	LOSS [training: 0.4338987280405555 | validation: 0.304651438742728]
	TIME [epoch: 8.42 sec]
EPOCH 133/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4106504146485387		[learning rate: 0.0067053]
		[batch 20/20] avg loss: 0.4834378335401853		[learning rate: 0.0066891]
	Learning Rate: 0.00668911
	LOSS [training: 0.447044124094362 | validation: 0.890927132343791]
	TIME [epoch: 8.41 sec]
EPOCH 134/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.745467273146217		[learning rate: 0.0066729]
		[batch 20/20] avg loss: 0.47074932398374997		[learning rate: 0.0066568]
	Learning Rate: 0.00665676
	LOSS [training: 0.6081082985649835 | validation: 0.38131480263719064]
	TIME [epoch: 8.45 sec]
EPOCH 135/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4216200658605379		[learning rate: 0.0066406]
		[batch 20/20] avg loss: 0.4706142704046192		[learning rate: 0.0066246]
	Learning Rate: 0.00662457
	LOSS [training: 0.4461171681325785 | validation: 0.3316216610818853]
	TIME [epoch: 8.42 sec]
EPOCH 136/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4629260543035426		[learning rate: 0.0066085]
		[batch 20/20] avg loss: 0.44289883844283906		[learning rate: 0.0065925]
	Learning Rate: 0.00659254
	LOSS [training: 0.45291244637319095 | validation: 0.34763721614309095]
	TIME [epoch: 8.41 sec]
EPOCH 137/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5551631636157421		[learning rate: 0.0065766]
		[batch 20/20] avg loss: 0.4777020586445132		[learning rate: 0.0065607]
	Learning Rate: 0.00656066
	LOSS [training: 0.5164326111301276 | validation: 0.5641259202262152]
	TIME [epoch: 8.43 sec]
EPOCH 138/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5084983915001122		[learning rate: 0.0065448]
		[batch 20/20] avg loss: 0.40555223368759863		[learning rate: 0.0065289]
	Learning Rate: 0.00652893
	LOSS [training: 0.45702531259385537 | validation: 0.5084791917306741]
	TIME [epoch: 8.45 sec]
EPOCH 139/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6296263490582535		[learning rate: 0.0065131]
		[batch 20/20] avg loss: 0.41434985239009137		[learning rate: 0.0064974]
	Learning Rate: 0.00649736
	LOSS [training: 0.5219881007241725 | validation: 0.35044298939994306]
	TIME [epoch: 8.41 sec]
EPOCH 140/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38083103033958476		[learning rate: 0.0064816]
		[batch 20/20] avg loss: 0.39599317946279844		[learning rate: 0.0064659]
	Learning Rate: 0.00646594
	LOSS [training: 0.3884121049011916 | validation: 0.6907760177833779]
	TIME [epoch: 8.43 sec]
EPOCH 141/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40062503832594476		[learning rate: 0.0064503]
		[batch 20/20] avg loss: 0.4234872575187099		[learning rate: 0.0064347]
	Learning Rate: 0.00643467
	LOSS [training: 0.4120561479223273 | validation: 0.269987984180165]
	TIME [epoch: 8.42 sec]
EPOCH 142/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4086034299214788		[learning rate: 0.0064191]
		[batch 20/20] avg loss: 0.3986123227737547		[learning rate: 0.0064036]
	Learning Rate: 0.00640355
	LOSS [training: 0.40360787634761675 | validation: 0.676786249316033]
	TIME [epoch: 8.44 sec]
EPOCH 143/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4799158005225622		[learning rate: 0.0063881]
		[batch 20/20] avg loss: 0.38594529696423796		[learning rate: 0.0063726]
	Learning Rate: 0.00637259
	LOSS [training: 0.43293054874340003 | validation: 0.49448742708914156]
	TIME [epoch: 8.43 sec]
EPOCH 144/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4418087149952642		[learning rate: 0.0063572]
		[batch 20/20] avg loss: 0.4928723600326532		[learning rate: 0.0063418]
	Learning Rate: 0.00634177
	LOSS [training: 0.4673405375139586 | validation: 0.22859658781205006]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_144.pth
	Model improved!!!
EPOCH 145/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36588173276703106		[learning rate: 0.0063264]
		[batch 20/20] avg loss: 0.3694252708837888		[learning rate: 0.0063111]
	Learning Rate: 0.0063111
	LOSS [training: 0.36765350182540985 | validation: 0.34306016308046666]
	TIME [epoch: 8.41 sec]
EPOCH 146/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46721752246491544		[learning rate: 0.0062958]
		[batch 20/20] avg loss: 0.29811248369409843		[learning rate: 0.0062806]
	Learning Rate: 0.00628058
	LOSS [training: 0.382665003079507 | validation: 0.31126299155634285]
	TIME [epoch: 8.45 sec]
EPOCH 147/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37841267776754484		[learning rate: 0.0062654]
		[batch 20/20] avg loss: 0.4282356202921143		[learning rate: 0.0062502]
	Learning Rate: 0.00625021
	LOSS [training: 0.40332414902982955 | validation: 0.4419259507064738]
	TIME [epoch: 8.42 sec]
EPOCH 148/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3929078576652075		[learning rate: 0.0062351]
		[batch 20/20] avg loss: 0.31016419429682196		[learning rate: 0.00622]
	Learning Rate: 0.00621999
	LOSS [training: 0.3515360259810148 | validation: 0.20842876385447812]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_148.pth
	Model improved!!!
EPOCH 149/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4636048484752191		[learning rate: 0.0062049]
		[batch 20/20] avg loss: 0.3819169446741811		[learning rate: 0.0061899]
	Learning Rate: 0.00618991
	LOSS [training: 0.42276089657470006 | validation: 0.20205251166529953]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_149.pth
	Model improved!!!
EPOCH 150/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.38681340259579244		[learning rate: 0.0061749]
		[batch 20/20] avg loss: 0.32523207294771617		[learning rate: 0.00616]
	Learning Rate: 0.00615997
	LOSS [training: 0.3560227377717543 | validation: 0.3481802497209572]
	TIME [epoch: 8.4 sec]
EPOCH 151/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.441064910000902		[learning rate: 0.0061451]
		[batch 20/20] avg loss: 0.4031669969413911		[learning rate: 0.0061302]
	Learning Rate: 0.00613019
	LOSS [training: 0.42211595347114655 | validation: 0.3273527226706511]
	TIME [epoch: 8.44 sec]
EPOCH 152/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28628652161160967		[learning rate: 0.0061153]
		[batch 20/20] avg loss: 0.4246943660540331		[learning rate: 0.0061005]
	Learning Rate: 0.00610054
	LOSS [training: 0.35549044383282136 | validation: 0.36309473199860165]
	TIME [epoch: 8.42 sec]
EPOCH 153/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.37135900119366533		[learning rate: 0.0060858]
		[batch 20/20] avg loss: 0.3008151131995348		[learning rate: 0.006071]
	Learning Rate: 0.00607104
	LOSS [training: 0.33608705719660015 | validation: 0.4061583193275889]
	TIME [epoch: 8.4 sec]
EPOCH 154/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45102757185637243		[learning rate: 0.0060563]
		[batch 20/20] avg loss: 0.3893608496930102		[learning rate: 0.0060417]
	Learning Rate: 0.00604168
	LOSS [training: 0.4201942107746913 | validation: 0.38821618540815306]
	TIME [epoch: 8.41 sec]
EPOCH 155/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3821151551060526		[learning rate: 0.0060271]
		[batch 20/20] avg loss: 0.37604730188657876		[learning rate: 0.0060125]
	Learning Rate: 0.00601247
	LOSS [training: 0.37908122849631576 | validation: 0.37484527548758045]
	TIME [epoch: 8.44 sec]
EPOCH 156/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46075792106390523		[learning rate: 0.0059979]
		[batch 20/20] avg loss: 0.4373661139016239		[learning rate: 0.0059834]
	Learning Rate: 0.00598339
	LOSS [training: 0.4490620174827645 | validation: 0.49687196787091464]
	TIME [epoch: 8.4 sec]
EPOCH 157/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3910069319056325		[learning rate: 0.0059689]
		[batch 20/20] avg loss: 0.45328174277174293		[learning rate: 0.0059545]
	Learning Rate: 0.00595446
	LOSS [training: 0.4221443373386878 | validation: 0.7671210263560533]
	TIME [epoch: 8.41 sec]
EPOCH 158/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.448636265038113		[learning rate: 0.00594]
		[batch 20/20] avg loss: 0.3277516135109035		[learning rate: 0.0059257]
	Learning Rate: 0.00592566
	LOSS [training: 0.38819393927450824 | validation: 0.46634493570685603]
	TIME [epoch: 8.42 sec]
EPOCH 159/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3774903160248587		[learning rate: 0.0059113]
		[batch 20/20] avg loss: 0.3865826496913177		[learning rate: 0.005897]
	Learning Rate: 0.00589701
	LOSS [training: 0.38203648285808817 | validation: 0.19140970878004268]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_159.pth
	Model improved!!!
EPOCH 160/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3716605974910562		[learning rate: 0.0058827]
		[batch 20/20] avg loss: 0.3024582450696652		[learning rate: 0.0058685]
	Learning Rate: 0.00586849
	LOSS [training: 0.33705942128036065 | validation: 0.2644548180654498]
	TIME [epoch: 8.41 sec]
EPOCH 161/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4793418868945455		[learning rate: 0.0058543]
		[batch 20/20] avg loss: 0.3321869531212194		[learning rate: 0.0058401]
	Learning Rate: 0.00584011
	LOSS [training: 0.40576442000788243 | validation: 0.3156901895116166]
	TIME [epoch: 8.42 sec]
EPOCH 162/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3250366178560707		[learning rate: 0.005826]
		[batch 20/20] avg loss: 0.46512020748059946		[learning rate: 0.0058119]
	Learning Rate: 0.00581187
	LOSS [training: 0.3950784126683351 | validation: 0.2563332074438696]
	TIME [epoch: 8.41 sec]
EPOCH 163/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3346261746367952		[learning rate: 0.0057978]
		[batch 20/20] avg loss: 0.30389987907221233		[learning rate: 0.0057838]
	Learning Rate: 0.00578376
	LOSS [training: 0.31926302685450375 | validation: 0.3478759958687233]
	TIME [epoch: 8.4 sec]
EPOCH 164/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643106186604389		[learning rate: 0.0057698]
		[batch 20/20] avg loss: 0.45364987564302933		[learning rate: 0.0057558]
	Learning Rate: 0.00575579
	LOSS [training: 0.3589802471517341 | validation: 0.5704115003937675]
	TIME [epoch: 8.43 sec]
EPOCH 165/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3503757389410084		[learning rate: 0.0057419]
		[batch 20/20] avg loss: 0.369220813267143		[learning rate: 0.005728]
	Learning Rate: 0.00572796
	LOSS [training: 0.35979827610407566 | validation: 0.2439838751888217]
	TIME [epoch: 8.42 sec]
EPOCH 166/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36542075643186234		[learning rate: 0.0057141]
		[batch 20/20] avg loss: 0.29026182254817073		[learning rate: 0.0057003]
	Learning Rate: 0.00570026
	LOSS [training: 0.32784128949001656 | validation: 0.47552267259534486]
	TIME [epoch: 8.41 sec]
EPOCH 167/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4556037144924878		[learning rate: 0.0056865]
		[batch 20/20] avg loss: 0.3379263689134769		[learning rate: 0.0056727]
	Learning Rate: 0.0056727
	LOSS [training: 0.3967650417029823 | validation: 0.3083817265553242]
	TIME [epoch: 8.39 sec]
EPOCH 168/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3866590839132932		[learning rate: 0.005659]
		[batch 20/20] avg loss: 0.3340944575401972		[learning rate: 0.0056453]
	Learning Rate: 0.00564526
	LOSS [training: 0.36037677072674507 | validation: 0.23478302429320705]
	TIME [epoch: 8.43 sec]
EPOCH 169/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4360866530645683		[learning rate: 0.0056316]
		[batch 20/20] avg loss: 0.2573172057017828		[learning rate: 0.005618]
	Learning Rate: 0.00561796
	LOSS [training: 0.34670192938317546 | validation: 0.2638194316373696]
	TIME [epoch: 8.41 sec]
EPOCH 170/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3605446058345406		[learning rate: 0.0056044]
		[batch 20/20] avg loss: 0.27603428370779387		[learning rate: 0.0055908]
	Learning Rate: 0.0055908
	LOSS [training: 0.3182894447711672 | validation: 0.3760288043517796]
	TIME [epoch: 8.41 sec]
EPOCH 171/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46715301591734876		[learning rate: 0.0055773]
		[batch 20/20] avg loss: 0.35646096094153823		[learning rate: 0.0055638]
	Learning Rate: 0.00556376
	LOSS [training: 0.4118069884294434 | validation: 0.536020683352737]
	TIME [epoch: 8.4 sec]
EPOCH 172/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3298707870584978		[learning rate: 0.0055503]
		[batch 20/20] avg loss: 0.26706159151437686		[learning rate: 0.0055369]
	Learning Rate: 0.00553685
	LOSS [training: 0.2984661892864373 | validation: 0.17443111239073245]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_172.pth
	Model improved!!!
EPOCH 173/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3569704658858449		[learning rate: 0.0055235]
		[batch 20/20] avg loss: 0.3113540588313678		[learning rate: 0.0055101]
	Learning Rate: 0.00551008
	LOSS [training: 0.33416226235860635 | validation: 0.5249873597388691]
	TIME [epoch: 8.43 sec]
EPOCH 174/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3895271065822789		[learning rate: 0.0054967]
		[batch 20/20] avg loss: 0.28501995237714867		[learning rate: 0.0054834]
	Learning Rate: 0.00548343
	LOSS [training: 0.3372735294797138 | validation: 0.26360968154936826]
	TIME [epoch: 8.41 sec]
EPOCH 175/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30552576649508606		[learning rate: 0.0054702]
		[batch 20/20] avg loss: 0.3569120425768273		[learning rate: 0.0054569]
	Learning Rate: 0.00545692
	LOSS [training: 0.33121890453595676 | validation: 0.3609096991544278]
	TIME [epoch: 8.4 sec]
EPOCH 176/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3341917669237852		[learning rate: 0.0054437]
		[batch 20/20] avg loss: 0.28174438956905545		[learning rate: 0.0054305]
	Learning Rate: 0.00543053
	LOSS [training: 0.30796807824642036 | validation: 0.23566035586420472]
	TIME [epoch: 8.44 sec]
EPOCH 177/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2533142341215467		[learning rate: 0.0054174]
		[batch 20/20] avg loss: 0.3321254468565405		[learning rate: 0.0054043]
	Learning Rate: 0.00540427
	LOSS [training: 0.29271984048904354 | validation: 0.20059355023157205]
	TIME [epoch: 8.42 sec]
EPOCH 178/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29024443326433663		[learning rate: 0.0053912]
		[batch 20/20] avg loss: 0.2772145655386917		[learning rate: 0.0053781]
	Learning Rate: 0.00537813
	LOSS [training: 0.28372949940151415 | validation: 0.5178371422514317]
	TIME [epoch: 8.4 sec]
EPOCH 179/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3622140594152105		[learning rate: 0.0053651]
		[batch 20/20] avg loss: 0.2905377284043814		[learning rate: 0.0053521]
	Learning Rate: 0.00535213
	LOSS [training: 0.32637589390979593 | validation: 0.21352632831786125]
	TIME [epoch: 8.41 sec]
EPOCH 180/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25410412844444663		[learning rate: 0.0053392]
		[batch 20/20] avg loss: 0.2846633890569037		[learning rate: 0.0053262]
	Learning Rate: 0.00532624
	LOSS [training: 0.26938375875067516 | validation: 0.18332306851473557]
	TIME [epoch: 8.42 sec]
EPOCH 181/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33225253696934554		[learning rate: 0.0053133]
		[batch 20/20] avg loss: 0.2769419936137961		[learning rate: 0.0053005]
	Learning Rate: 0.00530049
	LOSS [training: 0.30459726529157083 | validation: 0.28772572021260345]
	TIME [epoch: 8.44 sec]
EPOCH 182/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3631328442063165		[learning rate: 0.0052877]
		[batch 20/20] avg loss: 0.31306644093641595		[learning rate: 0.0052749]
	Learning Rate: 0.00527485
	LOSS [training: 0.33809964257136615 | validation: 0.20092185175902616]
	TIME [epoch: 8.39 sec]
EPOCH 183/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24943639585002356		[learning rate: 0.0052621]
		[batch 20/20] avg loss: 0.34482171863776034		[learning rate: 0.0052493]
	Learning Rate: 0.00524935
	LOSS [training: 0.29712905724389194 | validation: 0.38755107942945366]
	TIME [epoch: 8.42 sec]
EPOCH 184/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31668991027038396		[learning rate: 0.0052366]
		[batch 20/20] avg loss: 0.3032132595650932		[learning rate: 0.005224]
	Learning Rate: 0.00522396
	LOSS [training: 0.30995158491773855 | validation: 0.22174569376426662]
	TIME [epoch: 8.42 sec]
EPOCH 185/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24660987176935292		[learning rate: 0.0052113]
		[batch 20/20] avg loss: 0.3421169674854605		[learning rate: 0.0051987]
	Learning Rate: 0.0051987
	LOSS [training: 0.2943634196274067 | validation: 0.5499000188117013]
	TIME [epoch: 8.44 sec]
EPOCH 186/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2880583982524493		[learning rate: 0.0051861]
		[batch 20/20] avg loss: 0.3557546322694988		[learning rate: 0.0051736]
	Learning Rate: 0.00517356
	LOSS [training: 0.3219065152609741 | validation: 0.28542413538843264]
	TIME [epoch: 8.4 sec]
EPOCH 187/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3263240826705497		[learning rate: 0.005161]
		[batch 20/20] avg loss: 0.23913117691671		[learning rate: 0.0051485]
	Learning Rate: 0.00514854
	LOSS [training: 0.2827276297936299 | validation: 0.2240212208202177]
	TIME [epoch: 8.42 sec]
EPOCH 188/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3446327433952058		[learning rate: 0.0051361]
		[batch 20/20] avg loss: 0.2743444264914595		[learning rate: 0.0051236]
	Learning Rate: 0.00512364
	LOSS [training: 0.3094885849433327 | validation: 0.46415301678734866]
	TIME [epoch: 8.41 sec]
EPOCH 189/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3237470879181426		[learning rate: 0.0051112]
		[batch 20/20] avg loss: 0.3969838276758873		[learning rate: 0.0050989]
	Learning Rate: 0.00509887
	LOSS [training: 0.3603654577970149 | validation: 0.3434684795050972]
	TIME [epoch: 8.42 sec]
EPOCH 190/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3545986571045175		[learning rate: 0.0050865]
		[batch 20/20] avg loss: 0.3269203600279792		[learning rate: 0.0050742]
	Learning Rate: 0.00507421
	LOSS [training: 0.3407595085662483 | validation: 0.28635660503242777]
	TIME [epoch: 8.43 sec]
EPOCH 191/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28651487248934177		[learning rate: 0.0050619]
		[batch 20/20] avg loss: 0.32705928003965484		[learning rate: 0.0050497]
	Learning Rate: 0.00504967
	LOSS [training: 0.3067870762644983 | validation: 0.23279736945722412]
	TIME [epoch: 8.43 sec]
EPOCH 192/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2841610599520236		[learning rate: 0.0050374]
		[batch 20/20] avg loss: 0.39490326007933974		[learning rate: 0.0050253]
	Learning Rate: 0.00502525
	LOSS [training: 0.3395321600156817 | validation: 0.28482177959373706]
	TIME [epoch: 8.4 sec]
EPOCH 193/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26480702122778715		[learning rate: 0.0050131]
		[batch 20/20] avg loss: 0.302549112271372		[learning rate: 0.005001]
	Learning Rate: 0.00500095
	LOSS [training: 0.28367806674957957 | validation: 0.2526250664128342]
	TIME [epoch: 8.41 sec]
EPOCH 194/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3106274417613826		[learning rate: 0.0049888]
		[batch 20/20] avg loss: 0.3306806571208225		[learning rate: 0.0049768]
	Learning Rate: 0.00497677
	LOSS [training: 0.3206540494411025 | validation: 0.332115909002062]
	TIME [epoch: 8.46 sec]
EPOCH 195/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2564149271471669		[learning rate: 0.0049647]
		[batch 20/20] avg loss: 0.34382283995872504		[learning rate: 0.0049527]
	Learning Rate: 0.0049527
	LOSS [training: 0.300118883552946 | validation: 0.661210578782194]
	TIME [epoch: 8.4 sec]
EPOCH 196/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.327531866321974		[learning rate: 0.0049407]
		[batch 20/20] avg loss: 0.23191788743950087		[learning rate: 0.0049288]
	Learning Rate: 0.00492875
	LOSS [training: 0.27972487688073744 | validation: 0.18643705531204116]
	TIME [epoch: 8.42 sec]
EPOCH 197/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3759304193146587		[learning rate: 0.0049168]
		[batch 20/20] avg loss: 0.24569194384891818		[learning rate: 0.0049049]
	Learning Rate: 0.00490492
	LOSS [training: 0.31081118158178844 | validation: 0.26159688727664226]
	TIME [epoch: 8.43 sec]
EPOCH 198/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27843807404439574		[learning rate: 0.004893]
		[batch 20/20] avg loss: 0.27413528128663306		[learning rate: 0.0048812]
	Learning Rate: 0.0048812
	LOSS [training: 0.27628667766551446 | validation: 0.20759605329238393]
	TIME [epoch: 8.44 sec]
EPOCH 199/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20778689576120862		[learning rate: 0.0048694]
		[batch 20/20] avg loss: 0.37688805132760106		[learning rate: 0.0048576]
	Learning Rate: 0.00485759
	LOSS [training: 0.2923374735444048 | validation: 0.3412399647615336]
	TIME [epoch: 8.42 sec]
EPOCH 200/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2670372362560888		[learning rate: 0.0048458]
		[batch 20/20] avg loss: 0.2685670606382514		[learning rate: 0.0048341]
	Learning Rate: 0.0048341
	LOSS [training: 0.2678021484471701 | validation: 0.21800173928183952]
	TIME [epoch: 8.43 sec]
EPOCH 201/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.279992763411302		[learning rate: 0.0048224]
		[batch 20/20] avg loss: 0.23456510882831166		[learning rate: 0.0048107]
	Learning Rate: 0.00481072
	LOSS [training: 0.2572789361198068 | validation: 0.2147044278800182]
	TIME [epoch: 8.4 sec]
EPOCH 202/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2894625746788203		[learning rate: 0.0047991]
		[batch 20/20] avg loss: 0.2773148721937776		[learning rate: 0.0047875]
	Learning Rate: 0.00478746
	LOSS [training: 0.28338872343629895 | validation: 0.30662532805689935]
	TIME [epoch: 8.43 sec]
EPOCH 203/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29188723007412415		[learning rate: 0.0047759]
		[batch 20/20] avg loss: 0.26961946063701714		[learning rate: 0.0047643]
	Learning Rate: 0.00476431
	LOSS [training: 0.2807533453555707 | validation: 0.38656594264495237]
	TIME [epoch: 8.45 sec]
EPOCH 204/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3414526597206855		[learning rate: 0.0047528]
		[batch 20/20] avg loss: 0.34724206608610936		[learning rate: 0.0047413]
	Learning Rate: 0.00474127
	LOSS [training: 0.3443473629033974 | validation: 1.2514857202751475]
	TIME [epoch: 8.41 sec]
EPOCH 205/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.36539778531935146		[learning rate: 0.0047298]
		[batch 20/20] avg loss: 0.23796419138212643		[learning rate: 0.0047183]
	Learning Rate: 0.00471834
	LOSS [training: 0.30168098835073887 | validation: 0.3438853096864364]
	TIME [epoch: 8.41 sec]
EPOCH 206/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2712582686814819		[learning rate: 0.0047069]
		[batch 20/20] avg loss: 0.24582748407083593		[learning rate: 0.0046955]
	Learning Rate: 0.00469553
	LOSS [training: 0.25854287637615897 | validation: 0.29960898340345804]
	TIME [epoch: 8.44 sec]
EPOCH 207/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28116690782755027		[learning rate: 0.0046842]
		[batch 20/20] avg loss: 0.2578320946396382		[learning rate: 0.0046728]
	Learning Rate: 0.00467282
	LOSS [training: 0.26949950123359423 | validation: 0.27455666700211084]
	TIME [epoch: 8.45 sec]
EPOCH 208/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2862707179293729		[learning rate: 0.0046615]
		[batch 20/20] avg loss: 0.25621366319461036		[learning rate: 0.0046502]
	Learning Rate: 0.00465022
	LOSS [training: 0.2712421905619916 | validation: 0.25298712747095664]
	TIME [epoch: 8.41 sec]
EPOCH 209/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2691349208180075		[learning rate: 0.004639]
		[batch 20/20] avg loss: 0.2442233102408855		[learning rate: 0.0046277]
	Learning Rate: 0.00462773
	LOSS [training: 0.2566791155294465 | validation: 0.30998783713963673]
	TIME [epoch: 8.43 sec]
EPOCH 210/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24829648464288762		[learning rate: 0.0046165]
		[batch 20/20] avg loss: 0.30547441607668724		[learning rate: 0.0046054]
	Learning Rate: 0.00460536
	LOSS [training: 0.2768854503597874 | validation: 0.5134441608588424]
	TIME [epoch: 8.43 sec]
EPOCH 211/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.30003925828262296		[learning rate: 0.0045942]
		[batch 20/20] avg loss: 0.3548252669903095		[learning rate: 0.0045831]
	Learning Rate: 0.00458308
	LOSS [training: 0.3274322626364662 | validation: 0.3617880596914129]
	TIME [epoch: 8.44 sec]
EPOCH 212/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29627255568838046		[learning rate: 0.004572]
		[batch 20/20] avg loss: 0.2959751195490038		[learning rate: 0.0045609]
	Learning Rate: 0.00456092
	LOSS [training: 0.2961238376186922 | validation: 0.23358564892969993]
	TIME [epoch: 8.42 sec]
EPOCH 213/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2710012014932521		[learning rate: 0.0045499]
		[batch 20/20] avg loss: 0.30936858911634035		[learning rate: 0.0045389]
	Learning Rate: 0.00453887
	LOSS [training: 0.29018489530479624 | validation: 0.4633230627458004]
	TIME [epoch: 8.43 sec]
EPOCH 214/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.297768638005467		[learning rate: 0.0045279]
		[batch 20/20] avg loss: 0.30493009623884193		[learning rate: 0.0045169]
	Learning Rate: 0.00451692
	LOSS [training: 0.3013493671221545 | validation: 0.16806105731880278]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_214.pth
	Model improved!!!
EPOCH 215/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2556096851807136		[learning rate: 0.004506]
		[batch 20/20] avg loss: 0.2588012781140406		[learning rate: 0.0044951]
	Learning Rate: 0.00449507
	LOSS [training: 0.25720548164737705 | validation: 0.13068722470044686]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_215.pth
	Model improved!!!
EPOCH 216/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2962672728003216		[learning rate: 0.0044842]
		[batch 20/20] avg loss: 0.24705229343881469		[learning rate: 0.0044733]
	Learning Rate: 0.00447334
	LOSS [training: 0.2716597831195681 | validation: 0.13963633318344215]
	TIME [epoch: 8.42 sec]
EPOCH 217/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2446634849587197		[learning rate: 0.0044625]
		[batch 20/20] avg loss: 0.22335000322889673		[learning rate: 0.0044517]
	Learning Rate: 0.0044517
	LOSS [training: 0.23400674409380823 | validation: 0.18608976522837017]
	TIME [epoch: 8.4 sec]
EPOCH 218/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2709384276834174		[learning rate: 0.0044409]
		[batch 20/20] avg loss: 0.2560727987908388		[learning rate: 0.0044302]
	Learning Rate: 0.00443018
	LOSS [training: 0.26350561323712807 | validation: 0.2595038055052217]
	TIME [epoch: 8.39 sec]
EPOCH 219/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3048674504149492		[learning rate: 0.0044195]
		[batch 20/20] avg loss: 0.2892276957437047		[learning rate: 0.0044088]
	Learning Rate: 0.00440875
	LOSS [training: 0.29704757307932705 | validation: 0.22979576290208395]
	TIME [epoch: 8.42 sec]
EPOCH 220/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27542751682906563		[learning rate: 0.0043981]
		[batch 20/20] avg loss: 0.2851056097038026		[learning rate: 0.0043874]
	Learning Rate: 0.00438743
	LOSS [training: 0.2802665632664342 | validation: 0.35494111029914976]
	TIME [epoch: 8.43 sec]
EPOCH 221/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.265894572377381		[learning rate: 0.0043768]
		[batch 20/20] avg loss: 0.30421969798482923		[learning rate: 0.0043662]
	Learning Rate: 0.00436622
	LOSS [training: 0.2850571351811052 | validation: 0.33981879752634486]
	TIME [epoch: 8.4 sec]
EPOCH 222/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31750205919089763		[learning rate: 0.0043556]
		[batch 20/20] avg loss: 0.2594920511355313		[learning rate: 0.0043451]
	Learning Rate: 0.0043451
	LOSS [training: 0.2884970551632144 | validation: 0.1865250231957151]
	TIME [epoch: 8.4 sec]
EPOCH 223/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2401828589919282		[learning rate: 0.0043346]
		[batch 20/20] avg loss: 0.28623680212078007		[learning rate: 0.0043241]
	Learning Rate: 0.00432409
	LOSS [training: 0.26320983055635405 | validation: 0.17506805756056462]
	TIME [epoch: 8.42 sec]
EPOCH 224/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22306519838758235		[learning rate: 0.0043136]
		[batch 20/20] avg loss: 0.2517138974141425		[learning rate: 0.0043032]
	Learning Rate: 0.00430318
	LOSS [training: 0.23738954790086247 | validation: 0.18819080254906856]
	TIME [epoch: 8.43 sec]
EPOCH 225/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2038543245902224		[learning rate: 0.0042928]
		[batch 20/20] avg loss: 0.23744003742307926		[learning rate: 0.0042824]
	Learning Rate: 0.00428237
	LOSS [training: 0.22064718100665087 | validation: 0.12648664828650857]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_225.pth
	Model improved!!!
EPOCH 226/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24409704609887575		[learning rate: 0.004272]
		[batch 20/20] avg loss: 0.2704445965648207		[learning rate: 0.0042617]
	Learning Rate: 0.00426166
	LOSS [training: 0.25727082133184825 | validation: 0.20156264745735747]
	TIME [epoch: 8.41 sec]
EPOCH 227/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.34869906757552177		[learning rate: 0.0042513]
		[batch 20/20] avg loss: 0.22594978761768886		[learning rate: 0.0042411]
	Learning Rate: 0.00424105
	LOSS [training: 0.28732442759660526 | validation: 0.2610511891098575]
	TIME [epoch: 8.4 sec]
EPOCH 228/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2596078827740312		[learning rate: 0.0042308]
		[batch 20/20] avg loss: 0.2153815862225264		[learning rate: 0.0042205]
	Learning Rate: 0.00422054
	LOSS [training: 0.2374947344982788 | validation: 0.14486737223172608]
	TIME [epoch: 8.41 sec]
EPOCH 229/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.33355934473588056		[learning rate: 0.0042103]
		[batch 20/20] avg loss: 0.2640467175714764		[learning rate: 0.0042001]
	Learning Rate: 0.00420013
	LOSS [training: 0.2988030311536785 | validation: 0.14649494554191095]
	TIME [epoch: 8.39 sec]
EPOCH 230/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2859191191990729		[learning rate: 0.00419]
		[batch 20/20] avg loss: 0.22300481168079572		[learning rate: 0.0041798]
	Learning Rate: 0.00417982
	LOSS [training: 0.25446196543993427 | validation: 0.14339148165844032]
	TIME [epoch: 8.4 sec]
EPOCH 231/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2688983044984819		[learning rate: 0.0041697]
		[batch 20/20] avg loss: 0.31041049731374926		[learning rate: 0.0041596]
	Learning Rate: 0.00415961
	LOSS [training: 0.2896544009061156 | validation: 0.1906012957111944]
	TIME [epoch: 8.39 sec]
EPOCH 232/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27089090194512666		[learning rate: 0.0041495]
		[batch 20/20] avg loss: 0.2363077712527546		[learning rate: 0.0041395]
	Learning Rate: 0.0041395
	LOSS [training: 0.2535993365989406 | validation: 0.1267040093882073]
	TIME [epoch: 8.39 sec]
EPOCH 233/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22104391841374244		[learning rate: 0.0041295]
		[batch 20/20] avg loss: 0.2483868987435205		[learning rate: 0.0041195]
	Learning Rate: 0.00411948
	LOSS [training: 0.23471540857863146 | validation: 0.12733417223980203]
	TIME [epoch: 8.42 sec]
EPOCH 234/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2707977108944464		[learning rate: 0.0041095]
		[batch 20/20] avg loss: 0.28627813686410086		[learning rate: 0.0040996]
	Learning Rate: 0.00409956
	LOSS [training: 0.2785379238792737 | validation: 0.251357522911979]
	TIME [epoch: 8.41 sec]
EPOCH 235/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18642500195348966		[learning rate: 0.0040896]
		[batch 20/20] avg loss: 0.2637412217802694		[learning rate: 0.0040797]
	Learning Rate: 0.00407973
	LOSS [training: 0.22508311186687954 | validation: 0.22884162881383086]
	TIME [epoch: 8.38 sec]
EPOCH 236/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2342257103237233		[learning rate: 0.0040699]
		[batch 20/20] avg loss: 0.24646515557730356		[learning rate: 0.00406]
	Learning Rate: 0.00406
	LOSS [training: 0.2403454329505134 | validation: 0.13701644331944324]
	TIME [epoch: 8.41 sec]
EPOCH 237/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25570201285214467		[learning rate: 0.0040502]
		[batch 20/20] avg loss: 0.246667729639501		[learning rate: 0.0040404]
	Learning Rate: 0.00404037
	LOSS [training: 0.25118487124582284 | validation: 0.2124793192217901]
	TIME [epoch: 8.44 sec]
EPOCH 238/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3010578441628619		[learning rate: 0.0040306]
		[batch 20/20] avg loss: 0.26273495680155545		[learning rate: 0.0040208]
	Learning Rate: 0.00402083
	LOSS [training: 0.2818964004822086 | validation: 0.3282589578337982]
	TIME [epoch: 8.39 sec]
EPOCH 239/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29942290964563434		[learning rate: 0.0040111]
		[batch 20/20] avg loss: 0.2140305607184816		[learning rate: 0.0040014]
	Learning Rate: 0.00400139
	LOSS [training: 0.2567267351820579 | validation: 0.15390524432411595]
	TIME [epoch: 8.4 sec]
EPOCH 240/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22742656731123412		[learning rate: 0.0039917]
		[batch 20/20] avg loss: 0.22620088570655175		[learning rate: 0.003982]
	Learning Rate: 0.00398204
	LOSS [training: 0.2268137265088929 | validation: 0.24323420726325143]
	TIME [epoch: 8.41 sec]
EPOCH 241/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.27239890800903127		[learning rate: 0.0039724]
		[batch 20/20] avg loss: 0.2135078098445875		[learning rate: 0.0039628]
	Learning Rate: 0.00396278
	LOSS [training: 0.24295335892680944 | validation: 0.13391702843162334]
	TIME [epoch: 8.42 sec]
EPOCH 242/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2519810268594316		[learning rate: 0.0039532]
		[batch 20/20] avg loss: 0.2550871411201293		[learning rate: 0.0039436]
	Learning Rate: 0.00394362
	LOSS [training: 0.2535340839897805 | validation: 0.13225850607876224]
	TIME [epoch: 8.4 sec]
EPOCH 243/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21337473140264374		[learning rate: 0.0039341]
		[batch 20/20] avg loss: 0.2890123282332965		[learning rate: 0.0039245]
	Learning Rate: 0.00392455
	LOSS [training: 0.2511935298179701 | validation: 0.3458810292318677]
	TIME [epoch: 8.42 sec]
EPOCH 244/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2406254118538588		[learning rate: 0.003915]
		[batch 20/20] avg loss: 0.2404393956582517		[learning rate: 0.0039056]
	Learning Rate: 0.00390557
	LOSS [training: 0.24053240375605528 | validation: 0.35281327425617953]
	TIME [epoch: 8.41 sec]
EPOCH 245/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.29055351396039664		[learning rate: 0.0038961]
		[batch 20/20] avg loss: 0.2799034517050681		[learning rate: 0.0038867]
	Learning Rate: 0.00388668
	LOSS [training: 0.28522848283273233 | validation: 0.14701929229129806]
	TIME [epoch: 8.44 sec]
EPOCH 246/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24371206263369444		[learning rate: 0.0038773]
		[batch 20/20] avg loss: 0.19923405006006126		[learning rate: 0.0038679]
	Learning Rate: 0.00386789
	LOSS [training: 0.2214730563468778 | validation: 0.3475867809530874]
	TIME [epoch: 8.42 sec]
EPOCH 247/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2680291544195324		[learning rate: 0.0038585]
		[batch 20/20] avg loss: 0.2557012435692338		[learning rate: 0.0038492]
	Learning Rate: 0.00384918
	LOSS [training: 0.2618651989943831 | validation: 0.22873293496844233]
	TIME [epoch: 8.41 sec]
EPOCH 248/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22733105983058444		[learning rate: 0.0038399]
		[batch 20/20] avg loss: 0.24997999477411853		[learning rate: 0.0038306]
	Learning Rate: 0.00383057
	LOSS [training: 0.23865552730235154 | validation: 0.1378626383959013]
	TIME [epoch: 8.41 sec]
EPOCH 249/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23260719006775193		[learning rate: 0.0038213]
		[batch 20/20] avg loss: 0.2637435481753011		[learning rate: 0.003812]
	Learning Rate: 0.00381204
	LOSS [training: 0.24817536912152657 | validation: 0.2488056585815693]
	TIME [epoch: 8.39 sec]
EPOCH 250/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2887650091148227		[learning rate: 0.0038028]
		[batch 20/20] avg loss: 0.23428376129185594		[learning rate: 0.0037936]
	Learning Rate: 0.00379361
	LOSS [training: 0.2615243852033393 | validation: 0.17893574080448346]
	TIME [epoch: 8.42 sec]
EPOCH 251/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23253350835583114		[learning rate: 0.0037844]
		[batch 20/20] avg loss: 0.217100601022972		[learning rate: 0.0037753]
	Learning Rate: 0.00377526
	LOSS [training: 0.22481705468940155 | validation: 0.2583682029228066]
	TIME [epoch: 8.41 sec]
EPOCH 252/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20664778612812684		[learning rate: 0.0037661]
		[batch 20/20] avg loss: 0.24802334735361414		[learning rate: 0.003757]
	Learning Rate: 0.00375701
	LOSS [training: 0.22733556674087052 | validation: 0.31306428119876906]
	TIME [epoch: 8.39 sec]
EPOCH 253/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23136205355677855		[learning rate: 0.0037479]
		[batch 20/20] avg loss: 0.27108405561253235		[learning rate: 0.0037388]
	Learning Rate: 0.00373884
	LOSS [training: 0.25122305458465544 | validation: 0.2366516338711401]
	TIME [epoch: 8.39 sec]
EPOCH 254/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19749146191601444		[learning rate: 0.0037298]
		[batch 20/20] avg loss: 0.24579082321300721		[learning rate: 0.0037208]
	Learning Rate: 0.00372076
	LOSS [training: 0.22164114256451084 | validation: 0.1573674982639315]
	TIME [epoch: 8.44 sec]
EPOCH 255/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20626824255281093		[learning rate: 0.0037118]
		[batch 20/20] avg loss: 0.24617236665990133		[learning rate: 0.0037028]
	Learning Rate: 0.00370277
	LOSS [training: 0.22622030460635614 | validation: 0.18522410844576792]
	TIME [epoch: 8.39 sec]
EPOCH 256/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2643517253939195		[learning rate: 0.0036938]
		[batch 20/20] avg loss: 0.22327472092061534		[learning rate: 0.0036849]
	Learning Rate: 0.00368486
	LOSS [training: 0.24381322315726753 | validation: 0.2805106285927947]
	TIME [epoch: 8.4 sec]
EPOCH 257/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24881215117368183		[learning rate: 0.0036759]
		[batch 20/20] avg loss: 0.29201114400540373		[learning rate: 0.003667]
	Learning Rate: 0.00366704
	LOSS [training: 0.2704116475895428 | validation: 0.2652039079774843]
	TIME [epoch: 8.41 sec]
EPOCH 258/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2544304292289747		[learning rate: 0.0036582]
		[batch 20/20] avg loss: 0.2072465831163067		[learning rate: 0.0036493]
	Learning Rate: 0.00364931
	LOSS [training: 0.23083850617264073 | validation: 0.24025252487675536]
	TIME [epoch: 8.41 sec]
EPOCH 259/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21902274882370926		[learning rate: 0.0036405]
		[batch 20/20] avg loss: 0.2670045350667386		[learning rate: 0.0036317]
	Learning Rate: 0.00363166
	LOSS [training: 0.24301364194522387 | validation: 0.3011722036235195]
	TIME [epoch: 8.41 sec]
EPOCH 260/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2813145885435421		[learning rate: 0.0036229]
		[batch 20/20] avg loss: 0.2538698413293826		[learning rate: 0.0036141]
	Learning Rate: 0.0036141
	LOSS [training: 0.2675922149364623 | validation: 0.15655277208592253]
	TIME [epoch: 8.4 sec]
EPOCH 261/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23862975914835313		[learning rate: 0.0036053]
		[batch 20/20] avg loss: 0.2588919644524551		[learning rate: 0.0035966]
	Learning Rate: 0.00359662
	LOSS [training: 0.24876086180040408 | validation: 0.21876501173811588]
	TIME [epoch: 8.39 sec]
EPOCH 262/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2477316460030806		[learning rate: 0.0035879]
		[batch 20/20] avg loss: 0.3086253052864037		[learning rate: 0.0035792]
	Learning Rate: 0.00357923
	LOSS [training: 0.27817847564474213 | validation: 0.27006021354238546]
	TIME [epoch: 8.43 sec]
EPOCH 263/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2655652678843348		[learning rate: 0.0035706]
		[batch 20/20] avg loss: 0.20827089314593267		[learning rate: 0.0035619]
	Learning Rate: 0.00356192
	LOSS [training: 0.23691808051513372 | validation: 0.16523134131927963]
	TIME [epoch: 8.4 sec]
EPOCH 264/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21695390530535236		[learning rate: 0.0035533]
		[batch 20/20] avg loss: 0.22955175993960603		[learning rate: 0.0035447]
	Learning Rate: 0.0035447
	LOSS [training: 0.2232528326224792 | validation: 0.3067845966441674]
	TIME [epoch: 8.4 sec]
EPOCH 265/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2763721514800792		[learning rate: 0.0035361]
		[batch 20/20] avg loss: 0.2483397452929829		[learning rate: 0.0035276]
	Learning Rate: 0.00352755
	LOSS [training: 0.2623559483865311 | validation: 0.2748980450027765]
	TIME [epoch: 8.4 sec]
EPOCH 266/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25935827309710946		[learning rate: 0.003519]
		[batch 20/20] avg loss: 0.3219564885832763		[learning rate: 0.0035105]
	Learning Rate: 0.0035105
	LOSS [training: 0.29065738084019294 | validation: 0.19380003408860197]
	TIME [epoch: 8.38 sec]
EPOCH 267/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3347872884500833		[learning rate: 0.003502]
		[batch 20/20] avg loss: 0.20808424691146107		[learning rate: 0.0034935]
	Learning Rate: 0.00349352
	LOSS [training: 0.27143576768077216 | validation: 0.2809645540001897]
	TIME [epoch: 8.44 sec]
EPOCH 268/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22843600551207804		[learning rate: 0.0034851]
		[batch 20/20] avg loss: 0.2123969625612696		[learning rate: 0.0034766]
	Learning Rate: 0.00347663
	LOSS [training: 0.2204164840366738 | validation: 0.16079508940346393]
	TIME [epoch: 8.4 sec]
EPOCH 269/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15722876760288343		[learning rate: 0.0034682]
		[batch 20/20] avg loss: 0.24279663896458498		[learning rate: 0.0034598]
	Learning Rate: 0.00345981
	LOSS [training: 0.20001270328373422 | validation: 0.1908540984298866]
	TIME [epoch: 8.39 sec]
EPOCH 270/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18471514373780065		[learning rate: 0.0034514]
		[batch 20/20] avg loss: 0.22049302709786098		[learning rate: 0.0034431]
	Learning Rate: 0.00344308
	LOSS [training: 0.20260408541783087 | validation: 0.18003702656836568]
	TIME [epoch: 8.41 sec]
EPOCH 271/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18792504955794126		[learning rate: 0.0034347]
		[batch 20/20] avg loss: 0.19415112799951678		[learning rate: 0.0034264]
	Learning Rate: 0.00342643
	LOSS [training: 0.19103808877872902 | validation: 0.2955987068366078]
	TIME [epoch: 8.42 sec]
EPOCH 272/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2047368101368782		[learning rate: 0.0034181]
		[batch 20/20] avg loss: 0.21139771829768322		[learning rate: 0.0034099]
	Learning Rate: 0.00340986
	LOSS [training: 0.20806726421728072 | validation: 0.15172005951615078]
	TIME [epoch: 8.4 sec]
EPOCH 273/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20573125509274776		[learning rate: 0.0034016]
		[batch 20/20] avg loss: 0.21923232269123455		[learning rate: 0.0033934]
	Learning Rate: 0.00339337
	LOSS [training: 0.21248178889199115 | validation: 0.17624295860428246]
	TIME [epoch: 8.41 sec]
EPOCH 274/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22641297285950465		[learning rate: 0.0033852]
		[batch 20/20] avg loss: 0.33181700537859526		[learning rate: 0.003377]
	Learning Rate: 0.00337696
	LOSS [training: 0.27911498911905 | validation: 0.28818872362399905]
	TIME [epoch: 8.38 sec]
EPOCH 275/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20049615650976454		[learning rate: 0.0033688]
		[batch 20/20] avg loss: 0.17432757543947958		[learning rate: 0.0033606]
	Learning Rate: 0.00336063
	LOSS [training: 0.18741186597462203 | validation: 0.10958538949750557]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_275.pth
	Model improved!!!
EPOCH 276/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24054196475723452		[learning rate: 0.0033525]
		[batch 20/20] avg loss: 0.23794213455126725		[learning rate: 0.0033444]
	Learning Rate: 0.00334438
	LOSS [training: 0.23924204965425094 | validation: 0.2903421756148834]
	TIME [epoch: 8.46 sec]
EPOCH 277/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18832559573520957		[learning rate: 0.0033363]
		[batch 20/20] avg loss: 0.19345957762133173		[learning rate: 0.0033282]
	Learning Rate: 0.00332821
	LOSS [training: 0.19089258667827064 | validation: 0.13381615969568575]
	TIME [epoch: 8.44 sec]
EPOCH 278/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24293874557852782		[learning rate: 0.0033202]
		[batch 20/20] avg loss: 0.21021949553778918		[learning rate: 0.0033121]
	Learning Rate: 0.00331211
	LOSS [training: 0.2265791205581585 | validation: 0.26303428240983967]
	TIME [epoch: 8.46 sec]
EPOCH 279/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24712251445322053		[learning rate: 0.0033041]
		[batch 20/20] avg loss: 0.21265353415837734		[learning rate: 0.0032961]
	Learning Rate: 0.0032961
	LOSS [training: 0.22988802430579897 | validation: 0.26228803613366247]
	TIME [epoch: 8.43 sec]
EPOCH 280/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19729359798935303		[learning rate: 0.0032881]
		[batch 20/20] avg loss: 0.21310361747639517		[learning rate: 0.0032802]
	Learning Rate: 0.00328016
	LOSS [training: 0.2051986077328741 | validation: 0.20473950723500345]
	TIME [epoch: 8.46 sec]
EPOCH 281/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2766022960901848		[learning rate: 0.0032722]
		[batch 20/20] avg loss: 0.16912601022472146		[learning rate: 0.0032643]
	Learning Rate: 0.0032643
	LOSS [training: 0.22286415315745306 | validation: 0.23495001991073056]
	TIME [epoch: 8.47 sec]
EPOCH 282/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24000422381474115		[learning rate: 0.0032564]
		[batch 20/20] avg loss: 0.19814948515485448		[learning rate: 0.0032485]
	Learning Rate: 0.00324851
	LOSS [training: 0.21907685448479786 | validation: 0.22431417615221982]
	TIME [epoch: 8.43 sec]
EPOCH 283/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22247265548612422		[learning rate: 0.0032406]
		[batch 20/20] avg loss: 0.22224967617013452		[learning rate: 0.0032328]
	Learning Rate: 0.0032328
	LOSS [training: 0.2223611658281294 | validation: 0.22552736147931085]
	TIME [epoch: 8.43 sec]
EPOCH 284/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19303610965297507		[learning rate: 0.003225]
		[batch 20/20] avg loss: 0.19557426317563614		[learning rate: 0.0032172]
	Learning Rate: 0.00321717
	LOSS [training: 0.1943051864143056 | validation: 0.1862157211645584]
	TIME [epoch: 8.49 sec]
EPOCH 285/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2253699700575075		[learning rate: 0.0032094]
		[batch 20/20] avg loss: 0.19374925569515117		[learning rate: 0.0032016]
	Learning Rate: 0.00320161
	LOSS [training: 0.20955961287632935 | validation: 0.1666573217648114]
	TIME [epoch: 8.43 sec]
EPOCH 286/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2182749424490483		[learning rate: 0.0031939]
		[batch 20/20] avg loss: 0.2272254524058769		[learning rate: 0.0031861]
	Learning Rate: 0.00318613
	LOSS [training: 0.2227501974274626 | validation: 0.10740115174263366]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_286.pth
	Model improved!!!
EPOCH 287/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18454399018235657		[learning rate: 0.0031784]
		[batch 20/20] avg loss: 0.306612849854352		[learning rate: 0.0031707]
	Learning Rate: 0.00317072
	LOSS [training: 0.2455784200183543 | validation: 0.14474000944781995]
	TIME [epoch: 8.46 sec]
EPOCH 288/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17453713672742988		[learning rate: 0.003163]
		[batch 20/20] avg loss: 0.19916946149699075		[learning rate: 0.0031554]
	Learning Rate: 0.00315539
	LOSS [training: 0.18685329911221032 | validation: 0.21033923730780107]
	TIME [epoch: 8.47 sec]
EPOCH 289/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19485136271057935		[learning rate: 0.0031477]
		[batch 20/20] avg loss: 0.21256486110945133		[learning rate: 0.0031401]
	Learning Rate: 0.00314013
	LOSS [training: 0.20370811191001534 | validation: 0.17488609947458278]
	TIME [epoch: 8.43 sec]
EPOCH 290/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2518067121463041		[learning rate: 0.0031325]
		[batch 20/20] avg loss: 0.27244634624411207		[learning rate: 0.0031249]
	Learning Rate: 0.00312494
	LOSS [training: 0.2621265291952081 | validation: 0.19583983465649293]
	TIME [epoch: 8.45 sec]
EPOCH 291/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20741499464972907		[learning rate: 0.0031174]
		[batch 20/20] avg loss: 0.21526215423746456		[learning rate: 0.0031098]
	Learning Rate: 0.00310983
	LOSS [training: 0.21133857444359685 | validation: 0.1848944351728941]
	TIME [epoch: 8.46 sec]
EPOCH 292/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2398499291822787		[learning rate: 0.0031023]
		[batch 20/20] avg loss: 0.22201894147483517		[learning rate: 0.0030948]
	Learning Rate: 0.00309479
	LOSS [training: 0.2309344353285569 | validation: 0.27819911671364633]
	TIME [epoch: 8.45 sec]
EPOCH 293/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2261187235759506		[learning rate: 0.0030873]
		[batch 20/20] avg loss: 0.2017680889260874		[learning rate: 0.0030798]
	Learning Rate: 0.00307983
	LOSS [training: 0.21394340625101904 | validation: 0.12296428980968778]
	TIME [epoch: 8.45 sec]
EPOCH 294/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2313849180004365		[learning rate: 0.0030724]
		[batch 20/20] avg loss: 0.2218323273018858		[learning rate: 0.0030649]
	Learning Rate: 0.00306493
	LOSS [training: 0.22660862265116116 | validation: 0.254918987741047]
	TIME [epoch: 8.47 sec]
EPOCH 295/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.31394326475601153		[learning rate: 0.0030575]
		[batch 20/20] avg loss: 0.240907236249794		[learning rate: 0.0030501]
	Learning Rate: 0.00305011
	LOSS [training: 0.27742525050290273 | validation: 0.1987942739097548]
	TIME [epoch: 8.43 sec]
EPOCH 296/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18755047386073992		[learning rate: 0.0030427]
		[batch 20/20] avg loss: 0.15483014709302095		[learning rate: 0.0030354]
	Learning Rate: 0.00303536
	LOSS [training: 0.1711903104768805 | validation: 0.16010542303924763]
	TIME [epoch: 8.43 sec]
EPOCH 297/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24207677429194785		[learning rate: 0.003028]
		[batch 20/20] avg loss: 0.20965069215064042		[learning rate: 0.0030207]
	Learning Rate: 0.00302068
	LOSS [training: 0.22586373322129413 | validation: 0.3099281750884074]
	TIME [epoch: 8.49 sec]
EPOCH 298/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17878221297338165		[learning rate: 0.0030134]
		[batch 20/20] avg loss: 0.17246997592160732		[learning rate: 0.0030061]
	Learning Rate: 0.00300608
	LOSS [training: 0.1756260944474945 | validation: 0.2341008715273243]
	TIME [epoch: 8.43 sec]
EPOCH 299/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19774704928736744		[learning rate: 0.0029988]
		[batch 20/20] avg loss: 0.1604798644247329		[learning rate: 0.0029915]
	Learning Rate: 0.00299154
	LOSS [training: 0.17911345685605012 | validation: 0.1424884786990499]
	TIME [epoch: 8.43 sec]
EPOCH 300/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20633135649319448		[learning rate: 0.0029843]
		[batch 20/20] avg loss: 0.21310950338180432		[learning rate: 0.0029771]
	Learning Rate: 0.00297707
	LOSS [training: 0.20972042993749937 | validation: 0.1768635799577568]
	TIME [epoch: 8.46 sec]
EPOCH 301/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1647639276642059		[learning rate: 0.0029699]
		[batch 20/20] avg loss: 0.22811747095605028		[learning rate: 0.0029627]
	Learning Rate: 0.00296268
	LOSS [training: 0.1964406993101281 | validation: 0.26197872638333974]
	TIME [epoch: 8.47 sec]
EPOCH 302/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23093675589788773		[learning rate: 0.0029555]
		[batch 20/20] avg loss: 0.17331095649672015		[learning rate: 0.0029483]
	Learning Rate: 0.00294835
	LOSS [training: 0.20212385619730394 | validation: 0.23643675153020774]
	TIME [epoch: 8.43 sec]
EPOCH 303/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2565034980201021		[learning rate: 0.0029412]
		[batch 20/20] avg loss: 0.2016201019016513		[learning rate: 0.0029341]
	Learning Rate: 0.00293409
	LOSS [training: 0.2290617999608767 | validation: 0.22798624513050297]
	TIME [epoch: 8.45 sec]
EPOCH 304/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17486157977831768		[learning rate: 0.002927]
		[batch 20/20] avg loss: 0.19200635412862263		[learning rate: 0.0029199]
	Learning Rate: 0.0029199
	LOSS [training: 0.18343396695347017 | validation: 0.22773676740288262]
	TIME [epoch: 8.45 sec]
EPOCH 305/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2994686362005184		[learning rate: 0.0029128]
		[batch 20/20] avg loss: 0.16672143779102738		[learning rate: 0.0029058]
	Learning Rate: 0.00290578
	LOSS [training: 0.2330950369957729 | validation: 0.2804707902458044]
	TIME [epoch: 8.45 sec]
EPOCH 306/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19810384538284811		[learning rate: 0.0028987]
		[batch 20/20] avg loss: 0.17362539645107808		[learning rate: 0.0028917]
	Learning Rate: 0.00289173
	LOSS [training: 0.18586462091696304 | validation: 0.1416114158076061]
	TIME [epoch: 8.45 sec]
EPOCH 307/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1706400082927973		[learning rate: 0.0028847]
		[batch 20/20] avg loss: 0.18613627647708048		[learning rate: 0.0028777]
	Learning Rate: 0.00287775
	LOSS [training: 0.1783881423849389 | validation: 0.23719839877919144]
	TIME [epoch: 8.47 sec]
EPOCH 308/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.26317998544122434		[learning rate: 0.0028708]
		[batch 20/20] avg loss: 0.20021639775711356		[learning rate: 0.0028638]
	Learning Rate: 0.00286383
	LOSS [training: 0.23169819159916893 | validation: 0.1864050561929096]
	TIME [epoch: 8.42 sec]
EPOCH 309/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20528481954649885		[learning rate: 0.0028569]
		[batch 20/20] avg loss: 0.30777072520141074		[learning rate: 0.00285]
	Learning Rate: 0.00284998
	LOSS [training: 0.25652777237395485 | validation: 0.2434511456383894]
	TIME [epoch: 8.43 sec]
EPOCH 310/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18349958767977542		[learning rate: 0.0028431]
		[batch 20/20] avg loss: 0.1973701048399323		[learning rate: 0.0028362]
	Learning Rate: 0.0028362
	LOSS [training: 0.19043484625985388 | validation: 0.11996852204422835]
	TIME [epoch: 8.49 sec]
EPOCH 311/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1655893742817102		[learning rate: 0.0028293]
		[batch 20/20] avg loss: 0.19597328706913789		[learning rate: 0.0028225]
	Learning Rate: 0.00282248
	LOSS [training: 0.18078133067542396 | validation: 0.2844497721343291]
	TIME [epoch: 8.44 sec]
EPOCH 312/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22287341375940337		[learning rate: 0.0028157]
		[batch 20/20] avg loss: 0.2538961332597114		[learning rate: 0.0028088]
	Learning Rate: 0.00280884
	LOSS [training: 0.23838477350955736 | validation: 0.23917634779074354]
	TIME [epoch: 8.43 sec]
EPOCH 313/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2523419955767029		[learning rate: 0.002802]
		[batch 20/20] avg loss: 0.15663510637992917		[learning rate: 0.0027953]
	Learning Rate: 0.00279525
	LOSS [training: 0.20448855097831603 | validation: 0.1430375921226086]
	TIME [epoch: 8.44 sec]
EPOCH 314/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18663609278687443		[learning rate: 0.0027885]
		[batch 20/20] avg loss: 0.16667557845432424		[learning rate: 0.0027817]
	Learning Rate: 0.00278174
	LOSS [training: 0.17665583562059933 | validation: 0.12669922011676976]
	TIME [epoch: 8.48 sec]
EPOCH 315/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1795527035315046		[learning rate: 0.002775]
		[batch 20/20] avg loss: 0.16821683773336177		[learning rate: 0.0027683]
	Learning Rate: 0.00276828
	LOSS [training: 0.17388477063243318 | validation: 0.18748464856502728]
	TIME [epoch: 8.43 sec]
EPOCH 316/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17441453532773857		[learning rate: 0.0027616]
		[batch 20/20] avg loss: 0.16594081424271706		[learning rate: 0.0027549]
	Learning Rate: 0.0027549
	LOSS [training: 0.17017767478522777 | validation: 0.15215537414975955]
	TIME [epoch: 8.43 sec]
EPOCH 317/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15339027012732448		[learning rate: 0.0027482]
		[batch 20/20] avg loss: 0.2142881346506266		[learning rate: 0.0027416]
	Learning Rate: 0.00274157
	LOSS [training: 0.1838392023889755 | validation: 0.20740783424545944]
	TIME [epoch: 8.46 sec]
EPOCH 318/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23022935499168753		[learning rate: 0.0027349]
		[batch 20/20] avg loss: 0.1832543764607058		[learning rate: 0.0027283]
	Learning Rate: 0.00272832
	LOSS [training: 0.20674186572619666 | validation: 0.10239252484240993]
	TIME [epoch: 8.45 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_318.pth
	Model improved!!!
EPOCH 319/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1635953388371441		[learning rate: 0.0027217]
		[batch 20/20] avg loss: 0.18921892490154704		[learning rate: 0.0027151]
	Learning Rate: 0.00271512
	LOSS [training: 0.1764071318693456 | validation: 0.15183985493224303]
	TIME [epoch: 8.44 sec]
EPOCH 320/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15725896539882878		[learning rate: 0.0027086]
		[batch 20/20] avg loss: 0.1801322512594239		[learning rate: 0.002702]
	Learning Rate: 0.00270199
	LOSS [training: 0.16869560832912633 | validation: 0.129378374184773]
	TIME [epoch: 8.46 sec]
EPOCH 321/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1623194061840881		[learning rate: 0.0026955]
		[batch 20/20] avg loss: 0.15828095743076043		[learning rate: 0.0026889]
	Learning Rate: 0.00268893
	LOSS [training: 0.16030018180742428 | validation: 0.20832708799156818]
	TIME [epoch: 8.43 sec]
EPOCH 322/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17946392078597975		[learning rate: 0.0026824]
		[batch 20/20] avg loss: 0.2849094681129833		[learning rate: 0.0026759]
	Learning Rate: 0.00267592
	LOSS [training: 0.23218669444948153 | validation: 0.29431221411155883]
	TIME [epoch: 8.45 sec]
EPOCH 323/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1903489069675996		[learning rate: 0.0026694]
		[batch 20/20] avg loss: 0.1991224334515102		[learning rate: 0.002663]
	Learning Rate: 0.00266298
	LOSS [training: 0.1947356702095549 | validation: 0.15271787775764248]
	TIME [epoch: 8.46 sec]
EPOCH 324/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1577010319399421		[learning rate: 0.0026565]
		[batch 20/20] avg loss: 0.16084975987349476		[learning rate: 0.0026501]
	Learning Rate: 0.00265011
	LOSS [training: 0.15927539590671844 | validation: 0.14998010014523716]
	TIME [epoch: 8.44 sec]
EPOCH 325/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15500789075646612		[learning rate: 0.0026437]
		[batch 20/20] avg loss: 0.22390819492951647		[learning rate: 0.0026373]
	Learning Rate: 0.00263729
	LOSS [training: 0.18945804284299134 | validation: 0.11486784659282741]
	TIME [epoch: 8.43 sec]
EPOCH 326/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15614414071337118		[learning rate: 0.0026309]
		[batch 20/20] avg loss: 0.1271879711430557		[learning rate: 0.0026245]
	Learning Rate: 0.00262454
	LOSS [training: 0.14166605592821344 | validation: 0.23030591566346906]
	TIME [epoch: 8.46 sec]
EPOCH 327/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20143169961652935		[learning rate: 0.0026182]
		[batch 20/20] avg loss: 0.16679086280550645		[learning rate: 0.0026118]
	Learning Rate: 0.00261184
	LOSS [training: 0.18411128121101786 | validation: 0.11822132925504793]
	TIME [epoch: 8.47 sec]
EPOCH 328/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20110514046540687		[learning rate: 0.0026055]
		[batch 20/20] avg loss: 0.16593706792932636		[learning rate: 0.0025992]
	Learning Rate: 0.00259921
	LOSS [training: 0.18352110419736659 | validation: 0.1862182498652574]
	TIME [epoch: 8.42 sec]
EPOCH 329/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16083313027026336		[learning rate: 0.0025929]
		[batch 20/20] avg loss: 0.21734561942339137		[learning rate: 0.0025866]
	Learning Rate: 0.00258664
	LOSS [training: 0.18908937484682736 | validation: 0.16102453356504265]
	TIME [epoch: 8.45 sec]
EPOCH 330/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19406817179301925		[learning rate: 0.0025804]
		[batch 20/20] avg loss: 0.15215830307030548		[learning rate: 0.0025741]
	Learning Rate: 0.00257414
	LOSS [training: 0.17311323743166238 | validation: 0.1524434800135883]
	TIME [epoch: 8.43 sec]
EPOCH 331/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1743619170353193		[learning rate: 0.0025679]
		[batch 20/20] avg loss: 0.1975539221091492		[learning rate: 0.0025617]
	Learning Rate: 0.00256169
	LOSS [training: 0.18595791957223426 | validation: 0.14538218180246887]
	TIME [epoch: 8.46 sec]
EPOCH 332/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20620540759978923		[learning rate: 0.0025555]
		[batch 20/20] avg loss: 0.20206309406089057		[learning rate: 0.0025493]
	Learning Rate: 0.0025493
	LOSS [training: 0.20413425083033993 | validation: 0.19008851406138813]
	TIME [epoch: 8.45 sec]
EPOCH 333/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17079558022609154		[learning rate: 0.0025431]
		[batch 20/20] avg loss: 0.15121119492877969		[learning rate: 0.002537]
	Learning Rate: 0.00253697
	LOSS [training: 0.16100338757743565 | validation: 0.15176112917266565]
	TIME [epoch: 8.42 sec]
EPOCH 334/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1780515706831512		[learning rate: 0.0025308]
		[batch 20/20] avg loss: 0.19215666963516176		[learning rate: 0.0025247]
	Learning Rate: 0.0025247
	LOSS [training: 0.18510412015915648 | validation: 0.10376134056451297]
	TIME [epoch: 8.43 sec]
EPOCH 335/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17662891181184276		[learning rate: 0.0025186]
		[batch 20/20] avg loss: 0.19158640606567595		[learning rate: 0.0025125]
	Learning Rate: 0.0025125
	LOSS [training: 0.18410765893875936 | validation: 0.1430094773348545]
	TIME [epoch: 8.47 sec]
EPOCH 336/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16287895709660233		[learning rate: 0.0025064]
		[batch 20/20] avg loss: 0.20906215745899628		[learning rate: 0.0025003]
	Learning Rate: 0.00250035
	LOSS [training: 0.18597055727779935 | validation: 0.1888344552639067]
	TIME [epoch: 8.44 sec]
EPOCH 337/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19014158721125907		[learning rate: 0.0024943]
		[batch 20/20] avg loss: 0.14643750239880837		[learning rate: 0.0024883]
	Learning Rate: 0.00248825
	LOSS [training: 0.16828954480503372 | validation: 0.1003645294103822]
	TIME [epoch: 8.43 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_337.pth
	Model improved!!!
EPOCH 338/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21807591370870932		[learning rate: 0.0024822]
		[batch 20/20] avg loss: 0.21818816384080703		[learning rate: 0.0024762]
	Learning Rate: 0.00247622
	LOSS [training: 0.2181320387747582 | validation: 0.2460522376063623]
	TIME [epoch: 8.44 sec]
EPOCH 339/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20929992571952324		[learning rate: 0.0024702]
		[batch 20/20] avg loss: 0.22425638445157747		[learning rate: 0.0024642]
	Learning Rate: 0.00246425
	LOSS [training: 0.21677815508555037 | validation: 0.2204163642221245]
	TIME [epoch: 8.41 sec]
EPOCH 340/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.21485868649652945		[learning rate: 0.0024583]
		[batch 20/20] avg loss: 0.14610214859946025		[learning rate: 0.0024523]
	Learning Rate: 0.00245233
	LOSS [training: 0.18048041754799482 | validation: 0.16086861718394593]
	TIME [epoch: 8.45 sec]
EPOCH 341/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1991268762831771		[learning rate: 0.0024464]
		[batch 20/20] avg loss: 0.16575017166472467		[learning rate: 0.0024405]
	Learning Rate: 0.00244047
	LOSS [training: 0.1824385239739509 | validation: 0.17398596987687412]
	TIME [epoch: 8.43 sec]
EPOCH 342/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17746981749824237		[learning rate: 0.0024346]
		[batch 20/20] avg loss: 0.12469547287614442		[learning rate: 0.0024287]
	Learning Rate: 0.00242867
	LOSS [training: 0.15108264518719342 | validation: 0.1916599967879276]
	TIME [epoch: 8.42 sec]
EPOCH 343/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1824618211917018		[learning rate: 0.0024228]
		[batch 20/20] avg loss: 0.21859104272876467		[learning rate: 0.0024169]
	Learning Rate: 0.00241693
	LOSS [training: 0.20052643196023326 | validation: 0.14410832621170322]
	TIME [epoch: 8.43 sec]
EPOCH 344/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16596349020578088		[learning rate: 0.0024111]
		[batch 20/20] avg loss: 0.16320115556818435		[learning rate: 0.0024052]
	Learning Rate: 0.00240524
	LOSS [training: 0.16458232288698263 | validation: 0.1462224514268115]
	TIME [epoch: 8.46 sec]
EPOCH 345/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1825932227625909		[learning rate: 0.0023994]
		[batch 20/20] avg loss: 0.24056301547011097		[learning rate: 0.0023936]
	Learning Rate: 0.00239361
	LOSS [training: 0.211578119116351 | validation: 0.14514331860902013]
	TIME [epoch: 8.42 sec]
EPOCH 346/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12860062517384546		[learning rate: 0.0023878]
		[batch 20/20] avg loss: 0.1423575640370563		[learning rate: 0.002382]
	Learning Rate: 0.00238203
	LOSS [training: 0.13547909460545088 | validation: 0.1224766367064548]
	TIME [epoch: 8.43 sec]
EPOCH 347/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1713717933129306		[learning rate: 0.0023763]
		[batch 20/20] avg loss: 0.20565489878848292		[learning rate: 0.0023705]
	Learning Rate: 0.00237051
	LOSS [training: 0.18851334605070677 | validation: 0.09470537661741879]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_347.pth
	Model improved!!!
EPOCH 348/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2033600287844674		[learning rate: 0.0023648]
		[batch 20/20] avg loss: 0.1441088552971557		[learning rate: 0.002359]
	Learning Rate: 0.00235905
	LOSS [training: 0.1737344420408115 | validation: 0.14311417715622113]
	TIME [epoch: 8.44 sec]
EPOCH 349/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16954114056655936		[learning rate: 0.0023533]
		[batch 20/20] avg loss: 0.17173725984647023		[learning rate: 0.0023476]
	Learning Rate: 0.00234764
	LOSS [training: 0.17063920020651482 | validation: 0.11721196720587398]
	TIME [epoch: 8.44 sec]
EPOCH 350/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1715660264096424		[learning rate: 0.002342]
		[batch 20/20] avg loss: 0.17307356873789181		[learning rate: 0.0023363]
	Learning Rate: 0.00233629
	LOSS [training: 0.17231979757376711 | validation: 0.1286409937435263]
	TIME [epoch: 8.43 sec]
EPOCH 351/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17034789705393116		[learning rate: 0.0023306]
		[batch 20/20] avg loss: 0.1553879516816195		[learning rate: 0.002325]
	Learning Rate: 0.00232499
	LOSS [training: 0.16286792436777534 | validation: 0.16632344012032443]
	TIME [epoch: 8.41 sec]
EPOCH 352/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19524919449353945		[learning rate: 0.0023194]
		[batch 20/20] avg loss: 0.1607700544748732		[learning rate: 0.0023137]
	Learning Rate: 0.00231375
	LOSS [training: 0.1780096244842063 | validation: 0.11275016484755912]
	TIME [epoch: 8.43 sec]
EPOCH 353/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14385384645920335		[learning rate: 0.0023081]
		[batch 20/20] avg loss: 0.1568893867178807		[learning rate: 0.0023026]
	Learning Rate: 0.00230256
	LOSS [training: 0.150371616588542 | validation: 0.10703746883041164]
	TIME [epoch: 8.46 sec]
EPOCH 354/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1365404296727523		[learning rate: 0.002297]
		[batch 20/20] avg loss: 0.12360283963232892		[learning rate: 0.0022914]
	Learning Rate: 0.00229142
	LOSS [training: 0.13007163465254062 | validation: 0.14495179577750583]
	TIME [epoch: 8.42 sec]
EPOCH 355/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1518478759522212		[learning rate: 0.0022859]
		[batch 20/20] avg loss: 0.1411438227908631		[learning rate: 0.0022803]
	Learning Rate: 0.00228034
	LOSS [training: 0.14649584937154214 | validation: 0.2101293469907316]
	TIME [epoch: 8.41 sec]
EPOCH 356/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18841741685483082		[learning rate: 0.0022748]
		[batch 20/20] avg loss: 0.2183988922506141		[learning rate: 0.0022693]
	Learning Rate: 0.00226931
	LOSS [training: 0.2034081545527225 | validation: 0.15277466054615965]
	TIME [epoch: 8.43 sec]
EPOCH 357/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17563317062708494		[learning rate: 0.0022638]
		[batch 20/20] avg loss: 0.15255685949348788		[learning rate: 0.0022583]
	Learning Rate: 0.00225834
	LOSS [training: 0.1640950150602864 | validation: 0.14942533363601443]
	TIME [epoch: 8.46 sec]
EPOCH 358/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15155497570299564		[learning rate: 0.0022529]
		[batch 20/20] avg loss: 0.2183293884810858		[learning rate: 0.0022474]
	Learning Rate: 0.00224742
	LOSS [training: 0.18494218209204072 | validation: 0.2240387713296944]
	TIME [epoch: 8.42 sec]
EPOCH 359/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1723968112684791		[learning rate: 0.002242]
		[batch 20/20] avg loss: 0.14144294857230594		[learning rate: 0.0022366]
	Learning Rate: 0.00223655
	LOSS [training: 0.15691987992039252 | validation: 0.13937370683239486]
	TIME [epoch: 8.41 sec]
EPOCH 360/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1814311341182284		[learning rate: 0.0022311]
		[batch 20/20] avg loss: 0.24253009056771804		[learning rate: 0.0022257]
	Learning Rate: 0.00222574
	LOSS [training: 0.2119806123429732 | validation: 0.1055099499875755]
	TIME [epoch: 8.43 sec]
EPOCH 361/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16433661713142048		[learning rate: 0.0022203]
		[batch 20/20] avg loss: 0.18735899851894525		[learning rate: 0.002215]
	Learning Rate: 0.00221497
	LOSS [training: 0.17584780782518286 | validation: 0.10838501600340199]
	TIME [epoch: 8.45 sec]
EPOCH 362/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17068422663088761		[learning rate: 0.0022096]
		[batch 20/20] avg loss: 0.17935824573858217		[learning rate: 0.0022043]
	Learning Rate: 0.00220426
	LOSS [training: 0.1750212361847349 | validation: 0.14601660191542415]
	TIME [epoch: 8.42 sec]
EPOCH 363/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1319494979618777		[learning rate: 0.0021989]
		[batch 20/20] avg loss: 0.17160332852944943		[learning rate: 0.0021936]
	Learning Rate: 0.0021936
	LOSS [training: 0.1517764132456636 | validation: 0.133795562262278]
	TIME [epoch: 8.42 sec]
EPOCH 364/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1533108575195578		[learning rate: 0.0021883]
		[batch 20/20] avg loss: 0.15961527423452557		[learning rate: 0.002183]
	Learning Rate: 0.00218299
	LOSS [training: 0.15646306587704173 | validation: 0.11967211339686716]
	TIME [epoch: 8.44 sec]
EPOCH 365/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14798322710178713		[learning rate: 0.0021777]
		[batch 20/20] avg loss: 0.1456984794476615		[learning rate: 0.0021724]
	Learning Rate: 0.00217244
	LOSS [training: 0.14684085327472432 | validation: 0.10079336358198894]
	TIME [epoch: 8.43 sec]
EPOCH 366/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14016000319322047		[learning rate: 0.0021672]
		[batch 20/20] avg loss: 0.2038143116708658		[learning rate: 0.0021619]
	Learning Rate: 0.00216193
	LOSS [training: 0.17198715743204313 | validation: 0.27971682211873017]
	TIME [epoch: 8.42 sec]
EPOCH 367/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1834447569139989		[learning rate: 0.0021567]
		[batch 20/20] avg loss: 0.1628880940248638		[learning rate: 0.0021515]
	Learning Rate: 0.00215148
	LOSS [training: 0.17316642546943137 | validation: 0.15731990777886384]
	TIME [epoch: 8.43 sec]
EPOCH 368/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13734302423323227		[learning rate: 0.0021463]
		[batch 20/20] avg loss: 0.1525436584130415		[learning rate: 0.0021411]
	Learning Rate: 0.00214107
	LOSS [training: 0.14494334132313688 | validation: 0.10734985768561996]
	TIME [epoch: 8.43 sec]
EPOCH 369/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13779240642758717		[learning rate: 0.0021359]
		[batch 20/20] avg loss: 0.13998874548349652		[learning rate: 0.0021307]
	Learning Rate: 0.00213072
	LOSS [training: 0.13889057595554183 | validation: 0.12744578026535425]
	TIME [epoch: 8.42 sec]
EPOCH 370/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14752259333427836		[learning rate: 0.0021256]
		[batch 20/20] avg loss: 0.14970595619389562		[learning rate: 0.0021204]
	Learning Rate: 0.00212042
	LOSS [training: 0.148614274764087 | validation: 0.2396303155522193]
	TIME [epoch: 8.45 sec]
EPOCH 371/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.22886911451719252		[learning rate: 0.0021153]
		[batch 20/20] avg loss: 0.16151881268836496		[learning rate: 0.0021102]
	Learning Rate: 0.00211016
	LOSS [training: 0.1951939636027787 | validation: 0.12277234979940485]
	TIME [epoch: 8.44 sec]
EPOCH 372/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1589120356257191		[learning rate: 0.0021051]
		[batch 20/20] avg loss: 0.1621216315596103		[learning rate: 0.0021]
	Learning Rate: 0.00209996
	LOSS [training: 0.16051683359266466 | validation: 0.08520579195772815]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_372.pth
	Model improved!!!
EPOCH 373/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17311623538544		[learning rate: 0.0020949]
		[batch 20/20] avg loss: 0.1673751113814022		[learning rate: 0.0020898]
	Learning Rate: 0.0020898
	LOSS [training: 0.17024567338342111 | validation: 0.25105375387450274]
	TIME [epoch: 8.42 sec]
EPOCH 374/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1762037954169342		[learning rate: 0.0020847]
		[batch 20/20] avg loss: 0.19730473985316152		[learning rate: 0.0020797]
	Learning Rate: 0.0020797
	LOSS [training: 0.18675426763504782 | validation: 0.1499696783755447]
	TIME [epoch: 8.46 sec]
EPOCH 375/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15367482926173975		[learning rate: 0.0020747]
		[batch 20/20] avg loss: 0.16189299079278352		[learning rate: 0.0020696]
	Learning Rate: 0.00206964
	LOSS [training: 0.1577839100272616 | validation: 0.11670239778141098]
	TIME [epoch: 8.42 sec]
EPOCH 376/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16200695077071145		[learning rate: 0.0020646]
		[batch 20/20] avg loss: 0.1866947644040538		[learning rate: 0.0020596]
	Learning Rate: 0.00205963
	LOSS [training: 0.17435085758738264 | validation: 0.14266997753957053]
	TIME [epoch: 8.42 sec]
EPOCH 377/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13211783868319815		[learning rate: 0.0020546]
		[batch 20/20] avg loss: 0.16828458058378562		[learning rate: 0.0020497]
	Learning Rate: 0.00204967
	LOSS [training: 0.1502012096334919 | validation: 0.15707358429344087]
	TIME [epoch: 8.44 sec]
EPOCH 378/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15557917146944752		[learning rate: 0.0020447]
		[batch 20/20] avg loss: 0.14630848279979833		[learning rate: 0.0020398]
	Learning Rate: 0.00203976
	LOSS [training: 0.1509438271346229 | validation: 0.09018054998913999]
	TIME [epoch: 8.45 sec]
EPOCH 379/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1518884229579125		[learning rate: 0.0020348]
		[batch 20/20] avg loss: 0.16975719427348723		[learning rate: 0.0020299]
	Learning Rate: 0.0020299
	LOSS [training: 0.16082280861569984 | validation: 0.13103412727923242]
	TIME [epoch: 8.43 sec]
EPOCH 380/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20331164131906285		[learning rate: 0.002025]
		[batch 20/20] avg loss: 0.19476865935092888		[learning rate: 0.0020201]
	Learning Rate: 0.00202008
	LOSS [training: 0.19904015033499584 | validation: 0.1076437728656072]
	TIME [epoch: 8.42 sec]
EPOCH 381/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1562143281322832		[learning rate: 0.0020152]
		[batch 20/20] avg loss: 0.164621457657908		[learning rate: 0.0020103]
	Learning Rate: 0.00201031
	LOSS [training: 0.16041789289509562 | validation: 0.10074846529131726]
	TIME [epoch: 8.44 sec]
EPOCH 382/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1640302602071307		[learning rate: 0.0020054]
		[batch 20/20] avg loss: 0.19697292534653674		[learning rate: 0.0020006]
	Learning Rate: 0.00200059
	LOSS [training: 0.1805015927768337 | validation: 0.09244207352460161]
	TIME [epoch: 8.42 sec]
EPOCH 383/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14321179569848863		[learning rate: 0.0019957]
		[batch 20/20] avg loss: 0.1809304156055967		[learning rate: 0.0019909]
	Learning Rate: 0.00199091
	LOSS [training: 0.1620711056520427 | validation: 0.1699063845664402]
	TIME [epoch: 8.44 sec]
EPOCH 384/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17971881141031462		[learning rate: 0.0019861]
		[batch 20/20] avg loss: 0.13703251311710396		[learning rate: 0.0019813]
	Learning Rate: 0.00198129
	LOSS [training: 0.1583756622637093 | validation: 0.19951255001054485]
	TIME [epoch: 8.45 sec]
EPOCH 385/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2129828166117563		[learning rate: 0.0019765]
		[batch 20/20] avg loss: 0.13374739741760597		[learning rate: 0.0019717]
	Learning Rate: 0.00197171
	LOSS [training: 0.17336510701468116 | validation: 0.12445749503767532]
	TIME [epoch: 8.42 sec]
EPOCH 386/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17152098726591947		[learning rate: 0.0019669]
		[batch 20/20] avg loss: 0.14667574059518151		[learning rate: 0.0019622]
	Learning Rate: 0.00196217
	LOSS [training: 0.1590983639305505 | validation: 0.08685211498027562]
	TIME [epoch: 8.41 sec]
EPOCH 387/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.17433032497216522		[learning rate: 0.0019574]
		[batch 20/20] avg loss: 0.1301245379161965		[learning rate: 0.0019527]
	Learning Rate: 0.00195268
	LOSS [training: 0.15222743144418088 | validation: 0.09096996860686403]
	TIME [epoch: 8.46 sec]
EPOCH 388/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1422672417111905		[learning rate: 0.001948]
		[batch 20/20] avg loss: 0.16399760924497658		[learning rate: 0.0019432]
	Learning Rate: 0.00194324
	LOSS [training: 0.15313242547808356 | validation: 0.12968431090926427]
	TIME [epoch: 8.43 sec]
EPOCH 389/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1558544262520873		[learning rate: 0.0019385]
		[batch 20/20] avg loss: 0.16423785656067819		[learning rate: 0.0019338]
	Learning Rate: 0.00193384
	LOSS [training: 0.1600461414063828 | validation: 0.09187169508355529]
	TIME [epoch: 8.41 sec]
EPOCH 390/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12657146504182354		[learning rate: 0.0019292]
		[batch 20/20] avg loss: 0.14182834745312786		[learning rate: 0.0019245]
	Learning Rate: 0.00192449
	LOSS [training: 0.1341999062474757 | validation: 0.16738888837748428]
	TIME [epoch: 8.44 sec]
EPOCH 391/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14881678241593943		[learning rate: 0.0019198]
		[batch 20/20] avg loss: 0.12408483224999825		[learning rate: 0.0019152]
	Learning Rate: 0.00191518
	LOSS [training: 0.13645080733296883 | validation: 0.1558054121266356]
	TIME [epoch: 8.46 sec]
EPOCH 392/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12430972786474297		[learning rate: 0.0019105]
		[batch 20/20] avg loss: 0.1395177526974401		[learning rate: 0.0019059]
	Learning Rate: 0.00190592
	LOSS [training: 0.13191374028109154 | validation: 0.11867164134563632]
	TIME [epoch: 8.42 sec]
EPOCH 393/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15493621386467302		[learning rate: 0.0019013]
		[batch 20/20] avg loss: 0.14254759169203648		[learning rate: 0.0018967]
	Learning Rate: 0.00189671
	LOSS [training: 0.14874190277835475 | validation: 0.20672780582722228]
	TIME [epoch: 8.43 sec]
EPOCH 394/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.24193222386873345		[learning rate: 0.0018921]
		[batch 20/20] avg loss: 0.14040991245803253		[learning rate: 0.0018875]
	Learning Rate: 0.00188753
	LOSS [training: 0.19117106816338297 | validation: 0.14127677272330094]
	TIME [epoch: 8.43 sec]
EPOCH 395/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16440172845786577		[learning rate: 0.001883]
		[batch 20/20] avg loss: 0.16114729185318585		[learning rate: 0.0018784]
	Learning Rate: 0.00187841
	LOSS [training: 0.1627745101555258 | validation: 0.15786696579310225]
	TIME [epoch: 8.43 sec]
EPOCH 396/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14890295612037846		[learning rate: 0.0018739]
		[batch 20/20] avg loss: 0.1346626497437186		[learning rate: 0.0018693]
	Learning Rate: 0.00186932
	LOSS [training: 0.14178280293204856 | validation: 0.11289660849434183]
	TIME [epoch: 8.43 sec]
EPOCH 397/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14811998442034766		[learning rate: 0.0018648]
		[batch 20/20] avg loss: 0.10915373384063849		[learning rate: 0.0018603]
	Learning Rate: 0.00186028
	LOSS [training: 0.1286368591304931 | validation: 0.10132057740468338]
	TIME [epoch: 8.45 sec]
EPOCH 398/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12219000704912326		[learning rate: 0.0018558]
		[batch 20/20] avg loss: 0.16790011069879934		[learning rate: 0.0018513]
	Learning Rate: 0.00185129
	LOSS [training: 0.14504505887396135 | validation: 0.09674923274446151]
	TIME [epoch: 8.41 sec]
EPOCH 399/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15850973513778038		[learning rate: 0.0018468]
		[batch 20/20] avg loss: 0.13637810142245005		[learning rate: 0.0018423]
	Learning Rate: 0.00184233
	LOSS [training: 0.14744391828011522 | validation: 0.08890029994450827]
	TIME [epoch: 8.42 sec]
EPOCH 400/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12693726777977166		[learning rate: 0.0018379]
		[batch 20/20] avg loss: 0.14545584555395857		[learning rate: 0.0018334]
	Learning Rate: 0.00183343
	LOSS [training: 0.1361965566668651 | validation: 0.10534705987283693]
	TIME [epoch: 8.46 sec]
EPOCH 401/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1054742443281352		[learning rate: 0.001829]
		[batch 20/20] avg loss: 0.14271838519455038		[learning rate: 0.0018246]
	Learning Rate: 0.00182456
	LOSS [training: 0.1240963147613428 | validation: 0.1250678720790464]
	TIME [epoch: 8.41 sec]
EPOCH 402/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12865625784685036		[learning rate: 0.0018201]
		[batch 20/20] avg loss: 0.1581999706107576		[learning rate: 0.0018157]
	Learning Rate: 0.00181574
	LOSS [training: 0.14342811422880397 | validation: 0.13429507378072222]
	TIME [epoch: 8.41 sec]
EPOCH 403/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.19112839377368585		[learning rate: 0.0018113]
		[batch 20/20] avg loss: 0.10865768415858149		[learning rate: 0.001807]
	Learning Rate: 0.00180696
	LOSS [training: 0.14989303896613368 | validation: 0.11135092614490726]
	TIME [epoch: 8.44 sec]
EPOCH 404/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15976226694894796		[learning rate: 0.0018026]
		[batch 20/20] avg loss: 0.14538453017391734		[learning rate: 0.0017982]
	Learning Rate: 0.00179822
	LOSS [training: 0.15257339856143265 | validation: 0.15186986117440818]
	TIME [epoch: 8.43 sec]
EPOCH 405/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12428947758090078		[learning rate: 0.0017939]
		[batch 20/20] avg loss: 0.14492448818034828		[learning rate: 0.0017895]
	Learning Rate: 0.00178952
	LOSS [training: 0.13460698288062453 | validation: 0.0961025944018493]
	TIME [epoch: 8.41 sec]
EPOCH 406/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14786622195050866		[learning rate: 0.0017852]
		[batch 20/20] avg loss: 0.16131882708915715		[learning rate: 0.0017809]
	Learning Rate: 0.00178087
	LOSS [training: 0.1545925245198329 | validation: 0.08775331965642147]
	TIME [epoch: 8.43 sec]
EPOCH 407/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13095692151205543		[learning rate: 0.0017766]
		[batch 20/20] avg loss: 0.1285795598718188		[learning rate: 0.0017723]
	Learning Rate: 0.00177226
	LOSS [training: 0.1297682406919371 | validation: 0.22076422096234993]
	TIME [epoch: 8.42 sec]
EPOCH 408/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1573143598671222		[learning rate: 0.001768]
		[batch 20/20] avg loss: 0.1638637031926501		[learning rate: 0.0017637]
	Learning Rate: 0.00176369
	LOSS [training: 0.16058903152988616 | validation: 0.25933989342933145]
	TIME [epoch: 8.43 sec]
EPOCH 409/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14857132089195313		[learning rate: 0.0017594]
		[batch 20/20] avg loss: 0.12415046134173428		[learning rate: 0.0017552]
	Learning Rate: 0.00175516
	LOSS [training: 0.13636089111684369 | validation: 0.07352185846349786]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_409.pth
	Model improved!!!
EPOCH 410/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13043982370475551		[learning rate: 0.0017509]
		[batch 20/20] avg loss: 0.1699165618955538		[learning rate: 0.0017467]
	Learning Rate: 0.00174667
	LOSS [training: 0.15017819280015465 | validation: 0.1270510370725229]
	TIME [epoch: 8.43 sec]
EPOCH 411/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1465460296621117		[learning rate: 0.0017424]
		[batch 20/20] avg loss: 0.14270545405107754		[learning rate: 0.0017382]
	Learning Rate: 0.00173822
	LOSS [training: 0.14462574185659466 | validation: 0.19537848493172985]
	TIME [epoch: 8.41 sec]
EPOCH 412/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.169978420727135		[learning rate: 0.001734]
		[batch 20/20] avg loss: 0.14521414262524374		[learning rate: 0.0017298]
	Learning Rate: 0.00172982
	LOSS [training: 0.1575962816761894 | validation: 0.13158984501555543]
	TIME [epoch: 8.4 sec]
EPOCH 413/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11894423376551073		[learning rate: 0.0017256]
		[batch 20/20] avg loss: 0.14867877937148244		[learning rate: 0.0017215]
	Learning Rate: 0.00172145
	LOSS [training: 0.1338115065684966 | validation: 0.1089509777347449]
	TIME [epoch: 8.44 sec]
EPOCH 414/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11560622008064643		[learning rate: 0.0017173]
		[batch 20/20] avg loss: 0.14956086647381756		[learning rate: 0.0017131]
	Learning Rate: 0.00171313
	LOSS [training: 0.13258354327723196 | validation: 0.08838673541021595]
	TIME [epoch: 8.43 sec]
EPOCH 415/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13020679531056029		[learning rate: 0.001709]
		[batch 20/20] avg loss: 0.10805977428510465		[learning rate: 0.0017048]
	Learning Rate: 0.00170484
	LOSS [training: 0.11913328479783247 | validation: 0.08225707806350102]
	TIME [epoch: 8.4 sec]
EPOCH 416/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13744861738944011		[learning rate: 0.0017007]
		[batch 20/20] avg loss: 0.11960390000646179		[learning rate: 0.0016966]
	Learning Rate: 0.0016966
	LOSS [training: 0.12852625869795095 | validation: 0.18916247152812116]
	TIME [epoch: 8.39 sec]
EPOCH 417/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3306474477058654		[learning rate: 0.0016925]
		[batch 20/20] avg loss: 2.030244279927755		[learning rate: 0.0016884]
	Learning Rate: 0.00168839
	LOSS [training: 1.6804458638168103 | validation: 1.9389131211180026]
	TIME [epoch: 8.44 sec]
EPOCH 418/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3319773263194006		[learning rate: 0.0016843]
		[batch 20/20] avg loss: 1.1760898718837498		[learning rate: 0.0016802]
	Learning Rate: 0.00168023
	LOSS [training: 1.2540335991015752 | validation: 1.5481880310708318]
	TIME [epoch: 8.42 sec]
EPOCH 419/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1364278705115043		[learning rate: 0.0016762]
		[batch 20/20] avg loss: 1.1133575829277593		[learning rate: 0.0016721]
	Learning Rate: 0.0016721
	LOSS [training: 1.1248927267196318 | validation: 1.4871811184996884]
	TIME [epoch: 8.4 sec]
EPOCH 420/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8563941198619626		[learning rate: 0.0016681]
		[batch 20/20] avg loss: 0.2148279860264422		[learning rate: 0.001664]
	Learning Rate: 0.00166402
	LOSS [training: 0.5356110529442024 | validation: 0.220255569865751]
	TIME [epoch: 8.4 sec]
EPOCH 421/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.18815643149137976		[learning rate: 0.00166]
		[batch 20/20] avg loss: 0.17400836237233477		[learning rate: 0.001656]
	Learning Rate: 0.00165597
	LOSS [training: 0.1810823969318573 | validation: 0.10273308330632795]
	TIME [epoch: 8.44 sec]
EPOCH 422/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12093258356039154		[learning rate: 0.001652]
		[batch 20/20] avg loss: 0.15344495562768454		[learning rate: 0.001648]
	Learning Rate: 0.00164796
	LOSS [training: 0.13718876959403803 | validation: 0.10846874176381577]
	TIME [epoch: 8.42 sec]
EPOCH 423/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14570667988863878		[learning rate: 0.001644]
		[batch 20/20] avg loss: 0.1507906302022612		[learning rate: 0.00164]
	Learning Rate: 0.00163999
	LOSS [training: 0.14824865504545 | validation: 0.1242377698764825]
	TIME [epoch: 8.39 sec]
EPOCH 424/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12354618399771097		[learning rate: 0.001636]
		[batch 20/20] avg loss: 0.14472891472945723		[learning rate: 0.0016321]
	Learning Rate: 0.00163206
	LOSS [training: 0.1341375493635841 | validation: 0.1159149106712776]
	TIME [epoch: 8.4 sec]
EPOCH 425/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13852544582957665		[learning rate: 0.0016281]
		[batch 20/20] avg loss: 0.12485590866803409		[learning rate: 0.0016242]
	Learning Rate: 0.00162417
	LOSS [training: 0.1316906772488054 | validation: 0.24809573653683253]
	TIME [epoch: 8.43 sec]
EPOCH 426/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1384524456004283		[learning rate: 0.0016202]
		[batch 20/20] avg loss: 0.1023269335480566		[learning rate: 0.0016163]
	Learning Rate: 0.00161632
	LOSS [training: 0.12038968957424241 | validation: 0.06337461836664496]
	TIME [epoch: 8.42 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_426.pth
	Model improved!!!
EPOCH 427/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12225978197141499		[learning rate: 0.0016124]
		[batch 20/20] avg loss: 0.11590691635378987		[learning rate: 0.0016085]
	Learning Rate: 0.0016085
	LOSS [training: 0.11908334916260244 | validation: 0.10864727353628612]
	TIME [epoch: 8.4 sec]
EPOCH 428/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11841495375328522		[learning rate: 0.0016046]
		[batch 20/20] avg loss: 0.1311605558203521		[learning rate: 0.0016007]
	Learning Rate: 0.00160072
	LOSS [training: 0.12478775478681867 | validation: 0.1503998952723523]
	TIME [epoch: 8.42 sec]
EPOCH 429/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15034694150236733		[learning rate: 0.0015968]
		[batch 20/20] avg loss: 0.12522742593546066		[learning rate: 0.001593]
	Learning Rate: 0.00159298
	LOSS [training: 0.13778718371891402 | validation: 0.12552398235400747]
	TIME [epoch: 8.41 sec]
EPOCH 430/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11272353951360925		[learning rate: 0.0015891]
		[batch 20/20] avg loss: 0.1420767882951322		[learning rate: 0.0015853]
	Learning Rate: 0.00158528
	LOSS [training: 0.12740016390437073 | validation: 0.13853194940529592]
	TIME [epoch: 8.42 sec]
EPOCH 431/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11791153015422987		[learning rate: 0.0015814]
		[batch 20/20] avg loss: 0.12195110817631816		[learning rate: 0.0015776]
	Learning Rate: 0.00157761
	LOSS [training: 0.119931319165274 | validation: 0.15879508352622024]
	TIME [epoch: 8.4 sec]
EPOCH 432/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15262065241052394		[learning rate: 0.0015738]
		[batch 20/20] avg loss: 0.11756961289302026		[learning rate: 0.00157]
	Learning Rate: 0.00156998
	LOSS [training: 0.13509513265177206 | validation: 0.12486394281973476]
	TIME [epoch: 8.42 sec]
EPOCH 433/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11384112189110125		[learning rate: 0.0015662]
		[batch 20/20] avg loss: 0.11249211367732767		[learning rate: 0.0015624]
	Learning Rate: 0.00156239
	LOSS [training: 0.11316661778421447 | validation: 0.11342699069868806]
	TIME [epoch: 8.39 sec]
EPOCH 434/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11266969178448119		[learning rate: 0.0015586]
		[batch 20/20] avg loss: 0.13423434270304851		[learning rate: 0.0015548]
	Learning Rate: 0.00155483
	LOSS [training: 0.12345201724376484 | validation: 0.07714711553093595]
	TIME [epoch: 8.42 sec]
EPOCH 435/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11589250468933891		[learning rate: 0.0015511]
		[batch 20/20] avg loss: 0.11977905777571443		[learning rate: 0.0015473]
	Learning Rate: 0.00154732
	LOSS [training: 0.11783578123252669 | validation: 0.11864032340755633]
	TIME [epoch: 8.41 sec]
EPOCH 436/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16108089696400377		[learning rate: 0.0015436]
		[batch 20/20] avg loss: 0.1117740346543088		[learning rate: 0.0015398]
	Learning Rate: 0.00153983
	LOSS [training: 0.13642746580915627 | validation: 0.1168437387311931]
	TIME [epoch: 8.41 sec]
EPOCH 437/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11661727606847547		[learning rate: 0.0015361]
		[batch 20/20] avg loss: 0.1400559300856848		[learning rate: 0.0015324]
	Learning Rate: 0.00153239
	LOSS [training: 0.1283366030770801 | validation: 0.09186813327189808]
	TIME [epoch: 8.39 sec]
EPOCH 438/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1165119236003366		[learning rate: 0.0015287]
		[batch 20/20] avg loss: 0.11659981941430561		[learning rate: 0.001525]
	Learning Rate: 0.00152498
	LOSS [training: 0.11655587150732108 | validation: 0.15047609160180747]
	TIME [epoch: 8.41 sec]
EPOCH 439/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12752044412443514		[learning rate: 0.0015213]
		[batch 20/20] avg loss: 0.14292246855968643		[learning rate: 0.0015176]
	Learning Rate: 0.0015176
	LOSS [training: 0.1352214563420608 | validation: 0.09040245160176963]
	TIME [epoch: 8.43 sec]
EPOCH 440/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1205749835545987		[learning rate: 0.0015139]
		[batch 20/20] avg loss: 0.14958994054760097		[learning rate: 0.0015103]
	Learning Rate: 0.00151026
	LOSS [training: 0.13508246205109983 | validation: 0.14361426786054066]
	TIME [epoch: 8.41 sec]
EPOCH 441/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1389479505446612		[learning rate: 0.0015066]
		[batch 20/20] avg loss: 0.19206616882797706		[learning rate: 0.001503]
	Learning Rate: 0.00150296
	LOSS [training: 0.16550705968631912 | validation: 0.17819444038342036]
	TIME [epoch: 8.38 sec]
EPOCH 442/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14354686329509586		[learning rate: 0.0014993]
		[batch 20/20] avg loss: 0.13066461789152484		[learning rate: 0.0014957]
	Learning Rate: 0.00149569
	LOSS [training: 0.13710574059331035 | validation: 0.10375879352699871]
	TIME [epoch: 8.4 sec]
EPOCH 443/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12022512586061367		[learning rate: 0.0014921]
		[batch 20/20] avg loss: 0.12536004155053918		[learning rate: 0.0014885]
	Learning Rate: 0.00148846
	LOSS [training: 0.12279258370557641 | validation: 0.1369100726764554]
	TIME [epoch: 8.45 sec]
EPOCH 444/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13308203126004672		[learning rate: 0.0014849]
		[batch 20/20] avg loss: 0.12023240685090067		[learning rate: 0.0014813]
	Learning Rate: 0.00148126
	LOSS [training: 0.1266572190554737 | validation: 0.09033736749147105]
	TIME [epoch: 8.4 sec]
EPOCH 445/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397752565472144		[learning rate: 0.0014777]
		[batch 20/20] avg loss: 0.15094985273871414		[learning rate: 0.0014741]
	Learning Rate: 0.0014741
	LOSS [training: 0.14536255464296427 | validation: 0.0987518229698944]
	TIME [epoch: 8.38 sec]
EPOCH 446/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12973406400229626		[learning rate: 0.0014705]
		[batch 20/20] avg loss: 0.1330352712212706		[learning rate: 0.001467]
	Learning Rate: 0.00146697
	LOSS [training: 0.1313846676117834 | validation: 0.07626631990507665]
	TIME [epoch: 8.4 sec]
EPOCH 447/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14499276587057489		[learning rate: 0.0014634]
		[batch 20/20] avg loss: 0.11419183043993675		[learning rate: 0.0014599]
	Learning Rate: 0.00145988
	LOSS [training: 0.1295922981552558 | validation: 0.1268746763760093]
	TIME [epoch: 8.44 sec]
EPOCH 448/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10485895205589638		[learning rate: 0.0014563]
		[batch 20/20] avg loss: 0.13685126213623894		[learning rate: 0.0014528]
	Learning Rate: 0.00145282
	LOSS [training: 0.12085510709606764 | validation: 0.10001973406384934]
	TIME [epoch: 8.4 sec]
EPOCH 449/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10591463062693218		[learning rate: 0.0014493]
		[batch 20/20] avg loss: 0.11609090155234358		[learning rate: 0.0014458]
	Learning Rate: 0.00144579
	LOSS [training: 0.11100276608963788 | validation: 0.13181795675322364]
	TIME [epoch: 8.4 sec]
EPOCH 450/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12477746130657445		[learning rate: 0.0014423]
		[batch 20/20] avg loss: 0.14670486200621044		[learning rate: 0.0014388]
	Learning Rate: 0.0014388
	LOSS [training: 0.13574116165639244 | validation: 0.10078624450291893]
	TIME [epoch: 8.42 sec]
EPOCH 451/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12419815843190671		[learning rate: 0.0014353]
		[batch 20/20] avg loss: 0.12127078635678727		[learning rate: 0.0014318]
	Learning Rate: 0.00143184
	LOSS [training: 0.12273447239434701 | validation: 0.08803040814122]
	TIME [epoch: 8.42 sec]
EPOCH 452/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10819522391505931		[learning rate: 0.0014284]
		[batch 20/20] avg loss: 0.1034973702176194		[learning rate: 0.0014249]
	Learning Rate: 0.00142492
	LOSS [training: 0.10584629706633934 | validation: 0.09993281188577448]
	TIME [epoch: 8.41 sec]
EPOCH 453/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11519701638991875		[learning rate: 0.0014215]
		[batch 20/20] avg loss: 0.10778191013423008		[learning rate: 0.001418]
	Learning Rate: 0.00141803
	LOSS [training: 0.1114894632620744 | validation: 0.09588719994990653]
	TIME [epoch: 8.4 sec]
EPOCH 454/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11139081759944566		[learning rate: 0.0014146]
		[batch 20/20] avg loss: 0.10907480116297923		[learning rate: 0.0014112]
	Learning Rate: 0.00141117
	LOSS [training: 0.11023280938121242 | validation: 0.14816509161225117]
	TIME [epoch: 8.41 sec]
EPOCH 455/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13465442246252654		[learning rate: 0.0014078]
		[batch 20/20] avg loss: 0.10989064593857092		[learning rate: 0.0014043]
	Learning Rate: 0.00140434
	LOSS [training: 0.12227253420054873 | validation: 0.09072087675426675]
	TIME [epoch: 8.41 sec]
EPOCH 456/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1367870919042893		[learning rate: 0.0014009]
		[batch 20/20] avg loss: 0.14668646314753692		[learning rate: 0.0013976]
	Learning Rate: 0.00139755
	LOSS [training: 0.14173677752591313 | validation: 0.16730058942795556]
	TIME [epoch: 8.42 sec]
EPOCH 457/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1259415173284491		[learning rate: 0.0013942]
		[batch 20/20] avg loss: 0.12514766325689164		[learning rate: 0.0013908]
	Learning Rate: 0.0013908
	LOSS [training: 0.12554459029267037 | validation: 0.11454062332750518]
	TIME [epoch: 8.41 sec]
EPOCH 458/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13039275736495978		[learning rate: 0.0013874]
		[batch 20/20] avg loss: 0.12131149869887885		[learning rate: 0.0013841]
	Learning Rate: 0.00138407
	LOSS [training: 0.1258521280319193 | validation: 0.08451784266441546]
	TIME [epoch: 8.41 sec]
EPOCH 459/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11832231328037819		[learning rate: 0.0013807]
		[batch 20/20] avg loss: 0.12496190691126244		[learning rate: 0.0013774]
	Learning Rate: 0.00137738
	LOSS [training: 0.12164211009582031 | validation: 0.13892984536969039]
	TIME [epoch: 8.4 sec]
EPOCH 460/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12363074024377499		[learning rate: 0.001374]
		[batch 20/20] avg loss: 0.1296459322034387		[learning rate: 0.0013707]
	Learning Rate: 0.00137072
	LOSS [training: 0.12663833622360685 | validation: 0.1574702085366084]
	TIME [epoch: 8.42 sec]
EPOCH 461/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11825895833906305		[learning rate: 0.0013674]
		[batch 20/20] avg loss: 0.12690882825485889		[learning rate: 0.0013641]
	Learning Rate: 0.00136409
	LOSS [training: 0.12258389329696096 | validation: 0.1125285463059276]
	TIME [epoch: 8.42 sec]
EPOCH 462/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11204487312249767		[learning rate: 0.0013608]
		[batch 20/20] avg loss: 0.10620797426566378		[learning rate: 0.0013575]
	Learning Rate: 0.00135749
	LOSS [training: 0.10912642369408072 | validation: 0.08729283871292648]
	TIME [epoch: 8.39 sec]
EPOCH 463/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10442657576027559		[learning rate: 0.0013542]
		[batch 20/20] avg loss: 0.10255550904016335		[learning rate: 0.0013509]
	Learning Rate: 0.00135093
	LOSS [training: 0.10349104240021947 | validation: 0.12016722559562516]
	TIME [epoch: 8.4 sec]
EPOCH 464/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09728786092305837		[learning rate: 0.0013477]
		[batch 20/20] avg loss: 0.1252044288613887		[learning rate: 0.0013444]
	Learning Rate: 0.00134439
	LOSS [training: 0.11124614489222354 | validation: 0.09357680743380731]
	TIME [epoch: 8.44 sec]
EPOCH 465/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09642401883133564		[learning rate: 0.0013411]
		[batch 20/20] avg loss: 0.1742171704097378		[learning rate: 0.0013379]
	Learning Rate: 0.00133789
	LOSS [training: 0.13532059462053672 | validation: 0.13019477232118776]
	TIME [epoch: 8.41 sec]
EPOCH 466/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11468009635014434		[learning rate: 0.0013347]
		[batch 20/20] avg loss: 0.18125285793137041		[learning rate: 0.0013314]
	Learning Rate: 0.00133142
	LOSS [training: 0.14796647714075734 | validation: 0.13518319929500336]
	TIME [epoch: 8.41 sec]
EPOCH 467/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11426571634582947		[learning rate: 0.0013282]
		[batch 20/20] avg loss: 0.1289668790602463		[learning rate: 0.001325]
	Learning Rate: 0.00132498
	LOSS [training: 0.12161629770303788 | validation: 0.12158878841381725]
	TIME [epoch: 8.42 sec]
EPOCH 468/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12454558854069904		[learning rate: 0.0013218]
		[batch 20/20] avg loss: 0.11544504394502719		[learning rate: 0.0013186]
	Learning Rate: 0.00131858
	LOSS [training: 0.11999531624286312 | validation: 0.12928244694234703]
	TIME [epoch: 8.41 sec]
EPOCH 469/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14352697571769563		[learning rate: 0.0013154]
		[batch 20/20] avg loss: 0.1164377955002988		[learning rate: 0.0013122]
	Learning Rate: 0.0013122
	LOSS [training: 0.12998238560899722 | validation: 0.09113002915437271]
	TIME [epoch: 8.42 sec]
EPOCH 470/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11153724609648179		[learning rate: 0.001309]
		[batch 20/20] avg loss: 0.09852243980541982		[learning rate: 0.0013059]
	Learning Rate: 0.00130585
	LOSS [training: 0.10502984295095082 | validation: 0.12083301684604414]
	TIME [epoch: 8.41 sec]
EPOCH 471/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1269851598821267		[learning rate: 0.0013027]
		[batch 20/20] avg loss: 0.1179915705107458		[learning rate: 0.0012995]
	Learning Rate: 0.00129954
	LOSS [training: 0.12248836519643624 | validation: 0.09087618127065743]
	TIME [epoch: 8.42 sec]
EPOCH 472/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11803809339610927		[learning rate: 0.0012964]
		[batch 20/20] avg loss: 0.10860116850719297		[learning rate: 0.0012933]
	Learning Rate: 0.00129326
	LOSS [training: 0.11331963095165112 | validation: 0.10599857741527355]
	TIME [epoch: 8.4 sec]
EPOCH 473/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.16340698535567955		[learning rate: 0.0012901]
		[batch 20/20] avg loss: 0.11974204351416669		[learning rate: 0.001287]
	Learning Rate: 0.001287
	LOSS [training: 0.1415745144349231 | validation: 0.08168468770727323]
	TIME [epoch: 8.43 sec]
EPOCH 474/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13037685011376318		[learning rate: 0.0012839]
		[batch 20/20] avg loss: 0.11799860787643249		[learning rate: 0.0012808]
	Learning Rate: 0.00128078
	LOSS [training: 0.12418772899509785 | validation: 0.12431050856069367]
	TIME [epoch: 8.41 sec]
EPOCH 475/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11630186629253854		[learning rate: 0.0012777]
		[batch 20/20] avg loss: 0.10813622104996715		[learning rate: 0.0012746]
	Learning Rate: 0.00127458
	LOSS [training: 0.11221904367125284 | validation: 0.11132523438814557]
	TIME [epoch: 8.42 sec]
EPOCH 476/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11817554301097451		[learning rate: 0.0012715]
		[batch 20/20] avg loss: 0.11695882346211717		[learning rate: 0.0012684]
	Learning Rate: 0.00126842
	LOSS [training: 0.11756718323654583 | validation: 0.06923378041332973]
	TIME [epoch: 8.39 sec]
EPOCH 477/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09831635800941103		[learning rate: 0.0012653]
		[batch 20/20] avg loss: 0.11149864384300893		[learning rate: 0.0012623]
	Learning Rate: 0.00126229
	LOSS [training: 0.10490750092620997 | validation: 0.09700783865587442]
	TIME [epoch: 8.43 sec]
EPOCH 478/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13008453536489628		[learning rate: 0.0012592]
		[batch 20/20] avg loss: 0.13657580680997547		[learning rate: 0.0012562]
	Learning Rate: 0.00125618
	LOSS [training: 0.1333301710874359 | validation: 0.15175543077569248]
	TIME [epoch: 8.42 sec]
EPOCH 479/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15077948095073707		[learning rate: 0.0012531]
		[batch 20/20] avg loss: 0.10957749242322204		[learning rate: 0.0012501]
	Learning Rate: 0.00125011
	LOSS [training: 0.13017848668697954 | validation: 0.12177343739947083]
	TIME [epoch: 8.41 sec]
EPOCH 480/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09528681831929146		[learning rate: 0.0012471]
		[batch 20/20] avg loss: 0.1361393023696031		[learning rate: 0.0012441]
	Learning Rate: 0.00124406
	LOSS [training: 0.11571306034444728 | validation: 0.1454420153870601]
	TIME [epoch: 8.39 sec]
EPOCH 481/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12243814294552022		[learning rate: 0.0012411]
		[batch 20/20] avg loss: 0.13105257421802866		[learning rate: 0.001238]
	Learning Rate: 0.00123805
	LOSS [training: 0.12674535858177444 | validation: 0.22155111121454527]
	TIME [epoch: 8.43 sec]
EPOCH 482/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15299354797691533		[learning rate: 0.001235]
		[batch 20/20] avg loss: 0.11632874033596759		[learning rate: 0.0012321]
	Learning Rate: 0.00123206
	LOSS [training: 0.13466114415644148 | validation: 0.10090132584704518]
	TIME [epoch: 8.42 sec]
EPOCH 483/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1169560720256178		[learning rate: 0.0012291]
		[batch 20/20] avg loss: 0.10972452072179886		[learning rate: 0.0012261]
	Learning Rate: 0.0012261
	LOSS [training: 0.11334029637370832 | validation: 0.1276488424376577]
	TIME [epoch: 8.4 sec]
EPOCH 484/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11830208924051737		[learning rate: 0.0012231]
		[batch 20/20] avg loss: 0.6889825955672788		[learning rate: 0.0012202]
	Learning Rate: 0.00122017
	LOSS [training: 0.403642342403898 | validation: 1.9438619488494455]
	TIME [epoch: 8.42 sec]
EPOCH 485/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6995577113230085		[learning rate: 0.0012172]
		[batch 20/20] avg loss: 1.7538967295859251		[learning rate: 0.0012143]
	Learning Rate: 0.00121427
	LOSS [training: 1.7267272204544664 | validation: 2.140372730909996]
	TIME [epoch: 8.41 sec]
EPOCH 486/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8697007681134992		[learning rate: 0.0012113]
		[batch 20/20] avg loss: 1.5544933812341566		[learning rate: 0.0012084]
	Learning Rate: 0.0012084
	LOSS [training: 1.7120970746738275 | validation: 1.4730470582064141]
	TIME [epoch: 8.43 sec]
EPOCH 487/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7672851419043434		[learning rate: 0.0012055]
		[batch 20/20] avg loss: 0.3021465001267226		[learning rate: 0.0012026]
	Learning Rate: 0.00120256
	LOSS [training: 0.5347158210155329 | validation: 0.24901640060021074]
	TIME [epoch: 8.42 sec]
EPOCH 488/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.40969382969389256		[learning rate: 0.0011996]
		[batch 20/20] avg loss: 0.22173079070313345		[learning rate: 0.0011967]
	Learning Rate: 0.00119674
	LOSS [training: 0.31571231019851304 | validation: 0.20749694612350306]
	TIME [epoch: 8.4 sec]
EPOCH 489/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15975505862758516		[learning rate: 0.0011938]
		[batch 20/20] avg loss: 0.12171599671143529		[learning rate: 0.001191]
	Learning Rate: 0.00119095
	LOSS [training: 0.14073552766951025 | validation: 0.15693168767022678]
	TIME [epoch: 8.4 sec]
EPOCH 490/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13657539248824596		[learning rate: 0.0011881]
		[batch 20/20] avg loss: 0.14764409051177194		[learning rate: 0.0011852]
	Learning Rate: 0.00118519
	LOSS [training: 0.14210974150000893 | validation: 0.08749537643422746]
	TIME [epoch: 8.45 sec]
EPOCH 491/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10211629842481482		[learning rate: 0.0011823]
		[batch 20/20] avg loss: 0.15355204486606605		[learning rate: 0.0011795]
	Learning Rate: 0.00117946
	LOSS [training: 0.12783417164544048 | validation: 0.13814390485098882]
	TIME [epoch: 8.4 sec]
EPOCH 492/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.32926961700002577		[learning rate: 0.0011766]
		[batch 20/20] avg loss: 2.195264014233983		[learning rate: 0.0011738]
	Learning Rate: 0.00117376
	LOSS [training: 1.2622668156170045 | validation: 3.2783738084187526]
	TIME [epoch: 8.4 sec]
EPOCH 493/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.026914865420462		[learning rate: 0.0011709]
		[batch 20/20] avg loss: 4.028606231476575		[learning rate: 0.0011681]
	Learning Rate: 0.00116808
	LOSS [training: 4.027760548448518 | validation: 4.0211973399087775]
	TIME [epoch: 8.43 sec]
EPOCH 494/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.519126934441414		[learning rate: 0.0011653]
		[batch 20/20] avg loss: 3.496429341041498		[learning rate: 0.0011624]
	Learning Rate: 0.00116243
	LOSS [training: 3.507778137741456 | validation: 4.2963640371288685]
	TIME [epoch: 8.44 sec]
EPOCH 495/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.366167946072989		[learning rate: 0.0011596]
		[batch 20/20] avg loss: 3.292632423438133		[learning rate: 0.0011568]
	Learning Rate: 0.00115681
	LOSS [training: 3.8294001847555608 | validation: 2.0121363046589336]
	TIME [epoch: 8.42 sec]
EPOCH 496/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1716154916968455		[learning rate: 0.001154]
		[batch 20/20] avg loss: 2.420378676613475		[learning rate: 0.0011512]
	Learning Rate: 0.00115122
	LOSS [training: 2.29599708415516 | validation: 2.278484753190213]
	TIME [epoch: 8.43 sec]
EPOCH 497/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.115605781571714		[learning rate: 0.0011484]
		[batch 20/20] avg loss: 1.94064871868328		[learning rate: 0.0011457]
	Learning Rate: 0.00114565
	LOSS [training: 2.0281272501274974 | validation: 1.5191496161910383]
	TIME [epoch: 8.41 sec]
EPOCH 498/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.23295750190338		[learning rate: 0.0011429]
		[batch 20/20] avg loss: 1.0054228591203214		[learning rate: 0.0011401]
	Learning Rate: 0.00114011
	LOSS [training: 1.1191901805118507 | validation: 1.0835732976053478]
	TIME [epoch: 8.41 sec]
EPOCH 499/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3705131158763006		[learning rate: 0.0011374]
		[batch 20/20] avg loss: 0.11240293615065741		[learning rate: 0.0011346]
	Learning Rate: 0.0011346
	LOSS [training: 0.24145802601347904 | validation: 0.09189209454799083]
	TIME [epoch: 8.45 sec]
EPOCH 500/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13818110149941817		[learning rate: 0.0011319]
		[batch 20/20] avg loss: 0.12316838140889788		[learning rate: 0.0011291]
	Learning Rate: 0.00112911
	LOSS [training: 0.130674741454158 | validation: 0.11678855438105161]
	TIME [epoch: 8.42 sec]
EPOCH 501/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12089427978780845		[learning rate: 0.0011264]
		[batch 20/20] avg loss: 0.11158822659759468		[learning rate: 0.0011237]
	Learning Rate: 0.00112365
	LOSS [training: 0.11624125319270158 | validation: 0.07555930367792851]
	TIME [epoch: 8.41 sec]
EPOCH 502/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14438906270737317		[learning rate: 0.0011209]
		[batch 20/20] avg loss: 0.1310003167335673		[learning rate: 0.0011182]
	Learning Rate: 0.00111822
	LOSS [training: 0.13769468972047022 | validation: 0.11028897821141753]
	TIME [epoch: 8.43 sec]
EPOCH 503/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10944982679423523		[learning rate: 0.0011155]
		[batch 20/20] avg loss: 0.11429353227278724		[learning rate: 0.0011128]
	Learning Rate: 0.00111281
	LOSS [training: 0.11187167953351126 | validation: 0.07624013678264946]
	TIME [epoch: 8.44 sec]
EPOCH 504/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10328434926598122		[learning rate: 0.0011101]
		[batch 20/20] avg loss: 0.10966471345104216		[learning rate: 0.0011074]
	Learning Rate: 0.00110743
	LOSS [training: 0.1064745313585117 | validation: 0.0810208501121419]
	TIME [epoch: 8.42 sec]
EPOCH 505/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13387025957699877		[learning rate: 0.0011047]
		[batch 20/20] avg loss: 0.13570959752635367		[learning rate: 0.0011021]
	Learning Rate: 0.00110207
	LOSS [training: 0.1347899285516762 | validation: 0.09274263812585622]
	TIME [epoch: 8.44 sec]
EPOCH 506/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12541757102374002		[learning rate: 0.0010994]
		[batch 20/20] avg loss: 0.12619286088996856		[learning rate: 0.0010967]
	Learning Rate: 0.00109674
	LOSS [training: 0.1258052159568543 | validation: 0.07809311397933258]
	TIME [epoch: 8.41 sec]
EPOCH 507/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12261984215285957		[learning rate: 0.0010941]
		[batch 20/20] avg loss: 0.10776643422151214		[learning rate: 0.0010914]
	Learning Rate: 0.00109144
	LOSS [training: 0.11519313818718588 | validation: 0.08576748813333011]
	TIME [epoch: 8.43 sec]
EPOCH 508/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12563899537080486		[learning rate: 0.0010888]
		[batch 20/20] avg loss: 0.40888770214665754		[learning rate: 0.0010862]
	Learning Rate: 0.00108616
	LOSS [training: 0.2672633487587312 | validation: 1.064746570682975]
	TIME [epoch: 8.44 sec]
EPOCH 509/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.267083866867062		[learning rate: 0.0010835]
		[batch 20/20] avg loss: 3.0087171501732906		[learning rate: 0.0010809]
	Learning Rate: 0.00108091
	LOSS [training: 2.637900508520176 | validation: 2.273525195503685]
	TIME [epoch: 8.41 sec]
EPOCH 510/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7401747733683368		[learning rate: 0.0010783]
		[batch 20/20] avg loss: 2.1292997032572796		[learning rate: 0.0010757]
	Learning Rate: 0.00107568
	LOSS [training: 1.9347372383128083 | validation: 3.38018486033869]
	TIME [epoch: 8.42 sec]
EPOCH 511/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.4869611848765216		[learning rate: 0.0010731]
		[batch 20/20] avg loss: 3.7716944195314817		[learning rate: 0.0010705]
	Learning Rate: 0.00107048
	LOSS [training: 3.629327802204002 | validation: 4.350630978511864]
	TIME [epoch: 8.43 sec]
EPOCH 512/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.33964230686951		[learning rate: 0.0010679]
		[batch 20/20] avg loss: 3.7328665220789405		[learning rate: 0.0010653]
	Learning Rate: 0.0010653
	LOSS [training: 4.036254414474224 | validation: 3.5855915599481163]
	TIME [epoch: 8.44 sec]
EPOCH 513/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6779982337512154		[learning rate: 0.0010627]
		[batch 20/20] avg loss: 1.4280844122012533		[learning rate: 0.0010602]
	Learning Rate: 0.00106015
	LOSS [training: 2.053041322976234 | validation: 1.1899638861711272]
	TIME [epoch: 8.42 sec]
EPOCH 514/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.188001418042372		[learning rate: 0.0010576]
		[batch 20/20] avg loss: 0.2511674207947464		[learning rate: 0.001055]
	Learning Rate: 0.00105503
	LOSS [training: 0.7195844194185592 | validation: 0.19227026016286836]
	TIME [epoch: 8.43 sec]
EPOCH 515/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.3412372842359718		[learning rate: 0.0010525]
		[batch 20/20] avg loss: 1.018695526658999		[learning rate: 0.0010499]
	Learning Rate: 0.00104992
	LOSS [training: 0.6799664054474854 | validation: 1.079163026776576]
	TIME [epoch: 8.4 sec]
EPOCH 516/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2987317698380192		[learning rate: 0.0010474]
		[batch 20/20] avg loss: 1.5425949850516911		[learning rate: 0.0010448]
	Learning Rate: 0.00104485
	LOSS [training: 1.4206633774448552 | validation: 1.6352349712956273]
	TIME [epoch: 8.44 sec]
EPOCH 517/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6633669504392927		[learning rate: 0.0010423]
		[batch 20/20] avg loss: 1.5421950168354686		[learning rate: 0.0010398]
	Learning Rate: 0.00103979
	LOSS [training: 1.6027809836373805 | validation: 1.227746223498822]
	TIME [epoch: 8.43 sec]
EPOCH 518/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7851871101090799		[learning rate: 0.0010373]
		[batch 20/20] avg loss: 0.22136927569357434		[learning rate: 0.0010348]
	Learning Rate: 0.00103477
	LOSS [training: 0.5032781929013271 | validation: 0.6121046084766419]
	TIME [epoch: 8.41 sec]
EPOCH 519/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0845771506162067		[learning rate: 0.0010323]
		[batch 20/20] avg loss: 2.628221739205338		[learning rate: 0.0010298]
	Learning Rate: 0.00102976
	LOSS [training: 1.8563994449107724 | validation: 3.169726757994357]
	TIME [epoch: 8.41 sec]
EPOCH 520/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.4686696831601944		[learning rate: 0.0010273]
		[batch 20/20] avg loss: 3.478892683440616		[learning rate: 0.0010248]
	Learning Rate: 0.00102478
	LOSS [training: 3.4737811833004044 | validation: 3.8298837470485]
	TIME [epoch: 8.44 sec]
EPOCH 521/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.141909122065075		[learning rate: 0.0010223]
		[batch 20/20] avg loss: 4.945463591464893		[learning rate: 0.0010198]
	Learning Rate: 0.00101983
	LOSS [training: 4.543686356764985 | validation: 5.5010878587497105]
	TIME [epoch: 8.42 sec]
EPOCH 522/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.888065343739272		[learning rate: 0.0010174]
		[batch 20/20] avg loss: 4.2251726463204164		[learning rate: 0.0010149]
	Learning Rate: 0.00101489
	LOSS [training: 4.5566189950298455 | validation: 3.8495991920513064]
	TIME [epoch: 8.41 sec]
EPOCH 523/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.323211398235833		[learning rate: 0.0010124]
		[batch 20/20] avg loss: 2.9261678408585277		[learning rate: 0.00101]
	Learning Rate: 0.00100999
	LOSS [training: 3.12468961954718 | validation: 2.4327671468218734]
	TIME [epoch: 8.43 sec]
EPOCH 524/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6290402766535506		[learning rate: 0.0010075]
		[batch 20/20] avg loss: 2.549206449356689		[learning rate: 0.0010051]
	Learning Rate: 0.0010051
	LOSS [training: 2.5891233630051205 | validation: 2.5143694009578983]
	TIME [epoch: 8.41 sec]
EPOCH 525/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.459673927427006		[learning rate: 0.0010027]
		[batch 20/20] avg loss: 2.6544417940826337		[learning rate: 0.0010002]
	Learning Rate: 0.00100024
	LOSS [training: 2.55705786075482 | validation: 2.3010830001298874]
	TIME [epoch: 8.43 sec]
EPOCH 526/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.491780806786069		[learning rate: 0.00099782]
		[batch 20/20] avg loss: 2.709854157007711		[learning rate: 0.00099541]
	Learning Rate: 0.000995405
	LOSS [training: 2.60081748189689 | validation: 2.4874591964563715]
	TIME [epoch: 8.42 sec]
EPOCH 527/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6122378327747184		[learning rate: 0.000993]
		[batch 20/20] avg loss: 2.890655602770677		[learning rate: 0.00099059]
	Learning Rate: 0.000990592
	LOSS [training: 2.751446717772698 | validation: 2.712927800777232]
	TIME [epoch: 8.41 sec]
EPOCH 528/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.913213400232991		[learning rate: 0.00098819]
		[batch 20/20] avg loss: 2.7908819750415796		[learning rate: 0.0009858]
	Learning Rate: 0.000985801
	LOSS [training: 2.8520476876372847 | validation: 2.48525026441675]
	TIME [epoch: 8.39 sec]
EPOCH 529/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.4088913519782773		[learning rate: 0.00098341]
		[batch 20/20] avg loss: 4.072007201326478		[learning rate: 0.00098103]
	Learning Rate: 0.000981034
	LOSS [training: 3.7404492766523787 | validation: 4.256321106960262]
	TIME [epoch: 8.45 sec]
EPOCH 530/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.149586712536914		[learning rate: 0.00097866]
		[batch 20/20] avg loss: 4.203055031430499		[learning rate: 0.00097629]
	Learning Rate: 0.00097629
	LOSS [training: 4.176320871983707 | validation: 4.2057210040201385]
	TIME [epoch: 8.41 sec]
EPOCH 531/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.257212771834255		[learning rate: 0.00097393]
		[batch 20/20] avg loss: 3.902304952616496		[learning rate: 0.00097157]
	Learning Rate: 0.000971569
	LOSS [training: 4.0797588622253755 | validation: 3.8382128963511546]
	TIME [epoch: 8.4 sec]
EPOCH 532/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.5579924265652885		[learning rate: 0.00096922]
		[batch 20/20] avg loss: 3.0307065322866156		[learning rate: 0.00096687]
	Learning Rate: 0.000966871
	LOSS [training: 3.2943494794259522 | validation: 2.413856022576182]
	TIME [epoch: 8.4 sec]
EPOCH 533/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.464349460121952		[learning rate: 0.00096453]
		[batch 20/20] avg loss: 2.270881479010845		[learning rate: 0.00096219]
	Learning Rate: 0.000962195
	LOSS [training: 2.367615469566398 | validation: 1.921033231111668]
	TIME [epoch: 8.44 sec]
EPOCH 534/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.200558545515443		[learning rate: 0.00095987]
		[batch 20/20] avg loss: 2.165279653876805		[learning rate: 0.00095754]
	Learning Rate: 0.000957542
	LOSS [training: 2.1829190996961243 | validation: 2.147334903134281]
	TIME [epoch: 8.41 sec]
EPOCH 535/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.184528033823345		[learning rate: 0.00095522]
		[batch 20/20] avg loss: 1.974224413064124		[learning rate: 0.00095291]
	Learning Rate: 0.000952912
	LOSS [training: 2.0793762234437345 | validation: 1.672164698891048]
	TIME [epoch: 8.39 sec]
EPOCH 536/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9636536020854254		[learning rate: 0.0009506]
		[batch 20/20] avg loss: 2.3402479088203014		[learning rate: 0.0009483]
	Learning Rate: 0.000948304
	LOSS [training: 2.151950755452863 | validation: 2.509141240783824]
	TIME [epoch: 8.41 sec]
EPOCH 537/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.496375178838446		[learning rate: 0.00094601]
		[batch 20/20] avg loss: 0.9026319725466815		[learning rate: 0.00094372]
	Learning Rate: 0.000943718
	LOSS [training: 1.6995035756925632 | validation: 0.34499311312654524]
	TIME [epoch: 8.43 sec]
EPOCH 538/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.23614053842231253		[learning rate: 0.00094143]
		[batch 20/20] avg loss: 0.1810012459664177		[learning rate: 0.00093915]
	Learning Rate: 0.000939154
	LOSS [training: 0.20857089219436512 | validation: 0.14273139019925513]
	TIME [epoch: 8.4 sec]
EPOCH 539/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1488634197991924		[learning rate: 0.00093688]
		[batch 20/20] avg loss: 0.14751850520563578		[learning rate: 0.00093461]
	Learning Rate: 0.000934613
	LOSS [training: 0.1481909625024141 | validation: 0.129583993704769]
	TIME [epoch: 8.39 sec]
EPOCH 540/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15159946751654968		[learning rate: 0.00093235]
		[batch 20/20] avg loss: 0.13978787920650806		[learning rate: 0.00093009]
	Learning Rate: 0.000930093
	LOSS [training: 0.14569367336152883 | validation: 0.11642091589807872]
	TIME [epoch: 8.42 sec]
EPOCH 541/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12277814317411166		[learning rate: 0.00092784]
		[batch 20/20] avg loss: 0.16598519039306175		[learning rate: 0.0009256]
	Learning Rate: 0.000925595
	LOSS [training: 0.14438166678358672 | validation: 0.13358096198779157]
	TIME [epoch: 8.39 sec]
EPOCH 542/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.28128262456014386		[learning rate: 0.00092335]
		[batch 20/20] avg loss: 0.1330350069543985		[learning rate: 0.00092112]
	Learning Rate: 0.000921119
	LOSS [training: 0.2071588157572712 | validation: 0.1015030855171134]
	TIME [epoch: 8.43 sec]
EPOCH 543/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12014060908290028		[learning rate: 0.00091889]
		[batch 20/20] avg loss: 0.11934626155093239		[learning rate: 0.00091666]
	Learning Rate: 0.000916665
	LOSS [training: 0.11974343531691631 | validation: 0.13953527575228578]
	TIME [epoch: 8.41 sec]
EPOCH 544/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14974065521545885		[learning rate: 0.00091445]
		[batch 20/20] avg loss: 0.11330869151048129		[learning rate: 0.00091223]
	Learning Rate: 0.000912232
	LOSS [training: 0.13152467336297005 | validation: 0.10628564360727738]
	TIME [epoch: 8.4 sec]
EPOCH 545/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12402225686016592		[learning rate: 0.00091002]
		[batch 20/20] avg loss: 0.11675056038800222		[learning rate: 0.00090782]
	Learning Rate: 0.000907821
	LOSS [training: 0.12038640862408409 | validation: 0.12550726226693693]
	TIME [epoch: 8.39 sec]
EPOCH 546/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13772371809259532		[learning rate: 0.00090562]
		[batch 20/20] avg loss: 0.13480760297063113		[learning rate: 0.00090343]
	Learning Rate: 0.00090343
	LOSS [training: 0.1362656605316132 | validation: 0.09846534722732372]
	TIME [epoch: 8.43 sec]
EPOCH 547/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1793619383086516		[learning rate: 0.00090124]
		[batch 20/20] avg loss: 1.6312448517488534		[learning rate: 0.00089906]
	Learning Rate: 0.000899062
	LOSS [training: 0.9053033950287525 | validation: 1.5902372712041828]
	TIME [epoch: 8.42 sec]
EPOCH 548/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.978855687929385		[learning rate: 0.00089689]
		[batch 20/20] avg loss: 1.9232570450542235		[learning rate: 0.00089471]
	Learning Rate: 0.000894714
	LOSS [training: 1.9510563664918041 | validation: 1.6713157564499208]
	TIME [epoch: 8.4 sec]
EPOCH 549/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9463746805620938		[learning rate: 0.00089255]
		[batch 20/20] avg loss: 1.477012038852352		[learning rate: 0.00089039]
	Learning Rate: 0.000890387
	LOSS [training: 1.711693359707223 | validation: 0.829530134421313]
	TIME [epoch: 8.4 sec]
EPOCH 550/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7190458127970166		[learning rate: 0.00088823]
		[batch 20/20] avg loss: 0.8609838957102204		[learning rate: 0.00088608]
	Learning Rate: 0.000886081
	LOSS [training: 0.7900148542536185 | validation: 0.694646992178312]
	TIME [epoch: 8.43 sec]
EPOCH 551/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9821796901344662		[learning rate: 0.00088394]
		[batch 20/20] avg loss: 0.8928215690675152		[learning rate: 0.0008818]
	Learning Rate: 0.000881797
	LOSS [training: 0.9375006296009907 | validation: 0.9351818609418874]
	TIME [epoch: 8.41 sec]
EPOCH 552/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1462805494580324		[learning rate: 0.00087966]
		[batch 20/20] avg loss: 0.7227141332596926		[learning rate: 0.00087753]
	Learning Rate: 0.000877532
	LOSS [training: 0.9344973413588624 | validation: 0.3668319870892436]
	TIME [epoch: 8.4 sec]
EPOCH 553/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.20639036389062312		[learning rate: 0.00087541]
		[batch 20/20] avg loss: 0.15417890624084934		[learning rate: 0.00087329]
	Learning Rate: 0.000873289
	LOSS [training: 0.1802846350657362 | validation: 0.1328837639890168]
	TIME [epoch: 8.42 sec]
EPOCH 554/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.14096490876132722		[learning rate: 0.00087117]
		[batch 20/20] avg loss: 0.13008204415307703		[learning rate: 0.00086907]
	Learning Rate: 0.000869066
	LOSS [training: 0.1355234764572021 | validation: 0.117253723847849]
	TIME [epoch: 8.4 sec]
EPOCH 555/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13344469868855227		[learning rate: 0.00086696]
		[batch 20/20] avg loss: 0.1542890829288137		[learning rate: 0.00086486]
	Learning Rate: 0.000864863
	LOSS [training: 0.143866890808683 | validation: 0.19496864580636597]
	TIME [epoch: 8.43 sec]
EPOCH 556/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.2533000349499507		[learning rate: 0.00086277]
		[batch 20/20] avg loss: 0.15580052695362306		[learning rate: 0.00086068]
	Learning Rate: 0.000860681
	LOSS [training: 0.20455028095178687 | validation: 0.1617665130930481]
	TIME [epoch: 8.42 sec]
EPOCH 557/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11600382293835738		[learning rate: 0.0008586]
		[batch 20/20] avg loss: 0.10137408150089888		[learning rate: 0.00085652]
	Learning Rate: 0.000856519
	LOSS [training: 0.10868895221962813 | validation: 0.09851245750181134]
	TIME [epoch: 8.4 sec]
EPOCH 558/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09756863442112737		[learning rate: 0.00085445]
		[batch 20/20] avg loss: 0.11802231723836558		[learning rate: 0.00085238]
	Learning Rate: 0.000852377
	LOSS [training: 0.10779547582974647 | validation: 0.10053197633988595]
	TIME [epoch: 8.4 sec]
EPOCH 559/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11501019174131266		[learning rate: 0.00085031]
		[batch 20/20] avg loss: 0.21184673058773376		[learning rate: 0.00084825]
	Learning Rate: 0.000848255
	LOSS [training: 0.1634284611645232 | validation: 0.3102780783537452]
	TIME [epoch: 8.44 sec]
EPOCH 560/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0691214405068536		[learning rate: 0.0008462]
		[batch 20/20] avg loss: 1.4482599174162523		[learning rate: 0.00084415]
	Learning Rate: 0.000844153
	LOSS [training: 1.2586906789615528 | validation: 1.746459988096487]
	TIME [epoch: 8.4 sec]
EPOCH 561/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4039508361314637		[learning rate: 0.00084211]
		[batch 20/20] avg loss: 1.3304504633104468		[learning rate: 0.00084007]
	Learning Rate: 0.000840071
	LOSS [training: 1.3672006497209552 | validation: 1.6239675066724046]
	TIME [epoch: 8.41 sec]
EPOCH 562/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2197225359059096		[learning rate: 0.00083804]
		[batch 20/20] avg loss: 1.6453718576725316		[learning rate: 0.00083601]
	Learning Rate: 0.000836008
	LOSS [training: 1.4325471967892205 | validation: 2.2413151490761742]
	TIME [epoch: 8.41 sec]
EPOCH 563/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.189622465710258		[learning rate: 0.00083398]
		[batch 20/20] avg loss: 2.1991155021842026		[learning rate: 0.00083197]
	Learning Rate: 0.000831965
	LOSS [training: 2.19436898394723 | validation: 2.3951940672093537]
	TIME [epoch: 8.41 sec]
EPOCH 564/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.0616622140867547		[learning rate: 0.00082995]
		[batch 20/20] avg loss: 2.9083088775860313		[learning rate: 0.00082794]
	Learning Rate: 0.000827942
	LOSS [training: 2.484985545836394 | validation: 3.7493362931353174]
	TIME [epoch: 8.44 sec]
EPOCH 565/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.787819386230871		[learning rate: 0.00082594]
		[batch 20/20] avg loss: 3.9896214423344545		[learning rate: 0.00082394]
	Learning Rate: 0.000823938
	LOSS [training: 3.8887204142826635 | validation: 4.098771639937562]
	TIME [epoch: 8.4 sec]
EPOCH 566/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.082927686625183		[learning rate: 0.00082194]
		[batch 20/20] avg loss: 4.783580821924344		[learning rate: 0.00081995]
	Learning Rate: 0.000819954
	LOSS [training: 4.433254254274764 | validation: 4.949499134856081]
	TIME [epoch: 8.41 sec]
EPOCH 567/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.260347960649938		[learning rate: 0.00081797]
		[batch 20/20] avg loss: 3.9600340913323215		[learning rate: 0.00081599]
	Learning Rate: 0.000815989
	LOSS [training: 4.11019102599113 | validation: 3.8374017996841108]
	TIME [epoch: 8.42 sec]
EPOCH 568/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.534929231800897		[learning rate: 0.00081401]
		[batch 20/20] avg loss: 3.2800665108772113		[learning rate: 0.00081204]
	Learning Rate: 0.000812043
	LOSS [training: 3.4074978713390545 | validation: 3.4400884539404997]
	TIME [epoch: 8.43 sec]
EPOCH 569/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.910487904080051		[learning rate: 0.00081008]
		[batch 20/20] avg loss: 2.4960970202927646		[learning rate: 0.00080812]
	Learning Rate: 0.000808116
	LOSS [training: 2.7032924621864076 | validation: 2.5561007684349857]
	TIME [epoch: 8.43 sec]
EPOCH 570/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.689813708426637		[learning rate: 0.00080616]
		[batch 20/20] avg loss: 3.3101295283065526		[learning rate: 0.00080421]
	Learning Rate: 0.000804208
	LOSS [training: 2.9999716183665948 | validation: 3.7362286644441225]
	TIME [epoch: 8.4 sec]
EPOCH 571/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.51264514566705		[learning rate: 0.00080226]
		[batch 20/20] avg loss: 3.3969269947972265		[learning rate: 0.00080032]
	Learning Rate: 0.000800319
	LOSS [training: 3.454786070232138 | validation: 3.883274220046806]
	TIME [epoch: 8.42 sec]
EPOCH 572/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.4022341022898863		[learning rate: 0.00079838]
		[batch 20/20] avg loss: 3.1152444716999623		[learning rate: 0.00079645]
	Learning Rate: 0.000796449
	LOSS [training: 3.258739286994924 | validation: 3.440757603228214]
	TIME [epoch: 8.45 sec]
EPOCH 573/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.826968813140804		[learning rate: 0.00079452]
		[batch 20/20] avg loss: 2.63224727795149		[learning rate: 0.0007926]
	Learning Rate: 0.000792597
	LOSS [training: 2.7296080455461476 | validation: 3.228957985177998]
	TIME [epoch: 8.41 sec]
EPOCH 574/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.0656159339066713		[learning rate: 0.00079068]
		[batch 20/20] avg loss: 2.4567924139872086		[learning rate: 0.00078876]
	Learning Rate: 0.000788765
	LOSS [training: 2.76120417394694 | validation: 2.590158906476053]
	TIME [epoch: 8.43 sec]
EPOCH 575/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.45951123815628		[learning rate: 0.00078686]
		[batch 20/20] avg loss: 2.3114675277121606		[learning rate: 0.00078495]
	Learning Rate: 0.00078495
	LOSS [training: 2.38548938293422 | validation: 2.1490414649444873]
	TIME [epoch: 8.42 sec]
EPOCH 576/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9393038784632917		[learning rate: 0.00078305]
		[batch 20/20] avg loss: 3.3301411531529745		[learning rate: 0.00078115]
	Learning Rate: 0.000781154
	LOSS [training: 2.6347225158081335 | validation: 3.532303043600733]
	TIME [epoch: 8.44 sec]
EPOCH 577/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.6377608449438936		[learning rate: 0.00077926]
		[batch 20/20] avg loss: 4.020636913189007		[learning rate: 0.00077738]
	Learning Rate: 0.000777377
	LOSS [training: 3.8291988790664506 | validation: 4.4874128753842]
	TIME [epoch: 8.43 sec]
EPOCH 578/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.192364118873973		[learning rate: 0.00077549]
		[batch 20/20] avg loss: 3.0621277043708317		[learning rate: 0.00077362]
	Learning Rate: 0.000773618
	LOSS [training: 3.6272459116224036 | validation: 3.199907728164034]
	TIME [epoch: 8.4 sec]
EPOCH 579/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.830536941394361		[learning rate: 0.00077174]
		[batch 20/20] avg loss: 2.3244142058912		[learning rate: 0.00076988]
	Learning Rate: 0.000769877
	LOSS [training: 2.57747557364278 | validation: 2.8069357166526476]
	TIME [epoch: 8.42 sec]
EPOCH 580/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.220965843188714		[learning rate: 0.00076801]
		[batch 20/20] avg loss: 2.107321002080046		[learning rate: 0.00076615]
	Learning Rate: 0.000766154
	LOSS [training: 2.1641434226343805 | validation: 2.20163125724934]
	TIME [epoch: 8.42 sec]
EPOCH 581/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9095769222573717		[learning rate: 0.0007643]
		[batch 20/20] avg loss: 1.9922964879319984		[learning rate: 0.00076245]
	Learning Rate: 0.000762448
	LOSS [training: 1.9509367050946853 | validation: 2.0902485911096615]
	TIME [epoch: 8.42 sec]
EPOCH 582/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9907772301646005		[learning rate: 0.0007606]
		[batch 20/20] avg loss: 1.771485170144373		[learning rate: 0.00075876]
	Learning Rate: 0.000758761
	LOSS [training: 1.8811312001544866 | validation: 1.7297386304705056]
	TIME [epoch: 8.42 sec]
EPOCH 583/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7456250455003524		[learning rate: 0.00075692]
		[batch 20/20] avg loss: 1.7124903594223952		[learning rate: 0.00075509]
	Learning Rate: 0.000755092
	LOSS [training: 1.729057702461374 | validation: 1.913932751078502]
	TIME [epoch: 8.41 sec]
EPOCH 584/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.810452060635733		[learning rate: 0.00075326]
		[batch 20/20] avg loss: 0.6437897235244951		[learning rate: 0.00075144]
	Learning Rate: 0.000751441
	LOSS [training: 1.2271208920801142 | validation: 0.19743347587620116]
	TIME [epoch: 8.4 sec]
EPOCH 585/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.25702688881078944		[learning rate: 0.00074962]
		[batch 20/20] avg loss: 0.20614822128586302		[learning rate: 0.00074781]
	Learning Rate: 0.000747807
	LOSS [training: 0.23158755504832626 | validation: 0.1285645524527854]
	TIME [epoch: 8.45 sec]
EPOCH 586/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1832587343734025		[learning rate: 0.000746]
		[batch 20/20] avg loss: 0.130667148609983		[learning rate: 0.00074419]
	Learning Rate: 0.000744191
	LOSS [training: 0.15696294149169274 | validation: 0.09530433839334478]
	TIME [epoch: 8.4 sec]
EPOCH 587/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13164452811440353		[learning rate: 0.00074239]
		[batch 20/20] avg loss: 0.13713218521708376		[learning rate: 0.00074059]
	Learning Rate: 0.000740592
	LOSS [training: 0.13438835666574364 | validation: 0.10566589626718959]
	TIME [epoch: 8.41 sec]
EPOCH 588/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13862863532759326		[learning rate: 0.0007388]
		[batch 20/20] avg loss: 0.15742126948593965		[learning rate: 0.00073701]
	Learning Rate: 0.000737011
	LOSS [training: 0.14802495240676644 | validation: 0.10508855222772548]
	TIME [epoch: 8.42 sec]
EPOCH 589/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12060515948437571		[learning rate: 0.00073523]
		[batch 20/20] avg loss: 0.1204565427913413		[learning rate: 0.00073345]
	Learning Rate: 0.000733446
	LOSS [training: 0.12053085113785852 | validation: 0.1402398069341787]
	TIME [epoch: 8.42 sec]
EPOCH 590/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13470693560150354		[learning rate: 0.00073167]
		[batch 20/20] avg loss: 0.10139926281732217		[learning rate: 0.0007299]
	Learning Rate: 0.0007299
	LOSS [training: 0.11805309920941283 | validation: 0.08167695150614462]
	TIME [epoch: 8.43 sec]
EPOCH 591/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13393113092111716		[learning rate: 0.00072813]
		[batch 20/20] avg loss: 0.1309552553900846		[learning rate: 0.00072637]
	Learning Rate: 0.00072637
	LOSS [training: 0.13244319315560088 | validation: 0.06938971673727663]
	TIME [epoch: 8.41 sec]
EPOCH 592/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10481670635112561		[learning rate: 0.00072461]
		[batch 20/20] avg loss: 0.11832113301796574		[learning rate: 0.00072286]
	Learning Rate: 0.000722857
	LOSS [training: 0.11156891968454567 | validation: 0.0747283584411513]
	TIME [epoch: 8.41 sec]
EPOCH 593/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13870815253544905		[learning rate: 0.00072111]
		[batch 20/20] avg loss: 0.13443642427484415		[learning rate: 0.00071936]
	Learning Rate: 0.000719362
	LOSS [training: 0.1365722884051466 | validation: 0.10535781137015073]
	TIME [epoch: 8.44 sec]
EPOCH 594/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10479576360430334		[learning rate: 0.00071762]
		[batch 20/20] avg loss: 0.11882931694019479		[learning rate: 0.00071588]
	Learning Rate: 0.000715883
	LOSS [training: 0.1118125402722491 | validation: 0.11212967415584746]
	TIME [epoch: 8.42 sec]
EPOCH 595/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12198239780636629		[learning rate: 0.00071415]
		[batch 20/20] avg loss: 0.10132461967411924		[learning rate: 0.00071242]
	Learning Rate: 0.000712421
	LOSS [training: 0.11165350874024274 | validation: 0.08726500747223426]
	TIME [epoch: 8.41 sec]
EPOCH 596/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11186723839820893		[learning rate: 0.0007107]
		[batch 20/20] avg loss: 0.11578257721396712		[learning rate: 0.00070898]
	Learning Rate: 0.000708976
	LOSS [training: 0.11382490780608805 | validation: 0.11085558050420441]
	TIME [epoch: 8.42 sec]
EPOCH 597/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10899398934033504		[learning rate: 0.00070726]
		[batch 20/20] avg loss: 0.11203838664832963		[learning rate: 0.00070555]
	Learning Rate: 0.000705548
	LOSS [training: 0.11051618799433234 | validation: 0.10759414682407888]
	TIME [epoch: 8.4 sec]
EPOCH 598/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10332538396947424		[learning rate: 0.00070384]
		[batch 20/20] avg loss: 0.09455538964417456		[learning rate: 0.00070214]
	Learning Rate: 0.000702136
	LOSS [training: 0.09894038680682439 | validation: 0.08259672326934386]
	TIME [epoch: 8.44 sec]
EPOCH 599/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10957535463840214		[learning rate: 0.00070044]
		[batch 20/20] avg loss: 0.09768224097211627		[learning rate: 0.00069874]
	Learning Rate: 0.00069874
	LOSS [training: 0.1036287978052592 | validation: 0.09040410479866577]
	TIME [epoch: 8.42 sec]
EPOCH 600/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13603212339247087		[learning rate: 0.00069705]
		[batch 20/20] avg loss: 0.1022481067267601		[learning rate: 0.00069536]
	Learning Rate: 0.000695361
	LOSS [training: 0.11914011505961548 | validation: 0.08894545457347787]
	TIME [epoch: 8.4 sec]
EPOCH 601/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11892164321304537		[learning rate: 0.00069368]
		[batch 20/20] avg loss: 0.09956968373276878		[learning rate: 0.000692]
	Learning Rate: 0.000691999
	LOSS [training: 0.10924566347290707 | validation: 0.07520039769785815]
	TIME [epoch: 8.4 sec]
EPOCH 602/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09692646729389903		[learning rate: 0.00069032]
		[batch 20/20] avg loss: 0.09878808663481034		[learning rate: 0.00068865]
	Learning Rate: 0.000688652
	LOSS [training: 0.0978572769643547 | validation: 0.09757780614344388]
	TIME [epoch: 8.45 sec]
EPOCH 603/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10953591179082436		[learning rate: 0.00068699]
		[batch 20/20] avg loss: 0.10940361236302694		[learning rate: 0.00068532]
	Learning Rate: 0.000685322
	LOSS [training: 0.10946976207692563 | validation: 0.10506838478616831]
	TIME [epoch: 8.4 sec]
EPOCH 604/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12400204151786379		[learning rate: 0.00068366]
		[batch 20/20] avg loss: 0.11076755832928079		[learning rate: 0.00068201]
	Learning Rate: 0.000682008
	LOSS [training: 0.11738479992357229 | validation: 0.12379245626324756]
	TIME [epoch: 8.4 sec]
EPOCH 605/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.125387793407117		[learning rate: 0.00068036]
		[batch 20/20] avg loss: 0.11022379769589137		[learning rate: 0.00067871]
	Learning Rate: 0.00067871
	LOSS [training: 0.1178057955515042 | validation: 0.08505767231326238]
	TIME [epoch: 8.42 sec]
EPOCH 606/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.15134889565709367		[learning rate: 0.00067707]
		[batch 20/20] avg loss: 0.16853598804426373		[learning rate: 0.00067543]
	Learning Rate: 0.000675428
	LOSS [training: 0.15994244185067866 | validation: 0.13672811644162997]
	TIME [epoch: 8.41 sec]
EPOCH 607/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1393282515338808		[learning rate: 0.00067379]
		[batch 20/20] avg loss: 0.1400698827585077		[learning rate: 0.00067216]
	Learning Rate: 0.000672162
	LOSS [training: 0.13969906714619426 | validation: 0.0993581421489392]
	TIME [epoch: 8.42 sec]
EPOCH 608/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.129089245482272		[learning rate: 0.00067053]
		[batch 20/20] avg loss: 0.11127527225438483		[learning rate: 0.00066891]
	Learning Rate: 0.000668911
	LOSS [training: 0.12018225886832841 | validation: 0.08881096573673497]
	TIME [epoch: 8.42 sec]
EPOCH 609/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10033288554661698		[learning rate: 0.00066729]
		[batch 20/20] avg loss: 0.12548560669442407		[learning rate: 0.00066568]
	Learning Rate: 0.000665676
	LOSS [training: 0.11290924612052052 | validation: 0.08345216839346531]
	TIME [epoch: 8.4 sec]
EPOCH 610/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10510041103624561		[learning rate: 0.00066406]
		[batch 20/20] avg loss: 0.11685576173435568		[learning rate: 0.00066246]
	Learning Rate: 0.000662457
	LOSS [training: 0.11097808638530063 | validation: 0.10044453772560784]
	TIME [epoch: 8.41 sec]
EPOCH 611/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11199334539521615		[learning rate: 0.00066085]
		[batch 20/20] avg loss: 0.12020731263250214		[learning rate: 0.00065925]
	Learning Rate: 0.000659254
	LOSS [training: 0.11610032901385914 | validation: 0.12318821483656459]
	TIME [epoch: 8.44 sec]
EPOCH 612/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1171271582520212		[learning rate: 0.00065766]
		[batch 20/20] avg loss: 0.12057049803023345		[learning rate: 0.00065607]
	Learning Rate: 0.000656066
	LOSS [training: 0.11884882814112732 | validation: 0.08551645445699294]
	TIME [epoch: 8.4 sec]
EPOCH 613/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09477062625557037		[learning rate: 0.00065448]
		[batch 20/20] avg loss: 0.1064029664301069		[learning rate: 0.00065289]
	Learning Rate: 0.000652893
	LOSS [training: 0.10058679634283862 | validation: 0.08830102594330487]
	TIME [epoch: 8.41 sec]
EPOCH 614/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09660944624265778		[learning rate: 0.00065131]
		[batch 20/20] avg loss: 0.09825112269723123		[learning rate: 0.00064974]
	Learning Rate: 0.000649736
	LOSS [training: 0.09743028446994452 | validation: 0.1037909724045739]
	TIME [epoch: 8.41 sec]
EPOCH 615/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09632690785025451		[learning rate: 0.00064816]
		[batch 20/20] avg loss: 0.09782113531822292		[learning rate: 0.00064659]
	Learning Rate: 0.000646594
	LOSS [training: 0.09707402158423871 | validation: 0.1074646024413802]
	TIME [epoch: 8.42 sec]
EPOCH 616/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09624857833672097		[learning rate: 0.00064503]
		[batch 20/20] avg loss: 0.10196966807163158		[learning rate: 0.00064347]
	Learning Rate: 0.000643467
	LOSS [training: 0.09910912320417627 | validation: 0.09098585745250035]
	TIME [epoch: 8.42 sec]
EPOCH 617/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09255063268630286		[learning rate: 0.00064191]
		[batch 20/20] avg loss: 0.0945700366762233		[learning rate: 0.00064036]
	Learning Rate: 0.000640355
	LOSS [training: 0.09356033468126308 | validation: 0.10244570535073155]
	TIME [epoch: 8.4 sec]
EPOCH 618/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.13878196873314597		[learning rate: 0.00063881]
		[batch 20/20] avg loss: 0.10808169955232431		[learning rate: 0.00063726]
	Learning Rate: 0.000637259
	LOSS [training: 0.12343183414273513 | validation: 0.0739783554621232]
	TIME [epoch: 8.4 sec]
EPOCH 619/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.1006999023766495		[learning rate: 0.00063572]
		[batch 20/20] avg loss: 0.1019694655573578		[learning rate: 0.00063418]
	Learning Rate: 0.000634177
	LOSS [training: 0.10133468396700365 | validation: 0.0757865415250934]
	TIME [epoch: 8.44 sec]
EPOCH 620/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10212561946786021		[learning rate: 0.00063264]
		[batch 20/20] avg loss: 0.08559101417684051		[learning rate: 0.00063111]
	Learning Rate: 0.00063111
	LOSS [training: 0.09385831682235035 | validation: 0.06256575775924265]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl1_fix_noise/tr_study1/model_tr_study1_r1_20240218_115025/states/model_tr_study1_620.pth
	Model improved!!!
EPOCH 621/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09205357025764974		[learning rate: 0.00062958]
		[batch 20/20] avg loss: 0.09066037172254077		[learning rate: 0.00062806]
	Learning Rate: 0.000628058
	LOSS [training: 0.09135697099009527 | validation: 0.08221673269503017]
	TIME [epoch: 8.41 sec]
EPOCH 622/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10197523539132858		[learning rate: 0.00062654]
		[batch 20/20] avg loss: 0.0971012461603605		[learning rate: 0.00062502]
	Learning Rate: 0.000625021
	LOSS [training: 0.09953824077584454 | validation: 0.07926550468390617]
	TIME [epoch: 8.4 sec]
EPOCH 623/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09362961894948578		[learning rate: 0.00062351]
		[batch 20/20] avg loss: 0.10814031600001775		[learning rate: 0.000622]
	Learning Rate: 0.000621999
	LOSS [training: 0.10088496747475176 | validation: 0.0721436111016098]
	TIME [epoch: 8.41 sec]
EPOCH 624/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08584584931775344		[learning rate: 0.00062049]
		[batch 20/20] avg loss: 0.09084779965750239		[learning rate: 0.00061899]
	Learning Rate: 0.000618991
	LOSS [training: 0.0883468244876279 | validation: 0.07397584334415472]
	TIME [epoch: 8.44 sec]
EPOCH 625/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09321051791422551		[learning rate: 0.00061749]
		[batch 20/20] avg loss: 0.10262392467091838		[learning rate: 0.000616]
	Learning Rate: 0.000615997
	LOSS [training: 0.09791722129257195 | validation: 0.06999607687072322]
	TIME [epoch: 8.39 sec]
EPOCH 626/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09107093375809157		[learning rate: 0.00061451]
		[batch 20/20] avg loss: 0.0960298283257474		[learning rate: 0.00061302]
	Learning Rate: 0.000613019
	LOSS [training: 0.09355038104191948 | validation: 0.10073160853907398]
	TIME [epoch: 8.4 sec]
EPOCH 627/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10533742730312996		[learning rate: 0.00061153]
		[batch 20/20] avg loss: 0.10568836850266353		[learning rate: 0.00061005]
	Learning Rate: 0.000610054
	LOSS [training: 0.10551289790289672 | validation: 0.09041958570322936]
	TIME [epoch: 8.41 sec]
EPOCH 628/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.0949660272604777		[learning rate: 0.00060858]
		[batch 20/20] avg loss: 0.09163835904054227		[learning rate: 0.0006071]
	Learning Rate: 0.000607104
	LOSS [training: 0.09330219315050997 | validation: 0.07754365034525455]
	TIME [epoch: 8.42 sec]
EPOCH 629/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08251433186199596		[learning rate: 0.00060563]
		[batch 20/20] avg loss: 0.10338649737270526		[learning rate: 0.00060417]
	Learning Rate: 0.000604168
	LOSS [training: 0.09295041461735061 | validation: 0.07149540267698994]
	TIME [epoch: 8.41 sec]
EPOCH 630/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10204933941022368		[learning rate: 0.00060271]
		[batch 20/20] avg loss: 0.10064814417305187		[learning rate: 0.00060125]
	Learning Rate: 0.000601247
	LOSS [training: 0.10134874179163778 | validation: 0.06966109157507284]
	TIME [epoch: 8.4 sec]
EPOCH 631/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.10146156689962951		[learning rate: 0.00059979]
		[batch 20/20] avg loss: 0.09631498192450542		[learning rate: 0.00059834]
	Learning Rate: 0.000598339
	LOSS [training: 0.09888827441206746 | validation: 0.07081017125415928]
	TIME [epoch: 8.4 sec]
EPOCH 632/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.08801781179246157		[learning rate: 0.00059689]
		[batch 20/20] avg loss: 0.09868759935094969		[learning rate: 0.00059545]
	Learning Rate: 0.000595446
	LOSS [training: 0.09335270557170561 | validation: 0.10101862074528112]
	TIME [epoch: 8.44 sec]
EPOCH 633/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.11004729168388909		[learning rate: 0.000594]
		[batch 20/20] avg loss: 0.0980318892318777		[learning rate: 0.00059257]
	Learning Rate: 0.000592566
	LOSS [training: 0.10403959045788339 | validation: 0.07928876720800246]
	TIME [epoch: 8.4 sec]
EPOCH 634/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.12188166009904211		[learning rate: 0.00059113]
		[batch 20/20] avg loss: 0.09764731384541689		[learning rate: 0.0005897]
	Learning Rate: 0.000589701
	LOSS [training: 0.10976448697222951 | validation: 0.06871746631123564]
	TIME [epoch: 8.42 sec]
EPOCH 635/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09050669276961854		[learning rate: 0.00058827]
		[batch 20/20] avg loss: 0.08858100687971561		[learning rate: 0.00058685]
	Learning Rate: 0.000586849
	LOSS [training: 0.08954384982466708 | validation: 0.08545969736559371]
	TIME [epoch: 8.4 sec]
EPOCH 636/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09179721236520826		[learning rate: 0.00058543]
		[batch 20/20] avg loss: 0.10463808025237722		[learning rate: 0.00058401]
	Learning Rate: 0.000584011
	LOSS [training: 0.09821764630879275 | validation: 0.07069873909616917]
	TIME [epoch: 8.42 sec]
EPOCH 637/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.09237227142529554		[learning rate: 0.0005826]
		[batch 20/20] avg loss: 0.12697728661575314		[learning rate: 0.00058119]
	Learning Rate: 0.000581187
	LOSS [training: 0.10967477902052432 | validation: 0.1511555428024049]
	TIME [epoch: 8.43 sec]
EPOCH 638/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.46665115118518957		[learning rate: 0.00057978]
		[batch 20/20] avg loss: 0.5924392892983694		[learning rate: 0.00057838]
	Learning Rate: 0.000578376
	LOSS [training: 0.5295452202417794 | validation: 0.46505685141903486]
	TIME [epoch: 8.39 sec]
EPOCH 639/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7287113571325156		[learning rate: 0.00057698]
		[batch 20/20] avg loss: 0.7371899789648714		[learning rate: 0.00057558]
	Learning Rate: 0.000575579
	LOSS [training: 0.7329506680486937 | validation: 0.49352460369913576]
	TIME [epoch: 8.41 sec]
EPOCH 640/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8309054515351301		[learning rate: 0.00057419]
		[batch 20/20] avg loss: 1.0932842689799513		[learning rate: 0.0005728]
	Learning Rate: 0.000572796
	LOSS [training: 0.9620948602575405 | validation: 1.019964673914266]
	TIME [epoch: 8.41 sec]
EPOCH 641/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2247972180674416		[learning rate: 0.00057141]
		[batch 20/20] avg loss: 1.2940506515177612		[learning rate: 0.00057003]
	Learning Rate: 0.000570026
	LOSS [training: 1.2594239347926013 | validation: 1.206273188030583]
	TIME [epoch: 8.43 sec]
EPOCH 642/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4439585583246477		[learning rate: 0.00056865]
		[batch 20/20] avg loss: 1.6532035143137063		[learning rate: 0.00056727]
	Learning Rate: 0.00056727
	LOSS [training: 1.548581036319177 | validation: 1.4602296211266728]
	TIME [epoch: 8.42 sec]
EPOCH 643/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7178480824826021		[learning rate: 0.0005659]
		[batch 20/20] avg loss: 2.5581850816779452		[learning rate: 0.00056453]
	Learning Rate: 0.000564526
	LOSS [training: 2.1380165820802732 | validation: 2.80394478237322]
	TIME [epoch: 8.4 sec]
EPOCH 644/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.0159925820301856		[learning rate: 0.00056316]
		[batch 20/20] avg loss: 3.184613444756737		[learning rate: 0.0005618]
	Learning Rate: 0.000561796
	LOSS [training: 3.1003030133934617 | validation: 2.9025664800459112]
	TIME [epoch: 8.4 sec]
EPOCH 645/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.9091069089621473		[learning rate: 0.00056044]
		[batch 20/20] avg loss: 3.823450890824602		[learning rate: 0.00055908]
	Learning Rate: 0.00055908
	LOSS [training: 3.3662788998933757 | validation: 3.801814404945599]
	TIME [epoch: 8.44 sec]
EPOCH 646/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.106187063031916		[learning rate: 0.00055773]
		[batch 20/20] avg loss: 4.284996412766663		[learning rate: 0.00055638]
	Learning Rate: 0.000556376
	LOSS [training: 4.19559173789929 | validation: 3.893775384752529]
	TIME [epoch: 8.4 sec]
EPOCH 647/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.045926903837092		[learning rate: 0.00055503]
		[batch 20/20] avg loss: 4.015121213218431		[learning rate: 0.00055369]
	Learning Rate: 0.000553685
	LOSS [training: 4.030524058527761 | validation: 4.123841750178154]
	TIME [epoch: 8.4 sec]
EPOCH 648/1000:
	Training over batches...
		[batch 10/20] avg loss: 4.072981946803997		[learning rate: 0.00055235]
		[batch 20/20] avg loss: 3.454763933439439		[learning rate: 0.00055101]
	Learning Rate: 0.000551008
	LOSS [training: 3.7638729401217184 | validation: 3.170153944429836]
	TIME [epoch: 8.42 sec]
EPOCH 649/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.9615457683367885		[learning rate: 0.00054967]
		[batch 20/20] avg loss: 2.3709766936879904		[learning rate: 0.00054834]
	Learning Rate: 0.000548343
	LOSS [training: 2.666261231012389 | validation: 2.3819751669609177]
	TIME [epoch: 8.41 sec]
EPOCH 650/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6854243165374663		[learning rate: 0.00054702]
		[batch 20/20] avg loss: 2.71924761865995		[learning rate: 0.00054569]
	Learning Rate: 0.000545692
	LOSS [training: 2.702335967598708 | validation: 2.4270597855759233]
	TIME [epoch: 8.41 sec]
EPOCH 651/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6585100286461736		[learning rate: 0.00054437]
		[batch 20/20] avg loss: 2.0028878842092057		[learning rate: 0.00054305]
	Learning Rate: 0.000543053
	LOSS [training: 2.33069895642769 | validation: 1.809047508045978]
	TIME [epoch: 8.42 sec]
EPOCH 652/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.000191850094514		[learning rate: 0.00054174]
		[batch 20/20] avg loss: 2.095161495322461		[learning rate: 0.00054043]
	Learning Rate: 0.000540427
	LOSS [training: 2.0476766727084867 | validation: 1.94207961265717]
	TIME [epoch: 8.39 sec]
EPOCH 653/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.154134982157857		[learning rate: 0.00053912]
		[batch 20/20] avg loss: 2.1599311146866813		[learning rate: 0.00053781]
	Learning Rate: 0.000537813
	LOSS [training: 2.157033048422269 | validation: 1.7237603480888233]
	TIME [epoch: 8.4 sec]
EPOCH 654/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8750974735248942		[learning rate: 0.00053651]
		[batch 20/20] avg loss: 1.9191391702051732		[learning rate: 0.00053521]
	Learning Rate: 0.000535213
	LOSS [training: 1.8971183218650336 | validation: 1.9053072921300749]
	TIME [epoch: 8.44 sec]
EPOCH 655/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1039914396600765		[learning rate: 0.00053392]
		[batch 20/20] avg loss: 2.2488440318282		[learning rate: 0.00053262]
	Learning Rate: 0.000532624
	LOSS [training: 2.1764177357441374 | validation: 2.353141997910479]
	TIME [epoch: 8.39 sec]
EPOCH 656/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.8014359482594178		[learning rate: 0.00053134]
		[batch 20/20] avg loss: 2.744327133331562		[learning rate: 0.00053005]
	Learning Rate: 0.000530049
	LOSS [training: 2.772881540795489 | validation: 2.6909230629322862]
	TIME [epoch: 8.4 sec]
EPOCH 657/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.16543768953174		[learning rate: 0.00052877]
		[batch 20/20] avg loss: 3.4798447455489283		[learning rate: 0.00052749]
	Learning Rate: 0.000527485
	LOSS [training: 3.3226412175403333 | validation: 3.5171048095882753]
	TIME [epoch: 8.41 sec]
EPOCH 658/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.647440765191193		[learning rate: 0.00052621]
		[batch 20/20] avg loss: 3.7917796915182804		[learning rate: 0.00052493]
	Learning Rate: 0.000524935
	LOSS [training: 3.7196102283547363 | validation: 3.3725732956768644]
	TIME [epoch: 8.42 sec]
EPOCH 659/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.8644095895368205		[learning rate: 0.00052366]
		[batch 20/20] avg loss: 3.562648169804824		[learning rate: 0.0005224]
	Learning Rate: 0.000522396
	LOSS [training: 3.7135288796708217 | validation: 2.8740097276540357]
	TIME [epoch: 8.4 sec]
EPOCH 660/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.9821563081782494		[learning rate: 0.00052113]
		[batch 20/20] avg loss: 3.2966488362763955		[learning rate: 0.00051987]
	Learning Rate: 0.00051987
	LOSS [training: 3.139402572227322 | validation: 3.0265092019226962]
	TIME [epoch: 8.41 sec]
EPOCH 661/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.1690633140106526		[learning rate: 0.00051861]
		[batch 20/20] avg loss: 3.1816069514181735		[learning rate: 0.00051736]
	Learning Rate: 0.000517356
	LOSS [training: 3.175335132714413 | validation: 2.714204771935645]
	TIME [epoch: 8.4 sec]
EPOCH 662/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.1961058258127704		[learning rate: 0.0005161]
		[batch 20/20] avg loss: 3.1790625719757086		[learning rate: 0.00051485]
	Learning Rate: 0.000514854
	LOSS [training: 3.1875841988942395 | validation: 3.083690777873833]
	TIME [epoch: 8.41 sec]
EPOCH 663/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.124851025225575		[learning rate: 0.00051361]
		[batch 20/20] avg loss: 3.1941328559686495		[learning rate: 0.00051236]
	Learning Rate: 0.000512364
	LOSS [training: 3.1594919405971127 | validation: 2.594140798563914]
	TIME [epoch: 8.43 sec]
EPOCH 664/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.726149877946083		[learning rate: 0.00051112]
		[batch 20/20] avg loss: 3.082681344084226		[learning rate: 0.00050989]
	Learning Rate: 0.000509887
	LOSS [training: 2.904415611015154 | validation: 3.2881178003199962]
	TIME [epoch: 8.41 sec]
EPOCH 665/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.124306311859384		[learning rate: 0.00050865]
		[batch 20/20] avg loss: 3.432311686532755		[learning rate: 0.00050742]
	Learning Rate: 0.000507421
	LOSS [training: 3.27830899919607 | validation: 3.011361112492334]
	TIME [epoch: 8.39 sec]
EPOCH 666/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.101399544014874		[learning rate: 0.00050619]
		[batch 20/20] avg loss: 2.5820044855278814		[learning rate: 0.00050497]
	Learning Rate: 0.000504967
	LOSS [training: 2.8417020147713776 | validation: 2.4038586208616053]
	TIME [epoch: 8.41 sec]
EPOCH 667/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.735204397646713		[learning rate: 0.00050374]
		[batch 20/20] avg loss: 2.7886266512379874		[learning rate: 0.00050253]
	Learning Rate: 0.000502525
	LOSS [training: 2.76191552444235 | validation: 2.6596848527352392]
	TIME [epoch: 8.44 sec]
EPOCH 668/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.694387064735058		[learning rate: 0.00050131]
		[batch 20/20] avg loss: 2.38455600012768		[learning rate: 0.0005001]
	Learning Rate: 0.000500095
	LOSS [training: 2.5394715324313695 | validation: 1.986548133195016]
	TIME [epoch: 8.39 sec]
EPOCH 669/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3159116663012367		[learning rate: 0.00049888]
		[batch 20/20] avg loss: 2.0748421856964123		[learning rate: 0.00049768]
	Learning Rate: 0.000497677
	LOSS [training: 2.1953769259988247 | validation: 1.7716855656576358]
	TIME [epoch: 8.4 sec]
EPOCH 670/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.173682415327979		[learning rate: 0.00049647]
		[batch 20/20] avg loss: 2.4281340461253507		[learning rate: 0.00049527]
	Learning Rate: 0.00049527
	LOSS [training: 2.3009082307266646 | validation: 2.3412855968841333]
	TIME [epoch: 8.41 sec]
EPOCH 671/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.642921250488506		[learning rate: 0.00049407]
		[batch 20/20] avg loss: 3.197588826851187		[learning rate: 0.00049288]
	Learning Rate: 0.000492875
	LOSS [training: 2.920255038669846 | validation: 2.9081300753243866]
	TIME [epoch: 8.41 sec]
EPOCH 672/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.9784470759533344		[learning rate: 0.00049168]
		[batch 20/20] avg loss: 2.6682207028085125		[learning rate: 0.00049049]
	Learning Rate: 0.000490492
	LOSS [training: 2.8233338893809234 | validation: 1.952011313951639]
	TIME [epoch: 8.4 sec]
EPOCH 673/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1540281962849503		[learning rate: 0.0004893]
		[batch 20/20] avg loss: 1.887067449915452		[learning rate: 0.00048812]
	Learning Rate: 0.00048812
	LOSS [training: 2.020547823100201 | validation: 1.4683805102626002]
	TIME [epoch: 8.41 sec]
EPOCH 674/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.579550166466813		[learning rate: 0.00048694]
		[batch 20/20] avg loss: 1.4443208749819756		[learning rate: 0.00048576]
	Learning Rate: 0.000485759
	LOSS [training: 1.5119355207243943 | validation: 1.2231821205567102]
	TIME [epoch: 8.41 sec]
EPOCH 675/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3276839720629694		[learning rate: 0.00048458]
		[batch 20/20] avg loss: 1.234312277014364		[learning rate: 0.00048341]
	Learning Rate: 0.00048341
	LOSS [training: 1.2809981245386666 | validation: 1.2735004255565732]
	TIME [epoch: 8.41 sec]
EPOCH 676/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3223380426989186		[learning rate: 0.00048224]
		[batch 20/20] avg loss: 1.5727937919442339		[learning rate: 0.00048107]
	Learning Rate: 0.000481072
	LOSS [training: 1.447565917321576 | validation: 1.440045441656078]
	TIME [epoch: 8.41 sec]
EPOCH 677/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5007382079191276		[learning rate: 0.00047991]
		[batch 20/20] avg loss: 1.52853675834397		[learning rate: 0.00047875]
	Learning Rate: 0.000478746
	LOSS [training: 1.5146374831315488 | validation: 1.4682625305584809]
	TIME [epoch: 8.41 sec]
EPOCH 678/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7110256375355655		[learning rate: 0.00047759]
		[batch 20/20] avg loss: 1.721462702151196		[learning rate: 0.00047643]
	Learning Rate: 0.000476431
	LOSS [training: 1.7162441698433806 | validation: 1.6803145925637246]
	TIME [epoch: 8.4 sec]
EPOCH 679/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7279817170391358		[learning rate: 0.00047528]
		[batch 20/20] avg loss: 1.8321746981218048		[learning rate: 0.00047413]
	Learning Rate: 0.000474127
	LOSS [training: 1.7800782075804709 | validation: 1.732722963027769]
	TIME [epoch: 8.4 sec]
EPOCH 680/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8388959808784051		[learning rate: 0.00047298]
		[batch 20/20] avg loss: 1.7285673213007957		[learning rate: 0.00047183]
	Learning Rate: 0.000471834
	LOSS [training: 1.7837316510896002 | validation: 1.3892939741113293]
	TIME [epoch: 8.42 sec]
EPOCH 681/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.587573228149572		[learning rate: 0.00047069]
		[batch 20/20] avg loss: 1.7534647793051874		[learning rate: 0.00046955]
	Learning Rate: 0.000469553
	LOSS [training: 1.6705190037273803 | validation: 1.4579607449725716]
	TIME [epoch: 8.41 sec]
EPOCH 682/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7314099210074976		[learning rate: 0.00046842]
		[batch 20/20] avg loss: 1.893245638608454		[learning rate: 0.00046728]
	Learning Rate: 0.000467282
	LOSS [training: 1.8123277798079755 | validation: 1.6342137354578636]
	TIME [epoch: 8.41 sec]
EPOCH 683/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8469037889537698		[learning rate: 0.00046615]
		[batch 20/20] avg loss: 1.7360071717144987		[learning rate: 0.00046502]
	Learning Rate: 0.000465022
	LOSS [training: 1.7914554803341343 | validation: 1.4713127030359625]
	TIME [epoch: 8.39 sec]
EPOCH 684/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.789060016848276		[learning rate: 0.0004639]
		[batch 20/20] avg loss: 2.353150413755709		[learning rate: 0.00046277]
	Learning Rate: 0.000462773
	LOSS [training: 2.0711052153019924 | validation: 2.1247108247765945]
	TIME [epoch: 8.43 sec]
EPOCH 685/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.431646816035987		[learning rate: 0.00046165]
		[batch 20/20] avg loss: 2.438375720287678		[learning rate: 0.00046054]
	Learning Rate: 0.000460536
	LOSS [training: 2.4350112681618326 | validation: 2.090449869810982]
	TIME [epoch: 8.41 sec]
EPOCH 686/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.0872831304228177		[learning rate: 0.00045942]
		[batch 20/20] avg loss: 1.6646111149734029		[learning rate: 0.00045831]
	Learning Rate: 0.000458309
	LOSS [training: 1.8759471226981101 | validation: 1.4731102073502016]
	TIME [epoch: 8.41 sec]
EPOCH 687/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.493392582481903		[learning rate: 0.0004572]
		[batch 20/20] avg loss: 1.4845140185642616		[learning rate: 0.00045609]
	Learning Rate: 0.000456092
	LOSS [training: 1.4889533005230824 | validation: 1.3267699720429995]
	TIME [epoch: 8.39 sec]
EPOCH 688/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4818410547080492		[learning rate: 0.00045499]
		[batch 20/20] avg loss: 1.6618981144701799		[learning rate: 0.00045389]
	Learning Rate: 0.000453887
	LOSS [training: 1.5718695845891144 | validation: 1.8037183223534483]
	TIME [epoch: 8.42 sec]
EPOCH 689/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.497818539831608		[learning rate: 0.00045279]
		[batch 20/20] avg loss: 3.025974454543864		[learning rate: 0.00045169]
	Learning Rate: 0.000451692
	LOSS [training: 2.761896497187736 | validation: 3.03414937732303]
	TIME [epoch: 8.41 sec]
EPOCH 690/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.193293765930269		[learning rate: 0.0004506]
		[batch 20/20] avg loss: 3.098676645477156		[learning rate: 0.00044951]
	Learning Rate: 0.000449507
	LOSS [training: 3.1459852057037123 | validation: 2.783501823521642]
	TIME [epoch: 8.42 sec]
EPOCH 691/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.0509949178477953		[learning rate: 0.00044842]
		[batch 20/20] avg loss: 3.1762715888777153		[learning rate: 0.00044733]
	Learning Rate: 0.000447334
	LOSS [training: 3.1136332533627553 | validation: 3.4983192497358564]
	TIME [epoch: 8.39 sec]
EPOCH 692/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.7497601403395664		[learning rate: 0.00044625]
		[batch 20/20] avg loss: 3.4057090695606247		[learning rate: 0.00044517]
	Learning Rate: 0.00044517
	LOSS [training: 3.577734604950096 | validation: 3.066650006495032]
	TIME [epoch: 8.4 sec]
EPOCH 693/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.2595571269233354		[learning rate: 0.00044409]
		[batch 20/20] avg loss: 3.1362406365978184		[learning rate: 0.00044302]
	Learning Rate: 0.000443018
	LOSS [training: 3.197898881760577 | validation: 2.943071895270745]
	TIME [epoch: 8.42 sec]
EPOCH 694/1000:
	Training over batches...
		[batch 10/20] avg loss: 3.0905053813064196		[learning rate: 0.00044195]
		[batch 20/20] avg loss: 2.8796574449777266		[learning rate: 0.00044088]
	Learning Rate: 0.000440875
	LOSS [training: 2.9850814131420735 | validation: 2.893232500346001]
	TIME [epoch: 8.41 sec]
EPOCH 695/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.8368865738049385		[learning rate: 0.00043981]
		[batch 20/20] avg loss: 2.6422092035031093		[learning rate: 0.00043874]
	Learning Rate: 0.000438743
	LOSS [training: 2.7395478886540237 | validation: 2.296960570199288]
	TIME [epoch: 8.39 sec]
EPOCH 696/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.666309436390997		[learning rate: 0.00043768]
		[batch 20/20] avg loss: 2.3529275980049347		[learning rate: 0.00043662]
	Learning Rate: 0.000436622
	LOSS [training: 2.5096185171979655 | validation: 2.174661706456794]
	TIME [epoch: 8.39 sec]
EPOCH 697/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.374880805855266		[learning rate: 0.00043556]
		[batch 20/20] avg loss: 2.158395241773777		[learning rate: 0.00043451]
	Learning Rate: 0.00043451
	LOSS [training: 2.2666380238145214 | validation: 1.7676261204698462]
	TIME [epoch: 8.43 sec]
EPOCH 698/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9710913946382962		[learning rate: 0.00043346]
		[batch 20/20] avg loss: 1.841360414540016		[learning rate: 0.00043241]
	Learning Rate: 0.000432409
	LOSS [training: 1.9062259045891559 | validation: 1.2657899607209724]
	TIME [epoch: 8.42 sec]
EPOCH 699/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.21959509619102		[learning rate: 0.00043136]
		[batch 20/20] avg loss: 0.9744410711245589		[learning rate: 0.00043032]
	Learning Rate: 0.000430318
	LOSS [training: 1.0970180836577896 | validation: 0.8746816468033406]
	TIME [epoch: 8.41 sec]
EPOCH 700/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9768504154222389		[learning rate: 0.00042928]
		[batch 20/20] avg loss: 1.2149752766773068		[learning rate: 0.00042824]
	Learning Rate: 0.000428237
	LOSS [training: 1.095912846049773 | validation: 1.0729793962294356]
	TIME [epoch: 8.39 sec]
EPOCH 701/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1707729650470307		[learning rate: 0.0004272]
		[batch 20/20] avg loss: 1.302219490469756		[learning rate: 0.00042617]
	Learning Rate: 0.000426166
	LOSS [training: 1.2364962277583933 | validation: 1.17838078715462]
	TIME [epoch: 8.42 sec]
EPOCH 702/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2954061719886782		[learning rate: 0.00042513]
		[batch 20/20] avg loss: 1.1786256327297189		[learning rate: 0.00042411]
	Learning Rate: 0.000424105
	LOSS [training: 1.237015902359199 | validation: 1.2178736377554378]
	TIME [epoch: 8.43 sec]
EPOCH 703/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.428149139837566		[learning rate: 0.00042308]
		[batch 20/20] avg loss: 1.371930261318651		[learning rate: 0.00042205]
	Learning Rate: 0.000422054
	LOSS [training: 1.4000397005781084 | validation: 1.0041554678585638]
	TIME [epoch: 8.42 sec]
EPOCH 704/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0324475358543794		[learning rate: 0.00042103]
		[batch 20/20] avg loss: 1.1400151069835929		[learning rate: 0.00042001]
	Learning Rate: 0.000420013
	LOSS [training: 1.086231321418986 | validation: 1.2121723762627474]
	TIME [epoch: 8.42 sec]
EPOCH 705/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5484372281792755		[learning rate: 0.000419]
		[batch 20/20] avg loss: 1.9873638687672053		[learning rate: 0.00041798]
	Learning Rate: 0.000417982
	LOSS [training: 1.7679005484732404 | validation: 1.787959348364445]
	TIME [epoch: 8.44 sec]
EPOCH 706/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.2262120062755417		[learning rate: 0.00041697]
		[batch 20/20] avg loss: 1.858592550389636		[learning rate: 0.00041596]
	Learning Rate: 0.000415961
	LOSS [training: 2.042402278332589 | validation: 1.324240281793153]
	TIME [epoch: 8.43 sec]
EPOCH 707/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.180498042280417		[learning rate: 0.00041495]
		[batch 20/20] avg loss: 1.1399711848254728		[learning rate: 0.00041395]
	Learning Rate: 0.00041395
	LOSS [training: 1.1602346135529449 | validation: 0.996488355825069]
	TIME [epoch: 8.42 sec]
EPOCH 708/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0204767965413655		[learning rate: 0.00041295]
		[batch 20/20] avg loss: 0.846960136230875		[learning rate: 0.00041195]
	Learning Rate: 0.000411948
	LOSS [training: 0.9337184663861203 | validation: 0.730545048022124]
	TIME [epoch: 8.41 sec]
EPOCH 709/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1267119806622297		[learning rate: 0.00041095]
		[batch 20/20] avg loss: 1.0432999354773547		[learning rate: 0.00040996]
	Learning Rate: 0.000409956
	LOSS [training: 1.085005958069792 | validation: 0.7623062922905052]
	TIME [epoch: 8.39 sec]
EPOCH 710/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8551104269195114		[learning rate: 0.00040896]
		[batch 20/20] avg loss: 0.9641218047610917		[learning rate: 0.00040797]
	Learning Rate: 0.000407973
	LOSS [training: 0.9096161158403018 | validation: 0.9804449953321331]
	TIME [epoch: 8.42 sec]
EPOCH 711/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1701396355844198		[learning rate: 0.00040699]
		[batch 20/20] avg loss: 1.3478067327600778		[learning rate: 0.000406]
	Learning Rate: 0.000406
	LOSS [training: 1.258973184172249 | validation: 1.1973834997424389]
	TIME [epoch: 8.4 sec]
EPOCH 712/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3597614938150377		[learning rate: 0.00040502]
		[batch 20/20] avg loss: 1.0466182627795537		[learning rate: 0.00040404]
	Learning Rate: 0.000404037
	LOSS [training: 1.2031898782972958 | validation: 0.7995356211602999]
	TIME [epoch: 8.41 sec]
EPOCH 713/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9424638208730591		[learning rate: 0.00040306]
		[batch 20/20] avg loss: 0.8216181411329171		[learning rate: 0.00040208]
	Learning Rate: 0.000402083
	LOSS [training: 0.8820409810029881 | validation: 0.960141974611781]
	TIME [epoch: 8.4 sec]
EPOCH 714/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1050774579896472		[learning rate: 0.00040111]
		[batch 20/20] avg loss: 1.2826636531020084		[learning rate: 0.00040014]
	Learning Rate: 0.000400139
	LOSS [training: 1.1938705555458278 | validation: 1.165723101473125]
	TIME [epoch: 8.42 sec]
EPOCH 715/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.08393681923276		[learning rate: 0.00039917]
		[batch 20/20] avg loss: 0.9706966218893045		[learning rate: 0.0003982]
	Learning Rate: 0.000398204
	LOSS [training: 1.0273167205610325 | validation: 0.8524246824054313]
	TIME [epoch: 8.41 sec]
EPOCH 716/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.084241627235003		[learning rate: 0.00039724]
		[batch 20/20] avg loss: 1.270724093619614		[learning rate: 0.00039628]
	Learning Rate: 0.000396278
	LOSS [training: 1.1774828604273084 | validation: 1.0988393388924294]
	TIME [epoch: 8.41 sec]
EPOCH 717/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2695431337334155		[learning rate: 0.00039532]
		[batch 20/20] avg loss: 0.9311663190873339		[learning rate: 0.00039436]
	Learning Rate: 0.000394362
	LOSS [training: 1.100354726410375 | validation: 1.0121312057355605]
	TIME [epoch: 8.4 sec]
EPOCH 718/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2545435869937243		[learning rate: 0.00039341]
		[batch 20/20] avg loss: 1.5058424447764525		[learning rate: 0.00039245]
	Learning Rate: 0.000392455
	LOSS [training: 1.3801930158850884 | validation: 1.3009568658158213]
	TIME [epoch: 8.41 sec]
EPOCH 719/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3043073954173632		[learning rate: 0.0003915]
		[batch 20/20] avg loss: 1.1279863889328157		[learning rate: 0.00039056]
	Learning Rate: 0.000390557
	LOSS [training: 1.2161468921750893 | validation: 1.1069080830657343]
	TIME [epoch: 8.43 sec]
EPOCH 720/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1742991529720972		[learning rate: 0.00038961]
		[batch 20/20] avg loss: 1.0361046413002297		[learning rate: 0.00038867]
	Learning Rate: 0.000388668
	LOSS [training: 1.1052018971361632 | validation: 0.8827018073001984]
	TIME [epoch: 8.41 sec]
EPOCH 721/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1191982937092055		[learning rate: 0.00038773]
		[batch 20/20] avg loss: 0.9042421067689226		[learning rate: 0.00038679]
	Learning Rate: 0.000386789
	LOSS [training: 1.011720200239064 | validation: 0.782775269739377]
	TIME [epoch: 8.4 sec]
EPOCH 722/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8505007536623376		[learning rate: 0.00038585]
		[batch 20/20] avg loss: 0.7626803434266677		[learning rate: 0.00038492]
	Learning Rate: 0.000384918
	LOSS [training: 0.8065905485445027 | validation: 0.721811978374435]
	TIME [epoch: 8.41 sec]
EPOCH 723/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8347365573473763		[learning rate: 0.00038399]
		[batch 20/20] avg loss: 0.863161836701237		[learning rate: 0.00038306]
	Learning Rate: 0.000383057
	LOSS [training: 0.8489491970243067 | validation: 0.7257388483353975]
	TIME [epoch: 8.44 sec]
EPOCH 724/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8629932908708791		[learning rate: 0.00038213]
		[batch 20/20] avg loss: 1.0360738731163632		[learning rate: 0.0003812]
	Learning Rate: 0.000381204
	LOSS [training: 0.949533581993621 | validation: 0.8559329659478264]
	TIME [epoch: 8.39 sec]
EPOCH 725/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8790906287458256		[learning rate: 0.00038028]
		[batch 20/20] avg loss: 1.2389270911739172		[learning rate: 0.00037936]
	Learning Rate: 0.000379361
	LOSS [training: 1.0590088599598715 | validation: 1.141161666892931]
	TIME [epoch: 8.41 sec]
EPOCH 726/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.069108314600752		[learning rate: 0.00037844]
		[batch 20/20] avg loss: 0.9613360814954625		[learning rate: 0.00037753]
	Learning Rate: 0.000377526
	LOSS [training: 1.015222198048107 | validation: 0.8824967418107686]
	TIME [epoch: 8.41 sec]
EPOCH 727/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.93870954395252		[learning rate: 0.00037661]
		[batch 20/20] avg loss: 0.968392757806156		[learning rate: 0.0003757]
	Learning Rate: 0.000375701
	LOSS [training: 0.9535511508793381 | validation: 0.6786994549768173]
	TIME [epoch: 8.42 sec]
EPOCH 728/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8142407854855813		[learning rate: 0.00037479]
		[batch 20/20] avg loss: 0.9911366270315529		[learning rate: 0.00037388]
	Learning Rate: 0.000373884
	LOSS [training: 0.9026887062585672 | validation: 1.13352097412825]
	TIME [epoch: 8.4 sec]
EPOCH 729/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1812699533315203		[learning rate: 0.00037298]
		[batch 20/20] avg loss: 1.387759898869923		[learning rate: 0.00037208]
	Learning Rate: 0.000372076
	LOSS [training: 1.2845149261007216 | validation: 1.2577736593716888]
	TIME [epoch: 8.42 sec]
EPOCH 730/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2586127451224427		[learning rate: 0.00037118]
		[batch 20/20] avg loss: 0.8645795687762684		[learning rate: 0.00037028]
	Learning Rate: 0.000370277
	LOSS [training: 1.0615961569493557 | validation: 0.8464767840166216]
	TIME [epoch: 8.39 sec]
EPOCH 731/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8769423796740241		[learning rate: 0.00036938]
		[batch 20/20] avg loss: 0.8624383962923913		[learning rate: 0.00036849]
	Learning Rate: 0.000368486
	LOSS [training: 0.8696903879832079 | validation: 0.7511905469503015]
	TIME [epoch: 8.42 sec]
EPOCH 732/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8665733787724845		[learning rate: 0.00036759]
		[batch 20/20] avg loss: 0.7678430585836447		[learning rate: 0.0003667]
	Learning Rate: 0.000366704
	LOSS [training: 0.8172082186780646 | validation: 0.6855124167785533]
	TIME [epoch: 8.43 sec]
EPOCH 733/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7581975021164474		[learning rate: 0.00036582]
		[batch 20/20] avg loss: 0.9211148788861537		[learning rate: 0.00036493]
	Learning Rate: 0.000364931
	LOSS [training: 0.8396561905013007 | validation: 0.9383120600592239]
	TIME [epoch: 8.4 sec]
EPOCH 734/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9531495479837016		[learning rate: 0.00036405]
		[batch 20/20] avg loss: 0.6421575360528167		[learning rate: 0.00036317]
	Learning Rate: 0.000363166
	LOSS [training: 0.7976535420182591 | validation: 0.48358295532483847]
	TIME [epoch: 8.39 sec]
EPOCH 735/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5346828067056065		[learning rate: 0.00036229]
		[batch 20/20] avg loss: 0.5552451438083301		[learning rate: 0.00036141]
	Learning Rate: 0.00036141
	LOSS [training: 0.5449639752569684 | validation: 0.4906501493924089]
	TIME [epoch: 8.41 sec]
EPOCH 736/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5304545875354817		[learning rate: 0.00036053]
		[batch 20/20] avg loss: 0.6498883385914052		[learning rate: 0.00035966]
	Learning Rate: 0.000359662
	LOSS [training: 0.5901714630634433 | validation: 0.47433929588833684]
	TIME [epoch: 8.43 sec]
EPOCH 737/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6163286723659618		[learning rate: 0.00035879]
		[batch 20/20] avg loss: 0.5963192664725396		[learning rate: 0.00035792]
	Learning Rate: 0.000357923
	LOSS [training: 0.6063239694192508 | validation: 0.42339857774760814]
	TIME [epoch: 8.39 sec]
EPOCH 738/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5076740088141797		[learning rate: 0.00035706]
		[batch 20/20] avg loss: 0.5898753141832606		[learning rate: 0.00035619]
	Learning Rate: 0.000356192
	LOSS [training: 0.5487746614987201 | validation: 0.4693797623315554]
	TIME [epoch: 8.4 sec]
EPOCH 739/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5175916681751345		[learning rate: 0.00035533]
		[batch 20/20] avg loss: 0.8626080199744077		[learning rate: 0.00035447]
	Learning Rate: 0.00035447
	LOSS [training: 0.6900998440747711 | validation: 0.902386796466562]
	TIME [epoch: 8.41 sec]
EPOCH 740/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0251746943251634		[learning rate: 0.00035361]
		[batch 20/20] avg loss: 1.4751534557968464		[learning rate: 0.00035276]
	Learning Rate: 0.000352755
	LOSS [training: 1.250164075061005 | validation: 1.5866589870596663]
	TIME [epoch: 8.42 sec]
EPOCH 741/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8080569517865406		[learning rate: 0.0003519]
		[batch 20/20] avg loss: 1.7036067218191964		[learning rate: 0.00035105]
	Learning Rate: 0.00035105
	LOSS [training: 1.7558318368028687 | validation: 1.6481900728479826]
	TIME [epoch: 8.41 sec]
EPOCH 742/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.55864755604332		[learning rate: 0.0003502]
		[batch 20/20] avg loss: 1.3471629636344238		[learning rate: 0.00034935]
	Learning Rate: 0.000349352
	LOSS [training: 1.4529052598388719 | validation: 1.3364867342032796]
	TIME [epoch: 8.41 sec]
EPOCH 743/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5402200215222308		[learning rate: 0.00034851]
		[batch 20/20] avg loss: 1.5860494069184876		[learning rate: 0.00034766]
	Learning Rate: 0.000347663
	LOSS [training: 1.563134714220359 | validation: 1.3688013948521012]
	TIME [epoch: 8.39 sec]
EPOCH 744/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4077468765098518		[learning rate: 0.00034682]
		[batch 20/20] avg loss: 1.354713442702637		[learning rate: 0.00034598]
	Learning Rate: 0.000345981
	LOSS [training: 1.3812301596062444 | validation: 1.2544050284881245]
	TIME [epoch: 8.44 sec]
EPOCH 745/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4190034834993588		[learning rate: 0.00034514]
		[batch 20/20] avg loss: 1.740150938424416		[learning rate: 0.00034431]
	Learning Rate: 0.000344308
	LOSS [training: 1.5795772109618875 | validation: 1.5853990400084164]
	TIME [epoch: 8.41 sec]
EPOCH 746/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7035865621456137		[learning rate: 0.00034347]
		[batch 20/20] avg loss: 1.8697985287360546		[learning rate: 0.00034264]
	Learning Rate: 0.000342643
	LOSS [training: 1.7866925454408338 | validation: 1.7158540971201206]
	TIME [epoch: 8.39 sec]
EPOCH 747/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8719751369024895		[learning rate: 0.00034181]
		[batch 20/20] avg loss: 1.7918101001200732		[learning rate: 0.00034099]
	Learning Rate: 0.000340986
	LOSS [training: 1.8318926185112812 | validation: 1.6826098730544137]
	TIME [epoch: 8.41 sec]
EPOCH 748/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9125208332049533		[learning rate: 0.00034016]
		[batch 20/20] avg loss: 1.7428678689297925		[learning rate: 0.00033934]
	Learning Rate: 0.000339337
	LOSS [training: 1.8276943510673729 | validation: 1.5795632483703304]
	TIME [epoch: 8.42 sec]
EPOCH 749/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7180152452349968		[learning rate: 0.00033852]
		[batch 20/20] avg loss: 2.2262084055532223		[learning rate: 0.0003377]
	Learning Rate: 0.000337696
	LOSS [training: 1.9721118253941097 | validation: 2.009507727982481]
	TIME [epoch: 8.4 sec]
EPOCH 750/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.42018442985999		[learning rate: 0.00033688]
		[batch 20/20] avg loss: 2.6295729926870903		[learning rate: 0.00033606]
	Learning Rate: 0.000336063
	LOSS [training: 2.52487871127354 | validation: 2.339419577205455]
	TIME [epoch: 8.41 sec]
EPOCH 751/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.7639567538829986		[learning rate: 0.00033525]
		[batch 20/20] avg loss: 2.590008543093177		[learning rate: 0.00033444]
	Learning Rate: 0.000334438
	LOSS [training: 2.676982648488088 | validation: 2.4102821767771743]
	TIME [epoch: 8.4 sec]
EPOCH 752/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.6707606386619345		[learning rate: 0.00033363]
		[batch 20/20] avg loss: 2.9563820754039183		[learning rate: 0.00033282]
	Learning Rate: 0.000332821
	LOSS [training: 2.8135713570329264 | validation: 2.5819149699682917]
	TIME [epoch: 8.39 sec]
EPOCH 753/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.833254943072426		[learning rate: 0.00033202]
		[batch 20/20] avg loss: 2.8339515594290337		[learning rate: 0.00033121]
	Learning Rate: 0.000331211
	LOSS [training: 2.83360325125073 | validation: 2.4137605379190337]
	TIME [epoch: 8.44 sec]
EPOCH 754/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.627029356273326		[learning rate: 0.00033041]
		[batch 20/20] avg loss: 2.559234529232375		[learning rate: 0.00032961]
	Learning Rate: 0.00032961
	LOSS [training: 2.5931319427528505 | validation: 2.347985641848805]
	TIME [epoch: 8.4 sec]
EPOCH 755/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.541970875891749		[learning rate: 0.00032881]
		[batch 20/20] avg loss: 2.6266929574078373		[learning rate: 0.00032802]
	Learning Rate: 0.000328016
	LOSS [training: 2.584331916649793 | validation: 2.308656806900206]
	TIME [epoch: 8.39 sec]
EPOCH 756/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.449121023606165		[learning rate: 0.00032722]
		[batch 20/20] avg loss: 2.5831236444786376		[learning rate: 0.00032643]
	Learning Rate: 0.00032643
	LOSS [training: 2.516122334042401 | validation: 2.2017567618822556]
	TIME [epoch: 8.4 sec]
EPOCH 757/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.43315279123392		[learning rate: 0.00032564]
		[batch 20/20] avg loss: 2.4595516239104707		[learning rate: 0.00032485]
	Learning Rate: 0.000324851
	LOSS [training: 2.446352207572195 | validation: 2.1097805959107747]
	TIME [epoch: 8.44 sec]
EPOCH 758/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.332756723178264		[learning rate: 0.00032406]
		[batch 20/20] avg loss: 2.4346518262167764		[learning rate: 0.00032328]
	Learning Rate: 0.00032328
	LOSS [training: 2.38370427469752 | validation: 2.123665151202249]
	TIME [epoch: 8.39 sec]
EPOCH 759/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3885454540114304		[learning rate: 0.0003225]
		[batch 20/20] avg loss: 2.226106241154637		[learning rate: 0.00032172]
	Learning Rate: 0.000321717
	LOSS [training: 2.3073258475830336 | validation: 1.9146722531593907]
	TIME [epoch: 8.38 sec]
EPOCH 760/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.161420256872512		[learning rate: 0.00032094]
		[batch 20/20] avg loss: 2.1047693486660846		[learning rate: 0.00032016]
	Learning Rate: 0.000320161
	LOSS [training: 2.133094802769298 | validation: 1.8310410373642494]
	TIME [epoch: 8.4 sec]
EPOCH 761/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.2140229508772604		[learning rate: 0.00031939]
		[batch 20/20] avg loss: 2.138860446309577		[learning rate: 0.00031861]
	Learning Rate: 0.000318613
	LOSS [training: 2.1764416985934187 | validation: 1.6292794350345836]
	TIME [epoch: 8.42 sec]
EPOCH 762/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8188696409105116		[learning rate: 0.00031784]
		[batch 20/20] avg loss: 1.737050260824282		[learning rate: 0.00031707]
	Learning Rate: 0.000317072
	LOSS [training: 1.7779599508673964 | validation: 1.5283499992874043]
	TIME [epoch: 8.41 sec]
EPOCH 763/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6268888706890958		[learning rate: 0.0003163]
		[batch 20/20] avg loss: 1.7899170382626128		[learning rate: 0.00031554]
	Learning Rate: 0.000315539
	LOSS [training: 1.7084029544758539 | validation: 1.5123163463880842]
	TIME [epoch: 8.43 sec]
EPOCH 764/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7422709864165729		[learning rate: 0.00031477]
		[batch 20/20] avg loss: 1.7418679493632616		[learning rate: 0.00031401]
	Learning Rate: 0.000314013
	LOSS [training: 1.7420694678899173 | validation: 1.505459881615544]
	TIME [epoch: 8.38 sec]
EPOCH 765/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7098205299101863		[learning rate: 0.00031325]
		[batch 20/20] avg loss: 1.6330725089149123		[learning rate: 0.00031249]
	Learning Rate: 0.000312494
	LOSS [training: 1.6714465194125492 | validation: 1.418800163394063]
	TIME [epoch: 8.38 sec]
EPOCH 766/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6296859485862556		[learning rate: 0.00031174]
		[batch 20/20] avg loss: 1.5630099550368923		[learning rate: 0.00031098]
	Learning Rate: 0.000310983
	LOSS [training: 1.596347951811574 | validation: 1.0728757309888637]
	TIME [epoch: 8.42 sec]
EPOCH 767/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4462186465314228		[learning rate: 0.00031023]
		[batch 20/20] avg loss: 1.2787946450308172		[learning rate: 0.00030948]
	Learning Rate: 0.000309479
	LOSS [training: 1.3625066457811201 | validation: 1.0706154242635595]
	TIME [epoch: 8.4 sec]
EPOCH 768/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2272067900956456		[learning rate: 0.00030873]
		[batch 20/20] avg loss: 1.1432172501952613		[learning rate: 0.00030798]
	Learning Rate: 0.000307983
	LOSS [training: 1.1852120201454537 | validation: 0.7464401031936772]
	TIME [epoch: 8.39 sec]
EPOCH 769/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9456020386333307		[learning rate: 0.00030724]
		[batch 20/20] avg loss: 0.9248801633623567		[learning rate: 0.00030649]
	Learning Rate: 0.000306493
	LOSS [training: 0.9352411009978437 | validation: 0.9590986176302929]
	TIME [epoch: 8.38 sec]
EPOCH 770/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.197709738772295		[learning rate: 0.00030575]
		[batch 20/20] avg loss: 1.267537077433906		[learning rate: 0.00030501]
	Learning Rate: 0.000305011
	LOSS [training: 1.2326234081031004 | validation: 1.176464695408727]
	TIME [epoch: 8.41 sec]
EPOCH 771/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.365777209296803		[learning rate: 0.00030427]
		[batch 20/20] avg loss: 1.4061677968920352		[learning rate: 0.00030354]
	Learning Rate: 0.000303536
	LOSS [training: 1.385972503094419 | validation: 1.3258331314343401]
	TIME [epoch: 8.4 sec]
EPOCH 772/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3641642093192068		[learning rate: 0.0003028]
		[batch 20/20] avg loss: 1.5546466049715018		[learning rate: 0.00030207]
	Learning Rate: 0.000302068
	LOSS [training: 1.4594054071453542 | validation: 1.3956950876709748]
	TIME [epoch: 8.41 sec]
EPOCH 773/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6374667750959861		[learning rate: 0.00030134]
		[batch 20/20] avg loss: 1.7250214150611878		[learning rate: 0.00030061]
	Learning Rate: 0.000300608
	LOSS [training: 1.681244095078587 | validation: 1.596175036387699]
	TIME [epoch: 8.38 sec]
EPOCH 774/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7179335617589466		[learning rate: 0.00029988]
		[batch 20/20] avg loss: 1.6761362203254921		[learning rate: 0.00029915]
	Learning Rate: 0.000299154
	LOSS [training: 1.6970348910422188 | validation: 1.5853926788881647]
	TIME [epoch: 8.4 sec]
EPOCH 775/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.749767799030168		[learning rate: 0.00029843]
		[batch 20/20] avg loss: 1.6475462826935239		[learning rate: 0.00029771]
	Learning Rate: 0.000297707
	LOSS [training: 1.6986570408618458 | validation: 1.4684729175245277]
	TIME [epoch: 8.41 sec]
EPOCH 776/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5763800863380872		[learning rate: 0.00029699]
		[batch 20/20] avg loss: 1.3623809267445606		[learning rate: 0.00029627]
	Learning Rate: 0.000296268
	LOSS [training: 1.4693805065413237 | validation: 1.1954000807471952]
	TIME [epoch: 8.41 sec]
EPOCH 777/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2621478203278564		[learning rate: 0.00029555]
		[batch 20/20] avg loss: 1.3215752700659764		[learning rate: 0.00029483]
	Learning Rate: 0.000294835
	LOSS [training: 1.2918615451969164 | validation: 1.1922114216858726]
	TIME [epoch: 8.39 sec]
EPOCH 778/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2100620215421274		[learning rate: 0.00029412]
		[batch 20/20] avg loss: 1.090222554316256		[learning rate: 0.00029341]
	Learning Rate: 0.000293409
	LOSS [training: 1.1501422879291918 | validation: 0.9590784492230081]
	TIME [epoch: 8.39 sec]
EPOCH 779/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0511638687028015		[learning rate: 0.0002927]
		[batch 20/20] avg loss: 1.0393880485655944		[learning rate: 0.00029199]
	Learning Rate: 0.00029199
	LOSS [training: 1.0452759586341982 | validation: 1.077238663113351]
	TIME [epoch: 8.44 sec]
EPOCH 780/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0732856523045942		[learning rate: 0.00029128]
		[batch 20/20] avg loss: 0.995633490472628		[learning rate: 0.00029058]
	Learning Rate: 0.000290578
	LOSS [training: 1.0344595713886109 | validation: 0.8382337023751014]
	TIME [epoch: 8.4 sec]
EPOCH 781/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9444811343447606		[learning rate: 0.00028987]
		[batch 20/20] avg loss: 0.8795224085906928		[learning rate: 0.00028917]
	Learning Rate: 0.000289173
	LOSS [training: 0.9120017714677268 | validation: 0.709497122253681]
	TIME [epoch: 8.39 sec]
EPOCH 782/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9112451438854274		[learning rate: 0.00028847]
		[batch 20/20] avg loss: 0.858072522301978		[learning rate: 0.00028777]
	Learning Rate: 0.000287775
	LOSS [training: 0.8846588330937027 | validation: 0.7153005809170399]
	TIME [epoch: 8.38 sec]
EPOCH 783/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8928546560703934		[learning rate: 0.00028708]
		[batch 20/20] avg loss: 1.1620925819721484		[learning rate: 0.00028638]
	Learning Rate: 0.000286383
	LOSS [training: 1.027473619021271 | validation: 0.9198865512519062]
	TIME [epoch: 8.42 sec]
EPOCH 784/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9128141438356513		[learning rate: 0.00028569]
		[batch 20/20] avg loss: 0.7946199023967659		[learning rate: 0.000285]
	Learning Rate: 0.000284998
	LOSS [training: 0.8537170231162088 | validation: 0.7655947077738564]
	TIME [epoch: 8.41 sec]
EPOCH 785/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0147626697023413		[learning rate: 0.00028431]
		[batch 20/20] avg loss: 1.0894837299161635		[learning rate: 0.00028362]
	Learning Rate: 0.00028362
	LOSS [training: 1.0521231998092522 | validation: 0.9737271731943948]
	TIME [epoch: 8.41 sec]
EPOCH 786/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9438346912953627		[learning rate: 0.00028293]
		[batch 20/20] avg loss: 1.1181156866516386		[learning rate: 0.00028225]
	Learning Rate: 0.000282248
	LOSS [training: 1.0309751889735002 | validation: 0.9097373725056881]
	TIME [epoch: 8.39 sec]
EPOCH 787/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9616613041547477		[learning rate: 0.00028157]
		[batch 20/20] avg loss: 0.9633221665147212		[learning rate: 0.00028088]
	Learning Rate: 0.000280884
	LOSS [training: 0.9624917353347348 | validation: 0.8472712321821009]
	TIME [epoch: 8.41 sec]
EPOCH 788/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9124875360937267		[learning rate: 0.0002802]
		[batch 20/20] avg loss: 0.9809965641261886		[learning rate: 0.00027953]
	Learning Rate: 0.000279525
	LOSS [training: 0.9467420501099578 | validation: 0.6976633026749425]
	TIME [epoch: 8.4 sec]
EPOCH 789/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.872619698021448		[learning rate: 0.00027885]
		[batch 20/20] avg loss: 0.8196923618014168		[learning rate: 0.00027817]
	Learning Rate: 0.000278173
	LOSS [training: 0.8461560299114325 | validation: 0.4901789722855615]
	TIME [epoch: 8.41 sec]
EPOCH 790/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7099862557941801		[learning rate: 0.0002775]
		[batch 20/20] avg loss: 0.6346146091215877		[learning rate: 0.00027683]
	Learning Rate: 0.000276828
	LOSS [training: 0.6723004324578841 | validation: 0.4184712600374317]
	TIME [epoch: 8.41 sec]
EPOCH 791/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6034188076456501		[learning rate: 0.00027616]
		[batch 20/20] avg loss: 0.5926303604137887		[learning rate: 0.00027549]
	Learning Rate: 0.00027549
	LOSS [training: 0.5980245840297193 | validation: 0.5246044401371449]
	TIME [epoch: 8.41 sec]
EPOCH 792/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6264754798736496		[learning rate: 0.00027482]
		[batch 20/20] avg loss: 0.6347273724266228		[learning rate: 0.00027416]
	Learning Rate: 0.000274157
	LOSS [training: 0.6306014261501361 | validation: 0.4460839398825308]
	TIME [epoch: 8.42 sec]
EPOCH 793/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6047284039822485		[learning rate: 0.00027349]
		[batch 20/20] avg loss: 0.6223481447475228		[learning rate: 0.00027283]
	Learning Rate: 0.000272832
	LOSS [training: 0.6135382743648857 | validation: 0.4490384680115114]
	TIME [epoch: 8.39 sec]
EPOCH 794/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6048948530148921		[learning rate: 0.00027217]
		[batch 20/20] avg loss: 0.5834418509370073		[learning rate: 0.00027151]
	Learning Rate: 0.000271512
	LOSS [training: 0.5941683519759495 | validation: 0.46298444012074547]
	TIME [epoch: 8.39 sec]
EPOCH 795/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6388195429495026		[learning rate: 0.00027086]
		[batch 20/20] avg loss: 0.5766137406580926		[learning rate: 0.0002702]
	Learning Rate: 0.000270199
	LOSS [training: 0.6077166418037976 | validation: 0.5255197374743041]
	TIME [epoch: 8.4 sec]
EPOCH 796/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6347074856870858		[learning rate: 0.00026955]
		[batch 20/20] avg loss: 0.5781852539503787		[learning rate: 0.00026889]
	Learning Rate: 0.000268893
	LOSS [training: 0.6064463698187321 | validation: 0.4879748057676355]
	TIME [epoch: 8.42 sec]
EPOCH 797/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6454368799152229		[learning rate: 0.00026824]
		[batch 20/20] avg loss: 0.7198746962801337		[learning rate: 0.00026759]
	Learning Rate: 0.000267592
	LOSS [training: 0.6826557880976785 | validation: 0.6410266674286234]
	TIME [epoch: 8.4 sec]
EPOCH 798/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7307045432356595		[learning rate: 0.00026694]
		[batch 20/20] avg loss: 0.7003789439898579		[learning rate: 0.0002663]
	Learning Rate: 0.000266298
	LOSS [training: 0.7155417436127587 | validation: 0.6716625818780331]
	TIME [epoch: 8.38 sec]
EPOCH 799/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8044571532402214		[learning rate: 0.00026565]
		[batch 20/20] avg loss: 0.938804563884144		[learning rate: 0.00026501]
	Learning Rate: 0.000265011
	LOSS [training: 0.8716308585621825 | validation: 0.9535168692928215]
	TIME [epoch: 8.38 sec]
EPOCH 800/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9381984126341891		[learning rate: 0.00026437]
		[batch 20/20] avg loss: 0.834181621918658		[learning rate: 0.00026373]
	Learning Rate: 0.000263729
	LOSS [training: 0.8861900172764237 | validation: 0.7267566570387847]
	TIME [epoch: 8.41 sec]
EPOCH 801/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7596148305191279		[learning rate: 0.00026309]
		[batch 20/20] avg loss: 0.7369934584018522		[learning rate: 0.00026245]
	Learning Rate: 0.000262454
	LOSS [training: 0.7483041444604901 | validation: 0.7867429249558646]
	TIME [epoch: 8.42 sec]
EPOCH 802/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.89052819129656		[learning rate: 0.00026182]
		[batch 20/20] avg loss: 0.8369585439290198		[learning rate: 0.00026118]
	Learning Rate: 0.000261184
	LOSS [training: 0.86374336761279 | validation: 0.6804935339712652]
	TIME [epoch: 8.41 sec]
EPOCH 803/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7704407064196668		[learning rate: 0.00026055]
		[batch 20/20] avg loss: 0.7571190597654944		[learning rate: 0.00025992]
	Learning Rate: 0.000259921
	LOSS [training: 0.7637798830925806 | validation: 0.5859881805524738]
	TIME [epoch: 8.4 sec]
EPOCH 804/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6509383909168174		[learning rate: 0.00025929]
		[batch 20/20] avg loss: 0.5704608676083878		[learning rate: 0.00025866]
	Learning Rate: 0.000258665
	LOSS [training: 0.6106996292626026 | validation: 0.5190599655515531]
	TIME [epoch: 8.39 sec]
EPOCH 805/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6218888422241283		[learning rate: 0.00025804]
		[batch 20/20] avg loss: 0.6348108873589029		[learning rate: 0.00025741]
	Learning Rate: 0.000257414
	LOSS [training: 0.6283498647915157 | validation: 0.4951632462419475]
	TIME [epoch: 8.42 sec]
EPOCH 806/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5988213492488274		[learning rate: 0.00025679]
		[batch 20/20] avg loss: 0.5893211519934191		[learning rate: 0.00025617]
	Learning Rate: 0.000256169
	LOSS [training: 0.5940712506211233 | validation: 0.54780432483091]
	TIME [epoch: 8.42 sec]
EPOCH 807/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6570972988896606		[learning rate: 0.00025555]
		[batch 20/20] avg loss: 0.6095563150028541		[learning rate: 0.00025493]
	Learning Rate: 0.00025493
	LOSS [training: 0.6333268069462573 | validation: 0.5609181695530686]
	TIME [epoch: 8.41 sec]
EPOCH 808/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7225965666633798		[learning rate: 0.00025431]
		[batch 20/20] avg loss: 0.6841432346371479		[learning rate: 0.0002537]
	Learning Rate: 0.000253697
	LOSS [training: 0.7033699006502637 | validation: 0.5589495198652601]
	TIME [epoch: 8.39 sec]
EPOCH 809/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6746564465178759		[learning rate: 0.00025308]
		[batch 20/20] avg loss: 0.6486231158063276		[learning rate: 0.00025247]
	Learning Rate: 0.00025247
	LOSS [training: 0.6616397811621018 | validation: 0.5951278254480832]
	TIME [epoch: 8.42 sec]
EPOCH 810/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7574063028337185		[learning rate: 0.00025186]
		[batch 20/20] avg loss: 0.6788697418488595		[learning rate: 0.00025125]
	Learning Rate: 0.00025125
	LOSS [training: 0.718138022341289 | validation: 0.5134385476222225]
	TIME [epoch: 8.42 sec]
EPOCH 811/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6132306962126943		[learning rate: 0.00025064]
		[batch 20/20] avg loss: 0.7698386912312914		[learning rate: 0.00025003]
	Learning Rate: 0.000250035
	LOSS [training: 0.6915346937219928 | validation: 0.6268841056534491]
	TIME [epoch: 8.4 sec]
EPOCH 812/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6387620064152927		[learning rate: 0.00024943]
		[batch 20/20] avg loss: 0.810333183420183		[learning rate: 0.00024883]
	Learning Rate: 0.000248825
	LOSS [training: 0.7245475949177378 | validation: 0.7486852031466339]
	TIME [epoch: 8.39 sec]
EPOCH 813/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.796593444435256		[learning rate: 0.00024822]
		[batch 20/20] avg loss: 0.7476221186490336		[learning rate: 0.00024762]
	Learning Rate: 0.000247622
	LOSS [training: 0.7721077815421448 | validation: 0.497511833576323]
	TIME [epoch: 8.43 sec]
EPOCH 814/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6335960099285384		[learning rate: 0.00024702]
		[batch 20/20] avg loss: 0.6092794080642638		[learning rate: 0.00024642]
	Learning Rate: 0.000246425
	LOSS [training: 0.621437708996401 | validation: 0.5111602557785009]
	TIME [epoch: 8.43 sec]
EPOCH 815/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.675736325709293		[learning rate: 0.00024583]
		[batch 20/20] avg loss: 0.7846715743432221		[learning rate: 0.00024523]
	Learning Rate: 0.000245233
	LOSS [training: 0.7302039500262574 | validation: 0.6444724952638442]
	TIME [epoch: 8.4 sec]
EPOCH 816/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.750062812848788		[learning rate: 0.00024464]
		[batch 20/20] avg loss: 0.5962231840707026		[learning rate: 0.00024405]
	Learning Rate: 0.000244047
	LOSS [training: 0.6731429984597453 | validation: 0.5191536804948942]
	TIME [epoch: 8.39 sec]
EPOCH 817/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6144853210456845		[learning rate: 0.00024346]
		[batch 20/20] avg loss: 0.7243743583473206		[learning rate: 0.00024287]
	Learning Rate: 0.000242867
	LOSS [training: 0.6694298396965024 | validation: 0.5384917454150279]
	TIME [epoch: 8.42 sec]
EPOCH 818/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7043834040208532		[learning rate: 0.00024228]
		[batch 20/20] avg loss: 0.6914185567262554		[learning rate: 0.00024169]
	Learning Rate: 0.000241693
	LOSS [training: 0.6979009803735542 | validation: 0.6106405179604186]
	TIME [epoch: 8.42 sec]
EPOCH 819/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6973552557189538		[learning rate: 0.00024111]
		[batch 20/20] avg loss: 0.8054217750812256		[learning rate: 0.00024052]
	Learning Rate: 0.000240524
	LOSS [training: 0.7513885154000898 | validation: 0.632844555142867]
	TIME [epoch: 8.39 sec]
EPOCH 820/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7656050903443561		[learning rate: 0.00023994]
		[batch 20/20] avg loss: 0.8080200977191959		[learning rate: 0.00023936]
	Learning Rate: 0.000239361
	LOSS [training: 0.7868125940317758 | validation: 0.731511774671264]
	TIME [epoch: 8.4 sec]
EPOCH 821/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8080196505307878		[learning rate: 0.00023878]
		[batch 20/20] avg loss: 0.7838435324593942		[learning rate: 0.0002382]
	Learning Rate: 0.000238203
	LOSS [training: 0.7959315914950912 | validation: 0.7079794728789865]
	TIME [epoch: 8.41 sec]
EPOCH 822/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7301153114239222		[learning rate: 0.00023763]
		[batch 20/20] avg loss: 0.712458631929654		[learning rate: 0.00023705]
	Learning Rate: 0.000237051
	LOSS [training: 0.721286971676788 | validation: 0.5057535213587406]
	TIME [epoch: 8.43 sec]
EPOCH 823/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.659948005111594		[learning rate: 0.00023648]
		[batch 20/20] avg loss: 0.7232148650643188		[learning rate: 0.0002359]
	Learning Rate: 0.000235905
	LOSS [training: 0.6915814350879564 | validation: 0.5735026618756279]
	TIME [epoch: 8.4 sec]
EPOCH 824/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6853780571085404		[learning rate: 0.00023533]
		[batch 20/20] avg loss: 0.784257981503047		[learning rate: 0.00023476]
	Learning Rate: 0.000234764
	LOSS [training: 0.7348180193057938 | validation: 0.6574500119491871]
	TIME [epoch: 8.42 sec]
EPOCH 825/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9288586974576507		[learning rate: 0.0002342]
		[batch 20/20] avg loss: 0.9941396996788955		[learning rate: 0.00023363]
	Learning Rate: 0.000233629
	LOSS [training: 0.9614991985682734 | validation: 0.8299706259451162]
	TIME [epoch: 8.4 sec]
EPOCH 826/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9183491072721128		[learning rate: 0.00023306]
		[batch 20/20] avg loss: 1.0015323057758763		[learning rate: 0.0002325]
	Learning Rate: 0.000232499
	LOSS [training: 0.9599407065239947 | validation: 0.8343694108245209]
	TIME [epoch: 8.42 sec]
EPOCH 827/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9057117725627482		[learning rate: 0.00023194]
		[batch 20/20] avg loss: 0.931924265307121		[learning rate: 0.00023137]
	Learning Rate: 0.000231375
	LOSS [training: 0.9188180189349346 | validation: 0.7815040468800335]
	TIME [epoch: 8.42 sec]
EPOCH 828/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7907885018632788		[learning rate: 0.00023081]
		[batch 20/20] avg loss: 0.8125936682883431		[learning rate: 0.00023026]
	Learning Rate: 0.000230256
	LOSS [training: 0.801691085075811 | validation: 0.5895217633108194]
	TIME [epoch: 8.4 sec]
EPOCH 829/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7656422000226952		[learning rate: 0.0002297]
		[batch 20/20] avg loss: 0.6944498558836072		[learning rate: 0.00022914]
	Learning Rate: 0.000229142
	LOSS [training: 0.7300460279531513 | validation: 0.561922561223567]
	TIME [epoch: 8.41 sec]
EPOCH 830/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7054069385728989		[learning rate: 0.00022859]
		[batch 20/20] avg loss: 0.718375513555822		[learning rate: 0.00022803]
	Learning Rate: 0.000228034
	LOSS [training: 0.7118912260643603 | validation: 0.5564362074758719]
	TIME [epoch: 8.42 sec]
EPOCH 831/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6861556553789191		[learning rate: 0.00022748]
		[batch 20/20] avg loss: 0.7392489256380286		[learning rate: 0.00022693]
	Learning Rate: 0.000226931
	LOSS [training: 0.7127022905084737 | validation: 0.5883265865068532]
	TIME [epoch: 8.4 sec]
EPOCH 832/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7614014431972109		[learning rate: 0.00022638]
		[batch 20/20] avg loss: 0.6756142343352203		[learning rate: 0.00022583]
	Learning Rate: 0.000225834
	LOSS [training: 0.7185078387662154 | validation: 0.6029904751272512]
	TIME [epoch: 8.42 sec]
EPOCH 833/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6965927106230818		[learning rate: 0.00022529]
		[batch 20/20] avg loss: 0.7410440575031535		[learning rate: 0.00022474]
	Learning Rate: 0.000224742
	LOSS [training: 0.7188183840631176 | validation: 0.5867780848883548]
	TIME [epoch: 8.39 sec]
EPOCH 834/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7066987798999362		[learning rate: 0.0002242]
		[batch 20/20] avg loss: 0.7591924814283534		[learning rate: 0.00022366]
	Learning Rate: 0.000223655
	LOSS [training: 0.7329456306641446 | validation: 0.6117334138172363]
	TIME [epoch: 8.4 sec]
EPOCH 835/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7290857213761798		[learning rate: 0.00022311]
		[batch 20/20] avg loss: 0.798747796364121		[learning rate: 0.00022257]
	Learning Rate: 0.000222574
	LOSS [training: 0.7639167588701502 | validation: 0.6517140759262734]
	TIME [epoch: 8.44 sec]
EPOCH 836/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7348219243511652		[learning rate: 0.00022203]
		[batch 20/20] avg loss: 0.7593995488398895		[learning rate: 0.0002215]
	Learning Rate: 0.000221497
	LOSS [training: 0.7471107365955274 | validation: 0.6223354282568165]
	TIME [epoch: 8.39 sec]
EPOCH 837/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7347678242403756		[learning rate: 0.00022096]
		[batch 20/20] avg loss: 0.7364429850817373		[learning rate: 0.00022043]
	Learning Rate: 0.000220426
	LOSS [training: 0.7356054046610566 | validation: 0.6185761028495076]
	TIME [epoch: 8.41 sec]
EPOCH 838/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.690937293773625		[learning rate: 0.00021989]
		[batch 20/20] avg loss: 0.7710767764957213		[learning rate: 0.00021936]
	Learning Rate: 0.00021936
	LOSS [training: 0.7310070351346729 | validation: 0.6261142648467206]
	TIME [epoch: 8.39 sec]
EPOCH 839/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7593986664695622		[learning rate: 0.00021883]
		[batch 20/20] avg loss: 0.7630852709375378		[learning rate: 0.0002183]
	Learning Rate: 0.000218299
	LOSS [training: 0.7612419687035501 | validation: 0.5946390875290486]
	TIME [epoch: 8.43 sec]
EPOCH 840/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7409809478313697		[learning rate: 0.00021777]
		[batch 20/20] avg loss: 0.7739091957453285		[learning rate: 0.00021724]
	Learning Rate: 0.000217244
	LOSS [training: 0.7574450717883491 | validation: 0.709457739274966]
	TIME [epoch: 8.41 sec]
EPOCH 841/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.809631706891832		[learning rate: 0.00021672]
		[batch 20/20] avg loss: 0.7422138305776876		[learning rate: 0.00021619]
	Learning Rate: 0.000216193
	LOSS [training: 0.7759227687347596 | validation: 0.5631430657714238]
	TIME [epoch: 8.39 sec]
EPOCH 842/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6629628917812426		[learning rate: 0.00021567]
		[batch 20/20] avg loss: 0.7853413422295661		[learning rate: 0.00021515]
	Learning Rate: 0.000215148
	LOSS [training: 0.7241521170054044 | validation: 0.660215765438649]
	TIME [epoch: 8.4 sec]
EPOCH 843/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7822993235473622		[learning rate: 0.00021463]
		[batch 20/20] avg loss: 0.681487939048983		[learning rate: 0.00021411]
	Learning Rate: 0.000214107
	LOSS [training: 0.7318936312981725 | validation: 0.5239309092784858]
	TIME [epoch: 8.43 sec]
EPOCH 844/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.680359063727775		[learning rate: 0.00021359]
		[batch 20/20] avg loss: 0.6198277445304643		[learning rate: 0.00021307]
	Learning Rate: 0.000213072
	LOSS [training: 0.6500934041291195 | validation: 0.5791304038324956]
	TIME [epoch: 8.4 sec]
EPOCH 845/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7229457868940942		[learning rate: 0.00021256]
		[batch 20/20] avg loss: 0.6402216020699272		[learning rate: 0.00021204]
	Learning Rate: 0.000212042
	LOSS [training: 0.6815836944820106 | validation: 0.508553204218052]
	TIME [epoch: 8.4 sec]
EPOCH 846/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6327476067788458		[learning rate: 0.00021153]
		[batch 20/20] avg loss: 0.6260989835449019		[learning rate: 0.00021102]
	Learning Rate: 0.000211016
	LOSS [training: 0.6294232951618739 | validation: 0.5539075980014354]
	TIME [epoch: 8.4 sec]
EPOCH 847/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6962430402776191		[learning rate: 0.00021051]
		[batch 20/20] avg loss: 0.6118262217102869		[learning rate: 0.00021]
	Learning Rate: 0.000209996
	LOSS [training: 0.654034630993953 | validation: 0.5201074772686436]
	TIME [epoch: 8.39 sec]
EPOCH 848/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6550602037916632		[learning rate: 0.00020949]
		[batch 20/20] avg loss: 0.7323435543337932		[learning rate: 0.00020898]
	Learning Rate: 0.00020898
	LOSS [training: 0.6937018790627282 | validation: 0.5762671457111469]
	TIME [epoch: 8.44 sec]
EPOCH 849/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7046202501043519		[learning rate: 0.00020847]
		[batch 20/20] avg loss: 0.6511937689599089		[learning rate: 0.00020797]
	Learning Rate: 0.00020797
	LOSS [training: 0.6779070095321303 | validation: 0.4700124908453568]
	TIME [epoch: 8.39 sec]
EPOCH 850/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6135869838570481		[learning rate: 0.00020747]
		[batch 20/20] avg loss: 0.5700215957728754		[learning rate: 0.00020696]
	Learning Rate: 0.000206964
	LOSS [training: 0.5918042898149618 | validation: 0.44441426171794585]
	TIME [epoch: 8.4 sec]
EPOCH 851/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5633106962987104		[learning rate: 0.00020646]
		[batch 20/20] avg loss: 0.513910957278366		[learning rate: 0.00020596]
	Learning Rate: 0.000205963
	LOSS [training: 0.5386108267885382 | validation: 0.45178732295940377]
	TIME [epoch: 8.4 sec]
EPOCH 852/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5090222894300196		[learning rate: 0.00020546]
		[batch 20/20] avg loss: 0.593633988199626		[learning rate: 0.00020497]
	Learning Rate: 0.000204967
	LOSS [training: 0.5513281388148228 | validation: 0.44371944670601743]
	TIME [epoch: 8.41 sec]
EPOCH 853/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5298256166211124		[learning rate: 0.00020447]
		[batch 20/20] avg loss: 0.5839256017502533		[learning rate: 0.00020398]
	Learning Rate: 0.000203976
	LOSS [training: 0.5568756091856829 | validation: 0.4428196017006434]
	TIME [epoch: 8.41 sec]
EPOCH 854/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5461711017694697		[learning rate: 0.00020348]
		[batch 20/20] avg loss: 0.5332028745423728		[learning rate: 0.00020299]
	Learning Rate: 0.00020299
	LOSS [training: 0.5396869881559212 | validation: 0.40595215087055603]
	TIME [epoch: 8.4 sec]
EPOCH 855/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.509069946867546		[learning rate: 0.0002025]
		[batch 20/20] avg loss: 0.5285627719433862		[learning rate: 0.00020201]
	Learning Rate: 0.000202008
	LOSS [training: 0.5188163594054661 | validation: 0.42198220383009205]
	TIME [epoch: 8.38 sec]
EPOCH 856/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5064950618510287		[learning rate: 0.00020152]
		[batch 20/20] avg loss: 0.5607764818803913		[learning rate: 0.00020103]
	Learning Rate: 0.000201031
	LOSS [training: 0.5336357718657101 | validation: 0.3824161877190437]
	TIME [epoch: 8.43 sec]
EPOCH 857/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5048878561561675		[learning rate: 0.00020054]
		[batch 20/20] avg loss: 0.48557968946420943		[learning rate: 0.00020006]
	Learning Rate: 0.000200059
	LOSS [training: 0.49523377281018843 | validation: 0.3684557325202185]
	TIME [epoch: 8.41 sec]
EPOCH 858/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.49028088964094324		[learning rate: 0.00019957]
		[batch 20/20] avg loss: 0.5024625363704142		[learning rate: 0.00019909]
	Learning Rate: 0.000199091
	LOSS [training: 0.49637171300567884 | validation: 0.35487641329675174]
	TIME [epoch: 8.38 sec]
EPOCH 859/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45762758165778294		[learning rate: 0.00019861]
		[batch 20/20] avg loss: 0.522969387071406		[learning rate: 0.00019813]
	Learning Rate: 0.000198129
	LOSS [training: 0.4902984843645945 | validation: 0.35174835828684814]
	TIME [epoch: 8.4 sec]
EPOCH 860/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4784257790597014		[learning rate: 0.00019765]
		[batch 20/20] avg loss: 0.4815300013114937		[learning rate: 0.00019717]
	Learning Rate: 0.000197171
	LOSS [training: 0.47997789018559767 | validation: 0.39104572177653707]
	TIME [epoch: 8.41 sec]
EPOCH 861/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5304188890667021		[learning rate: 0.00019669]
		[batch 20/20] avg loss: 0.5419669246908371		[learning rate: 0.00019622]
	Learning Rate: 0.000196217
	LOSS [training: 0.5361929068787695 | validation: 0.5045301742011068]
	TIME [epoch: 8.41 sec]
EPOCH 862/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.58690714710823		[learning rate: 0.00019574]
		[batch 20/20] avg loss: 0.6099686913136539		[learning rate: 0.00019527]
	Learning Rate: 0.000195268
	LOSS [training: 0.5984379192109419 | validation: 0.4542670757912512]
	TIME [epoch: 8.4 sec]
EPOCH 863/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5052395678340103		[learning rate: 0.0001948]
		[batch 20/20] avg loss: 0.4871285854135059		[learning rate: 0.00019432]
	Learning Rate: 0.000194324
	LOSS [training: 0.4961840766237581 | validation: 0.34043943931831494]
	TIME [epoch: 8.41 sec]
EPOCH 864/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44412927802028734		[learning rate: 0.00019385]
		[batch 20/20] avg loss: 0.47063369250240294		[learning rate: 0.00019338]
	Learning Rate: 0.000193384
	LOSS [training: 0.45738148526134514 | validation: 0.33483313709949575]
	TIME [epoch: 8.38 sec]
EPOCH 865/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4722415695151783		[learning rate: 0.00019292]
		[batch 20/20] avg loss: 0.4574950385547737		[learning rate: 0.00019245]
	Learning Rate: 0.000192449
	LOSS [training: 0.4648683040349761 | validation: 0.33543800315889527]
	TIME [epoch: 8.42 sec]
EPOCH 866/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4451525149534395		[learning rate: 0.00019198]
		[batch 20/20] avg loss: 0.45983487869350836		[learning rate: 0.00019152]
	Learning Rate: 0.000191518
	LOSS [training: 0.45249369682347407 | validation: 0.32598273369375425]
	TIME [epoch: 8.4 sec]
EPOCH 867/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45840306878491344		[learning rate: 0.00019105]
		[batch 20/20] avg loss: 0.4643239334966355		[learning rate: 0.00019059]
	Learning Rate: 0.000190592
	LOSS [training: 0.4613635011407745 | validation: 0.34734359934991305]
	TIME [epoch: 8.39 sec]
EPOCH 868/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42136105429191584		[learning rate: 0.00019013]
		[batch 20/20] avg loss: 0.48148339564642273		[learning rate: 0.00018967]
	Learning Rate: 0.000189671
	LOSS [training: 0.45142222496916934 | validation: 0.3182908919852533]
	TIME [epoch: 8.38 sec]
EPOCH 869/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4345066986749875		[learning rate: 0.00018921]
		[batch 20/20] avg loss: 0.47246634882844674		[learning rate: 0.00018875]
	Learning Rate: 0.000188753
	LOSS [training: 0.4534865237517171 | validation: 0.31393983718552115]
	TIME [epoch: 8.43 sec]
EPOCH 870/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45808138653472225		[learning rate: 0.0001883]
		[batch 20/20] avg loss: 0.45185882314824577		[learning rate: 0.00018784]
	Learning Rate: 0.000187841
	LOSS [training: 0.454970104841484 | validation: 0.39506841734177056]
	TIME [epoch: 8.41 sec]
EPOCH 871/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4533019988867455		[learning rate: 0.00018739]
		[batch 20/20] avg loss: 0.445896413148938		[learning rate: 0.00018693]
	Learning Rate: 0.000186932
	LOSS [training: 0.4495992060178418 | validation: 0.33047217616634356]
	TIME [epoch: 8.38 sec]
EPOCH 872/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.45065950633133606		[learning rate: 0.00018648]
		[batch 20/20] avg loss: 0.4325173387112976		[learning rate: 0.00018603]
	Learning Rate: 0.000186028
	LOSS [training: 0.4415884225213169 | validation: 0.3275524632684988]
	TIME [epoch: 8.41 sec]
EPOCH 873/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44930111897288133		[learning rate: 0.00018558]
		[batch 20/20] avg loss: 0.4325417470849916		[learning rate: 0.00018513]
	Learning Rate: 0.000185129
	LOSS [training: 0.44092143302893644 | validation: 0.32048506607107774]
	TIME [epoch: 8.41 sec]
EPOCH 874/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4257675260249577		[learning rate: 0.00018468]
		[batch 20/20] avg loss: 0.43593817518351574		[learning rate: 0.00018423]
	Learning Rate: 0.000184233
	LOSS [training: 0.4308528506042367 | validation: 0.3429438554297797]
	TIME [epoch: 8.39 sec]
EPOCH 875/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4258065727866536		[learning rate: 0.00018379]
		[batch 20/20] avg loss: 0.4134293505102148		[learning rate: 0.00018334]
	Learning Rate: 0.000183343
	LOSS [training: 0.4196179616484342 | validation: 0.3406657362132862]
	TIME [epoch: 8.39 sec]
EPOCH 876/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42601948504541715		[learning rate: 0.0001829]
		[batch 20/20] avg loss: 0.4163020656823718		[learning rate: 0.00018246]
	Learning Rate: 0.000182456
	LOSS [training: 0.4211607753638945 | validation: 0.33480851734038053]
	TIME [epoch: 8.4 sec]
EPOCH 877/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.41844912006461277		[learning rate: 0.00018201]
		[batch 20/20] avg loss: 0.4161360467500758		[learning rate: 0.00018157]
	Learning Rate: 0.000181574
	LOSS [training: 0.41729258340734426 | validation: 0.3049474507790685]
	TIME [epoch: 8.38 sec]
EPOCH 878/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.42090098179991503		[learning rate: 0.00018113]
		[batch 20/20] avg loss: 0.45697955525282163		[learning rate: 0.0001807]
	Learning Rate: 0.000180696
	LOSS [training: 0.43894026852636836 | validation: 0.3825747171988551]
	TIME [epoch: 8.42 sec]
EPOCH 879/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.44084422176938143		[learning rate: 0.00018026]
		[batch 20/20] avg loss: 0.4924023275691184		[learning rate: 0.00017982]
	Learning Rate: 0.000179822
	LOSS [training: 0.4666232746692499 | validation: 0.47862849749581127]
	TIME [epoch: 8.4 sec]
EPOCH 880/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5597682162120391		[learning rate: 0.00017939]
		[batch 20/20] avg loss: 0.5471903865146162		[learning rate: 0.00017895]
	Learning Rate: 0.000178952
	LOSS [training: 0.5534793013633277 | validation: 0.41945282360563346]
	TIME [epoch: 8.38 sec]
EPOCH 881/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4757100106479455		[learning rate: 0.00017852]
		[batch 20/20] avg loss: 0.4697613612135691		[learning rate: 0.00017809]
	Learning Rate: 0.000178087
	LOSS [training: 0.4727356859307573 | validation: 0.43175528772828214]
	TIME [epoch: 8.4 sec]
EPOCH 882/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.4914219469234875		[learning rate: 0.00017766]
		[batch 20/20] avg loss: 0.551639645343939		[learning rate: 0.00017723]
	Learning Rate: 0.000177226
	LOSS [training: 0.5215307961337133 | validation: 0.5807301964885568]
	TIME [epoch: 8.41 sec]
EPOCH 883/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7447558612436554		[learning rate: 0.0001768]
		[batch 20/20] avg loss: 0.6528088520092622		[learning rate: 0.00017637]
	Learning Rate: 0.000176369
	LOSS [training: 0.6987823566264588 | validation: 0.7439427332750147]
	TIME [epoch: 8.38 sec]
EPOCH 884/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8624552560854836		[learning rate: 0.00017594]
		[batch 20/20] avg loss: 0.9572969353952455		[learning rate: 0.00017552]
	Learning Rate: 0.000175516
	LOSS [training: 0.9098760957403644 | validation: 0.8087331864161383]
	TIME [epoch: 8.4 sec]
EPOCH 885/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9390610268096473		[learning rate: 0.00017509]
		[batch 20/20] avg loss: 0.9637072458217004		[learning rate: 0.00017467]
	Learning Rate: 0.000174667
	LOSS [training: 0.951384136315674 | validation: 0.8790478105391386]
	TIME [epoch: 8.38 sec]
EPOCH 886/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9069773964637038		[learning rate: 0.00017424]
		[batch 20/20] avg loss: 0.7639819808046098		[learning rate: 0.00017382]
	Learning Rate: 0.000173822
	LOSS [training: 0.8354796886341568 | validation: 0.8127268450459839]
	TIME [epoch: 8.4 sec]
EPOCH 887/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7845037060940846		[learning rate: 0.0001734]
		[batch 20/20] avg loss: 0.6138855636053279		[learning rate: 0.00017298]
	Learning Rate: 0.000172982
	LOSS [training: 0.6991946348497062 | validation: 0.6578767014920932]
	TIME [epoch: 8.41 sec]
EPOCH 888/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.8163977092912225		[learning rate: 0.00017256]
		[batch 20/20] avg loss: 0.9547237717437327		[learning rate: 0.00017215]
	Learning Rate: 0.000172145
	LOSS [training: 0.8855607405174777 | validation: 0.9121576499761798]
	TIME [epoch: 8.37 sec]
EPOCH 889/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9509879282103902		[learning rate: 0.00017173]
		[batch 20/20] avg loss: 1.099386575688775		[learning rate: 0.00017131]
	Learning Rate: 0.000171313
	LOSS [training: 1.0251872519495828 | validation: 1.046879459358478]
	TIME [epoch: 8.39 sec]
EPOCH 890/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1353665112466094		[learning rate: 0.0001709]
		[batch 20/20] avg loss: 1.0679271358792444		[learning rate: 0.00017048]
	Learning Rate: 0.000170484
	LOSS [training: 1.1016468235629269 | validation: 0.9720506962001731]
	TIME [epoch: 8.41 sec]
EPOCH 891/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0703311581009884		[learning rate: 0.00017007]
		[batch 20/20] avg loss: 0.826712431319713		[learning rate: 0.00016966]
	Learning Rate: 0.00016966
	LOSS [training: 0.9485217947103506 | validation: 0.6066616572712411]
	TIME [epoch: 8.41 sec]
EPOCH 892/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6462581549424564		[learning rate: 0.00016925]
		[batch 20/20] avg loss: 0.7406605601013825		[learning rate: 0.00016884]
	Learning Rate: 0.000168839
	LOSS [training: 0.6934593575219197 | validation: 0.7184011108614268]
	TIME [epoch: 8.39 sec]
EPOCH 893/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.878034859576821		[learning rate: 0.00016843]
		[batch 20/20] avg loss: 0.890715796292327		[learning rate: 0.00016802]
	Learning Rate: 0.000168023
	LOSS [training: 0.8843753279345739 | validation: 0.7850030387028684]
	TIME [epoch: 8.4 sec]
EPOCH 894/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.864643405390886		[learning rate: 0.00016762]
		[batch 20/20] avg loss: 0.942874182220903		[learning rate: 0.00016721]
	Learning Rate: 0.00016721
	LOSS [training: 0.9037587938058944 | validation: 0.9285159198265185]
	TIME [epoch: 8.38 sec]
EPOCH 895/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9516946853720208		[learning rate: 0.00016681]
		[batch 20/20] avg loss: 0.9537626894419823		[learning rate: 0.0001664]
	Learning Rate: 0.000166402
	LOSS [training: 0.9527286874070015 | validation: 0.7797157208265266]
	TIME [epoch: 8.41 sec]
EPOCH 896/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.873131357364478		[learning rate: 0.000166]
		[batch 20/20] avg loss: 0.9396245422424399		[learning rate: 0.0001656]
	Learning Rate: 0.000165597
	LOSS [training: 0.9063779498034586 | validation: 0.8492742196706822]
	TIME [epoch: 8.4 sec]
EPOCH 897/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0045305178650916		[learning rate: 0.0001652]
		[batch 20/20] avg loss: 1.0752484435650165		[learning rate: 0.0001648]
	Learning Rate: 0.000164796
	LOSS [training: 1.039889480715054 | validation: 1.0235431519905027]
	TIME [epoch: 8.37 sec]
EPOCH 898/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.171622384026174		[learning rate: 0.0001644]
		[batch 20/20] avg loss: 1.2050394121011583		[learning rate: 0.000164]
	Learning Rate: 0.000163999
	LOSS [training: 1.188330898063666 | validation: 1.1956180341213019]
	TIME [epoch: 8.38 sec]
EPOCH 899/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.2969641319168814		[learning rate: 0.0001636]
		[batch 20/20] avg loss: 1.4717767122992833		[learning rate: 0.00016321]
	Learning Rate: 0.000163206
	LOSS [training: 1.3843704221080821 | validation: 1.3424250221447074]
	TIME [epoch: 8.41 sec]
EPOCH 900/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4282013524456176		[learning rate: 0.00016281]
		[batch 20/20] avg loss: 1.5259376886775515		[learning rate: 0.00016242]
	Learning Rate: 0.000162417
	LOSS [training: 1.477069520561585 | validation: 1.3709031643152434]
	TIME [epoch: 8.38 sec]
EPOCH 901/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5503324903174582		[learning rate: 0.00016202]
		[batch 20/20] avg loss: 1.593421679397799		[learning rate: 0.00016163]
	Learning Rate: 0.000161632
	LOSS [training: 1.571877084857629 | validation: 1.5006995954903566]
	TIME [epoch: 8.38 sec]
EPOCH 902/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.606093441721271		[learning rate: 0.00016124]
		[batch 20/20] avg loss: 1.6118030909595003		[learning rate: 0.00016085]
	Learning Rate: 0.00016085
	LOSS [training: 1.6089482663403856 | validation: 1.395360131995882]
	TIME [epoch: 8.4 sec]
EPOCH 903/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5347069758795926		[learning rate: 0.00016046]
		[batch 20/20] avg loss: 1.5408276907769707		[learning rate: 0.00016007]
	Learning Rate: 0.000160072
	LOSS [training: 1.5377673333282815 | validation: 1.4832417237681828]
	TIME [epoch: 8.38 sec]
EPOCH 904/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7603207756623664		[learning rate: 0.00015968]
		[batch 20/20] avg loss: 1.6909388544408888		[learning rate: 0.0001593]
	Learning Rate: 0.000159298
	LOSS [training: 1.7256298150516276 | validation: 1.5404368912222657]
	TIME [epoch: 8.4 sec]
EPOCH 905/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7133625030373676		[learning rate: 0.00015891]
		[batch 20/20] avg loss: 1.7197458880964505		[learning rate: 0.00015853]
	Learning Rate: 0.000158528
	LOSS [training: 1.7165541955669092 | validation: 1.595709885572773]
	TIME [epoch: 8.39 sec]
EPOCH 906/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7333825864704366		[learning rate: 0.00015814]
		[batch 20/20] avg loss: 1.6897243346487922		[learning rate: 0.00015776]
	Learning Rate: 0.000157761
	LOSS [training: 1.7115534605596145 | validation: 1.5863497327630567]
	TIME [epoch: 8.39 sec]
EPOCH 907/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6783042923118345		[learning rate: 0.00015738]
		[batch 20/20] avg loss: 1.8481480822942398		[learning rate: 0.000157]
	Learning Rate: 0.000156998
	LOSS [training: 1.7632261873030373 | validation: 1.716666827734564]
	TIME [epoch: 8.37 sec]
EPOCH 908/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9050458150895129		[learning rate: 0.00015662]
		[batch 20/20] avg loss: 1.7549511519022445		[learning rate: 0.00015624]
	Learning Rate: 0.000156239
	LOSS [training: 1.8299984834958785 | validation: 1.612134511819853]
	TIME [epoch: 8.41 sec]
EPOCH 909/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.6894129956362307		[learning rate: 0.00015586]
		[batch 20/20] avg loss: 1.800509772574975		[learning rate: 0.00015548]
	Learning Rate: 0.000155483
	LOSS [training: 1.7449613841056029 | validation: 1.580104247965325]
	TIME [epoch: 8.4 sec]
EPOCH 910/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.672263078580945		[learning rate: 0.00015511]
		[batch 20/20] avg loss: 1.765188036049635		[learning rate: 0.00015473]
	Learning Rate: 0.000154732
	LOSS [training: 1.71872555731529 | validation: 1.5674304314301262]
	TIME [epoch: 8.37 sec]
EPOCH 911/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.830605749968253		[learning rate: 0.00015436]
		[batch 20/20] avg loss: 1.8795123467516206		[learning rate: 0.00015398]
	Learning Rate: 0.000153983
	LOSS [training: 1.8550590483599372 | validation: 1.902688430261588]
	TIME [epoch: 8.37 sec]
EPOCH 912/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.070258682764422		[learning rate: 0.00015361]
		[batch 20/20] avg loss: 2.2533599301023624		[learning rate: 0.00015324]
	Learning Rate: 0.000153239
	LOSS [training: 2.1618093064333923 | validation: 2.057217874182606]
	TIME [epoch: 8.42 sec]
EPOCH 913/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.146683121139099		[learning rate: 0.00015287]
		[batch 20/20] avg loss: 2.2647717676807395		[learning rate: 0.0001525]
	Learning Rate: 0.000152498
	LOSS [training: 2.205727444409919 | validation: 2.0115349371190625]
	TIME [epoch: 8.38 sec]
EPOCH 914/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.169386311731369		[learning rate: 0.00015213]
		[batch 20/20] avg loss: 2.1680144033701736		[learning rate: 0.00015176]
	Learning Rate: 0.00015176
	LOSS [training: 2.1687003575507715 | validation: 1.91828483150685]
	TIME [epoch: 8.38 sec]
EPOCH 915/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1655365533956124		[learning rate: 0.00015139]
		[batch 20/20] avg loss: 2.0277564228545506		[learning rate: 0.00015103]
	Learning Rate: 0.000151026
	LOSS [training: 2.096646488125082 | validation: 1.8841364435038872]
	TIME [epoch: 8.39 sec]
EPOCH 916/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.0833879837220985		[learning rate: 0.00015066]
		[batch 20/20] avg loss: 2.0537517657933444		[learning rate: 0.0001503]
	Learning Rate: 0.000150296
	LOSS [training: 2.0685698747577215 | validation: 1.8094713051723474]
	TIME [epoch: 8.38 sec]
EPOCH 917/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.9102723188603172		[learning rate: 0.00014993]
		[batch 20/20] avg loss: 2.0809569545636384		[learning rate: 0.00014957]
	Learning Rate: 0.000149569
	LOSS [training: 1.9956146367119778 | validation: 1.8370521771129322]
	TIME [epoch: 8.4 sec]
EPOCH 918/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.051722918414994		[learning rate: 0.00014921]
		[batch 20/20] avg loss: 2.2762048657709393		[learning rate: 0.00014885]
	Learning Rate: 0.000148846
	LOSS [training: 2.1639638920929665 | validation: 2.0329413042337845]
	TIME [epoch: 8.39 sec]
EPOCH 919/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.2383996887225033		[learning rate: 0.00014849]
		[batch 20/20] avg loss: 2.345119675634219		[learning rate: 0.00014813]
	Learning Rate: 0.000148126
	LOSS [training: 2.2917596821783612 | validation: 2.1584316321006827]
	TIME [epoch: 8.39 sec]
EPOCH 920/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.4308435987550414		[learning rate: 0.00014777]
		[batch 20/20] avg loss: 2.2967772092475482		[learning rate: 0.00014741]
	Learning Rate: 0.00014741
	LOSS [training: 2.3638104040012946 | validation: 2.163806177807788]
	TIME [epoch: 8.38 sec]
EPOCH 921/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.4068472234492395		[learning rate: 0.00014705]
		[batch 20/20] avg loss: 2.2112389702333237		[learning rate: 0.0001467]
	Learning Rate: 0.000146697
	LOSS [training: 2.3090430968412816 | validation: 2.158739508914798]
	TIME [epoch: 8.41 sec]
EPOCH 922/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.252315195254519		[learning rate: 0.00014634]
		[batch 20/20] avg loss: 2.3006580308605784		[learning rate: 0.00014599]
	Learning Rate: 0.000145988
	LOSS [training: 2.276486613057549 | validation: 2.0731537840579306]
	TIME [epoch: 8.4 sec]
EPOCH 923/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.191922181293877		[learning rate: 0.00014563]
		[batch 20/20] avg loss: 2.2319160462417083		[learning rate: 0.00014528]
	Learning Rate: 0.000145282
	LOSS [training: 2.2119191137677916 | validation: 2.108297282896162]
	TIME [epoch: 8.38 sec]
EPOCH 924/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3010306293083183		[learning rate: 0.00014493]
		[batch 20/20] avg loss: 2.3022254334026657		[learning rate: 0.00014458]
	Learning Rate: 0.000144579
	LOSS [training: 2.301628031355492 | validation: 2.1468662198636053]
	TIME [epoch: 8.38 sec]
EPOCH 925/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.35834234607074		[learning rate: 0.00014423]
		[batch 20/20] avg loss: 2.415305076650007		[learning rate: 0.00014388]
	Learning Rate: 0.00014388
	LOSS [training: 2.3868237113603743 | validation: 2.140239096135088]
	TIME [epoch: 8.41 sec]
EPOCH 926/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3032562697996632		[learning rate: 0.00014353]
		[batch 20/20] avg loss: 2.1469307054079323		[learning rate: 0.00014318]
	Learning Rate: 0.000143184
	LOSS [training: 2.225093487603798 | validation: 2.0269865082863383]
	TIME [epoch: 8.39 sec]
EPOCH 927/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.0566214868736608		[learning rate: 0.00014284]
		[batch 20/20] avg loss: 2.1648238348724917		[learning rate: 0.00014249]
	Learning Rate: 0.000142492
	LOSS [training: 2.1107226608730754 | validation: 1.941307033209036]
	TIME [epoch: 8.38 sec]
EPOCH 928/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.184780589200318		[learning rate: 0.00014215]
		[batch 20/20] avg loss: 2.2396207663360235		[learning rate: 0.0001418]
	Learning Rate: 0.000141803
	LOSS [training: 2.21220067776817 | validation: 2.121465844247562]
	TIME [epoch: 8.39 sec]
EPOCH 929/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1810822763004554		[learning rate: 0.00014146]
		[batch 20/20] avg loss: 2.544849395743049		[learning rate: 0.00014112]
	Learning Rate: 0.000141117
	LOSS [training: 2.3629658360217523 | validation: 2.3377962562441557]
	TIME [epoch: 8.39 sec]
EPOCH 930/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.439780124627588		[learning rate: 0.00014078]
		[batch 20/20] avg loss: 2.213439564402572		[learning rate: 0.00014043]
	Learning Rate: 0.000140434
	LOSS [training: 2.3266098445150796 | validation: 2.132501330292099]
	TIME [epoch: 8.4 sec]
EPOCH 931/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.358521840452822		[learning rate: 0.00014009]
		[batch 20/20] avg loss: 2.30945952031417		[learning rate: 0.00013976]
	Learning Rate: 0.000139755
	LOSS [training: 2.3339906803834958 | validation: 2.2362012896547308]
	TIME [epoch: 8.38 sec]
EPOCH 932/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3972599643832035		[learning rate: 0.00013942]
		[batch 20/20] avg loss: 2.444225574635353		[learning rate: 0.00013908]
	Learning Rate: 0.00013908
	LOSS [training: 2.420742769509278 | validation: 2.374921374154688]
	TIME [epoch: 8.4 sec]
EPOCH 933/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.5057031188226846		[learning rate: 0.00013874]
		[batch 20/20] avg loss: 2.4713503920660878		[learning rate: 0.00013841]
	Learning Rate: 0.000138407
	LOSS [training: 2.488526755444386 | validation: 2.506801172062611]
	TIME [epoch: 8.38 sec]
EPOCH 934/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.4826075749583483		[learning rate: 0.00013807]
		[batch 20/20] avg loss: 2.5577082626767913		[learning rate: 0.00013774]
	Learning Rate: 0.000137738
	LOSS [training: 2.52015791881757 | validation: 2.4355426138237686]
	TIME [epoch: 8.4 sec]
EPOCH 935/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.514277132500988		[learning rate: 0.0001374]
		[batch 20/20] avg loss: 2.3723321110125926		[learning rate: 0.00013707]
	Learning Rate: 0.000137072
	LOSS [training: 2.4433046217567904 | validation: 2.3450821968342805]
	TIME [epoch: 8.4 sec]
EPOCH 936/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.34870782024047		[learning rate: 0.00013674]
		[batch 20/20] avg loss: 2.339992900899109		[learning rate: 0.00013641]
	Learning Rate: 0.000136409
	LOSS [training: 2.3443503605697895 | validation: 2.2395930151587304]
	TIME [epoch: 8.39 sec]
EPOCH 937/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.048860227901889		[learning rate: 0.00013608]
		[batch 20/20] avg loss: 2.2269090271376113		[learning rate: 0.00013575]
	Learning Rate: 0.000135749
	LOSS [training: 2.13788462751975 | validation: 2.2221726653813354]
	TIME [epoch: 8.38 sec]
EPOCH 938/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.2428739652650904		[learning rate: 0.00013542]
		[batch 20/20] avg loss: 2.327416695454377		[learning rate: 0.00013509]
	Learning Rate: 0.000135093
	LOSS [training: 2.2851453303597333 | validation: 2.31487348374865]
	TIME [epoch: 8.4 sec]
EPOCH 939/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.3504759726895905		[learning rate: 0.00013477]
		[batch 20/20] avg loss: 2.2645754977429595		[learning rate: 0.00013444]
	Learning Rate: 0.000134439
	LOSS [training: 2.3075257352162746 | validation: 2.1534272753883092]
	TIME [epoch: 8.4 sec]
EPOCH 940/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.42671916402776		[learning rate: 0.00013411]
		[batch 20/20] avg loss: 2.160809249228109		[learning rate: 0.00013379]
	Learning Rate: 0.000133789
	LOSS [training: 2.2937642066279342 | validation: 2.072391657707618]
	TIME [epoch: 8.4 sec]
EPOCH 941/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.1883293814410996		[learning rate: 0.00013347]
		[batch 20/20] avg loss: 2.314381788267866		[learning rate: 0.00013314]
	Learning Rate: 0.000133142
	LOSS [training: 2.2513555848544833 | validation: 2.0988195001248355]
	TIME [epoch: 8.37 sec]
EPOCH 942/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.239954861599303		[learning rate: 0.00013282]
		[batch 20/20] avg loss: 2.2014453495719257		[learning rate: 0.0001325]
	Learning Rate: 0.000132498
	LOSS [training: 2.220700105585615 | validation: 2.109737479517251]
	TIME [epoch: 8.39 sec]
EPOCH 943/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.221867446639476		[learning rate: 0.00013218]
		[batch 20/20] avg loss: 2.3190188965472407		[learning rate: 0.00013186]
	Learning Rate: 0.000131858
	LOSS [training: 2.270443171593359 | validation: 2.183494201317939]
	TIME [epoch: 8.41 sec]
EPOCH 944/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.333909275405925		[learning rate: 0.00013154]
		[batch 20/20] avg loss: 2.356595946768543		[learning rate: 0.00013122]
	Learning Rate: 0.00013122
	LOSS [training: 2.3452526110872336 | validation: 2.509855473532342]
	TIME [epoch: 8.38 sec]
EPOCH 945/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.229887694206505		[learning rate: 0.0001309]
		[batch 20/20] avg loss: 2.3025498685944217		[learning rate: 0.00013059]
	Learning Rate: 0.000130585
	LOSS [training: 2.2662187814004633 | validation: 2.4070557175459477]
	TIME [epoch: 8.38 sec]
EPOCH 946/1000:
	Training over batches...
		[batch 10/20] avg loss: 2.092119569223968		[learning rate: 0.00013027]
		[batch 20/20] avg loss: 1.9863357743726529		[learning rate: 0.00012995]
	Learning Rate: 0.000129954
	LOSS [training: 2.0392276717983107 | validation: 1.8923436113719991]
	TIME [epoch: 8.4 sec]
EPOCH 947/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7839933296927069		[learning rate: 0.00012964]
		[batch 20/20] avg loss: 2.038540243606392		[learning rate: 0.00012933]
	Learning Rate: 0.000129326
	LOSS [training: 1.9112667866495496 | validation: 1.8932270497024861]
	TIME [epoch: 8.4 sec]
EPOCH 948/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.854797402862521		[learning rate: 0.00012901]
		[batch 20/20] avg loss: 1.8473926177318618		[learning rate: 0.0001287]
	Learning Rate: 0.0001287
	LOSS [training: 1.8510950102971915 | validation: 1.7066079329460377]
	TIME [epoch: 8.39 sec]
EPOCH 949/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.756004455869887		[learning rate: 0.00012839]
		[batch 20/20] avg loss: 1.6971394353305365		[learning rate: 0.00012808]
	Learning Rate: 0.000128078
	LOSS [training: 1.7265719456002118 | validation: 1.6515072024113646]
	TIME [epoch: 8.4 sec]
EPOCH 950/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7629174622653814		[learning rate: 0.00012777]
		[batch 20/20] avg loss: 1.6065772430362792		[learning rate: 0.00012746]
	Learning Rate: 0.000127458
	LOSS [training: 1.6847473526508299 | validation: 1.621914628147783]
	TIME [epoch: 8.38 sec]
EPOCH 951/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8632997461243908		[learning rate: 0.00012715]
		[batch 20/20] avg loss: 1.8870911358272457		[learning rate: 0.00012684]
	Learning Rate: 0.000126842
	LOSS [training: 1.8751954409758185 | validation: 1.8311824957366185]
	TIME [epoch: 8.42 sec]
EPOCH 952/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.8746164896378903		[learning rate: 0.00012653]
		[batch 20/20] avg loss: 1.6987177012285994		[learning rate: 0.00012623]
	Learning Rate: 0.000126229
	LOSS [training: 1.7866670954332449 | validation: 1.647550014693093]
	TIME [epoch: 8.39 sec]
EPOCH 953/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.7252935605401756		[learning rate: 0.00012592]
		[batch 20/20] avg loss: 1.4935181844143344		[learning rate: 0.00012562]
	Learning Rate: 0.000125618
	LOSS [training: 1.609405872477255 | validation: 1.4240555011427136]
	TIME [epoch: 8.37 sec]
EPOCH 954/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.529639961775308		[learning rate: 0.00012531]
		[batch 20/20] avg loss: 1.5142537734437596		[learning rate: 0.00012501]
	Learning Rate: 0.000125011
	LOSS [training: 1.5219468676095338 | validation: 1.4829949101957969]
	TIME [epoch: 8.39 sec]
EPOCH 955/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5095154270984341		[learning rate: 0.00012471]
		[batch 20/20] avg loss: 1.6635546473463587		[learning rate: 0.00012441]
	Learning Rate: 0.000124406
	LOSS [training: 1.5865350372223965 | validation: 1.61264746752475]
	TIME [epoch: 8.41 sec]
EPOCH 956/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4999309409273336		[learning rate: 0.00012411]
		[batch 20/20] avg loss: 1.5501169371777694		[learning rate: 0.0001238]
	Learning Rate: 0.000123805
	LOSS [training: 1.5250239390525513 | validation: 1.6129471041881045]
	TIME [epoch: 8.39 sec]
EPOCH 957/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.5467247318978548		[learning rate: 0.0001235]
		[batch 20/20] avg loss: 1.4896055735466245		[learning rate: 0.00012321]
	Learning Rate: 0.000123206
	LOSS [training: 1.5181651527222395 | validation: 1.5546281392610908]
	TIME [epoch: 8.39 sec]
EPOCH 958/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.456626666880051		[learning rate: 0.00012291]
		[batch 20/20] avg loss: 1.4904534714980673		[learning rate: 0.00012261]
	Learning Rate: 0.00012261
	LOSS [training: 1.4735400691890588 | validation: 1.5688711939091136]
	TIME [epoch: 8.38 sec]
EPOCH 959/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.406593614822457		[learning rate: 0.00012231]
		[batch 20/20] avg loss: 1.279768915150852		[learning rate: 0.00012202]
	Learning Rate: 0.000122017
	LOSS [training: 1.3431812649866544 | validation: 1.3495430113301965]
	TIME [epoch: 8.38 sec]
EPOCH 960/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1011772393798496		[learning rate: 0.00012172]
		[batch 20/20] avg loss: 1.0997790599273778		[learning rate: 0.00012143]
	Learning Rate: 0.000121427
	LOSS [training: 1.1004781496536138 | validation: 1.0152072759239765]
	TIME [epoch: 8.42 sec]
EPOCH 961/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.9808614979326379		[learning rate: 0.00012113]
		[batch 20/20] avg loss: 0.8384760311213393		[learning rate: 0.00012084]
	Learning Rate: 0.00012084
	LOSS [training: 0.9096687645269885 | validation: 0.8113640418752825]
	TIME [epoch: 8.37 sec]
EPOCH 962/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.829274915767417		[learning rate: 0.00012055]
		[batch 20/20] avg loss: 0.6949926981494648		[learning rate: 0.00012026]
	Learning Rate: 0.000120256
	LOSS [training: 0.7621338069584407 | validation: 0.6168966301440051]
	TIME [epoch: 8.37 sec]
EPOCH 963/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6345379468845307		[learning rate: 0.00011996]
		[batch 20/20] avg loss: 0.6050963394937398		[learning rate: 0.00011967]
	Learning Rate: 0.000119674
	LOSS [training: 0.6198171431891353 | validation: 0.5160149384483155]
	TIME [epoch: 8.39 sec]
EPOCH 964/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5646843777964461		[learning rate: 0.00011938]
		[batch 20/20] avg loss: 0.5695813549538338		[learning rate: 0.0001191]
	Learning Rate: 0.000119095
	LOSS [training: 0.5671328663751399 | validation: 0.4913381320426454]
	TIME [epoch: 8.42 sec]
EPOCH 965/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.579749987539719		[learning rate: 0.00011881]
		[batch 20/20] avg loss: 0.5626985611409234		[learning rate: 0.00011852]
	Learning Rate: 0.000118519
	LOSS [training: 0.5712242743403211 | validation: 0.5133215651185845]
	TIME [epoch: 8.37 sec]
EPOCH 966/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6365272198386502		[learning rate: 0.00011823]
		[batch 20/20] avg loss: 0.7590203023839446		[learning rate: 0.00011795]
	Learning Rate: 0.000117946
	LOSS [training: 0.6977737611112973 | validation: 0.6692702002726287]
	TIME [epoch: 8.38 sec]
EPOCH 967/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6886893199222051		[learning rate: 0.00011766]
		[batch 20/20] avg loss: 0.5684739703927628		[learning rate: 0.00011738]
	Learning Rate: 0.000117376
	LOSS [training: 0.6285816451574839 | validation: 0.4823536344911135]
	TIME [epoch: 8.39 sec]
EPOCH 968/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5481054354458064		[learning rate: 0.00011709]
		[batch 20/20] avg loss: 0.5419785499670995		[learning rate: 0.00011681]
	Learning Rate: 0.000116808
	LOSS [training: 0.545041992706453 | validation: 0.4499648876319028]
	TIME [epoch: 8.4 sec]
EPOCH 969/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5278419478492642		[learning rate: 0.00011653]
		[batch 20/20] avg loss: 0.5363664983825396		[learning rate: 0.00011624]
	Learning Rate: 0.000116243
	LOSS [training: 0.5321042231159019 | validation: 0.4495302577114555]
	TIME [epoch: 8.38 sec]
EPOCH 970/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5312363862233591		[learning rate: 0.00011596]
		[batch 20/20] avg loss: 0.551053219448759		[learning rate: 0.00011568]
	Learning Rate: 0.000115681
	LOSS [training: 0.5411448028360589 | validation: 0.5331985008224474]
	TIME [epoch: 8.38 sec]
EPOCH 971/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6275252700902161		[learning rate: 0.0001154]
		[batch 20/20] avg loss: 0.6895531693155063		[learning rate: 0.00011512]
	Learning Rate: 0.000115122
	LOSS [training: 0.6585392197028611 | validation: 0.6459140134809409]
	TIME [epoch: 8.39 sec]
EPOCH 972/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6867707388632536		[learning rate: 0.00011484]
		[batch 20/20] avg loss: 0.6945076012104658		[learning rate: 0.00011457]
	Learning Rate: 0.000114565
	LOSS [training: 0.6906391700368598 | validation: 0.6687163705234906]
	TIME [epoch: 8.39 sec]
EPOCH 973/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7379685955954196		[learning rate: 0.00011429]
		[batch 20/20] avg loss: 0.6711237419622862		[learning rate: 0.00011401]
	Learning Rate: 0.000114011
	LOSS [training: 0.704546168778853 | validation: 0.548261702953937]
	TIME [epoch: 8.4 sec]
EPOCH 974/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6168717001896474		[learning rate: 0.00011374]
		[batch 20/20] avg loss: 0.6153351682039714		[learning rate: 0.00011346]
	Learning Rate: 0.00011346
	LOSS [training: 0.6161034341968092 | validation: 0.5190632779313995]
	TIME [epoch: 8.38 sec]
EPOCH 975/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6171542630708822		[learning rate: 0.00011319]
		[batch 20/20] avg loss: 0.6024991146657696		[learning rate: 0.00011291]
	Learning Rate: 0.000112911
	LOSS [training: 0.6098266888683259 | validation: 0.537316875881492]
	TIME [epoch: 8.39 sec]
EPOCH 976/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6169786987762516		[learning rate: 0.00011264]
		[batch 20/20] avg loss: 0.6228267299623672		[learning rate: 0.00011237]
	Learning Rate: 0.000112365
	LOSS [training: 0.6199027143693094 | validation: 0.5455155051189107]
	TIME [epoch: 8.39 sec]
EPOCH 977/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5623125757600741		[learning rate: 0.00011209]
		[batch 20/20] avg loss: 0.6576132705432902		[learning rate: 0.00011182]
	Learning Rate: 0.000111822
	LOSS [training: 0.6099629231516822 | validation: 0.5126529798781215]
	TIME [epoch: 8.4 sec]
EPOCH 978/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5978633666739045		[learning rate: 0.00011155]
		[batch 20/20] avg loss: 0.5970267883551765		[learning rate: 0.00011128]
	Learning Rate: 0.000111281
	LOSS [training: 0.5974450775145403 | validation: 0.5371990467379503]
	TIME [epoch: 8.38 sec]
EPOCH 979/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.5975671745438162		[learning rate: 0.00011101]
		[batch 20/20] avg loss: 0.6676589700645843		[learning rate: 0.00011074]
	Learning Rate: 0.000110743
	LOSS [training: 0.6326130723042003 | validation: 0.5741576518826441]
	TIME [epoch: 8.4 sec]
EPOCH 980/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.700280557740579		[learning rate: 0.00011047]
		[batch 20/20] avg loss: 0.6991089379854765		[learning rate: 0.00011021]
	Learning Rate: 0.000110207
	LOSS [training: 0.6996947478630278 | validation: 0.6810976729565621]
	TIME [epoch: 8.39 sec]
EPOCH 981/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7400624758740919		[learning rate: 0.00010994]
		[batch 20/20] avg loss: 0.6912003603973476		[learning rate: 0.00010967]
	Learning Rate: 0.000109674
	LOSS [training: 0.7156314181357197 | validation: 0.5574458430224921]
	TIME [epoch: 8.4 sec]
EPOCH 982/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.640899769109606		[learning rate: 0.00010941]
		[batch 20/20] avg loss: 0.6291351758829155		[learning rate: 0.00010914]
	Learning Rate: 0.000109144
	LOSS [training: 0.6350174724962608 | validation: 0.5040767508705875]
	TIME [epoch: 8.39 sec]
EPOCH 983/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.6499781570332641		[learning rate: 0.00010888]
		[batch 20/20] avg loss: 0.7104472615526812		[learning rate: 0.00010862]
	Learning Rate: 0.000108616
	LOSS [training: 0.6802127092929726 | validation: 0.6797341913359538]
	TIME [epoch: 8.39 sec]
EPOCH 984/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7885589498378703		[learning rate: 0.00010835]
		[batch 20/20] avg loss: 0.7387719084096653		[learning rate: 0.00010809]
	Learning Rate: 0.000108091
	LOSS [training: 0.7636654291237679 | validation: 0.6715263227514048]
	TIME [epoch: 8.4 sec]
EPOCH 985/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.771363259698916		[learning rate: 0.00010783]
		[batch 20/20] avg loss: 0.7525780436238768		[learning rate: 0.00010757]
	Learning Rate: 0.000107568
	LOSS [training: 0.7619706516613963 | validation: 0.6750115787061484]
	TIME [epoch: 8.38 sec]
EPOCH 986/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.740488608666006		[learning rate: 0.00010731]
		[batch 20/20] avg loss: 0.7675332106183302		[learning rate: 0.00010705]
	Learning Rate: 0.000107048
	LOSS [training: 0.7540109096421681 | validation: 0.70971437454775]
	TIME [epoch: 8.41 sec]
EPOCH 987/1000:
	Training over batches...
		[batch 10/20] avg loss: 0.7669506598670564		[learning rate: 0.00010679]
		[batch 20/20] avg loss: 0.9043610606851329		[learning rate: 0.00010653]
	Learning Rate: 0.00010653
	LOSS [training: 0.8356558602760948 | validation: 0.9132612137908835]
	TIME [epoch: 8.39 sec]
EPOCH 988/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.0081220684593788		[learning rate: 0.00010627]
		[batch 20/20] avg loss: 1.1573639949284869		[learning rate: 0.00010602]
	Learning Rate: 0.000106015
	LOSS [training: 1.0827430316939328 | validation: 0.99155697102228]
	TIME [epoch: 8.4 sec]
EPOCH 989/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.110335230391567		[learning rate: 0.00010576]
		[batch 20/20] avg loss: 1.2537497099235346		[learning rate: 0.0001055]
	Learning Rate: 0.000105503
	LOSS [training: 1.1820424701575507 | validation: 1.0294189680888062]
	TIME [epoch: 8.38 sec]
EPOCH 990/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1036335518915146		[learning rate: 0.00010525]
		[batch 20/20] avg loss: 1.1487844918836299		[learning rate: 0.00010499]
	Learning Rate: 0.000104992
	LOSS [training: 1.1262090218875724 | validation: 1.07199077344123]
	TIME [epoch: 8.41 sec]
EPOCH 991/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.172406448992044		[learning rate: 0.00010474]
		[batch 20/20] avg loss: 1.3320047999802274		[learning rate: 0.00010448]
	Learning Rate: 0.000104485
	LOSS [training: 1.2522056244861353 | validation: 1.1969971454060868]
	TIME [epoch: 8.4 sec]
EPOCH 992/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3973732884030696		[learning rate: 0.00010423]
		[batch 20/20] avg loss: 1.2279355783599513		[learning rate: 0.00010398]
	Learning Rate: 0.000103979
	LOSS [training: 1.3126544333815104 | validation: 1.192305000979464]
	TIME [epoch: 8.4 sec]
EPOCH 993/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3256268808669391		[learning rate: 0.00010373]
		[batch 20/20] avg loss: 1.1312956749140415		[learning rate: 0.00010348]
	Learning Rate: 0.000103477
	LOSS [training: 1.2284612778904902 | validation: 1.1221007018859834]
	TIME [epoch: 8.38 sec]
EPOCH 994/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.1295276072519613		[learning rate: 0.00010323]
		[batch 20/20] avg loss: 1.2536139044685202		[learning rate: 0.00010298]
	Learning Rate: 0.000102976
	LOSS [training: 1.1915707558602409 | validation: 1.1755094473321739]
	TIME [epoch: 8.37 sec]
EPOCH 995/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3704388928869327		[learning rate: 0.00010273]
		[batch 20/20] avg loss: 1.3362349784305128		[learning rate: 0.00010248]
	Learning Rate: 0.000102478
	LOSS [training: 1.3533369356587226 | validation: 1.2493563665912446]
	TIME [epoch: 8.37 sec]
EPOCH 996/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.40379699512713		[learning rate: 0.00010223]
		[batch 20/20] avg loss: 1.3641716691568264		[learning rate: 0.00010198]
	Learning Rate: 0.000101983
	LOSS [training: 1.3839843321419782 | validation: 1.3171628212063258]
	TIME [epoch: 8.37 sec]
EPOCH 997/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.381986294976856		[learning rate: 0.00010174]
		[batch 20/20] avg loss: 1.5072158399673292		[learning rate: 0.00010149]
	Learning Rate: 0.000101489
	LOSS [training: 1.4446010674720924 | validation: 1.280782484994396]
	TIME [epoch: 8.36 sec]
EPOCH 998/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.4027790351434013		[learning rate: 0.00010124]
		[batch 20/20] avg loss: 1.415556957140327		[learning rate: 0.000101]
	Learning Rate: 0.000100999
	LOSS [training: 1.409167996141864 | validation: 1.2510732025647757]
	TIME [epoch: 8.37 sec]
EPOCH 999/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.446523377244795		[learning rate: 0.00010075]
		[batch 20/20] avg loss: 1.4094256162742156		[learning rate: 0.00010051]
	Learning Rate: 0.00010051
	LOSS [training: 1.4279744967595052 | validation: 1.2039782617377124]
	TIME [epoch: 8.39 sec]
EPOCH 1000/1000:
	Training over batches...
		[batch 10/20] avg loss: 1.3577659281225436		[learning rate: 0.00010027]
		[batch 20/20] avg loss: 1.4799498774772784		[learning rate: 0.00010002]
	Learning Rate: 0.000100024
	LOSS [training: 1.4188579027999109 | validation: 1.306746197233874]
	TIME [epoch: 8.37 sec]
Finished training in 8517.852 seconds.
