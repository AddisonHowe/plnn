Args:
Namespace(name='model_tr_study205', outdir='out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3', training_data='data/transition_rate_studies/tr_study205/tr_study205_training/r3', validation_data='data/transition_rate_studies/tr_study205/tr_study205_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3997543702

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.629932596495326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.629932596495326 | validation: 12.764550358911865]
	TIME [epoch: 53.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.509824101104806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.509824101104806 | validation: 11.963459812072907]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.502770888548676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.502770888548676 | validation: 12.690044957049663]
	TIME [epoch: 9.57 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.048196754888277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.048196754888277 | validation: 11.595930937970364]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.626008580470952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.626008580470952 | validation: 12.149853373278463]
	TIME [epoch: 9.55 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.523217303095436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.523217303095436 | validation: 11.170542799031232]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.99960648352392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.99960648352392 | validation: 10.829222082129162]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.018993595504803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.018993595504803 | validation: 9.52126546883977]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.521142989929157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.521142989929157 | validation: 9.810918747606918]
	TIME [epoch: 9.56 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.586112920606901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.586112920606901 | validation: 8.674459496052915]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.738643491779186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.738643491779186 | validation: 10.496817414856379]
	TIME [epoch: 9.54 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.972108671704405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.972108671704405 | validation: 9.33999334772511]
	TIME [epoch: 9.55 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.79518128441947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.79518128441947 | validation: 8.867969068509952]
	TIME [epoch: 9.57 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.729983615414136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.729983615414136 | validation: 8.964416247878956]
	TIME [epoch: 9.54 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.461316466243249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.461316466243249 | validation: 7.502035085641405]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.963411204417945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.963411204417945 | validation: 7.564076194473894]
	TIME [epoch: 9.53 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.329686392390061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.329686392390061 | validation: 8.012579840174888]
	TIME [epoch: 9.55 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.272935401521994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.272935401521994 | validation: 7.919638873887379]
	TIME [epoch: 9.54 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.790959639271439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.790959639271439 | validation: 7.560724804262666]
	TIME [epoch: 9.53 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.680213236309706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.680213236309706 | validation: 6.7484183529426005]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.296063599210877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.296063599210877 | validation: 6.886071108646539]
	TIME [epoch: 9.56 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.418392827060901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.418392827060901 | validation: 6.37098024256174]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.449568796566211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.449568796566211 | validation: 6.281583357609182]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.284977291907774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.284977291907774 | validation: 6.564135943616796]
	TIME [epoch: 9.54 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.219370176255768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.219370176255768 | validation: 6.33667238534604]
	TIME [epoch: 9.57 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.464411024285974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.464411024285974 | validation: 6.249871431489098]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.254753808333836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.254753808333836 | validation: 6.907543551302015]
	TIME [epoch: 9.55 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.703520110699449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.703520110699449 | validation: 7.820982425908296]
	TIME [epoch: 9.57 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.492628304767843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.492628304767843 | validation: 6.145608535641356]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.1039347191913755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.1039347191913755 | validation: 6.865016025432901]
	TIME [epoch: 9.56 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.180240254798133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.180240254798133 | validation: 6.425538215547357]
	TIME [epoch: 9.56 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.058425262654575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.058425262654575 | validation: 6.110596237432449]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.925364889800757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.925364889800757 | validation: 6.214469313495853]
	TIME [epoch: 9.56 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.535881601116991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.535881601116991 | validation: 5.970174187687985]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.75102359730295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.75102359730295 | validation: 5.890721021459846]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.979956366568978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.979956366568978 | validation: 5.985270425717143]
	TIME [epoch: 9.58 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.615756038533832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.615756038533832 | validation: 5.758650687776931]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.64506196330372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.64506196330372 | validation: 6.08622876881472]
	TIME [epoch: 9.55 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.770543817962371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.770543817962371 | validation: 5.671364556291824]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.309850292769342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.309850292769342 | validation: 6.142510334710612]
	TIME [epoch: 9.57 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.249803735423344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.249803735423344 | validation: 5.5167910986153315]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.068362601899565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.068362601899565 | validation: 5.305230365544463]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.793549819096668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.793549819096668 | validation: 5.703037161803258]
	TIME [epoch: 9.56 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.784417182456491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.784417182456491 | validation: 5.702232602688913]
	TIME [epoch: 9.57 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.80454873310787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.80454873310787 | validation: 4.73934818947044]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.505933270333018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.505933270333018 | validation: 4.676565436951825]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2774453449869245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2774453449869245 | validation: 4.347623268617936]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.154732461488206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.154732461488206 | validation: 4.478479410395699]
	TIME [epoch: 9.57 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.977445762259662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.977445762259662 | validation: 4.577726227833237]
	TIME [epoch: 9.55 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.003723056447376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.003723056447376 | validation: 4.201253994869971]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.290370559454081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.290370559454081 | validation: 6.2406859136360975]
	TIME [epoch: 9.55 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.649579461127296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.649579461127296 | validation: 3.9969745691374388]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8083391503334276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8083391503334276 | validation: 4.178136491536265]
	TIME [epoch: 9.56 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7566662451546664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7566662451546664 | validation: 4.150125393894619]
	TIME [epoch: 9.55 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7229713967833065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7229713967833065 | validation: 3.893738764587143]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5559531042907553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5559531042907553 | validation: 4.124165826303643]
	TIME [epoch: 9.59 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507422809286189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.507422809286189 | validation: 4.1956301442062305]
	TIME [epoch: 9.56 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608705567648059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.608705567648059 | validation: 4.642368177171761]
	TIME [epoch: 9.56 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6101960909105393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6101960909105393 | validation: 4.054680639580004]
	TIME [epoch: 9.56 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.484290273373451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.484290273373451 | validation: 3.6819396450098485]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.452573563791282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.452573563791282 | validation: 4.017372105269631]
	TIME [epoch: 9.56 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.417336149334345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.417336149334345 | validation: 3.6313380175698278]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3444152902871798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3444152902871798 | validation: 3.68626340523796]
	TIME [epoch: 9.56 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4166146057615547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4166146057615547 | validation: 3.7452862130981135]
	TIME [epoch: 9.59 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3991593432291944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3991593432291944 | validation: 3.6390201124808947]
	TIME [epoch: 9.56 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2558395197379637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2558395197379637 | validation: 3.6106487155660365]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2384426541783853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2384426541783853 | validation: 3.332391547221066]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4608026813362827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4608026813362827 | validation: 3.675455303691854]
	TIME [epoch: 9.58 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6385302744447663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6385302744447663 | validation: 5.145217388517335]
	TIME [epoch: 9.56 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9514869151660887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9514869151660887 | validation: 4.015210004226634]
	TIME [epoch: 9.55 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4529675755918823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4529675755918823 | validation: 5.309695718080036]
	TIME [epoch: 9.57 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643427999804305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.643427999804305 | validation: 3.273891295406125]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2106369082206414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2106369082206414 | validation: 3.483286276851985]
	TIME [epoch: 9.56 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.296106222383924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.296106222383924 | validation: 3.3936860042392936]
	TIME [epoch: 9.56 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.132454115371712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.132454115371712 | validation: 3.5771722053681176]
	TIME [epoch: 9.57 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.065568150047429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.065568150047429 | validation: 5.567984286100966]
	TIME [epoch: 9.56 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.359757411863346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359757411863346 | validation: 4.859858686255737]
	TIME [epoch: 9.56 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3499681629239193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3499681629239193 | validation: 3.6262809889772134]
	TIME [epoch: 9.56 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.224822371075768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.224822371075768 | validation: 3.5477028703914746]
	TIME [epoch: 9.56 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1295328858470346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1295328858470346 | validation: 3.700378912123521]
	TIME [epoch: 9.57 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1350621503254175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1350621503254175 | validation: 4.534788550250412]
	TIME [epoch: 9.57 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1723510645616626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1723510645616626 | validation: 3.9621098212991024]
	TIME [epoch: 9.56 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.27941066470636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.27941066470636 | validation: 3.756952085118627]
	TIME [epoch: 9.58 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1244023702816586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1244023702816586 | validation: 3.7072950162311424]
	TIME [epoch: 9.59 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1939363331989425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1939363331989425 | validation: 3.4953479298715546]
	TIME [epoch: 9.56 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1313208759417703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1313208759417703 | validation: 3.737500596237697]
	TIME [epoch: 9.56 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2500496918350263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2500496918350263 | validation: 3.343076668105545]
	TIME [epoch: 9.57 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1715616948090335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1715616948090335 | validation: 3.2672459014410635]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.331446733349575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.331446733349575 | validation: 3.612688396774643]
	TIME [epoch: 9.55 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1443179355863835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1443179355863835 | validation: 3.5287184233172946]
	TIME [epoch: 9.55 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.061671456076497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.061671456076497 | validation: 3.537148832518633]
	TIME [epoch: 9.57 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9754191326833115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9754191326833115 | validation: 3.487310506351457]
	TIME [epoch: 9.55 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3431106678879297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3431106678879297 | validation: 3.0543458829341446]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0513773570602756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0513773570602756 | validation: 3.793938509698894]
	TIME [epoch: 9.54 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.139127847322535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139127847322535 | validation: 3.6946361356439823]
	TIME [epoch: 9.58 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0775189733226638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0775189733226638 | validation: 3.611772814000201]
	TIME [epoch: 9.56 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9647035010487754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9647035010487754 | validation: 3.34015769690122]
	TIME [epoch: 9.54 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.063425456748951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.063425456748951 | validation: 3.871860251393989]
	TIME [epoch: 9.55 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1600517682010834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1600517682010834 | validation: 3.5624963170671893]
	TIME [epoch: 9.58 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0669887665772406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0669887665772406 | validation: 3.496431535673722]
	TIME [epoch: 9.55 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1252964916004773		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 3.1252964916004773 | validation: 3.712868132791828]
	TIME [epoch: 9.54 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1334296205659635		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 3.1334296205659635 | validation: 3.2875863169793638]
	TIME [epoch: 9.55 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.012873314371013		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 3.012873314371013 | validation: 3.176437353899729]
	TIME [epoch: 9.58 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0094733983398645		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.0094733983398645 | validation: 3.2549665165368644]
	TIME [epoch: 9.57 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0048104606900163		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 3.0048104606900163 | validation: 3.355307434110888]
	TIME [epoch: 9.55 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9265399958998164		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 2.9265399958998164 | validation: 3.16246482147894]
	TIME [epoch: 9.55 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.935575229539255		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 2.935575229539255 | validation: 3.949334807486378]
	TIME [epoch: 9.58 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1875028678867743		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.1875028678867743 | validation: 3.1513272435767012]
	TIME [epoch: 9.55 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9917541284522557		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 2.9917541284522557 | validation: 3.033932962256482]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.186142737163292		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.186142737163292 | validation: 3.0958498350175034]
	TIME [epoch: 9.54 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.081344315570079		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.081344315570079 | validation: 3.7524437674849302]
	TIME [epoch: 9.57 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0655618016897166		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.0655618016897166 | validation: 3.366183435958559]
	TIME [epoch: 9.56 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0173702622448006		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.0173702622448006 | validation: 4.231167896194907]
	TIME [epoch: 9.55 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.133336109733115		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.133336109733115 | validation: 4.0650092790538395]
	TIME [epoch: 9.55 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2177676684062133		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.2177676684062133 | validation: 3.177927909249304]
	TIME [epoch: 9.58 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8582338557795173		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 2.8582338557795173 | validation: 4.072819459794656]
	TIME [epoch: 9.56 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.097447316371492		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.097447316371492 | validation: 3.4395770181670504]
	TIME [epoch: 9.56 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.010770507706577		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.010770507706577 | validation: 3.3267453851695485]
	TIME [epoch: 9.54 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.036890763928696		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.036890763928696 | validation: 3.942160843451378]
	TIME [epoch: 9.57 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0340484821135547		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.0340484821135547 | validation: 3.3970943405883745]
	TIME [epoch: 9.56 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975032179095409		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 2.975032179095409 | validation: 3.398521345624892]
	TIME [epoch: 9.56 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9398413301693447		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 2.9398413301693447 | validation: 3.209255689070525]
	TIME [epoch: 9.54 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9855427136331336		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 2.9855427136331336 | validation: 3.3019486278129593]
	TIME [epoch: 9.58 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.24376500538643		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 3.24376500538643 | validation: 4.714244417838213]
	TIME [epoch: 9.54 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.425604452234937		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 4.425604452234937 | validation: 3.66938316061848]
	TIME [epoch: 9.55 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0331965387659166		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 4.0331965387659166 | validation: 4.935496363047705]
	TIME [epoch: 9.54 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.001216940469424		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 4.001216940469424 | validation: 3.464326271602693]
	TIME [epoch: 9.58 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591446737413129		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.591446737413129 | validation: 5.149019622239283]
	TIME [epoch: 9.56 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.081963686348372		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 4.081963686348372 | validation: 3.5517552917935893]
	TIME [epoch: 9.55 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.542562381535479		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.542562381535479 | validation: 4.557365340297245]
	TIME [epoch: 9.55 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601248332755644		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.601248332755644 | validation: 3.5835285553952274]
	TIME [epoch: 9.57 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1237591259351776		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.1237591259351776 | validation: 3.8306209488221032]
	TIME [epoch: 9.55 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.243061786796821		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.243061786796821 | validation: 3.3608705419812033]
	TIME [epoch: 9.56 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0398555597486605		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.0398555597486605 | validation: 3.262147481523314]
	TIME [epoch: 9.57 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.037607137315689		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 3.037607137315689 | validation: 3.555670930060198]
	TIME [epoch: 9.58 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0518668059753065		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.0518668059753065 | validation: 3.473256336384734]
	TIME [epoch: 9.57 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0400183186747975		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.0400183186747975 | validation: 3.4542831609124307]
	TIME [epoch: 9.56 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029379065589331		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.029379065589331 | validation: 3.4083444852071487]
	TIME [epoch: 9.55 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994816414412342		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 2.994816414412342 | validation: 3.2930311921322204]
	TIME [epoch: 9.57 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9621299082824386		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 2.9621299082824386 | validation: 3.327011858231913]
	TIME [epoch: 9.55 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9581694889697094		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 2.9581694889697094 | validation: 4.324134395660912]
	TIME [epoch: 9.55 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.131300997487812		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.131300997487812 | validation: 3.6720855923894913]
	TIME [epoch: 9.54 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980021787257426		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 2.980021787257426 | validation: 3.2551041617043595]
	TIME [epoch: 9.58 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8694561098592817		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 2.8694561098592817 | validation: 3.4038709534722003]
	TIME [epoch: 9.57 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.886209187331744		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 2.886209187331744 | validation: 3.3730685356497543]
	TIME [epoch: 9.56 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9733028147613303		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 2.9733028147613303 | validation: 3.34291368363151]
	TIME [epoch: 9.56 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9089198013449975		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 2.9089198013449975 | validation: 3.09921851547546]
	TIME [epoch: 9.58 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.905775949787775		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 2.905775949787775 | validation: 3.604960932396112]
	TIME [epoch: 9.56 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8531049368787675		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 2.8531049368787675 | validation: 3.669813088664457]
	TIME [epoch: 9.56 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.197799315529469		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.197799315529469 | validation: 3.52507905544486]
	TIME [epoch: 9.56 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.090836109407533		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.090836109407533 | validation: 3.799237004863455]
	TIME [epoch: 9.59 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.268084087298674		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.268084087298674 | validation: 3.63885262306896]
	TIME [epoch: 9.56 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1528807868507824		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.1528807868507824 | validation: 3.3115811653829827]
	TIME [epoch: 9.55 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9942559138036775		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 2.9942559138036775 | validation: 3.395207066690064]
	TIME [epoch: 9.57 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.867728083336493		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 2.867728083336493 | validation: 3.1216734498832706]
	TIME [epoch: 9.58 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8712158474310554		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 2.8712158474310554 | validation: 3.263604220604667]
	TIME [epoch: 9.56 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7719585647989207		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 2.7719585647989207 | validation: 3.085863522809264]
	TIME [epoch: 9.56 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.84858413369565		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 2.84858413369565 | validation: 3.8290314849396045]
	TIME [epoch: 9.56 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.86142674152346		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 2.86142674152346 | validation: 3.5779362074575056]
	TIME [epoch: 9.57 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.801303554969463		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 2.801303554969463 | validation: 3.639917876596984]
	TIME [epoch: 9.56 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.859047045698909		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 2.859047045698909 | validation: 3.2647191117878833]
	TIME [epoch: 9.54 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8422575867812396		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 2.8422575867812396 | validation: 3.305390225916934]
	TIME [epoch: 9.55 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7206729558111133		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 2.7206729558111133 | validation: 3.315340855631132]
	TIME [epoch: 9.58 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8575946789959845		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 2.8575946789959845 | validation: 3.7691253610561795]
	TIME [epoch: 9.55 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8492433379611835		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 2.8492433379611835 | validation: 2.978797429426696]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7056289656947485		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 2.7056289656947485 | validation: 3.6333239508716795]
	TIME [epoch: 9.56 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.756790241380916		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 2.756790241380916 | validation: 3.1693574743180313]
	TIME [epoch: 9.57 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7053601192620365		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 3.7053601192620365 | validation: 3.976953489912422]
	TIME [epoch: 9.56 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.403929773717837		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 3.403929773717837 | validation: 4.387717823266001]
	TIME [epoch: 9.54 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5896329510085763		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 3.5896329510085763 | validation: 3.615457764613852]
	TIME [epoch: 9.57 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.110627531411652		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.110627531411652 | validation: 3.17508914631444]
	TIME [epoch: 9.58 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8249380306047107		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 2.8249380306047107 | validation: 2.8501266923956354]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8551147228136626		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 2.8551147228136626 | validation: 3.009319787507544]
	TIME [epoch: 9.56 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.653887669975039		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 2.653887669975039 | validation: 3.097276759124611]
	TIME [epoch: 9.56 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.656693021463549		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 2.656693021463549 | validation: 3.231807161536668]
	TIME [epoch: 9.58 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3999039706113736		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.3999039706113736 | validation: 4.1738894765053836]
	TIME [epoch: 9.57 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0689254975870304		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 3.0689254975870304 | validation: 2.891509437980509]
	TIME [epoch: 9.56 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.087404856471124		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 3.087404856471124 | validation: 3.3300271782161257]
	TIME [epoch: 9.56 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.934786143051615		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 2.934786143051615 | validation: 3.0103839300526123]
	TIME [epoch: 9.58 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976074752446157		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 2.976074752446157 | validation: 3.4780449401962437]
	TIME [epoch: 9.56 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.79419347369485		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 2.79419347369485 | validation: 3.2512860038140547]
	TIME [epoch: 9.56 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.790209771187574		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 2.790209771187574 | validation: 2.931284022643053]
	TIME [epoch: 9.57 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.693563362849467		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 2.693563362849467 | validation: 3.468436440665152]
	TIME [epoch: 9.58 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7631007026071073		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 2.7631007026071073 | validation: 3.034844857621132]
	TIME [epoch: 9.57 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.658555150643034		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 2.658555150643034 | validation: 3.2809963571113783]
	TIME [epoch: 9.56 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.542723480043389		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 2.542723480043389 | validation: 4.683539832467393]
	TIME [epoch: 9.57 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.689635905315393		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.689635905315393 | validation: 2.9574906200863142]
	TIME [epoch: 9.58 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8285269071541688		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 2.8285269071541688 | validation: 3.5760957331509906]
	TIME [epoch: 9.56 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.883484318639944		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 2.883484318639944 | validation: 3.0149415689392085]
	TIME [epoch: 9.56 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.615712685237534		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 2.615712685237534 | validation: 2.9774631450342945]
	TIME [epoch: 9.56 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.613449883745793		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 2.613449883745793 | validation: 3.14411039159959]
	TIME [epoch: 9.57 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00076971884976		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.00076971884976 | validation: 2.688197670828061]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.600495282212498		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 2.600495282212498 | validation: 3.141139994019488]
	TIME [epoch: 9.56 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6569609708007706		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 2.6569609708007706 | validation: 3.2004369863720963]
	TIME [epoch: 9.57 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.642848995632349		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 2.642848995632349 | validation: 3.0814235574265796]
	TIME [epoch: 9.57 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.554771171484849		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 2.554771171484849 | validation: 3.1300357648250703]
	TIME [epoch: 9.55 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5517790678121375		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 2.5517790678121375 | validation: 3.4435661040632737]
	TIME [epoch: 9.56 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.832966846702199		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 2.832966846702199 | validation: 2.7038308843367913]
	TIME [epoch: 9.57 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.600823818096471		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 2.600823818096471 | validation: 2.9891069160591632]
	TIME [epoch: 9.57 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8497040237593074		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 2.8497040237593074 | validation: 4.844003918179196]
	TIME [epoch: 9.57 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.276261788097197		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 3.276261788097197 | validation: 2.7879142968642174]
	TIME [epoch: 9.56 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7323966621957054		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 2.7323966621957054 | validation: 2.8746077719379537]
	TIME [epoch: 9.57 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.598537795031231		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 2.598537795031231 | validation: 3.200920004047923]
	TIME [epoch: 9.57 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6705288957688564		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 2.6705288957688564 | validation: 3.0374543676357337]
	TIME [epoch: 9.56 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5175243135932623		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 2.5175243135932623 | validation: 3.1996186214160036]
	TIME [epoch: 9.56 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4661184300664893		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 2.4661184300664893 | validation: 4.4006280793501675]
	TIME [epoch: 9.57 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.680035943148188		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.680035943148188 | validation: 3.31007325938809]
	TIME [epoch: 9.57 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2900421348245446		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.2900421348245446 | validation: 3.9970430506987475]
	TIME [epoch: 9.55 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2533150060388523		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 3.2533150060388523 | validation: 2.8057821928739437]
	TIME [epoch: 9.56 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.218446543405773		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.218446543405773 | validation: 3.7468198251074005]
	TIME [epoch: 9.57 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2056008286920004		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.2056008286920004 | validation: 3.2631840467542355]
	TIME [epoch: 9.57 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6638876060347374		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 2.6638876060347374 | validation: 3.7677065871755504]
	TIME [epoch: 9.56 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5459030808894845		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 3.5459030808894845 | validation: 3.2061610475409146]
	TIME [epoch: 9.56 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0086542017748235		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 3.0086542017748235 | validation: 3.2404198252288348]
	TIME [epoch: 9.59 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7962920694548914		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 2.7962920694548914 | validation: 2.768634322267719]
	TIME [epoch: 9.57 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6223028816144778		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 2.6223028816144778 | validation: 2.887067867825615]
	TIME [epoch: 9.57 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.485887448045383		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 2.485887448045383 | validation: 3.1376033440923035]
	TIME [epoch: 9.56 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.600755365063028		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 2.600755365063028 | validation: 3.0754712099008903]
	TIME [epoch: 9.57 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.331329699859301		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 3.331329699859301 | validation: 5.245988284632506]
	TIME [epoch: 9.57 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0291877518402206		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.0291877518402206 | validation: 3.500545969861488]
	TIME [epoch: 9.57 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6727565475531234		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 2.6727565475531234 | validation: 2.7086036412794545]
	TIME [epoch: 9.55 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.455617932010968		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 2.455617932010968 | validation: 3.001262212593121]
	TIME [epoch: 9.58 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4937174828269817		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 2.4937174828269817 | validation: 3.289471220685881]
	TIME [epoch: 9.57 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.581339846895527		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 2.581339846895527 | validation: 2.988472535635358]
	TIME [epoch: 9.57 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3795796894557832		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 2.3795796894557832 | validation: 3.563175965672552]
	TIME [epoch: 9.57 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0694176774666437		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.0694176774666437 | validation: 3.002785287923392]
	TIME [epoch: 9.58 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7548938742755515		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 2.7548938742755515 | validation: 3.1448233923661464]
	TIME [epoch: 9.57 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.723564699600991		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 2.723564699600991 | validation: 2.747225237950805]
	TIME [epoch: 9.56 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4397691769947736		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 2.4397691769947736 | validation: 3.242197479853268]
	TIME [epoch: 9.57 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4994248956098883		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 2.4994248956098883 | validation: 2.9340345974448083]
	TIME [epoch: 9.58 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3854110070904087		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 2.3854110070904087 | validation: 3.3734189045031546]
	TIME [epoch: 9.57 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9152167430017504		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 2.9152167430017504 | validation: 2.937243353749951]
	TIME [epoch: 9.57 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.601793978183346		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 2.601793978183346 | validation: 3.3678086193647836]
	TIME [epoch: 9.57 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.644455539169836		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 2.644455539169836 | validation: 2.774795486417139]
	TIME [epoch: 9.6 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.480673093388456		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 2.480673093388456 | validation: 2.7692011156322143]
	TIME [epoch: 9.58 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5102416757140804		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 2.5102416757140804 | validation: 4.703841223668467]
	TIME [epoch: 9.57 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5657039978687535		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.5657039978687535 | validation: 2.676056577512772]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9803601818473133		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 2.9803601818473133 | validation: 3.734211399253583]
	TIME [epoch: 9.59 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.868907859372443		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 2.868907859372443 | validation: 2.795471336682906]
	TIME [epoch: 9.57 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4939624625419996		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 2.4939624625419996 | validation: 2.921081683643722]
	TIME [epoch: 9.57 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5952851228619283		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 2.5952851228619283 | validation: 2.7437526023338212]
	TIME [epoch: 9.56 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4468502528210556		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 2.4468502528210556 | validation: 2.839048877146708]
	TIME [epoch: 9.58 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3563139959556487		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 2.3563139959556487 | validation: 4.2250765425723715]
	TIME [epoch: 9.57 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7715076901483457		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 2.7715076901483457 | validation: 3.147661674222986]
	TIME [epoch: 9.56 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4223230643104676		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 2.4223230643104676 | validation: 2.70823376772604]
	TIME [epoch: 9.57 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3813230145322146		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 2.3813230145322146 | validation: 3.1515918951547497]
	TIME [epoch: 9.58 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3637921806000435		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 2.3637921806000435 | validation: 2.9497911544647906]
	TIME [epoch: 9.57 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.389000952769861		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 2.389000952769861 | validation: 2.7475010295175406]
	TIME [epoch: 9.56 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2093527484248265		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 2.2093527484248265 | validation: 3.900558172293719]
	TIME [epoch: 9.57 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5955435737240533		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 2.5955435737240533 | validation: 2.79422951800196]
	TIME [epoch: 9.59 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.392472507366554		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 2.392472507366554 | validation: 2.8976518553472]
	TIME [epoch: 9.57 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4102866141830623		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 2.4102866141830623 | validation: 2.806743074976665]
	TIME [epoch: 9.57 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4889138210066037		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 2.4889138210066037 | validation: 2.701551889333813]
	TIME [epoch: 9.57 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3199228836062895		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 3.3199228836062895 | validation: 3.4355533451366056]
	TIME [epoch: 9.58 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.548949421243612		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 2.548949421243612 | validation: 2.6661110675563493]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2974890561119294		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 2.2974890561119294 | validation: 2.946079794077472]
	TIME [epoch: 9.57 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1300546783347363		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.1300546783347363 | validation: 3.0600366450416137]
	TIME [epoch: 9.56 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3815423487539773		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 2.3815423487539773 | validation: 2.717641740550013]
	TIME [epoch: 9.58 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.357633034513659		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 2.357633034513659 | validation: 2.7595284075179776]
	TIME [epoch: 9.56 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3666011660072437		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 2.3666011660072437 | validation: 4.457443896555725]
	TIME [epoch: 9.57 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8908199238716783		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 2.8908199238716783 | validation: 2.5374102610886538]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.517719615643236		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 2.517719615643236 | validation: 2.9749565945870042]
	TIME [epoch: 9.59 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.548911496920744		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 2.548911496920744 | validation: 2.5641944573884157]
	TIME [epoch: 9.57 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7281511580167996		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 2.7281511580167996 | validation: 3.2744861177555142]
	TIME [epoch: 9.57 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4470502608183415		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 2.4470502608183415 | validation: 2.589313572999306]
	TIME [epoch: 9.57 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.292368840611225		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 2.292368840611225 | validation: 2.9153106177713948]
	TIME [epoch: 9.58 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.342226562681997		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 2.342226562681997 | validation: 2.7933291339872945]
	TIME [epoch: 9.57 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.377624053233638		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 2.377624053233638 | validation: 3.12461948323461]
	TIME [epoch: 9.57 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3237238420423982		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 2.3237238420423982 | validation: 2.7339696647206946]
	TIME [epoch: 9.55 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.279377102584847		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 2.279377102584847 | validation: 2.961624369151644]
	TIME [epoch: 9.59 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5589134442205443		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 2.5589134442205443 | validation: 3.1850790121154575]
	TIME [epoch: 9.57 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.384149329923903		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 2.384149329923903 | validation: 2.82954942144627]
	TIME [epoch: 9.57 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278077658754751		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 2.278077658754751 | validation: 3.045327373522431]
	TIME [epoch: 9.57 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2772372661418565		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 2.2772372661418565 | validation: 2.987829175409686]
	TIME [epoch: 9.6 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4761922319702068		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 2.4761922319702068 | validation: 3.109806645431464]
	TIME [epoch: 9.57 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4123037229961075		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 2.4123037229961075 | validation: 3.6559912046612464]
	TIME [epoch: 9.56 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.724052061814773		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 2.724052061814773 | validation: 2.6218772854544334]
	TIME [epoch: 9.56 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4441054386621666		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 2.4441054386621666 | validation: 3.0625197984257455]
	TIME [epoch: 9.59 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5124707191462803		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 2.5124707191462803 | validation: 2.715884900620389]
	TIME [epoch: 9.57 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3792519330453987		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 2.3792519330453987 | validation: 4.4175826144125985]
	TIME [epoch: 9.57 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.257650060464203		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.257650060464203 | validation: 2.737648794403136]
	TIME [epoch: 9.58 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.776710203686206		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 2.776710203686206 | validation: 3.7629227115905635]
	TIME [epoch: 9.59 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990172716732302		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 2.990172716732302 | validation: 2.8209948100301463]
	TIME [epoch: 9.56 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.563415677773717		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 2.563415677773717 | validation: 2.6634562237978914]
	TIME [epoch: 9.56 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2860061466911095		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 2.2860061466911095 | validation: 2.6593136100134824]
	TIME [epoch: 9.56 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1860088537495725		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 2.1860088537495725 | validation: 2.9007915686270556]
	TIME [epoch: 9.58 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.415535339229433		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 2.415535339229433 | validation: 2.862046130420082]
	TIME [epoch: 9.57 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.311864443939652		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 2.311864443939652 | validation: 3.1828489919655807]
	TIME [epoch: 9.57 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3052624571381886		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 2.3052624571381886 | validation: 2.93061198484898]
	TIME [epoch: 9.56 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2745703788153193		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 2.2745703788153193 | validation: 3.2360432115042546]
	TIME [epoch: 9.59 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.367403926122137		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 2.367403926122137 | validation: 2.689492190776069]
	TIME [epoch: 9.58 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4182821178845346		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 2.4182821178845346 | validation: 2.445999848259038]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.444541851368464		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 2.444541851368464 | validation: 3.4682280361734708]
	TIME [epoch: 9.58 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.49581078528817		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 2.49581078528817 | validation: 2.4873111152311904]
	TIME [epoch: 9.58 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4001515287996384		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 2.4001515287996384 | validation: 2.4402598757371585]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.91573116937229		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 2.91573116937229 | validation: 3.6486301598254194]
	TIME [epoch: 9.57 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.861959428327759		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 2.861959428327759 | validation: 2.631101722337797]
	TIME [epoch: 9.56 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.746738657921245		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 2.746738657921245 | validation: 3.563389921467975]
	TIME [epoch: 9.58 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.692336462586236		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 2.692336462586236 | validation: 2.6724107867237406]
	TIME [epoch: 9.56 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5220594536362038		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 2.5220594536362038 | validation: 3.1643902684374665]
	TIME [epoch: 9.55 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4270687732906038		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 2.4270687732906038 | validation: 2.5867054857003917]
	TIME [epoch: 9.56 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265520402317697		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 2.265520402317697 | validation: 2.693952505534532]
	TIME [epoch: 9.58 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.328416236513659		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 2.328416236513659 | validation: 2.6620861838054233]
	TIME [epoch: 9.56 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456068674395944		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 2.2456068674395944 | validation: 3.03286236447531]
	TIME [epoch: 9.57 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2403428001357		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 2.2403428001357 | validation: 2.743567666426285]
	TIME [epoch: 9.57 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2253847628264736		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 2.2253847628264736 | validation: 2.76616435814132]
	TIME [epoch: 9.59 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2630266805362433		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 2.2630266805362433 | validation: 2.481868562955349]
	TIME [epoch: 9.57 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.694680066107611		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 2.694680066107611 | validation: 3.2191618116893355]
	TIME [epoch: 9.58 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.390310596293551		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 2.390310596293551 | validation: 2.854146893538407]
	TIME [epoch: 9.57 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211940002313232		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 2.211940002313232 | validation: 2.5390064957175618]
	TIME [epoch: 9.58 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.158445117585891		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 2.158445117585891 | validation: 2.625497545971577]
	TIME [epoch: 9.56 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2193264473863787		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 2.2193264473863787 | validation: 2.5664904861711255]
	TIME [epoch: 9.56 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192678685363446		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 2.192678685363446 | validation: 2.5828369810488767]
	TIME [epoch: 9.57 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1566108397511368		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 2.1566108397511368 | validation: 2.754644862847877]
	TIME [epoch: 9.58 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2417506433979306		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 2.2417506433979306 | validation: 2.4556654493716787]
	TIME [epoch: 9.56 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1288460375062663		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 2.1288460375062663 | validation: 3.3962004491834508]
	TIME [epoch: 9.57 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2567243754744046		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 2.2567243754744046 | validation: 2.6391133732681453]
	TIME [epoch: 9.57 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188955860057602		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 2.188955860057602 | validation: 2.6251422500709833]
	TIME [epoch: 9.57 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2548182794282776		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 2.2548182794282776 | validation: 2.6874956104166436]
	TIME [epoch: 9.57 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.120265887886649		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 2.120265887886649 | validation: 2.9139078252367265]
	TIME [epoch: 9.58 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5357454172119716		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 2.5357454172119716 | validation: 2.480407278313191]
	TIME [epoch: 9.58 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.330791621409832		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 2.330791621409832 | validation: 3.235686548064235]
	TIME [epoch: 9.58 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4103610517190215		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 2.4103610517190215 | validation: 2.7667189599635513]
	TIME [epoch: 9.58 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1634194224726193		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 2.1634194224726193 | validation: 2.4373484114847463]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.203234778819265		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 2.203234778819265 | validation: 3.231742442609708]
	TIME [epoch: 9.58 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.807139654072608		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 2.807139654072608 | validation: 2.9327482137702834]
	TIME [epoch: 9.58 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.561571847822312		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 2.561571847822312 | validation: 3.8576466651543706]
	TIME [epoch: 9.58 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7808962836687745		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 2.7808962836687745 | validation: 2.8056167358625066]
	TIME [epoch: 9.57 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4861063575140516		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 2.4861063575140516 | validation: 3.6024860683657396]
	TIME [epoch: 9.6 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.594928239864687		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 2.594928239864687 | validation: 2.4532116323750093]
	TIME [epoch: 9.58 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.434811003104455		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 2.434811003104455 | validation: 3.2557400128613807]
	TIME [epoch: 9.57 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.502021882676331		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 2.502021882676331 | validation: 2.586578724869822]
	TIME [epoch: 9.57 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.568589266140294		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 2.568589266140294 | validation: 2.6234754094515895]
	TIME [epoch: 9.58 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.256783636781834		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 2.256783636781834 | validation: 2.6189171972591807]
	TIME [epoch: 9.56 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1853595545527997		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 2.1853595545527997 | validation: 2.5366581354527606]
	TIME [epoch: 9.56 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.17379198851648		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 2.17379198851648 | validation: 2.8983042462568567]
	TIME [epoch: 9.56 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2138420585185403		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 2.2138420585185403 | validation: 3.0361659261518446]
	TIME [epoch: 9.59 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.350957860106979		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 2.350957860106979 | validation: 2.4567313474542205]
	TIME [epoch: 9.57 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214593234384288		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 2.214593234384288 | validation: 2.564594143035451]
	TIME [epoch: 9.57 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3251648430524394		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 2.3251648430524394 | validation: 2.552810372353195]
	TIME [epoch: 9.55 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278978441766994		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 2.278978441766994 | validation: 2.468253971109862]
	TIME [epoch: 9.59 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1015691322688186		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 2.1015691322688186 | validation: 2.6294678940268414]
	TIME [epoch: 9.58 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2290640464863047		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 2.2290640464863047 | validation: 2.4655413141730214]
	TIME [epoch: 9.56 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.149743202264456		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 2.149743202264456 | validation: 2.6775258869106073]
	TIME [epoch: 9.56 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0890113602055376		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 2.0890113602055376 | validation: 2.4192113186407305]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.125139712537518		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 2.125139712537518 | validation: 2.572518190631139]
	TIME [epoch: 9.56 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0769273517341222		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 2.0769273517341222 | validation: 2.5048596706381194]
	TIME [epoch: 9.56 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0438801909952735		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 2.0438801909952735 | validation: 2.8874030343965535]
	TIME [epoch: 9.57 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0940446844782636		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 2.0940446844782636 | validation: 2.6258438829890443]
	TIME [epoch: 9.58 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1185522727017663		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 2.1185522727017663 | validation: 2.855310483556077]
	TIME [epoch: 9.56 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.106978495358055		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 2.106978495358055 | validation: 2.926534765388952]
	TIME [epoch: 9.57 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.324203262404432		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 2.324203262404432 | validation: 2.395168847197268]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.203435090738855		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 2.203435090738855 | validation: 2.4877002681373876]
	TIME [epoch: 9.57 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2057277410786327		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 2.2057277410786327 | validation: 2.408279027030233]
	TIME [epoch: 9.57 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5310263690817925		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 2.5310263690817925 | validation: 3.473337960354744]
	TIME [epoch: 9.56 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.542361715024923		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 2.542361715024923 | validation: 2.526996057801603]
	TIME [epoch: 9.57 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3136793914274545		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 2.3136793914274545 | validation: 2.680404446364236]
	TIME [epoch: 9.58 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.083075937839722		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 2.083075937839722 | validation: 2.6405153430191852]
	TIME [epoch: 9.56 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2693851046958957		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 2.2693851046958957 | validation: 2.6326363480719603]
	TIME [epoch: 9.56 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.21553836570955		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 2.21553836570955 | validation: 2.8928416461521578]
	TIME [epoch: 9.56 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0876019730971587		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 2.0876019730971587 | validation: 2.5631208607550855]
	TIME [epoch: 9.58 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0790840621815527		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 2.0790840621815527 | validation: 2.7352459232369406]
	TIME [epoch: 9.56 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1687286184231027		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 2.1687286184231027 | validation: 2.8069638565542885]
	TIME [epoch: 9.57 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1195952174437824		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 2.1195952174437824 | validation: 2.392406630217285]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1129901248821463		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 2.1129901248821463 | validation: 2.3797165450572066]
	TIME [epoch: 9.59 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214330643118558		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 2.214330643118558 | validation: 2.5660869444872936]
	TIME [epoch: 9.56 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0845057028009477		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 2.0845057028009477 | validation: 2.417740198245453]
	TIME [epoch: 9.56 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.041462291327527		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 2.041462291327527 | validation: 2.5847203359047013]
	TIME [epoch: 9.56 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.008101334071637		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 2.008101334071637 | validation: 3.5873840105444934]
	TIME [epoch: 9.57 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.229893471527547		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 2.229893471527547 | validation: 2.655447292315792]
	TIME [epoch: 9.56 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214301241441745		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 2.214301241441745 | validation: 2.550460919196464]
	TIME [epoch: 9.56 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.083457906783811		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 2.083457906783811 | validation: 2.561945987903382]
	TIME [epoch: 9.56 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0447254800633146		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 2.0447254800633146 | validation: 2.3857332448112927]
	TIME [epoch: 9.57 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.304625251920951		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 2.304625251920951 | validation: 3.1703137120604947]
	TIME [epoch: 9.56 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.279072614983902		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 2.279072614983902 | validation: 2.3846076962028384]
	TIME [epoch: 9.56 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1655287548930353		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 2.1655287548930353 | validation: 3.1591938354025597]
	TIME [epoch: 9.57 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5163860719394826		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 2.5163860719394826 | validation: 2.7886242697149766]
	TIME [epoch: 9.59 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1124732557615857		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 2.1124732557615857 | validation: 2.6612841008565256]
	TIME [epoch: 9.56 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.99645720316067		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 1.99645720316067 | validation: 2.499503482331718]
	TIME [epoch: 9.56 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.161706080145339		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 2.161706080145339 | validation: 2.4474728248405446]
	TIME [epoch: 9.57 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244172989406285		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 2.244172989406285 | validation: 2.394369384860535]
	TIME [epoch: 9.57 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.060953701453625		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 2.060953701453625 | validation: 2.463955758561484]
	TIME [epoch: 9.56 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0509311816639517		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 2.0509311816639517 | validation: 2.6247807262124776]
	TIME [epoch: 9.56 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.116968824347499		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 2.116968824347499 | validation: 2.483635712465]
	TIME [epoch: 9.56 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.017496791360435		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 2.017496791360435 | validation: 2.3755101887486196]
	TIME [epoch: 9.59 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244551908573543		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 2.244551908573543 | validation: 3.424287233347954]
	TIME [epoch: 9.56 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2832158051211886		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 2.2832158051211886 | validation: 2.49710852435361]
	TIME [epoch: 9.56 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0383781838649866		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 2.0383781838649866 | validation: 3.1336639040405063]
	TIME [epoch: 9.56 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.215183399367935		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 2.215183399367935 | validation: 2.7758989693419256]
	TIME [epoch: 9.58 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1131773046029667		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 2.1131773046029667 | validation: 2.732002101179703]
	TIME [epoch: 9.56 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1892234105354786		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 2.1892234105354786 | validation: 2.4181530173402748]
	TIME [epoch: 9.65 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3561939842158663		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 2.3561939842158663 | validation: 3.2107046001444837]
	TIME [epoch: 9.57 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3033853103534723		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 2.3033853103534723 | validation: 2.4203791924289395]
	TIME [epoch: 9.58 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.078912914577357		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 2.078912914577357 | validation: 2.430110424282101]
	TIME [epoch: 9.56 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0008391221642237		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 2.0008391221642237 | validation: 2.385372816347348]
	TIME [epoch: 9.56 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0273860642739603		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 2.0273860642739603 | validation: 2.3421495971220976]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.124273963280307		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 2.124273963280307 | validation: 3.1557879673273757]
	TIME [epoch: 9.59 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.30479068240994		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 2.30479068240994 | validation: 2.490129940062596]
	TIME [epoch: 9.57 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.393114224379287		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 2.393114224379287 | validation: 2.5301653310309415]
	TIME [epoch: 9.58 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.342769297277308		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 2.342769297277308 | validation: 2.934873562844956]
	TIME [epoch: 9.58 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3664133809030177		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 2.3664133809030177 | validation: 2.4320700252192764]
	TIME [epoch: 9.59 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2127054342773205		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 2.2127054342773205 | validation: 2.991182716065474]
	TIME [epoch: 9.57 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.111230940326053		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 2.111230940326053 | validation: 2.3842245174364987]
	TIME [epoch: 9.57 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0199027707810484		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 2.0199027707810484 | validation: 2.554307142443512]
	TIME [epoch: 9.58 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0467609776437286		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 2.0467609776437286 | validation: 2.3879200485149]
	TIME [epoch: 9.58 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3128772908604103		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 2.3128772908604103 | validation: 2.906060223463769]
	TIME [epoch: 9.57 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.471024798264262		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 2.471024798264262 | validation: 2.423531231568236]
	TIME [epoch: 9.58 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.062153781141207		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 2.062153781141207 | validation: 3.048967730033263]
	TIME [epoch: 9.59 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1797365330994145		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 2.1797365330994145 | validation: 2.389514378773365]
	TIME [epoch: 9.59 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9685326083853514		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 1.9685326083853514 | validation: 2.599803144059007]
	TIME [epoch: 9.58 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.023761519629562		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 2.023761519629562 | validation: 2.4983782352047332]
	TIME [epoch: 9.58 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.085468638281708		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 2.085468638281708 | validation: 2.5273552509643693]
	TIME [epoch: 9.58 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2903135043164653		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 2.2903135043164653 | validation: 2.438280759486444]
	TIME [epoch: 9.58 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1592628275660233		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 2.1592628275660233 | validation: 2.74508316786495]
	TIME [epoch: 9.56 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.038158286543799		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 2.038158286543799 | validation: 2.383159446848614]
	TIME [epoch: 9.57 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.026346608199657		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 2.026346608199657 | validation: 2.6836770447356115]
	TIME [epoch: 9.57 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9581481853602114		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 1.9581481853602114 | validation: 3.0393407883040666]
	TIME [epoch: 9.57 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3841001785352818		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 2.3841001785352818 | validation: 2.558121562269896]
	TIME [epoch: 9.57 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1081068771584346		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 2.1081068771584346 | validation: 2.6598774450486036]
	TIME [epoch: 9.57 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040567092979883		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 2.040567092979883 | validation: 2.5156324779673107]
	TIME [epoch: 9.57 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9768966670766464		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 1.9768966670766464 | validation: 2.63073424724749]
	TIME [epoch: 9.58 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0509112723172715		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 2.0509112723172715 | validation: 2.4307164400784194]
	TIME [epoch: 9.56 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.031315973604343		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 2.031315973604343 | validation: 2.824582007289272]
	TIME [epoch: 9.57 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210861661581242		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 2.210861661581242 | validation: 2.341294427487567]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.204448263269812		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 2.204448263269812 | validation: 3.055943089461043]
	TIME [epoch: 9.57 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2817702583164743		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 2.2817702583164743 | validation: 2.4800726423760473]
	TIME [epoch: 9.58 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9609609426745649		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 1.9609609426745649 | validation: 2.3339045384840915]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.030880466584117		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 2.030880466584117 | validation: 3.815214486393922]
	TIME [epoch: 9.58 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4318800337618316		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 2.4318800337618316 | validation: 2.397569108513269]
	TIME [epoch: 9.58 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3450975054149152		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 2.3450975054149152 | validation: 3.0930716185424325]
	TIME [epoch: 9.56 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2510562186406657		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 2.2510562186406657 | validation: 2.6201083191442875]
	TIME [epoch: 9.57 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.250894324960118		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 2.250894324960118 | validation: 2.8426750905172176]
	TIME [epoch: 9.58 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0446574479082305		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 2.0446574479082305 | validation: 2.358343476303588]
	TIME [epoch: 9.56 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3946689327051534		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 2.3946689327051534 | validation: 2.4120373360346834]
	TIME [epoch: 9.57 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1839565470027926		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 2.1839565470027926 | validation: 2.929982858625215]
	TIME [epoch: 9.57 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.111184508039357		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 2.111184508039357 | validation: 2.486744234492251]
	TIME [epoch: 9.59 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.012929786617843		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 2.012929786617843 | validation: 2.444031146628543]
	TIME [epoch: 9.57 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.014192655708355		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 2.014192655708355 | validation: 2.473152500526885]
	TIME [epoch: 9.58 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.966419000203156		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 1.966419000203156 | validation: 2.6813585855064996]
	TIME [epoch: 9.58 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040509335132988		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 2.040509335132988 | validation: 2.3736965869994737]
	TIME [epoch: 9.59 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9208725921816296		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 1.9208725921816296 | validation: 2.3981732182551885]
	TIME [epoch: 9.58 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9885302326541257		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 1.9885302326541257 | validation: 2.453217893029737]
	TIME [epoch: 9.57 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2180077593805265		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 2.2180077593805265 | validation: 2.5080478199164054]
	TIME [epoch: 9.57 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0250091391681		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 2.0250091391681 | validation: 2.6396557754925185]
	TIME [epoch: 9.57 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.07567421365826		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 2.07567421365826 | validation: 2.332677092423336]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0444004567215837		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 2.0444004567215837 | validation: 3.1921306545979644]
	TIME [epoch: 9.58 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.314654091819253		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 2.314654091819253 | validation: 2.616769162836117]
	TIME [epoch: 9.56 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.148360453626651		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 2.148360453626651 | validation: 2.6118178954686573]
	TIME [epoch: 9.59 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193478659051228		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 2.193478659051228 | validation: 2.3599503313157615]
	TIME [epoch: 9.58 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1148897274314034		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 2.1148897274314034 | validation: 2.821061938026424]
	TIME [epoch: 9.57 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.06807696112725		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 2.06807696112725 | validation: 2.55799929191112]
	TIME [epoch: 9.58 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.090387864531698		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 2.090387864531698 | validation: 2.404922679275273]
	TIME [epoch: 9.59 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0616086086986956		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 2.0616086086986956 | validation: 2.3449169917927577]
	TIME [epoch: 9.58 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9257397759503985		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 1.9257397759503985 | validation: 3.2864883020445665]
	TIME [epoch: 9.58 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1879270771269885		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 2.1879270771269885 | validation: 2.4153727632784916]
	TIME [epoch: 9.57 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0301633440669167		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 2.0301633440669167 | validation: 2.406071698139623]
	TIME [epoch: 9.59 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.002558888456563		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 2.002558888456563 | validation: 2.6872674231208276]
	TIME [epoch: 9.59 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.009516472321287		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 2.009516472321287 | validation: 2.359374471645491]
	TIME [epoch: 9.58 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9666594684021093		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 1.9666594684021093 | validation: 2.405841810857703]
	TIME [epoch: 9.58 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9336580133014007		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 1.9336580133014007 | validation: 2.4153083215458713]
	TIME [epoch: 9.6 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9284787236849223		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 1.9284787236849223 | validation: 2.725242745348439]
	TIME [epoch: 9.57 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.048312097743268		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 2.048312097743268 | validation: 2.431086396203942]
	TIME [epoch: 9.58 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9391917716771334		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 1.9391917716771334 | validation: 2.381269652953497]
	TIME [epoch: 9.57 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9514744425389534		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 1.9514744425389534 | validation: 2.5051331270793624]
	TIME [epoch: 9.58 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9954436149895514		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 1.9954436149895514 | validation: 2.3597635682316263]
	TIME [epoch: 9.57 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.96963218458442		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 1.96963218458442 | validation: 2.8926221887828025]
	TIME [epoch: 9.57 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0910856172565553		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 2.0910856172565553 | validation: 2.52046080980566]
	TIME [epoch: 9.56 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.091093115612783		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 2.091093115612783 | validation: 2.4988119994509668]
	TIME [epoch: 9.59 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.963445315504298		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 1.963445315504298 | validation: 2.8358617919003404]
	TIME [epoch: 9.57 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9982749564528721		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 1.9982749564528721 | validation: 2.3236196362034214]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0177849407795563		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 2.0177849407795563 | validation: 3.0486325353495283]
	TIME [epoch: 9.56 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2518168555558913		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 2.2518168555558913 | validation: 2.368180440500909]
	TIME [epoch: 9.58 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.173965482558202		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 2.173965482558202 | validation: 2.923096483293437]
	TIME [epoch: 9.57 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1875643639890034		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 2.1875643639890034 | validation: 2.4827706433783296]
	TIME [epoch: 9.55 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.055848018785094		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 2.055848018785094 | validation: 2.432076841301386]
	TIME [epoch: 9.56 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0983772760605595		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 2.0983772760605595 | validation: 2.4996791000427865]
	TIME [epoch: 9.58 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9880527853023255		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 1.9880527853023255 | validation: 2.7430814409069986]
	TIME [epoch: 9.57 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.055473848724405		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 2.055473848724405 | validation: 2.440349943890465]
	TIME [epoch: 9.56 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9468781436607827		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 1.9468781436607827 | validation: 2.322806272287885]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_479.pth
	Model improved!!!
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.022735855480984		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 2.022735855480984 | validation: 2.373599693393384]
	TIME [epoch: 9.58 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.15309782139797		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 2.15309782139797 | validation: 2.740044805071665]
	TIME [epoch: 9.54 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0902975323593678		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 2.0902975323593678 | validation: 2.498746777828093]
	TIME [epoch: 9.56 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0194639322137684		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 2.0194639322137684 | validation: 2.385877001550221]
	TIME [epoch: 9.55 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0707569657230107		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 2.0707569657230107 | validation: 2.65956407494607]
	TIME [epoch: 9.59 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2150892324776095		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 2.2150892324776095 | validation: 2.407892560081352]
	TIME [epoch: 9.56 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0966693543455235		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 2.0966693543455235 | validation: 2.399379692163868]
	TIME [epoch: 9.56 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.091599726173755		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 2.091599726173755 | validation: 2.871811699219454]
	TIME [epoch: 9.56 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9923799618976201		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 1.9923799618976201 | validation: 2.4883113523606513]
	TIME [epoch: 9.59 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0010403238985184		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 2.0010403238985184 | validation: 2.3950887526445745]
	TIME [epoch: 9.56 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181408951274956		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 2.181408951274956 | validation: 2.5394153455488295]
	TIME [epoch: 9.55 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0588096861817		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 2.0588096861817 | validation: 2.469011871127149]
	TIME [epoch: 9.56 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9960155031363258		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 1.9960155031363258 | validation: 2.335817575024372]
	TIME [epoch: 9.58 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0980147167257983		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 2.0980147167257983 | validation: 2.565488397502538]
	TIME [epoch: 9.56 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9365545930774526		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 1.9365545930774526 | validation: 2.6013510844572223]
	TIME [epoch: 9.57 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.041393479660041		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 2.041393479660041 | validation: 2.352051999497757]
	TIME [epoch: 9.57 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.91946919051132		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 1.91946919051132 | validation: 2.6718934387474507]
	TIME [epoch: 9.58 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0735381087182403		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 2.0735381087182403 | validation: 2.5415770471119097]
	TIME [epoch: 9.81 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9561538113278565		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 1.9561538113278565 | validation: 2.8014433117975894]
	TIME [epoch: 9.58 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0669021956745577		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 2.0669021956745577 | validation: 2.7648381929302523]
	TIME [epoch: 9.56 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1852038390818462		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 2.1852038390818462 | validation: 2.3595918406542613]
	TIME [epoch: 9.59 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.986491385386494		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 1.986491385386494 | validation: 2.5840170504512328]
	TIME [epoch: 9.57 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9742813142408124		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 1.9742813142408124 | validation: 2.3977030443867875]
	TIME [epoch: 9.57 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.912598204914855		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 1.912598204914855 | validation: 2.3400254175255277]
	TIME [epoch: 9.57 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9360838654552854		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 1.9360838654552854 | validation: 2.596136595215688]
	TIME [epoch: 9.58 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0222062377080894		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 2.0222062377080894 | validation: 2.4633383565111986]
	TIME [epoch: 9.56 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.364823233585935		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 2.364823233585935 | validation: 2.4813035987633674]
	TIME [epoch: 9.56 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9167626159593785		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 1.9167626159593785 | validation: 2.3495200106677667]
	TIME [epoch: 9.58 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9253731749371839		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 1.9253731749371839 | validation: 2.740107270424328]
	TIME [epoch: 9.57 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.126924104564846		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 2.126924104564846 | validation: 2.5386471481243267]
	TIME [epoch: 9.57 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0107836822290825		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 2.0107836822290825 | validation: 2.3979326764824553]
	TIME [epoch: 9.57 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.919008893801362		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 1.919008893801362 | validation: 2.494317241244584]
	TIME [epoch: 9.57 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9079777655339236		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 1.9079777655339236 | validation: 2.2880487379089494]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.859317494582268		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 1.859317494582268 | validation: 2.361636610003057]
	TIME [epoch: 9.56 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8803303730067746		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 1.8803303730067746 | validation: 2.4031465755248504]
	TIME [epoch: 9.57 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9791376798151608		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 1.9791376798151608 | validation: 2.320840132694803]
	TIME [epoch: 9.57 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8842157170770022		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 1.8842157170770022 | validation: 2.620672515877768]
	TIME [epoch: 9.58 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0800291784799727		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 2.0800291784799727 | validation: 2.491698249460429]
	TIME [epoch: 9.56 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0710284026822956		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 2.0710284026822956 | validation: 2.4429760250143464]
	TIME [epoch: 9.57 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9141488742006003		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 1.9141488742006003 | validation: 2.3690910907832614]
	TIME [epoch: 9.57 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.864501901986003		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 1.864501901986003 | validation: 2.3699623944381005]
	TIME [epoch: 9.58 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461978019829478		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 1.9461978019829478 | validation: 2.3363043975766873]
	TIME [epoch: 9.57 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.054662823846031		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 2.054662823846031 | validation: 2.7127468820023433]
	TIME [epoch: 9.56 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0360818981761333		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 2.0360818981761333 | validation: 2.339679777776547]
	TIME [epoch: 9.57 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9615237052534986		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 1.9615237052534986 | validation: 2.401289626049669]
	TIME [epoch: 9.58 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8999751070548956		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 1.8999751070548956 | validation: 2.4017173088706967]
	TIME [epoch: 9.56 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9639784245753387		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 1.9639784245753387 | validation: 2.3727276936213544]
	TIME [epoch: 9.57 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9921356848935936		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 1.9921356848935936 | validation: 2.341334998473351]
	TIME [epoch: 9.58 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8987229590320247		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 1.8987229590320247 | validation: 2.301259862525179]
	TIME [epoch: 9.57 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9744894634737933		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 1.9744894634737933 | validation: 2.4593835881304043]
	TIME [epoch: 9.56 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9027948074757155		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 1.9027948074757155 | validation: 2.4825164308773395]
	TIME [epoch: 9.57 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.006611212185649		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 2.006611212185649 | validation: 2.4450732782852533]
	TIME [epoch: 9.58 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9126175422226583		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 1.9126175422226583 | validation: 2.336618433501919]
	TIME [epoch: 9.57 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8997314005382127		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 1.8997314005382127 | validation: 2.6604400019711396]
	TIME [epoch: 9.57 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3732508879585		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 2.3732508879585 | validation: 2.4153477067442326]
	TIME [epoch: 9.56 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0738035442677027		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 2.0738035442677027 | validation: 2.497569948425995]
	TIME [epoch: 9.59 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9104652492414196		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 1.9104652492414196 | validation: 2.40272213728879]
	TIME [epoch: 9.57 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.90189818781757		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 1.90189818781757 | validation: 2.628403225186546]
	TIME [epoch: 9.58 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0369734367801464		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 2.0369734367801464 | validation: 2.512420433378977]
	TIME [epoch: 9.56 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.907777636645071		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 1.907777636645071 | validation: 2.381877211695838]
	TIME [epoch: 9.59 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9116497079271817		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 1.9116497079271817 | validation: 2.4286798890188352]
	TIME [epoch: 9.56 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8702377565133503		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 1.8702377565133503 | validation: 2.4383825429818087]
	TIME [epoch: 9.57 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9706694772577484		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 1.9706694772577484 | validation: 2.3043864994846808]
	TIME [epoch: 9.56 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8938487754479467		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 1.8938487754479467 | validation: 2.373642273305319]
	TIME [epoch: 9.58 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9641796284723398		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 1.9641796284723398 | validation: 2.4452577650151013]
	TIME [epoch: 9.57 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8774899275912993		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 1.8774899275912993 | validation: 2.324732993734665]
	TIME [epoch: 9.57 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9286478846898434		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 1.9286478846898434 | validation: 2.4728323084773964]
	TIME [epoch: 9.57 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.901637265001344		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 1.901637265001344 | validation: 2.37497296822573]
	TIME [epoch: 9.58 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941197177410578		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 1.941197177410578 | validation: 2.3067689003892293]
	TIME [epoch: 9.57 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8538021110184482		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 1.8538021110184482 | validation: 2.3236940112295734]
	TIME [epoch: 9.56 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8487547019669528		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 1.8487547019669528 | validation: 2.6232865252736515]
	TIME [epoch: 9.55 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0938776678432522		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 2.0938776678432522 | validation: 2.3115394392074315]
	TIME [epoch: 9.57 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9261545166468224		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 1.9261545166468224 | validation: 2.3560516421558315]
	TIME [epoch: 9.56 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.039183694970754		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 2.039183694970754 | validation: 2.6501585614086673]
	TIME [epoch: 9.56 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9384635734708575		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 1.9384635734708575 | validation: 2.3139989868149833]
	TIME [epoch: 9.56 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9366399127296856		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 1.9366399127296856 | validation: 2.627271300296791]
	TIME [epoch: 9.58 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.889584721169399		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 1.889584721169399 | validation: 2.3324632792999584]
	TIME [epoch: 9.56 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8857687363498534		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 1.8857687363498534 | validation: 2.2958997936332977]
	TIME [epoch: 9.57 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8367535600346536		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 1.8367535600346536 | validation: 2.320031013972811]
	TIME [epoch: 9.56 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8199859659145694		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 1.8199859659145694 | validation: 3.1517513907181134]
	TIME [epoch: 9.58 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1136518781632647		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 2.1136518781632647 | validation: 2.3170345018930654]
	TIME [epoch: 9.56 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9176139194012076		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 1.9176139194012076 | validation: 2.333986940518161]
	TIME [epoch: 9.56 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8566711911977627		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 1.8566711911977627 | validation: 2.319695874496182]
	TIME [epoch: 9.57 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421010572886286		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 1.9421010572886286 | validation: 2.31381301477068]
	TIME [epoch: 9.57 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9864091652298703		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 1.9864091652298703 | validation: 2.4093641080968853]
	TIME [epoch: 9.57 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.00399663069067		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 2.00399663069067 | validation: 2.364388409760902]
	TIME [epoch: 9.56 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9286344311999184		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 1.9286344311999184 | validation: 2.2781099291762463]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.833851151163291		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 1.833851151163291 | validation: 2.2917540845494324]
	TIME [epoch: 9.59 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0006142912199545		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 2.0006142912199545 | validation: 2.5954480384794096]
	TIME [epoch: 9.57 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9727978805013098		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 1.9727978805013098 | validation: 2.4445241440974663]
	TIME [epoch: 9.56 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8508991422617775		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 1.8508991422617775 | validation: 2.921914923013818]
	TIME [epoch: 9.56 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0504671610004914		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 2.0504671610004914 | validation: 2.3471189563142842]
	TIME [epoch: 9.57 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8609006381727184		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 1.8609006381727184 | validation: 2.4037491646552174]
	TIME [epoch: 9.55 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.910899643517825		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 1.910899643517825 | validation: 2.314070608973827]
	TIME [epoch: 9.55 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8361928800381446		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 1.8361928800381446 | validation: 2.285127382262188]
	TIME [epoch: 9.56 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.99837383999376		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 1.99837383999376 | validation: 2.5698117278094212]
	TIME [epoch: 9.58 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8836225310088608		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 1.8836225310088608 | validation: 2.3123772264661695]
	TIME [epoch: 9.56 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8540961283061101		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 1.8540961283061101 | validation: 2.312247312506416]
	TIME [epoch: 9.56 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9270334790338857		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 1.9270334790338857 | validation: 2.3553514728605247]
	TIME [epoch: 9.55 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9456312827979267		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 1.9456312827979267 | validation: 2.3241802086692864]
	TIME [epoch: 9.57 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8812742239078974		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 1.8812742239078974 | validation: 2.3797860991775917]
	TIME [epoch: 9.57 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9289674856728212		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 1.9289674856728212 | validation: 2.6388085623413655]
	TIME [epoch: 9.55 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9146246816962709		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 1.9146246816962709 | validation: 2.4681838270818175]
	TIME [epoch: 9.56 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9237456668649116		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 1.9237456668649116 | validation: 2.3103600723823976]
	TIME [epoch: 9.58 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8648094761831626		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 1.8648094761831626 | validation: 2.3008420348821286]
	TIME [epoch: 9.57 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8399249237340254		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 1.8399249237340254 | validation: 2.3156896797952524]
	TIME [epoch: 9.56 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8640797571459806		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 1.8640797571459806 | validation: 2.497335595542823]
	TIME [epoch: 9.56 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9165031608758056		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 1.9165031608758056 | validation: 2.3038855431450584]
	TIME [epoch: 9.57 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8657495425207968		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 1.8657495425207968 | validation: 2.407284788981849]
	TIME [epoch: 9.56 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8831016694180065		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 1.8831016694180065 | validation: 2.355383057420165]
	TIME [epoch: 9.56 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.844522046748747		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 1.844522046748747 | validation: 2.45661519307444]
	TIME [epoch: 9.56 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9449390702438087		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 1.9449390702438087 | validation: 2.308276153548713]
	TIME [epoch: 9.58 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.905044091181687		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 1.905044091181687 | validation: 2.4347668913850846]
	TIME [epoch: 9.58 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8554418126891103		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 1.8554418126891103 | validation: 2.4788718432849888]
	TIME [epoch: 9.58 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8885890738584847		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 1.8885890738584847 | validation: 2.3761854466351915]
	TIME [epoch: 9.58 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8732571891224246		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 1.8732571891224246 | validation: 2.2864729654998857]
	TIME [epoch: 9.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8198728440149208		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 1.8198728440149208 | validation: 2.4235560542348638]
	TIME [epoch: 9.58 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9289388305446618		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 1.9289388305446618 | validation: 2.3781959099460237]
	TIME [epoch: 9.58 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.929588538522679		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 1.929588538522679 | validation: 2.3817320015558527]
	TIME [epoch: 9.58 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.928268036548363		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 1.928268036548363 | validation: 2.3005715981863655]
	TIME [epoch: 9.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8399480663096548		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 1.8399480663096548 | validation: 2.319857080665165]
	TIME [epoch: 9.59 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8407672228934104		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 1.8407672228934104 | validation: 2.380935671933045]
	TIME [epoch: 9.59 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9109078702740248		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 1.9109078702740248 | validation: 2.5013618734524017]
	TIME [epoch: 9.58 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.926877727485889		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 1.926877727485889 | validation: 2.5655647690719126]
	TIME [epoch: 9.62 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9179004316662183		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 1.9179004316662183 | validation: 2.4146117871126767]
	TIME [epoch: 9.59 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9399160976499457		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 1.9399160976499457 | validation: 2.3972590578280326]
	TIME [epoch: 9.59 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8647251713363484		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 1.8647251713363484 | validation: 2.6590525139105607]
	TIME [epoch: 9.59 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9047248070037366		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 1.9047248070037366 | validation: 2.298112466286948]
	TIME [epoch: 9.61 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8772086280646092		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 1.8772086280646092 | validation: 2.2938958230005038]
	TIME [epoch: 9.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.852373112491025		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 1.852373112491025 | validation: 2.490543659693811]
	TIME [epoch: 9.58 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9804536496057192		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 1.9804536496057192 | validation: 2.283964024016233]
	TIME [epoch: 9.58 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.887012277976784		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 1.887012277976784 | validation: 2.6328043537684342]
	TIME [epoch: 9.62 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.889261928796715		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 1.889261928796715 | validation: 2.426643996130257]
	TIME [epoch: 9.58 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.953800012078673		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 1.953800012078673 | validation: 2.353708823428284]
	TIME [epoch: 9.58 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8676549390284287		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 1.8676549390284287 | validation: 2.4260573975779387]
	TIME [epoch: 9.59 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.879056426655815		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 1.879056426655815 | validation: 2.267197036548688]
	TIME [epoch: 9.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8798373608438002		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 1.8798373608438002 | validation: 2.5692808672008676]
	TIME [epoch: 9.56 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.891187183589365		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 1.891187183589365 | validation: 2.35712287176238]
	TIME [epoch: 9.56 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8381464011206128		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 1.8381464011206128 | validation: 2.3840684508495973]
	TIME [epoch: 9.57 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.827855655015958		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 1.827855655015958 | validation: 2.421729301478498]
	TIME [epoch: 9.58 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.92040914417236		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 1.92040914417236 | validation: 2.3110744651159045]
	TIME [epoch: 9.57 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9634702650160016		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 1.9634702650160016 | validation: 2.747246106841314]
	TIME [epoch: 9.56 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.084379548020821		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 2.084379548020821 | validation: 2.491694097346301]
	TIME [epoch: 9.58 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9400855889408775		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 1.9400855889408775 | validation: 2.3353192807931946]
	TIME [epoch: 9.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8571317111439605		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 1.8571317111439605 | validation: 2.427659355181678]
	TIME [epoch: 9.57 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.030291091016667		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 2.030291091016667 | validation: 2.341459252677099]
	TIME [epoch: 9.56 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0374443555146278		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 2.0374443555146278 | validation: 2.261406927542182]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8853505441459515		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 1.8853505441459515 | validation: 2.443759027056657]
	TIME [epoch: 9.58 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9102610366707182		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 1.9102610366707182 | validation: 2.2696640829392116]
	TIME [epoch: 9.56 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8385734447627236		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 1.8385734447627236 | validation: 2.5383787913045848]
	TIME [epoch: 9.57 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9082026739705054		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 1.9082026739705054 | validation: 2.3769490767483155]
	TIME [epoch: 9.58 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8533705513663403		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 1.8533705513663403 | validation: 2.580765782816662]
	TIME [epoch: 9.57 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9377746980506505		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 1.9377746980506505 | validation: 2.2821912607551322]
	TIME [epoch: 9.57 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8410311688366836		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 1.8410311688366836 | validation: 2.4517008391260346]
	TIME [epoch: 9.57 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8571189126885712		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 1.8571189126885712 | validation: 2.425434111624849]
	TIME [epoch: 9.58 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9490637948865672		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 1.9490637948865672 | validation: 2.295001706480947]
	TIME [epoch: 9.56 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8309414295719673		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 1.8309414295719673 | validation: 2.2700519246453252]
	TIME [epoch: 9.55 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8744853276254438		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 1.8744853276254438 | validation: 2.4033585067685443]
	TIME [epoch: 9.55 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8579266551495643		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 1.8579266551495643 | validation: 2.2932073380830045]
	TIME [epoch: 9.56 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8476836155840146		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 1.8476836155840146 | validation: 2.398662375613618]
	TIME [epoch: 9.57 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.837283534604508		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 1.837283534604508 | validation: 2.321438250972512]
	TIME [epoch: 9.56 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8902271524364331		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 1.8902271524364331 | validation: 2.659161546827895]
	TIME [epoch: 9.55 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9192610937101862		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 1.9192610937101862 | validation: 2.2762034811612297]
	TIME [epoch: 9.58 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8797375963743204		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 1.8797375963743204 | validation: 2.2789797454205867]
	TIME [epoch: 9.56 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8215201511314132		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 1.8215201511314132 | validation: 2.313303839408962]
	TIME [epoch: 9.56 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8538835630514527		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 1.8538835630514527 | validation: 2.2749338296237465]
	TIME [epoch: 9.56 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8943736816188188		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 1.8943736816188188 | validation: 2.3121060205246846]
	TIME [epoch: 9.57 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.818291092993227		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 1.818291092993227 | validation: 2.317984895017094]
	TIME [epoch: 9.57 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8455913699786688		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 1.8455913699786688 | validation: 2.5112770390674646]
	TIME [epoch: 9.56 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.850448209539994		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 1.850448209539994 | validation: 2.325986646427201]
	TIME [epoch: 9.56 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.916233404715991		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 1.916233404715991 | validation: 2.3495687086320407]
	TIME [epoch: 9.57 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.854101851153954		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 1.854101851153954 | validation: 2.289924096565201]
	TIME [epoch: 9.57 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8708261019179289		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 1.8708261019179289 | validation: 2.3359637714987556]
	TIME [epoch: 9.57 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.796650273008448		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 1.796650273008448 | validation: 2.3120170048551505]
	TIME [epoch: 9.57 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9160933521625725		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 1.9160933521625725 | validation: 2.2834716427957953]
	TIME [epoch: 9.59 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.812472629032897		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 1.812472629032897 | validation: 2.309003977186762]
	TIME [epoch: 9.56 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.860112151677571		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 1.860112151677571 | validation: 2.3722073167206146]
	TIME [epoch: 9.57 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8385948625599517		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 1.8385948625599517 | validation: 2.2748571305468603]
	TIME [epoch: 9.56 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8067190394592607		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 1.8067190394592607 | validation: 2.3039702182823607]
	TIME [epoch: 9.57 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8579694895498882		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 1.8579694895498882 | validation: 2.5557923819166324]
	TIME [epoch: 9.58 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.92765828754534		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 1.92765828754534 | validation: 2.3730908309418646]
	TIME [epoch: 9.56 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8071504554544624		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 1.8071504554544624 | validation: 2.3182241629126823]
	TIME [epoch: 9.55 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.892934000629269		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 1.892934000629269 | validation: 2.404709983414557]
	TIME [epoch: 9.58 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.849548557519391		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 1.849548557519391 | validation: 2.337118630868769]
	TIME [epoch: 9.56 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8155423561772484		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 1.8155423561772484 | validation: 2.5107165487372374]
	TIME [epoch: 9.55 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8597068487719917		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 1.8597068487719917 | validation: 2.3269740275788]
	TIME [epoch: 9.54 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8734014588029844		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 1.8734014588029844 | validation: 2.3664174136265816]
	TIME [epoch: 9.56 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8323561141155271		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 1.8323561141155271 | validation: 2.2889034868154705]
	TIME [epoch: 9.54 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.846059582166495		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 1.846059582166495 | validation: 2.4397428923349658]
	TIME [epoch: 9.54 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8384170035315832		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 1.8384170035315832 | validation: 2.2896054598799433]
	TIME [epoch: 9.54 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8240766611449442		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 1.8240766611449442 | validation: 2.2846685448544424]
	TIME [epoch: 9.57 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0260285256678072		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 2.0260285256678072 | validation: 2.2690041862757635]
	TIME [epoch: 9.55 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8215535153149758		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 1.8215535153149758 | validation: 2.262476735555673]
	TIME [epoch: 9.55 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.846374685072518		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 1.846374685072518 | validation: 2.330269122590969]
	TIME [epoch: 9.55 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8699382874490937		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 1.8699382874490937 | validation: 2.4940636246197165]
	TIME [epoch: 9.57 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8448848347984133		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 1.8448848347984133 | validation: 2.3998489675182526]
	TIME [epoch: 9.56 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8499655533994166		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 1.8499655533994166 | validation: 2.3408400245908703]
	TIME [epoch: 9.56 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9231099504820623		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 1.9231099504820623 | validation: 2.3038007904194746]
	TIME [epoch: 9.56 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8408092859309613		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 1.8408092859309613 | validation: 2.4176574641685966]
	TIME [epoch: 9.57 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8335197647100334		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 1.8335197647100334 | validation: 2.2736902271954085]
	TIME [epoch: 9.55 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.812425856651735		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 1.812425856651735 | validation: 2.3264559805428355]
	TIME [epoch: 9.56 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.808177473378855		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 1.808177473378855 | validation: 2.2776358659843474]
	TIME [epoch: 9.56 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8127698986725267		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 1.8127698986725267 | validation: 2.2664756910785457]
	TIME [epoch: 9.56 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8510791171561798		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 1.8510791171561798 | validation: 2.3797250320625616]
	TIME [epoch: 9.55 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8510032547900643		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 1.8510032547900643 | validation: 2.517867984734745]
	TIME [epoch: 9.56 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8854785506756326		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 1.8854785506756326 | validation: 2.3596262701671877]
	TIME [epoch: 9.55 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.003148288267396		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 2.003148288267396 | validation: 2.3616902351297338]
	TIME [epoch: 9.58 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8581563249187032		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 1.8581563249187032 | validation: 2.5271443183403566]
	TIME [epoch: 9.55 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.882121905945337		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 1.882121905945337 | validation: 2.2803898229865767]
	TIME [epoch: 9.55 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8566001020446812		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 1.8566001020446812 | validation: 2.419798525077544]
	TIME [epoch: 9.56 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8787211183371852		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 1.8787211183371852 | validation: 2.3458006926420727]
	TIME [epoch: 9.58 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.799412644590022		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 1.799412644590022 | validation: 2.2994696463056097]
	TIME [epoch: 9.55 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.796861307382187		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 1.796861307382187 | validation: 2.316841511593618]
	TIME [epoch: 9.57 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8343939454249187		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 1.8343939454249187 | validation: 2.3065434053229943]
	TIME [epoch: 9.54 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8192056499018332		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 1.8192056499018332 | validation: 2.4238006866708783]
	TIME [epoch: 9.58 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8381476512500032		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 1.8381476512500032 | validation: 2.2759384103779587]
	TIME [epoch: 9.55 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7999631806441112		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 1.7999631806441112 | validation: 2.351861468373053]
	TIME [epoch: 9.55 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8238537066775535		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 1.8238537066775535 | validation: 2.2872022946110833]
	TIME [epoch: 9.55 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8567069803782044		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 1.8567069803782044 | validation: 2.389121872009488]
	TIME [epoch: 9.58 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8710518695063463		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 1.8710518695063463 | validation: 2.2968765891324185]
	TIME [epoch: 9.56 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8123045969556606		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 1.8123045969556606 | validation: 2.283927452931558]
	TIME [epoch: 9.56 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8427180758817918		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 1.8427180758817918 | validation: 2.310084940357076]
	TIME [epoch: 9.55 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8807616312140918		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 1.8807616312140918 | validation: 2.3455309971434035]
	TIME [epoch: 9.59 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8411350844266505		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 1.8411350844266505 | validation: 2.2869238035163018]
	TIME [epoch: 9.56 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.787434087833125		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 1.787434087833125 | validation: 2.455657026263434]
	TIME [epoch: 9.57 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.836093706645706		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 1.836093706645706 | validation: 2.4381975677464562]
	TIME [epoch: 9.56 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8216006938610665		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 1.8216006938610665 | validation: 2.2649885408208474]
	TIME [epoch: 9.59 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8410368144458307		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 1.8410368144458307 | validation: 2.3702599958576975]
	TIME [epoch: 9.56 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8408969462560567		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 1.8408969462560567 | validation: 2.340153369419705]
	TIME [epoch: 9.55 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.82547134655391		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 1.82547134655391 | validation: 2.2534827792462138]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_709.pth
	Model improved!!!
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8666940934874752		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 1.8666940934874752 | validation: 2.343570291193035]
	TIME [epoch: 9.58 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8281094666195838		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 1.8281094666195838 | validation: 2.3094082105693583]
	TIME [epoch: 9.55 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7967264348016763		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 1.7967264348016763 | validation: 2.300570979174897]
	TIME [epoch: 9.55 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8131626803138459		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 1.8131626803138459 | validation: 2.3831362641304343]
	TIME [epoch: 9.55 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8205732507412726		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 1.8205732507412726 | validation: 2.2761613338653404]
	TIME [epoch: 9.58 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.819975713471552		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 1.819975713471552 | validation: 2.3264329947623597]
	TIME [epoch: 9.56 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8617474257452717		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 1.8617474257452717 | validation: 2.3067776701393305]
	TIME [epoch: 9.56 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.835812307354478		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 1.835812307354478 | validation: 2.375040613435492]
	TIME [epoch: 9.56 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.858547440019445		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 1.858547440019445 | validation: 2.3673977222344456]
	TIME [epoch: 9.59 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8193078762664077		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 1.8193078762664077 | validation: 2.4357330537510657]
	TIME [epoch: 9.57 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.851115188911875		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 1.851115188911875 | validation: 2.3292847743088805]
	TIME [epoch: 9.57 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8357191854194117		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 1.8357191854194117 | validation: 2.325550377026587]
	TIME [epoch: 9.57 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9178234051908116		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 1.9178234051908116 | validation: 2.3640496387383063]
	TIME [epoch: 9.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8541321009384775		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 1.8541321009384775 | validation: 2.2850940039171754]
	TIME [epoch: 9.56 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8386979840166782		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 1.8386979840166782 | validation: 2.310376305044697]
	TIME [epoch: 9.56 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8408198736496637		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 1.8408198736496637 | validation: 2.4740135681770337]
	TIME [epoch: 9.55 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.838095835288107		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 1.838095835288107 | validation: 2.3782006186544367]
	TIME [epoch: 9.6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8150188941068646		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 1.8150188941068646 | validation: 2.317794832569881]
	TIME [epoch: 9.57 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8231932843608742		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 1.8231932843608742 | validation: 2.430225219024071]
	TIME [epoch: 9.57 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.840067019015739		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 1.840067019015739 | validation: 2.285673357671697]
	TIME [epoch: 9.56 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.862478330869005		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 1.862478330869005 | validation: 2.2963280830494854]
	TIME [epoch: 9.59 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.802823327671937		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 1.802823327671937 | validation: 2.3050480598113827]
	TIME [epoch: 9.56 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.80519163563823		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 1.80519163563823 | validation: 2.4550371871568197]
	TIME [epoch: 9.57 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8396197931009095		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 1.8396197931009095 | validation: 2.289164840028248]
	TIME [epoch: 9.56 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8823760207866318		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 1.8823760207866318 | validation: 2.527707469291865]
	TIME [epoch: 9.58 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.851566817142449		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 1.851566817142449 | validation: 2.540951366257193]
	TIME [epoch: 9.56 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8657451057365766		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 1.8657451057365766 | validation: 2.2796011056381817]
	TIME [epoch: 9.56 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9034118187129692		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 1.9034118187129692 | validation: 2.601747871360519]
	TIME [epoch: 9.55 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8923912704354358		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 1.8923912704354358 | validation: 2.297141331484011]
	TIME [epoch: 9.58 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8049720107915985		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 1.8049720107915985 | validation: 2.3007268458740686]
	TIME [epoch: 9.55 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.855221329482358		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 1.855221329482358 | validation: 2.298391081406687]
	TIME [epoch: 9.55 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8150270504171242		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 1.8150270504171242 | validation: 2.3479020932539285]
	TIME [epoch: 9.56 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9267581178165194		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 1.9267581178165194 | validation: 2.339488042505906]
	TIME [epoch: 9.58 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8339070299608902		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 1.8339070299608902 | validation: 2.3580964416031667]
	TIME [epoch: 9.54 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8147317276211203		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 1.8147317276211203 | validation: 2.2738228199797756]
	TIME [epoch: 9.56 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8047201380087614		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 1.8047201380087614 | validation: 2.267915162988394]
	TIME [epoch: 9.55 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8645940074488139		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 1.8645940074488139 | validation: 2.2914013110485665]
	TIME [epoch: 9.59 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.802814995783311		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 1.802814995783311 | validation: 2.359732175119699]
	TIME [epoch: 9.55 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8043736875100638		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 1.8043736875100638 | validation: 2.314707408741625]
	TIME [epoch: 9.55 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8679761692787165		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 1.8679761692787165 | validation: 2.2919589975907666]
	TIME [epoch: 9.53 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8219144183130684		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 1.8219144183130684 | validation: 2.296116978953939]
	TIME [epoch: 9.57 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8179068922172514		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 1.8179068922172514 | validation: 2.2683300277694958]
	TIME [epoch: 9.56 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8102449363689292		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 1.8102449363689292 | validation: 2.3817157155988906]
	TIME [epoch: 9.54 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8477494377802959		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 1.8477494377802959 | validation: 2.284479116849553]
	TIME [epoch: 9.55 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8236534025377646		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 1.8236534025377646 | validation: 2.4914577311036292]
	TIME [epoch: 9.58 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8621440491096888		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 1.8621440491096888 | validation: 2.3370625556640867]
	TIME [epoch: 9.54 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.807788962180394		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 1.807788962180394 | validation: 2.4766247180758323]
	TIME [epoch: 9.57 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8558416842323868		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 1.8558416842323868 | validation: 2.3353684409352153]
	TIME [epoch: 9.56 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.834231003601862		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 1.834231003601862 | validation: 2.606747438601082]
	TIME [epoch: 9.58 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.862757967973001		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 1.862757967973001 | validation: 2.275528347696027]
	TIME [epoch: 9.55 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7996649415852246		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 1.7996649415852246 | validation: 2.2890480556531325]
	TIME [epoch: 9.55 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8140875525438336		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 1.8140875525438336 | validation: 2.359748004667641]
	TIME [epoch: 9.55 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8695698711859272		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 1.8695698711859272 | validation: 2.4455297557071]
	TIME [epoch: 9.57 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8673068976836913		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 1.8673068976836913 | validation: 2.334666224143369]
	TIME [epoch: 9.55 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8026671284912479		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 1.8026671284912479 | validation: 2.2962017279766207]
	TIME [epoch: 9.56 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.857386559266266		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 1.857386559266266 | validation: 2.4105447050157385]
	TIME [epoch: 9.56 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8139716919271733		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 1.8139716919271733 | validation: 2.282858429478838]
	TIME [epoch: 9.57 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8418172075084231		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 1.8418172075084231 | validation: 2.2707176881383244]
	TIME [epoch: 9.56 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8069098217596378		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 1.8069098217596378 | validation: 2.3495964707459227]
	TIME [epoch: 9.55 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8999796276067564		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 1.8999796276067564 | validation: 2.2985586992536207]
	TIME [epoch: 9.54 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.825942933740531		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 1.825942933740531 | validation: 2.3088616037115437]
	TIME [epoch: 9.57 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7925033137896573		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 1.7925033137896573 | validation: 2.2657417921784915]
	TIME [epoch: 9.54 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.828759876180587		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 1.828759876180587 | validation: 2.3387154859491086]
	TIME [epoch: 9.57 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8284075248148013		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 1.8284075248148013 | validation: 2.268221128421358]
	TIME [epoch: 9.57 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.794865241072501		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 1.794865241072501 | validation: 2.2809362837918776]
	TIME [epoch: 9.58 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8192642704850397		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 1.8192642704850397 | validation: 2.2642535085363678]
	TIME [epoch: 9.55 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8114691216240115		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 1.8114691216240115 | validation: 2.2984961644349258]
	TIME [epoch: 9.53 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8053391037420006		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 1.8053391037420006 | validation: 2.2907289315377457]
	TIME [epoch: 9.57 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8287891795779623		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 1.8287891795779623 | validation: 2.496663299714167]
	TIME [epoch: 9.56 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.844175618066113		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 1.844175618066113 | validation: 2.2794037622984518]
	TIME [epoch: 9.53 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7909156175281118		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 1.7909156175281118 | validation: 2.4193323219736893]
	TIME [epoch: 9.55 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8578001684336383		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 1.8578001684336383 | validation: 2.311513813218663]
	TIME [epoch: 9.56 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8153980305203787		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 1.8153980305203787 | validation: 2.399808762824624]
	TIME [epoch: 9.54 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8012335238776473		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 1.8012335238776473 | validation: 2.2760022785024017]
	TIME [epoch: 9.54 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7984030639847846		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 1.7984030639847846 | validation: 2.3218927236833684]
	TIME [epoch: 9.54 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8151123036436179		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 1.8151123036436179 | validation: 2.2727618511219747]
	TIME [epoch: 9.58 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7994503212624626		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 1.7994503212624626 | validation: 2.345763838958113]
	TIME [epoch: 9.55 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.888663881738794		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 1.888663881738794 | validation: 2.2940388574115134]
	TIME [epoch: 9.55 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7982028481367358		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 1.7982028481367358 | validation: 2.2727594543353566]
	TIME [epoch: 9.54 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.79697993536194		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 1.79697993536194 | validation: 2.269081542864102]
	TIME [epoch: 9.55 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8140965581137327		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 1.8140965581137327 | validation: 2.382251801544]
	TIME [epoch: 9.58 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8238702617025695		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 1.8238702617025695 | validation: 2.436002428300599]
	TIME [epoch: 9.56 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8261308218044132		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 1.8261308218044132 | validation: 2.314293097154217]
	TIME [epoch: 9.56 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7967831641564733		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 1.7967831641564733 | validation: 2.269572541793545]
	TIME [epoch: 9.57 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7809247830849824		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 1.7809247830849824 | validation: 2.2695503140494955]
	TIME [epoch: 9.57 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8070043907067248		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 1.8070043907067248 | validation: 2.2533605302212716]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_795.pth
	Model improved!!!
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8804509767203546		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 1.8804509767203546 | validation: 2.4216589709928393]
	TIME [epoch: 9.55 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8181758163664061		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 1.8181758163664061 | validation: 2.2955010571478125]
	TIME [epoch: 9.57 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8009608656178409		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 1.8009608656178409 | validation: 2.2924976306656695]
	TIME [epoch: 9.55 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8284673131655862		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 1.8284673131655862 | validation: 2.3163541839491972]
	TIME [epoch: 9.54 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7952770068551864		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 1.7952770068551864 | validation: 2.30522308196155]
	TIME [epoch: 9.55 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8138519085546183		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 1.8138519085546183 | validation: 2.3228134381329024]
	TIME [epoch: 9.57 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8748452840939536		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 1.8748452840939536 | validation: 2.3085265653202076]
	TIME [epoch: 9.55 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8445577975553966		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 1.8445577975553966 | validation: 2.282662847805704]
	TIME [epoch: 9.56 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8055292611196658		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 1.8055292611196658 | validation: 2.316612197241528]
	TIME [epoch: 9.58 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8112167875488006		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 1.8112167875488006 | validation: 2.306000682964594]
	TIME [epoch: 9.57 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8076364756194796		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 1.8076364756194796 | validation: 2.3794402071471197]
	TIME [epoch: 9.56 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8034347299610256		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 1.8034347299610256 | validation: 2.3186141045401003]
	TIME [epoch: 9.55 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7742211532976206		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 1.7742211532976206 | validation: 2.3188342254794185]
	TIME [epoch: 9.54 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7999569422849286		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 1.7999569422849286 | validation: 2.296311384318768]
	TIME [epoch: 9.59 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8127134198770036		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 1.8127134198770036 | validation: 2.3188661879036863]
	TIME [epoch: 9.58 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8081156589931175		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 1.8081156589931175 | validation: 2.351170428789148]
	TIME [epoch: 9.54 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7907886638723074		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 1.7907886638723074 | validation: 2.3110020588627487]
	TIME [epoch: 9.54 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.799481945164792		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 1.799481945164792 | validation: 2.2718246778830906]
	TIME [epoch: 9.55 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.78872770954045		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 1.78872770954045 | validation: 2.4261710496463507]
	TIME [epoch: 9.55 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7969887421438187		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 1.7969887421438187 | validation: 2.496604247934751]
	TIME [epoch: 9.55 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8398461358977507		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 1.8398461358977507 | validation: 2.3582519629911896]
	TIME [epoch: 9.54 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8249148247382496		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 1.8249148247382496 | validation: 2.3362027999567068]
	TIME [epoch: 9.56 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8133064629579068		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 1.8133064629579068 | validation: 2.261836620639986]
	TIME [epoch: 9.57 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7792753080791637		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 1.7792753080791637 | validation: 2.2614798447178135]
	TIME [epoch: 9.55 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7944250320595774		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 1.7944250320595774 | validation: 2.2830368751817667]
	TIME [epoch: 9.55 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.82111900324522		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 1.82111900324522 | validation: 2.286096313631623]
	TIME [epoch: 9.57 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8111642064145301		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 1.8111642064145301 | validation: 2.3189706619304333]
	TIME [epoch: 9.55 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7990138525816675		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 1.7990138525816675 | validation: 2.320733444148029]
	TIME [epoch: 9.55 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.852212225983282		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 1.852212225983282 | validation: 2.4019570325969717]
	TIME [epoch: 9.56 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8066592597622744		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 1.8066592597622744 | validation: 2.324733989568183]
	TIME [epoch: 9.58 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8017514374558516		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 1.8017514374558516 | validation: 2.3188065025020324]
	TIME [epoch: 9.56 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7958302803692625		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 1.7958302803692625 | validation: 2.3925902002826405]
	TIME [epoch: 9.56 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.803249789334675		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 1.803249789334675 | validation: 2.2760165515894992]
	TIME [epoch: 9.56 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7948175830679098		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 1.7948175830679098 | validation: 2.3166444580213703]
	TIME [epoch: 9.56 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.802449968860077		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 1.802449968860077 | validation: 2.3157773010600566]
	TIME [epoch: 9.57 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8391841587959294		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 1.8391841587959294 | validation: 2.29747709493825]
	TIME [epoch: 9.54 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8083300705407797		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 1.8083300705407797 | validation: 2.309883245558527]
	TIME [epoch: 9.53 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8294770778279226		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 1.8294770778279226 | validation: 2.357344584190957]
	TIME [epoch: 9.57 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7907283235531561		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 1.7907283235531561 | validation: 2.268783016327092]
	TIME [epoch: 9.55 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7689299605711857		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 1.7689299605711857 | validation: 2.251711270999716]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7791445425724042		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 1.7791445425724042 | validation: 2.28088190968921]
	TIME [epoch: 9.54 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.796267705726247		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 1.796267705726247 | validation: 2.257135531033912]
	TIME [epoch: 9.57 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.804197543737478		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 1.804197543737478 | validation: 2.287081932571781]
	TIME [epoch: 9.54 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8081324272940738		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 1.8081324272940738 | validation: 2.2981046094926754]
	TIME [epoch: 9.56 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8269024058095216		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 1.8269024058095216 | validation: 2.2615993922302473]
	TIME [epoch: 9.55 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7767944169330527		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 1.7767944169330527 | validation: 2.3244106601277617]
	TIME [epoch: 9.56 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8180171755865149		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 1.8180171755865149 | validation: 2.2674830961043484]
	TIME [epoch: 9.53 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.790691626217874		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 1.790691626217874 | validation: 2.246677461090604]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9095764682138465		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 1.9095764682138465 | validation: 2.397059177678749]
	TIME [epoch: 9.55 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8162132953446335		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 1.8162132953446335 | validation: 2.295672596307797]
	TIME [epoch: 9.57 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7875847914297438		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 1.7875847914297438 | validation: 2.26198203487678]
	TIME [epoch: 9.57 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7870752731787498		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 1.7870752731787498 | validation: 2.27563017338377]
	TIME [epoch: 9.56 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7841135998793276		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 1.7841135998793276 | validation: 2.304969497819574]
	TIME [epoch: 9.56 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8194461853191495		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 1.8194461853191495 | validation: 2.293794728343023]
	TIME [epoch: 9.58 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.789090610744052		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 1.789090610744052 | validation: 2.283056609425125]
	TIME [epoch: 9.56 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.829058480487901		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 1.829058480487901 | validation: 2.2902128051023856]
	TIME [epoch: 9.55 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.867003899123283		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 1.867003899123283 | validation: 2.2802760665795097]
	TIME [epoch: 9.55 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7847785114546515		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 1.7847785114546515 | validation: 2.3267977413213687]
	TIME [epoch: 9.57 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8396485904272524		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 1.8396485904272524 | validation: 2.308268464714508]
	TIME [epoch: 9.55 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.793481458100408		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 1.793481458100408 | validation: 2.2625315171850002]
	TIME [epoch: 9.55 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7922092489519987		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 1.7922092489519987 | validation: 2.322522717132051]
	TIME [epoch: 9.55 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7778419479391594		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 1.7778419479391594 | validation: 2.267383258163866]
	TIME [epoch: 9.58 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7922109296447506		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 1.7922109296447506 | validation: 2.360878090201672]
	TIME [epoch: 9.55 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8687560325136636		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 1.8687560325136636 | validation: 2.2607843139833497]
	TIME [epoch: 9.56 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7761190333679053		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 1.7761190333679053 | validation: 2.343689074822247]
	TIME [epoch: 9.55 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7906840351325635		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 1.7906840351325635 | validation: 2.400792998430836]
	TIME [epoch: 9.57 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.815361093755337		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 1.815361093755337 | validation: 2.271068758976752]
	TIME [epoch: 9.56 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7805786596352378		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 1.7805786596352378 | validation: 2.2770614908538915]
	TIME [epoch: 9.56 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7689554408355463		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 1.7689554408355463 | validation: 2.29248550237396]
	TIME [epoch: 9.55 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8071517731135593		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 1.8071517731135593 | validation: 2.285735387919642]
	TIME [epoch: 9.56 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8076232242669028		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 1.8076232242669028 | validation: 2.281173164041314]
	TIME [epoch: 9.55 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7640388549571937		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 1.7640388549571937 | validation: 2.3126095067062478]
	TIME [epoch: 9.54 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7817918022392973		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 1.7817918022392973 | validation: 2.3291576958858444]
	TIME [epoch: 9.56 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.769510278047552		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 1.769510278047552 | validation: 2.485002557319841]
	TIME [epoch: 9.58 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8237855094804476		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 1.8237855094804476 | validation: 2.3593738449200297]
	TIME [epoch: 9.56 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8423206586152496		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 1.8423206586152496 | validation: 2.339265990641291]
	TIME [epoch: 9.55 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8131460676936235		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 1.8131460676936235 | validation: 2.3976482840499256]
	TIME [epoch: 9.57 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.847980730910043		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 1.847980730910043 | validation: 2.2878882644110923]
	TIME [epoch: 9.57 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8001286128785303		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 1.8001286128785303 | validation: 2.3345994586627983]
	TIME [epoch: 9.56 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7724145960554023		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 1.7724145960554023 | validation: 2.275141709542644]
	TIME [epoch: 9.54 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8018673685096467		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 1.8018673685096467 | validation: 2.302229032108477]
	TIME [epoch: 9.56 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.840644706027632		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 1.840644706027632 | validation: 2.2894349768230966]
	TIME [epoch: 9.58 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7855449210171546		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 1.7855449210171546 | validation: 2.2830723609044368]
	TIME [epoch: 9.57 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7731560592868203		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 1.7731560592868203 | validation: 2.2922749114660785]
	TIME [epoch: 9.55 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7944089515269308		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 1.7944089515269308 | validation: 2.268098226491768]
	TIME [epoch: 9.56 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.792456443031276		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 1.792456443031276 | validation: 2.2748983437808654]
	TIME [epoch: 9.58 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7831063533674356		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 1.7831063533674356 | validation: 2.2524184413372916]
	TIME [epoch: 9.56 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7979000818064095		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 1.7979000818064095 | validation: 2.2858186650016212]
	TIME [epoch: 9.56 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8092188509992604		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 1.8092188509992604 | validation: 2.2891015726617367]
	TIME [epoch: 9.57 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7757971656379254		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 1.7757971656379254 | validation: 2.2752883614300026]
	TIME [epoch: 9.59 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8264653468181649		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 1.8264653468181649 | validation: 2.2984186992870015]
	TIME [epoch: 9.58 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7760583622703283		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 1.7760583622703283 | validation: 2.3905189086086573]
	TIME [epoch: 9.58 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.791842113799629		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 1.791842113799629 | validation: 2.303636783043863]
	TIME [epoch: 9.57 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.816160351567491		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 1.816160351567491 | validation: 2.272347956397616]
	TIME [epoch: 9.59 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7798690437463143		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 1.7798690437463143 | validation: 2.327991467683906]
	TIME [epoch: 9.58 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8027643837309486		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 1.8027643837309486 | validation: 2.306262699677126]
	TIME [epoch: 9.55 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7780536588549727		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 1.7780536588549727 | validation: 2.2754361367700047]
	TIME [epoch: 9.57 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7993143712812778		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 1.7993143712812778 | validation: 2.2576196731970692]
	TIME [epoch: 9.58 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8143138800186775		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 1.8143138800186775 | validation: 2.3342488419341656]
	TIME [epoch: 9.57 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.772133257804733		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 1.772133257804733 | validation: 2.321563338106557]
	TIME [epoch: 9.56 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7852981163787383		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 1.7852981163787383 | validation: 2.308068916272632]
	TIME [epoch: 9.57 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.775491295095764		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 1.775491295095764 | validation: 2.2768023826595307]
	TIME [epoch: 9.58 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7702172991463136		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 1.7702172991463136 | validation: 2.271712549409415]
	TIME [epoch: 9.57 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8144361148808634		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 1.8144361148808634 | validation: 2.266729766674286]
	TIME [epoch: 9.57 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8203833964921983		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 1.8203833964921983 | validation: 2.2971343757675204]
	TIME [epoch: 9.57 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7958358260012637		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 1.7958358260012637 | validation: 2.263285820646192]
	TIME [epoch: 9.57 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.782811059399716		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 1.782811059399716 | validation: 2.2481095286968933]
	TIME [epoch: 9.58 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8083131811592152		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 1.8083131811592152 | validation: 2.2852312934234584]
	TIME [epoch: 9.56 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7642018237363586		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 1.7642018237363586 | validation: 2.30765915784052]
	TIME [epoch: 9.59 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.804091638557797		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 1.804091638557797 | validation: 2.329389673114242]
	TIME [epoch: 9.57 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7859447071544732		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 1.7859447071544732 | validation: 2.286107049664122]
	TIME [epoch: 9.56 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7775382665676482		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 1.7775382665676482 | validation: 2.341204261262453]
	TIME [epoch: 9.55 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.776324667931787		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 1.776324667931787 | validation: 2.288662886489262]
	TIME [epoch: 9.57 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8495683461862906		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 1.8495683461862906 | validation: 2.256456546444877]
	TIME [epoch: 9.58 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7928477114272723		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 1.7928477114272723 | validation: 2.3268551261103956]
	TIME [epoch: 9.56 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7964841843103367		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 1.7964841843103367 | validation: 2.252213450693424]
	TIME [epoch: 9.56 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7907767130940506		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 1.7907767130940506 | validation: 2.2576756908642457]
	TIME [epoch: 9.57 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8035702856865263		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 1.8035702856865263 | validation: 2.345633635798651]
	TIME [epoch: 9.57 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7686255136062865		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 1.7686255136062865 | validation: 2.246100675639843]
	TIME [epoch: 9.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7722519828102485		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 1.7722519828102485 | validation: 2.277451872977117]
	TIME [epoch: 9.57 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7644047302838082		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 1.7644047302838082 | validation: 2.2536860969399144]
	TIME [epoch: 9.57 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.766604267909344		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 1.766604267909344 | validation: 2.2674784671859745]
	TIME [epoch: 9.57 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7820922106895984		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 1.7820922106895984 | validation: 2.2929417929039158]
	TIME [epoch: 9.57 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7887594056404332		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 1.7887594056404332 | validation: 2.381727118880456]
	TIME [epoch: 9.57 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8147251411322423		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 1.8147251411322423 | validation: 2.2648638558439536]
	TIME [epoch: 9.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7664968447228637		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 1.7664968447228637 | validation: 2.271355021038331]
	TIME [epoch: 9.57 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7648002160600296		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 1.7648002160600296 | validation: 2.2503380912945583]
	TIME [epoch: 9.56 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7904906176906887		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 1.7904906176906887 | validation: 2.3286753568343257]
	TIME [epoch: 9.56 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8248443666203809		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 1.8248443666203809 | validation: 2.276530935179444]
	TIME [epoch: 9.57 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.784242734758991		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 1.784242734758991 | validation: 2.2565032289935782]
	TIME [epoch: 9.57 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7775097557281572		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 1.7775097557281572 | validation: 2.453208982564865]
	TIME [epoch: 9.57 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.82994172625218		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 1.82994172625218 | validation: 2.256752040925097]
	TIME [epoch: 9.8 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7945470126207703		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 1.7945470126207703 | validation: 2.2828626125601086]
	TIME [epoch: 9.58 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7794991957543167		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 1.7794991957543167 | validation: 2.291072274703485]
	TIME [epoch: 9.56 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8015503326424216		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 1.8015503326424216 | validation: 2.2536442967190333]
	TIME [epoch: 9.56 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7802425148222425		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 1.7802425148222425 | validation: 2.265017869060633]
	TIME [epoch: 9.56 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7720237770374438		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 1.7720237770374438 | validation: 2.2767081594427494]
	TIME [epoch: 9.58 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8039082055034794		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 1.8039082055034794 | validation: 2.2562551175566097]
	TIME [epoch: 9.57 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.759986497476623		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 1.759986497476623 | validation: 2.264970120629683]
	TIME [epoch: 9.56 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7644698131155985		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 1.7644698131155985 | validation: 2.265024611402676]
	TIME [epoch: 9.56 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7949404691354716		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 1.7949404691354716 | validation: 2.254372700977833]
	TIME [epoch: 9.58 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7780692442356354		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 1.7780692442356354 | validation: 2.2654256539156354]
	TIME [epoch: 9.57 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8052795293673245		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 1.8052795293673245 | validation: 2.3702838124877226]
	TIME [epoch: 9.57 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7889673924200262		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 1.7889673924200262 | validation: 2.330929280667742]
	TIME [epoch: 9.57 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7813573758951915		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 1.7813573758951915 | validation: 2.3063193483777233]
	TIME [epoch: 9.57 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8080889338186754		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 1.8080889338186754 | validation: 2.291132224795147]
	TIME [epoch: 9.56 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.766727839845697		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 1.766727839845697 | validation: 2.253924550182387]
	TIME [epoch: 9.56 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7994214845432641		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 1.7994214845432641 | validation: 2.3571519157068592]
	TIME [epoch: 9.56 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7797659102174066		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 1.7797659102174066 | validation: 2.2458999658316303]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_944.pth
	Model improved!!!
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.774685786318269		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 1.774685786318269 | validation: 2.3721880592621742]
	TIME [epoch: 9.56 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.808002038441191		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 1.808002038441191 | validation: 2.2561338656854297]
	TIME [epoch: 9.56 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.801670982463579		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 1.801670982463579 | validation: 2.28723074826227]
	TIME [epoch: 9.56 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.784593849905773		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 1.784593849905773 | validation: 2.2455253142448295]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_948.pth
	Model improved!!!
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7848668562204946		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 1.7848668562204946 | validation: 2.2995950772470666]
	TIME [epoch: 9.56 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7738177070786265		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 1.7738177070786265 | validation: 2.302199840516431]
	TIME [epoch: 9.56 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8151505592799861		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 1.8151505592799861 | validation: 2.2488077774241204]
	TIME [epoch: 9.56 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74407885311313		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 1.74407885311313 | validation: 2.3289414944031255]
	TIME [epoch: 9.58 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.776479301173134		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 1.776479301173134 | validation: 2.277720891867647]
	TIME [epoch: 9.56 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8330104571146968		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 1.8330104571146968 | validation: 2.2552599859831517]
	TIME [epoch: 9.56 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7883567643378033		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 1.7883567643378033 | validation: 2.268904835426391]
	TIME [epoch: 9.56 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.808677803110593		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 1.808677803110593 | validation: 2.350112379171887]
	TIME [epoch: 9.57 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7832439261456159		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 1.7832439261456159 | validation: 2.284968940168071]
	TIME [epoch: 9.56 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7922586647792755		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 1.7922586647792755 | validation: 2.3259624227830638]
	TIME [epoch: 9.56 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8096528149708964		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 1.8096528149708964 | validation: 2.256035257332573]
	TIME [epoch: 9.56 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7546238380705017		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 1.7546238380705017 | validation: 2.248525831099183]
	TIME [epoch: 9.59 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7569844457279964		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 1.7569844457279964 | validation: 2.333187535298021]
	TIME [epoch: 9.56 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7744065526630144		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 1.7744065526630144 | validation: 2.35268851959165]
	TIME [epoch: 9.57 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7960821456304394		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 1.7960821456304394 | validation: 2.2642118221221468]
	TIME [epoch: 9.56 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.764858874941945		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 1.764858874941945 | validation: 2.272880806078263]
	TIME [epoch: 9.58 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7721136945384033		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 1.7721136945384033 | validation: 2.2627138457617613]
	TIME [epoch: 9.56 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7683672954589116		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 1.7683672954589116 | validation: 2.341130984640131]
	TIME [epoch: 9.56 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7919983698860555		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 1.7919983698860555 | validation: 2.297861853507717]
	TIME [epoch: 9.55 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7641658031793028		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 1.7641658031793028 | validation: 2.288151946604]
	TIME [epoch: 9.58 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7724445757723006		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 1.7724445757723006 | validation: 2.3635971319388176]
	TIME [epoch: 9.55 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8068265228842724		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 1.8068265228842724 | validation: 2.279316168243635]
	TIME [epoch: 9.56 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7798772341205418		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 1.7798772341205418 | validation: 2.2938974909011893]
	TIME [epoch: 9.56 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8002175947313304		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 1.8002175947313304 | validation: 2.3204438567893577]
	TIME [epoch: 9.59 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7981307038894894		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 1.7981307038894894 | validation: 2.2754639406743613]
	TIME [epoch: 9.56 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7500726452857331		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 1.7500726452857331 | validation: 2.270741905444375]
	TIME [epoch: 9.57 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.772634829239104		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 1.772634829239104 | validation: 2.284764777077661]
	TIME [epoch: 9.55 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7756053272936985		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 1.7756053272936985 | validation: 2.256414394099773]
	TIME [epoch: 9.58 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7856251604346984		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 1.7856251604346984 | validation: 2.3124702022647963]
	TIME [epoch: 9.55 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.77596853393158		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 1.77596853393158 | validation: 2.273551451886514]
	TIME [epoch: 9.57 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.768016384109081		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 1.768016384109081 | validation: 2.2770137924360223]
	TIME [epoch: 9.56 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8260934395178272		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 1.8260934395178272 | validation: 2.2537595111131954]
	TIME [epoch: 9.58 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7556499595226434		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 1.7556499595226434 | validation: 2.2694270324889794]
	TIME [epoch: 9.57 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8051289778718278		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 1.8051289778718278 | validation: 2.3096745684048945]
	TIME [epoch: 9.56 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.761347403905956		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 1.761347403905956 | validation: 2.2736607949545893]
	TIME [epoch: 9.56 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7577701748091297		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 1.7577701748091297 | validation: 2.2979430395843115]
	TIME [epoch: 9.58 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7518673791453623		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 1.7518673791453623 | validation: 2.26751972130014]
	TIME [epoch: 9.57 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7798171087622692		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 1.7798171087622692 | validation: 2.2702490554030788]
	TIME [epoch: 9.56 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7564301576027077		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 1.7564301576027077 | validation: 2.272101334695398]
	TIME [epoch: 9.57 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7585850048272598		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 1.7585850048272598 | validation: 2.2604393490683234]
	TIME [epoch: 9.58 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.767968039618749		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 1.767968039618749 | validation: 2.2530099497370064]
	TIME [epoch: 9.56 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7749330746326042		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 1.7749330746326042 | validation: 2.3016544126707332]
	TIME [epoch: 9.57 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7800424635280863		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 1.7800424635280863 | validation: 2.3040882841560313]
	TIME [epoch: 9.58 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7805504809546058		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 1.7805504809546058 | validation: 2.297352147991631]
	TIME [epoch: 9.58 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.762681743473826		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 1.762681743473826 | validation: 2.270136100020354]
	TIME [epoch: 9.56 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7739114202679584		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 1.7739114202679584 | validation: 2.2709266784627267]
	TIME [epoch: 9.57 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7602627041030765		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 1.7602627041030765 | validation: 2.2436280571633005]
	TIME [epoch: 9.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7558865208785215		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 1.7558865208785215 | validation: 2.336192336224018]
	TIME [epoch: 9.59 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7615216262273363		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 1.7615216262273363 | validation: 2.2571724483428195]
	TIME [epoch: 9.57 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7942080955858053		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 1.7942080955858053 | validation: 2.322294947908721]
	TIME [epoch: 9.58 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7968048886047228		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 1.7968048886047228 | validation: 2.2917611371495377]
	TIME [epoch: 9.59 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7708359156036764		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 1.7708359156036764 | validation: 2.2796578335265925]
	TIME [epoch: 9.59 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7647852273478755		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 1.7647852273478755 | validation: 2.2707564696135534]
	TIME [epoch: 9.58 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7513430690956526		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 1.7513430690956526 | validation: 2.266327322898892]
	TIME [epoch: 9.57 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7607781125429334		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 1.7607781125429334 | validation: 2.3028065242326425]
	TIME [epoch: 9.58 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7792760165233663		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 1.7792760165233663 | validation: 2.2516697681383024]
	TIME [epoch: 9.57 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.769852890600776		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 1.769852890600776 | validation: 2.2809019720137997]
	TIME [epoch: 9.58 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7801621447873448		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 1.7801621447873448 | validation: 2.2547328967589833]
	TIME [epoch: 9.58 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7787823073404039		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 1.7787823073404039 | validation: 2.2577819806616946]
	TIME [epoch: 9.59 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.760878918085489		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 1.760878918085489 | validation: 2.2477000145903765]
	TIME [epoch: 9.57 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.76549368148792		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 1.76549368148792 | validation: 2.263213524183276]
	TIME [epoch: 9.57 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7877876592826791		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 1.7877876592826791 | validation: 2.3300169908707593]
	TIME [epoch: 9.59 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7641436406931263		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 1.7641436406931263 | validation: 2.2681347289617184]
	TIME [epoch: 9.57 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7676051217077462		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 1.7676051217077462 | validation: 2.2861476713580373]
	TIME [epoch: 9.57 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7653468384520152		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 1.7653468384520152 | validation: 2.274279405191903]
	TIME [epoch: 9.58 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7596138526547371		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 1.7596138526547371 | validation: 2.280180451825328]
	TIME [epoch: 9.58 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.775127397811638		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 1.775127397811638 | validation: 2.265927458766535]
	TIME [epoch: 9.57 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7758889204222243		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 1.7758889204222243 | validation: 2.26234452734449]
	TIME [epoch: 9.57 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7903754923991575		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 1.7903754923991575 | validation: 2.3269377444136534]
	TIME [epoch: 9.56 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7886447870675084		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 1.7886447870675084 | validation: 2.26716250347942]
	TIME [epoch: 9.59 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.774345512937899		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 1.774345512937899 | validation: 2.2959309190691197]
	TIME [epoch: 9.57 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7537808924937228		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 1.7537808924937228 | validation: 2.290216859499615]
	TIME [epoch: 9.58 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7530119028446225		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 1.7530119028446225 | validation: 2.2844706557801295]
	TIME [epoch: 9.57 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7770610752468705		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 1.7770610752468705 | validation: 2.293122683433725]
	TIME [epoch: 9.57 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7881391770346622		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 1.7881391770346622 | validation: 2.270379771792175]
	TIME [epoch: 9.56 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7522172467362034		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 1.7522172467362034 | validation: 2.253839337714959]
	TIME [epoch: 9.58 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7736553167478832		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 1.7736553167478832 | validation: 2.252691911521449]
	TIME [epoch: 9.59 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8423198243631187		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 1.8423198243631187 | validation: 2.2964197065103646]
	TIME [epoch: 9.57 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7801734181221172		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 1.7801734181221172 | validation: 2.2645078114499326]
	TIME [epoch: 9.57 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.779557827083766		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 1.779557827083766 | validation: 2.278229328284196]
	TIME [epoch: 9.57 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7873888334995685		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 1.7873888334995685 | validation: 2.252666802033907]
	TIME [epoch: 9.58 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7749523914995256		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 1.7749523914995256 | validation: 2.2518575894495023]
	TIME [epoch: 9.57 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7620747198923703		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 1.7620747198923703 | validation: 2.3137572764770504]
	TIME [epoch: 9.57 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7904432161815897		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 1.7904432161815897 | validation: 2.2655020098498535]
	TIME [epoch: 9.57 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7614021967100069		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 1.7614021967100069 | validation: 2.267493711944827]
	TIME [epoch: 9.59 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7550552916376858		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 1.7550552916376858 | validation: 2.264877030078439]
	TIME [epoch: 9.58 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.758950292728062		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 1.758950292728062 | validation: 2.2627299143493653]
	TIME [epoch: 9.56 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7942534919778814		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 1.7942534919778814 | validation: 2.277529472070083]
	TIME [epoch: 9.57 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7590398797474596		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 1.7590398797474596 | validation: 2.275102944426507]
	TIME [epoch: 9.58 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7838565057007834		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 1.7838565057007834 | validation: 2.267681394608439]
	TIME [epoch: 9.57 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7583589293845001		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 1.7583589293845001 | validation: 2.2583353526495142]
	TIME [epoch: 9.58 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7605570169916391		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 1.7605570169916391 | validation: 2.263071921153129]
	TIME [epoch: 9.57 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.778474521744458		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 1.778474521744458 | validation: 2.2992248048830364]
	TIME [epoch: 9.58 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7648164101344972		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 1.7648164101344972 | validation: 2.321992409894219]
	TIME [epoch: 9.58 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7674418775317382		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 1.7674418775317382 | validation: 2.2961160400632954]
	TIME [epoch: 9.58 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.764026970458556		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 1.764026970458556 | validation: 2.2720902626250234]
	TIME [epoch: 9.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7609904040766295		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 1.7609904040766295 | validation: 2.301666719062059]
	TIME [epoch: 9.58 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7744966303873952		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 1.7744966303873952 | validation: 2.2705588664853367]
	TIME [epoch: 9.58 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.78045497615437		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 1.78045497615437 | validation: 2.337893470418054]
	TIME [epoch: 9.59 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7822833280351884		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 1.7822833280351884 | validation: 2.2691596870245294]
	TIME [epoch: 9.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7715029017354997		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 1.7715029017354997 | validation: 2.262273332497866]
	TIME [epoch: 9.58 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7543607498679354		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 1.7543607498679354 | validation: 2.2871513148782703]
	TIME [epoch: 9.57 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7636630554172914		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 1.7636630554172914 | validation: 2.2531226759040353]
	TIME [epoch: 9.57 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7786558672841895		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 1.7786558672841895 | validation: 2.2800042092919166]
	TIME [epoch: 9.59 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7510927780677765		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 1.7510927780677765 | validation: 2.2518096735056496]
	TIME [epoch: 9.57 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7540305999993215		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 1.7540305999993215 | validation: 2.254933168836846]
	TIME [epoch: 9.58 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7814863764920315		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 1.7814863764920315 | validation: 2.3166661007935843]
	TIME [epoch: 9.59 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.77205744741716		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 1.77205744741716 | validation: 2.2968223524694427]
	TIME [epoch: 9.58 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.762295944230202		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 1.762295944230202 | validation: 2.267687924057042]
	TIME [epoch: 9.57 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75678663003507		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 1.75678663003507 | validation: 2.270457278838212]
	TIME [epoch: 9.56 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7845187719583713		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 1.7845187719583713 | validation: 2.255932231527237]
	TIME [epoch: 9.59 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.761721050620994		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 1.761721050620994 | validation: 2.2511751315969692]
	TIME [epoch: 9.57 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7561489004535986		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 1.7561489004535986 | validation: 2.2637175256738806]
	TIME [epoch: 9.57 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7612652781820433		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 1.7612652781820433 | validation: 2.2899413249482623]
	TIME [epoch: 9.57 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7808749674811815		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 1.7808749674811815 | validation: 2.265126252111895]
	TIME [epoch: 9.59 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.765471069044953		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 1.765471069044953 | validation: 2.3086728148362865]
	TIME [epoch: 9.58 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7752745139355142		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 1.7752745139355142 | validation: 2.2657237590639596]
	TIME [epoch: 9.57 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7567688884540082		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 1.7567688884540082 | validation: 2.257415826004624]
	TIME [epoch: 9.57 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7790482209242395		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 1.7790482209242395 | validation: 2.256563029583983]
	TIME [epoch: 9.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.759771237650321		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 1.759771237650321 | validation: 2.2696519665203803]
	TIME [epoch: 9.57 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7657271017393676		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 1.7657271017393676 | validation: 2.3438268071776247]
	TIME [epoch: 9.58 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7840029861407385		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 1.7840029861407385 | validation: 2.2821076198476224]
	TIME [epoch: 9.58 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7550025192491934		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 1.7550025192491934 | validation: 2.2965116612513006]
	TIME [epoch: 9.58 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.768676436439365		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 1.768676436439365 | validation: 2.2993779048318745]
	TIME [epoch: 9.58 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7655983131733375		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 1.7655983131733375 | validation: 2.3150090537217456]
	TIME [epoch: 9.58 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.768530142225434		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 1.768530142225434 | validation: 2.260823916924999]
	TIME [epoch: 9.59 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.77138784445191		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 1.77138784445191 | validation: 2.2739987729090125]
	TIME [epoch: 9.58 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7649737753025079		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 1.7649737753025079 | validation: 2.2567925089268783]
	TIME [epoch: 9.57 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.76358065458483		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 1.76358065458483 | validation: 2.292279032342217]
	TIME [epoch: 9.57 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7639819329095672		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 1.7639819329095672 | validation: 2.3165119626534447]
	TIME [epoch: 9.59 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.773154278984976		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 1.773154278984976 | validation: 2.26934793715486]
	TIME [epoch: 9.57 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7698306856335364		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 1.7698306856335364 | validation: 2.2868611742410834]
	TIME [epoch: 9.58 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7487538849664386		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 1.7487538849664386 | validation: 2.2483467202599177]
	TIME [epoch: 9.57 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7591679294403597		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 1.7591679294403597 | validation: 2.2618100588988703]
	TIME [epoch: 9.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7635165999140798		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 1.7635165999140798 | validation: 2.2366647938990063]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1083.pth
	Model improved!!!
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7603072158972903		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 1.7603072158972903 | validation: 2.249360287954172]
	TIME [epoch: 9.58 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7659684799835176		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 1.7659684799835176 | validation: 2.271500575226208]
	TIME [epoch: 9.57 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7638537024568781		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 1.7638537024568781 | validation: 2.2606773282687658]
	TIME [epoch: 9.59 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.757429147527858		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 1.757429147527858 | validation: 2.2460410685035086]
	TIME [epoch: 9.57 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7598792282750995		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 1.7598792282750995 | validation: 2.2653155086704304]
	TIME [epoch: 9.57 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7627615166391852		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 1.7627615166391852 | validation: 2.266767491260359]
	TIME [epoch: 9.58 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.767011185784775		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 1.767011185784775 | validation: 2.260090810757987]
	TIME [epoch: 9.57 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74516252660504		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 1.74516252660504 | validation: 2.2550570290803824]
	TIME [epoch: 9.57 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7860958623121768		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 1.7860958623121768 | validation: 2.254328242262099]
	TIME [epoch: 9.56 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7674120288740525		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 1.7674120288740525 | validation: 2.258947136976157]
	TIME [epoch: 9.58 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.763188278881222		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 1.763188278881222 | validation: 2.244376142019828]
	TIME [epoch: 9.56 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7638401421880032		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 1.7638401421880032 | validation: 2.28302922175826]
	TIME [epoch: 9.56 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7772135977085308		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 1.7772135977085308 | validation: 2.267646503497468]
	TIME [epoch: 9.57 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7542749046303037		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 1.7542749046303037 | validation: 2.2577665657045074]
	TIME [epoch: 9.59 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7735215297933729		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 1.7735215297933729 | validation: 2.2608584508700065]
	TIME [epoch: 9.57 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7632352752165315		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 1.7632352752165315 | validation: 2.2621042470078176]
	TIME [epoch: 9.58 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7508496887098688		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 1.7508496887098688 | validation: 2.2389708652858]
	TIME [epoch: 9.57 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.780017294120697		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 1.780017294120697 | validation: 2.2572480933355026]
	TIME [epoch: 9.59 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7494490366104585		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 1.7494490366104585 | validation: 2.247417916090418]
	TIME [epoch: 9.57 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7561723059936518		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 1.7561723059936518 | validation: 2.275587079300339]
	TIME [epoch: 9.56 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.76786422299604		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 1.76786422299604 | validation: 2.2757376171685855]
	TIME [epoch: 9.57 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7662129056641604		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 1.7662129056641604 | validation: 2.25141982520114]
	TIME [epoch: 9.58 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7866657564690178		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 1.7866657564690178 | validation: 2.244634840013865]
	TIME [epoch: 9.56 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7505861879048037		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 1.7505861879048037 | validation: 2.3048454695092513]
	TIME [epoch: 9.57 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7575187537216046		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 1.7575187537216046 | validation: 2.2498015306442567]
	TIME [epoch: 9.59 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7663865739728606		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 1.7663865739728606 | validation: 2.2634072486756094]
	TIME [epoch: 9.58 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7627510720526836		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 1.7627510720526836 | validation: 2.242773390065766]
	TIME [epoch: 9.56 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7479527986941883		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 1.7479527986941883 | validation: 2.270550052129291]
	TIME [epoch: 9.57 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7542205379025309		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 1.7542205379025309 | validation: 2.2578640157780625]
	TIME [epoch: 9.58 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7549274943385498		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 1.7549274943385498 | validation: 2.3026558367302545]
	TIME [epoch: 9.57 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7658331881078708		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 1.7658331881078708 | validation: 2.2556067136840943]
	TIME [epoch: 9.58 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7525666847652217		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 1.7525666847652217 | validation: 2.2536004331238466]
	TIME [epoch: 9.58 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7588281511665769		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 1.7588281511665769 | validation: 2.3835319523049776]
	TIME [epoch: 9.59 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.776900534734973		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 1.776900534734973 | validation: 2.2607480425269295]
	TIME [epoch: 9.56 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7604487873906443		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 1.7604487873906443 | validation: 2.2715174019780666]
	TIME [epoch: 9.57 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7550039783624505		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 1.7550039783624505 | validation: 2.271327497358227]
	TIME [epoch: 9.58 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7602959681775936		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 1.7602959681775936 | validation: 2.2890011938895407]
	TIME [epoch: 9.58 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7667979317637879		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 1.7667979317637879 | validation: 2.2438415220322914]
	TIME [epoch: 9.57 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7742895766929334		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 1.7742895766929334 | validation: 2.2740672545244833]
	TIME [epoch: 9.56 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7777584203315269		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 1.7777584203315269 | validation: 2.2757288030556966]
	TIME [epoch: 9.59 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75031459212494		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 1.75031459212494 | validation: 2.2948518571889687]
	TIME [epoch: 9.57 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7554360316445574		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 1.7554360316445574 | validation: 2.2629004288694268]
	TIME [epoch: 9.58 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7574294982745058		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 1.7574294982745058 | validation: 2.249275924381754]
	TIME [epoch: 9.56 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475412594175104		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 1.7475412594175104 | validation: 2.250412424660031]
	TIME [epoch: 9.58 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7581067267780348		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 1.7581067267780348 | validation: 2.318922556341276]
	TIME [epoch: 9.56 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7656595210942314		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 1.7656595210942314 | validation: 2.2628623375363532]
	TIME [epoch: 9.57 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.763702175107115		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 1.763702175107115 | validation: 2.256913884140692]
	TIME [epoch: 9.56 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7584751119676607		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 1.7584751119676607 | validation: 2.322726184598684]
	TIME [epoch: 9.59 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7581900981465268		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 1.7581900981465268 | validation: 2.2998917355323316]
	TIME [epoch: 9.56 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7654484763358855		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 1.7654484763358855 | validation: 2.2732292066262216]
	TIME [epoch: 9.57 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748623974187439		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 1.748623974187439 | validation: 2.252668263701482]
	TIME [epoch: 9.57 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7538816904192394		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 1.7538816904192394 | validation: 2.252250766999598]
	TIME [epoch: 9.57 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7512572976298384		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 1.7512572976298384 | validation: 2.257939963539729]
	TIME [epoch: 9.56 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7656256920599442		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 1.7656256920599442 | validation: 2.2951638721373957]
	TIME [epoch: 9.57 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7567647045291601		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 1.7567647045291601 | validation: 2.2735527454855715]
	TIME [epoch: 9.58 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7486527994655723		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 1.7486527994655723 | validation: 2.253349167143391]
	TIME [epoch: 9.57 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7725483439737943		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 1.7725483439737943 | validation: 2.257738466515815]
	TIME [epoch: 9.57 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7712329232023987		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 1.7712329232023987 | validation: 2.2629488195009015]
	TIME [epoch: 9.56 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7625862065678568		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 1.7625862065678568 | validation: 2.2549389660644854]
	TIME [epoch: 9.58 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75393704077026		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 1.75393704077026 | validation: 2.2756789406759275]
	TIME [epoch: 9.56 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750168070596167		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 1.750168070596167 | validation: 2.2557850496226304]
	TIME [epoch: 9.56 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7676685367114042		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 1.7676685367114042 | validation: 2.252552452339062]
	TIME [epoch: 9.58 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7522276444596478		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 1.7522276444596478 | validation: 2.264809393269008]
	TIME [epoch: 9.58 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7675077653298292		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 1.7675077653298292 | validation: 2.2827199223771895]
	TIME [epoch: 9.56 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7570751453320637		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 1.7570751453320637 | validation: 2.2395973817861217]
	TIME [epoch: 9.56 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8127057582672137		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 1.8127057582672137 | validation: 2.321425037017635]
	TIME [epoch: 9.57 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7695229735734883		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 1.7695229735734883 | validation: 2.2631686323445606]
	TIME [epoch: 9.58 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7495424084228344		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 1.7495424084228344 | validation: 2.2817242223612597]
	TIME [epoch: 9.56 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7564973821836851		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 1.7564973821836851 | validation: 2.2657763014973056]
	TIME [epoch: 9.56 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7526211453310812		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 1.7526211453310812 | validation: 2.292370243532635]
	TIME [epoch: 9.57 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.766313972079916		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 1.766313972079916 | validation: 2.275936961588581]
	TIME [epoch: 9.57 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7570475252569935		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 1.7570475252569935 | validation: 2.2664165845125983]
	TIME [epoch: 9.56 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7507457391597943		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 1.7507457391597943 | validation: 2.269123522946109]
	TIME [epoch: 9.56 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7519992402485904		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 1.7519992402485904 | validation: 2.271674104167131]
	TIME [epoch: 9.58 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7649171033480564		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 1.7649171033480564 | validation: 2.318648954636296]
	TIME [epoch: 9.56 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7547710727653718		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 1.7547710727653718 | validation: 2.2569119000259676]
	TIME [epoch: 9.57 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7668984878258804		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 1.7668984878258804 | validation: 2.2518425504196813]
	TIME [epoch: 9.56 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7675767578728667		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 1.7675767578728667 | validation: 2.2735501777500775]
	TIME [epoch: 9.58 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.76341750769741		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 1.76341750769741 | validation: 2.2781653255957157]
	TIME [epoch: 9.56 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.77477244020182		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 1.77477244020182 | validation: 2.272675338745448]
	TIME [epoch: 9.56 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7494302593905176		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 1.7494302593905176 | validation: 2.299131426917894]
	TIME [epoch: 9.57 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7630229126772778		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 1.7630229126772778 | validation: 2.265844959427259]
	TIME [epoch: 9.59 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.757411236824462		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 1.757411236824462 | validation: 2.2580944342287426]
	TIME [epoch: 9.57 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751470881393687		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 1.751470881393687 | validation: 2.2451698766834407]
	TIME [epoch: 9.57 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7553945117048304		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 1.7553945117048304 | validation: 2.2583251367752655]
	TIME [epoch: 9.57 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7662099328401673		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 1.7662099328401673 | validation: 2.283567187971965]
	TIME [epoch: 9.57 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7736919578981578		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 1.7736919578981578 | validation: 2.280826426971529]
	TIME [epoch: 9.57 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7590599333810446		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 1.7590599333810446 | validation: 2.288435672017105]
	TIME [epoch: 9.56 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7576424487305853		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 1.7576424487305853 | validation: 2.2862616583311746]
	TIME [epoch: 9.57 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752456696433019		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 1.752456696433019 | validation: 2.273339434667146]
	TIME [epoch: 9.56 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7698509225109706		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 1.7698509225109706 | validation: 2.2671673565065156]
	TIME [epoch: 9.55 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7723585031803062		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 1.7723585031803062 | validation: 2.2847751714708355]
	TIME [epoch: 9.56 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7503763722864498		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 1.7503763722864498 | validation: 2.2539928707293706]
	TIME [epoch: 9.58 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7502471959757742		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 1.7502471959757742 | validation: 2.2974967659504006]
	TIME [epoch: 9.57 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.755751165197918		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 1.755751165197918 | validation: 2.2690953147494084]
	TIME [epoch: 9.57 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7903558680252438		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 1.7903558680252438 | validation: 2.247259301722457]
	TIME [epoch: 9.57 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7546425023746657		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 1.7546425023746657 | validation: 2.2531055376881794]
	TIME [epoch: 9.58 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7558311575434449		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 1.7558311575434449 | validation: 2.278829971464089]
	TIME [epoch: 9.57 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7560484539472239		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 1.7560484539472239 | validation: 2.2462141602521615]
	TIME [epoch: 9.57 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7585595571914		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 1.7585595571914 | validation: 2.2447548511736275]
	TIME [epoch: 9.57 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7511576478228896		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 1.7511576478228896 | validation: 2.2517775815375574]
	TIME [epoch: 9.59 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468859118265399		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 1.7468859118265399 | validation: 2.277121492110386]
	TIME [epoch: 9.56 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752926617775626		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 1.752926617775626 | validation: 2.30129156710034]
	TIME [epoch: 9.56 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.805243724255273		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 1.805243724255273 | validation: 2.27536476164494]
	TIME [epoch: 9.57 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7564557631338726		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 1.7564557631338726 | validation: 2.2773533390291125]
	TIME [epoch: 9.57 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7571442949379645		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 1.7571442949379645 | validation: 2.2973181597641203]
	TIME [epoch: 9.57 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750984928355366		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 1.750984928355366 | validation: 2.2533219713866357]
	TIME [epoch: 9.56 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.772958361061221		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 1.772958361061221 | validation: 2.2784583283437447]
	TIME [epoch: 9.59 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7490076968111428		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 1.7490076968111428 | validation: 2.2826109748424614]
	TIME [epoch: 9.57 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.756821291716252		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 1.756821291716252 | validation: 2.239367711200829]
	TIME [epoch: 9.56 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7549645455820144		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 1.7549645455820144 | validation: 2.2370499473302554]
	TIME [epoch: 9.56 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7733867028444734		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 1.7733867028444734 | validation: 2.3120873451024546]
	TIME [epoch: 9.58 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7586224694279218		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 1.7586224694279218 | validation: 2.2602725512759263]
	TIME [epoch: 9.57 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7675790430502203		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 1.7675790430502203 | validation: 2.3011396013189658]
	TIME [epoch: 9.56 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7739450901196054		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 1.7739450901196054 | validation: 2.243580290914199]
	TIME [epoch: 9.57 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.754566001498431		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 1.754566001498431 | validation: 2.2468603578049544]
	TIME [epoch: 9.58 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7510567883893924		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 1.7510567883893924 | validation: 2.2467914543918543]
	TIME [epoch: 9.56 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.757855411711963		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 1.757855411711963 | validation: 2.270696229981627]
	TIME [epoch: 9.56 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7642448563788187		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 1.7642448563788187 | validation: 2.2620904963763144]
	TIME [epoch: 9.57 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.755829582137031		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 1.755829582137031 | validation: 2.3224383165143934]
	TIME [epoch: 9.57 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7520150986461318		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 1.7520150986461318 | validation: 2.279960849956315]
	TIME [epoch: 9.57 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7656929492413163		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 1.7656929492413163 | validation: 2.2488765593642928]
	TIME [epoch: 9.56 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.760570101878919		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 1.760570101878919 | validation: 2.2807810576753917]
	TIME [epoch: 9.58 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752884716719614		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 1.752884716719614 | validation: 2.2510922256576116]
	TIME [epoch: 9.57 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7551937922403358		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 1.7551937922403358 | validation: 2.278271507039199]
	TIME [epoch: 9.57 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7511483068873162		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 1.7511483068873162 | validation: 2.263403823448172]
	TIME [epoch: 9.56 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7632577878591618		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 1.7632577878591618 | validation: 2.2663959937287492]
	TIME [epoch: 9.6 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493472262537015		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 1.7493472262537015 | validation: 2.2691296469085183]
	TIME [epoch: 9.57 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.755686762459837		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 1.755686762459837 | validation: 2.2628589149517]
	TIME [epoch: 9.57 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750184905909291		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 1.750184905909291 | validation: 2.273891089048195]
	TIME [epoch: 9.56 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7549691195135886		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 1.7549691195135886 | validation: 2.2849621232955095]
	TIME [epoch: 9.57 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7549587123518673		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 1.7549587123518673 | validation: 2.2565161030168244]
	TIME [epoch: 9.56 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748093368973617		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 1.748093368973617 | validation: 2.2521072541836675]
	TIME [epoch: 9.57 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7506906939914657		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 1.7506906939914657 | validation: 2.283743946309369]
	TIME [epoch: 9.56 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7709087663192722		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 1.7709087663192722 | validation: 2.2501990507236482]
	TIME [epoch: 9.57 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446127806472074		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 1.7446127806472074 | validation: 2.258387482332425]
	TIME [epoch: 9.56 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7497754334664435		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 1.7497754334664435 | validation: 2.2486923841359556]
	TIME [epoch: 9.57 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.742931681077053		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 1.742931681077053 | validation: 2.246352709854634]
	TIME [epoch: 9.57 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.759311889191564		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 1.759311889191564 | validation: 2.2485386478096663]
	TIME [epoch: 9.57 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7786812507307015		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 1.7786812507307015 | validation: 2.255847782043523]
	TIME [epoch: 9.56 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7518964396111527		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 1.7518964396111527 | validation: 2.273825434784691]
	TIME [epoch: 9.57 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7556365959272646		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 1.7556365959272646 | validation: 2.2631825767227505]
	TIME [epoch: 9.56 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7617893454582343		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 1.7617893454582343 | validation: 2.242810101985687]
	TIME [epoch: 9.56 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7633651719817984		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 1.7633651719817984 | validation: 2.2586395775258272]
	TIME [epoch: 9.57 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7462268001595735		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 1.7462268001595735 | validation: 2.2412455910462388]
	TIME [epoch: 9.56 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7537462121718221		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 1.7537462121718221 | validation: 2.2486629805117917]
	TIME [epoch: 9.58 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.765177810297768		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 1.765177810297768 | validation: 2.25591444577159]
	TIME [epoch: 9.57 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7720103360298485		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 1.7720103360298485 | validation: 2.2629462388514408]
	TIME [epoch: 9.56 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7457411149691624		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 1.7457411149691624 | validation: 2.2635008304863677]
	TIME [epoch: 9.54 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747286833704581		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 1.747286833704581 | validation: 2.261847259292685]
	TIME [epoch: 9.55 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7505835561410374		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 1.7505835561410374 | validation: 2.2756451609224952]
	TIME [epoch: 9.56 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7509863618083812		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 1.7509863618083812 | validation: 2.2557570443278383]
	TIME [epoch: 9.56 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.754722554605878		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 1.754722554605878 | validation: 2.266510665050665]
	TIME [epoch: 9.57 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7523266934643682		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 1.7523266934643682 | validation: 2.2619136013285295]
	TIME [epoch: 9.56 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7700565627940594		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 1.7700565627940594 | validation: 2.278574072126687]
	TIME [epoch: 9.55 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75555420567058		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 1.75555420567058 | validation: 2.285440779111127]
	TIME [epoch: 9.57 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7510129039636042		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 1.7510129039636042 | validation: 2.2411375881136903]
	TIME [epoch: 9.57 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7500187739956368		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 1.7500187739956368 | validation: 2.263907681981117]
	TIME [epoch: 9.57 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7648140273168835		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 1.7648140273168835 | validation: 2.2663162567995676]
	TIME [epoch: 9.56 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7478232928950406		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 1.7478232928950406 | validation: 2.2700846578822826]
	TIME [epoch: 9.56 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.753252437131025		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 1.753252437131025 | validation: 2.256249500558662]
	TIME [epoch: 9.55 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.754528787195969		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 1.754528787195969 | validation: 2.2400945146106612]
	TIME [epoch: 9.57 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7550901193676132		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 1.7550901193676132 | validation: 2.251579294213258]
	TIME [epoch: 9.55 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7624818833125881		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 1.7624818833125881 | validation: 2.2420791458037144]
	TIME [epoch: 9.56 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468187494565903		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 1.7468187494565903 | validation: 2.239371678480861]
	TIME [epoch: 9.58 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.755992006745291		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 1.755992006745291 | validation: 2.236776086016639]
	TIME [epoch: 9.55 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.772038906486682		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 1.772038906486682 | validation: 2.245084403315804]
	TIME [epoch: 9.55 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750033091702567		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 1.750033091702567 | validation: 2.3077761067134768]
	TIME [epoch: 9.58 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.762624815043716		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 1.762624815043716 | validation: 2.258897571844901]
	TIME [epoch: 9.57 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7436580857231454		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 1.7436580857231454 | validation: 2.245771763814212]
	TIME [epoch: 9.56 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748155567698523		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 1.748155567698523 | validation: 2.28135233284571]
	TIME [epoch: 9.56 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7590163712165616		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 1.7590163712165616 | validation: 2.2544571587004554]
	TIME [epoch: 9.58 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421133855430628		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 1.7421133855430628 | validation: 2.2484735796084823]
	TIME [epoch: 9.56 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.760764639779222		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 1.760764639779222 | validation: 2.2605112577678277]
	TIME [epoch: 9.58 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7465513436357814		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 1.7465513436357814 | validation: 2.29519852200059]
	TIME [epoch: 9.56 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7589932657091272		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 1.7589932657091272 | validation: 2.2957636756724553]
	TIME [epoch: 9.58 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.756864526439164		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 1.756864526439164 | validation: 2.2468349577765263]
	TIME [epoch: 9.55 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7563081769269697		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 1.7563081769269697 | validation: 2.250217765320436]
	TIME [epoch: 9.57 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748868572340743		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 1.748868572340743 | validation: 2.2837590730365838]
	TIME [epoch: 9.56 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7535918201828236		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 1.7535918201828236 | validation: 2.240587217197142]
	TIME [epoch: 9.59 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750795171206605		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 1.750795171206605 | validation: 2.254741394764103]
	TIME [epoch: 9.56 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7435408141049087		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 1.7435408141049087 | validation: 2.2809999957739553]
	TIME [epoch: 9.57 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.753175986524374		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 1.753175986524374 | validation: 2.2612907977871535]
	TIME [epoch: 9.55 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7662163614992217		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 1.7662163614992217 | validation: 2.2686811789704064]
	TIME [epoch: 9.59 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7547328995966065		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 1.7547328995966065 | validation: 2.2585779168555025]
	TIME [epoch: 9.55 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454692193266517		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 1.7454692193266517 | validation: 2.259941065593713]
	TIME [epoch: 9.57 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7576447794637626		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 1.7576447794637626 | validation: 2.266005632153861]
	TIME [epoch: 9.58 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7504072775386734		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 1.7504072775386734 | validation: 2.233186939223796]
	TIME [epoch: 9.59 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1271.pth
	Model improved!!!
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7439924802158866		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 1.7439924802158866 | validation: 2.2532024347073505]
	TIME [epoch: 9.56 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746705364228595		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 1.746705364228595 | validation: 2.2817844804235294]
	TIME [epoch: 9.55 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7487180930493154		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 1.7487180930493154 | validation: 2.271232473022216]
	TIME [epoch: 9.54 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.755556913493931		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 1.755556913493931 | validation: 2.26973482676724]
	TIME [epoch: 9.55 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7590433260482705		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 1.7590433260482705 | validation: 2.2466212352317987]
	TIME [epoch: 9.56 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747452068793113		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 1.747452068793113 | validation: 2.239253192271607]
	TIME [epoch: 9.56 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7534869667096902		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 1.7534869667096902 | validation: 2.2750931198966944]
	TIME [epoch: 9.57 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7472904846908452		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 1.7472904846908452 | validation: 2.296726139313372]
	TIME [epoch: 9.56 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7664666590304698		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 1.7664666590304698 | validation: 2.2593963796184835]
	TIME [epoch: 9.55 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.742756931308613		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 1.742756931308613 | validation: 2.2634595830552975]
	TIME [epoch: 9.56 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7496142701842785		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 1.7496142701842785 | validation: 2.2468727421027808]
	TIME [epoch: 9.58 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7628361664522836		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 1.7628361664522836 | validation: 2.2554179092158537]
	TIME [epoch: 9.55 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7502982352353715		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 1.7502982352353715 | validation: 2.259963236729344]
	TIME [epoch: 9.53 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7612020468642597		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 1.7612020468642597 | validation: 2.261753532820437]
	TIME [epoch: 9.57 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7498333859162187		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 1.7498333859162187 | validation: 2.2652122056188175]
	TIME [epoch: 9.56 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7525583575819685		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 1.7525583575819685 | validation: 2.255739252775997]
	TIME [epoch: 9.55 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7435721211481134		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 1.7435721211481134 | validation: 2.240661182869924]
	TIME [epoch: 9.54 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.756795376994346		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 1.756795376994346 | validation: 2.2440188762996867]
	TIME [epoch: 9.57 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468489052056555		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 1.7468489052056555 | validation: 2.2574749523432622]
	TIME [epoch: 9.55 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7645134459292529		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 1.7645134459292529 | validation: 2.248308579943839]
	TIME [epoch: 9.55 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.756090803572463		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 1.756090803572463 | validation: 2.238255511688858]
	TIME [epoch: 9.56 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7669749878280818		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 1.7669749878280818 | validation: 2.270795833931201]
	TIME [epoch: 9.56 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7661892352595416		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 1.7661892352595416 | validation: 2.342426813601317]
	TIME [epoch: 9.55 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.770476887024584		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 1.770476887024584 | validation: 2.2588351965553106]
	TIME [epoch: 9.56 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7467925779984328		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 1.7467925779984328 | validation: 2.2729876341378255]
	TIME [epoch: 9.55 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.764836710329939		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 1.764836710329939 | validation: 2.249874445564726]
	TIME [epoch: 9.56 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.761695987233292		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 1.761695987233292 | validation: 2.268497130516982]
	TIME [epoch: 9.56 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745153701464929		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 1.745153701464929 | validation: 2.2299926606522638]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1299.pth
	Model improved!!!
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438010296351494		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 1.7438010296351494 | validation: 2.265933078898028]
	TIME [epoch: 9.55 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.758039082411909		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 1.758039082411909 | validation: 2.2600253485186106]
	TIME [epoch: 9.56 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7466727368287596		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 1.7466727368287596 | validation: 2.2515977848967537]
	TIME [epoch: 9.53 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7510882177391072		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 1.7510882177391072 | validation: 2.25072085073903]
	TIME [epoch: 9.54 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7486564060344052		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 1.7486564060344052 | validation: 2.299387368790672]
	TIME [epoch: 9.56 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7590942320483562		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 1.7590942320483562 | validation: 2.253392143526976]
	TIME [epoch: 9.55 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7515862849981914		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 1.7515862849981914 | validation: 2.253496623139926]
	TIME [epoch: 9.55 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7557435429992458		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 1.7557435429992458 | validation: 2.250401645129303]
	TIME [epoch: 9.55 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752827134575575		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 1.752827134575575 | validation: 2.2473291797588413]
	TIME [epoch: 9.56 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7498490509201243		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 1.7498490509201243 | validation: 2.2521977354009852]
	TIME [epoch: 9.55 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7479488215245322		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 1.7479488215245322 | validation: 2.262177467487696]
	TIME [epoch: 9.54 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7449254561187828		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 1.7449254561187828 | validation: 2.2776586623027346]
	TIME [epoch: 9.54 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7414054140863633		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 1.7414054140863633 | validation: 2.271950214119489]
	TIME [epoch: 9.56 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7565506787078728		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 1.7565506787078728 | validation: 2.253963795843013]
	TIME [epoch: 9.54 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.749065082943794		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 1.749065082943794 | validation: 2.238786863656257]
	TIME [epoch: 9.53 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7545451283932803		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 1.7545451283932803 | validation: 2.247413644169013]
	TIME [epoch: 9.55 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7426038698430115		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 1.7426038698430115 | validation: 2.2516081389560485]
	TIME [epoch: 9.55 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7674761602707676		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 1.7674761602707676 | validation: 2.2604032759875667]
	TIME [epoch: 9.54 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475383746606306		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 1.7475383746606306 | validation: 2.273298345673359]
	TIME [epoch: 9.54 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7540154963503838		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 1.7540154963503838 | validation: 2.2868660946724053]
	TIME [epoch: 9.55 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7540948536579457		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 1.7540948536579457 | validation: 2.2643362073628626]
	TIME [epoch: 9.54 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7548548426191282		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 1.7548548426191282 | validation: 2.3121399680343218]
	TIME [epoch: 9.54 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.755586640403139		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 1.755586640403139 | validation: 2.258679754033607]
	TIME [epoch: 9.54 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7645281791066236		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 1.7645281791066236 | validation: 2.3223782264407697]
	TIME [epoch: 9.58 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.763821656683118		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 1.763821656683118 | validation: 2.2627142802400044]
	TIME [epoch: 9.53 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746861746142406		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 1.746861746142406 | validation: 2.2355898227821664]
	TIME [epoch: 9.55 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751736652190549		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 1.751736652190549 | validation: 2.260856275675727]
	TIME [epoch: 9.52 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.753993452718499		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 1.753993452718499 | validation: 2.285544890663061]
	TIME [epoch: 9.56 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7506298186183322		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 1.7506298186183322 | validation: 2.284208775198697]
	TIME [epoch: 9.55 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748304681234679		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 1.748304681234679 | validation: 2.252829244868363]
	TIME [epoch: 9.55 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7418858263032018		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 1.7418858263032018 | validation: 2.254795669180919]
	TIME [epoch: 9.52 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7453926836552092		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 1.7453926836552092 | validation: 2.269569532935284]
	TIME [epoch: 9.54 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7561984910924422		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 1.7561984910924422 | validation: 2.2829334039343987]
	TIME [epoch: 9.53 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7516350038180888		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 1.7516350038180888 | validation: 2.2966303832844015]
	TIME [epoch: 9.54 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.758036953662398		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 1.758036953662398 | validation: 2.2461035480263356]
	TIME [epoch: 9.55 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7506423132301354		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 1.7506423132301354 | validation: 2.2942987422589316]
	TIME [epoch: 9.55 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443691906043397		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 1.7443691906043397 | validation: 2.263969223574357]
	TIME [epoch: 9.55 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7497732890694617		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 1.7497732890694617 | validation: 2.257146995012031]
	TIME [epoch: 9.56 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7603885815948488		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 1.7603885815948488 | validation: 2.265765682524931]
	TIME [epoch: 9.56 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7539309968460994		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 1.7539309968460994 | validation: 2.25077906126243]
	TIME [epoch: 9.55 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751145302096763		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 1.751145302096763 | validation: 2.2852884944360166]
	TIME [epoch: 9.54 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75149610176788		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 1.75149610176788 | validation: 2.2881479047140516]
	TIME [epoch: 9.53 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7579290249568842		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 1.7579290249568842 | validation: 2.2521934565231323]
	TIME [epoch: 9.56 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7523280118790976		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 1.7523280118790976 | validation: 2.2455198088749975]
	TIME [epoch: 9.54 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442178629280918		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 1.7442178629280918 | validation: 2.2400563005534337]
	TIME [epoch: 9.54 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7486737376361288		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 1.7486737376361288 | validation: 2.271142056164432]
	TIME [epoch: 9.54 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7517884570262237		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 1.7517884570262237 | validation: 2.2496445967680416]
	TIME [epoch: 9.56 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746828629539321		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 1.746828629539321 | validation: 2.2451020189612563]
	TIME [epoch: 9.54 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7448479418739953		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 1.7448479418739953 | validation: 2.2601114768683073]
	TIME [epoch: 9.55 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.749597094556302		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 1.749597094556302 | validation: 2.2790380445907696]
	TIME [epoch: 9.54 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7690045239234735		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 1.7690045239234735 | validation: 2.274592489564305]
	TIME [epoch: 9.55 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440847121976701		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 1.7440847121976701 | validation: 2.264777489178]
	TIME [epoch: 9.55 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7505270884468367		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 1.7505270884468367 | validation: 2.2509309961569777]
	TIME [epoch: 9.55 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7408960439952335		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 1.7408960439952335 | validation: 2.253374477808659]
	TIME [epoch: 9.55 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748507374781117		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 1.748507374781117 | validation: 2.2861937314334235]
	TIME [epoch: 9.55 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7512002176717694		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 1.7512002176717694 | validation: 2.2479693891914545]
	TIME [epoch: 9.53 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752316846666283		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 1.752316846666283 | validation: 2.2602863698411895]
	TIME [epoch: 9.55 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7416468257865296		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 1.7416468257865296 | validation: 2.248833538219389]
	TIME [epoch: 9.56 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7418395061518264		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 1.7418395061518264 | validation: 2.2550315293123315]
	TIME [epoch: 9.54 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7473537415147802		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 1.7473537415147802 | validation: 2.3023912372532243]
	TIME [epoch: 9.55 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7725672480586734		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 1.7725672480586734 | validation: 2.2876383484910807]
	TIME [epoch: 9.54 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441887486862246		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 1.7441887486862246 | validation: 2.2466891461914935]
	TIME [epoch: 9.56 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7572910592998678		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 1.7572910592998678 | validation: 2.269796454459264]
	TIME [epoch: 9.55 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7470747005610001		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 1.7470747005610001 | validation: 2.2561055595629353]
	TIME [epoch: 9.54 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7496492438017444		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 1.7496492438017444 | validation: 2.2720592861936924]
	TIME [epoch: 9.53 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7522738815031318		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 1.7522738815031318 | validation: 2.268491755518339]
	TIME [epoch: 9.57 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7464675233328575		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 1.7464675233328575 | validation: 2.2508019836668884]
	TIME [epoch: 9.55 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7451754946405882		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 1.7451754946405882 | validation: 2.2754388811291157]
	TIME [epoch: 9.55 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7531726480561685		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 1.7531726480561685 | validation: 2.271325460478831]
	TIME [epoch: 9.56 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7474074587654111		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 1.7474074587654111 | validation: 2.254316046290517]
	TIME [epoch: 9.55 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7481858897316656		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 1.7481858897316656 | validation: 2.2508035676054683]
	TIME [epoch: 9.54 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7498042505123728		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 1.7498042505123728 | validation: 2.243288900465376]
	TIME [epoch: 9.54 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7492261040616828		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 1.7492261040616828 | validation: 2.254772678009638]
	TIME [epoch: 9.56 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7434022591927516		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 1.7434022591927516 | validation: 2.26553533666729]
	TIME [epoch: 9.57 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751900363946773		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 1.751900363946773 | validation: 2.2545931481905153]
	TIME [epoch: 9.54 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7558153467829771		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 1.7558153467829771 | validation: 2.26014862681855]
	TIME [epoch: 9.53 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7483324370840723		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 1.7483324370840723 | validation: 2.267072766578315]
	TIME [epoch: 9.57 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7473206183060916		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 1.7473206183060916 | validation: 2.252589720458614]
	TIME [epoch: 9.54 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7522174271005135		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 1.7522174271005135 | validation: 2.2602545020141873]
	TIME [epoch: 9.54 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743552626925276		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 1.743552626925276 | validation: 2.2650177192916203]
	TIME [epoch: 9.53 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7463613885375129		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 1.7463613885375129 | validation: 2.280401373442263]
	TIME [epoch: 9.56 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.753316813802313		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 1.753316813802313 | validation: 2.27614651449236]
	TIME [epoch: 9.53 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7550955131930799		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 1.7550955131930799 | validation: 2.3038092108138133]
	TIME [epoch: 9.53 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7555305640974563		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 1.7555305640974563 | validation: 2.259713315974955]
	TIME [epoch: 9.54 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7480360854003787		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 1.7480360854003787 | validation: 2.2414572367126975]
	TIME [epoch: 9.58 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7520395772168584		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 1.7520395772168584 | validation: 2.2469377632419585]
	TIME [epoch: 9.54 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745317959078276		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 1.745317959078276 | validation: 2.255635653567316]
	TIME [epoch: 9.55 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7497345202915031		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 1.7497345202915031 | validation: 2.2618223399281576]
	TIME [epoch: 9.53 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752683149546582		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 1.752683149546582 | validation: 2.2728985128503774]
	TIME [epoch: 9.55 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475641507743347		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 1.7475641507743347 | validation: 2.2497779808359666]
	TIME [epoch: 9.55 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468092066670473		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 1.7468092066670473 | validation: 2.27210471810339]
	TIME [epoch: 9.54 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7427032673049545		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 1.7427032673049545 | validation: 2.261742508556795]
	TIME [epoch: 9.56 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746116350530738		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 1.746116350530738 | validation: 2.288888319962327]
	TIME [epoch: 9.55 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7458992200353491		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 1.7458992200353491 | validation: 2.266095692425411]
	TIME [epoch: 9.54 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7469325139700391		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 1.7469325139700391 | validation: 2.2832807534293313]
	TIME [epoch: 9.54 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7511022685107587		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 1.7511022685107587 | validation: 2.249698406345184]
	TIME [epoch: 9.56 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437444261235384		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 1.7437444261235384 | validation: 2.2550475396760614]
	TIME [epoch: 9.54 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745244473745182		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 1.745244473745182 | validation: 2.262229677801206]
	TIME [epoch: 9.53 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7482083599070317		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 1.7482083599070317 | validation: 2.2720498636433093]
	TIME [epoch: 9.54 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7597540848925532		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 1.7597540848925532 | validation: 2.2727858096019022]
	TIME [epoch: 9.55 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7535255651385149		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 1.7535255651385149 | validation: 2.2708057614995467]
	TIME [epoch: 9.53 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7432054345015786		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 1.7432054345015786 | validation: 2.248373667676111]
	TIME [epoch: 9.53 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7635357973933246		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 1.7635357973933246 | validation: 2.256609888215306]
	TIME [epoch: 9.55 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7470155417501463		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 1.7470155417501463 | validation: 2.255853047360021]
	TIME [epoch: 9.55 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7428530921155		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 1.7428530921155 | validation: 2.274050705410009]
	TIME [epoch: 9.54 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459596284607648		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 1.7459596284607648 | validation: 2.2466186697364363]
	TIME [epoch: 9.52 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7418749123938266		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 1.7418749123938266 | validation: 2.2486651457371636]
	TIME [epoch: 9.57 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739135160087161		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 1.739135160087161 | validation: 2.2515818744296743]
	TIME [epoch: 9.53 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442130495710004		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 1.7442130495710004 | validation: 2.2550045662280263]
	TIME [epoch: 9.55 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7547898349471098		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 1.7547898349471098 | validation: 2.261986375371632]
	TIME [epoch: 9.54 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743281383315857		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 1.743281383315857 | validation: 2.2631765465982734]
	TIME [epoch: 9.56 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747730190330234		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 1.747730190330234 | validation: 2.258753144735471]
	TIME [epoch: 9.53 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7583887822070339		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 1.7583887822070339 | validation: 2.276002910548223]
	TIME [epoch: 9.53 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7573447661355779		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 1.7573447661355779 | validation: 2.265066591781566]
	TIME [epoch: 9.54 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746476922415494		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 1.746476922415494 | validation: 2.2641756186839586]
	TIME [epoch: 9.57 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7488860722474748		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 1.7488860722474748 | validation: 2.2637986003601895]
	TIME [epoch: 9.53 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739371449550033		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 1.739371449550033 | validation: 2.2480656411274693]
	TIME [epoch: 9.55 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445031308772674		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 1.7445031308772674 | validation: 2.2534826293706285]
	TIME [epoch: 9.54 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429017300560592		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 1.7429017300560592 | validation: 2.2551351566789197]
	TIME [epoch: 9.57 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745560845771213		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 1.745560845771213 | validation: 2.2497535998933262]
	TIME [epoch: 9.53 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7527459463103583		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 1.7527459463103583 | validation: 2.2777260807202047]
	TIME [epoch: 9.55 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476126322088337		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 1.7476126322088337 | validation: 2.2459652418945413]
	TIME [epoch: 9.54 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7515614977912626		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 1.7515614977912626 | validation: 2.2449488866193614]
	TIME [epoch: 9.55 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7458907811349775		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 1.7458907811349775 | validation: 2.2399796314707756]
	TIME [epoch: 9.53 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7433868467178943		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 1.7433868467178943 | validation: 2.264401932998976]
	TIME [epoch: 9.55 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739430934834166		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 1.739430934834166 | validation: 2.262059570476181]
	TIME [epoch: 9.56 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7422161340906517		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 1.7422161340906517 | validation: 2.2590864018600105]
	TIME [epoch: 9.54 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7502439812146071		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 1.7502439812146071 | validation: 2.252534136285383]
	TIME [epoch: 9.55 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7587016106044806		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 1.7587016106044806 | validation: 2.2587765483973676]
	TIME [epoch: 9.56 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475405253246827		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 1.7475405253246827 | validation: 2.255389462682306]
	TIME [epoch: 9.56 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744573506409441		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 1.744573506409441 | validation: 2.260614483354675]
	TIME [epoch: 9.55 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7543423180884254		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 1.7543423180884254 | validation: 2.255092936010264]
	TIME [epoch: 9.54 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445510243071727		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 1.7445510243071727 | validation: 2.2634692322836374]
	TIME [epoch: 9.55 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7428339970519147		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 1.7428339970519147 | validation: 2.2476872813013524]
	TIME [epoch: 9.58 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7412624266324748		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 1.7412624266324748 | validation: 2.2654748468665624]
	TIME [epoch: 9.53 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740677177853603		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 1.740677177853603 | validation: 2.2375810929391395]
	TIME [epoch: 9.54 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421430671839466		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 1.7421430671839466 | validation: 2.255451811552634]
	TIME [epoch: 9.55 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7464220746870975		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 1.7464220746870975 | validation: 2.2674095033084654]
	TIME [epoch: 9.55 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.757508746307958		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 1.757508746307958 | validation: 2.2979238493115197]
	TIME [epoch: 9.54 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7510667416878611		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 1.7510667416878611 | validation: 2.2570212611364684]
	TIME [epoch: 9.52 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7444152255010317		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 1.7444152255010317 | validation: 2.2502277572747267]
	TIME [epoch: 9.55 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7559335811489725		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 1.7559335811489725 | validation: 2.2633743330278886]
	TIME [epoch: 9.54 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748854388424819		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 1.748854388424819 | validation: 2.237109224251877]
	TIME [epoch: 9.54 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7479415936695624		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 1.7479415936695624 | validation: 2.2379913185136413]
	TIME [epoch: 9.53 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.742666992845249		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 1.742666992845249 | validation: 2.260048868214103]
	TIME [epoch: 9.56 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751915832458605		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 1.751915832458605 | validation: 2.262176273611258]
	TIME [epoch: 9.54 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7485936343162822		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 1.7485936343162822 | validation: 2.257253655392808]
	TIME [epoch: 9.53 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7533006999921654		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 1.7533006999921654 | validation: 2.261357883284712]
	TIME [epoch: 9.56 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.749234982715746		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 1.749234982715746 | validation: 2.2683843212914545]
	TIME [epoch: 9.57 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7414616935489575		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 1.7414616935489575 | validation: 2.2804906780975736]
	TIME [epoch: 9.54 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440900407986966		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 1.7440900407986966 | validation: 2.2413404865602775]
	TIME [epoch: 9.54 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7447094727446213		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 1.7447094727446213 | validation: 2.270663372582631]
	TIME [epoch: 9.53 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7492243929417834		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 1.7492243929417834 | validation: 2.2663192311877625]
	TIME [epoch: 9.57 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442484264282363		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 1.7442484264282363 | validation: 2.2322073107823237]
	TIME [epoch: 9.55 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461764693712065		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 1.7461764693712065 | validation: 2.268099231494739]
	TIME [epoch: 9.55 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460163433698739		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 1.7460163433698739 | validation: 2.2501139000875225]
	TIME [epoch: 9.55 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7451897073480118		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 1.7451897073480118 | validation: 2.2713744512749656]
	TIME [epoch: 9.54 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7592109363088562		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 1.7592109363088562 | validation: 2.2463313379157923]
	TIME [epoch: 9.54 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7485640631845023		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 1.7485640631845023 | validation: 2.2661472102649647]
	TIME [epoch: 9.52 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429276395064015		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 1.7429276395064015 | validation: 2.257740053180956]
	TIME [epoch: 9.55 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7458793713375205		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 1.7458793713375205 | validation: 2.243489639973368]
	TIME [epoch: 9.54 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7596679748232518		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 1.7596679748232518 | validation: 2.2642473615157908]
	TIME [epoch: 9.55 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744383628634489		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 1.744383628634489 | validation: 2.263841815211977]
	TIME [epoch: 9.54 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750636516795217		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 1.750636516795217 | validation: 2.260257232490671]
	TIME [epoch: 9.55 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7449366997454063		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 1.7449366997454063 | validation: 2.232886363377511]
	TIME [epoch: 9.55 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425368509988814		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 1.7425368509988814 | validation: 2.2563316484199087]
	TIME [epoch: 9.53 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441397767090703		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 1.7441397767090703 | validation: 2.264052793122385]
	TIME [epoch: 9.54 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7464070348673206		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 1.7464070348673206 | validation: 2.246634235282113]
	TIME [epoch: 9.57 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493098907992692		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 1.7493098907992692 | validation: 2.236145326405938]
	TIME [epoch: 9.55 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7367403429964416		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 1.7367403429964416 | validation: 2.2440167321426796]
	TIME [epoch: 9.55 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7463073838912657		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 1.7463073838912657 | validation: 2.2442396881732747]
	TIME [epoch: 9.56 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7500488431535364		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 1.7500488431535364 | validation: 2.268067924098562]
	TIME [epoch: 9.55 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459061404323386		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 1.7459061404323386 | validation: 2.2377184983969376]
	TIME [epoch: 9.54 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7528418862506006		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 1.7528418862506006 | validation: 2.2664012969107215]
	TIME [epoch: 9.53 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.754019224911938		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 1.754019224911938 | validation: 2.2417981740531907]
	TIME [epoch: 9.55 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7411640793096363		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 1.7411640793096363 | validation: 2.284067195932638]
	TIME [epoch: 9.54 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7540214000852525		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 1.7540214000852525 | validation: 2.2595873474260926]
	TIME [epoch: 9.55 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437257223029647		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 1.7437257223029647 | validation: 2.2524492807362724]
	TIME [epoch: 9.55 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75228052685639		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 1.75228052685639 | validation: 2.287147687479983]
	TIME [epoch: 9.57 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7563085001104501		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 1.7563085001104501 | validation: 2.276104988127627]
	TIME [epoch: 9.54 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7501272810752735		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 1.7501272810752735 | validation: 2.2537955946455153]
	TIME [epoch: 9.56 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7449759752336516		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 1.7449759752336516 | validation: 2.248391273300044]
	TIME [epoch: 9.54 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7482574494573513		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 1.7482574494573513 | validation: 2.27159310842635]
	TIME [epoch: 9.57 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7557097003788382		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 1.7557097003788382 | validation: 2.265706248997329]
	TIME [epoch: 9.55 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751632078534175		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 1.751632078534175 | validation: 2.2908831595933097]
	TIME [epoch: 9.55 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7553176346663655		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 1.7553176346663655 | validation: 2.285659018752071]
	TIME [epoch: 9.53 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750370171475107		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 1.750370171475107 | validation: 2.256019875761384]
	TIME [epoch: 9.57 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748165110196626		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 1.748165110196626 | validation: 2.2680476043576725]
	TIME [epoch: 9.55 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7426129676067041		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 1.7426129676067041 | validation: 2.2577227877244614]
	TIME [epoch: 9.56 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7458116741013612		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 1.7458116741013612 | validation: 2.2520950802175865]
	TIME [epoch: 9.55 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7420123781302705		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 1.7420123781302705 | validation: 2.2642051640128646]
	TIME [epoch: 9.55 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745657159928027		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 1.745657159928027 | validation: 2.2528199914259006]
	TIME [epoch: 9.53 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468830344690738		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 1.7468830344690738 | validation: 2.2595507293404857]
	TIME [epoch: 9.55 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7506457822250951		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 1.7506457822250951 | validation: 2.2392996868299697]
	TIME [epoch: 9.57 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7470501608576696		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 1.7470501608576696 | validation: 2.2719924679030603]
	TIME [epoch: 9.55 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7452330905319762		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 1.7452330905319762 | validation: 2.270285665621606]
	TIME [epoch: 9.53 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7519715702453957		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 1.7519715702453957 | validation: 2.2459736796508225]
	TIME [epoch: 9.54 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7544545836082044		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 1.7544545836082044 | validation: 2.269300308641236]
	TIME [epoch: 9.56 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7448096403565114		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 1.7448096403565114 | validation: 2.2677445016875883]
	TIME [epoch: 9.56 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454828414473043		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 1.7454828414473043 | validation: 2.249084388152158]
	TIME [epoch: 9.54 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751047473741816		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 1.751047473741816 | validation: 2.2609514651979157]
	TIME [epoch: 9.55 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461354475730961		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 1.7461354475730961 | validation: 2.2483464034770537]
	TIME [epoch: 9.56 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7469838718372919		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 1.7469838718372919 | validation: 2.2456781174967655]
	TIME [epoch: 9.55 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.757130290513226		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 1.757130290513226 | validation: 2.264075626656693]
	TIME [epoch: 9.54 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7504784397491366		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 1.7504784397491366 | validation: 2.2576960482625803]
	TIME [epoch: 9.55 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7480063646391997		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 1.7480063646391997 | validation: 2.2695029983783566]
	TIME [epoch: 9.55 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7533664354673555		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 1.7533664354673555 | validation: 2.250293097244652]
	TIME [epoch: 9.56 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437071057119353		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 1.7437071057119353 | validation: 2.2555108835539217]
	TIME [epoch: 9.52 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7488928569067972		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 1.7488928569067972 | validation: 2.301083625494403]
	TIME [epoch: 9.55 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7501284031486914		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 1.7501284031486914 | validation: 2.261360127817781]
	TIME [epoch: 9.54 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7540955660993998		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 1.7540955660993998 | validation: 2.26069966727743]
	TIME [epoch: 9.56 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7453245736646525		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 1.7453245736646525 | validation: 2.2461492815691817]
	TIME [epoch: 9.54 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7394054616199148		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 1.7394054616199148 | validation: 2.277622868686975]
	TIME [epoch: 9.57 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7565754616939517		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 1.7565754616939517 | validation: 2.2510979809591642]
	TIME [epoch: 9.55 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7514458025555997		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 1.7514458025555997 | validation: 2.263948053450345]
	TIME [epoch: 9.53 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7526280859192727		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 1.7526280859192727 | validation: 2.2532173245421814]
	TIME [epoch: 9.55 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461571669300036		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 1.7461571669300036 | validation: 2.2673731017469168]
	TIME [epoch: 9.57 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461828377036226		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 1.7461828377036226 | validation: 2.2661621369464577]
	TIME [epoch: 9.54 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7474374038815632		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 1.7474374038815632 | validation: 2.262361394922534]
	TIME [epoch: 9.54 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740343685495829		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 1.740343685495829 | validation: 2.2564059403577024]
	TIME [epoch: 9.53 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7544590093444306		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 1.7544590093444306 | validation: 2.2584943910386874]
	TIME [epoch: 9.56 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7553305541433009		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 1.7553305541433009 | validation: 2.268692179912576]
	TIME [epoch: 9.54 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460780006996797		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 1.7460780006996797 | validation: 2.259863578899466]
	TIME [epoch: 9.54 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7495719018245783		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 1.7495719018245783 | validation: 2.2747117463940576]
	TIME [epoch: 9.54 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461764053585025		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 1.7461764053585025 | validation: 2.2494030905413083]
	TIME [epoch: 9.55 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748025402296527		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 1.748025402296527 | validation: 2.249345004768077]
	TIME [epoch: 9.55 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7595242678236267		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 1.7595242678236267 | validation: 2.254301956785683]
	TIME [epoch: 9.54 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748174193581665		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 1.748174193581665 | validation: 2.2509196581437836]
	TIME [epoch: 9.56 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751180610772715		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 1.751180610772715 | validation: 2.264374218723574]
	TIME [epoch: 9.54 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7415931995527		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 1.7415931995527 | validation: 2.2579922470030733]
	TIME [epoch: 9.55 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7430682974942333		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 1.7430682974942333 | validation: 2.258698959419599]
	TIME [epoch: 9.55 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.749346214950706		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 1.749346214950706 | validation: 2.2286627774541263]
	TIME [epoch: 9.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1531.pth
	Model improved!!!
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7419798330356098		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 1.7419798330356098 | validation: 2.2547362282544965]
	TIME [epoch: 9.56 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7412494395348657		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 1.7412494395348657 | validation: 2.257370137547136]
	TIME [epoch: 9.55 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438286360846214		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 1.7438286360846214 | validation: 2.281066255228916]
	TIME [epoch: 9.53 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748091390334146		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 1.748091390334146 | validation: 2.239874321991056]
	TIME [epoch: 9.57 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7373184345980888		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 1.7373184345980888 | validation: 2.2643310626617357]
	TIME [epoch: 9.54 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7522930078683618		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 1.7522930078683618 | validation: 2.292143300313709]
	TIME [epoch: 9.53 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7471454339619363		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 1.7471454339619363 | validation: 2.2607219902962954]
	TIME [epoch: 9.55 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.738790106361418		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 1.738790106361418 | validation: 2.2277266458443052]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1539.pth
	Model improved!!!
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7451567989354408		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 1.7451567989354408 | validation: 2.252353490177453]
	TIME [epoch: 9.53 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748469043581429		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 1.748469043581429 | validation: 2.2413147545413836]
	TIME [epoch: 9.54 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751760420866915		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 1.751760420866915 | validation: 2.249124065833882]
	TIME [epoch: 9.56 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7449711846086309		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 1.7449711846086309 | validation: 2.255041025268857]
	TIME [epoch: 9.53 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441439329304316		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 1.7441439329304316 | validation: 2.267793939481133]
	TIME [epoch: 9.54 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7527983930507136		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 1.7527983930507136 | validation: 2.2371730150539846]
	TIME [epoch: 9.55 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493756768262663		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 1.7493756768262663 | validation: 2.2601213751208373]
	TIME [epoch: 9.56 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7520879462600412		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 1.7520879462600412 | validation: 2.2459356562627897]
	TIME [epoch: 9.53 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7539929604824978		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 1.7539929604824978 | validation: 2.2440294007970425]
	TIME [epoch: 9.55 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7491301057572375		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 1.7491301057572375 | validation: 2.2403600208635677]
	TIME [epoch: 9.55 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446742725412896		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 1.7446742725412896 | validation: 2.2543841828547424]
	TIME [epoch: 9.57 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750991603993333		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 1.750991603993333 | validation: 2.242802695868817]
	TIME [epoch: 9.55 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.741656839100178		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 1.741656839100178 | validation: 2.2673383020476714]
	TIME [epoch: 9.55 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429927976605577		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 1.7429927976605577 | validation: 2.243448125781233]
	TIME [epoch: 9.57 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7508350433224398		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 1.7508350433224398 | validation: 2.2558838346644103]
	TIME [epoch: 9.56 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7471241592855953		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 1.7471241592855953 | validation: 2.2503827663173883]
	TIME [epoch: 9.54 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7444213742933115		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 1.7444213742933115 | validation: 2.2480168523149455]
	TIME [epoch: 9.54 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7481326500686556		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 1.7481326500686556 | validation: 2.2421401475111766]
	TIME [epoch: 9.55 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.742094410183438		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 1.742094410183438 | validation: 2.257135386286248]
	TIME [epoch: 9.55 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7434699203578128		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 1.7434699203578128 | validation: 2.2642104949279296]
	TIME [epoch: 9.54 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7410442066592857		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 1.7410442066592857 | validation: 2.241654747653566]
	TIME [epoch: 9.56 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7488667652755416		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 1.7488667652755416 | validation: 2.251044169422664]
	TIME [epoch: 9.55 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7413272430533724		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 1.7413272430533724 | validation: 2.2567700220867533]
	TIME [epoch: 9.56 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425160193763205		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 1.7425160193763205 | validation: 2.2535907333558924]
	TIME [epoch: 9.55 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7391986794630843		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 1.7391986794630843 | validation: 2.2499198663146522]
	TIME [epoch: 9.54 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7491175312493161		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 1.7491175312493161 | validation: 2.241203262368421]
	TIME [epoch: 9.57 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750133284730245		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 1.750133284730245 | validation: 2.244562937807687]
	TIME [epoch: 9.54 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744830297577996		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 1.744830297577996 | validation: 2.2560385841651587]
	TIME [epoch: 9.55 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7527296729880408		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 1.7527296729880408 | validation: 2.2741495326586505]
	TIME [epoch: 9.55 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7552157359657936		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 1.7552157359657936 | validation: 2.2662813203104824]
	TIME [epoch: 9.56 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7433265174632198		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 1.7433265174632198 | validation: 2.280181279294668]
	TIME [epoch: 9.54 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7533059997362757		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 1.7533059997362757 | validation: 2.2723361143012415]
	TIME [epoch: 9.54 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746403375851873		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 1.746403375851873 | validation: 2.2473993728400736]
	TIME [epoch: 9.56 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7401587651096677		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 1.7401587651096677 | validation: 2.2441345660068994]
	TIME [epoch: 9.54 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7457625691959606		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 1.7457625691959606 | validation: 2.2615648713585803]
	TIME [epoch: 9.55 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7474749880527196		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 1.7474749880527196 | validation: 2.271768904255857]
	TIME [epoch: 9.54 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476762488754491		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 1.7476762488754491 | validation: 2.254895977585199]
	TIME [epoch: 9.55 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745891940006079		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 1.745891940006079 | validation: 2.2519553352710697]
	TIME [epoch: 9.53 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476109280036467		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 1.7476109280036467 | validation: 2.2396533689776423]
	TIME [epoch: 9.52 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7555507668145638		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 1.7555507668145638 | validation: 2.2367984760180075]
	TIME [epoch: 9.53 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7427407576933187		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 1.7427407576933187 | validation: 2.2355835838352003]
	TIME [epoch: 9.55 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7393709023462072		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 1.7393709023462072 | validation: 2.2443117776668164]
	TIME [epoch: 9.53 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7408360525607143		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 1.7408360525607143 | validation: 2.268005097731241]
	TIME [epoch: 9.52 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7496882907081577		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 1.7496882907081577 | validation: 2.2642076031675344]
	TIME [epoch: 9.53 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7452531339821118		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 1.7452531339821118 | validation: 2.258453309635234]
	TIME [epoch: 9.55 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7477149092595823		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 1.7477149092595823 | validation: 2.26762687073915]
	TIME [epoch: 9.54 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7426921769355908		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 1.7426921769355908 | validation: 2.2554229732224265]
	TIME [epoch: 9.53 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7514720968614697		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 1.7514720968614697 | validation: 2.253300141346591]
	TIME [epoch: 9.52 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7419705821515599		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 1.7419705821515599 | validation: 2.263870475997713]
	TIME [epoch: 9.55 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429643291734636		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 1.7429643291734636 | validation: 2.259425640127316]
	TIME [epoch: 9.52 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438145284441362		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 1.7438145284441362 | validation: 2.2447326937270358]
	TIME [epoch: 9.54 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743678739096816		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 1.743678739096816 | validation: 2.2406869963748015]
	TIME [epoch: 9.55 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7410900896443482		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 1.7410900896443482 | validation: 2.2728744556462277]
	TIME [epoch: 9.54 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.749047854712106		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 1.749047854712106 | validation: 2.284422775760822]
	TIME [epoch: 9.53 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7526129752571626		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 1.7526129752571626 | validation: 2.258542784634944]
	TIME [epoch: 9.53 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7466837337804606		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 1.7466837337804606 | validation: 2.240557227538354]
	TIME [epoch: 9.55 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743288493135108		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 1.743288493135108 | validation: 2.2543906087050583]
	TIME [epoch: 9.52 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7419487430981975		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 1.7419487430981975 | validation: 2.239962115136875]
	TIME [epoch: 9.54 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438324432861758		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 1.7438324432861758 | validation: 2.2417298467365057]
	TIME [epoch: 9.55 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7458241551303524		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 1.7458241551303524 | validation: 2.2520756243728797]
	TIME [epoch: 9.56 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446210209342499		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 1.7446210209342499 | validation: 2.2399681138587524]
	TIME [epoch: 9.54 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746028298409757		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 1.746028298409757 | validation: 2.266091718512047]
	TIME [epoch: 9.54 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7464926623970065		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 1.7464926623970065 | validation: 2.2594450535167816]
	TIME [epoch: 9.54 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460852050825495		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 1.7460852050825495 | validation: 2.2557362265456202]
	TIME [epoch: 9.54 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.752209445521082		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 1.752209445521082 | validation: 2.2502155558337638]
	TIME [epoch: 9.55 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739043331121876		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 1.739043331121876 | validation: 2.250867372253401]
	TIME [epoch: 9.51 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7526431873454336		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 1.7526431873454336 | validation: 2.2405388286548327]
	TIME [epoch: 9.55 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454362405928863		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 1.7454362405928863 | validation: 2.262720153330591]
	TIME [epoch: 9.54 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740426155862608		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 1.740426155862608 | validation: 2.2507454548076953]
	TIME [epoch: 9.54 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7511273781812888		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 1.7511273781812888 | validation: 2.2565967037451165]
	TIME [epoch: 9.54 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7472422346909258		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 1.7472422346909258 | validation: 2.2549214156479485]
	TIME [epoch: 9.57 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7418070859038859		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 1.7418070859038859 | validation: 2.245207461721958]
	TIME [epoch: 9.54 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459456978244539		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 1.7459456978244539 | validation: 2.2520014447425196]
	TIME [epoch: 9.55 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745769197058093		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 1.745769197058093 | validation: 2.26298218593844]
	TIME [epoch: 9.53 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7439024864658357		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 1.7439024864658357 | validation: 2.2767338104958528]
	TIME [epoch: 9.55 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7455429734156787		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 1.7455429734156787 | validation: 2.269262834240353]
	TIME [epoch: 9.53 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7527340710429242		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 1.7527340710429242 | validation: 2.250154547297346]
	TIME [epoch: 9.52 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747311831076025		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 1.747311831076025 | validation: 2.252830708019114]
	TIME [epoch: 9.54 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7513653768626682		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 1.7513653768626682 | validation: 2.2440732245745254]
	TIME [epoch: 9.55 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454863710397805		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 1.7454863710397805 | validation: 2.251846795954776]
	TIME [epoch: 9.54 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421186039876595		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 1.7421186039876595 | validation: 2.250257839926512]
	TIME [epoch: 9.53 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746257089986883		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 1.746257089986883 | validation: 2.2539053497257404]
	TIME [epoch: 9.55 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7405626406335675		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 1.7405626406335675 | validation: 2.253635599962645]
	TIME [epoch: 9.54 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425984060462412		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 1.7425984060462412 | validation: 2.2498930505059715]
	TIME [epoch: 9.53 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7453863507644034		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 1.7453863507644034 | validation: 2.255015000108401]
	TIME [epoch: 9.52 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7494886680570698		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 1.7494886680570698 | validation: 2.280436359351964]
	TIME [epoch: 9.54 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750201574539767		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 1.750201574539767 | validation: 2.25498415387324]
	TIME [epoch: 9.53 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7491741662006188		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 1.7491741662006188 | validation: 2.277218480953111]
	TIME [epoch: 9.53 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743582844696565		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 1.743582844696565 | validation: 2.2448679922156902]
	TIME [epoch: 9.53 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468203427459799		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 1.7468203427459799 | validation: 2.252568617995605]
	TIME [epoch: 9.55 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445907844384867		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 1.7445907844384867 | validation: 2.2689037751403824]
	TIME [epoch: 9.54 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744607348782612		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 1.744607348782612 | validation: 2.264957324002941]
	TIME [epoch: 9.53 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7515010956195265		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 1.7515010956195265 | validation: 2.247176311467706]
	TIME [epoch: 9.52 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7458431961669167		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 1.7458431961669167 | validation: 2.2562208769084493]
	TIME [epoch: 9.54 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440352162454478		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 1.7440352162454478 | validation: 2.25925412514445]
	TIME [epoch: 9.55 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442174725658213		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 1.7442174725658213 | validation: 2.2565474018606273]
	TIME [epoch: 9.53 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7469608635009268		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 1.7469608635009268 | validation: 2.260536042738741]
	TIME [epoch: 9.54 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7435296354788332		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 1.7435296354788332 | validation: 2.255589535338411]
	TIME [epoch: 9.54 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459794419146555		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 1.7459794419146555 | validation: 2.259429259433315]
	TIME [epoch: 9.53 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7498818870587847		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 1.7498818870587847 | validation: 2.246838274813951]
	TIME [epoch: 9.53 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747603823606493		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 1.747603823606493 | validation: 2.2631364262604525]
	TIME [epoch: 9.55 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445022192888726		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 1.7445022192888726 | validation: 2.245729157951766]
	TIME [epoch: 9.53 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74595033626773		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 1.74595033626773 | validation: 2.266157533762676]
	TIME [epoch: 9.53 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7384260772242137		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 1.7384260772242137 | validation: 2.2512618760265433]
	TIME [epoch: 9.54 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7471795316462693		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 1.7471795316462693 | validation: 2.263503602292758]
	TIME [epoch: 9.56 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748218948959622		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 1.748218948959622 | validation: 2.2627878272764104]
	TIME [epoch: 9.53 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7407603943575576		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 1.7407603943575576 | validation: 2.2593050093483447]
	TIME [epoch: 9.53 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7414848896223556		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 1.7414848896223556 | validation: 2.2552435808035005]
	TIME [epoch: 9.52 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7426614520458732		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 1.7426614520458732 | validation: 2.2426913922351535]
	TIME [epoch: 9.55 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.749173147938566		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 1.749173147938566 | validation: 2.25677906384557]
	TIME [epoch: 9.55 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446840280628237		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 1.7446840280628237 | validation: 2.25079421834948]
	TIME [epoch: 9.54 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7432838928491232		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 1.7432838928491232 | validation: 2.257704410495571]
	TIME [epoch: 9.53 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459088072574764		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 1.7459088072574764 | validation: 2.2491492778606905]
	TIME [epoch: 9.54 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7455836260980468		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 1.7455836260980468 | validation: 2.2708632278807652]
	TIME [epoch: 9.53 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7462245758283756		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 1.7462245758283756 | validation: 2.244843813593565]
	TIME [epoch: 9.54 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7412722273118142		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 1.7412722273118142 | validation: 2.2521070502711287]
	TIME [epoch: 9.56 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475671541653917		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 1.7475671541653917 | validation: 2.2551828692141354]
	TIME [epoch: 9.56 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7566166349934378		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 1.7566166349934378 | validation: 2.2610123831832887]
	TIME [epoch: 9.53 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746026099362091		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 1.746026099362091 | validation: 2.2540898790450212]
	TIME [epoch: 9.55 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7468001275834735		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 1.7468001275834735 | validation: 2.2589205652177995]
	TIME [epoch: 9.56 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7453597053231682		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 1.7453597053231682 | validation: 2.255125609219971]
	TIME [epoch: 9.53 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746209644084098		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 1.746209644084098 | validation: 2.2463000311672014]
	TIME [epoch: 9.52 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7530307220853707		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 1.7530307220853707 | validation: 2.2349884215161078]
	TIME [epoch: 9.53 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.742686313113273		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 1.742686313113273 | validation: 2.2538258840089256]
	TIME [epoch: 9.56 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7451555783688861		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 1.7451555783688861 | validation: 2.23872186464916]
	TIME [epoch: 9.52 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7465562389662614		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 1.7465562389662614 | validation: 2.2345695924627615]
	TIME [epoch: 9.55 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7390224039665731		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 1.7390224039665731 | validation: 2.230518111456067]
	TIME [epoch: 9.55 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7405549425791957		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 1.7405549425791957 | validation: 2.2606051907857965]
	TIME [epoch: 9.56 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443971058117256		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 1.7443971058117256 | validation: 2.2488941768311794]
	TIME [epoch: 9.54 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440784362251853		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 1.7440784362251853 | validation: 2.268415705870256]
	TIME [epoch: 9.53 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750239762965781		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 1.750239762965781 | validation: 2.2697176708227325]
	TIME [epoch: 9.52 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7500910696100078		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 1.7500910696100078 | validation: 2.2398816570153843]
	TIME [epoch: 9.57 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7411105075501143		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 1.7411105075501143 | validation: 2.257363360415796]
	TIME [epoch: 9.55 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7385871522997884		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 1.7385871522997884 | validation: 2.2455957365141224]
	TIME [epoch: 9.55 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745975486238867		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 1.745975486238867 | validation: 2.252633291004262]
	TIME [epoch: 9.56 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446136126827536		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 1.7446136126827536 | validation: 2.250766323394264]
	TIME [epoch: 9.56 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7431158751953195		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 1.7431158751953195 | validation: 2.2537129717675146]
	TIME [epoch: 9.54 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744061647919769		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 1.744061647919769 | validation: 2.258168274864614]
	TIME [epoch: 9.55 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7432517713698652		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 1.7432517713698652 | validation: 2.2589187640660224]
	TIME [epoch: 9.57 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7449610934295001		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 1.7449610934295001 | validation: 2.2603348337199862]
	TIME [epoch: 9.54 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7407610656966335		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 1.7407610656966335 | validation: 2.250230531834713]
	TIME [epoch: 9.54 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7452833111508135		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 1.7452833111508135 | validation: 2.2533227568538554]
	TIME [epoch: 9.55 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7472542721433153		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 1.7472542721433153 | validation: 2.259089110857697]
	TIME [epoch: 9.57 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7426891135961236		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 1.7426891135961236 | validation: 2.251217755959313]
	TIME [epoch: 9.55 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740957101388728		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 1.740957101388728 | validation: 2.2599515036589755]
	TIME [epoch: 9.54 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7467722696643275		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 1.7467722696643275 | validation: 2.2400342103033695]
	TIME [epoch: 9.52 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7435688269912473		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 1.7435688269912473 | validation: 2.2511754738705996]
	TIME [epoch: 9.56 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429034528252665		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 1.7429034528252665 | validation: 2.2447365382947018]
	TIME [epoch: 9.53 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7486110895010285		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 1.7486110895010285 | validation: 2.2539874019779793]
	TIME [epoch: 9.54 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7478812967268087		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 1.7478812967268087 | validation: 2.275675955538127]
	TIME [epoch: 9.55 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7387395299876096		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 1.7387395299876096 | validation: 2.2661666664957476]
	TIME [epoch: 9.53 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7501156304712786		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 1.7501156304712786 | validation: 2.2826359176880353]
	TIME [epoch: 9.54 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7470934655353196		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 1.7470934655353196 | validation: 2.2530104130193314]
	TIME [epoch: 9.53 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7434211109376352		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 1.7434211109376352 | validation: 2.2712685824296335]
	TIME [epoch: 9.55 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441586895251457		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 1.7441586895251457 | validation: 2.258127198402483]
	TIME [epoch: 9.54 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7462075160917572		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 1.7462075160917572 | validation: 2.2558232112700587]
	TIME [epoch: 9.56 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454858814360066		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 1.7454858814360066 | validation: 2.240567247035122]
	TIME [epoch: 9.53 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7399829824074704		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 1.7399829824074704 | validation: 2.2482303105827066]
	TIME [epoch: 9.57 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7419597441787555		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 1.7419597441787555 | validation: 2.254401226999822]
	TIME [epoch: 9.53 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7495758056578208		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 1.7495758056578208 | validation: 2.2649902109911544]
	TIME [epoch: 9.55 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7503178338658576		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 1.7503178338658576 | validation: 2.265922678637801]
	TIME [epoch: 9.55 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7469751351711085		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 1.7469751351711085 | validation: 2.2538494003783636]
	TIME [epoch: 9.58 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7402614638251208		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 1.7402614638251208 | validation: 2.2636633758385307]
	TIME [epoch: 9.53 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.747459714234663		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 1.747459714234663 | validation: 2.2676071313615234]
	TIME [epoch: 9.54 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7447177763210706		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 1.7447177763210706 | validation: 2.2651253691469457]
	TIME [epoch: 9.54 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443281655157912		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 1.7443281655157912 | validation: 2.2587226304527763]
	TIME [epoch: 9.58 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7356943673983611		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 1.7356943673983611 | validation: 2.248263304885046]
	TIME [epoch: 9.55 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7466441526242722		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 1.7466441526242722 | validation: 2.268446825425172]
	TIME [epoch: 9.53 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445848040563579		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 1.7445848040563579 | validation: 2.2633255287297955]
	TIME [epoch: 9.56 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7474815486568254		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 1.7474815486568254 | validation: 2.243027539875421]
	TIME [epoch: 9.55 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7369306510859288		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 1.7369306510859288 | validation: 2.2493495078541765]
	TIME [epoch: 9.53 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437107551167776		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 1.7437107551167776 | validation: 2.2461527357436064]
	TIME [epoch: 9.54 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437381215130394		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 1.7437381215130394 | validation: 2.2395500382838196]
	TIME [epoch: 9.55 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440194497248929		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 1.7440194497248929 | validation: 2.2537777486703345]
	TIME [epoch: 9.56 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461726779758155		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 1.7461726779758155 | validation: 2.2539969015983035]
	TIME [epoch: 9.55 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443493765280018		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 1.7443493765280018 | validation: 2.239073554725042]
	TIME [epoch: 9.53 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746041343849144		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 1.746041343849144 | validation: 2.25033673748725]
	TIME [epoch: 9.55 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442116185373695		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 1.7442116185373695 | validation: 2.258916689853744]
	TIME [epoch: 9.56 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443266649082099		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 1.7443266649082099 | validation: 2.2571968518356162]
	TIME [epoch: 9.54 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7426038470438734		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 1.7426038470438734 | validation: 2.2544251944544893]
	TIME [epoch: 9.56 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7386995001340584		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 1.7386995001340584 | validation: 2.2484455845082216]
	TIME [epoch: 9.57 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7412546498236565		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 1.7412546498236565 | validation: 2.2599916637833615]
	TIME [epoch: 9.56 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425617922507175		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 1.7425617922507175 | validation: 2.2544161740688438]
	TIME [epoch: 9.55 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7491511153407522		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 1.7491511153407522 | validation: 2.2341062056125396]
	TIME [epoch: 9.55 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7449765093778447		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 1.7449765093778447 | validation: 2.2385151324353165]
	TIME [epoch: 9.56 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461875422534192		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 1.7461875422534192 | validation: 2.2383950573170246]
	TIME [epoch: 9.56 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.758201279378102		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 1.758201279378102 | validation: 2.2365695217890313]
	TIME [epoch: 9.53 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7483984882378025		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 1.7483984882378025 | validation: 2.2444719650137697]
	TIME [epoch: 9.56 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7524490332592724		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 1.7524490332592724 | validation: 2.2694821153675604]
	TIME [epoch: 9.54 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744111298335725		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 1.744111298335725 | validation: 2.2759198552909927]
	TIME [epoch: 9.54 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7384617921153374		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 1.7384617921153374 | validation: 2.241082499284707]
	TIME [epoch: 9.54 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7490577255921906		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 1.7490577255921906 | validation: 2.2521747162247467]
	TIME [epoch: 9.58 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748638938450468		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 1.748638938450468 | validation: 2.236361296362346]
	TIME [epoch: 9.55 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7508191334882786		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 1.7508191334882786 | validation: 2.2437066586408823]
	TIME [epoch: 9.56 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748218023477726		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 1.748218023477726 | validation: 2.2370448167544446]
	TIME [epoch: 9.56 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740938666014522		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 1.740938666014522 | validation: 2.248691349543207]
	TIME [epoch: 9.57 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7472094376199618		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 1.7472094376199618 | validation: 2.253151085194484]
	TIME [epoch: 9.56 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7345339364894898		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 1.7345339364894898 | validation: 2.2481198438728986]
	TIME [epoch: 9.55 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7510712035144411		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 1.7510712035144411 | validation: 2.2321979695995404]
	TIME [epoch: 9.56 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445964038615471		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 1.7445964038615471 | validation: 2.2554096258798744]
	TIME [epoch: 9.57 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7499868741025257		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 1.7499868741025257 | validation: 2.256612860413354]
	TIME [epoch: 9.55 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7448038647438788		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 1.7448038647438788 | validation: 2.2549019908915002]
	TIME [epoch: 9.53 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739494939452302		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 1.739494939452302 | validation: 2.264913862090757]
	TIME [epoch: 9.56 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7430593145446742		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 1.7430593145446742 | validation: 2.248940361116618]
	TIME [epoch: 9.56 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7436161686758005		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 1.7436161686758005 | validation: 2.268743782174143]
	TIME [epoch: 9.56 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454847160033384		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 1.7454847160033384 | validation: 2.2490390131495768]
	TIME [epoch: 9.55 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.737465795718699		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 1.737465795718699 | validation: 2.2531700585667807]
	TIME [epoch: 9.57 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7477667065862392		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 1.7477667065862392 | validation: 2.280601065243221]
	TIME [epoch: 9.55 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493793316444168		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 1.7493793316444168 | validation: 2.2468046847521084]
	TIME [epoch: 9.55 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743517020421189		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 1.743517020421189 | validation: 2.24600328637604]
	TIME [epoch: 9.52 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7456842851982917		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 1.7456842851982917 | validation: 2.2463580427076693]
	TIME [epoch: 9.59 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7492762265772783		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 1.7492762265772783 | validation: 2.2568758936216198]
	TIME [epoch: 9.54 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7467798011010884		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 1.7467798011010884 | validation: 2.265602606365158]
	TIME [epoch: 9.55 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7480634854802808		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 1.7480634854802808 | validation: 2.2670054994986133]
	TIME [epoch: 9.56 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7375255371898828		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 1.7375255371898828 | validation: 2.2555212252199373]
	TIME [epoch: 9.59 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454910735516223		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 1.7454910735516223 | validation: 2.2504768289274195]
	TIME [epoch: 9.54 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7466226289782163		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 1.7466226289782163 | validation: 2.2486761117291336]
	TIME [epoch: 9.54 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743001887026362		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 1.743001887026362 | validation: 2.2475867736977047]
	TIME [epoch: 9.54 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7448764212417445		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 1.7448764212417445 | validation: 2.2498196673143855]
	TIME [epoch: 9.56 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745848068529891		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 1.745848068529891 | validation: 2.2589450590016114]
	TIME [epoch: 9.55 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7357316228844883		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 1.7357316228844883 | validation: 2.247021177608986]
	TIME [epoch: 9.56 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7423255057462506		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 1.7423255057462506 | validation: 2.2450531689601614]
	TIME [epoch: 9.57 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7393699992529914		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 1.7393699992529914 | validation: 2.2540580819728575]
	TIME [epoch: 9.55 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748150852071556		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 1.748150852071556 | validation: 2.260265809430042]
	TIME [epoch: 9.55 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7456957532505881		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 1.7456957532505881 | validation: 2.2434033518734076]
	TIME [epoch: 9.55 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7501443389968139		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 1.7501443389968139 | validation: 2.2484710486547237]
	TIME [epoch: 9.56 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446480707474818		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 1.7446480707474818 | validation: 2.2365041231819878]
	TIME [epoch: 9.57 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7380689454043996		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 1.7380689454043996 | validation: 2.2508932321751955]
	TIME [epoch: 9.55 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7433993811187356		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 1.7433993811187356 | validation: 2.252155419729246]
	TIME [epoch: 9.55 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7471713032786584		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 1.7471713032786584 | validation: 2.263314180367503]
	TIME [epoch: 9.57 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7394116529972976		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 1.7394116529972976 | validation: 2.2536336260561676]
	TIME [epoch: 9.55 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.738626090011379		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 1.738626090011379 | validation: 2.2433587051608446]
	TIME [epoch: 9.57 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425410915972581		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 1.7425410915972581 | validation: 2.2578044577693697]
	TIME [epoch: 9.55 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7344853839137442		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 1.7344853839137442 | validation: 2.2520523617809243]
	TIME [epoch: 9.58 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7401882599293326		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 1.7401882599293326 | validation: 2.2390969322067464]
	TIME [epoch: 9.56 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7371799867621078		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 1.7371799867621078 | validation: 2.246017226715829]
	TIME [epoch: 9.55 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7469670518192633		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 1.7469670518192633 | validation: 2.247511222485506]
	TIME [epoch: 9.57 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7488553837152423		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 1.7488553837152423 | validation: 2.233242046519196]
	TIME [epoch: 9.57 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7430041634963398		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 1.7430041634963398 | validation: 2.257060055426919]
	TIME [epoch: 9.55 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7520933514755535		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 1.7520933514755535 | validation: 2.2408388873652303]
	TIME [epoch: 9.54 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7439305342347968		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 1.7439305342347968 | validation: 2.232346182303682]
	TIME [epoch: 9.57 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740842191432876		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 1.740842191432876 | validation: 2.2576644516172943]
	TIME [epoch: 9.55 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743068464723239		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 1.743068464723239 | validation: 2.2586042643470545]
	TIME [epoch: 9.55 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7396762347237864		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 1.7396762347237864 | validation: 2.2461126391289072]
	TIME [epoch: 9.54 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7408950794517826		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 1.7408950794517826 | validation: 2.25094688482452]
	TIME [epoch: 9.57 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446051883937361		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 1.7446051883937361 | validation: 2.250783096868077]
	TIME [epoch: 9.56 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7420044670686856		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 1.7420044670686856 | validation: 2.2674405049594974]
	TIME [epoch: 9.55 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7465945049363036		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 1.7465945049363036 | validation: 2.2645628948472214]
	TIME [epoch: 9.55 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7473980644297566		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 1.7473980644297566 | validation: 2.2398782466555636]
	TIME [epoch: 9.57 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7398206751709844		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 1.7398206751709844 | validation: 2.2420882499122134]
	TIME [epoch: 9.55 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425142608531872		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 1.7425142608531872 | validation: 2.2511318799630393]
	TIME [epoch: 9.55 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7360474026474115		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 1.7360474026474115 | validation: 2.263972150215904]
	TIME [epoch: 9.55 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750934222016707		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 1.750934222016707 | validation: 2.2471899098980876]
	TIME [epoch: 9.55 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429755884972855		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 1.7429755884972855 | validation: 2.241150302970288]
	TIME [epoch: 9.54 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475106498218327		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 1.7475106498218327 | validation: 2.2271373430621133]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r3_20240219_220617/states/model_tr_study205_1794.pth
	Model improved!!!
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7484846156014826		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 1.7484846156014826 | validation: 2.238075910714013]
	TIME [epoch: 9.56 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746192695076921		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 1.746192695076921 | validation: 2.240267490248807]
	TIME [epoch: 9.53 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438130762989132		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 1.7438130762989132 | validation: 2.2462482704253497]
	TIME [epoch: 9.53 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746315669877316		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 1.746315669877316 | validation: 2.245886855224578]
	TIME [epoch: 9.52 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7475503958790806		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 1.7475503958790806 | validation: 2.255659403613144]
	TIME [epoch: 9.55 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7365420043280562		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 1.7365420043280562 | validation: 2.2593016830837063]
	TIME [epoch: 9.53 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440806219376097		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 1.7440806219376097 | validation: 2.2631682657955556]
	TIME [epoch: 9.53 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7409494516598485		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 1.7409494516598485 | validation: 2.2312207003857996]
	TIME [epoch: 9.52 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7456597869480448		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 1.7456597869480448 | validation: 2.2641386337458886]
	TIME [epoch: 9.55 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7448678535314506		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 1.7448678535314506 | validation: 2.2617848579084074]
	TIME [epoch: 9.52 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740580333917587		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 1.740580333917587 | validation: 2.255839246163206]
	TIME [epoch: 9.53 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7452657622411187		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 1.7452657622411187 | validation: 2.2475426513042764]
	TIME [epoch: 9.53 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740966471683413		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 1.740966471683413 | validation: 2.263985043719456]
	TIME [epoch: 9.54 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7383206312423176		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 1.7383206312423176 | validation: 2.2555348942319657]
	TIME [epoch: 9.53 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744976567931372		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 1.744976567931372 | validation: 2.2502870673735993]
	TIME [epoch: 9.52 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7470864925893097		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 1.7470864925893097 | validation: 2.26101268180333]
	TIME [epoch: 9.53 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7383063847310616		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 1.7383063847310616 | validation: 2.244000696015167]
	TIME [epoch: 9.52 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493111562637131		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 1.7493111562637131 | validation: 2.2570687826593927]
	TIME [epoch: 9.52 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442258279557847		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 1.7442258279557847 | validation: 2.2506839077323155]
	TIME [epoch: 9.52 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743609525073851		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 1.743609525073851 | validation: 2.2727502566377145]
	TIME [epoch: 9.53 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7471170977423824		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 1.7471170977423824 | validation: 2.2649566733372306]
	TIME [epoch: 9.52 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443127409910961		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 1.7443127409910961 | validation: 2.26912145603608]
	TIME [epoch: 9.53 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743897041510579		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 1.743897041510579 | validation: 2.248271831763521]
	TIME [epoch: 9.53 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438186803550617		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 1.7438186803550617 | validation: 2.2563056310493663]
	TIME [epoch: 9.54 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7435569804093434		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 1.7435569804093434 | validation: 2.2439492836214208]
	TIME [epoch: 9.54 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459930342762953		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 1.7459930342762953 | validation: 2.2541263959818223]
	TIME [epoch: 9.52 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751582248201546		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 1.751582248201546 | validation: 2.2500642677751252]
	TIME [epoch: 9.53 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7433811802228358		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 1.7433811802228358 | validation: 2.250930080461334]
	TIME [epoch: 9.54 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7404947978572358		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 1.7404947978572358 | validation: 2.2451789002782307]
	TIME [epoch: 9.53 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746462761714561		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 1.746462761714561 | validation: 2.2526690655926855]
	TIME [epoch: 9.53 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7395002994898818		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 1.7395002994898818 | validation: 2.2578945427186268]
	TIME [epoch: 9.54 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429735576831138		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 1.7429735576831138 | validation: 2.2684518148047683]
	TIME [epoch: 9.54 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7469601179702638		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 1.7469601179702638 | validation: 2.2600691570774876]
	TIME [epoch: 9.54 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425704861396771		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 1.7425704861396771 | validation: 2.2640143860408104]
	TIME [epoch: 9.53 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7381662937192055		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 1.7381662937192055 | validation: 2.258873594857097]
	TIME [epoch: 9.55 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7363347097494575		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 1.7363347097494575 | validation: 2.256120247436959]
	TIME [epoch: 9.53 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745858795962602		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 1.745858795962602 | validation: 2.2423891041468984]
	TIME [epoch: 9.54 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7455838240483748		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 1.7455838240483748 | validation: 2.251044914845715]
	TIME [epoch: 9.53 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7406891247427452		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 1.7406891247427452 | validation: 2.2350498019804266]
	TIME [epoch: 9.55 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7459579201550643		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 1.7459579201550643 | validation: 2.246590993103347]
	TIME [epoch: 9.54 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7484155574567732		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 1.7484155574567732 | validation: 2.2620956734103412]
	TIME [epoch: 9.53 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7382233690847961		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 1.7382233690847961 | validation: 2.2382879202615804]
	TIME [epoch: 9.52 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7372984865873817		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 1.7372984865873817 | validation: 2.2548405454207505]
	TIME [epoch: 9.55 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744827661292113		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 1.744827661292113 | validation: 2.260960187600216]
	TIME [epoch: 9.53 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429169048486575		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 1.7429169048486575 | validation: 2.2464964659746274]
	TIME [epoch: 9.53 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7463057582721255		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 1.7463057582721255 | validation: 2.2607793746524067]
	TIME [epoch: 9.53 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7464132395151206		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 1.7464132395151206 | validation: 2.2493236736107107]
	TIME [epoch: 9.54 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7453431065231748		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 1.7453431065231748 | validation: 2.258090153463771]
	TIME [epoch: 9.52 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421491943643979		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 1.7421491943643979 | validation: 2.248069200485704]
	TIME [epoch: 9.53 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7489645036865074		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 1.7489645036865074 | validation: 2.259090222766933]
	TIME [epoch: 9.54 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460847720645838		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 1.7460847720645838 | validation: 2.247046516663883]
	TIME [epoch: 9.53 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746850990423011		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 1.746850990423011 | validation: 2.246900437455853]
	TIME [epoch: 9.52 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750140916515953		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 1.750140916515953 | validation: 2.2472928307595668]
	TIME [epoch: 9.53 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748570977773842		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 1.748570977773842 | validation: 2.2428841033920897]
	TIME [epoch: 9.54 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7433658073609366		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 1.7433658073609366 | validation: 2.263337695479287]
	TIME [epoch: 9.54 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441315719103414		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 1.7441315719103414 | validation: 2.2429016274991826]
	TIME [epoch: 9.52 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7427975704493683		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 1.7427975704493683 | validation: 2.2604914078542175]
	TIME [epoch: 9.53 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7500511172711484		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 1.7500511172711484 | validation: 2.2616397740997183]
	TIME [epoch: 9.53 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740345659305876		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 1.740345659305876 | validation: 2.2506654837158653]
	TIME [epoch: 9.52 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7417936658488187		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 1.7417936658488187 | validation: 2.2541621237588108]
	TIME [epoch: 9.53 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7514692592581131		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 1.7514692592581131 | validation: 2.2671341939566516]
	TIME [epoch: 9.53 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7416519456127137		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 1.7416519456127137 | validation: 2.2652934773768774]
	TIME [epoch: 9.55 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745481179893077		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 1.745481179893077 | validation: 2.2463657999261]
	TIME [epoch: 9.53 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7422096739480424		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 1.7422096739480424 | validation: 2.268738960369607]
	TIME [epoch: 9.53 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74265775105947		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 1.74265775105947 | validation: 2.2504351368357947]
	TIME [epoch: 9.54 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74532094566741		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 1.74532094566741 | validation: 2.2639208627075225]
	TIME [epoch: 9.53 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7470200526702038		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 1.7470200526702038 | validation: 2.23573056327307]
	TIME [epoch: 9.54 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493763861336444		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 1.7493763861336444 | validation: 2.260611515548566]
	TIME [epoch: 9.52 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7479622424730308		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 1.7479622424730308 | validation: 2.2463412018602815]
	TIME [epoch: 9.54 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7390689215799653		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 1.7390689215799653 | validation: 2.241157996114552]
	TIME [epoch: 9.53 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751076605144826		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 1.751076605144826 | validation: 2.252349633332371]
	TIME [epoch: 9.52 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7511667857989637		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 1.7511667857989637 | validation: 2.2434763208469297]
	TIME [epoch: 9.53 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7401701968582362		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 1.7401701968582362 | validation: 2.2530598856186272]
	TIME [epoch: 9.54 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7401403282868837		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 1.7401403282868837 | validation: 2.23599638091744]
	TIME [epoch: 9.53 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7462995145421818		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 1.7462995145421818 | validation: 2.2328311739530484]
	TIME [epoch: 9.52 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7416027855993892		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 1.7416027855993892 | validation: 2.2550394223034385]
	TIME [epoch: 9.53 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743514634223659		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 1.743514634223659 | validation: 2.2480984527483607]
	TIME [epoch: 9.54 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7358426083998608		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 1.7358426083998608 | validation: 2.236246129783296]
	TIME [epoch: 9.54 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7447758626228975		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 1.7447758626228975 | validation: 2.257934633141906]
	TIME [epoch: 9.52 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739276550963763		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 1.739276550963763 | validation: 2.246810944495515]
	TIME [epoch: 9.54 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7412091598423705		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 1.7412091598423705 | validation: 2.245973524929443]
	TIME [epoch: 9.54 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7450866295336744		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 1.7450866295336744 | validation: 2.2536830112448594]
	TIME [epoch: 9.53 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7399436838721414		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 1.7399436838721414 | validation: 2.266803081114607]
	TIME [epoch: 9.53 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7402551507588544		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 1.7402551507588544 | validation: 2.253436858842913]
	TIME [epoch: 9.54 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7439794933090298		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 1.7439794933090298 | validation: 2.257815364837538]
	TIME [epoch: 9.53 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740082998136423		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 1.740082998136423 | validation: 2.2577588845194954]
	TIME [epoch: 9.52 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744128576830193		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 1.744128576830193 | validation: 2.241757027489106]
	TIME [epoch: 9.53 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7415309516564534		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 1.7415309516564534 | validation: 2.2452996870830364]
	TIME [epoch: 9.55 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7415209964725176		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 1.7415209964725176 | validation: 2.2453901905679583]
	TIME [epoch: 9.53 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740450815600004		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 1.740450815600004 | validation: 2.2449046219903273]
	TIME [epoch: 9.53 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421202846221937		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 1.7421202846221937 | validation: 2.2471020075139463]
	TIME [epoch: 9.52 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7496196623577123		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 1.7496196623577123 | validation: 2.2533560740957186]
	TIME [epoch: 9.55 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7363681741623367		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 1.7363681741623367 | validation: 2.265341128848195]
	TIME [epoch: 9.53 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437980244128546		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 1.7437980244128546 | validation: 2.2672072389024773]
	TIME [epoch: 9.54 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425096722438933		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 1.7425096722438933 | validation: 2.2442315414044676]
	TIME [epoch: 9.53 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7490221513826403		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 1.7490221513826403 | validation: 2.2404585517862556]
	TIME [epoch: 9.55 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7481434744178663		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 1.7481434744178663 | validation: 2.2430120274266723]
	TIME [epoch: 9.54 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7411949475756188		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 1.7411949475756188 | validation: 2.2416587624021496]
	TIME [epoch: 9.54 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7480116967453008		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 1.7480116967453008 | validation: 2.242357470391993]
	TIME [epoch: 9.54 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7410156192017898		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 1.7410156192017898 | validation: 2.2558675461483673]
	TIME [epoch: 9.54 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7497487972096522		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 1.7497487972096522 | validation: 2.258653892362698]
	TIME [epoch: 9.54 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7455699368463347		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 1.7455699368463347 | validation: 2.247647432501948]
	TIME [epoch: 9.56 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441849407621182		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 1.7441849407621182 | validation: 2.250038526197708]
	TIME [epoch: 9.56 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.73956881368994		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 1.73956881368994 | validation: 2.2517332906006176]
	TIME [epoch: 9.55 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476962339470539		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 1.7476962339470539 | validation: 2.2537323510161578]
	TIME [epoch: 9.56 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7492048519552956		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 1.7492048519552956 | validation: 2.2522256817493993]
	TIME [epoch: 9.54 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7412739582793197		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 1.7412739582793197 | validation: 2.25618601277959]
	TIME [epoch: 9.56 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461413164171566		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 1.7461413164171566 | validation: 2.2433460963677083]
	TIME [epoch: 9.56 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460131471780251		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 1.7460131471780251 | validation: 2.2601549350764962]
	TIME [epoch: 9.56 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7448527542623828		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 1.7448527542623828 | validation: 2.249408246158742]
	TIME [epoch: 9.53 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7500165551613533		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 1.7500165551613533 | validation: 2.249630364406505]
	TIME [epoch: 9.57 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7374387615912918		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 1.7374387615912918 | validation: 2.2518429078315414]
	TIME [epoch: 9.53 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743295368450692		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 1.743295368450692 | validation: 2.2400210567585437]
	TIME [epoch: 9.55 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476131165417321		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 1.7476131165417321 | validation: 2.2795067096653736]
	TIME [epoch: 9.55 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7473976324377216		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 1.7473976324377216 | validation: 2.245754946610116]
	TIME [epoch: 9.56 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.739578965028824		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 1.739578965028824 | validation: 2.2662423430288756]
	TIME [epoch: 9.54 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7490004179337795		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 1.7490004179337795 | validation: 2.261567943797428]
	TIME [epoch: 9.55 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7425040014956223		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 1.7425040014956223 | validation: 2.2605985851022328]
	TIME [epoch: 9.55 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7440404028912124		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 1.7440404028912124 | validation: 2.259873805691051]
	TIME [epoch: 9.55 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7361022310346244		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 1.7361022310346244 | validation: 2.2675371472912604]
	TIME [epoch: 9.55 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7457832407911038		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 1.7457832407911038 | validation: 2.262324683749703]
	TIME [epoch: 9.54 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.746117600971175		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 1.746117600971175 | validation: 2.2574377176540152]
	TIME [epoch: 9.55 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743540043328481		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 1.743540043328481 | validation: 2.262430290054544]
	TIME [epoch: 9.56 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7493571629646945		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 1.7493571629646945 | validation: 2.2538524288117285]
	TIME [epoch: 9.55 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750019678126605		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 1.750019678126605 | validation: 2.254104412560501]
	TIME [epoch: 9.55 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7402111244400202		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 1.7402111244400202 | validation: 2.2657150618370725]
	TIME [epoch: 9.58 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7444480374371572		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 1.7444480374371572 | validation: 2.2578613386565705]
	TIME [epoch: 9.55 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460472005668344		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 1.7460472005668344 | validation: 2.2509642793643283]
	TIME [epoch: 9.55 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7399647652918382		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 1.7399647652918382 | validation: 2.2546460966320043]
	TIME [epoch: 9.54 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7401215688983107		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 1.7401215688983107 | validation: 2.2660419917490975]
	TIME [epoch: 9.56 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460790193880318		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 1.7460790193880318 | validation: 2.2532825615437817]
	TIME [epoch: 9.55 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7483223274523987		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 1.7483223274523987 | validation: 2.2629827505742317]
	TIME [epoch: 9.54 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7454340099255936		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 1.7454340099255936 | validation: 2.268193866802517]
	TIME [epoch: 9.55 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442789805777685		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 1.7442789805777685 | validation: 2.249353221411162]
	TIME [epoch: 9.55 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7403534293811238		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 1.7403534293811238 | validation: 2.2440474560544437]
	TIME [epoch: 9.56 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442676982322187		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 1.7442676982322187 | validation: 2.25549072328104]
	TIME [epoch: 9.54 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7402403838794593		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 1.7402403838794593 | validation: 2.2538138265925944]
	TIME [epoch: 9.56 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7402249232043778		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 1.7402249232043778 | validation: 2.2576986221644524]
	TIME [epoch: 9.54 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7445316952508498		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 1.7445316952508498 | validation: 2.2355397802879815]
	TIME [epoch: 9.55 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7432313617723678		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 1.7432313617723678 | validation: 2.2554842272149074]
	TIME [epoch: 9.54 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.738541971881567		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 1.738541971881567 | validation: 2.2500187239781573]
	TIME [epoch: 9.57 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7446101505471319		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 1.7446101505471319 | validation: 2.249584869630819]
	TIME [epoch: 9.55 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421429928253562		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 1.7421429928253562 | validation: 2.262386969357215]
	TIME [epoch: 9.56 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7480646100287707		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 1.7480646100287707 | validation: 2.241001561388685]
	TIME [epoch: 9.56 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.741111829811263		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 1.741111829811263 | validation: 2.248430357460669]
	TIME [epoch: 9.57 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7417456199116366		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 1.7417456199116366 | validation: 2.2497191034132804]
	TIME [epoch: 9.56 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7399338652085308		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 1.7399338652085308 | validation: 2.2462817120057945]
	TIME [epoch: 9.56 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7461093789059423		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 1.7461093789059423 | validation: 2.2512200451577247]
	TIME [epoch: 9.56 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7430522658333794		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 1.7430522658333794 | validation: 2.2480651923526502]
	TIME [epoch: 9.55 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437064138032277		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 1.7437064138032277 | validation: 2.2447904692053395]
	TIME [epoch: 9.56 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442903516072357		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 1.7442903516072357 | validation: 2.2600645563051542]
	TIME [epoch: 9.57 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.741240977584543		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 1.741240977584543 | validation: 2.2521257721306154]
	TIME [epoch: 9.57 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7483465095095039		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 1.7483465095095039 | validation: 2.257342703474957]
	TIME [epoch: 9.56 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.741290716080675		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 1.741290716080675 | validation: 2.274217851303309]
	TIME [epoch: 9.55 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7429250620018581		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 1.7429250620018581 | validation: 2.259771635958042]
	TIME [epoch: 9.56 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745344101400206		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 1.745344101400206 | validation: 2.252916612189427]
	TIME [epoch: 9.56 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7472093098606603		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 1.7472093098606603 | validation: 2.2520244089926664]
	TIME [epoch: 9.57 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74517715054183		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 1.74517715054183 | validation: 2.261885234427187]
	TIME [epoch: 9.55 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441768542637976		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 1.7441768542637976 | validation: 2.2587790543769253]
	TIME [epoch: 9.55 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7410981993403063		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 1.7410981993403063 | validation: 2.2478234527868226]
	TIME [epoch: 9.57 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7417804989563006		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 1.7417804989563006 | validation: 2.253072179519851]
	TIME [epoch: 9.55 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7447560838675311		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 1.7447560838675311 | validation: 2.2568916293192087]
	TIME [epoch: 9.54 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7457257698176563		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 1.7457257698176563 | validation: 2.251627525708291]
	TIME [epoch: 9.57 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7437561688366094		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 1.7437561688366094 | validation: 2.256184544377739]
	TIME [epoch: 9.56 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7447890715261842		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 1.7447890715261842 | validation: 2.2525108478466063]
	TIME [epoch: 9.55 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743709215726168		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 1.743709215726168 | validation: 2.2480861490291337]
	TIME [epoch: 9.55 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7466758361814982		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 1.7466758361814982 | validation: 2.257937891716927]
	TIME [epoch: 9.57 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7431563185224086		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 1.7431563185224086 | validation: 2.2444349150589704]
	TIME [epoch: 9.55 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7432142146426404		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 1.7432142146426404 | validation: 2.2404132321806056]
	TIME [epoch: 9.55 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7457595711825284		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 1.7457595711825284 | validation: 2.250572094277322]
	TIME [epoch: 9.55 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7460005027102354		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 1.7460005027102354 | validation: 2.2660411610386997]
	TIME [epoch: 9.58 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7414518858338788		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 1.7414518858338788 | validation: 2.258579409253408]
	TIME [epoch: 9.55 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7413209288656262		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 1.7413209288656262 | validation: 2.2583519508391747]
	TIME [epoch: 9.56 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7455003443511727		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 1.7455003443511727 | validation: 2.2535468191518215]
	TIME [epoch: 9.57 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7413978595944783		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 1.7413978595944783 | validation: 2.261708747012767]
	TIME [epoch: 9.58 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7390052659721171		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 1.7390052659721171 | validation: 2.2500544150191604]
	TIME [epoch: 9.56 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744348191153629		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 1.744348191153629 | validation: 2.2379591399342673]
	TIME [epoch: 9.55 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441822960466904		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 1.7441822960466904 | validation: 2.2433252132279655]
	TIME [epoch: 9.55 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443312928163444		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 1.7443312928163444 | validation: 2.256902301685932]
	TIME [epoch: 9.57 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7444786665844318		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 1.7444786665844318 | validation: 2.2630937562673514]
	TIME [epoch: 9.55 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7479752278191065		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 1.7479752278191065 | validation: 2.258763711257108]
	TIME [epoch: 9.55 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7478335849796616		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 1.7478335849796616 | validation: 2.2545270795395305]
	TIME [epoch: 9.57 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7433541090353568		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 1.7433541090353568 | validation: 2.253590980576888]
	TIME [epoch: 9.57 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.748507914371369		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 1.748507914371369 | validation: 2.2750293680472304]
	TIME [epoch: 9.55 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.751602688152746		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 1.751602688152746 | validation: 2.260275775577336]
	TIME [epoch: 9.56 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7411106892613368		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 1.7411106892613368 | validation: 2.253431210594188]
	TIME [epoch: 9.57 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7443114347740418		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 1.7443114347740418 | validation: 2.2618407456441627]
	TIME [epoch: 9.56 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7450500037082324		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 1.7450500037082324 | validation: 2.25039531820572]
	TIME [epoch: 9.56 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.745867037171383		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 1.745867037171383 | validation: 2.2558106437680854]
	TIME [epoch: 9.56 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7438385342591036		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 1.7438385342591036 | validation: 2.246423289583739]
	TIME [epoch: 9.57 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7479321188115633		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 1.7479321188115633 | validation: 2.248525670475258]
	TIME [epoch: 9.56 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7393610409941895		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 1.7393610409941895 | validation: 2.2510903847770902]
	TIME [epoch: 9.56 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7474047880742845		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 1.7474047880742845 | validation: 2.247599476832659]
	TIME [epoch: 9.57 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7441022419285068		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 1.7441022419285068 | validation: 2.257578354501571]
	TIME [epoch: 9.58 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.735634967384127		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 1.735634967384127 | validation: 2.271391142839748]
	TIME [epoch: 9.57 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7419596533198132		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 1.7419596533198132 | validation: 2.2648744282559146]
	TIME [epoch: 9.56 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.742961278574694		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 1.742961278574694 | validation: 2.255984803755341]
	TIME [epoch: 9.58 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7361825431274354		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 1.7361825431274354 | validation: 2.2393630549010393]
	TIME [epoch: 9.57 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7423307401038919		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 1.7423307401038919 | validation: 2.2466515625935655]
	TIME [epoch: 9.56 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7417733997980016		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 1.7417733997980016 | validation: 2.2436525570883528]
	TIME [epoch: 9.56 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.74815352480422		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 1.74815352480422 | validation: 2.2539996331381165]
	TIME [epoch: 9.56 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.738851708469379		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 1.738851708469379 | validation: 2.2568373207268504]
	TIME [epoch: 9.55 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7411091163348371		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 1.7411091163348371 | validation: 2.2440136903724937]
	TIME [epoch: 9.56 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476581851430768		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 1.7476581851430768 | validation: 2.2610621396256385]
	TIME [epoch: 9.55 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7400201091758731		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 1.7400201091758731 | validation: 2.245644408830016]
	TIME [epoch: 9.56 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.744441738268869		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 1.744441738268869 | validation: 2.2472814478375995]
	TIME [epoch: 9.57 sec]
Finished training in 19258.035 seconds.
