Args:
Namespace(name='model_tr_study205', outdir='out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2', training_data='data/transition_rate_studies/tr_study205/tr_study205_training/r2', validation_data='data/transition_rate_studies/tr_study205/tr_study205_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3271711064

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.203532239765487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.203532239765487 | validation: 12.003229057800066]
	TIME [epoch: 53.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.980195445736644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.980195445736644 | validation: 10.35947822553786]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.828194321545428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.828194321545428 | validation: 9.242623218691946]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.473640257593479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.473640257593479 | validation: 8.507918270598628]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.206605593189508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.206605593189508 | validation: 8.109302730575536]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.86904871691002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.86904871691002 | validation: 6.598208810639633]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.245514500920294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.245514500920294 | validation: 5.692566919058695]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.855556270107556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.855556270107556 | validation: 5.957841176828778]
	TIME [epoch: 9.51 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.5883672643313345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5883672643313345 | validation: 5.591853283217943]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.434947630358751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.434947630358751 | validation: 5.689251636067313]
	TIME [epoch: 9.51 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.133209579322106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.133209579322106 | validation: 5.198480646866858]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.827660938652372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.827660938652372 | validation: 5.009778098128855]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885417307722662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.885417307722662 | validation: 4.827951957049904]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.828292793780828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.828292793780828 | validation: 5.83432158584472]
	TIME [epoch: 9.52 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.846351189746965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.846351189746965 | validation: 5.098672502544814]
	TIME [epoch: 9.51 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5072418420592175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5072418420592175 | validation: 5.0252159461375765]
	TIME [epoch: 9.51 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.312738692812083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.312738692812083 | validation: 4.477465475019652]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.339498253852719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.339498253852719 | validation: 4.223977429507923]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0815323393366025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0815323393366025 | validation: 4.211559763350823]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.944008159384137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.944008159384137 | validation: 4.639898031717134]
	TIME [epoch: 9.53 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8733822059324448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8733822059324448 | validation: 4.726197033375159]
	TIME [epoch: 9.5 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.020713458001373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.020713458001373 | validation: 3.9267370051670345]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.672050704231533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.672050704231533 | validation: 3.7364942450979006]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7397021410088342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7397021410088342 | validation: 4.070708311113596]
	TIME [epoch: 9.53 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6798557710163307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6798557710163307 | validation: 3.738180662627996]
	TIME [epoch: 9.52 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.683569589119461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.683569589119461 | validation: 4.13741296187385]
	TIME [epoch: 9.51 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.741565199494153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.741565199494153 | validation: 4.078027469395632]
	TIME [epoch: 9.52 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49446010040601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.49446010040601 | validation: 4.169441002414798]
	TIME [epoch: 9.53 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6075997054654834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6075997054654834 | validation: 3.7312976706197283]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4097323486870104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4097323486870104 | validation: 4.0319651956426625]
	TIME [epoch: 9.51 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.481950220874519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.481950220874519 | validation: 4.073818382144537]
	TIME [epoch: 9.51 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3978147093906315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3978147093906315 | validation: 3.701674236416567]
	TIME [epoch: 9.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.434708716946589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.434708716946589 | validation: 3.8633992512124484]
	TIME [epoch: 9.52 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.414649715913318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.414649715913318 | validation: 3.845735817908937]
	TIME [epoch: 9.52 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3794053199262053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3794053199262053 | validation: 3.952895729420445]
	TIME [epoch: 9.51 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3651829863676026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3651829863676026 | validation: 4.283314516143078]
	TIME [epoch: 9.53 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.426600491495943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.426600491495943 | validation: 3.7819041523252825]
	TIME [epoch: 9.52 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3785320427525534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3785320427525534 | validation: 3.6545492409697125]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4123188442814483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4123188442814483 | validation: 3.447974998019995]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.319707877944982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.319707877944982 | validation: 3.542432378899231]
	TIME [epoch: 9.52 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.285880053323851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.285880053323851 | validation: 3.6098992436919684]
	TIME [epoch: 9.51 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2909929263429105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2909929263429105 | validation: 3.8050418948471907]
	TIME [epoch: 9.5 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.34379431742518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.34379431742518 | validation: 3.8747961460715428]
	TIME [epoch: 9.51 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.305288592864456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.305288592864456 | validation: 3.757456787195971]
	TIME [epoch: 9.53 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.29188984129962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.29188984129962 | validation: 3.670672719889739]
	TIME [epoch: 9.51 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3814836819478855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3814836819478855 | validation: 3.5547927173352467]
	TIME [epoch: 9.5 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3159469364234324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3159469364234324 | validation: 3.775206538514414]
	TIME [epoch: 9.5 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.271755562051079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.271755562051079 | validation: 3.6177026033830195]
	TIME [epoch: 9.53 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3143360273317057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3143360273317057 | validation: 3.3980519835131036]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2436614445796863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2436614445796863 | validation: 4.357574032230183]
	TIME [epoch: 9.5 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3718074915120892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3718074915120892 | validation: 4.405303109186227]
	TIME [epoch: 9.5 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.398336451716914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.398336451716914 | validation: 3.746031515450987]
	TIME [epoch: 9.52 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.30704822021766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.30704822021766 | validation: 3.774142793803801]
	TIME [epoch: 9.5 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.359212489176707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359212489176707 | validation: 3.379032597715163]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3341744443286694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3341744443286694 | validation: 4.824531386221806]
	TIME [epoch: 9.49 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.45477336062942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.45477336062942 | validation: 3.743267573140173]
	TIME [epoch: 9.52 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.263326305586945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.263326305586945 | validation: 5.247582736386335]
	TIME [epoch: 9.5 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9084245357642535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9084245357642535 | validation: 3.7524691607568825]
	TIME [epoch: 9.51 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2287421021259077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2287421021259077 | validation: 3.7255830153121425]
	TIME [epoch: 9.5 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2904020598593027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2904020598593027 | validation: 3.817136930492062]
	TIME [epoch: 9.52 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2715486230337896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2715486230337896 | validation: 4.423639717121477]
	TIME [epoch: 9.5 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.224474724517388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.224474724517388 | validation: 3.9192665386980656]
	TIME [epoch: 9.5 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.32531906102185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.32531906102185 | validation: 3.448644087503282]
	TIME [epoch: 9.49 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.296070940887521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.296070940887521 | validation: 4.318523119409641]
	TIME [epoch: 9.52 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4883853027683545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4883853027683545 | validation: 3.2713339046479644]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3239649974977943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3239649974977943 | validation: 4.176379860791121]
	TIME [epoch: 9.5 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3114853961215074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3114853961215074 | validation: 3.5083413653138735]
	TIME [epoch: 9.49 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3874828252409754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3874828252409754 | validation: 3.664755023089032]
	TIME [epoch: 9.52 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.283875296349539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.283875296349539 | validation: 3.534938937808598]
	TIME [epoch: 9.5 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1524274963117724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1524274963117724 | validation: 4.740686633510394]
	TIME [epoch: 9.5 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3730120826232395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3730120826232395 | validation: 3.9430828343625315]
	TIME [epoch: 9.5 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3094122663241166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3094122663241166 | validation: 3.166226183374039]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.261817686596069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.261817686596069 | validation: 5.7632637709162235]
	TIME [epoch: 9.5 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8201691609427924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8201691609427924 | validation: 3.2901532218435925]
	TIME [epoch: 9.5 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.13529276419875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.13529276419875 | validation: 3.553923784166286]
	TIME [epoch: 9.49 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.108565211478902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.108565211478902 | validation: 3.49988831554342]
	TIME [epoch: 9.53 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1683431767747328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1683431767747328 | validation: 3.5201254867425975]
	TIME [epoch: 9.5 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1434233675293015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1434233675293015 | validation: 3.350264554211279]
	TIME [epoch: 9.5 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.135618764918659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.135618764918659 | validation: 3.6294038857795]
	TIME [epoch: 9.5 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.180776160416804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.180776160416804 | validation: 3.568203840993979]
	TIME [epoch: 9.52 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5362857807820065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5362857807820065 | validation: 4.0640578685479625]
	TIME [epoch: 9.49 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5523447298688553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5523447298688553 | validation: 3.5631199089838077]
	TIME [epoch: 9.48 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.229085794894388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.229085794894388 | validation: 3.615336370167361]
	TIME [epoch: 9.5 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.199477460711187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.199477460711187 | validation: 3.571763877155368]
	TIME [epoch: 9.51 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.201614330748034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.201614330748034 | validation: 3.421566095560712]
	TIME [epoch: 9.5 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.129069026847342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.129069026847342 | validation: 3.4805562185688372]
	TIME [epoch: 9.5 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1417301891620815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1417301891620815 | validation: 3.6608177174012395]
	TIME [epoch: 9.5 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1874600784811085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1874600784811085 | validation: 3.4308819064074534]
	TIME [epoch: 9.52 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0706890674802203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0706890674802203 | validation: 3.7900438371584064]
	TIME [epoch: 9.5 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.159894477886729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.159894477886729 | validation: 4.156787016662846]
	TIME [epoch: 9.49 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2725516400045045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2725516400045045 | validation: 3.644714903721079]
	TIME [epoch: 9.49 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1503748414794073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1503748414794073 | validation: 3.4988700686891243]
	TIME [epoch: 9.51 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.163431850522781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.163431850522781 | validation: 3.270452933064455]
	TIME [epoch: 9.5 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1115265197720943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1115265197720943 | validation: 3.212485832948663]
	TIME [epoch: 9.5 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.17530145920455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.17530145920455 | validation: 3.4117329649066654]
	TIME [epoch: 9.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1349902919896695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1349902919896695 | validation: 3.5805338462659937]
	TIME [epoch: 9.52 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2024021256673096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2024021256673096 | validation: 3.558748844100189]
	TIME [epoch: 9.5 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.26084856296863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.26084856296863 | validation: 3.6441617881999178]
	TIME [epoch: 9.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.298550855846571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.298550855846571 | validation: 3.2427251050888755]
	TIME [epoch: 9.49 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3967661410672667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3967661410672667 | validation: 3.8741937293328057]
	TIME [epoch: 9.52 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1936113967038757		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 3.1936113967038757 | validation: 3.5077922849398293]
	TIME [epoch: 9.5 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.103984340270918		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 3.103984340270918 | validation: 3.6134751001355743]
	TIME [epoch: 9.49 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1723827732080068		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 3.1723827732080068 | validation: 3.368907351882337]
	TIME [epoch: 9.5 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1192308301872553		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.1192308301872553 | validation: 3.5057901718060243]
	TIME [epoch: 9.51 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0850955014751245		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 3.0850955014751245 | validation: 3.802761923167684]
	TIME [epoch: 9.51 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0812977648146345		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 3.0812977648146345 | validation: 4.0991876277633095]
	TIME [epoch: 9.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1279283704094585		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.1279283704094585 | validation: 3.5316669956404327]
	TIME [epoch: 9.5 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1479471735197735		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.1479471735197735 | validation: 3.461859775806936]
	TIME [epoch: 9.52 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0731363966131773		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.0731363966131773 | validation: 3.4990956345368267]
	TIME [epoch: 9.5 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1062863837314296		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.1062863837314296 | validation: 3.837103149430151]
	TIME [epoch: 9.48 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3549007527635775		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.3549007527635775 | validation: 3.2670104807882185]
	TIME [epoch: 9.49 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2732344662952477		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.2732344662952477 | validation: 3.7351737853986497]
	TIME [epoch: 9.51 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0333351857527235		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.0333351857527235 | validation: 4.153750042306103]
	TIME [epoch: 9.5 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.133334434820993		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.133334434820993 | validation: 3.5225979215690075]
	TIME [epoch: 9.5 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.178867697815766		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.178867697815766 | validation: 3.4577831566089188]
	TIME [epoch: 9.5 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1434972340203275		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.1434972340203275 | validation: 4.922110840120905]
	TIME [epoch: 9.52 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.387070260676926		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.387070260676926 | validation: 3.4907800854394]
	TIME [epoch: 9.49 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0085469226024246		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.0085469226024246 | validation: 3.5687383233837373]
	TIME [epoch: 9.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3920571247469176		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.3920571247469176 | validation: 3.9231437873261785]
	TIME [epoch: 9.51 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1239879561536883		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.1239879561536883 | validation: 3.4904454267378906]
	TIME [epoch: 9.53 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.941262739146734		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 2.941262739146734 | validation: 3.440342878553561]
	TIME [epoch: 9.5 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990056524333661		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 2.990056524333661 | validation: 3.5315694885934]
	TIME [epoch: 9.49 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0083936741965633		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 3.0083936741965633 | validation: 3.554280523953067]
	TIME [epoch: 9.49 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0007489464683914		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 3.0007489464683914 | validation: 3.357094571336085]
	TIME [epoch: 9.51 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9229973317895506		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 2.9229973317895506 | validation: 3.385563716331635]
	TIME [epoch: 9.5 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495538017185052		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 2.9495538017185052 | validation: 3.6945742390454313]
	TIME [epoch: 9.49 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.021443272071095		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.021443272071095 | validation: 3.244547819046518]
	TIME [epoch: 9.49 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9472070723647996		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 2.9472070723647996 | validation: 3.2414024839031415]
	TIME [epoch: 9.51 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.840641345830777		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 2.840641345830777 | validation: 3.331945006589977]
	TIME [epoch: 9.5 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9589855504741083		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 2.9589855504741083 | validation: 3.3161154238754604]
	TIME [epoch: 9.49 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.915829144472785		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 2.915829144472785 | validation: 3.1467600759462258]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.876069471265327		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 2.876069471265327 | validation: 3.377754996717679]
	TIME [epoch: 9.53 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8995143937726837		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 2.8995143937726837 | validation: 3.146144469545017]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9002694400076003		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 2.9002694400076003 | validation: 3.2762685327112337]
	TIME [epoch: 9.51 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.848297422236931		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 2.848297422236931 | validation: 3.3308649060004054]
	TIME [epoch: 9.52 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9630950888376923		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 2.9630950888376923 | validation: 3.1827360481185667]
	TIME [epoch: 9.54 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9368496033965465		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 2.9368496033965465 | validation: 3.2488973254942954]
	TIME [epoch: 9.52 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.840997670434328		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 2.840997670434328 | validation: 3.255024019278282]
	TIME [epoch: 9.52 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7853088565091024		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 2.7853088565091024 | validation: 4.075805457331856]
	TIME [epoch: 9.51 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9124124838904075		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.9124124838904075 | validation: 3.4209545411167506]
	TIME [epoch: 9.54 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.613375690876976		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.613375690876976 | validation: 4.711525992026896]
	TIME [epoch: 9.52 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1313635095099457		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.1313635095099457 | validation: 4.397526264017144]
	TIME [epoch: 9.51 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0649920521405023		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.0649920521405023 | validation: 3.1392151692054804]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.894811957989345		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 2.894811957989345 | validation: 3.2336815739247466]
	TIME [epoch: 9.53 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.876803719045096		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 2.876803719045096 | validation: 3.671732850044092]
	TIME [epoch: 9.51 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1260220203206854		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.1260220203206854 | validation: 3.0055094367493123]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.869865390966106		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 2.869865390966106 | validation: 4.870636730023502]
	TIME [epoch: 9.51 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4187001580817737		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.4187001580817737 | validation: 3.153507136954062]
	TIME [epoch: 9.53 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9484756060927793		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 2.9484756060927793 | validation: 3.081736903180264]
	TIME [epoch: 9.51 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8228565388604596		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 2.8228565388604596 | validation: 3.4741225141566985]
	TIME [epoch: 9.51 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.786650002978736		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 2.786650002978736 | validation: 4.3012648894928445]
	TIME [epoch: 9.51 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.11083273495169		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.11083273495169 | validation: 3.035159453474947]
	TIME [epoch: 9.54 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0219515228605607		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.0219515228605607 | validation: 3.3322631185775107]
	TIME [epoch: 9.51 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.778948555192972		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 2.778948555192972 | validation: 3.7721021137769175]
	TIME [epoch: 9.52 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.850250804091198		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 2.850250804091198 | validation: 3.430940710784757]
	TIME [epoch: 9.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.759392135243214		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 2.759392135243214 | validation: 3.9946488130972626]
	TIME [epoch: 9.53 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034734512452125		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.034734512452125 | validation: 2.957485992484377]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.828648934192947		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 2.828648934192947 | validation: 3.386122271964444]
	TIME [epoch: 9.51 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.061391058326808		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 3.061391058326808 | validation: 3.045972115240953]
	TIME [epoch: 9.5 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8185418089528227		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 2.8185418089528227 | validation: 3.392019655168714]
	TIME [epoch: 9.53 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.782872685153492		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 2.782872685153492 | validation: 3.3531924235251753]
	TIME [epoch: 9.5 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9364889048772786		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 2.9364889048772786 | validation: 2.9401994004176393]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1395201175407936		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.1395201175407936 | validation: 2.9803724883087233]
	TIME [epoch: 9.51 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.790720528330797		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 2.790720528330797 | validation: 3.0531738908321597]
	TIME [epoch: 9.53 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7768895239769975		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 2.7768895239769975 | validation: 3.581873390981854]
	TIME [epoch: 9.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8668301975277615		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 2.8668301975277615 | validation: 4.114735957612802]
	TIME [epoch: 9.5 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0325018140538527		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.0325018140538527 | validation: 3.2198432304202207]
	TIME [epoch: 9.49 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7718067731038643		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 2.7718067731038643 | validation: 3.1235037329829276]
	TIME [epoch: 9.53 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.650547737662852		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 2.650547737662852 | validation: 3.52903728491458]
	TIME [epoch: 9.51 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9011849251639523		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 2.9011849251639523 | validation: 2.992904505137267]
	TIME [epoch: 9.51 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8019870996212264		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 2.8019870996212264 | validation: 3.2975723090288205]
	TIME [epoch: 9.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.732991734845624		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 2.732991734845624 | validation: 2.9707695927735074]
	TIME [epoch: 9.52 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6677604843884057		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 2.6677604843884057 | validation: 3.3447822946008046]
	TIME [epoch: 9.49 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.656857643473635		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 2.656857643473635 | validation: 3.0918751145245826]
	TIME [epoch: 9.5 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.846565652008443		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 2.846565652008443 | validation: 3.947722571096368]
	TIME [epoch: 9.5 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9152282443171083		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 2.9152282443171083 | validation: 3.0408643939978264]
	TIME [epoch: 9.53 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.713456785631905		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 2.713456785631905 | validation: 3.1080505252038986]
	TIME [epoch: 9.5 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5953250415957063		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 2.5953250415957063 | validation: 3.595635875128643]
	TIME [epoch: 9.5 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7772443656059584		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 2.7772443656059584 | validation: 3.1278292431311083]
	TIME [epoch: 9.49 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.68056768387883		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 2.68056768387883 | validation: 3.0379645359678715]
	TIME [epoch: 9.53 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.66018670644866		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 2.66018670644866 | validation: 3.0013731143933042]
	TIME [epoch: 9.5 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6213071666699292		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 2.6213071666699292 | validation: 3.3699208733295087]
	TIME [epoch: 9.5 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.657191761719203		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 2.657191761719203 | validation: 3.441893032534872]
	TIME [epoch: 9.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6890808503112447		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 2.6890808503112447 | validation: 3.2535152553543822]
	TIME [epoch: 9.52 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6737563353354643		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 2.6737563353354643 | validation: 2.966249677072568]
	TIME [epoch: 9.5 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6211038367039543		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 2.6211038367039543 | validation: 3.1344372306066894]
	TIME [epoch: 9.5 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.587728764301088		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 2.587728764301088 | validation: 3.2283062227389205]
	TIME [epoch: 9.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.636305315121883		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 2.636305315121883 | validation: 3.3397794643454852]
	TIME [epoch: 9.53 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6673013087550848		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 2.6673013087550848 | validation: 3.365533365511063]
	TIME [epoch: 9.5 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.601926134378339		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 2.601926134378339 | validation: 3.1305307006883827]
	TIME [epoch: 9.49 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.833354670459021		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 2.833354670459021 | validation: 2.8032149658878485]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6607593839104835		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 2.6607593839104835 | validation: 3.1195186382696263]
	TIME [epoch: 9.51 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.701813859751273		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 2.701813859751273 | validation: 3.0583807221376165]
	TIME [epoch: 9.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.665946365782488		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 2.665946365782488 | validation: 3.152184697075587]
	TIME [epoch: 9.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8640753034564446		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 2.8640753034564446 | validation: 2.889662580674786]
	TIME [epoch: 9.5 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5885903632818907		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 2.5885903632818907 | validation: 3.058165614062963]
	TIME [epoch: 9.52 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5967257057538133		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 2.5967257057538133 | validation: 3.591926253585111]
	TIME [epoch: 9.5 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.790531657422076		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 2.790531657422076 | validation: 2.859932938917981]
	TIME [epoch: 9.49 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.673108817952046		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 2.673108817952046 | validation: 3.0787781329031136]
	TIME [epoch: 9.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5765320250594366		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 2.5765320250594366 | validation: 2.9248258663673896]
	TIME [epoch: 9.52 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5723177280105602		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 2.5723177280105602 | validation: 3.119978989911126]
	TIME [epoch: 9.49 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5394434707417513		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 2.5394434707417513 | validation: 2.84955919266625]
	TIME [epoch: 9.49 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7511151561461755		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 2.7511151561461755 | validation: 3.336006742772984]
	TIME [epoch: 9.49 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7563245135036896		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 2.7563245135036896 | validation: 3.327756562193043]
	TIME [epoch: 9.52 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6891549617443777		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 2.6891549617443777 | validation: 2.800410073684592]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.515822577260291		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 2.515822577260291 | validation: 3.1782206224803202]
	TIME [epoch: 9.5 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6000067474989046		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 2.6000067474989046 | validation: 3.0450530479553413]
	TIME [epoch: 9.49 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6281934953396475		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 2.6281934953396475 | validation: 2.9037533102731636]
	TIME [epoch: 9.52 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5255058133218005		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 2.5255058133218005 | validation: 3.184286609038336]
	TIME [epoch: 9.49 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6018530253344037		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 2.6018530253344037 | validation: 2.9201523522291057]
	TIME [epoch: 9.49 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6281695618164256		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 2.6281695618164256 | validation: 2.854950716819609]
	TIME [epoch: 9.5 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5840961212727667		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 2.5840961212727667 | validation: 2.8175747986985153]
	TIME [epoch: 9.51 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6130157418516564		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 2.6130157418516564 | validation: 3.3762221512869406]
	TIME [epoch: 9.49 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1052436671459893		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 3.1052436671459893 | validation: 3.0437058277570634]
	TIME [epoch: 9.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1996215257811254		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.1996215257811254 | validation: 3.978326129805042]
	TIME [epoch: 9.48 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.210715416010747		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.210715416010747 | validation: 3.2187154208647764]
	TIME [epoch: 9.51 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024025463699483		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 3.024025463699483 | validation: 3.6523280695975076]
	TIME [epoch: 9.49 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.786998173831001		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 2.786998173831001 | validation: 3.3598564010399787]
	TIME [epoch: 9.49 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5560542232257744		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 2.5560542232257744 | validation: 2.9634767061799]
	TIME [epoch: 9.49 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5218391627197394		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 2.5218391627197394 | validation: 2.7891613261358623]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.553793216468603		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 2.553793216468603 | validation: 3.15536695809829]
	TIME [epoch: 9.48 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.532412944983258		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 2.532412944983258 | validation: 2.8433158952713145]
	TIME [epoch: 9.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5982246301905096		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 2.5982246301905096 | validation: 2.8570133216956153]
	TIME [epoch: 9.5 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4494294701993886		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 2.4494294701993886 | validation: 3.0888539403783417]
	TIME [epoch: 9.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6081399014473403		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 2.6081399014473403 | validation: 3.2803076726745237]
	TIME [epoch: 9.49 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6467369084997525		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 2.6467369084997525 | validation: 2.813250692185542]
	TIME [epoch: 9.49 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.724347253282401		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 2.724347253282401 | validation: 3.2265458903493136]
	TIME [epoch: 9.49 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6360881668618257		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 2.6360881668618257 | validation: 2.7236340950990745]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5366419989439972		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 2.5366419989439972 | validation: 3.2158988868984295]
	TIME [epoch: 9.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6152887422169324		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 2.6152887422169324 | validation: 2.7694729018938533]
	TIME [epoch: 9.5 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.582541888850735		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 2.582541888850735 | validation: 3.2398685223436474]
	TIME [epoch: 9.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5532823413777304		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 2.5532823413777304 | validation: 2.727703122842246]
	TIME [epoch: 9.52 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5158456929898456		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 2.5158456929898456 | validation: 2.881127984069435]
	TIME [epoch: 9.49 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5030197312966527		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 2.5030197312966527 | validation: 2.733791321332524]
	TIME [epoch: 9.49 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4723163101738854		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 2.4723163101738854 | validation: 2.7131728515632485]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.758792778593132		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 2.758792778593132 | validation: 3.201863108119696]
	TIME [epoch: 9.51 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.48233086941086		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 2.48233086941086 | validation: 2.9532411317016884]
	TIME [epoch: 9.51 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.492085723969322		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 2.492085723969322 | validation: 3.5083481038954183]
	TIME [epoch: 9.51 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.655836873582323		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 2.655836873582323 | validation: 2.9095265669518398]
	TIME [epoch: 9.53 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5778328243165882		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 2.5778328243165882 | validation: 2.916487068144122]
	TIME [epoch: 9.52 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.529632107681136		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 2.529632107681136 | validation: 3.0082584208004666]
	TIME [epoch: 9.52 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4394235377478726		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 2.4394235377478726 | validation: 2.810376134563217]
	TIME [epoch: 9.52 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.393604495398757		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 2.393604495398757 | validation: 3.0074504015303627]
	TIME [epoch: 9.52 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4949909551275624		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 2.4949909551275624 | validation: 2.9267319702090906]
	TIME [epoch: 9.52 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.592940366231322		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 2.592940366231322 | validation: 2.7420704527862334]
	TIME [epoch: 9.51 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.417265796845293		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 2.417265796845293 | validation: 2.6848681684560747]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4457426588935056		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 2.4457426588935056 | validation: 2.909471171448356]
	TIME [epoch: 9.52 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.451824844370445		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 2.451824844370445 | validation: 2.972298582091106]
	TIME [epoch: 9.53 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.721593140020896		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 2.721593140020896 | validation: 2.909019260943328]
	TIME [epoch: 9.51 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4623641350543073		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 2.4623641350543073 | validation: 3.021617212685172]
	TIME [epoch: 9.51 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.432283460024309		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 2.432283460024309 | validation: 2.961244061544099]
	TIME [epoch: 9.52 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5332042583474337		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 2.5332042583474337 | validation: 2.7515713006766442]
	TIME [epoch: 9.52 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3912485709325635		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 2.3912485709325635 | validation: 3.1661601693483816]
	TIME [epoch: 9.51 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.495631241856436		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 2.495631241856436 | validation: 2.65491852368703]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4642880429328358		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 2.4642880429328358 | validation: 2.7122194207606056]
	TIME [epoch: 9.52 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.478381809762527		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 2.478381809762527 | validation: 2.841867185348071]
	TIME [epoch: 9.53 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4410310399553454		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 2.4410310399553454 | validation: 2.9960406988731076]
	TIME [epoch: 9.51 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4565838162611224		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 2.4565838162611224 | validation: 2.8526124392826184]
	TIME [epoch: 9.51 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5453134533053117		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 2.5453134533053117 | validation: 3.0851110458943722]
	TIME [epoch: 9.52 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.409003315814347		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 2.409003315814347 | validation: 2.955570590486019]
	TIME [epoch: 9.52 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3764948090479954		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 2.3764948090479954 | validation: 2.7697083792865125]
	TIME [epoch: 9.51 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.392356623206982		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 2.392356623206982 | validation: 3.9230093787239957]
	TIME [epoch: 9.52 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.708501853711126		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 2.708501853711126 | validation: 2.7448938906055]
	TIME [epoch: 9.52 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.321297543603803		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 2.321297543603803 | validation: 2.967976616912202]
	TIME [epoch: 9.53 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4423475042544345		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 2.4423475042544345 | validation: 3.182487228815189]
	TIME [epoch: 9.51 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5155671728776574		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 2.5155671728776574 | validation: 2.7270591404079556]
	TIME [epoch: 9.51 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.403590183679087		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 2.403590183679087 | validation: 3.3820771817355224]
	TIME [epoch: 9.52 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.554885105760469		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 2.554885105760469 | validation: 2.7194930614140924]
	TIME [epoch: 9.53 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3687117356012672		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 2.3687117356012672 | validation: 2.754620483467998]
	TIME [epoch: 9.51 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.416949667263303		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 2.416949667263303 | validation: 2.900458800126501]
	TIME [epoch: 9.51 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3557889094294793		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 2.3557889094294793 | validation: 3.289879378716937]
	TIME [epoch: 9.52 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4978662741140654		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 2.4978662741140654 | validation: 2.8541255375855066]
	TIME [epoch: 9.52 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3810030535571918		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 2.3810030535571918 | validation: 3.343057167501565]
	TIME [epoch: 9.51 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6790782742222303		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 2.6790782742222303 | validation: 2.753162811681684]
	TIME [epoch: 9.51 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3523529827431133		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 2.3523529827431133 | validation: 2.7161403022554533]
	TIME [epoch: 9.52 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.552723268153819		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 2.552723268153819 | validation: 2.7527519016591038]
	TIME [epoch: 9.52 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.554356209973686		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 2.554356209973686 | validation: 2.781547558462585]
	TIME [epoch: 9.51 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4429514460553365		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 2.4429514460553365 | validation: 2.839545953373885]
	TIME [epoch: 9.51 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3644866439276813		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 2.3644866439276813 | validation: 2.6917179781147587]
	TIME [epoch: 9.52 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3281616980495183		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 2.3281616980495183 | validation: 2.856373944050414]
	TIME [epoch: 9.52 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3494034745042764		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 2.3494034745042764 | validation: 2.8493051517636094]
	TIME [epoch: 9.51 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.326694150515807		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 2.326694150515807 | validation: 3.1696661209402515]
	TIME [epoch: 9.52 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4933855678507926		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 2.4933855678507926 | validation: 2.7348446526984254]
	TIME [epoch: 9.52 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4167681826060305		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 2.4167681826060305 | validation: 2.9229836010296357]
	TIME [epoch: 9.52 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.392743540056698		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 2.392743540056698 | validation: 2.6697221296361433]
	TIME [epoch: 9.5 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.342344545502722		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 2.342344545502722 | validation: 2.7177221258723696]
	TIME [epoch: 9.51 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4258279465152546		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 2.4258279465152546 | validation: 2.7987272124655225]
	TIME [epoch: 9.52 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.331033825924382		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 2.331033825924382 | validation: 2.639851370561759]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.410363917564021		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 2.410363917564021 | validation: 2.928108897915464]
	TIME [epoch: 9.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.265618257408337		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.265618257408337 | validation: 2.962023717081594]
	TIME [epoch: 9.5 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.429818508299144		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 2.429818508299144 | validation: 2.7740023008374557]
	TIME [epoch: 9.52 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3531787274792357		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 2.3531787274792357 | validation: 2.868561204181124]
	TIME [epoch: 9.5 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5078403567830967		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 2.5078403567830967 | validation: 2.9359228942233266]
	TIME [epoch: 9.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.555144302431372		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 2.555144302431372 | validation: 2.7000338535272523]
	TIME [epoch: 9.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3253630052830916		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 2.3253630052830916 | validation: 2.939083315303484]
	TIME [epoch: 9.51 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.246896857985037		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 2.246896857985037 | validation: 2.655385092395669]
	TIME [epoch: 9.51 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4036225643061067		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 2.4036225643061067 | validation: 2.8601625938182984]
	TIME [epoch: 9.5 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.342929299299342		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 2.342929299299342 | validation: 2.8379842906089]
	TIME [epoch: 9.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.450133146375136		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 2.450133146375136 | validation: 2.631847538459923]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3488953447799052		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 2.3488953447799052 | validation: 3.1103654244039274]
	TIME [epoch: 9.51 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3574338613801107		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 2.3574338613801107 | validation: 3.2691513560447034]
	TIME [epoch: 9.49 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3764671102732593		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 2.3764671102732593 | validation: 2.7440356948105786]
	TIME [epoch: 9.49 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5369148944040067		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 2.5369148944040067 | validation: 2.7541354734535104]
	TIME [epoch: 9.52 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.32656530698886		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 2.32656530698886 | validation: 2.8423116526856678]
	TIME [epoch: 9.5 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2742999819898584		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 2.2742999819898584 | validation: 2.8420937137621842]
	TIME [epoch: 9.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.413735019578642		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 2.413735019578642 | validation: 2.890323769769574]
	TIME [epoch: 9.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3392221770036064		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 2.3392221770036064 | validation: 3.4687557147719645]
	TIME [epoch: 9.52 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4829431120758847		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 2.4829431120758847 | validation: 2.708555326650767]
	TIME [epoch: 9.51 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2931226128531863		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 2.2931226128531863 | validation: 2.7888613865259013]
	TIME [epoch: 9.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.411000675975651		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 2.411000675975651 | validation: 2.832147732015587]
	TIME [epoch: 9.51 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.326241068036465		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 2.326241068036465 | validation: 2.7388196716221307]
	TIME [epoch: 9.52 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2943061534412443		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 2.2943061534412443 | validation: 2.7391289394265037]
	TIME [epoch: 9.5 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.227144675146159		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 2.227144675146159 | validation: 3.3361922680337477]
	TIME [epoch: 9.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.511272164209076		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 2.511272164209076 | validation: 3.282396573550773]
	TIME [epoch: 9.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.421698626685079		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 2.421698626685079 | validation: 2.946165190152285]
	TIME [epoch: 9.52 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.300333230627914		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 2.300333230627914 | validation: 3.1740725560264322]
	TIME [epoch: 9.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.456152447628557		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 2.456152447628557 | validation: 2.7754710410087906]
	TIME [epoch: 9.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2774629997759392		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 2.2774629997759392 | validation: 2.7709157464871303]
	TIME [epoch: 9.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4194622453382735		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 2.4194622453382735 | validation: 3.431526760301022]
	TIME [epoch: 9.52 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.579276003639801		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 2.579276003639801 | validation: 2.7453572656761134]
	TIME [epoch: 9.51 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.379091085756212		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 2.379091085756212 | validation: 3.61886939839472]
	TIME [epoch: 9.5 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.784684508293094		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 2.784684508293094 | validation: 2.8200737827558497]
	TIME [epoch: 9.5 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.340776228041799		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 2.340776228041799 | validation: 2.7802633409788338]
	TIME [epoch: 9.51 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.294619604863004		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 2.294619604863004 | validation: 2.8002055548150913]
	TIME [epoch: 9.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.316754156594783		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 2.316754156594783 | validation: 2.9678439859545676]
	TIME [epoch: 9.49 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255640852220742		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 2.255640852220742 | validation: 2.853426263289672]
	TIME [epoch: 9.49 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.316576861380173		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 2.316576861380173 | validation: 2.721899117454327]
	TIME [epoch: 9.52 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3090184164283776		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 2.3090184164283776 | validation: 2.70212032602853]
	TIME [epoch: 9.5 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4003491097922813		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 2.4003491097922813 | validation: 2.7512812391883585]
	TIME [epoch: 9.49 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.311500242759608		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 2.311500242759608 | validation: 2.6630767121679697]
	TIME [epoch: 9.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.489090435629142		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 2.489090435629142 | validation: 3.4775770307177987]
	TIME [epoch: 9.52 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.42395199179009		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 2.42395199179009 | validation: 2.8165698179036998]
	TIME [epoch: 9.49 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3987392801091842		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 2.3987392801091842 | validation: 3.055904010637484]
	TIME [epoch: 9.5 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3167471195449663		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 2.3167471195449663 | validation: 2.817538161069076]
	TIME [epoch: 9.49 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.396262222386492		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 2.396262222386492 | validation: 3.079409045502816]
	TIME [epoch: 9.52 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5248510006947895		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 2.5248510006947895 | validation: 2.7252196833646507]
	TIME [epoch: 9.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2537505937655107		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 2.2537505937655107 | validation: 3.209604939235035]
	TIME [epoch: 9.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3727984414297		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 2.3727984414297 | validation: 2.887966099333448]
	TIME [epoch: 9.51 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.313727327586586		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 2.313727327586586 | validation: 2.7072127180857524]
	TIME [epoch: 9.52 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2783985139805605		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 2.2783985139805605 | validation: 3.2121479874862304]
	TIME [epoch: 9.5 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5823370359084343		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 2.5823370359084343 | validation: 2.688599494131711]
	TIME [epoch: 9.5 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.314917546291577		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 2.314917546291577 | validation: 2.65781935237207]
	TIME [epoch: 9.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.679320598846333		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 2.679320598846333 | validation: 3.10155360549375]
	TIME [epoch: 9.53 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4760542225428095		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 2.4760542225428095 | validation: 3.023266329121089]
	TIME [epoch: 9.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243237955912997		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 2.243237955912997 | validation: 2.6467426130137244]
	TIME [epoch: 9.49 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3426142333142947		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 2.3426142333142947 | validation: 2.732010069048105]
	TIME [epoch: 9.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.331694514982525		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 2.331694514982525 | validation: 2.7790238701851604]
	TIME [epoch: 9.52 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2465015359297036		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 2.2465015359297036 | validation: 3.242595215884642]
	TIME [epoch: 9.49 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.385001957986028		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 2.385001957986028 | validation: 2.6443794117385724]
	TIME [epoch: 9.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844942836766874		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 2.1844942836766874 | validation: 2.88902837715836]
	TIME [epoch: 9.51 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3618610848458794		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 2.3618610848458794 | validation: 2.7471625282496017]
	TIME [epoch: 9.53 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2592983362103443		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 2.2592983362103443 | validation: 2.803013419831657]
	TIME [epoch: 9.51 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.216990525465424		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 2.216990525465424 | validation: 2.9095000336835968]
	TIME [epoch: 9.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.308011955013188		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 2.308011955013188 | validation: 2.6729148445704802]
	TIME [epoch: 9.51 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266411920720098		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 2.266411920720098 | validation: 2.9223599247140464]
	TIME [epoch: 9.52 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.267823513395016		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 2.267823513395016 | validation: 2.7504198076192683]
	TIME [epoch: 9.51 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.222834315369869		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 2.222834315369869 | validation: 2.669945952686989]
	TIME [epoch: 9.49 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6176219604300335		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 2.6176219604300335 | validation: 3.437968663199998]
	TIME [epoch: 9.49 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5981406521151014		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 2.5981406521151014 | validation: 2.790745276996207]
	TIME [epoch: 9.52 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2540914981592293		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 2.2540914981592293 | validation: 2.6814061117903707]
	TIME [epoch: 9.52 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2657758231352494		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 2.2657758231352494 | validation: 2.8144707275576737]
	TIME [epoch: 9.51 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.220227861629403		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 2.220227861629403 | validation: 3.1306658929253808]
	TIME [epoch: 9.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4075849865138492		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 2.4075849865138492 | validation: 2.9642141098855355]
	TIME [epoch: 9.52 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.280504015199157		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 2.280504015199157 | validation: 2.645985149173285]
	TIME [epoch: 9.51 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3325481089298092		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 2.3325481089298092 | validation: 3.2951565850924585]
	TIME [epoch: 9.51 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.384889456671001		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 2.384889456671001 | validation: 2.639784320098246]
	TIME [epoch: 9.49 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248064750541482		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 2.248064750541482 | validation: 2.6093467504239176]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.422947972216897		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 2.422947972216897 | validation: 2.7288934685576014]
	TIME [epoch: 9.51 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.229947710930255		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 2.229947710930255 | validation: 2.7654032801531865]
	TIME [epoch: 9.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2636694226985865		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 2.2636694226985865 | validation: 2.771026608489092]
	TIME [epoch: 9.51 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1727207580992345		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 2.1727207580992345 | validation: 3.1581465355815124]
	TIME [epoch: 9.52 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3178192403981557		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 2.3178192403981557 | validation: 2.7883758880805805]
	TIME [epoch: 9.51 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2482093497160993		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 2.2482093497160993 | validation: 2.698501056048174]
	TIME [epoch: 9.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2364274768492467		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 2.2364274768492467 | validation: 2.8536837588512016]
	TIME [epoch: 9.51 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.297837576687272		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 2.297837576687272 | validation: 3.0648300353667457]
	TIME [epoch: 9.52 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.342524107700317		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 2.342524107700317 | validation: 2.648076712422265]
	TIME [epoch: 9.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1538864767478314		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 2.1538864767478314 | validation: 3.0301651050391234]
	TIME [epoch: 9.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.290811349036217		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 2.290811349036217 | validation: 2.7870460058806805]
	TIME [epoch: 9.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.396942633370913		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 2.396942633370913 | validation: 2.6246252255631823]
	TIME [epoch: 9.52 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.219407479195025		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 2.219407479195025 | validation: 2.7900064086164558]
	TIME [epoch: 9.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5259103751679373		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 2.5259103751679373 | validation: 3.179477819553712]
	TIME [epoch: 9.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4206972823084065		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 2.4206972823084065 | validation: 2.7045357930793217]
	TIME [epoch: 9.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288744246545794		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 2.288744246545794 | validation: 2.6897498138068774]
	TIME [epoch: 9.52 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2265598014354384		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 2.2265598014354384 | validation: 2.596243539260317]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3426445990643736		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 2.3426445990643736 | validation: 2.695464542400168]
	TIME [epoch: 9.51 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2153470095062366		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 2.2153470095062366 | validation: 2.759050355996753]
	TIME [epoch: 9.51 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.544690084067862		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 2.544690084067862 | validation: 2.7193426962216325]
	TIME [epoch: 9.52 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3818389099299244		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 2.3818389099299244 | validation: 2.8999382414159567]
	TIME [epoch: 9.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.223702878805273		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 2.223702878805273 | validation: 2.735372297180653]
	TIME [epoch: 9.51 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1534674250062134		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 2.1534674250062134 | validation: 2.772919581576859]
	TIME [epoch: 9.52 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1700520444173024		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 2.1700520444173024 | validation: 2.6075458039438155]
	TIME [epoch: 9.52 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.291980040412862		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 2.291980040412862 | validation: 2.7481379099094596]
	TIME [epoch: 9.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179634279622344		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 2.179634279622344 | validation: 2.6460030426149705]
	TIME [epoch: 9.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188611228099796		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 2.188611228099796 | validation: 2.703298795519164]
	TIME [epoch: 9.49 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.268773840079529		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 2.268773840079529 | validation: 3.272819638057159]
	TIME [epoch: 9.51 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4884228630219516		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 2.4884228630219516 | validation: 2.6005601197925206]
	TIME [epoch: 9.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234461237575232		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 2.234461237575232 | validation: 2.61647112957917]
	TIME [epoch: 9.51 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.199823254777305		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 2.199823254777305 | validation: 2.8284117335677093]
	TIME [epoch: 9.51 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2159972568630972		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 2.2159972568630972 | validation: 2.5862499340735976]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.158484507453262		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 2.158484507453262 | validation: 2.694937788977427]
	TIME [epoch: 9.51 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1859730886752073		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 2.1859730886752073 | validation: 2.689352116393929]
	TIME [epoch: 9.51 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4796854437587457		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 2.4796854437587457 | validation: 3.2400005332861124]
	TIME [epoch: 9.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7164409021744707		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 2.7164409021744707 | validation: 2.73314959867952]
	TIME [epoch: 9.53 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2113287379045743		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 2.2113287379045743 | validation: 2.6573539009087086]
	TIME [epoch: 9.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259356245474456		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 2.259356245474456 | validation: 2.643549979705329]
	TIME [epoch: 9.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1400577595053165		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 2.1400577595053165 | validation: 2.7989430519784753]
	TIME [epoch: 9.5 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.31279200850176		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 2.31279200850176 | validation: 2.6209741087301746]
	TIME [epoch: 9.51 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1381420303447105		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 2.1381420303447105 | validation: 2.7726257491506545]
	TIME [epoch: 9.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.177136469004201		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 2.177136469004201 | validation: 2.73135094569498]
	TIME [epoch: 9.5 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2084018828726117		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 2.2084018828726117 | validation: 2.569112963293225]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.104629522052312		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 2.104629522052312 | validation: 2.643345501222413]
	TIME [epoch: 9.52 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.166185342269999		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 2.166185342269999 | validation: 2.924400428514571]
	TIME [epoch: 9.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1650086299041744		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 2.1650086299041744 | validation: 2.746415507361348]
	TIME [epoch: 9.49 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208933453241248		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 2.208933453241248 | validation: 2.630058721091018]
	TIME [epoch: 9.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2888134863146297		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 2.2888134863146297 | validation: 2.5858026945545953]
	TIME [epoch: 9.52 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.150620992723482		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 2.150620992723482 | validation: 2.6816400514424177]
	TIME [epoch: 9.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1888507601802445		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 2.1888507601802445 | validation: 2.8091920401082904]
	TIME [epoch: 9.49 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3066450896772324		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 2.3066450896772324 | validation: 3.1000230269933855]
	TIME [epoch: 9.5 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241389938488253		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 2.241389938488253 | validation: 2.750632267274047]
	TIME [epoch: 9.52 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1914787262273534		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 2.1914787262273534 | validation: 2.650817856354208]
	TIME [epoch: 9.49 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2429496400547952		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 2.2429496400547952 | validation: 2.900169279046404]
	TIME [epoch: 9.49 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2791772895404376		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 2.2791772895404376 | validation: 2.6932538110079314]
	TIME [epoch: 9.49 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1515356624794912		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 2.1515356624794912 | validation: 2.9720233437172454]
	TIME [epoch: 9.52 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2163558405821524		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 2.2163558405821524 | validation: 2.9283911068369926]
	TIME [epoch: 9.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.222729132861851		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 2.222729132861851 | validation: 3.002019572650329]
	TIME [epoch: 9.49 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262525545136551		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 2.262525545136551 | validation: 2.6927222786071696]
	TIME [epoch: 9.49 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1699317619091163		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 2.1699317619091163 | validation: 2.5815080577166536]
	TIME [epoch: 9.52 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.349907258448158		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 2.349907258448158 | validation: 2.7292593784068813]
	TIME [epoch: 9.5 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3622075696040765		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 2.3622075696040765 | validation: 2.866892844468156]
	TIME [epoch: 9.5 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.351241952757371		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 2.351241952757371 | validation: 2.757737316737944]
	TIME [epoch: 9.49 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2113978450467244		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 2.2113978450467244 | validation: 2.5832805178468976]
	TIME [epoch: 9.52 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1549211024996415		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 2.1549211024996415 | validation: 2.9109648803540416]
	TIME [epoch: 9.49 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.222211532554967		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 2.222211532554967 | validation: 2.6497559164879023]
	TIME [epoch: 9.5 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1641195387900973		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 2.1641195387900973 | validation: 2.6794738472125736]
	TIME [epoch: 9.5 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1183925187718637		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 2.1183925187718637 | validation: 2.69550882989203]
	TIME [epoch: 9.51 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1361068580171847		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 2.1361068580171847 | validation: 2.87345835280033]
	TIME [epoch: 9.49 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189792789893822		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 2.189792789893822 | validation: 2.5923718060933867]
	TIME [epoch: 9.49 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1927374305943474		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 2.1927374305943474 | validation: 2.6697080000857323]
	TIME [epoch: 9.49 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1564597192181116		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 2.1564597192181116 | validation: 2.9973951113266377]
	TIME [epoch: 9.51 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2663881764666245		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 2.2663881764666245 | validation: 2.73254227710855]
	TIME [epoch: 9.51 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2536450111115305		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 2.2536450111115305 | validation: 2.775192670970586]
	TIME [epoch: 9.5 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1955630108142254		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 2.1955630108142254 | validation: 2.7859340548693807]
	TIME [epoch: 9.48 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.176964433647493		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 2.176964433647493 | validation: 3.461902913438106]
	TIME [epoch: 9.5 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4632280334590173		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 2.4632280334590173 | validation: 2.6958799974307714]
	TIME [epoch: 9.5 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1940209076432415		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 2.1940209076432415 | validation: 2.6692806515208507]
	TIME [epoch: 9.48 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1311300644924693		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 2.1311300644924693 | validation: 2.620998023153707]
	TIME [epoch: 9.49 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1785645874034527		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 2.1785645874034527 | validation: 2.605974245740804]
	TIME [epoch: 9.51 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4209838580338205		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 2.4209838580338205 | validation: 2.624610141588911]
	TIME [epoch: 9.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1945734733160576		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 2.1945734733160576 | validation: 2.6211980094138734]
	TIME [epoch: 9.5 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1391677449671045		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 2.1391677449671045 | validation: 2.657149850747538]
	TIME [epoch: 9.5 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2414648041152843		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 2.2414648041152843 | validation: 2.7050097954641643]
	TIME [epoch: 9.52 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.159907026147156		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 2.159907026147156 | validation: 2.632305216092742]
	TIME [epoch: 9.51 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2064363188085743		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 2.2064363188085743 | validation: 2.595751968643366]
	TIME [epoch: 9.51 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.165861317341951		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 2.165861317341951 | validation: 2.5993828973616644]
	TIME [epoch: 9.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1334090710634803		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 2.1334090710634803 | validation: 2.6433950523133785]
	TIME [epoch: 9.52 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.160142522908294		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 2.160142522908294 | validation: 2.7038211964080916]
	TIME [epoch: 9.5 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3194328519156002		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 2.3194328519156002 | validation: 3.117542372879798]
	TIME [epoch: 9.5 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2857806645674414		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 2.2857806645674414 | validation: 3.0540096995228465]
	TIME [epoch: 9.52 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3836779639263423		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 2.3836779639263423 | validation: 2.717341422977883]
	TIME [epoch: 9.52 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234787423637098		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 2.234787423637098 | validation: 2.9940603179721608]
	TIME [epoch: 9.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2449282441246785		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 2.2449282441246785 | validation: 2.694211140944326]
	TIME [epoch: 9.5 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.123250834603166		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 2.123250834603166 | validation: 2.7353656896486336]
	TIME [epoch: 9.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.159299149474511		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 2.159299149474511 | validation: 2.9076453767864083]
	TIME [epoch: 9.51 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2279910703799066		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 2.2279910703799066 | validation: 2.8593123368103104]
	TIME [epoch: 9.49 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1786129190425614		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 2.1786129190425614 | validation: 2.66721659294403]
	TIME [epoch: 9.5 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1661943368594807		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 2.1661943368594807 | validation: 2.6155366550522956]
	TIME [epoch: 9.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.123486094856252		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 2.123486094856252 | validation: 2.5871540968585793]
	TIME [epoch: 9.51 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20504618632998		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 2.20504618632998 | validation: 2.6382023042088445]
	TIME [epoch: 9.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221657346347661		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 2.221657346347661 | validation: 2.6738059354438786]
	TIME [epoch: 9.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.141987528754146		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 2.141987528754146 | validation: 2.916056796771619]
	TIME [epoch: 9.49 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.31407503758034		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 2.31407503758034 | validation: 2.6790493556723693]
	TIME [epoch: 9.51 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1096426372531494		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 2.1096426372531494 | validation: 2.6635520846934653]
	TIME [epoch: 9.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.136814563854633		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 2.136814563854633 | validation: 2.7869122615714303]
	TIME [epoch: 9.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1274424568274544		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 2.1274424568274544 | validation: 2.796848212149963]
	TIME [epoch: 9.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1170141923411903		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 2.1170141923411903 | validation: 2.937779362300663]
	TIME [epoch: 9.51 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.219656728300092		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 2.219656728300092 | validation: 2.6123481446654684]
	TIME [epoch: 9.49 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1238082661570155		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 2.1238082661570155 | validation: 2.679421655301986]
	TIME [epoch: 9.48 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1914889078147572		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 2.1914889078147572 | validation: 2.666698397163101]
	TIME [epoch: 9.49 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5388217749151027		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 2.5388217749151027 | validation: 2.6811595155453016]
	TIME [epoch: 9.51 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.243941327358758		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 2.243941327358758 | validation: 2.707640636081896]
	TIME [epoch: 9.49 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1241268531309117		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 2.1241268531309117 | validation: 2.768977714640542]
	TIME [epoch: 9.49 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1526337070121686		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 2.1526337070121686 | validation: 2.8513450562318754]
	TIME [epoch: 9.48 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260232683718977		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 2.260232683718977 | validation: 2.5449549506076403]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1821689527579853		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 2.1821689527579853 | validation: 2.5794312193545843]
	TIME [epoch: 9.49 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0771060611091987		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 2.0771060611091987 | validation: 2.815973484910054]
	TIME [epoch: 9.51 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187398957205446		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 2.187398957205446 | validation: 2.6478332612196414]
	TIME [epoch: 9.51 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181752061150342		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 2.181752061150342 | validation: 2.723812831423501]
	TIME [epoch: 9.54 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1448907511136115		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 2.1448907511136115 | validation: 2.589789338044946]
	TIME [epoch: 9.51 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.160717286308441		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 2.160717286308441 | validation: 2.6097124299581584]
	TIME [epoch: 9.5 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1138069347973856		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 2.1138069347973856 | validation: 2.7674599104083657]
	TIME [epoch: 9.51 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1894767610707664		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 2.1894767610707664 | validation: 2.5606820840093474]
	TIME [epoch: 9.53 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.164262987721615		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 2.164262987721615 | validation: 2.5947411748172318]
	TIME [epoch: 9.5 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0998060040033963		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 2.0998060040033963 | validation: 2.750647177838081]
	TIME [epoch: 9.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.163933640822761		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 2.163933640822761 | validation: 2.6641687869089945]
	TIME [epoch: 9.51 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.10429436899185		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 2.10429436899185 | validation: 2.6226117305735737]
	TIME [epoch: 9.53 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18598740326221		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 2.18598740326221 | validation: 2.988102215922181]
	TIME [epoch: 9.51 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2831535190034744		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 2.2831535190034744 | validation: 2.6709524365075374]
	TIME [epoch: 9.52 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1507207963785113		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 2.1507207963785113 | validation: 2.74711631845138]
	TIME [epoch: 9.52 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270043038035552		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 2.270043038035552 | validation: 2.5692878402716834]
	TIME [epoch: 9.53 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.17808495823499		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 2.17808495823499 | validation: 2.6541207773603794]
	TIME [epoch: 9.51 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.121972110468987		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 2.121972110468987 | validation: 2.7481216575960197]
	TIME [epoch: 9.51 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1684453266975696		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 2.1684453266975696 | validation: 2.779334302955373]
	TIME [epoch: 9.51 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1396085180075		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 2.1396085180075 | validation: 2.6161045705246333]
	TIME [epoch: 9.53 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.096062606240087		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 2.096062606240087 | validation: 2.5675626016705078]
	TIME [epoch: 9.52 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0616244219605973		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 2.0616244219605973 | validation: 2.5606643637310023]
	TIME [epoch: 9.51 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.100767206139633		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 2.100767206139633 | validation: 2.6672344669342896]
	TIME [epoch: 9.51 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.114868162527098		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 2.114868162527098 | validation: 2.564821076818648]
	TIME [epoch: 9.53 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1354383718865764		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 2.1354383718865764 | validation: 2.763435417916345]
	TIME [epoch: 9.51 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.148342842228165		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 2.148342842228165 | validation: 3.0284289950096626]
	TIME [epoch: 9.51 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1729148074434916		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 2.1729148074434916 | validation: 2.5879598380258644]
	TIME [epoch: 9.51 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1175912019201997		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 2.1175912019201997 | validation: 2.820297466043507]
	TIME [epoch: 9.53 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.133332788885016		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 2.133332788885016 | validation: 2.660662660367957]
	TIME [epoch: 9.51 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0925384549107653		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 2.0925384549107653 | validation: 2.566628874302662]
	TIME [epoch: 9.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.284008744596146		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 2.284008744596146 | validation: 2.6568676108302975]
	TIME [epoch: 9.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1403523868238574		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 2.1403523868238574 | validation: 2.558593021568932]
	TIME [epoch: 9.52 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1190149990966862		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 2.1190149990966862 | validation: 2.568914438931193]
	TIME [epoch: 9.51 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198278774280815		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 2.198278774280815 | validation: 2.8720946497825577]
	TIME [epoch: 9.51 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1507319988891114		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 2.1507319988891114 | validation: 2.558870864331751]
	TIME [epoch: 9.51 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0883133699001766		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 2.0883133699001766 | validation: 2.604300898082462]
	TIME [epoch: 9.53 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.21240155577992		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 2.21240155577992 | validation: 2.5866811142707475]
	TIME [epoch: 9.52 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1587218140057978		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 2.1587218140057978 | validation: 2.85572348483258]
	TIME [epoch: 9.51 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.170819613514158		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 2.170819613514158 | validation: 2.603219991459018]
	TIME [epoch: 9.51 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.137397053504098		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 2.137397053504098 | validation: 2.7782694322105566]
	TIME [epoch: 9.53 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1428218601678424		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 2.1428218601678424 | validation: 2.5867976188496407]
	TIME [epoch: 9.51 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0874219796264186		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 2.0874219796264186 | validation: 2.5725755965130714]
	TIME [epoch: 9.51 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.08389497760783		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 2.08389497760783 | validation: 2.5466558606584684]
	TIME [epoch: 9.51 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1639701782188174		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 2.1639701782188174 | validation: 2.595628708547917]
	TIME [epoch: 9.53 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.124014068518355		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 2.124014068518355 | validation: 2.6669111051005587]
	TIME [epoch: 9.52 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.105497680424194		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 2.105497680424194 | validation: 2.5382118896300927]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.059236327290462		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 2.059236327290462 | validation: 2.9242140341678913]
	TIME [epoch: 9.51 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23061416779134		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 2.23061416779134 | validation: 2.5504958008915914]
	TIME [epoch: 9.53 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.137062800167935		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 2.137062800167935 | validation: 2.7837817501525697]
	TIME [epoch: 9.51 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258362349120245		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 2.258362349120245 | validation: 2.5915539272289583]
	TIME [epoch: 9.52 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289790923196427		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 2.289790923196427 | validation: 2.984702411418681]
	TIME [epoch: 9.51 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1801879937596356		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 2.1801879937596356 | validation: 2.8203143333053378]
	TIME [epoch: 9.53 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.113135525690092		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 2.113135525690092 | validation: 2.6159507193234743]
	TIME [epoch: 9.51 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.09117196277825		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 2.09117196277825 | validation: 2.640801353687717]
	TIME [epoch: 9.51 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0506040145082847		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 2.0506040145082847 | validation: 2.5356667215715123]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.094723941050291		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 2.094723941050291 | validation: 2.545608334225464]
	TIME [epoch: 9.53 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1042082558578503		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 2.1042082558578503 | validation: 2.635053376746944]
	TIME [epoch: 9.51 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1160264386714482		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 2.1160264386714482 | validation: 2.776745657548542]
	TIME [epoch: 9.51 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.104848892808186		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 2.104848892808186 | validation: 2.800860232657626]
	TIME [epoch: 9.51 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1667753020677254		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 2.1667753020677254 | validation: 2.616626271244193]
	TIME [epoch: 9.53 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.114720198762653		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 2.114720198762653 | validation: 2.756123867072026]
	TIME [epoch: 9.51 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.116550609666983		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 2.116550609666983 | validation: 2.5907915841526687]
	TIME [epoch: 9.53 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.04281560437435		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 2.04281560437435 | validation: 2.537345005369796]
	TIME [epoch: 9.51 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1562265053453133		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 2.1562265053453133 | validation: 2.5378316076851664]
	TIME [epoch: 9.53 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181653680748537		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 2.181653680748537 | validation: 2.6941655779265536]
	TIME [epoch: 9.51 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.110209291907401		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 2.110209291907401 | validation: 2.542512535388687]
	TIME [epoch: 9.51 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0551418497149587		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 2.0551418497149587 | validation: 2.574193901409775]
	TIME [epoch: 9.51 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0973206308882526		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 2.0973206308882526 | validation: 2.525005014514051]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0669043057989014		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 2.0669043057989014 | validation: 2.625700873385526]
	TIME [epoch: 9.51 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.078670274958528		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 2.078670274958528 | validation: 2.5182284228205924]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0311492320390423		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 2.0311492320390423 | validation: 2.5897017178157706]
	TIME [epoch: 9.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.095153796653233		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 2.095153796653233 | validation: 2.552062592526102]
	TIME [epoch: 9.52 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040543331523564		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 2.040543331523564 | validation: 2.6338611189655525]
	TIME [epoch: 9.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1101822829032946		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 2.1101822829032946 | validation: 2.5739131577783407]
	TIME [epoch: 9.51 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0859758534759276		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 2.0859758534759276 | validation: 2.653679897048012]
	TIME [epoch: 9.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.219836658084172		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 2.219836658084172 | validation: 2.740985118252462]
	TIME [epoch: 9.52 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.136858203218607		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 2.136858203218607 | validation: 2.5920200177998756]
	TIME [epoch: 9.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.054120499354813		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 2.054120499354813 | validation: 2.5194104855619224]
	TIME [epoch: 9.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0281923285943018		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 2.0281923285943018 | validation: 2.520182876970686]
	TIME [epoch: 9.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.132984031304265		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 2.132984031304265 | validation: 3.0051752757813017]
	TIME [epoch: 9.52 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1658865253320263		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 2.1658865253320263 | validation: 2.5896588625818624]
	TIME [epoch: 9.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.153556513896574		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 2.153556513896574 | validation: 2.6453903143749904]
	TIME [epoch: 9.49 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.089279651291098		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 2.089279651291098 | validation: 2.8421368546136483]
	TIME [epoch: 9.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2247132819606095		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 2.2247132819606095 | validation: 2.57790870761334]
	TIME [epoch: 9.52 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0698273447253057		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 2.0698273447253057 | validation: 2.713902431329341]
	TIME [epoch: 9.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.16893648542229		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 2.16893648542229 | validation: 2.534225516488358]
	TIME [epoch: 9.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.045696131433165		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 2.045696131433165 | validation: 2.6871922437619267]
	TIME [epoch: 9.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.082903109311375		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 2.082903109311375 | validation: 2.5704143255848955]
	TIME [epoch: 9.52 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.080798320572469		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 2.080798320572469 | validation: 2.534955358064753]
	TIME [epoch: 9.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1122548983294687		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 2.1122548983294687 | validation: 2.521579466329359]
	TIME [epoch: 9.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0256255693432452		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 2.0256255693432452 | validation: 2.7503485494806807]
	TIME [epoch: 9.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1293727381351046		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 2.1293727381351046 | validation: 2.560002121516473]
	TIME [epoch: 9.52 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1319642959718696		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 2.1319642959718696 | validation: 2.671978679601484]
	TIME [epoch: 9.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1726124755389966		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 2.1726124755389966 | validation: 2.568058623239366]
	TIME [epoch: 9.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0616350189760015		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 2.0616350189760015 | validation: 2.6058124067441724]
	TIME [epoch: 9.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0528236512025595		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 2.0528236512025595 | validation: 2.560800951809457]
	TIME [epoch: 9.52 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.131719640210684		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 2.131719640210684 | validation: 2.6472739219517143]
	TIME [epoch: 9.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0510352934722347		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 2.0510352934722347 | validation: 2.5825490331428957]
	TIME [epoch: 9.49 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0867466557077945		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 2.0867466557077945 | validation: 2.704253116600078]
	TIME [epoch: 9.49 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.142785982096546		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 2.142785982096546 | validation: 2.574851345718621]
	TIME [epoch: 9.52 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0862823947743063		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 2.0862823947743063 | validation: 2.5864179066852273]
	TIME [epoch: 9.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.101495829853918		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 2.101495829853918 | validation: 2.6259273448257043]
	TIME [epoch: 9.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.112900794533625		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 2.112900794533625 | validation: 2.5548266209391164]
	TIME [epoch: 9.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1020693991155692		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 2.1020693991155692 | validation: 2.6714597091988095]
	TIME [epoch: 9.52 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1550376801518114		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 2.1550376801518114 | validation: 2.7377091458089193]
	TIME [epoch: 9.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1298706952854767		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 2.1298706952854767 | validation: 2.6230026798489594]
	TIME [epoch: 9.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1025187136338395		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 2.1025187136338395 | validation: 2.6197363012603074]
	TIME [epoch: 9.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.102260900003624		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 2.102260900003624 | validation: 2.6665819191868976]
	TIME [epoch: 9.52 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.063079220001354		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 2.063079220001354 | validation: 2.7058537479408336]
	TIME [epoch: 9.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0847708075585962		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 2.0847708075585962 | validation: 2.7741303469666527]
	TIME [epoch: 9.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1271215847784357		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 2.1271215847784357 | validation: 2.530207653802805]
	TIME [epoch: 9.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.043963603174425		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 2.043963603174425 | validation: 2.5865199349050894]
	TIME [epoch: 9.52 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.063749335751506		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 2.063749335751506 | validation: 2.5589952795554063]
	TIME [epoch: 9.49 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.036524185239857		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 2.036524185239857 | validation: 2.8578986456221775]
	TIME [epoch: 9.49 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2152110602114035		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 2.2152110602114035 | validation: 2.6186094296182594]
	TIME [epoch: 9.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.137626695066416		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 2.137626695066416 | validation: 2.6047914065217426]
	TIME [epoch: 9.52 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.174490830917435		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 2.174490830917435 | validation: 2.5297198921875252]
	TIME [epoch: 9.49 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0363994468781947		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 2.0363994468781947 | validation: 2.5830145426559166]
	TIME [epoch: 9.49 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.13400479810976		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 2.13400479810976 | validation: 2.6752618165787836]
	TIME [epoch: 9.49 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.041735529621776		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 2.041735529621776 | validation: 2.668736272341284]
	TIME [epoch: 9.52 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0972735120743633		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 2.0972735120743633 | validation: 2.5241708240147505]
	TIME [epoch: 9.49 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0297476323965253		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 2.0297476323965253 | validation: 2.6128204726797577]
	TIME [epoch: 9.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.029719563562911		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 2.029719563562911 | validation: 2.6059467369447282]
	TIME [epoch: 9.51 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.104467376156496		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 2.104467376156496 | validation: 2.571472057228121]
	TIME [epoch: 9.52 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1423486675909436		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 2.1423486675909436 | validation: 2.6151587513830403]
	TIME [epoch: 9.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0426231659500678		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 2.0426231659500678 | validation: 2.6031566869568317]
	TIME [epoch: 9.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.024814892975348		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 2.024814892975348 | validation: 2.523769837138417]
	TIME [epoch: 9.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040811929719071		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 2.040811929719071 | validation: 2.559548679211509]
	TIME [epoch: 9.51 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1018720711861305		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 2.1018720711861305 | validation: 2.6565584669488795]
	TIME [epoch: 9.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.108113417634786		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 2.108113417634786 | validation: 2.5621176887407557]
	TIME [epoch: 9.49 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0429051958008384		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 2.0429051958008384 | validation: 2.5849094960479566]
	TIME [epoch: 9.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0511470236311014		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 2.0511470236311014 | validation: 2.5291723199710963]
	TIME [epoch: 9.51 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1129776635096134		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 2.1129776635096134 | validation: 2.593394598714638]
	TIME [epoch: 9.49 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.030227092166405		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 2.030227092166405 | validation: 2.7305060826971466]
	TIME [epoch: 9.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.085478264620204		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 2.085478264620204 | validation: 2.6148165203658347]
	TIME [epoch: 9.49 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.053173573585396		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 2.053173573585396 | validation: 2.7336613198688724]
	TIME [epoch: 9.51 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.082706539021139		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 2.082706539021139 | validation: 2.620583648045635]
	TIME [epoch: 9.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.072456756101235		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 2.072456756101235 | validation: 2.5198725169508056]
	TIME [epoch: 9.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.070638113579627		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 2.070638113579627 | validation: 2.8039510977598345]
	TIME [epoch: 9.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.075789423250913		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 2.075789423250913 | validation: 2.59078506923921]
	TIME [epoch: 9.52 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0155742149346656		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 2.0155742149346656 | validation: 2.5473959168368294]
	TIME [epoch: 9.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2028299777346625		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 2.2028299777346625 | validation: 2.5678607545629304]
	TIME [epoch: 9.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0506450286711964		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 2.0506450286711964 | validation: 2.5844750024172165]
	TIME [epoch: 9.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.116869141971864		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 2.116869141971864 | validation: 2.6098629202950474]
	TIME [epoch: 9.52 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0780849229103278		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 2.0780849229103278 | validation: 2.528493886249218]
	TIME [epoch: 9.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9979787827005766		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 1.9979787827005766 | validation: 2.548023991738318]
	TIME [epoch: 9.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.063795200447461		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 2.063795200447461 | validation: 2.5828867372408557]
	TIME [epoch: 9.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0473129472820517		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 2.0473129472820517 | validation: 2.5417083241004317]
	TIME [epoch: 9.52 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0387428963796355		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 2.0387428963796355 | validation: 2.524084159854176]
	TIME [epoch: 9.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.022690746577873		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 2.022690746577873 | validation: 2.6491578076810813]
	TIME [epoch: 9.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.067945184863791		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 2.067945184863791 | validation: 2.503084748241932]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.085870828190297		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 2.085870828190297 | validation: 2.5589669631584466]
	TIME [epoch: 9.52 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0077759542171423		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 2.0077759542171423 | validation: 2.538433510116415]
	TIME [epoch: 9.49 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0512118543464264		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 2.0512118543464264 | validation: 2.549695973630682]
	TIME [epoch: 9.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1158436488528762		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 2.1158436488528762 | validation: 2.6118712372155755]
	TIME [epoch: 9.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0313416734129204		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 2.0313416734129204 | validation: 2.6079505825937384]
	TIME [epoch: 9.51 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1121675686310306		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 2.1121675686310306 | validation: 2.567108066600693]
	TIME [epoch: 9.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0313688366491998		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 2.0313688366491998 | validation: 2.5345471828432413]
	TIME [epoch: 9.49 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0443487607352737		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 2.0443487607352737 | validation: 2.5360128818364043]
	TIME [epoch: 9.49 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.063821440871929		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 2.063821440871929 | validation: 2.714712718563445]
	TIME [epoch: 9.51 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1216395322783552		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 2.1216395322783552 | validation: 2.643218389532068]
	TIME [epoch: 9.49 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0342193005070803		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 2.0342193005070803 | validation: 2.667800753045837]
	TIME [epoch: 9.49 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0307184751620833		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 2.0307184751620833 | validation: 2.554571138330812]
	TIME [epoch: 9.49 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.024205902919678		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 2.024205902919678 | validation: 2.7370004876166805]
	TIME [epoch: 9.51 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0995740445206392		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 2.0995740445206392 | validation: 2.5786738203378237]
	TIME [epoch: 9.49 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.045275831131627		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 2.045275831131627 | validation: 2.51657612157309]
	TIME [epoch: 9.49 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.03519268891863		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 2.03519268891863 | validation: 2.5475773602570606]
	TIME [epoch: 9.49 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0218908741248507		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 2.0218908741248507 | validation: 2.5254006445408383]
	TIME [epoch: 9.51 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0749780349682574		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 2.0749780349682574 | validation: 2.5577446634122096]
	TIME [epoch: 9.49 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0282166804229504		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 2.0282166804229504 | validation: 2.5591098644021595]
	TIME [epoch: 9.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0173454610647696		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 2.0173454610647696 | validation: 2.5327768473177876]
	TIME [epoch: 9.49 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0784309771785106		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 2.0784309771785106 | validation: 2.542649331271865]
	TIME [epoch: 9.52 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.041101357195508		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 2.041101357195508 | validation: 2.559782114198673]
	TIME [epoch: 9.49 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040266481740111		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 2.040266481740111 | validation: 2.5731740936514864]
	TIME [epoch: 9.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.998626345009019		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 1.998626345009019 | validation: 2.523794791538266]
	TIME [epoch: 9.49 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0488695753600195		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 2.0488695753600195 | validation: 2.7710606193045533]
	TIME [epoch: 9.51 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1056738304017073		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 2.1056738304017073 | validation: 2.517283212353049]
	TIME [epoch: 9.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.013182419522045		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 2.013182419522045 | validation: 2.5217593810263956]
	TIME [epoch: 9.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.063776638620581		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 2.063776638620581 | validation: 2.5394737274742676]
	TIME [epoch: 9.5 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0814376600278695		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 2.0814376600278695 | validation: 2.5423456469981978]
	TIME [epoch: 9.52 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0344217816023695		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 2.0344217816023695 | validation: 2.6477261202672633]
	TIME [epoch: 9.49 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.064430538529608		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 2.064430538529608 | validation: 2.534791380904421]
	TIME [epoch: 9.49 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0164295559883643		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 2.0164295559883643 | validation: 2.6093421783559463]
	TIME [epoch: 9.49 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0304770356134303		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 2.0304770356134303 | validation: 2.604374075147096]
	TIME [epoch: 9.51 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0881834409066187		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 2.0881834409066187 | validation: 2.572031438983163]
	TIME [epoch: 9.49 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0238856317217815		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 2.0238856317217815 | validation: 2.6013500820058435]
	TIME [epoch: 9.49 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.040824367921984		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 2.040824367921984 | validation: 2.717578898845853]
	TIME [epoch: 9.49 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.049784424660961		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 2.049784424660961 | validation: 2.545602261398806]
	TIME [epoch: 9.51 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.011693872701559		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 2.011693872701559 | validation: 2.9022189318996605]
	TIME [epoch: 9.49 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1153250897250144		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 2.1153250897250144 | validation: 2.553386686885598]
	TIME [epoch: 9.49 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0581720437887614		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 2.0581720437887614 | validation: 2.6426788518502127]
	TIME [epoch: 9.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0287255164533855		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 2.0287255164533855 | validation: 2.627024550533306]
	TIME [epoch: 9.51 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0529969360266827		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 2.0529969360266827 | validation: 2.5807305742815703]
	TIME [epoch: 9.49 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.033447431779164		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 2.033447431779164 | validation: 2.521123962030175]
	TIME [epoch: 9.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0309837411853087		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 2.0309837411853087 | validation: 2.5281934687590915]
	TIME [epoch: 9.48 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0344660741679585		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 2.0344660741679585 | validation: 2.5507130709704655]
	TIME [epoch: 9.51 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0466008601287893		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 2.0466008601287893 | validation: 2.5162093503319842]
	TIME [epoch: 9.49 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0262875629039114		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 2.0262875629039114 | validation: 2.5181870999409557]
	TIME [epoch: 9.49 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.020805489141478		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 2.020805489141478 | validation: 2.6491064590100404]
	TIME [epoch: 9.5 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.054132872433551		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 2.054132872433551 | validation: 2.6066991663184127]
	TIME [epoch: 9.52 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0655903723854		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 2.0655903723854 | validation: 2.6023998172157783]
	TIME [epoch: 9.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0492275194164993		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 2.0492275194164993 | validation: 2.652301781463673]
	TIME [epoch: 9.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0172574895567976		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 2.0172574895567976 | validation: 2.7706766278134984]
	TIME [epoch: 9.49 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0809854148182256		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 2.0809854148182256 | validation: 2.586064788607839]
	TIME [epoch: 9.51 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.034323704831554		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 2.034323704831554 | validation: 2.629496500562755]
	TIME [epoch: 9.49 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.054113316460595		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 2.054113316460595 | validation: 2.553596056863423]
	TIME [epoch: 9.49 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0190372283603364		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 2.0190372283603364 | validation: 2.5857013639133006]
	TIME [epoch: 9.49 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0238358497397106		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 2.0238358497397106 | validation: 2.614317091447774]
	TIME [epoch: 9.52 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0596723032842372		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 2.0596723032842372 | validation: 2.550563709055844]
	TIME [epoch: 9.49 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0176305166722264		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 2.0176305166722264 | validation: 2.6271629125777576]
	TIME [epoch: 9.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0544439000674477		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 2.0544439000674477 | validation: 2.5346447186518084]
	TIME [epoch: 9.49 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.994348911680279		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 1.994348911680279 | validation: 2.590947686676701]
	TIME [epoch: 9.51 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0714288803804175		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 2.0714288803804175 | validation: 2.642988210712668]
	TIME [epoch: 9.49 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0674268675809535		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 2.0674268675809535 | validation: 2.6326899533913104]
	TIME [epoch: 9.49 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.073403963626789		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 2.073403963626789 | validation: 2.5927121088585285]
	TIME [epoch: 9.49 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.04464499773934		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 2.04464499773934 | validation: 2.547947188604713]
	TIME [epoch: 9.52 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0116243268901965		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 2.0116243268901965 | validation: 2.5375597456664574]
	TIME [epoch: 9.49 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0540347869778026		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 2.0540347869778026 | validation: 2.5248829830618487]
	TIME [epoch: 9.48 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.020521233875889		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 2.020521233875889 | validation: 2.516016003629742]
	TIME [epoch: 9.49 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0524042079202216		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 2.0524042079202216 | validation: 2.584115701400255]
	TIME [epoch: 9.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.039821990077029		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 2.039821990077029 | validation: 2.536471101295081]
	TIME [epoch: 9.49 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.008623723530757		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 2.008623723530757 | validation: 2.736863898994501]
	TIME [epoch: 9.49 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0541905143494965		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 2.0541905143494965 | validation: 2.640349869082072]
	TIME [epoch: 9.49 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0394107659326814		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 2.0394107659326814 | validation: 2.560814611943255]
	TIME [epoch: 9.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.02224168376582		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 2.02224168376582 | validation: 2.551364442867601]
	TIME [epoch: 9.49 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.024621128523063		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 2.024621128523063 | validation: 2.6074808382770103]
	TIME [epoch: 9.49 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0787708358921257		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 2.0787708358921257 | validation: 2.523620633178227]
	TIME [epoch: 9.49 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9998142119537792		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 1.9998142119537792 | validation: 2.5467189988173318]
	TIME [epoch: 9.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9943140862730782		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 1.9943140862730782 | validation: 2.5245331238617474]
	TIME [epoch: 9.48 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0521247958123734		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 2.0521247958123734 | validation: 2.6029989248832304]
	TIME [epoch: 9.49 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0094156224652098		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 2.0094156224652098 | validation: 2.515800376376519]
	TIME [epoch: 9.48 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0214126324161605		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 2.0214126324161605 | validation: 2.555237818701345]
	TIME [epoch: 9.51 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0315322136328597		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 2.0315322136328597 | validation: 2.53273176140863]
	TIME [epoch: 9.49 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.018287255308335		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 2.018287255308335 | validation: 2.523924170291896]
	TIME [epoch: 9.49 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.009035792155714		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 2.009035792155714 | validation: 2.5546353677925984]
	TIME [epoch: 9.48 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.001231594042127		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 2.001231594042127 | validation: 2.5796093548872068]
	TIME [epoch: 9.51 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0081338225400716		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 2.0081338225400716 | validation: 2.515618060359417]
	TIME [epoch: 9.49 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9932980265238693		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 1.9932980265238693 | validation: 2.666749558180034]
	TIME [epoch: 9.49 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0372837809189925		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 2.0372837809189925 | validation: 2.5937232114600453]
	TIME [epoch: 9.48 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0670436923946824		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 2.0670436923946824 | validation: 2.578128681561136]
	TIME [epoch: 9.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0151914201039403		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 2.0151914201039403 | validation: 2.5584698371345165]
	TIME [epoch: 9.49 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0028669061882547		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 2.0028669061882547 | validation: 2.6259990637088]
	TIME [epoch: 9.49 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.037106187095915		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 2.037106187095915 | validation: 2.5169135079750853]
	TIME [epoch: 9.49 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0243142739975175		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 2.0243142739975175 | validation: 2.5674210204024117]
	TIME [epoch: 9.51 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9995598627874123		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 1.9995598627874123 | validation: 2.5774567475912242]
	TIME [epoch: 9.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0124076604143686		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 2.0124076604143686 | validation: 2.504744292584373]
	TIME [epoch: 9.49 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0021897730444045		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 2.0021897730444045 | validation: 2.54468396101537]
	TIME [epoch: 9.48 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0597685632146425		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 2.0597685632146425 | validation: 2.577045629933765]
	TIME [epoch: 9.51 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0225945898015993		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 2.0225945898015993 | validation: 2.582520573473621]
	TIME [epoch: 9.5 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.012539235386713		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 2.012539235386713 | validation: 2.6363065891855006]
	TIME [epoch: 9.51 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.038303211423255		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 2.038303211423255 | validation: 2.6821017891109302]
	TIME [epoch: 9.49 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.047808283641367		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 2.047808283641367 | validation: 2.618454122484727]
	TIME [epoch: 9.51 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.025442445174072		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 2.025442445174072 | validation: 2.5303124068436915]
	TIME [epoch: 9.48 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9927702109640848		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 1.9927702109640848 | validation: 2.7547974483565336]
	TIME [epoch: 9.49 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1960347078363918		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 2.1960347078363918 | validation: 2.5051280666540263]
	TIME [epoch: 9.49 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.999520228823188		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 1.999520228823188 | validation: 2.5287897556231282]
	TIME [epoch: 9.51 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0064795305072507		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 2.0064795305072507 | validation: 2.5465479841736833]
	TIME [epoch: 9.49 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.000699918211849		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 2.000699918211849 | validation: 2.5410207951370873]
	TIME [epoch: 9.49 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0120544486276577		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 2.0120544486276577 | validation: 2.525262450763823]
	TIME [epoch: 9.49 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0185918473435978		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 2.0185918473435978 | validation: 2.5217703070047146]
	TIME [epoch: 9.51 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0113306576964725		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 2.0113306576964725 | validation: 2.520226621513643]
	TIME [epoch: 9.48 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0506890421428907		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 2.0506890421428907 | validation: 2.5178482525530037]
	TIME [epoch: 9.49 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.006440724665697		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 2.006440724665697 | validation: 2.5555447125569506]
	TIME [epoch: 9.48 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0384056332706963		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 2.0384056332706963 | validation: 2.536058609668088]
	TIME [epoch: 9.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0451665064537927		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 2.0451665064537927 | validation: 2.520193864775615]
	TIME [epoch: 9.49 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.004525381264938		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 2.004525381264938 | validation: 2.7244504117648445]
	TIME [epoch: 9.48 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0910918659310105		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 2.0910918659310105 | validation: 2.5182978275875523]
	TIME [epoch: 9.48 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.008626981837284		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 2.008626981837284 | validation: 2.581024852471236]
	TIME [epoch: 9.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0006633302295254		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 2.0006633302295254 | validation: 2.5125327445112466]
	TIME [epoch: 9.48 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.079256431064477		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 2.079256431064477 | validation: 2.5465053461474128]
	TIME [epoch: 9.48 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.997144396524385		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 1.997144396524385 | validation: 2.561637977138822]
	TIME [epoch: 9.49 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0070981731018427		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 2.0070981731018427 | validation: 2.5367339059713503]
	TIME [epoch: 9.51 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9951219221209588		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 1.9951219221209588 | validation: 2.617983758672094]
	TIME [epoch: 9.48 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0932562315823127		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 2.0932562315823127 | validation: 2.550758694713452]
	TIME [epoch: 9.49 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0075903003019695		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 2.0075903003019695 | validation: 2.5118686781603303]
	TIME [epoch: 9.49 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.017939678618828		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 2.017939678618828 | validation: 2.6230911007578754]
	TIME [epoch: 9.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0420974075118377		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 2.0420974075118377 | validation: 2.6355556787730894]
	TIME [epoch: 9.48 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.036897057046393		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 2.036897057046393 | validation: 2.5063924900634027]
	TIME [epoch: 9.49 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9838000264890858		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 1.9838000264890858 | validation: 2.5567096313128155]
	TIME [epoch: 9.49 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.022880410616184		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 2.022880410616184 | validation: 2.6224135376352935]
	TIME [epoch: 9.51 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.031468808129575		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 2.031468808129575 | validation: 2.54806396579439]
	TIME [epoch: 9.49 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.031685609619246		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 2.031685609619246 | validation: 2.6104910142587743]
	TIME [epoch: 9.48 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0422493967407713		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 2.0422493967407713 | validation: 2.5007344845346156]
	TIME [epoch: 9.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_766.pth
	Model improved!!!
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.042141177426562		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 2.042141177426562 | validation: 2.561016424384452]
	TIME [epoch: 9.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188134809448511		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 2.188134809448511 | validation: 2.5511398803216183]
	TIME [epoch: 9.49 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0304964859148633		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 2.0304964859148633 | validation: 2.5223991184109447]
	TIME [epoch: 9.48 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.999045932926272		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 1.999045932926272 | validation: 2.5895123820036]
	TIME [epoch: 9.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.044867596688632		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 2.044867596688632 | validation: 2.54912308384382]
	TIME [epoch: 9.51 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.059931182310803		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 2.059931182310803 | validation: 2.5177933680762488]
	TIME [epoch: 9.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9961432919433975		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 1.9961432919433975 | validation: 2.518770018931712]
	TIME [epoch: 9.48 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.022059815677399		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 2.022059815677399 | validation: 2.5502120843548965]
	TIME [epoch: 9.49 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9940254128307306		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 1.9940254128307306 | validation: 2.533481762663335]
	TIME [epoch: 9.51 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.010032641604483		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 2.010032641604483 | validation: 2.571176354167765]
	TIME [epoch: 9.49 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.049708070294952		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 2.049708070294952 | validation: 2.5449133619854094]
	TIME [epoch: 9.49 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9924281392840169		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 1.9924281392840169 | validation: 2.5549543584745145]
	TIME [epoch: 9.49 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9938917859800829		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 1.9938917859800829 | validation: 2.516236850095776]
	TIME [epoch: 9.51 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9782595897783597		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 1.9782595897783597 | validation: 2.527070214750606]
	TIME [epoch: 9.49 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0345170165270865		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 2.0345170165270865 | validation: 2.504022084115243]
	TIME [epoch: 9.49 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9885185659128068		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 1.9885185659128068 | validation: 2.510260164366554]
	TIME [epoch: 9.48 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9870893959422253		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 1.9870893959422253 | validation: 2.564802451869418]
	TIME [epoch: 9.51 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.029993845516851		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 2.029993845516851 | validation: 2.5961770912790634]
	TIME [epoch: 9.49 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0336989731220925		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 2.0336989731220925 | validation: 2.517817426297219]
	TIME [epoch: 9.49 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9941353955178613		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 1.9941353955178613 | validation: 2.531496059723394]
	TIME [epoch: 9.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0030741826910066		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 2.0030741826910066 | validation: 2.5515291150356676]
	TIME [epoch: 9.52 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0239479906025872		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 2.0239479906025872 | validation: 2.569015895658108]
	TIME [epoch: 9.49 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.036049431947391		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 2.036049431947391 | validation: 2.508962996803472]
	TIME [epoch: 9.49 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1615687703483		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 2.1615687703483 | validation: 2.524538733796499]
	TIME [epoch: 9.48 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0089343857208193		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 2.0089343857208193 | validation: 2.507453204224138]
	TIME [epoch: 9.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.00046601800405		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 2.00046601800405 | validation: 2.5005547119186127]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9924264544482497		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 1.9924264544482497 | validation: 2.5956708296739834]
	TIME [epoch: 9.48 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0171946632401525		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 2.0171946632401525 | validation: 2.6197491222202856]
	TIME [epoch: 9.49 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.038327386271372		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 2.038327386271372 | validation: 2.5607481364420583]
	TIME [epoch: 9.51 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0867045389622483		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 2.0867045389622483 | validation: 2.532038795504995]
	TIME [epoch: 9.49 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0016935184888154		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 2.0016935184888154 | validation: 2.5029441637685403]
	TIME [epoch: 9.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.029542901828804		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 2.029542901828804 | validation: 2.4869188160399553]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_798.pth
	Model improved!!!
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9759520416919283		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 1.9759520416919283 | validation: 2.510836736326785]
	TIME [epoch: 9.52 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.97766692320163		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 1.97766692320163 | validation: 2.5889650183330684]
	TIME [epoch: 9.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0194659474289054		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 2.0194659474289054 | validation: 2.5032743222258507]
	TIME [epoch: 9.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.023140141683099		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 2.023140141683099 | validation: 2.5133454760539955]
	TIME [epoch: 9.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9880815548292197		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 1.9880815548292197 | validation: 2.5212557130720086]
	TIME [epoch: 9.52 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0395523809826193		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 2.0395523809826193 | validation: 2.507271516582617]
	TIME [epoch: 9.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9767342300881041		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 1.9767342300881041 | validation: 2.5061649280514513]
	TIME [epoch: 9.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9916350158549434		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 1.9916350158549434 | validation: 2.5077872258682232]
	TIME [epoch: 9.49 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9933227404011382		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 1.9933227404011382 | validation: 2.53361056207925]
	TIME [epoch: 9.51 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9885329837823307		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 1.9885329837823307 | validation: 2.510408178384227]
	TIME [epoch: 9.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9979311807008517		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 1.9979311807008517 | validation: 2.5438764132227885]
	TIME [epoch: 9.49 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9803266950157918		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 1.9803266950157918 | validation: 2.5213929426842157]
	TIME [epoch: 9.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0379577726801195		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 2.0379577726801195 | validation: 2.5022612573588323]
	TIME [epoch: 9.52 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9764963146245225		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 1.9764963146245225 | validation: 2.510157130293443]
	TIME [epoch: 9.49 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9777302608458533		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 1.9777302608458533 | validation: 2.5337676446475927]
	TIME [epoch: 9.49 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9938999387090561		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 1.9938999387090561 | validation: 2.4958916341077866]
	TIME [epoch: 9.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9779301357700796		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 1.9779301357700796 | validation: 2.5622982942025363]
	TIME [epoch: 9.51 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.981198837651739		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 1.981198837651739 | validation: 2.502976618584942]
	TIME [epoch: 9.49 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0648557423043576		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 2.0648557423043576 | validation: 2.574508921856299]
	TIME [epoch: 9.49 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0702047755302035		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 2.0702047755302035 | validation: 2.502669321408614]
	TIME [epoch: 9.49 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0270739933648465		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 2.0270739933648465 | validation: 2.5107422186603783]
	TIME [epoch: 9.52 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9835483055270864		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 1.9835483055270864 | validation: 2.5155403590379475]
	TIME [epoch: 9.5 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.980902704316252		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 1.980902704316252 | validation: 2.5241841181712714]
	TIME [epoch: 9.49 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.998609286293902		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 1.998609286293902 | validation: 2.5149979075308018]
	TIME [epoch: 9.49 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9895439038248839		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 1.9895439038248839 | validation: 2.5890699298323567]
	TIME [epoch: 9.51 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0041881798811354		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 2.0041881798811354 | validation: 2.4987824542329715]
	TIME [epoch: 9.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0058546427202453		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 2.0058546427202453 | validation: 2.543849171588871]
	TIME [epoch: 9.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0114453928605194		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 2.0114453928605194 | validation: 2.614169211234058]
	TIME [epoch: 9.49 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0313027675551076		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 2.0313027675551076 | validation: 2.540591590091369]
	TIME [epoch: 9.52 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0327294054860148		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 2.0327294054860148 | validation: 2.500719901748392]
	TIME [epoch: 9.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0205465218301626		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 2.0205465218301626 | validation: 2.6024956282372806]
	TIME [epoch: 9.49 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0286437266807926		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 2.0286437266807926 | validation: 2.5205430038044607]
	TIME [epoch: 9.49 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.988866028951421		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 1.988866028951421 | validation: 2.522044491936938]
	TIME [epoch: 9.51 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9866350810410267		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 1.9866350810410267 | validation: 2.509595345263453]
	TIME [epoch: 9.49 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.975262078411602		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 1.975262078411602 | validation: 2.5543239209741975]
	TIME [epoch: 9.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.002022338533269		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 2.002022338533269 | validation: 2.498272495875316]
	TIME [epoch: 9.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9772708131626089		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 1.9772708131626089 | validation: 2.5628511707782025]
	TIME [epoch: 9.51 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9955063406309292		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 1.9955063406309292 | validation: 2.5140790235570583]
	TIME [epoch: 9.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.013137623876177		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 2.013137623876177 | validation: 2.501250362398479]
	TIME [epoch: 9.49 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0032353761808057		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 2.0032353761808057 | validation: 2.63917567642249]
	TIME [epoch: 9.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0165314642526466		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 2.0165314642526466 | validation: 2.5529442398056776]
	TIME [epoch: 9.52 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.991650105171707		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 1.991650105171707 | validation: 2.6017546099205653]
	TIME [epoch: 9.49 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.005333611987354		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 2.005333611987354 | validation: 2.5699973855445615]
	TIME [epoch: 9.48 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.002509284099837		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 2.002509284099837 | validation: 2.5527069338402657]
	TIME [epoch: 9.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9999112663850362		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 1.9999112663850362 | validation: 2.532737213067849]
	TIME [epoch: 9.51 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.038601468188954		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 2.038601468188954 | validation: 2.533985236269584]
	TIME [epoch: 9.49 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.999335416757718		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 1.999335416757718 | validation: 2.5142239945299454]
	TIME [epoch: 9.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9943472064753975		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 1.9943472064753975 | validation: 2.5041495315154907]
	TIME [epoch: 9.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9854854643627569		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 1.9854854643627569 | validation: 2.495156666592705]
	TIME [epoch: 9.52 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9927351782871896		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 1.9927351782871896 | validation: 2.507299728553052]
	TIME [epoch: 9.48 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9976989102050198		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 1.9976989102050198 | validation: 2.5107155127182152]
	TIME [epoch: 9.49 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9967479791633238		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 1.9967479791633238 | validation: 2.5373890836349027]
	TIME [epoch: 9.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9865023320042383		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 1.9865023320042383 | validation: 2.500091138560096]
	TIME [epoch: 9.51 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9846169628152341		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 1.9846169628152341 | validation: 2.5606781903486397]
	TIME [epoch: 9.49 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9887330028700703		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 1.9887330028700703 | validation: 2.4941827816109674]
	TIME [epoch: 9.49 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0020590743123874		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 2.0020590743123874 | validation: 2.5543183786475216]
	TIME [epoch: 9.49 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.994601905047545		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 1.994601905047545 | validation: 2.5765059410371594]
	TIME [epoch: 9.51 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0055210444667315		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 2.0055210444667315 | validation: 2.511373604253295]
	TIME [epoch: 9.49 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9884711192073083		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 1.9884711192073083 | validation: 2.506148003454621]
	TIME [epoch: 9.49 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9751402401881155		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 1.9751402401881155 | validation: 2.51252260126435]
	TIME [epoch: 9.49 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9860887510134362		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 1.9860887510134362 | validation: 2.5081904053116046]
	TIME [epoch: 9.51 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.984136880894059		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 1.984136880894059 | validation: 2.5914389439540146]
	TIME [epoch: 9.49 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0053334901418443		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 2.0053334901418443 | validation: 2.540125985619609]
	TIME [epoch: 9.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.972237664316144		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 1.972237664316144 | validation: 2.5739049792349675]
	TIME [epoch: 9.49 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0024051724026894		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 2.0024051724026894 | validation: 2.4951278998161746]
	TIME [epoch: 9.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0091027711537155		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 2.0091027711537155 | validation: 2.523068046374444]
	TIME [epoch: 9.49 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.975237410357888		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 1.975237410357888 | validation: 2.4913854662982824]
	TIME [epoch: 9.49 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9870461671943052		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 1.9870461671943052 | validation: 2.5739561580745147]
	TIME [epoch: 9.48 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9989026357273958		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 1.9989026357273958 | validation: 2.5036392649061834]
	TIME [epoch: 9.51 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9810705095756884		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 1.9810705095756884 | validation: 2.5012374240395276]
	TIME [epoch: 9.48 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9961122938878226		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 1.9961122938878226 | validation: 2.5308514705991296]
	TIME [epoch: 9.49 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9922258575286822		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 1.9922258575286822 | validation: 2.5528309942135454]
	TIME [epoch: 9.49 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0481700696985734		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 2.0481700696985734 | validation: 2.5647553124077276]
	TIME [epoch: 9.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0003413803599157		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 2.0003413803599157 | validation: 2.525573751856124]
	TIME [epoch: 9.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0422705924591162		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 2.0422705924591162 | validation: 2.51585673102883]
	TIME [epoch: 9.48 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9671579111778315		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 1.9671579111778315 | validation: 2.48764361497726]
	TIME [epoch: 9.49 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9704619388838456		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 1.9704619388838456 | validation: 2.5267085454567657]
	TIME [epoch: 9.51 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0683444758198055		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 2.0683444758198055 | validation: 2.6012302619430585]
	TIME [epoch: 9.49 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.007608941756124		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 2.007608941756124 | validation: 2.514553322689519]
	TIME [epoch: 9.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9746819250675984		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 1.9746819250675984 | validation: 2.51693093180245]
	TIME [epoch: 9.49 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0170030153503853		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 2.0170030153503853 | validation: 2.5040246283234007]
	TIME [epoch: 9.51 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9733583562225534		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 1.9733583562225534 | validation: 2.497834522500248]
	TIME [epoch: 9.49 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9806833426031751		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 1.9806833426031751 | validation: 2.5614510407313413]
	TIME [epoch: 9.48 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.016460184990671		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 2.016460184990671 | validation: 2.5183488778430223]
	TIME [epoch: 9.48 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.994549009888832		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 1.994549009888832 | validation: 2.5741529511738652]
	TIME [epoch: 9.51 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9824684246373998		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 1.9824684246373998 | validation: 2.5050191749198314]
	TIME [epoch: 9.48 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9667797140141716		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 1.9667797140141716 | validation: 2.5089744298175574]
	TIME [epoch: 9.49 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9784047635918642		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 1.9784047635918642 | validation: 2.5171929425099493]
	TIME [epoch: 9.49 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.979784195261736		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 1.979784195261736 | validation: 2.4996796138160313]
	TIME [epoch: 9.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.96987920433236		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 1.96987920433236 | validation: 2.4983705171025328]
	TIME [epoch: 9.49 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9657156399782416		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 1.9657156399782416 | validation: 2.5131445691907506]
	TIME [epoch: 9.48 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.987896152760219		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 1.987896152760219 | validation: 2.5108527168273698]
	TIME [epoch: 9.48 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9758637123177152		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 1.9758637123177152 | validation: 2.5351312751259236]
	TIME [epoch: 9.51 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9854027965699288		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 1.9854027965699288 | validation: 2.494443493858506]
	TIME [epoch: 9.49 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.977263738673407		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 1.977263738673407 | validation: 2.5037233371497614]
	TIME [epoch: 9.49 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9770599759333571		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 1.9770599759333571 | validation: 2.5348826702291847]
	TIME [epoch: 9.48 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9801083052249986		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 1.9801083052249986 | validation: 2.4974563409234363]
	TIME [epoch: 9.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.978777997782781		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 1.978777997782781 | validation: 2.5117446315513017]
	TIME [epoch: 9.49 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.977848764503197		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 1.977848764503197 | validation: 2.513073826636845]
	TIME [epoch: 9.48 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9796980801607045		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 1.9796980801607045 | validation: 2.49447738793636]
	TIME [epoch: 9.48 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.981959025045984		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 1.981959025045984 | validation: 2.513483201291217]
	TIME [epoch: 9.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9701398157888872		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 1.9701398157888872 | validation: 2.504847932740276]
	TIME [epoch: 9.48 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9902676927894245		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 1.9902676927894245 | validation: 2.5170523522021533]
	TIME [epoch: 9.48 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0009725092714086		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 2.0009725092714086 | validation: 2.5578749516891235]
	TIME [epoch: 9.49 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9752941246949316		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 1.9752941246949316 | validation: 2.5289780100171955]
	TIME [epoch: 9.51 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9878499636132694		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 1.9878499636132694 | validation: 2.553826323814263]
	TIME [epoch: 9.49 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9850839044446986		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 1.9850839044446986 | validation: 2.515816167157065]
	TIME [epoch: 9.48 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9785624942544164		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 1.9785624942544164 | validation: 2.529009890998006]
	TIME [epoch: 9.48 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.988824198897723		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 1.988824198897723 | validation: 2.501708357994021]
	TIME [epoch: 9.51 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9660211005063117		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 1.9660211005063117 | validation: 2.5062690955909948]
	TIME [epoch: 9.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.968716182844797		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 1.968716182844797 | validation: 2.518478900083823]
	TIME [epoch: 9.49 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0229137912607777		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 2.0229137912607777 | validation: 2.5893452182781584]
	TIME [epoch: 9.49 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9986390824980649		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 1.9986390824980649 | validation: 2.510196839331268]
	TIME [epoch: 9.51 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9931088261556695		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 1.9931088261556695 | validation: 2.495059035143327]
	TIME [epoch: 9.49 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9699865882311738		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 1.9699865882311738 | validation: 2.4930456096688043]
	TIME [epoch: 9.48 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9805362609154613		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 1.9805362609154613 | validation: 2.49179572204704]
	TIME [epoch: 9.48 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9679990404397756		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 1.9679990404397756 | validation: 2.5078028552887623]
	TIME [epoch: 9.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.995321389953928		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 1.995321389953928 | validation: 2.5050944859628963]
	TIME [epoch: 9.49 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.971378632142906		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 1.971378632142906 | validation: 2.5353866575180057]
	TIME [epoch: 9.48 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9779406549629157		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 1.9779406549629157 | validation: 2.501395747249926]
	TIME [epoch: 9.49 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9730920822502285		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 1.9730920822502285 | validation: 2.490273430831304]
	TIME [epoch: 9.51 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9895747930374372		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 1.9895747930374372 | validation: 2.5007013675139915]
	TIME [epoch: 9.48 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9919525092103036		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 1.9919525092103036 | validation: 2.497901191140722]
	TIME [epoch: 9.49 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9992888998508405		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 1.9992888998508405 | validation: 2.4873522287101926]
	TIME [epoch: 9.48 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9759289134961608		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 1.9759289134961608 | validation: 2.4858693095854316]
	TIME [epoch: 9.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9783256734282482		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 1.9783256734282482 | validation: 2.485663736295608]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9877931029010292		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 1.9877931029010292 | validation: 2.496305269804282]
	TIME [epoch: 9.49 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9632574868528638		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 1.9632574868528638 | validation: 2.5410959505956834]
	TIME [epoch: 9.51 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9647861846521082		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 1.9647861846521082 | validation: 2.4825354725299604]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9913549374123931		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 1.9913549374123931 | validation: 2.520464775032204]
	TIME [epoch: 9.51 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9759361731644187		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 1.9759361731644187 | validation: 2.536214022841209]
	TIME [epoch: 9.51 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9766051897285588		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 1.9766051897285588 | validation: 2.5379039655926072]
	TIME [epoch: 9.51 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9827908668163707		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 1.9827908668163707 | validation: 2.4977624627830926]
	TIME [epoch: 9.52 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9648155653897927		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 1.9648155653897927 | validation: 2.549885850931703]
	TIME [epoch: 9.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9820832699249131		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 1.9820832699249131 | validation: 2.500075019267738]
	TIME [epoch: 9.51 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9872470617069347		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 1.9872470617069347 | validation: 2.529651668276674]
	TIME [epoch: 9.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9755943506469447		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 1.9755943506469447 | validation: 2.5200581068104664]
	TIME [epoch: 9.52 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9859761411509271		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 1.9859761411509271 | validation: 2.485449914520667]
	TIME [epoch: 9.51 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9812847137770297		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 1.9812847137770297 | validation: 2.5021915429978714]
	TIME [epoch: 9.51 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9709341247575403		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 1.9709341247575403 | validation: 2.4934482397749]
	TIME [epoch: 9.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9640346688808212		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 1.9640346688808212 | validation: 2.5195699460020204]
	TIME [epoch: 9.52 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9783262524472462		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 1.9783262524472462 | validation: 2.5339318884100357]
	TIME [epoch: 9.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9784426858738655		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 1.9784426858738655 | validation: 2.5169305890611393]
	TIME [epoch: 9.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9704859540767856		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 1.9704859540767856 | validation: 2.533477331030771]
	TIME [epoch: 9.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9806943594133384		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 1.9806943594133384 | validation: 2.6116071407604067]
	TIME [epoch: 9.52 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0035228854459497		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 2.0035228854459497 | validation: 2.4829296043852898]
	TIME [epoch: 9.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.970539595327039		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 1.970539595327039 | validation: 2.5027358044872106]
	TIME [epoch: 9.51 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.984130446542752		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 1.984130446542752 | validation: 2.5235739163419844]
	TIME [epoch: 9.5 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9894449670852137		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 1.9894449670852137 | validation: 2.4989187960473456]
	TIME [epoch: 9.52 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.96518145820447		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 1.96518145820447 | validation: 2.4866403548692984]
	TIME [epoch: 9.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9555678828784413		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 1.9555678828784413 | validation: 2.503738363397726]
	TIME [epoch: 9.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9735204750887494		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 1.9735204750887494 | validation: 2.4868322072070903]
	TIME [epoch: 9.51 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9623739925621657		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 1.9623739925621657 | validation: 2.4955765458502377]
	TIME [epoch: 9.52 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9661299625417243		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 1.9661299625417243 | validation: 2.5233588189732017]
	TIME [epoch: 9.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9721630361935063		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 1.9721630361935063 | validation: 2.4939408692874507]
	TIME [epoch: 9.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9595188925865739		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 1.9595188925865739 | validation: 2.526930918261591]
	TIME [epoch: 9.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9648099832093124		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 1.9648099832093124 | validation: 2.494583468892857]
	TIME [epoch: 9.52 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9725799443684155		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 1.9725799443684155 | validation: 2.5606761971212273]
	TIME [epoch: 9.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9792037673440823		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 1.9792037673440823 | validation: 2.487477788894473]
	TIME [epoch: 9.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.967765324325418		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 1.967765324325418 | validation: 2.511571498312047]
	TIME [epoch: 9.52 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9626331437453863		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 1.9626331437453863 | validation: 2.514104685128562]
	TIME [epoch: 9.53 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9582087468320477		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 1.9582087468320477 | validation: 2.5245101007188784]
	TIME [epoch: 9.51 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.969548527526045		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 1.969548527526045 | validation: 2.5007453510209934]
	TIME [epoch: 9.51 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9641457389239967		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 1.9641457389239967 | validation: 2.5068104409360554]
	TIME [epoch: 9.51 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9718818206946582		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 1.9718818206946582 | validation: 2.5076386570156464]
	TIME [epoch: 9.53 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9684322015803368		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 1.9684322015803368 | validation: 2.5151791821692537]
	TIME [epoch: 9.51 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9671005491956326		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 1.9671005491956326 | validation: 2.5214371515179153]
	TIME [epoch: 9.51 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9715919681256804		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 1.9715919681256804 | validation: 2.5012411912051444]
	TIME [epoch: 9.51 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9899052163183257		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 1.9899052163183257 | validation: 2.478860845865179]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9604077848967887		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 1.9604077848967887 | validation: 2.49055260272583]
	TIME [epoch: 9.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9783298233651798		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 1.9783298233651798 | validation: 2.5284642595574383]
	TIME [epoch: 9.51 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9688098107352363		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 1.9688098107352363 | validation: 2.5021312471880344]
	TIME [epoch: 9.51 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9794149995626857		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 1.9794149995626857 | validation: 2.4985103125959136]
	TIME [epoch: 9.52 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9867646223495874		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 1.9867646223495874 | validation: 2.5078223875575962]
	TIME [epoch: 9.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9651790516118588		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 1.9651790516118588 | validation: 2.483336153973259]
	TIME [epoch: 9.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9679851983139873		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 1.9679851983139873 | validation: 2.496955811857771]
	TIME [epoch: 9.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9870636820241905		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 1.9870636820241905 | validation: 2.492689764263178]
	TIME [epoch: 9.52 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9556029243796043		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 1.9556029243796043 | validation: 2.4995320267588697]
	TIME [epoch: 9.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9642190076465469		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 1.9642190076465469 | validation: 2.532721692394284]
	TIME [epoch: 9.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.986183701937032		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 1.986183701937032 | validation: 2.5001354387595196]
	TIME [epoch: 9.5 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9609508945609349		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 1.9609508945609349 | validation: 2.5447385435486916]
	TIME [epoch: 9.52 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9960599893302047		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 1.9960599893302047 | validation: 2.5079135688640486]
	TIME [epoch: 9.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9673479813846542		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 1.9673479813846542 | validation: 2.492644282131713]
	TIME [epoch: 9.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9908410474152194		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 1.9908410474152194 | validation: 2.512635647781379]
	TIME [epoch: 9.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9687797086081869		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 1.9687797086081869 | validation: 2.515107798398395]
	TIME [epoch: 9.53 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9769129177807898		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 1.9769129177807898 | validation: 2.498826243385934]
	TIME [epoch: 9.51 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9797562828120872		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 1.9797562828120872 | validation: 2.5017513979054598]
	TIME [epoch: 9.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9575290039943156		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 1.9575290039943156 | validation: 2.523898478363011]
	TIME [epoch: 9.51 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.969850426094871		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 1.969850426094871 | validation: 2.5076184476713936]
	TIME [epoch: 9.53 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.968115452648907		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 1.968115452648907 | validation: 2.4944049055080906]
	TIME [epoch: 9.51 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9855848895058106		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 1.9855848895058106 | validation: 2.5513135833711407]
	TIME [epoch: 9.51 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9711076110923529		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 1.9711076110923529 | validation: 2.5153465621975877]
	TIME [epoch: 9.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9748445636645677		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 1.9748445636645677 | validation: 2.4932277379116363]
	TIME [epoch: 9.55 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9579185384389528		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 1.9579185384389528 | validation: 2.5062579601763275]
	TIME [epoch: 9.52 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9618164737982393		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 1.9618164737982393 | validation: 2.4932727439276428]
	TIME [epoch: 9.53 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.992799833256862		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 1.992799833256862 | validation: 2.4965385211015367]
	TIME [epoch: 9.55 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.966554252212553		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 1.966554252212553 | validation: 2.5198384202671535]
	TIME [epoch: 9.55 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9667792293363562		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 1.9667792293363562 | validation: 2.4884266343576615]
	TIME [epoch: 9.53 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.002453680262746		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 2.002453680262746 | validation: 2.502092493864174]
	TIME [epoch: 9.53 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9686193128350316		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 1.9686193128350316 | validation: 2.488702921798677]
	TIME [epoch: 9.53 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.967333688708683		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 1.967333688708683 | validation: 2.4878465347234844]
	TIME [epoch: 9.55 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9625155541140935		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 1.9625155541140935 | validation: 2.495172697667614]
	TIME [epoch: 9.53 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.961324688910142		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 1.961324688910142 | validation: 2.498807013030186]
	TIME [epoch: 9.53 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9617697962461442		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 1.9617697962461442 | validation: 2.521615088741527]
	TIME [epoch: 9.53 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9602281178980263		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 1.9602281178980263 | validation: 2.4974882263130698]
	TIME [epoch: 9.54 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.988469250146773		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 1.988469250146773 | validation: 2.5029971302110816]
	TIME [epoch: 9.52 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9662407565170021		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 1.9662407565170021 | validation: 2.5317183846777587]
	TIME [epoch: 9.53 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9803416636723838		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 1.9803416636723838 | validation: 2.5242308697045663]
	TIME [epoch: 9.53 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.972749408792465		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 1.972749408792465 | validation: 2.4933107974763233]
	TIME [epoch: 9.53 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9887177193880916		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 1.9887177193880916 | validation: 2.5659855914611502]
	TIME [epoch: 9.53 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.975075365358044		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 1.975075365358044 | validation: 2.5249723630332275]
	TIME [epoch: 9.52 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9620201711737721		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 1.9620201711737721 | validation: 2.539125372457934]
	TIME [epoch: 9.54 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9631368110708565		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 1.9631368110708565 | validation: 2.521467078725633]
	TIME [epoch: 9.54 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9552413836626072		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 1.9552413836626072 | validation: 2.483613817285321]
	TIME [epoch: 9.53 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954829047471133		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 1.954829047471133 | validation: 2.5101250664577752]
	TIME [epoch: 9.53 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9839411098603605		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 1.9839411098603605 | validation: 2.5126218636596795]
	TIME [epoch: 9.56 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9633970044997635		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 1.9633970044997635 | validation: 2.49992723277789]
	TIME [epoch: 9.53 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9703873457314522		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 1.9703873457314522 | validation: 2.50085155936974]
	TIME [epoch: 9.52 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9712176081724089		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 1.9712176081724089 | validation: 2.521370469725244]
	TIME [epoch: 9.51 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.96686977649614		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 1.96686977649614 | validation: 2.4907190428772044]
	TIME [epoch: 9.54 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.976163033527774		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 1.976163033527774 | validation: 2.557575510467338]
	TIME [epoch: 9.55 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.97451345021184		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 1.97451345021184 | validation: 2.5026540709034446]
	TIME [epoch: 9.53 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9663353667697585		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 1.9663353667697585 | validation: 2.4895741819813124]
	TIME [epoch: 9.52 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.969126262066284		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 1.969126262066284 | validation: 2.5158335734615744]
	TIME [epoch: 9.54 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.966250825368202		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 1.966250825368202 | validation: 2.4777172495085154]
	TIME [epoch: 9.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1023.pth
	Model improved!!!
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9643582307304908		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 1.9643582307304908 | validation: 2.514818578366611]
	TIME [epoch: 9.53 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9703812200510966		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 1.9703812200510966 | validation: 2.5162755866320725]
	TIME [epoch: 9.52 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.975797391623884		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 1.975797391623884 | validation: 2.5063064372950055]
	TIME [epoch: 9.51 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.967506833761742		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 1.967506833761742 | validation: 2.5035602389061604]
	TIME [epoch: 9.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.986038484090615		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 1.986038484090615 | validation: 2.4939792951826885]
	TIME [epoch: 9.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.968663050534473		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 1.968663050534473 | validation: 2.50422789874421]
	TIME [epoch: 9.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9605009551078294		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 1.9605009551078294 | validation: 2.5292281042099076]
	TIME [epoch: 9.51 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.977380136544324		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 1.977380136544324 | validation: 2.4841384074964283]
	TIME [epoch: 9.52 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9656052979787024		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 1.9656052979787024 | validation: 2.4954490394474456]
	TIME [epoch: 9.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9652848375581171		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 1.9652848375581171 | validation: 2.49584142753948]
	TIME [epoch: 9.51 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.961918879948458		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 1.961918879948458 | validation: 2.510371172533747]
	TIME [epoch: 9.52 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.971484388257852		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 1.971484388257852 | validation: 2.488475328827845]
	TIME [epoch: 9.51 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9685737650746553		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 1.9685737650746553 | validation: 2.4877420655166493]
	TIME [epoch: 9.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9660024251353996		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 1.9660024251353996 | validation: 2.488262909566024]
	TIME [epoch: 9.51 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9540469260934419		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 1.9540469260934419 | validation: 2.4991768916488595]
	TIME [epoch: 9.51 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9911176336469723		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 1.9911176336469723 | validation: 2.4983383852569614]
	TIME [epoch: 9.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9643543671454065		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 1.9643543671454065 | validation: 2.5134759628611936]
	TIME [epoch: 9.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9676798577554195		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 1.9676798577554195 | validation: 2.5278378490038342]
	TIME [epoch: 9.54 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9684794422207752		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 1.9684794422207752 | validation: 2.47182264314738]
	TIME [epoch: 9.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1042.pth
	Model improved!!!
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.958275504139905		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 1.958275504139905 | validation: 2.5121763276755393]
	TIME [epoch: 9.51 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9529563220391009		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 1.9529563220391009 | validation: 2.5001042563886737]
	TIME [epoch: 9.51 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9575903713153786		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 1.9575903713153786 | validation: 2.488896358087716]
	TIME [epoch: 9.51 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9717996060854497		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 1.9717996060854497 | validation: 2.4880886373468]
	TIME [epoch: 9.53 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.959496734739583		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 1.959496734739583 | validation: 2.5204885225788924]
	TIME [epoch: 9.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9665229461998213		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 1.9665229461998213 | validation: 2.5174111659048024]
	TIME [epoch: 9.51 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9692318507346207		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 1.9692318507346207 | validation: 2.4896386978521643]
	TIME [epoch: 9.51 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9565077119524574		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 1.9565077119524574 | validation: 2.490931552560174]
	TIME [epoch: 9.52 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.959218439783569		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 1.959218439783569 | validation: 2.4929049484821624]
	TIME [epoch: 9.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9592664673002893		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 1.9592664673002893 | validation: 2.495816068208866]
	TIME [epoch: 9.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9606680309646145		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 1.9606680309646145 | validation: 2.4798771190900766]
	TIME [epoch: 9.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9534111473980587		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 1.9534111473980587 | validation: 2.524663210898711]
	TIME [epoch: 9.51 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9584305799527484		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 1.9584305799527484 | validation: 2.4890138248696094]
	TIME [epoch: 9.49 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9717401933060146		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 1.9717401933060146 | validation: 2.4847226743132187]
	TIME [epoch: 9.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9496317289878864		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 1.9496317289878864 | validation: 2.500269585252745]
	TIME [epoch: 9.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9514439769775915		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 1.9514439769775915 | validation: 2.4860910799022613]
	TIME [epoch: 9.51 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9668651193773958		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 1.9668651193773958 | validation: 2.5573112270160396]
	TIME [epoch: 9.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9771718213340308		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 1.9771718213340308 | validation: 2.5035509021665825]
	TIME [epoch: 9.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9736422441608226		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 1.9736422441608226 | validation: 2.508652176620245]
	TIME [epoch: 9.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9612182459939014		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 1.9612182459939014 | validation: 2.4905706186698415]
	TIME [epoch: 9.51 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9833447665229222		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 1.9833447665229222 | validation: 2.5196941732395244]
	TIME [epoch: 9.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9686477717582211		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 1.9686477717582211 | validation: 2.498324265022701]
	TIME [epoch: 9.49 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9581885583047836		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 1.9581885583047836 | validation: 2.47876150838097]
	TIME [epoch: 9.49 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9672211892560363		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 1.9672211892560363 | validation: 2.528628079164267]
	TIME [epoch: 9.51 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9875705723647905		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 1.9875705723647905 | validation: 2.4865824579909193]
	TIME [epoch: 9.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9824117937052552		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 1.9824117937052552 | validation: 2.4886504386161796]
	TIME [epoch: 9.49 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9610635634665425		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 1.9610635634665425 | validation: 2.5234189273991485]
	TIME [epoch: 9.49 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.964654878586836		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 1.964654878586836 | validation: 2.473330548035774]
	TIME [epoch: 9.51 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9740424667225291		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 1.9740424667225291 | validation: 2.4892954568938843]
	TIME [epoch: 9.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.961220706500369		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 1.961220706500369 | validation: 2.488038023863382]
	TIME [epoch: 9.49 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9595496492380877		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 1.9595496492380877 | validation: 2.5614921332423397]
	TIME [epoch: 9.49 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9716367791078764		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 1.9716367791078764 | validation: 2.4992000043537677]
	TIME [epoch: 9.51 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9523932929101693		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 1.9523932929101693 | validation: 2.504171956580644]
	TIME [epoch: 9.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9688192332229169		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 1.9688192332229169 | validation: 2.49667638440656]
	TIME [epoch: 9.49 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.969410940177037		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 1.969410940177037 | validation: 2.503011600141055]
	TIME [epoch: 9.49 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9561785255224282		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 1.9561785255224282 | validation: 2.4944754095476136]
	TIME [epoch: 9.51 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9631485832859201		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 1.9631485832859201 | validation: 2.5155201520128037]
	TIME [epoch: 9.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.958871037617915		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 1.958871037617915 | validation: 2.4967687663612947]
	TIME [epoch: 9.49 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9618347698142664		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 1.9618347698142664 | validation: 2.487791977671753]
	TIME [epoch: 9.49 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9545444297632137		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 1.9545444297632137 | validation: 2.515397687609178]
	TIME [epoch: 9.51 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9647047698049427		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 1.9647047698049427 | validation: 2.505754152436802]
	TIME [epoch: 9.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9697048570352433		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 1.9697048570352433 | validation: 2.4786014119705357]
	TIME [epoch: 9.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9475448835030829		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 1.9475448835030829 | validation: 2.494012021635097]
	TIME [epoch: 9.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9627957305180839		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 1.9627957305180839 | validation: 2.4884289521019323]
	TIME [epoch: 9.51 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9568856516159105		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 1.9568856516159105 | validation: 2.5000515411684634]
	TIME [epoch: 9.49 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9616451717489014		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 1.9616451717489014 | validation: 2.475169462025605]
	TIME [epoch: 9.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9658110098159614		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 1.9658110098159614 | validation: 2.515499636768796]
	TIME [epoch: 9.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9645516739924982		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 1.9645516739924982 | validation: 2.506644953651818]
	TIME [epoch: 9.51 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9697864313134446		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 1.9697864313134446 | validation: 2.522839007908987]
	TIME [epoch: 9.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.975842749941418		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 1.975842749941418 | validation: 2.493189848866165]
	TIME [epoch: 9.49 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.961383378717669		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 1.961383378717669 | validation: 2.517013155603159]
	TIME [epoch: 9.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9640628605697263		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 1.9640628605697263 | validation: 2.5033804521028027]
	TIME [epoch: 9.51 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9623441558354382		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 1.9623441558354382 | validation: 2.4959854105081263]
	TIME [epoch: 9.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9589628012347284		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 1.9589628012347284 | validation: 2.489209828378535]
	TIME [epoch: 9.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9471520174618746		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 1.9471520174618746 | validation: 2.5198499505206673]
	TIME [epoch: 9.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.971305815206584		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 1.971305815206584 | validation: 2.4950921041194576]
	TIME [epoch: 9.51 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9575822121103972		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 1.9575822121103972 | validation: 2.5018398748392703]
	TIME [epoch: 9.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.961262555812374		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 1.961262555812374 | validation: 2.4950349743507014]
	TIME [epoch: 9.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9504099950740508		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 1.9504099950740508 | validation: 2.4979625126179426]
	TIME [epoch: 9.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9667096899554988		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 1.9667096899554988 | validation: 2.4990291677504888]
	TIME [epoch: 9.51 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9509597341524576		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 1.9509597341524576 | validation: 2.49199669260015]
	TIME [epoch: 9.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955125004987007		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 1.955125004987007 | validation: 2.5533666787736324]
	TIME [epoch: 9.49 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9794006588413204		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 1.9794006588413204 | validation: 2.512339430973049]
	TIME [epoch: 9.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9590254170493193		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 1.9590254170493193 | validation: 2.489279339687715]
	TIME [epoch: 9.51 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9668604299170944		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 1.9668604299170944 | validation: 2.4863054511339318]
	TIME [epoch: 9.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9567541829062924		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 1.9567541829062924 | validation: 2.478243037027443]
	TIME [epoch: 9.49 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9551333898793786		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 1.9551333898793786 | validation: 2.5115428308831222]
	TIME [epoch: 9.49 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9508265742936906		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 1.9508265742936906 | validation: 2.499282703287047]
	TIME [epoch: 9.52 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9584106520652607		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 1.9584106520652607 | validation: 2.508435959289096]
	TIME [epoch: 9.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.967124428556644		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 1.967124428556644 | validation: 2.506380332043891]
	TIME [epoch: 9.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9558434903438922		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 1.9558434903438922 | validation: 2.4858983788235496]
	TIME [epoch: 9.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.964414978787941		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 1.964414978787941 | validation: 2.4906610508528253]
	TIME [epoch: 9.51 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9730732710997685		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 1.9730732710997685 | validation: 2.5014153207546164]
	TIME [epoch: 9.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9666709457628833		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 1.9666709457628833 | validation: 2.4912007433527306]
	TIME [epoch: 9.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.957190153121081		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 1.957190153121081 | validation: 2.4871510847825395]
	TIME [epoch: 9.49 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9638191178330437		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 1.9638191178330437 | validation: 2.491597824481605]
	TIME [epoch: 9.51 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9696306046507785		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 1.9696306046507785 | validation: 2.476063045201936]
	TIME [epoch: 9.49 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945745668023448		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 1.945745668023448 | validation: 2.497516809049776]
	TIME [epoch: 9.49 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9543940898268108		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 1.9543940898268108 | validation: 2.491870470622184]
	TIME [epoch: 9.49 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9607331387920066		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 1.9607331387920066 | validation: 2.5224311763170117]
	TIME [epoch: 9.51 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9545246265073613		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 1.9545246265073613 | validation: 2.566860258394476]
	TIME [epoch: 9.49 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9750501172317647		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 1.9750501172317647 | validation: 2.483255210059764]
	TIME [epoch: 9.49 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9588766138033855		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 1.9588766138033855 | validation: 2.497301802319179]
	TIME [epoch: 9.49 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9785139683761266		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 1.9785139683761266 | validation: 2.507759802250636]
	TIME [epoch: 9.51 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955694383329679		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 1.955694383329679 | validation: 2.496659172094857]
	TIME [epoch: 9.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9582813848309908		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 1.9582813848309908 | validation: 2.4960605384397536]
	TIME [epoch: 9.49 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9657208875387642		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 1.9657208875387642 | validation: 2.5164532442401653]
	TIME [epoch: 9.49 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9554750656719235		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 1.9554750656719235 | validation: 2.4856785371760424]
	TIME [epoch: 9.51 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9576513351230158		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 1.9576513351230158 | validation: 2.4950842546885643]
	TIME [epoch: 9.49 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9546540571075557		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 1.9546540571075557 | validation: 2.491061018984015]
	TIME [epoch: 9.49 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9608838875072245		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 1.9608838875072245 | validation: 2.5179724506639594]
	TIME [epoch: 9.49 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9764039097085526		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 1.9764039097085526 | validation: 2.490100506128604]
	TIME [epoch: 9.51 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9552844367610782		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 1.9552844367610782 | validation: 2.4923414402109736]
	TIME [epoch: 9.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9499874889146944		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 1.9499874889146944 | validation: 2.485085375101782]
	TIME [epoch: 9.49 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9590635301082464		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 1.9590635301082464 | validation: 2.514189718989235]
	TIME [epoch: 9.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9703176342177309		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 1.9703176342177309 | validation: 2.4928141210258246]
	TIME [epoch: 9.51 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9514342985996023		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 1.9514342985996023 | validation: 2.5111080417810068]
	TIME [epoch: 9.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954766987773762		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 1.954766987773762 | validation: 2.489734522686457]
	TIME [epoch: 9.49 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.960965669585058		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 1.960965669585058 | validation: 2.5085932673104008]
	TIME [epoch: 9.49 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9595499625053876		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 1.9595499625053876 | validation: 2.4898538869105495]
	TIME [epoch: 9.52 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9582912577036307		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 1.9582912577036307 | validation: 2.4890382316359583]
	TIME [epoch: 9.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9700306916926151		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 1.9700306916926151 | validation: 2.4972551032656605]
	TIME [epoch: 9.49 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9546803432897797		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 1.9546803432897797 | validation: 2.4813122106069416]
	TIME [epoch: 9.49 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9538076945563596		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 1.9538076945563596 | validation: 2.5245980003445148]
	TIME [epoch: 9.51 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9614374040883509		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 1.9614374040883509 | validation: 2.500618614630249]
	TIME [epoch: 9.49 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.963014634576724		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 1.963014634576724 | validation: 2.4880476814872408]
	TIME [epoch: 9.49 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9500373521606444		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 1.9500373521606444 | validation: 2.502589935854414]
	TIME [epoch: 9.5 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9587831805855718		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 1.9587831805855718 | validation: 2.4737156565564895]
	TIME [epoch: 9.51 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9566309790517535		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 1.9566309790517535 | validation: 2.4884232171965186]
	TIME [epoch: 9.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9465295989279146		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 1.9465295989279146 | validation: 2.496188182450654]
	TIME [epoch: 9.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9494180709866793		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 1.9494180709866793 | validation: 2.5052135211964726]
	TIME [epoch: 9.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9564650936406474		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 1.9564650936406474 | validation: 2.4919884658230833]
	TIME [epoch: 9.52 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9561791138110451		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 1.9561791138110451 | validation: 2.4900493614619403]
	TIME [epoch: 9.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9560918501486402		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 1.9560918501486402 | validation: 2.4934201268068152]
	TIME [epoch: 9.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9465525272552298		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 1.9465525272552298 | validation: 2.5363830679077495]
	TIME [epoch: 9.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9568077950954244		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 1.9568077950954244 | validation: 2.525563665947522]
	TIME [epoch: 9.52 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9658767187321786		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 1.9658767187321786 | validation: 2.492888175059548]
	TIME [epoch: 9.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9571094722860292		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 1.9571094722860292 | validation: 2.4839534282296847]
	TIME [epoch: 9.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9578429605171923		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 1.9578429605171923 | validation: 2.5107177972497072]
	TIME [epoch: 9.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9735474925220546		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 1.9735474925220546 | validation: 2.489857136627714]
	TIME [epoch: 9.51 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9468448264793365		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 1.9468448264793365 | validation: 2.4865989750764217]
	TIME [epoch: 9.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9569941364938912		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 1.9569941364938912 | validation: 2.4865125444689475]
	TIME [epoch: 9.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948528582436619		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 1.948528582436619 | validation: 2.5080368983546855]
	TIME [epoch: 9.49 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9607654165828268		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 1.9607654165828268 | validation: 2.4979051849852847]
	TIME [epoch: 9.51 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9490374986691148		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 1.9490374986691148 | validation: 2.4875076493860973]
	TIME [epoch: 9.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954842042875367		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 1.954842042875367 | validation: 2.4847292220709756]
	TIME [epoch: 9.49 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9540414113582696		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 1.9540414113582696 | validation: 2.498205705314449]
	TIME [epoch: 9.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.970855163857042		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 1.970855163857042 | validation: 2.557008339159389]
	TIME [epoch: 9.51 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.977928010893675		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 1.977928010893675 | validation: 2.491959446029824]
	TIME [epoch: 9.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9520046338845662		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 1.9520046338845662 | validation: 2.4817963907062213]
	TIME [epoch: 9.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955702508261183		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 1.955702508261183 | validation: 2.4849121885806436]
	TIME [epoch: 9.49 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9583926062205201		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 1.9583926062205201 | validation: 2.4857345282269083]
	TIME [epoch: 9.51 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.95661171650971		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 1.95661171650971 | validation: 2.4899745040891883]
	TIME [epoch: 9.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9551256917587345		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 1.9551256917587345 | validation: 2.4964857746223044]
	TIME [epoch: 9.49 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.965685008035242		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 1.965685008035242 | validation: 2.519346482647357]
	TIME [epoch: 9.49 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9483535992653487		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 1.9483535992653487 | validation: 2.491807539234203]
	TIME [epoch: 9.51 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9532561152278851		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 1.9532561152278851 | validation: 2.480090881855273]
	TIME [epoch: 9.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9594894003614445		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 1.9594894003614445 | validation: 2.510190521467294]
	TIME [epoch: 9.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9617248752282588		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 1.9617248752282588 | validation: 2.492550091600802]
	TIME [epoch: 9.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9576166518670288		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 1.9576166518670288 | validation: 2.4766711479150048]
	TIME [epoch: 9.51 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9561443473300948		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 1.9561443473300948 | validation: 2.4944132042351845]
	TIME [epoch: 9.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.965961623562584		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 1.965961623562584 | validation: 2.5053548043987037]
	TIME [epoch: 9.49 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9631187708345323		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 1.9631187708345323 | validation: 2.4910961004067786]
	TIME [epoch: 9.49 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9600959469570498		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 1.9600959469570498 | validation: 2.4778174843889182]
	TIME [epoch: 9.51 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.960231687151386		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 1.960231687151386 | validation: 2.498272611399157]
	TIME [epoch: 9.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.963382346655313		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 1.963382346655313 | validation: 2.496958429851608]
	TIME [epoch: 9.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9504710247928583		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 1.9504710247928583 | validation: 2.4968711242732646]
	TIME [epoch: 9.49 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9529946714502118		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 1.9529946714502118 | validation: 2.501509731800863]
	TIME [epoch: 9.51 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9523599674312222		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 1.9523599674312222 | validation: 2.48782886378003]
	TIME [epoch: 9.49 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9566259448329688		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 1.9566259448329688 | validation: 2.492059347398216]
	TIME [epoch: 9.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9550141492244322		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 1.9550141492244322 | validation: 2.4922237017717404]
	TIME [epoch: 9.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954666499064676		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 1.954666499064676 | validation: 2.5015348066046155]
	TIME [epoch: 9.52 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9553212787448817		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 1.9553212787448817 | validation: 2.5050405984355204]
	TIME [epoch: 9.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9544423274985487		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 1.9544423274985487 | validation: 2.487552059533198]
	TIME [epoch: 9.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9543333182868419		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 1.9543333182868419 | validation: 2.4872185691387143]
	TIME [epoch: 9.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9508316641266916		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 1.9508316641266916 | validation: 2.5039958267901032]
	TIME [epoch: 9.51 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9455910952026945		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 1.9455910952026945 | validation: 2.4949082492508077]
	TIME [epoch: 9.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9576798690389805		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 1.9576798690389805 | validation: 2.4885933584738575]
	TIME [epoch: 9.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9523682928435875		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 1.9523682928435875 | validation: 2.497012018931707]
	TIME [epoch: 9.49 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9523333598078019		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 1.9523333598078019 | validation: 2.4885845521879437]
	TIME [epoch: 9.51 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9615529352044916		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 1.9615529352044916 | validation: 2.4972051716409243]
	TIME [epoch: 9.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9491614851547212		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 1.9491614851547212 | validation: 2.490228059508989]
	TIME [epoch: 9.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9563438871345646		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 1.9563438871345646 | validation: 2.5063462285300195]
	TIME [epoch: 9.49 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.956430494511307		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 1.956430494511307 | validation: 2.5023428275949224]
	TIME [epoch: 9.51 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9565994970673473		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 1.9565994970673473 | validation: 2.501544181192064]
	TIME [epoch: 9.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9626886293936103		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 1.9626886293936103 | validation: 2.489637248391606]
	TIME [epoch: 9.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9497008245441776		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 1.9497008245441776 | validation: 2.4835029362072296]
	TIME [epoch: 9.49 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9548384699418713		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 1.9548384699418713 | validation: 2.4827059217005014]
	TIME [epoch: 9.51 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9615042185959122		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 1.9615042185959122 | validation: 2.490948274810295]
	TIME [epoch: 9.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9505449003250477		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 1.9505449003250477 | validation: 2.490838099379671]
	TIME [epoch: 9.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9637878965654352		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 1.9637878965654352 | validation: 2.500259812383193]
	TIME [epoch: 9.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9597380466628054		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 1.9597380466628054 | validation: 2.4987926830703304]
	TIME [epoch: 9.51 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9730449085308153		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 1.9730449085308153 | validation: 2.515916162703611]
	TIME [epoch: 9.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9653783008353898		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 1.9653783008353898 | validation: 2.495624645010562]
	TIME [epoch: 9.49 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428926819866983		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 1.9428926819866983 | validation: 2.498609166742453]
	TIME [epoch: 9.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9477744354886286		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 1.9477744354886286 | validation: 2.497879096784761]
	TIME [epoch: 9.51 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467045681832147		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 1.9467045681832147 | validation: 2.509074590031592]
	TIME [epoch: 9.51 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9469279981385483		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 1.9469279981385483 | validation: 2.485989290144759]
	TIME [epoch: 9.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.951953761294483		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 1.951953761294483 | validation: 2.4967471061786566]
	TIME [epoch: 9.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444905462178834		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 1.9444905462178834 | validation: 2.4895103171049486]
	TIME [epoch: 9.51 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9537457891583152		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 1.9537457891583152 | validation: 2.4880701655647885]
	TIME [epoch: 9.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9580609959887283		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 1.9580609959887283 | validation: 2.5506693669693328]
	TIME [epoch: 9.49 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9757034431025609		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 1.9757034431025609 | validation: 2.4921813586398374]
	TIME [epoch: 9.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9484905279467217		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 1.9484905279467217 | validation: 2.51166246648928]
	TIME [epoch: 9.51 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9506188228012111		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 1.9506188228012111 | validation: 2.495424145095664]
	TIME [epoch: 9.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9466864192428666		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 1.9466864192428666 | validation: 2.50385764644864]
	TIME [epoch: 9.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9516254658195513		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 1.9516254658195513 | validation: 2.472112478779704]
	TIME [epoch: 9.49 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9480878220545386		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 1.9480878220545386 | validation: 2.481233460863564]
	TIME [epoch: 9.51 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9511657459730753		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 1.9511657459730753 | validation: 2.502384502680555]
	TIME [epoch: 9.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9606274271915354		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 1.9606274271915354 | validation: 2.474988374882276]
	TIME [epoch: 9.49 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9609874315626385		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 1.9609874315626385 | validation: 2.5182591358771105]
	TIME [epoch: 9.49 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9481254123040859		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 1.9481254123040859 | validation: 2.477147209058656]
	TIME [epoch: 9.51 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9561538373300522		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 1.9561538373300522 | validation: 2.502499719885988]
	TIME [epoch: 9.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9590808609631605		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 1.9590808609631605 | validation: 2.490920483441852]
	TIME [epoch: 9.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9596329995906843		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 1.9596329995906843 | validation: 2.503155262562167]
	TIME [epoch: 9.49 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.958827986267457		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 1.958827986267457 | validation: 2.5059626988067025]
	TIME [epoch: 9.51 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9639984781796098		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 1.9639984781796098 | validation: 2.4773260892708264]
	TIME [epoch: 9.49 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9605965436303898		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 1.9605965436303898 | validation: 2.490335616103985]
	TIME [epoch: 9.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9540101159482532		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 1.9540101159482532 | validation: 2.4817718653529823]
	TIME [epoch: 9.49 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9628475848462963		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 1.9628475848462963 | validation: 2.4688041364260966]
	TIME [epoch: 9.51 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1242.pth
	Model improved!!!
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467066833560847		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 1.9467066833560847 | validation: 2.469289722455101]
	TIME [epoch: 9.49 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9549399079119696		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 1.9549399079119696 | validation: 2.499827312972679]
	TIME [epoch: 9.49 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948101350321949		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 1.948101350321949 | validation: 2.497008019067859]
	TIME [epoch: 9.49 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9567025750230567		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 1.9567025750230567 | validation: 2.5019015635053603]
	TIME [epoch: 9.51 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9634064389437218		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 1.9634064389437218 | validation: 2.5060010608130026]
	TIME [epoch: 9.49 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9533660396611192		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 1.9533660396611192 | validation: 2.49008443367242]
	TIME [epoch: 9.49 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9518216238799244		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 1.9518216238799244 | validation: 2.4804653730715494]
	TIME [epoch: 9.49 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94708427318424		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 1.94708427318424 | validation: 2.485894696808529]
	TIME [epoch: 9.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.951268511928388		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 1.951268511928388 | validation: 2.5244641390039515]
	TIME [epoch: 9.49 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9660796637613622		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 1.9660796637613622 | validation: 2.4842214892763272]
	TIME [epoch: 9.49 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9514587941649766		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 1.9514587941649766 | validation: 2.4997154035557547]
	TIME [epoch: 9.49 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9556548627137968		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 1.9556548627137968 | validation: 2.475936478034996]
	TIME [epoch: 9.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9531947746519893		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 1.9531947746519893 | validation: 2.503427716680875]
	TIME [epoch: 9.5 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9436220739739887		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 1.9436220739739887 | validation: 2.4859123451918634]
	TIME [epoch: 9.47 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955944035233441		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 1.955944035233441 | validation: 2.5061837253468413]
	TIME [epoch: 9.48 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9439569862684454		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 1.9439569862684454 | validation: 2.4949481673877116]
	TIME [epoch: 9.5 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9528036285010202		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 1.9528036285010202 | validation: 2.5007740675860783]
	TIME [epoch: 9.49 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9561632647562988		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 1.9561632647562988 | validation: 2.4901236642209277]
	TIME [epoch: 9.48 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949496244549413		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 1.949496244549413 | validation: 2.4951534604842376]
	TIME [epoch: 9.49 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9502214886744604		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 1.9502214886744604 | validation: 2.494360731066572]
	TIME [epoch: 9.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460581565599078		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 1.9460581565599078 | validation: 2.4883795986509685]
	TIME [epoch: 9.48 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9559227055533999		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 1.9559227055533999 | validation: 2.5170571929336556]
	TIME [epoch: 9.48 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433382598443452		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 1.9433382598443452 | validation: 2.5047715142790854]
	TIME [epoch: 9.49 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9549891952087133		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 1.9549891952087133 | validation: 2.4946094758410418]
	TIME [epoch: 9.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9512281591611882		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 1.9512281591611882 | validation: 2.498218801442561]
	TIME [epoch: 9.49 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9510500811744265		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 1.9510500811744265 | validation: 2.477640397263709]
	TIME [epoch: 9.49 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9525545499866006		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 1.9525545499866006 | validation: 2.488512641770681]
	TIME [epoch: 9.47 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954526249117587		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 1.954526249117587 | validation: 2.493672525884789]
	TIME [epoch: 9.49 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9480392515462448		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 1.9480392515462448 | validation: 2.497212572216726]
	TIME [epoch: 9.49 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9506192804980746		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 1.9506192804980746 | validation: 2.4773537657763907]
	TIME [epoch: 9.49 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.950673227050423		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 1.950673227050423 | validation: 2.5155400658477167]
	TIME [epoch: 9.49 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9549461707843587		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 1.9549461707843587 | validation: 2.496788847002225]
	TIME [epoch: 9.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498521327289993		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 1.9498521327289993 | validation: 2.4801608184889905]
	TIME [epoch: 9.47 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9606088924430878		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 1.9606088924430878 | validation: 2.504877948664117]
	TIME [epoch: 9.48 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9437088838991623		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 1.9437088838991623 | validation: 2.496585547756052]
	TIME [epoch: 9.49 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9510319628194872		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 1.9510319628194872 | validation: 2.4939332352697208]
	TIME [epoch: 9.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9550899803991997		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 1.9550899803991997 | validation: 2.5207584243155137]
	TIME [epoch: 9.49 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9598877527010774		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 1.9598877527010774 | validation: 2.501404763276312]
	TIME [epoch: 9.48 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9579376716560077		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 1.9579376716560077 | validation: 2.4946839676303854]
	TIME [epoch: 9.48 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9505291882916116		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 1.9505291882916116 | validation: 2.4882344064204944]
	TIME [epoch: 9.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9524341223293356		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 1.9524341223293356 | validation: 2.498037542579057]
	TIME [epoch: 9.49 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948859642538261		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 1.948859642538261 | validation: 2.500856851146462]
	TIME [epoch: 9.48 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9491701915316155		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 1.9491701915316155 | validation: 2.472689616389628]
	TIME [epoch: 9.49 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9490102043264304		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 1.9490102043264304 | validation: 2.4937521765879915]
	TIME [epoch: 9.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9516529389043527		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 1.9516529389043527 | validation: 2.485072032015113]
	TIME [epoch: 9.49 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9435704905859843		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 1.9435704905859843 | validation: 2.482614435582292]
	TIME [epoch: 9.49 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9465283112322063		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 1.9465283112322063 | validation: 2.499996343915065]
	TIME [epoch: 9.49 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9528369091331221		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 1.9528369091331221 | validation: 2.4901272732520345]
	TIME [epoch: 9.51 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9412984087898715		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 1.9412984087898715 | validation: 2.492625973446563]
	TIME [epoch: 9.49 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9515772449609223		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 1.9515772449609223 | validation: 2.481654806033701]
	TIME [epoch: 9.48 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9584942291896934		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 1.9584942291896934 | validation: 2.496792654929464]
	TIME [epoch: 9.48 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946623601659385		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 1.946623601659385 | validation: 2.505819883373417]
	TIME [epoch: 9.51 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9514925287590086		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 1.9514925287590086 | validation: 2.4979827541147337]
	TIME [epoch: 9.5 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947601823516552		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 1.947601823516552 | validation: 2.5005879537708293]
	TIME [epoch: 9.49 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9497914661221625		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 1.9497914661221625 | validation: 2.5088711366149785]
	TIME [epoch: 9.49 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9642163698702972		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 1.9642163698702972 | validation: 2.5148316988517307]
	TIME [epoch: 9.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.950542179342752		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 1.950542179342752 | validation: 2.4992025960156106]
	TIME [epoch: 9.49 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9510217369582172		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 1.9510217369582172 | validation: 2.4664929887314146]
	TIME [epoch: 9.48 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1300.pth
	Model improved!!!
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.951976755521676		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 1.951976755521676 | validation: 2.493521916068389]
	TIME [epoch: 9.48 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9509383639270765		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 1.9509383639270765 | validation: 2.480344919745536]
	TIME [epoch: 9.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9497289391847505		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 1.9497289391847505 | validation: 2.502428820252535]
	TIME [epoch: 9.48 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9576854476976244		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 1.9576854476976244 | validation: 2.486430080210482]
	TIME [epoch: 9.48 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498953839202862		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 1.9498953839202862 | validation: 2.4895561718585615]
	TIME [epoch: 9.48 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955872054842461		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 1.955872054842461 | validation: 2.497364359719192]
	TIME [epoch: 9.51 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940109962541181		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 1.940109962541181 | validation: 2.485601417755686]
	TIME [epoch: 9.49 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9435189139123892		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 1.9435189139123892 | validation: 2.48463079065586]
	TIME [epoch: 9.49 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940883103638598		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 1.940883103638598 | validation: 2.480370473188399]
	TIME [epoch: 9.48 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9471994577091665		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 1.9471994577091665 | validation: 2.4901184975567086]
	TIME [epoch: 9.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9476247840664123		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 1.9476247840664123 | validation: 2.5004677961945396]
	TIME [epoch: 9.49 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9488453217969837		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 1.9488453217969837 | validation: 2.515764370054046]
	TIME [epoch: 9.48 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9567741345484362		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 1.9567741345484362 | validation: 2.4841695229553062]
	TIME [epoch: 9.48 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461275034087155		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 1.9461275034087155 | validation: 2.4898226119559763]
	TIME [epoch: 9.51 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9495676886081483		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 1.9495676886081483 | validation: 2.4795496645881148]
	TIME [epoch: 9.49 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9529963438731255		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 1.9529963438731255 | validation: 2.47697974441309]
	TIME [epoch: 9.48 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954344752692815		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 1.954344752692815 | validation: 2.4697224753382194]
	TIME [epoch: 9.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9489183390218243		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 1.9489183390218243 | validation: 2.4936696919071397]
	TIME [epoch: 9.49 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947631637936086		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 1.947631637936086 | validation: 2.4817752005494738]
	TIME [epoch: 9.48 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9528949141259666		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 1.9528949141259666 | validation: 2.5008670483437254]
	TIME [epoch: 9.48 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498319162786308		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 1.9498319162786308 | validation: 2.49631884554396]
	TIME [epoch: 9.49 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9492015128726066		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 1.9492015128726066 | validation: 2.5112071690640243]
	TIME [epoch: 9.51 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9589232473738023		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 1.9589232473738023 | validation: 2.509589198037666]
	TIME [epoch: 9.49 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9425059774692701		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 1.9425059774692701 | validation: 2.501588887462245]
	TIME [epoch: 9.48 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9503143599360637		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 1.9503143599360637 | validation: 2.481212146443412]
	TIME [epoch: 9.49 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9412663949246245		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 1.9412663949246245 | validation: 2.4877902181942892]
	TIME [epoch: 9.49 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947558270436526		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 1.947558270436526 | validation: 2.4782534765233994]
	TIME [epoch: 9.48 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9476144974030878		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 1.9476144974030878 | validation: 2.4980585253733016]
	TIME [epoch: 9.48 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9625314980026598		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 1.9625314980026598 | validation: 2.500037764471762]
	TIME [epoch: 9.48 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9452228344528535		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 1.9452228344528535 | validation: 2.4921638541376074]
	TIME [epoch: 9.51 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9580918357416266		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 1.9580918357416266 | validation: 2.481262911462634]
	TIME [epoch: 9.49 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9489108932652364		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 1.9489108932652364 | validation: 2.500131798694122]
	TIME [epoch: 9.49 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9539387924032574		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 1.9539387924032574 | validation: 2.483722564400158]
	TIME [epoch: 9.48 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9517092544020411		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 1.9517092544020411 | validation: 2.4871787784362125]
	TIME [epoch: 9.51 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942776815254763		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 1.942776815254763 | validation: 2.48089742000046]
	TIME [epoch: 9.49 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498974391687998		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 1.9498974391687998 | validation: 2.4726441629330047]
	TIME [epoch: 9.49 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.954249353322514		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 1.954249353322514 | validation: 2.493610256470447]
	TIME [epoch: 9.48 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.952183233297341		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 1.952183233297341 | validation: 2.5016609311032396]
	TIME [epoch: 9.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9531742017341798		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 1.9531742017341798 | validation: 2.4942915730482897]
	TIME [epoch: 9.49 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9530058552388716		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 1.9530058552388716 | validation: 2.476309703338776]
	TIME [epoch: 9.49 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9505969341444782		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 1.9505969341444782 | validation: 2.469778355834648]
	TIME [epoch: 9.49 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9548930749900664		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 1.9548930749900664 | validation: 2.4681736534887153]
	TIME [epoch: 9.51 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394350571418433		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 1.9394350571418433 | validation: 2.487147533328905]
	TIME [epoch: 9.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9482891590959817		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 1.9482891590959817 | validation: 2.470097385057955]
	TIME [epoch: 9.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.951775880962493		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 1.951775880962493 | validation: 2.5079219871895]
	TIME [epoch: 9.49 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947744541290735		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 1.947744541290735 | validation: 2.5112144246199954]
	TIME [epoch: 9.51 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9742434766726114		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 1.9742434766726114 | validation: 2.498713582178327]
	TIME [epoch: 9.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9413519284587615		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 1.9413519284587615 | validation: 2.506261220867864]
	TIME [epoch: 9.48 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.953357124842049		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 1.953357124842049 | validation: 2.4706989187126864]
	TIME [epoch: 9.48 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943261946215424		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 1.943261946215424 | validation: 2.4844360633904876]
	TIME [epoch: 9.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9592995694935613		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 1.9592995694935613 | validation: 2.479137047774422]
	TIME [epoch: 9.49 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945579226941621		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 1.945579226941621 | validation: 2.487473664775441]
	TIME [epoch: 9.49 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9470482338420456		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 1.9470482338420456 | validation: 2.4960391125220567]
	TIME [epoch: 9.48 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421085609596678		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 1.9421085609596678 | validation: 2.474170519902438]
	TIME [epoch: 9.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94041361165023		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 1.94041361165023 | validation: 2.484857267041846]
	TIME [epoch: 9.48 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443754547993908		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 1.9443754547993908 | validation: 2.4902938639067305]
	TIME [epoch: 9.48 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9470296591416336		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 1.9470296591416336 | validation: 2.474181152846505]
	TIME [epoch: 9.47 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9488850955885724		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 1.9488850955885724 | validation: 2.4929720595668274]
	TIME [epoch: 9.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9425115914326838		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 1.9425115914326838 | validation: 2.472447630427965]
	TIME [epoch: 9.48 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9633423594067891		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 1.9633423594067891 | validation: 2.496192752267177]
	TIME [epoch: 9.48 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9489119823183276		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 1.9489119823183276 | validation: 2.4930334787312884]
	TIME [epoch: 9.48 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9472298919389437		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 1.9472298919389437 | validation: 2.492295350705042]
	TIME [epoch: 9.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.953008133669297		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 1.953008133669297 | validation: 2.4968236820756893]
	TIME [epoch: 9.49 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9503859761145272		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 1.9503859761145272 | validation: 2.4875220864642906]
	TIME [epoch: 9.48 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461679330030832		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 1.9461679330030832 | validation: 2.498312022079556]
	TIME [epoch: 9.48 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9415432449336614		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 1.9415432449336614 | validation: 2.474951094169888]
	TIME [epoch: 9.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9496729347944217		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 1.9496729347944217 | validation: 2.4951582563550323]
	TIME [epoch: 9.49 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946030210842536		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 1.946030210842536 | validation: 2.4867263947783806]
	TIME [epoch: 9.49 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460976774099108		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 1.9460976774099108 | validation: 2.4810930001670184]
	TIME [epoch: 9.49 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9513835879670107		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 1.9513835879670107 | validation: 2.470531999939815]
	TIME [epoch: 9.49 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9466732590036266		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 1.9466732590036266 | validation: 2.474662213788816]
	TIME [epoch: 9.49 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9517501377261097		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 1.9517501377261097 | validation: 2.502712051632605]
	TIME [epoch: 9.48 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9566846073492212		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 1.9566846073492212 | validation: 2.5164670656055566]
	TIME [epoch: 9.48 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.968396052961158		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 1.968396052961158 | validation: 2.5192819379512685]
	TIME [epoch: 9.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9536395458649713		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 1.9536395458649713 | validation: 2.4914722285637287]
	TIME [epoch: 9.49 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9442251184426316		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 1.9442251184426316 | validation: 2.488013991831019]
	TIME [epoch: 9.49 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9452469125534442		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 1.9452469125534442 | validation: 2.4785300794140426]
	TIME [epoch: 9.49 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.957012400923405		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 1.957012400923405 | validation: 2.491670478100221]
	TIME [epoch: 9.51 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9440625449934594		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 1.9440625449934594 | validation: 2.491036321351189]
	TIME [epoch: 9.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9456541969554384		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 1.9456541969554384 | validation: 2.4963890615188267]
	TIME [epoch: 9.48 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9514316484645362		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 1.9514316484645362 | validation: 2.470392005799092]
	TIME [epoch: 9.48 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9503379528450089		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 1.9503379528450089 | validation: 2.4923262183537096]
	TIME [epoch: 9.49 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9501831748648482		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 1.9501831748648482 | validation: 2.4789278590963977]
	TIME [epoch: 9.49 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9459003053063075		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 1.9459003053063075 | validation: 2.489393027035149]
	TIME [epoch: 9.49 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9505017778887948		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 1.9505017778887948 | validation: 2.4945206423300403]
	TIME [epoch: 9.49 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9544460398312888		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 1.9544460398312888 | validation: 2.474632429190099]
	TIME [epoch: 9.49 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9478906749277152		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 1.9478906749277152 | validation: 2.4918661965800952]
	TIME [epoch: 9.49 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9423202110885385		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 1.9423202110885385 | validation: 2.491654251329942]
	TIME [epoch: 9.49 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946720731269935		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 1.946720731269935 | validation: 2.4856849566867174]
	TIME [epoch: 9.49 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9434725840745184		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 1.9434725840745184 | validation: 2.4950461153167316]
	TIME [epoch: 9.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943045393956855		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 1.943045393956855 | validation: 2.4922988370893244]
	TIME [epoch: 9.49 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9477137218191118		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 1.9477137218191118 | validation: 2.4969378336064474]
	TIME [epoch: 9.48 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9506696218330646		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 1.9506696218330646 | validation: 2.4824222191687886]
	TIME [epoch: 9.48 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946653099013059		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 1.946653099013059 | validation: 2.4915863705200203]
	TIME [epoch: 9.51 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946820462302395		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 1.946820462302395 | validation: 2.4859664081819712]
	TIME [epoch: 9.48 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.951024881701354		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 1.951024881701354 | validation: 2.500089004778388]
	TIME [epoch: 9.49 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9485216453905896		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 1.9485216453905896 | validation: 2.4815874478714073]
	TIME [epoch: 9.49 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943289620792085		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 1.943289620792085 | validation: 2.4946149038534142]
	TIME [epoch: 9.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428374451825559		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 1.9428374451825559 | validation: 2.5068535900909756]
	TIME [epoch: 9.49 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949154314086078		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 1.949154314086078 | validation: 2.5024760889499613]
	TIME [epoch: 9.49 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9499079944529907		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 1.9499079944529907 | validation: 2.4925660438802617]
	TIME [epoch: 9.5 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.950462506647466		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 1.950462506647466 | validation: 2.517804648281946]
	TIME [epoch: 9.51 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.956348157017467		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 1.956348157017467 | validation: 2.48252045960728]
	TIME [epoch: 9.49 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9431331842882051		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 1.9431331842882051 | validation: 2.492031745795221]
	TIME [epoch: 9.48 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94433528616186		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 1.94433528616186 | validation: 2.4916650089313768]
	TIME [epoch: 9.48 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9374536561232802		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 1.9374536561232802 | validation: 2.485580098991738]
	TIME [epoch: 9.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9422830881617474		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 1.9422830881617474 | validation: 2.4941693948533517]
	TIME [epoch: 9.48 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9447659022033403		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 1.9447659022033403 | validation: 2.481206380091528]
	TIME [epoch: 9.49 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9466703174722617		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 1.9466703174722617 | validation: 2.4881958287783728]
	TIME [epoch: 9.49 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487123374252682		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 1.9487123374252682 | validation: 2.4902426596832723]
	TIME [epoch: 9.52 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460800940804788		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 1.9460800940804788 | validation: 2.4976554421071633]
	TIME [epoch: 9.49 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9439885598818982		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 1.9439885598818982 | validation: 2.475455731251563]
	TIME [epoch: 9.48 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9472234337642864		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 1.9472234337642864 | validation: 2.499311188866268]
	TIME [epoch: 9.49 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9506946925658848		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 1.9506946925658848 | validation: 2.5155997145115885]
	TIME [epoch: 9.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467865431241953		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 1.9467865431241953 | validation: 2.5117481517643063]
	TIME [epoch: 9.48 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9560623005885833		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 1.9560623005885833 | validation: 2.4855297694137146]
	TIME [epoch: 9.49 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9441591194395706		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 1.9441591194395706 | validation: 2.4833218352721325]
	TIME [epoch: 9.48 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.950839078014792		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 1.950839078014792 | validation: 2.5004325449511184]
	TIME [epoch: 9.5 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9422870567559574		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 1.9422870567559574 | validation: 2.485404901973491]
	TIME [epoch: 9.49 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9458983910852314		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 1.9458983910852314 | validation: 2.4812510108682746]
	TIME [epoch: 9.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9474195353412327		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 1.9474195353412327 | validation: 2.485926786634116]
	TIME [epoch: 9.49 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9479713744503542		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 1.9479713744503542 | validation: 2.4791185180022173]
	TIME [epoch: 9.51 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.950789769956699		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 1.950789769956699 | validation: 2.4801651272017025]
	TIME [epoch: 9.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498537102138642		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 1.9498537102138642 | validation: 2.513829452853611]
	TIME [epoch: 9.49 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.958198398906561		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 1.958198398906561 | validation: 2.498755641192484]
	TIME [epoch: 9.49 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9508822614250725		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 1.9508822614250725 | validation: 2.500323468310301]
	TIME [epoch: 9.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9571772491086186		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 1.9571772491086186 | validation: 2.50513806592278]
	TIME [epoch: 9.49 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945051767108089		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 1.945051767108089 | validation: 2.4959873660212026]
	TIME [epoch: 9.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9402917800766488		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 1.9402917800766488 | validation: 2.4978723990420293]
	TIME [epoch: 9.49 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9418357727766		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 1.9418357727766 | validation: 2.4831742288885232]
	TIME [epoch: 9.51 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9408179875586167		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 1.9408179875586167 | validation: 2.474679113178347]
	TIME [epoch: 9.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9466860257419192		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 1.9466860257419192 | validation: 2.4883348768272304]
	TIME [epoch: 9.49 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9479771019198993		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 1.9479771019198993 | validation: 2.4803342284448244]
	TIME [epoch: 9.49 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944747710921342		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 1.944747710921342 | validation: 2.4931610020583936]
	TIME [epoch: 9.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.95135548603091		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 1.95135548603091 | validation: 2.4985581459324973]
	TIME [epoch: 9.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433421831633475		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 1.9433421831633475 | validation: 2.4903520442287657]
	TIME [epoch: 9.48 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9422613771892867		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 1.9422613771892867 | validation: 2.480527900747499]
	TIME [epoch: 9.48 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9401874451727021		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 1.9401874451727021 | validation: 2.488726454632541]
	TIME [epoch: 9.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9535711439563475		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 1.9535711439563475 | validation: 2.502915069368983]
	TIME [epoch: 9.48 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9576928437388126		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 1.9576928437388126 | validation: 2.4992801167557928]
	TIME [epoch: 9.49 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9538952849091427		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 1.9538952849091427 | validation: 2.4961686008707495]
	TIME [epoch: 9.49 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9527488013762226		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 1.9527488013762226 | validation: 2.5039568376717023]
	TIME [epoch: 9.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9617005108559962		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 1.9617005108559962 | validation: 2.4795877408063114]
	TIME [epoch: 9.49 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433668724969373		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 1.9433668724969373 | validation: 2.504583332200823]
	TIME [epoch: 9.48 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9457719064616668		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 1.9457719064616668 | validation: 2.5037404356158994]
	TIME [epoch: 9.49 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947884968142748		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 1.947884968142748 | validation: 2.496632609708914]
	TIME [epoch: 9.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9454987370232395		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 1.9454987370232395 | validation: 2.481503353988892]
	TIME [epoch: 9.49 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9480461951935648		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 1.9480461951935648 | validation: 2.494663147552418]
	TIME [epoch: 9.49 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946096682451813		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 1.946096682451813 | validation: 2.48415037932433]
	TIME [epoch: 9.49 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9400341114920192		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 1.9400341114920192 | validation: 2.500991023627011]
	TIME [epoch: 9.5 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9573178699768483		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 1.9573178699768483 | validation: 2.5069193544072363]
	TIME [epoch: 9.49 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461514944557752		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 1.9461514944557752 | validation: 2.487957192941242]
	TIME [epoch: 9.48 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9483940711471028		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 1.9483940711471028 | validation: 2.487271107801095]
	TIME [epoch: 9.49 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9500228201339298		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 1.9500228201339298 | validation: 2.4774582480745844]
	TIME [epoch: 9.51 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9531621003022248		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 1.9531621003022248 | validation: 2.5032981091834303]
	TIME [epoch: 9.49 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9541581840812845		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 1.9541581840812845 | validation: 2.490313016836324]
	TIME [epoch: 9.49 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9457699993022064		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 1.9457699993022064 | validation: 2.475711455009582]
	TIME [epoch: 9.47 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9385963512517073		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 1.9385963512517073 | validation: 2.4757262779629463]
	TIME [epoch: 9.5 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9515686148758404		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 1.9515686148758404 | validation: 2.4894174312420767]
	TIME [epoch: 9.49 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9458548287475033		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 1.9458548287475033 | validation: 2.4910574960872665]
	TIME [epoch: 9.49 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.952225195869067		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 1.952225195869067 | validation: 2.467034643047331]
	TIME [epoch: 9.48 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9448200495656542		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 1.9448200495656542 | validation: 2.482299085539205]
	TIME [epoch: 9.5 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9387759364353634		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 1.9387759364353634 | validation: 2.4879613703170076]
	TIME [epoch: 9.49 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948500679610597		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 1.948500679610597 | validation: 2.473467904332857]
	TIME [epoch: 9.47 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9494806894864865		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 1.9494806894864865 | validation: 2.490579659955117]
	TIME [epoch: 9.49 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9530553190244397		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 1.9530553190244397 | validation: 2.4783144328598463]
	TIME [epoch: 9.5 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955530982664945		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 1.955530982664945 | validation: 2.4831151184721136]
	TIME [epoch: 9.48 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467392984453535		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 1.9467392984453535 | validation: 2.481406681522377]
	TIME [epoch: 9.48 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9510161946030078		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 1.9510161946030078 | validation: 2.484138533193318]
	TIME [epoch: 9.48 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487899253233363		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 1.9487899253233363 | validation: 2.4707371283153785]
	TIME [epoch: 9.5 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.935425025302933		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 1.935425025302933 | validation: 2.4787658097228435]
	TIME [epoch: 9.48 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946052523849056		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 1.946052523849056 | validation: 2.495593197939899]
	TIME [epoch: 9.49 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948253414511618		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 1.948253414511618 | validation: 2.491977014276588]
	TIME [epoch: 9.49 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9538510033759668		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 1.9538510033759668 | validation: 2.5001277470910486]
	TIME [epoch: 9.5 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444986677400842		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 1.9444986677400842 | validation: 2.485283222633115]
	TIME [epoch: 9.5 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498703602846326		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 1.9498703602846326 | validation: 2.490081629323672]
	TIME [epoch: 9.49 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9504738838759352		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 1.9504738838759352 | validation: 2.4903833843757903]
	TIME [epoch: 9.49 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9484720531543058		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 1.9484720531543058 | validation: 2.495282494042478]
	TIME [epoch: 9.51 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9480064903117484		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 1.9480064903117484 | validation: 2.4927475026239767]
	TIME [epoch: 9.49 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946849476877998		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 1.946849476877998 | validation: 2.5075940076557335]
	TIME [epoch: 9.49 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9485336653316327		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 1.9485336653316327 | validation: 2.5007407403516706]
	TIME [epoch: 9.48 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9427616841228539		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 1.9427616841228539 | validation: 2.4840659617551824]
	TIME [epoch: 9.51 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944804906719023		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 1.944804906719023 | validation: 2.4852897263185776]
	TIME [epoch: 9.49 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9541622138414376		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 1.9541622138414376 | validation: 2.477224050315115]
	TIME [epoch: 9.48 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467749896781963		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 1.9467749896781963 | validation: 2.4896156831592617]
	TIME [epoch: 9.49 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943904643717459		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 1.943904643717459 | validation: 2.4936201443556416]
	TIME [epoch: 9.5 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9407166929207815		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 1.9407166929207815 | validation: 2.495808249865988]
	TIME [epoch: 9.48 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949406248249089		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 1.949406248249089 | validation: 2.5189189900686104]
	TIME [epoch: 9.48 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9620975385107742		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 1.9620975385107742 | validation: 2.5102075874758754]
	TIME [epoch: 9.48 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9558652414414504		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 1.9558652414414504 | validation: 2.484212420282517]
	TIME [epoch: 9.5 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9452053008222585		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 1.9452053008222585 | validation: 2.4954021523931313]
	TIME [epoch: 9.49 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9409024851915941		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 1.9409024851915941 | validation: 2.5005801327472947]
	TIME [epoch: 9.49 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487903916731444		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 1.9487903916731444 | validation: 2.4997819786997755]
	TIME [epoch: 9.49 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421236311932275		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 1.9421236311932275 | validation: 2.4866475628331224]
	TIME [epoch: 9.49 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9518629309698774		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 1.9518629309698774 | validation: 2.4958678301946033]
	TIME [epoch: 9.48 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9482932790237186		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 1.9482932790237186 | validation: 2.4873601997891184]
	TIME [epoch: 9.49 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9452103740926088		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 1.9452103740926088 | validation: 2.4889456176620253]
	TIME [epoch: 9.48 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943957103438212		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 1.943957103438212 | validation: 2.482895666968859]
	TIME [epoch: 9.51 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444710265781115		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 1.9444710265781115 | validation: 2.4813576306516985]
	TIME [epoch: 9.49 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9457977325701976		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 1.9457977325701976 | validation: 2.482078059744754]
	TIME [epoch: 9.48 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9477415427900204		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 1.9477415427900204 | validation: 2.507169263788657]
	TIME [epoch: 9.48 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9516361749021716		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 1.9516361749021716 | validation: 2.482372765256835]
	TIME [epoch: 9.53 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421344610697786		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 1.9421344610697786 | validation: 2.4887884758451095]
	TIME [epoch: 9.49 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9538757801770206		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 1.9538757801770206 | validation: 2.5027502794624703]
	TIME [epoch: 9.48 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9513561683917982		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 1.9513561683917982 | validation: 2.4862766442018085]
	TIME [epoch: 9.48 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.952346937533901		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 1.952346937533901 | validation: 2.4819337254294704]
	TIME [epoch: 9.51 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9453762924326121		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 1.9453762924326121 | validation: 2.485036650731877]
	TIME [epoch: 9.49 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433725267148092		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 1.9433725267148092 | validation: 2.4814984503558444]
	TIME [epoch: 9.48 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9494061166075798		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 1.9494061166075798 | validation: 2.499965873261675]
	TIME [epoch: 9.49 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9563566568820108		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 1.9563566568820108 | validation: 2.5052654423147724]
	TIME [epoch: 9.5 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9506658115099433		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 1.9506658115099433 | validation: 2.487218972274998]
	TIME [epoch: 9.49 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9463164344256647		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 1.9463164344256647 | validation: 2.4990408811983227]
	TIME [epoch: 9.49 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9522546012138047		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 1.9522546012138047 | validation: 2.486266934526085]
	TIME [epoch: 9.48 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9463092493268292		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 1.9463092493268292 | validation: 2.465799009859493]
	TIME [epoch: 9.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1514.pth
	Model improved!!!
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445376639107987		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 1.9445376639107987 | validation: 2.492087076318476]
	TIME [epoch: 9.48 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944326913849297		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 1.944326913849297 | validation: 2.491096765262834]
	TIME [epoch: 9.48 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9434950545421816		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 1.9434950545421816 | validation: 2.4892158730258704]
	TIME [epoch: 9.5 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9414666182516047		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 1.9414666182516047 | validation: 2.4939538685434837]
	TIME [epoch: 9.49 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445207170226761		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 1.9445207170226761 | validation: 2.4893608336957893]
	TIME [epoch: 9.48 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419623118180138		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 1.9419623118180138 | validation: 2.494611191703483]
	TIME [epoch: 9.48 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9415713966606862		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 1.9415713966606862 | validation: 2.4745790329468047]
	TIME [epoch: 9.48 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9449022765015993		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 1.9449022765015993 | validation: 2.4820491004875325]
	TIME [epoch: 9.5 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947152328548927		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 1.947152328548927 | validation: 2.4906957500630527]
	TIME [epoch: 9.47 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9468150069519923		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 1.9468150069519923 | validation: 2.4742396197310392]
	TIME [epoch: 9.47 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9426112384514074		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 1.9426112384514074 | validation: 2.4770125061550523]
	TIME [epoch: 9.48 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9496081529632014		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 1.9496081529632014 | validation: 2.476032921574624]
	TIME [epoch: 9.49 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939392819475049		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 1.939392819475049 | validation: 2.486314500205669]
	TIME [epoch: 9.48 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945305825496773		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 1.945305825496773 | validation: 2.4883726269369975]
	TIME [epoch: 9.48 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9508836428356553		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 1.9508836428356553 | validation: 2.483568330155733]
	TIME [epoch: 9.47 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943927237308491		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 1.943927237308491 | validation: 2.493886857546825]
	TIME [epoch: 9.49 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947121357806417		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 1.947121357806417 | validation: 2.4784287755241428]
	TIME [epoch: 9.49 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433378859288504		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 1.9433378859288504 | validation: 2.4924259681297736]
	TIME [epoch: 9.48 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948750769611226		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 1.948750769611226 | validation: 2.4921840161514557]
	TIME [epoch: 9.48 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394111769005513		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 1.9394111769005513 | validation: 2.486526220770004]
	TIME [epoch: 9.5 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945953995712125		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 1.945953995712125 | validation: 2.48639144896335]
	TIME [epoch: 9.49 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9463242036375015		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 1.9463242036375015 | validation: 2.474852816199178]
	TIME [epoch: 9.47 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9469978796706582		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 1.9469978796706582 | validation: 2.480785682275106]
	TIME [epoch: 9.49 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9390812955819214		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 1.9390812955819214 | validation: 2.49911350577821]
	TIME [epoch: 9.5 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9400518015334256		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 1.9400518015334256 | validation: 2.491424635468797]
	TIME [epoch: 9.49 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428016660961394		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 1.9428016660961394 | validation: 2.4885685397423543]
	TIME [epoch: 9.48 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487380439368802		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 1.9487380439368802 | validation: 2.4925226468544426]
	TIME [epoch: 9.49 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9499976797201992		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 1.9499976797201992 | validation: 2.4718546348332184]
	TIME [epoch: 9.5 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949519409593996		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 1.949519409593996 | validation: 2.4779409867512006]
	TIME [epoch: 9.49 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940952016842651		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 1.940952016842651 | validation: 2.486332547891538]
	TIME [epoch: 9.48 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949957038127998		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 1.949957038127998 | validation: 2.4949278513161746]
	TIME [epoch: 9.49 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394646263259385		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 1.9394646263259385 | validation: 2.4906704058160973]
	TIME [epoch: 9.5 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9522434637248463		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 1.9522434637248463 | validation: 2.4984678947557604]
	TIME [epoch: 9.48 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942251349423584		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 1.942251349423584 | validation: 2.4971478492213026]
	TIME [epoch: 9.48 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943911708504562		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 1.943911708504562 | validation: 2.490052373691489]
	TIME [epoch: 9.48 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487894715890337		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 1.9487894715890337 | validation: 2.4754974428284346]
	TIME [epoch: 9.49 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9515183951974648		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 1.9515183951974648 | validation: 2.4952595024197546]
	TIME [epoch: 9.48 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944505017772833		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 1.944505017772833 | validation: 2.489093028894421]
	TIME [epoch: 9.48 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9469585178863746		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 1.9469585178863746 | validation: 2.4913151238974245]
	TIME [epoch: 9.47 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9398280120134301		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 1.9398280120134301 | validation: 2.476717733310723]
	TIME [epoch: 9.51 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9397551802855428		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 1.9397551802855428 | validation: 2.496053705306253]
	TIME [epoch: 9.5 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9496741355278082		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 1.9496741355278082 | validation: 2.474132574384677]
	TIME [epoch: 9.48 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9489289469525797		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 1.9489289469525797 | validation: 2.4803257233108353]
	TIME [epoch: 9.47 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9492683280242435		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 1.9492683280242435 | validation: 2.4879171160096467]
	TIME [epoch: 9.49 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9531757289085092		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 1.9531757289085092 | validation: 2.4785051020491875]
	TIME [epoch: 9.48 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461257628668118		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 1.9461257628668118 | validation: 2.4817444144143734]
	TIME [epoch: 9.49 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9518435025557306		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 1.9518435025557306 | validation: 2.4888362959622734]
	TIME [epoch: 9.48 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9414220442509447		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 1.9414220442509447 | validation: 2.4735767513843228]
	TIME [epoch: 9.5 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9466777949552356		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 1.9466777949552356 | validation: 2.4770060589675165]
	TIME [epoch: 9.49 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460990163490526		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 1.9460990163490526 | validation: 2.4806795341065064]
	TIME [epoch: 9.48 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9420496074711775		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 1.9420496074711775 | validation: 2.492625477266657]
	TIME [epoch: 9.48 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9411137102792906		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 1.9411137102792906 | validation: 2.4806623359033497]
	TIME [epoch: 9.5 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9459052886694956		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 1.9459052886694956 | validation: 2.4884968362423336]
	TIME [epoch: 9.5 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444417577041264		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 1.9444417577041264 | validation: 2.48775544826824]
	TIME [epoch: 9.48 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9522947779477118		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 1.9522947779477118 | validation: 2.4892873397048554]
	TIME [epoch: 9.49 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9475892398817252		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 1.9475892398817252 | validation: 2.4935964805650097]
	TIME [epoch: 9.51 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9477508191691737		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 1.9477508191691737 | validation: 2.476309701272055]
	TIME [epoch: 9.5 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9410253450439832		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 1.9410253450439832 | validation: 2.489767995966374]
	TIME [epoch: 9.47 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9425679071769324		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 1.9425679071769324 | validation: 2.4904510868246206]
	TIME [epoch: 9.49 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9473986744702536		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 1.9473986744702536 | validation: 2.4972689897598683]
	TIME [epoch: 9.5 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9434225969152408		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 1.9434225969152408 | validation: 2.4895698852901442]
	TIME [epoch: 9.48 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460944140316407		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 1.9460944140316407 | validation: 2.4663146928381776]
	TIME [epoch: 9.49 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433586710504276		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 1.9433586710504276 | validation: 2.4739587453949805]
	TIME [epoch: 9.48 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9436341858678627		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 1.9436341858678627 | validation: 2.4903971535544183]
	TIME [epoch: 9.5 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9410051516385056		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 1.9410051516385056 | validation: 2.4919291427078583]
	TIME [epoch: 9.5 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9477058998680163		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 1.9477058998680163 | validation: 2.4791152839632655]
	TIME [epoch: 9.49 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94283008324323		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 1.94283008324323 | validation: 2.499243541007695]
	TIME [epoch: 9.49 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9447835525428157		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 1.9447835525428157 | validation: 2.480550049103399]
	TIME [epoch: 9.51 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9415161317751468		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 1.9415161317751468 | validation: 2.4889127618626183]
	TIME [epoch: 9.5 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9472398873032595		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 1.9472398873032595 | validation: 2.493055844934286]
	TIME [epoch: 9.49 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943924195199385		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 1.943924195199385 | validation: 2.476566055032369]
	TIME [epoch: 9.49 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943218798318437		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 1.943218798318437 | validation: 2.4803828381283]
	TIME [epoch: 9.5 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9423311952737916		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 1.9423311952737916 | validation: 2.476432761157166]
	TIME [epoch: 9.49 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9550768201841175		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 1.9550768201841175 | validation: 2.4823435420509488]
	TIME [epoch: 9.48 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9480530463527574		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 1.9480530463527574 | validation: 2.495372340928449]
	TIME [epoch: 9.48 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9380252182233648		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 1.9380252182233648 | validation: 2.4792289174681397]
	TIME [epoch: 9.5 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947051095794451		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 1.947051095794451 | validation: 2.474420375170108]
	TIME [epoch: 9.49 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9426542010401449		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 1.9426542010401449 | validation: 2.487409114805866]
	TIME [epoch: 9.49 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949091938276851		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 1.949091938276851 | validation: 2.494156858861497]
	TIME [epoch: 9.49 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9425856551489133		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 1.9425856551489133 | validation: 2.4841658211227036]
	TIME [epoch: 9.51 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443923052563228		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 1.9443923052563228 | validation: 2.4715850959373884]
	TIME [epoch: 9.5 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9464513362351727		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 1.9464513362351727 | validation: 2.475607421470392]
	TIME [epoch: 9.49 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9490048476638413		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 1.9490048476638413 | validation: 2.4824051721512186]
	TIME [epoch: 9.5 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9362581766597475		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 1.9362581766597475 | validation: 2.482951873190151]
	TIME [epoch: 9.52 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944126912786341		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 1.944126912786341 | validation: 2.492439337447322]
	TIME [epoch: 9.49 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9384440784643702		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 1.9384440784643702 | validation: 2.484481656881489]
	TIME [epoch: 9.49 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9410341701277793		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 1.9410341701277793 | validation: 2.4746882050989973]
	TIME [epoch: 9.48 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9392197884012599		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 1.9392197884012599 | validation: 2.476520838236624]
	TIME [epoch: 9.51 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94878089503149		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 1.94878089503149 | validation: 2.486179510642781]
	TIME [epoch: 9.49 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9447865350602522		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 1.9447865350602522 | validation: 2.480300353420671]
	TIME [epoch: 9.49 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947835306207454		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 1.947835306207454 | validation: 2.496565419801458]
	TIME [epoch: 9.49 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9464688837982493		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 1.9464688837982493 | validation: 2.5017600190770417]
	TIME [epoch: 9.51 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444778912683318		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 1.9444778912683318 | validation: 2.480750469859891]
	TIME [epoch: 9.49 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9429213528412461		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 1.9429213528412461 | validation: 2.4905632879904114]
	TIME [epoch: 9.49 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9486193417693294		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 1.9486193417693294 | validation: 2.4897056514600737]
	TIME [epoch: 9.49 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444787035091884		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 1.9444787035091884 | validation: 2.4871409895417975]
	TIME [epoch: 9.5 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9515299563281332		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 1.9515299563281332 | validation: 2.484543598847785]
	TIME [epoch: 9.48 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943722991275623		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 1.943722991275623 | validation: 2.4835567734093513]
	TIME [epoch: 9.49 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419328601153487		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 1.9419328601153487 | validation: 2.479448342745824]
	TIME [epoch: 9.48 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9442620135712567		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 1.9442620135712567 | validation: 2.496564568330889]
	TIME [epoch: 9.51 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9505043385654985		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 1.9505043385654985 | validation: 2.4843943775349233]
	TIME [epoch: 9.48 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9535342689645643		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 1.9535342689645643 | validation: 2.4825063937938237]
	TIME [epoch: 9.49 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394819690798795		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 1.9394819690798795 | validation: 2.4897556442479747]
	TIME [epoch: 9.48 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9416882692127022		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 1.9416882692127022 | validation: 2.497232211985076]
	TIME [epoch: 9.5 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487347915569735		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 1.9487347915569735 | validation: 2.498787202521967]
	TIME [epoch: 9.5 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.93982356494583		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 1.93982356494583 | validation: 2.4880861548629367]
	TIME [epoch: 9.48 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9391777224325284		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 1.9391777224325284 | validation: 2.477338262274321]
	TIME [epoch: 9.49 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9424299393154858		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 1.9424299393154858 | validation: 2.487455419845616]
	TIME [epoch: 9.52 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9432669113022811		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 1.9432669113022811 | validation: 2.4929282426339303]
	TIME [epoch: 9.49 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9376169889643902		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 1.9376169889643902 | validation: 2.486198621596383]
	TIME [epoch: 9.49 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9400022748811359		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 1.9400022748811359 | validation: 2.484359049101792]
	TIME [epoch: 9.48 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9403606767363617		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 1.9403606767363617 | validation: 2.4853444141513594]
	TIME [epoch: 9.49 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946922741020601		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 1.946922741020601 | validation: 2.482894910756586]
	TIME [epoch: 9.49 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9454239100764994		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 1.9454239100764994 | validation: 2.471116908235859]
	TIME [epoch: 9.49 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467877696122913		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 1.9467877696122913 | validation: 2.4784807880691555]
	TIME [epoch: 9.49 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467278308240032		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 1.9467278308240032 | validation: 2.4819208162299278]
	TIME [epoch: 9.51 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9429514448306413		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 1.9429514448306413 | validation: 2.48797704342195]
	TIME [epoch: 9.49 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939819960650949		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 1.939819960650949 | validation: 2.4859744750750337]
	TIME [epoch: 9.49 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461126263336166		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 1.9461126263336166 | validation: 2.482383208439889]
	TIME [epoch: 9.49 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9437180381502253		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 1.9437180381502253 | validation: 2.4947205004874364]
	TIME [epoch: 9.5 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9406489628105796		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 1.9406489628105796 | validation: 2.4833172492720275]
	TIME [epoch: 9.49 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9399006119437676		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 1.9399006119437676 | validation: 2.4847490682943802]
	TIME [epoch: 9.49 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9440989468634697		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 1.9440989468634697 | validation: 2.479719747490628]
	TIME [epoch: 9.5 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9474198920294963		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 1.9474198920294963 | validation: 2.5052780708619613]
	TIME [epoch: 9.5 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9485932815789113		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 1.9485932815789113 | validation: 2.4957503130624783]
	TIME [epoch: 9.49 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9424593210637267		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 1.9424593210637267 | validation: 2.486580153564298]
	TIME [epoch: 9.48 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421493765749034		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 1.9421493765749034 | validation: 2.4752778446414085]
	TIME [epoch: 9.49 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9475823356569908		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 1.9475823356569908 | validation: 2.489600905224066]
	TIME [epoch: 9.49 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947729042939376		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 1.947729042939376 | validation: 2.47350127787841]
	TIME [epoch: 9.49 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9493835793407324		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 1.9493835793407324 | validation: 2.487333865214072]
	TIME [epoch: 9.48 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9429165411088696		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 1.9429165411088696 | validation: 2.484555889137926]
	TIME [epoch: 9.48 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9407654844066493		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 1.9407654844066493 | validation: 2.483512398263859]
	TIME [epoch: 9.5 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944133220093045		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 1.944133220093045 | validation: 2.480262872414226]
	TIME [epoch: 9.5 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9457612740880559		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 1.9457612740880559 | validation: 2.4768981266427947]
	TIME [epoch: 9.49 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946815244303174		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 1.946815244303174 | validation: 2.493290547134192]
	TIME [epoch: 9.5 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9476226447063592		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 1.9476226447063592 | validation: 2.483975469627258]
	TIME [epoch: 9.51 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947500865643462		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 1.947500865643462 | validation: 2.484252222899247]
	TIME [epoch: 9.49 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9430927588994351		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 1.9430927588994351 | validation: 2.4797322320929003]
	TIME [epoch: 9.49 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9407151197315193		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 1.9407151197315193 | validation: 2.4583619113624064]
	TIME [epoch: 9.49 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r2_20240219_205222/states/model_tr_study205_1653.pth
	Model improved!!!
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9435694799154486		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 1.9435694799154486 | validation: 2.5007546319419984]
	TIME [epoch: 9.5 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9455350460085732		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 1.9455350460085732 | validation: 2.48476325656235]
	TIME [epoch: 9.49 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9462550888177248		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 1.9462550888177248 | validation: 2.4845063792875544]
	TIME [epoch: 9.49 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9501750966846518		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 1.9501750966846518 | validation: 2.481819582358183]
	TIME [epoch: 9.48 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944875777956168		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 1.944875777956168 | validation: 2.479792637980417]
	TIME [epoch: 9.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9398296874101315		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 1.9398296874101315 | validation: 2.4838448941144753]
	TIME [epoch: 9.49 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446346208661383		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 1.9446346208661383 | validation: 2.4832890438401605]
	TIME [epoch: 9.49 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942781196668097		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 1.942781196668097 | validation: 2.481334541662685]
	TIME [epoch: 9.48 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9442918451496418		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 1.9442918451496418 | validation: 2.4762787580011585]
	TIME [epoch: 9.5 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941872117137042		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 1.941872117137042 | validation: 2.4863136818606937]
	TIME [epoch: 9.48 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9475833681865093		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 1.9475833681865093 | validation: 2.4853182806235616]
	TIME [epoch: 9.49 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9531778725097815		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 1.9531778725097815 | validation: 2.496362362180039]
	TIME [epoch: 9.48 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445048545453656		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 1.9445048545453656 | validation: 2.4858858820248826]
	TIME [epoch: 9.51 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9377863977277912		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 1.9377863977277912 | validation: 2.494049207815276]
	TIME [epoch: 9.48 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9464727774340034		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 1.9464727774340034 | validation: 2.4938595653924422]
	TIME [epoch: 9.5 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9469311164795862		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 1.9469311164795862 | validation: 2.4829171837808683]
	TIME [epoch: 9.49 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9418337737559455		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 1.9418337737559455 | validation: 2.4668578093649827]
	TIME [epoch: 9.51 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9416722476084467		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 1.9416722476084467 | validation: 2.485595125367396]
	TIME [epoch: 9.48 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939115943599242		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 1.939115943599242 | validation: 2.487925925125119]
	TIME [epoch: 9.49 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9377518638184825		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 1.9377518638184825 | validation: 2.4945281617109303]
	TIME [epoch: 9.48 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9415528881243176		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 1.9415528881243176 | validation: 2.489996028417656]
	TIME [epoch: 9.51 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433641104156103		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 1.9433641104156103 | validation: 2.4745957977813497]
	TIME [epoch: 9.49 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945814824783898		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 1.945814824783898 | validation: 2.481083706516419]
	TIME [epoch: 9.5 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9474730952752037		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 1.9474730952752037 | validation: 2.4693401835693054]
	TIME [epoch: 9.49 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9407361000872754		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 1.9407361000872754 | validation: 2.4971629514670055]
	TIME [epoch: 9.5 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942168466620276		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 1.942168466620276 | validation: 2.498126473867812]
	TIME [epoch: 9.48 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9370483946064474		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 1.9370483946064474 | validation: 2.480010758016848]
	TIME [epoch: 9.49 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9414742136489154		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 1.9414742136489154 | validation: 2.480415224415743]
	TIME [epoch: 9.49 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9392577332806151		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 1.9392577332806151 | validation: 2.481548426643002]
	TIME [epoch: 9.51 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9378470509602597		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 1.9378470509602597 | validation: 2.4863551950032767]
	TIME [epoch: 9.49 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461894257685437		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 1.9461894257685437 | validation: 2.493837997474691]
	TIME [epoch: 9.49 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9409559506110277		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 1.9409559506110277 | validation: 2.479318765405795]
	TIME [epoch: 9.49 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9399367106687666		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 1.9399367106687666 | validation: 2.4906679849668607]
	TIME [epoch: 9.51 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9438941025289154		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 1.9438941025289154 | validation: 2.4902696993936884]
	TIME [epoch: 9.49 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9436519420979217		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 1.9436519420979217 | validation: 2.482901324606389]
	TIME [epoch: 9.49 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940765149253015		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 1.940765149253015 | validation: 2.487506990928874]
	TIME [epoch: 9.48 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446398037899517		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 1.9446398037899517 | validation: 2.475643299522766]
	TIME [epoch: 9.51 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9405210136074533		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 1.9405210136074533 | validation: 2.4854990580248635]
	TIME [epoch: 9.49 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9426805270293286		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 1.9426805270293286 | validation: 2.477018094113322]
	TIME [epoch: 9.49 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9482592088686779		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 1.9482592088686779 | validation: 2.485941917524885]
	TIME [epoch: 9.48 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9420455989230896		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 1.9420455989230896 | validation: 2.4938912763040473]
	TIME [epoch: 9.51 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9416283562950032		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 1.9416283562950032 | validation: 2.501096324829279]
	TIME [epoch: 9.49 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9380682373114997		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 1.9380682373114997 | validation: 2.488116931422549]
	TIME [epoch: 9.49 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9479221049133848		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 1.9479221049133848 | validation: 2.4850994368881145]
	TIME [epoch: 9.48 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9335135684883433		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 1.9335135684883433 | validation: 2.488549275302642]
	TIME [epoch: 9.51 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460294253843962		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 1.9460294253843962 | validation: 2.475569023090025]
	TIME [epoch: 9.48 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.93964470333406		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 1.93964470333406 | validation: 2.4923971714811195]
	TIME [epoch: 9.49 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9459084184912154		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 1.9459084184912154 | validation: 2.489563423686586]
	TIME [epoch: 9.49 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9491713754549007		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 1.9491713754549007 | validation: 2.4881034782683074]
	TIME [epoch: 9.5 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9442838690374995		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 1.9442838690374995 | validation: 2.4843657411836975]
	TIME [epoch: 9.49 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939591467882758		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 1.939591467882758 | validation: 2.479367479407162]
	TIME [epoch: 9.48 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.937245512017661		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 1.937245512017661 | validation: 2.4832822241195145]
	TIME [epoch: 9.49 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9375075606753875		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 1.9375075606753875 | validation: 2.4828063289081]
	TIME [epoch: 9.51 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9460801048423915		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 1.9460801048423915 | validation: 2.4911622189598677]
	TIME [epoch: 9.49 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942912100461879		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 1.942912100461879 | validation: 2.4873275400624615]
	TIME [epoch: 9.5 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9426700225811704		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 1.9426700225811704 | validation: 2.483367445248034]
	TIME [epoch: 9.48 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944074721303955		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 1.944074721303955 | validation: 2.4810131888234723]
	TIME [epoch: 9.5 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9490860258823894		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 1.9490860258823894 | validation: 2.4806280470106006]
	TIME [epoch: 9.49 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940968179900807		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 1.940968179900807 | validation: 2.4831458072414114]
	TIME [epoch: 9.49 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9482606626041157		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 1.9482606626041157 | validation: 2.49226915498444]
	TIME [epoch: 9.49 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945501229985935		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 1.945501229985935 | validation: 2.491152457328866]
	TIME [epoch: 9.51 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9427117496428195		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 1.9427117496428195 | validation: 2.5072848078485346]
	TIME [epoch: 9.49 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9468222364496726		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 1.9468222364496726 | validation: 2.4946287038397497]
	TIME [epoch: 9.49 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9417189292010444		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 1.9417189292010444 | validation: 2.4890080233375103]
	TIME [epoch: 9.48 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9366465817345382		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 1.9366465817345382 | validation: 2.4801624921641694]
	TIME [epoch: 9.49 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9508259504157561		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 1.9508259504157561 | validation: 2.5009781554324095]
	TIME [epoch: 9.49 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9482838425399105		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 1.9482838425399105 | validation: 2.490731420487035]
	TIME [epoch: 9.49 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9358702101336154		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 1.9358702101336154 | validation: 2.486153226425707]
	TIME [epoch: 9.48 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9492898312442684		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 1.9492898312442684 | validation: 2.4785337007955053]
	TIME [epoch: 9.51 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949732812107482		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 1.949732812107482 | validation: 2.4837500762133735]
	TIME [epoch: 9.49 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9453046322826306		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 1.9453046322826306 | validation: 2.494634080617292]
	TIME [epoch: 9.48 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9395265708857423		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 1.9395265708857423 | validation: 2.4815236105490412]
	TIME [epoch: 9.49 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9427947974068942		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 1.9427947974068942 | validation: 2.479315867022811]
	TIME [epoch: 9.5 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9366183328317614		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 1.9366183328317614 | validation: 2.48623885940437]
	TIME [epoch: 9.47 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9373598220537473		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 1.9373598220537473 | validation: 2.4851201801100578]
	TIME [epoch: 9.49 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9377476515041625		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 1.9377476515041625 | validation: 2.4922386933916783]
	TIME [epoch: 9.49 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421966236229515		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 1.9421966236229515 | validation: 2.4758228549457892]
	TIME [epoch: 9.51 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9413936895355235		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 1.9413936895355235 | validation: 2.4958769036380732]
	TIME [epoch: 9.48 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946938087986931		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 1.946938087986931 | validation: 2.481516118348894]
	TIME [epoch: 9.47 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9470428138562554		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 1.9470428138562554 | validation: 2.471424998720656]
	TIME [epoch: 9.49 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944674687362085		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 1.944674687362085 | validation: 2.4815823341560783]
	TIME [epoch: 9.52 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9403740503870313		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 1.9403740503870313 | validation: 2.4658599660619696]
	TIME [epoch: 9.49 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942652280580208		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 1.942652280580208 | validation: 2.48098062879878]
	TIME [epoch: 9.48 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9478185545950573		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 1.9478185545950573 | validation: 2.4770150617953592]
	TIME [epoch: 9.49 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9425604864853856		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 1.9425604864853856 | validation: 2.4850746222984066]
	TIME [epoch: 9.51 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9357773997072294		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 1.9357773997072294 | validation: 2.4994297911729193]
	TIME [epoch: 9.49 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941240142627801		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 1.941240142627801 | validation: 2.4737327572079004]
	TIME [epoch: 9.48 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9432244710757829		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 1.9432244710757829 | validation: 2.4890233903813757]
	TIME [epoch: 9.48 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9364650966449575		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 1.9364650966449575 | validation: 2.4822026284537198]
	TIME [epoch: 9.51 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946337625654562		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 1.946337625654562 | validation: 2.478904873964272]
	TIME [epoch: 9.48 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9434821551695491		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 1.9434821551695491 | validation: 2.4844170235199967]
	TIME [epoch: 9.48 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9414991787736333		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 1.9414991787736333 | validation: 2.48159472992427]
	TIME [epoch: 9.49 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944069305522889		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 1.944069305522889 | validation: 2.478162787417895]
	TIME [epoch: 9.5 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941276418498095		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 1.941276418498095 | validation: 2.4800567038702765]
	TIME [epoch: 9.48 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9491875341954041		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 1.9491875341954041 | validation: 2.477569196183339]
	TIME [epoch: 9.49 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9516361920887562		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 1.9516361920887562 | validation: 2.4809866912237575]
	TIME [epoch: 9.48 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942834545113237		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 1.942834545113237 | validation: 2.471216856270853]
	TIME [epoch: 9.5 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.938128773442438		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 1.938128773442438 | validation: 2.471578746691666]
	TIME [epoch: 9.48 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9476650862344713		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 1.9476650862344713 | validation: 2.473000326634341]
	TIME [epoch: 9.49 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9399646075124075		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 1.9399646075124075 | validation: 2.48117636145048]
	TIME [epoch: 9.49 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9377308665686925		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 1.9377308665686925 | validation: 2.4903736618863626]
	TIME [epoch: 9.5 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9424495403592679		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 1.9424495403592679 | validation: 2.4844202941330993]
	TIME [epoch: 9.48 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9464205196238062		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 1.9464205196238062 | validation: 2.487994938293665]
	TIME [epoch: 9.49 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461651588231241		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 1.9461651588231241 | validation: 2.4900925062001327]
	TIME [epoch: 9.49 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9471095631304736		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 1.9471095631304736 | validation: 2.4846380285009957]
	TIME [epoch: 9.5 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.937051591251879		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 1.937051591251879 | validation: 2.4843646414228915]
	TIME [epoch: 9.48 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433981908468287		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 1.9433981908468287 | validation: 2.4718642440396903]
	TIME [epoch: 9.49 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9409930683190695		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 1.9409930683190695 | validation: 2.4688485556644304]
	TIME [epoch: 9.49 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9453827816549896		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 1.9453827816549896 | validation: 2.4770991419456334]
	TIME [epoch: 9.51 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419359574026012		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 1.9419359574026012 | validation: 2.4812582912613683]
	TIME [epoch: 9.48 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9411222332488791		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 1.9411222332488791 | validation: 2.4822770449830727]
	TIME [epoch: 9.48 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9365229830223918		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 1.9365229830223918 | validation: 2.5003387808853343]
	TIME [epoch: 9.48 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9379163872200245		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 1.9379163872200245 | validation: 2.482592166599257]
	TIME [epoch: 9.51 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9399364487574808		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 1.9399364487574808 | validation: 2.494154431550994]
	TIME [epoch: 9.49 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9455640718287508		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 1.9455640718287508 | validation: 2.4900231824718384]
	TIME [epoch: 9.5 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9432241382396875		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 1.9432241382396875 | validation: 2.4720609915498337]
	TIME [epoch: 9.49 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9421639205351906		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 1.9421639205351906 | validation: 2.4872227080780256]
	TIME [epoch: 9.51 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9431879515385595		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 1.9431879515385595 | validation: 2.487247740804607]
	TIME [epoch: 9.48 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942411120232451		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 1.942411120232451 | validation: 2.467917331339143]
	TIME [epoch: 9.48 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461786804439865		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 1.9461786804439865 | validation: 2.4835664872126975]
	TIME [epoch: 9.49 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941300271527019		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 1.941300271527019 | validation: 2.4892631021218374]
	TIME [epoch: 9.52 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.947225617371488		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 1.947225617371488 | validation: 2.4812202735478026]
	TIME [epoch: 9.49 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9380373487280487		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 1.9380373487280487 | validation: 2.475789727004008]
	TIME [epoch: 9.48 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9397594088759285		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 1.9397594088759285 | validation: 2.490296739901939]
	TIME [epoch: 9.48 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9413352577681777		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 1.9413352577681777 | validation: 2.4737275685210474]
	TIME [epoch: 9.49 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940502315488721		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 1.940502315488721 | validation: 2.462644634744613]
	TIME [epoch: 9.48 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9380397326822174		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 1.9380397326822174 | validation: 2.4942977361135155]
	TIME [epoch: 9.49 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940708670847652		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 1.940708670847652 | validation: 2.4821215663318053]
	TIME [epoch: 9.49 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443102289001193		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 1.9443102289001193 | validation: 2.4925310159421934]
	TIME [epoch: 9.5 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428432082499558		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 1.9428432082499558 | validation: 2.480405280588339]
	TIME [epoch: 9.48 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940091658828529		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 1.940091658828529 | validation: 2.4844210133509943]
	TIME [epoch: 9.49 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9456443607392633		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 1.9456443607392633 | validation: 2.4874294970036432]
	TIME [epoch: 9.48 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9441546231415985		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 1.9441546231415985 | validation: 2.4705843350619103]
	TIME [epoch: 9.5 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9435775231784145		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 1.9435775231784145 | validation: 2.49344777258542]
	TIME [epoch: 9.49 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942406177942742		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 1.942406177942742 | validation: 2.481485700008289]
	TIME [epoch: 9.48 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9374402486670406		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 1.9374402486670406 | validation: 2.4719848576471737]
	TIME [epoch: 9.47 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419174141593516		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 1.9419174141593516 | validation: 2.4776934667351207]
	TIME [epoch: 9.49 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9476906118726212		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 1.9476906118726212 | validation: 2.499203412081844]
	TIME [epoch: 9.49 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9497619697814101		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 1.9497619697814101 | validation: 2.476730188512264]
	TIME [epoch: 9.48 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9469472158365648		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 1.9469472158365648 | validation: 2.4793460241596774]
	TIME [epoch: 9.49 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9405805316777247		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 1.9405805316777247 | validation: 2.4925657886525454]
	TIME [epoch: 9.5 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9490882440453454		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 1.9490882440453454 | validation: 2.4864066613313947]
	TIME [epoch: 9.48 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446145812497957		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 1.9446145812497957 | validation: 2.473879054309333]
	TIME [epoch: 9.49 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9425610067801784		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 1.9425610067801784 | validation: 2.4817069642486227]
	TIME [epoch: 9.49 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.949605014279711		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 1.949605014279711 | validation: 2.4849212185704035]
	TIME [epoch: 9.51 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9356573279675662		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 1.9356573279675662 | validation: 2.482635044459194]
	TIME [epoch: 9.5 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9494150426039412		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 1.9494150426039412 | validation: 2.481695334156204]
	TIME [epoch: 9.49 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9365242611233295		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 1.9365242611233295 | validation: 2.4841185788937223]
	TIME [epoch: 9.49 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9478797953969988		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 1.9478797953969988 | validation: 2.4913277927791295]
	TIME [epoch: 9.51 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9454733242420226		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 1.9454733242420226 | validation: 2.470444381406694]
	TIME [epoch: 9.49 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9388181909153865		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 1.9388181909153865 | validation: 2.484671250395711]
	TIME [epoch: 9.47 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9374257238091943		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 1.9374257238091943 | validation: 2.4753976127898567]
	TIME [epoch: 9.48 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9395753663874704		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 1.9395753663874704 | validation: 2.477140939011118]
	TIME [epoch: 9.5 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446752284110675		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 1.9446752284110675 | validation: 2.4841858740832463]
	TIME [epoch: 9.47 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9444896001419654		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 1.9444896001419654 | validation: 2.4917682069211597]
	TIME [epoch: 9.49 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.937249449978929		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 1.937249449978929 | validation: 2.4955011929659703]
	TIME [epoch: 9.48 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9385993469159608		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 1.9385993469159608 | validation: 2.4938548796449673]
	TIME [epoch: 9.51 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9499631389555652		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 1.9499631389555652 | validation: 2.489565744117129]
	TIME [epoch: 9.49 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9459455651234254		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 1.9459455651234254 | validation: 2.487597488336977]
	TIME [epoch: 9.48 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9420913818003993		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 1.9420913818003993 | validation: 2.4799078437031774]
	TIME [epoch: 9.48 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419078288023996		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 1.9419078288023996 | validation: 2.494325607099668]
	TIME [epoch: 9.5 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9442933725449507		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 1.9442933725449507 | validation: 2.481385988831473]
	TIME [epoch: 9.49 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9453479082364509		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 1.9453479082364509 | validation: 2.4869849083831794]
	TIME [epoch: 9.49 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9383716802075484		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 1.9383716802075484 | validation: 2.4796922656285534]
	TIME [epoch: 9.49 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9403366714266361		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 1.9403366714266361 | validation: 2.4795235892952094]
	TIME [epoch: 9.5 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9463956515856402		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 1.9463956515856402 | validation: 2.4918233477525566]
	TIME [epoch: 9.48 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9363324647278446		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 1.9363324647278446 | validation: 2.4837034862268417]
	TIME [epoch: 9.49 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9457342270647324		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 1.9457342270647324 | validation: 2.49332147293361]
	TIME [epoch: 9.49 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445161678622198		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 1.9445161678622198 | validation: 2.483358363066343]
	TIME [epoch: 9.51 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941569828220743		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 1.941569828220743 | validation: 2.4955757500536335]
	TIME [epoch: 9.49 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9396821010004828		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 1.9396821010004828 | validation: 2.468564600427954]
	TIME [epoch: 9.48 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9507454480160575		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 1.9507454480160575 | validation: 2.495996540130299]
	TIME [epoch: 9.48 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.95054195743749		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 1.95054195743749 | validation: 2.4800777720960476]
	TIME [epoch: 9.51 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9406139559773248		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 1.9406139559773248 | validation: 2.483668747226609]
	TIME [epoch: 9.48 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9333561129851549		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 1.9333561129851549 | validation: 2.4935841215747394]
	TIME [epoch: 9.49 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9438099227109373		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 1.9438099227109373 | validation: 2.491049010099755]
	TIME [epoch: 9.5 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9400706435704254		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 1.9400706435704254 | validation: 2.494266634219824]
	TIME [epoch: 9.5 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9417186408905565		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 1.9417186408905565 | validation: 2.4791812150552324]
	TIME [epoch: 9.48 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394610375743482		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 1.9394610375743482 | validation: 2.5053492080645094]
	TIME [epoch: 9.49 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94129998825816		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 1.94129998825816 | validation: 2.486361484270199]
	TIME [epoch: 9.48 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9458404079316334		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 1.9458404079316334 | validation: 2.4851713029059206]
	TIME [epoch: 9.52 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9450544771998899		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 1.9450544771998899 | validation: 2.4739812552812372]
	TIME [epoch: 9.49 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9449221078022318		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 1.9449221078022318 | validation: 2.475452361418672]
	TIME [epoch: 9.48 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9381326714397957		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 1.9381326714397957 | validation: 2.49062767661845]
	TIME [epoch: 9.49 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9422986176225536		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 1.9422986176225536 | validation: 2.477253699298004]
	TIME [epoch: 9.5 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944519765254587		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 1.944519765254587 | validation: 2.482480820456697]
	TIME [epoch: 9.49 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9401869073509257		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 1.9401869073509257 | validation: 2.472371958292903]
	TIME [epoch: 9.49 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419537321017954		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 1.9419537321017954 | validation: 2.488883994831422]
	TIME [epoch: 9.48 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9402953381836334		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 1.9402953381836334 | validation: 2.47560544035883]
	TIME [epoch: 9.5 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9375394222588718		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 1.9375394222588718 | validation: 2.4915063123984735]
	TIME [epoch: 9.48 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9426335837272035		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 1.9426335837272035 | validation: 2.480410775351055]
	TIME [epoch: 9.48 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.936228866354434		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 1.936228866354434 | validation: 2.485657487206105]
	TIME [epoch: 9.48 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461955447161958		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 1.9461955447161958 | validation: 2.480486021282148]
	TIME [epoch: 9.51 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9482393468569181		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 1.9482393468569181 | validation: 2.4839486357854406]
	TIME [epoch: 9.49 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9351489536005293		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 1.9351489536005293 | validation: 2.4961691545363687]
	TIME [epoch: 9.49 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942255635230208		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 1.942255635230208 | validation: 2.483150197063557]
	TIME [epoch: 9.5 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9410766350353037		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 1.9410766350353037 | validation: 2.4869736620602816]
	TIME [epoch: 9.52 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9423194980534972		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 1.9423194980534972 | validation: 2.474217507207784]
	TIME [epoch: 9.49 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9502976838024206		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 1.9502976838024206 | validation: 2.502898636662962]
	TIME [epoch: 9.49 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945514774298212		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 1.945514774298212 | validation: 2.4834331152981455]
	TIME [epoch: 9.49 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9411078206380838		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 1.9411078206380838 | validation: 2.4928330367151115]
	TIME [epoch: 9.51 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433930052549608		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 1.9433930052549608 | validation: 2.482188339194228]
	TIME [epoch: 9.49 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443842106156424		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 1.9443842106156424 | validation: 2.4824057850834014]
	TIME [epoch: 9.49 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943754258540953		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 1.943754258540953 | validation: 2.480415296554912]
	TIME [epoch: 9.48 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9455027577310862		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 1.9455027577310862 | validation: 2.4797566106010267]
	TIME [epoch: 9.51 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9448873113392935		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 1.9448873113392935 | validation: 2.4894258117330925]
	TIME [epoch: 9.49 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.938615501181189		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 1.938615501181189 | validation: 2.492908773415194]
	TIME [epoch: 9.5 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9378418554389445		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 1.9378418554389445 | validation: 2.482848049866268]
	TIME [epoch: 9.48 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.948100670906309		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 1.948100670906309 | validation: 2.49106600038333]
	TIME [epoch: 9.5 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939807626657584		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 1.939807626657584 | validation: 2.477627062685196]
	TIME [epoch: 9.49 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9430105738648238		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 1.9430105738648238 | validation: 2.491261023383797]
	TIME [epoch: 9.49 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943390878909313		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 1.943390878909313 | validation: 2.472844625081561]
	TIME [epoch: 9.48 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942410749964889		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 1.942410749964889 | validation: 2.4868228572848303]
	TIME [epoch: 9.52 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461676696203813		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 1.9461676696203813 | validation: 2.4762287402420187]
	TIME [epoch: 9.49 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945410142662821		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 1.945410142662821 | validation: 2.4864292831236714]
	TIME [epoch: 9.49 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9437597195151461		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 1.9437597195151461 | validation: 2.486310018248093]
	TIME [epoch: 9.49 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9498073626795034		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 1.9498073626795034 | validation: 2.4838462851255225]
	TIME [epoch: 9.5 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9458337536590915		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 1.9458337536590915 | validation: 2.4812970374968426]
	TIME [epoch: 9.49 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9420941476224698		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 1.9420941476224698 | validation: 2.4749219021958626]
	TIME [epoch: 9.47 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.937433745234058		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 1.937433745234058 | validation: 2.4845090532905516]
	TIME [epoch: 9.48 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945212604659749		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 1.945212604659749 | validation: 2.4802913205324333]
	TIME [epoch: 9.5 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9396623919711824		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 1.9396623919711824 | validation: 2.472791388072561]
	TIME [epoch: 9.47 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945426622860947		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 1.945426622860947 | validation: 2.480308974202495]
	TIME [epoch: 9.49 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945360088821369		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 1.945360088821369 | validation: 2.4608275366783037]
	TIME [epoch: 9.49 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9423594563092066		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 1.9423594563092066 | validation: 2.4864539200030458]
	TIME [epoch: 9.51 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9466360621203642		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 1.9466360621203642 | validation: 2.494222303476916]
	TIME [epoch: 9.48 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461156451819566		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 1.9461156451819566 | validation: 2.486053486684574]
	TIME [epoch: 9.48 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467335539553337		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 1.9467335539553337 | validation: 2.4876858538469624]
	TIME [epoch: 9.49 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9472312118156427		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 1.9472312118156427 | validation: 2.4730927883436395]
	TIME [epoch: 9.5 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9380479547230216		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 1.9380479547230216 | validation: 2.4645915773509866]
	TIME [epoch: 9.48 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94259006560265		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 1.94259006560265 | validation: 2.4930837132117762]
	TIME [epoch: 9.47 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941046744002803		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 1.941046744002803 | validation: 2.4696539274552154]
	TIME [epoch: 9.48 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446179380216684		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 1.9446179380216684 | validation: 2.4883322209630054]
	TIME [epoch: 9.51 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943897406203763		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 1.943897406203763 | validation: 2.477065221583881]
	TIME [epoch: 9.49 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9423678509773787		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 1.9423678509773787 | validation: 2.483520147149272]
	TIME [epoch: 9.49 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9420413199918816		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 1.9420413199918816 | validation: 2.4810508972808907]
	TIME [epoch: 9.49 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.939218108382264		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 1.939218108382264 | validation: 2.4894156752192234]
	TIME [epoch: 9.51 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9480571534997697		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 1.9480571534997697 | validation: 2.4755597778386638]
	TIME [epoch: 9.49 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943358488438884		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 1.943358488438884 | validation: 2.4761554574505698]
	TIME [epoch: 9.5 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428634944074372		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 1.9428634944074372 | validation: 2.4901541067705155]
	TIME [epoch: 9.49 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9422773837805607		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 1.9422773837805607 | validation: 2.4808583268069953]
	TIME [epoch: 9.51 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443943888909647		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 1.9443943888909647 | validation: 2.487431785396265]
	TIME [epoch: 9.49 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9481485576778443		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 1.9481485576778443 | validation: 2.4874450873362943]
	TIME [epoch: 9.48 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943149564095594		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 1.943149564095594 | validation: 2.4920137602649572]
	TIME [epoch: 9.49 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9430060339085755		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 1.9430060339085755 | validation: 2.4692612184252876]
	TIME [epoch: 9.5 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.946890651755524		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 1.946890651755524 | validation: 2.4981655600685952]
	TIME [epoch: 9.49 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443503756978213		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 1.9443503756978213 | validation: 2.4933328503351095]
	TIME [epoch: 9.49 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9375356197705442		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 1.9375356197705442 | validation: 2.4751038783343153]
	TIME [epoch: 9.49 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9410781533141894		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 1.9410781533141894 | validation: 2.500689326747369]
	TIME [epoch: 9.51 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942497835768322		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 1.942497835768322 | validation: 2.4779650740174683]
	TIME [epoch: 9.49 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9373292867065952		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 1.9373292867065952 | validation: 2.4837361988118682]
	TIME [epoch: 9.49 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9431861534536998		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 1.9431861534536998 | validation: 2.490239731989053]
	TIME [epoch: 9.48 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9504226600925811		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 1.9504226600925811 | validation: 2.4905366078377784]
	TIME [epoch: 9.51 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9424099652435445		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 1.9424099652435445 | validation: 2.4730474852309277]
	TIME [epoch: 9.49 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9392755715112702		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 1.9392755715112702 | validation: 2.4780503644728675]
	TIME [epoch: 9.48 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.943055797911865		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 1.943055797911865 | validation: 2.4883257069018585]
	TIME [epoch: 9.49 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9402157575837156		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 1.9402157575837156 | validation: 2.47492327480046]
	TIME [epoch: 9.51 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9418168817367463		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 1.9418168817367463 | validation: 2.4832667651106664]
	TIME [epoch: 9.49 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9494604312807844		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 1.9494604312807844 | validation: 2.4953442978501363]
	TIME [epoch: 9.48 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428434834252428		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 1.9428434834252428 | validation: 2.4848866011049457]
	TIME [epoch: 9.48 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9449848334181723		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 1.9449848334181723 | validation: 2.4798197771825214]
	TIME [epoch: 9.5 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9451886883236902		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 1.9451886883236902 | validation: 2.4638671951936737]
	TIME [epoch: 9.48 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.93952251015689		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 1.93952251015689 | validation: 2.4764841289046933]
	TIME [epoch: 9.48 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9448305211205468		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 1.9448305211205468 | validation: 2.484176136824904]
	TIME [epoch: 9.48 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9443466327991576		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 1.9443466327991576 | validation: 2.4751304609760933]
	TIME [epoch: 9.5 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945323257365604		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 1.945323257365604 | validation: 2.4814143977326495]
	TIME [epoch: 9.49 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9486532053802463		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 1.9486532053802463 | validation: 2.467846575030829]
	TIME [epoch: 9.49 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9495239780406397		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 1.9495239780406397 | validation: 2.4701974579825663]
	TIME [epoch: 9.49 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9462630273322687		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 1.9462630273322687 | validation: 2.472933033408623]
	TIME [epoch: 9.51 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446284576648503		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 1.9446284576648503 | validation: 2.4770844864839763]
	TIME [epoch: 9.49 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9487019705744768		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 1.9487019705744768 | validation: 2.468712778940638]
	TIME [epoch: 9.48 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394221679601322		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 1.9394221679601322 | validation: 2.4803428220410333]
	TIME [epoch: 9.49 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.94468685648897		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 1.94468685648897 | validation: 2.4773393094115654]
	TIME [epoch: 9.5 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9478963316797002		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 1.9478963316797002 | validation: 2.4740292968265387]
	TIME [epoch: 9.49 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9391675341009535		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 1.9391675341009535 | validation: 2.4853482774880122]
	TIME [epoch: 9.49 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428941754904119		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 1.9428941754904119 | validation: 2.4906060098325074]
	TIME [epoch: 9.49 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9473388801943856		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 1.9473388801943856 | validation: 2.468940297197662]
	TIME [epoch: 9.51 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944694590033364		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 1.944694590033364 | validation: 2.4730117935661178]
	TIME [epoch: 9.49 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9407867805809491		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 1.9407867805809491 | validation: 2.4847793315289355]
	TIME [epoch: 9.48 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9467312975609645		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 1.9467312975609645 | validation: 2.4753551882561333]
	TIME [epoch: 9.49 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945489678240855		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 1.945489678240855 | validation: 2.484907187248175]
	TIME [epoch: 9.5 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9478827398756438		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 1.9478827398756438 | validation: 2.4888153931687134]
	TIME [epoch: 9.48 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9435713648283368		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 1.9435713648283368 | validation: 2.470568286323745]
	TIME [epoch: 9.48 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433096105835728		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 1.9433096105835728 | validation: 2.4848045124709572]
	TIME [epoch: 9.48 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9432838217664763		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 1.9432838217664763 | validation: 2.4877204196899236]
	TIME [epoch: 9.5 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9385994211997073		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 1.9385994211997073 | validation: 2.482932950739801]
	TIME [epoch: 9.48 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942354850085635		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 1.942354850085635 | validation: 2.4819632126483446]
	TIME [epoch: 9.48 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9409576311634253		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 1.9409576311634253 | validation: 2.4773115370382413]
	TIME [epoch: 9.48 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445347460101705		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 1.9445347460101705 | validation: 2.499542153055682]
	TIME [epoch: 9.51 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944300188956222		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 1.944300188956222 | validation: 2.483689882813654]
	TIME [epoch: 9.5 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.941030830011534		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 1.941030830011534 | validation: 2.4817323563448888]
	TIME [epoch: 9.49 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9438822795456674		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 1.9438822795456674 | validation: 2.4830915031977385]
	TIME [epoch: 9.48 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9459082206043192		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 1.9459082206043192 | validation: 2.476104046275291]
	TIME [epoch: 9.49 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944879829800427		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 1.944879829800427 | validation: 2.486756484934813]
	TIME [epoch: 9.48 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9432628833629289		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 1.9432628833629289 | validation: 2.494792576167518]
	TIME [epoch: 9.48 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9392979321122545		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 1.9392979321122545 | validation: 2.485369074514512]
	TIME [epoch: 9.49 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9465528641813639		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 1.9465528641813639 | validation: 2.4956749384954104]
	TIME [epoch: 9.51 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9448808771033537		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 1.9448808771033537 | validation: 2.4800391293491697]
	TIME [epoch: 9.49 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.937988389050446		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 1.937988389050446 | validation: 2.486360103316345]
	TIME [epoch: 9.5 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9377446087549615		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 1.9377446087549615 | validation: 2.4924951644193176]
	TIME [epoch: 9.49 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428006772832909		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 1.9428006772832909 | validation: 2.4915292638451105]
	TIME [epoch: 9.51 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9419041888421256		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 1.9419041888421256 | validation: 2.4892463940537874]
	TIME [epoch: 9.49 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942670968092359		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 1.942670968092359 | validation: 2.4867099143794507]
	TIME [epoch: 9.49 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9423968739833046		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 1.9423968739833046 | validation: 2.4881821061769966]
	TIME [epoch: 9.5 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9327270094373965		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 1.9327270094373965 | validation: 2.4751596598334213]
	TIME [epoch: 9.51 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9384451248143815		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 1.9384451248143815 | validation: 2.492858136726334]
	TIME [epoch: 9.48 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9462150869827455		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 1.9462150869827455 | validation: 2.475206664476848]
	TIME [epoch: 9.49 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9404010919001204		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 1.9404010919001204 | validation: 2.474770530817174]
	TIME [epoch: 9.49 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.940380972606711		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 1.940380972606711 | validation: 2.490094238735919]
	TIME [epoch: 9.5 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9464302177131472		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 1.9464302177131472 | validation: 2.4905405616625793]
	TIME [epoch: 9.49 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942760509776577		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 1.942760509776577 | validation: 2.494126709108755]
	TIME [epoch: 9.5 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9406494060111712		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 1.9406494060111712 | validation: 2.486941819392868]
	TIME [epoch: 9.48 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9390943108660614		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 1.9390943108660614 | validation: 2.491854939893292]
	TIME [epoch: 9.51 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9414860206887703		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 1.9414860206887703 | validation: 2.4819476429289855]
	TIME [epoch: 9.49 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9384660551204262		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 1.9384660551204262 | validation: 2.484678490141504]
	TIME [epoch: 9.49 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9506697582333683		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 1.9506697582333683 | validation: 2.486102189987439]
	TIME [epoch: 9.49 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9385782746443283		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 1.9385782746443283 | validation: 2.4917223311249845]
	TIME [epoch: 9.51 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9468141252714766		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 1.9468141252714766 | validation: 2.4918292433207037]
	TIME [epoch: 9.48 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9393167124776718		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 1.9393167124776718 | validation: 2.484750593116829]
	TIME [epoch: 9.48 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9400583528779105		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 1.9400583528779105 | validation: 2.480072757584652]
	TIME [epoch: 9.48 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9422838925549677		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 1.9422838925549677 | validation: 2.485786646325851]
	TIME [epoch: 9.49 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446207860252311		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 1.9446207860252311 | validation: 2.489282448140204]
	TIME [epoch: 9.47 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9428241171748675		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 1.9428241171748675 | validation: 2.4921483978375742]
	TIME [epoch: 9.48 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9397046465264167		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 1.9397046465264167 | validation: 2.498314451768881]
	TIME [epoch: 9.49 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394358861753354		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 1.9394358861753354 | validation: 2.4868797711328305]
	TIME [epoch: 9.5 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9369456809635093		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 1.9369456809635093 | validation: 2.478004747556947]
	TIME [epoch: 9.48 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9441656254682187		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 1.9441656254682187 | validation: 2.481372240660359]
	TIME [epoch: 9.48 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945956858983194		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 1.945956858983194 | validation: 2.4784556744528423]
	TIME [epoch: 9.49 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446473436803635		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 1.9446473436803635 | validation: 2.485596247863689]
	TIME [epoch: 9.5 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9445779523572324		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 1.9445779523572324 | validation: 2.4892825226530837]
	TIME [epoch: 9.49 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9458632802153475		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 1.9458632802153475 | validation: 2.4879648087872397]
	TIME [epoch: 9.49 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9461692488178344		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 1.9461692488178344 | validation: 2.4885172535948055]
	TIME [epoch: 9.5 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9499088049793205		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 1.9499088049793205 | validation: 2.4793802873550663]
	TIME [epoch: 9.51 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.937187014284774		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 1.937187014284774 | validation: 2.4838305005451025]
	TIME [epoch: 9.49 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433706602587937		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 1.9433706602587937 | validation: 2.4782607084586075]
	TIME [epoch: 9.49 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9464828298309702		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 1.9464828298309702 | validation: 2.489308998437483]
	TIME [epoch: 9.48 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9451450283075153		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 1.9451450283075153 | validation: 2.488673851579098]
	TIME [epoch: 9.51 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9446928744095153		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 1.9446928744095153 | validation: 2.483677939992678]
	TIME [epoch: 9.49 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9388817526829585		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 1.9388817526829585 | validation: 2.4866049836856994]
	TIME [epoch: 9.5 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.942698762555283		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 1.942698762555283 | validation: 2.4791392539072223]
	TIME [epoch: 9.49 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9403091044879655		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 1.9403091044879655 | validation: 2.4809597759902005]
	TIME [epoch: 9.51 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9435581695234496		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 1.9435581695234496 | validation: 2.4833171552891566]
	TIME [epoch: 9.48 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9450119604317528		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 1.9450119604317528 | validation: 2.4971307097239084]
	TIME [epoch: 9.49 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.944036325029895		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 1.944036325029895 | validation: 2.484366787831478]
	TIME [epoch: 9.49 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9394943283477386		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 1.9394943283477386 | validation: 2.487169943918358]
	TIME [epoch: 9.51 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9401827214512892		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 1.9401827214512892 | validation: 2.487785525115678]
	TIME [epoch: 9.49 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9378708249823813		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 1.9378708249823813 | validation: 2.489622133778523]
	TIME [epoch: 9.48 sec]
Finished training in 19126.349 seconds.
