Args:
Namespace(name='model_tr_study205', outdir='out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0', training_data='data/transition_rate_studies/tr_study205/tr_study205_training/r0', validation_data='data/transition_rate_studies/tr_study205/tr_study205_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 502731531

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.796571638242732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.796571638242732 | validation: 11.586182803188365]
	TIME [epoch: 54.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 12.939948249229754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 12.939948249229754 | validation: 12.573032795908839]
	TIME [epoch: 9.78 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.570196459216199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.570196459216199 | validation: 10.561405388775905]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.715210180069402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.715210180069402 | validation: 11.394040126963299]
	TIME [epoch: 9.78 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.099563560270061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.099563560270061 | validation: 7.4614529902698585]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.561549889185029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.561549889185029 | validation: 7.820925079479514]
	TIME [epoch: 9.79 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.6529581729668195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6529581729668195 | validation: 7.025696026782432]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.865469092900087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.865469092900087 | validation: 6.702423909057627]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.475447986205626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.475447986205626 | validation: 5.88739853899396]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.527027071750557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.527027071750557 | validation: 6.122000708977493]
	TIME [epoch: 9.76 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.196914290839056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.196914290839056 | validation: 6.628274537279741]
	TIME [epoch: 9.76 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.231378907777101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.231378907777101 | validation: 5.47621404614935]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.697104072595209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.697104072595209 | validation: 6.013267281749076]
	TIME [epoch: 9.76 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.899534390439284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.899534390439284 | validation: 5.275358933893183]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.588123710983568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.588123710983568 | validation: 5.978185269461818]
	TIME [epoch: 9.76 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.756389830005057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.756389830005057 | validation: 6.554442609991528]
	TIME [epoch: 9.77 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.646466134341627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.646466134341627 | validation: 5.309556837609518]
	TIME [epoch: 9.75 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.349789227565575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.349789227565575 | validation: 5.9669608053937875]
	TIME [epoch: 9.76 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.885235175696943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.885235175696943 | validation: 5.558924432707072]
	TIME [epoch: 9.76 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.395738355890778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.395738355890778 | validation: 5.219732918227568]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.232020579291634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.232020579291634 | validation: 5.014239911565347]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.647025037582134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.647025037582134 | validation: 8.429694404792118]
	TIME [epoch: 9.75 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.255231575029123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.255231575029123 | validation: 5.18566322718988]
	TIME [epoch: 9.78 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.238161644303619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.238161644303619 | validation: 5.967760293334197]
	TIME [epoch: 9.75 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3797068234667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3797068234667 | validation: 5.11580914919302]
	TIME [epoch: 9.75 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.176146405311487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.176146405311487 | validation: 5.1698303749899015]
	TIME [epoch: 9.77 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.297392353534586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.297392353534586 | validation: 4.770373406435811]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.272359057750904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.272359057750904 | validation: 4.719600465971239]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.243840031327354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.243840031327354 | validation: 5.12516255718999]
	TIME [epoch: 9.76 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.241312763421989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.241312763421989 | validation: 4.832116192346537]
	TIME [epoch: 9.78 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.271704327938204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.271704327938204 | validation: 4.782301034410823]
	TIME [epoch: 9.76 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.366806639175567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.366806639175567 | validation: 4.555194025458006]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.972664445240657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.972664445240657 | validation: 4.654649923113532]
	TIME [epoch: 9.77 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.139306903102666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.139306903102666 | validation: 5.299063908158489]
	TIME [epoch: 9.76 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.211794344653749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.211794344653749 | validation: 5.099348170665818]
	TIME [epoch: 9.76 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.142099718936693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.142099718936693 | validation: 4.858298489328405]
	TIME [epoch: 9.76 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0864036478556836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0864036478556836 | validation: 5.229884751830443]
	TIME [epoch: 9.77 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.084854085452557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.084854085452557 | validation: 5.023280819839376]
	TIME [epoch: 9.76 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.057233748138685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.057233748138685 | validation: 5.318861945005899]
	TIME [epoch: 9.77 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.089435801732016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.089435801732016 | validation: 5.05931850688868]
	TIME [epoch: 9.77 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.899207251915945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.899207251915945 | validation: 5.085761455769924]
	TIME [epoch: 9.76 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.893271930733101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.893271930733101 | validation: 5.944286331867884]
	TIME [epoch: 9.76 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.27049139089905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.27049139089905 | validation: 4.669478205888228]
	TIME [epoch: 9.76 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8663343307003695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8663343307003695 | validation: 4.776902928236389]
	TIME [epoch: 9.78 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.111302191686475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.111302191686475 | validation: 4.983622705171893]
	TIME [epoch: 9.76 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8828320921205077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8828320921205077 | validation: 4.795189164710967]
	TIME [epoch: 9.76 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.770413404291806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.770413404291806 | validation: 5.36581488865083]
	TIME [epoch: 9.76 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9522628280771244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9522628280771244 | validation: 4.768795989724127]
	TIME [epoch: 9.78 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.701624015698041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.701624015698041 | validation: 5.291774124213843]
	TIME [epoch: 9.75 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9197496385600927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9197496385600927 | validation: 4.807342540352991]
	TIME [epoch: 9.75 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8038435557217616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8038435557217616 | validation: 5.133591467372281]
	TIME [epoch: 9.77 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8647070219742865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8647070219742865 | validation: 5.586765836096865]
	TIME [epoch: 9.76 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.122372674252344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.122372674252344 | validation: 4.407583359629696]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8428414136620446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8428414136620446 | validation: 4.308849760825039]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6643151885091925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6643151885091925 | validation: 5.543178001092581]
	TIME [epoch: 9.77 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9823224496441982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9823224496441982 | validation: 4.638591289740289]
	TIME [epoch: 9.75 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6346303242506885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6346303242506885 | validation: 5.265883516589244]
	TIME [epoch: 9.74 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.008498069163491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008498069163491 | validation: 4.80907974432779]
	TIME [epoch: 9.76 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.665910169085648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.665910169085648 | validation: 5.182123545672172]
	TIME [epoch: 9.75 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.868364769660657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.868364769660657 | validation: 4.923409785772803]
	TIME [epoch: 9.74 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617652191086119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.617652191086119 | validation: 4.409732371244107]
	TIME [epoch: 9.74 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77439515341323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.77439515341323 | validation: 4.826074474064042]
	TIME [epoch: 9.76 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783870658261793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.783870658261793 | validation: 4.367985826943474]
	TIME [epoch: 9.75 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.685813167290699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.685813167290699 | validation: 5.063460007911456]
	TIME [epoch: 9.75 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.764685030999946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.764685030999946 | validation: 4.37708657546542]
	TIME [epoch: 9.75 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862058980745823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.862058980745823 | validation: 4.376303774568862]
	TIME [epoch: 9.74 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.72316722120803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.72316722120803 | validation: 5.609508798599448]
	TIME [epoch: 9.73 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6521603713227484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6521603713227484 | validation: 4.671946268037607]
	TIME [epoch: 9.73 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.086761907352314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.086761907352314 | validation: 4.485645624957264]
	TIME [epoch: 9.76 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.614611648113833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.614611648113833 | validation: 4.49280009775444]
	TIME [epoch: 9.74 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.831979722528806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.831979722528806 | validation: 4.271850001682545]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.702354271276792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.702354271276792 | validation: 4.718806727120349]
	TIME [epoch: 9.77 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.033382033581639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033382033581639 | validation: 4.396299629340035]
	TIME [epoch: 9.76 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.870632635245048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.870632635245048 | validation: 4.565536336119249]
	TIME [epoch: 9.75 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5282000380604677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5282000380604677 | validation: 4.65477948471182]
	TIME [epoch: 9.75 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.637880144072679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.637880144072679 | validation: 4.513862879774994]
	TIME [epoch: 9.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5924584346496835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5924584346496835 | validation: 4.704569078669153]
	TIME [epoch: 9.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6785210966270534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6785210966270534 | validation: 4.960380940413619]
	TIME [epoch: 9.75 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4232419122032463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4232419122032463 | validation: 4.737570922687964]
	TIME [epoch: 9.75 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6776802095420607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6776802095420607 | validation: 4.599163667648803]
	TIME [epoch: 9.77 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521954647323214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.521954647323214 | validation: 4.330477353416235]
	TIME [epoch: 9.75 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629272897748618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.629272897748618 | validation: 4.585715732780937]
	TIME [epoch: 9.75 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5961597292442193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5961597292442193 | validation: 4.740158312273402]
	TIME [epoch: 9.76 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60888758319426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.60888758319426 | validation: 4.785299185786475]
	TIME [epoch: 9.75 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4197380932332835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4197380932332835 | validation: 5.056855153075566]
	TIME [epoch: 9.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5978945623944285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5978945623944285 | validation: 4.749028160615462]
	TIME [epoch: 9.75 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6618068231537633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6618068231537633 | validation: 4.42439687993956]
	TIME [epoch: 9.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.472876034909855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.472876034909855 | validation: 4.703303141385262]
	TIME [epoch: 9.75 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601646317661178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.601646317661178 | validation: 4.53527278189087]
	TIME [epoch: 9.75 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5393404125151244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5393404125151244 | validation: 4.57246295246238]
	TIME [epoch: 9.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.488369984443593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.488369984443593 | validation: 4.607224115512549]
	TIME [epoch: 9.75 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.436557729675182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.436557729675182 | validation: 4.91810590056465]
	TIME [epoch: 9.75 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6306836021916373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6306836021916373 | validation: 4.407694766164289]
	TIME [epoch: 9.75 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497590512099984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.497590512099984 | validation: 5.054983948553767]
	TIME [epoch: 9.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513409027133773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.513409027133773 | validation: 4.559366559887485]
	TIME [epoch: 9.75 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.44569707633287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.44569707633287 | validation: 4.728948571053799]
	TIME [epoch: 9.75 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515777253719795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.515777253719795 | validation: 4.3845935002194105]
	TIME [epoch: 9.76 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.424077887691465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.424077887691465 | validation: 4.255245751420607]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4489480425849406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4489480425849406 | validation: 4.434270596727303]
	TIME [epoch: 9.74 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3626326295342617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3626326295342617 | validation: 4.450301979461239]
	TIME [epoch: 9.74 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4509498575157593		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 3.4509498575157593 | validation: 4.745625754176494]
	TIME [epoch: 9.76 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.481478795885566		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 3.481478795885566 | validation: 4.457142521243644]
	TIME [epoch: 9.74 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.388024884356394		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 3.388024884356394 | validation: 3.996482154700517]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.43873861886226		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.43873861886226 | validation: 4.513420876331346]
	TIME [epoch: 9.75 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.357292858956945		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 3.357292858956945 | validation: 4.685889454661018]
	TIME [epoch: 9.76 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.377955799344182		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 3.377955799344182 | validation: 4.319779116605305]
	TIME [epoch: 9.73 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.324222174195912		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.324222174195912 | validation: 5.056563581604926]
	TIME [epoch: 9.74 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5528560504102153		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.5528560504102153 | validation: 4.071358797423752]
	TIME [epoch: 9.75 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.404694960082898		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.404694960082898 | validation: 4.9644071289863865]
	TIME [epoch: 9.75 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.295492811882679		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.295492811882679 | validation: 4.891597955902301]
	TIME [epoch: 9.74 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.654732016113317		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.654732016113317 | validation: 4.417616927875706]
	TIME [epoch: 9.74 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.483583468469791		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.483583468469791 | validation: 4.332647766106008]
	TIME [epoch: 9.76 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3485397540278483		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.3485397540278483 | validation: 4.292347747299797]
	TIME [epoch: 9.73 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.304052448487254		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.304052448487254 | validation: 4.08729755444388]
	TIME [epoch: 9.74 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.359139995892947		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.359139995892947 | validation: 4.143929901248886]
	TIME [epoch: 9.76 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2874893322922554		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.2874893322922554 | validation: 4.696342342767129]
	TIME [epoch: 9.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4648248395510017		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.4648248395510017 | validation: 4.072445061507391]
	TIME [epoch: 9.74 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3108499907410454		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.3108499907410454 | validation: 4.979488151798884]
	TIME [epoch: 9.74 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4496172312446602		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.4496172312446602 | validation: 3.952387026417974]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1726643705408337		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.1726643705408337 | validation: 4.495937888217362]
	TIME [epoch: 9.75 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2862654627470116		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 3.2862654627470116 | validation: 4.312576262236726]
	TIME [epoch: 9.74 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.259599464075902		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 3.259599464075902 | validation: 4.7422749920139085]
	TIME [epoch: 9.74 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3044206886810557		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 3.3044206886810557 | validation: 3.9777549731266935]
	TIME [epoch: 9.75 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.309242400144709		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 3.309242400144709 | validation: 4.606080637586421]
	TIME [epoch: 9.73 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2868709560927583		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 3.2868709560927583 | validation: 4.153546932632461]
	TIME [epoch: 9.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1629033687978767		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 3.1629033687978767 | validation: 4.736913449332839]
	TIME [epoch: 9.75 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2045416183841295		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.2045416183841295 | validation: 4.136824730183573]
	TIME [epoch: 9.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.327514799880389		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.327514799880389 | validation: 4.191192241872706]
	TIME [epoch: 9.73 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.956325811607163		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 4.956325811607163 | validation: 4.177666359142064]
	TIME [epoch: 9.75 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2834537078285764		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.2834537078285764 | validation: 4.339917467985669]
	TIME [epoch: 9.75 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2005224059675754		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.2005224059675754 | validation: 4.258814894713141]
	TIME [epoch: 9.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.249854043354037		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.249854043354037 | validation: 4.200057417277196]
	TIME [epoch: 9.74 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1269018001709696		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.1269018001709696 | validation: 4.118809573777454]
	TIME [epoch: 9.76 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1222912799785667		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.1222912799785667 | validation: 3.822453599480303]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1109508642468486		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 3.1109508642468486 | validation: 4.867895298737996]
	TIME [epoch: 9.74 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.266781643582226		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.266781643582226 | validation: 4.316240814698989]
	TIME [epoch: 9.74 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2134353101051913		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.2134353101051913 | validation: 4.168122369981362]
	TIME [epoch: 9.76 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.088906329534443		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.088906329534443 | validation: 4.153137299952216]
	TIME [epoch: 9.74 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0621253704632756		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.0621253704632756 | validation: 4.251401254516143]
	TIME [epoch: 9.75 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102256106085488		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.102256106085488 | validation: 4.4010587495637115]
	TIME [epoch: 9.77 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3238079817341104		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.3238079817341104 | validation: 4.2812152539282184]
	TIME [epoch: 9.75 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.159670641452528		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.159670641452528 | validation: 3.9769601135741555]
	TIME [epoch: 9.74 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0075117076962385		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.0075117076962385 | validation: 4.202211823817333]
	TIME [epoch: 9.74 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2022059723775107		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 3.2022059723775107 | validation: 4.016980310321176]
	TIME [epoch: 9.77 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.204408955318639		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.204408955318639 | validation: 4.904818617822918]
	TIME [epoch: 9.75 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1988705036965146		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.1988705036965146 | validation: 4.084778497065812]
	TIME [epoch: 9.74 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1483955899793523		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.1483955899793523 | validation: 4.558417294468993]
	TIME [epoch: 9.75 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4714289214113214		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.4714289214113214 | validation: 3.9024758863788755]
	TIME [epoch: 9.74 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3851368690694428		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.3851368690694428 | validation: 4.24017007857885]
	TIME [epoch: 9.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1535387025960686		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.1535387025960686 | validation: 3.844662649888217]
	TIME [epoch: 9.73 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.02030700943654		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.02030700943654 | validation: 4.1777970903448765]
	TIME [epoch: 9.76 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.119737038064211		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.119737038064211 | validation: 4.247231196650015]
	TIME [epoch: 9.74 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0375170366366295		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.0375170366366295 | validation: 4.380415815945953]
	TIME [epoch: 9.74 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2210201985950198		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.2210201985950198 | validation: 4.055037390932566]
	TIME [epoch: 9.75 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0378682973125115		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.0378682973125115 | validation: 4.0121584513863855]
	TIME [epoch: 9.75 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.073388231991995		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 3.073388231991995 | validation: 3.837534570893737]
	TIME [epoch: 9.75 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9978388378132794		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 2.9978388378132794 | validation: 3.929879534607412]
	TIME [epoch: 9.75 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020987362217852		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 3.020987362217852 | validation: 4.094700972538675]
	TIME [epoch: 9.76 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.049926721586983		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 3.049926721586983 | validation: 4.135105891711297]
	TIME [epoch: 9.75 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0049094046254097		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 3.0049094046254097 | validation: 4.084329486306016]
	TIME [epoch: 9.75 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0396164995877326		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.0396164995877326 | validation: 4.484795739850163]
	TIME [epoch: 9.75 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0283970387676247		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.0283970387676247 | validation: 4.800207699463908]
	TIME [epoch: 9.76 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.945407992196242		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 2.945407992196242 | validation: 4.818422161875607]
	TIME [epoch: 9.74 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0877170905259077		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.0877170905259077 | validation: 4.292039627961043]
	TIME [epoch: 9.74 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8385044608014867		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 2.8385044608014867 | validation: 4.6389967678330475]
	TIME [epoch: 9.76 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0776427826951216		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 3.0776427826951216 | validation: 4.055869484975112]
	TIME [epoch: 9.75 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.052532309843893		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.052532309843893 | validation: 4.227230986699273]
	TIME [epoch: 9.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9514789896614606		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 2.9514789896614606 | validation: 4.4952107275592335]
	TIME [epoch: 9.74 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976728922378835		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 2.976728922378835 | validation: 5.0277184563145605]
	TIME [epoch: 9.76 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1393807310185706		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 3.1393807310185706 | validation: 3.905784612138678]
	TIME [epoch: 9.74 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8830604283485277		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 2.8830604283485277 | validation: 4.213707000260852]
	TIME [epoch: 9.74 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8670127768888536		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 2.8670127768888536 | validation: 4.299942671164599]
	TIME [epoch: 9.75 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.922991040233845		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 2.922991040233845 | validation: 3.7709754118185974]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9136966781580673		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 2.9136966781580673 | validation: 4.059781463694739]
	TIME [epoch: 9.74 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978640834309514		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 2.978640834309514 | validation: 3.7277334908238777]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.844193882325147		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 2.844193882325147 | validation: 4.238002434011153]
	TIME [epoch: 9.76 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9205574226143725		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 2.9205574226143725 | validation: 3.7735290009640825]
	TIME [epoch: 9.76 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8326587949908335		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 2.8326587949908335 | validation: 4.196650464299579]
	TIME [epoch: 9.75 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2190420472689176		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 3.2190420472689176 | validation: 3.807345131473103]
	TIME [epoch: 9.77 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9450800680345868		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 2.9450800680345868 | validation: 4.043695609387253]
	TIME [epoch: 9.77 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9357159590407442		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 2.9357159590407442 | validation: 4.198494742125602]
	TIME [epoch: 9.75 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961804589729504		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 2.961804589729504 | validation: 3.9164110353399106]
	TIME [epoch: 9.75 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.80346249715921		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 2.80346249715921 | validation: 3.9809463358993065]
	TIME [epoch: 9.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9139896980758206		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 2.9139896980758206 | validation: 4.1109582216573655]
	TIME [epoch: 9.76 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.873904936757592		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 2.873904936757592 | validation: 3.8535824676012305]
	TIME [epoch: 9.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8605588737303767		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 2.8605588737303767 | validation: 4.0726357135585]
	TIME [epoch: 9.76 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.809225727702016		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 2.809225727702016 | validation: 4.080558819267201]
	TIME [epoch: 9.77 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8691420200782405		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 2.8691420200782405 | validation: 3.958403834276156]
	TIME [epoch: 9.75 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8347685289128584		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 2.8347685289128584 | validation: 4.102440399296182]
	TIME [epoch: 9.75 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8502949839116876		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 2.8502949839116876 | validation: 4.128701738654811]
	TIME [epoch: 9.77 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8670944743014504		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 2.8670944743014504 | validation: 3.857321140996873]
	TIME [epoch: 9.75 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.894772155815734		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 2.894772155815734 | validation: 3.9487498818769655]
	TIME [epoch: 9.74 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.841760858513221		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 2.841760858513221 | validation: 4.015488328036965]
	TIME [epoch: 9.75 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8345299927327465		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 2.8345299927327465 | validation: 4.010421770035031]
	TIME [epoch: 9.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8430038604951333		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 2.8430038604951333 | validation: 4.092904120637517]
	TIME [epoch: 9.75 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.785308065220202		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 2.785308065220202 | validation: 3.9288492701387736]
	TIME [epoch: 9.75 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0728050221347276		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.0728050221347276 | validation: 3.8278868211228563]
	TIME [epoch: 9.77 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.889110261924313		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 2.889110261924313 | validation: 4.1149370470145135]
	TIME [epoch: 9.76 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.887687105169436		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 2.887687105169436 | validation: 4.070091395831708]
	TIME [epoch: 9.76 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7691417171413035		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 2.7691417171413035 | validation: 3.8288107924214647]
	TIME [epoch: 9.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8025926587834418		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 2.8025926587834418 | validation: 3.7864005418321827]
	TIME [epoch: 9.76 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8513709475681344		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 2.8513709475681344 | validation: 3.9861864793932287]
	TIME [epoch: 9.75 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0232138901323022		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 3.0232138901323022 | validation: 3.8354988866878093]
	TIME [epoch: 9.74 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.867446142574997		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 2.867446142574997 | validation: 4.02364631754157]
	TIME [epoch: 9.76 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8712694561087493		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 2.8712694561087493 | validation: 3.8896183605878196]
	TIME [epoch: 9.76 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7287312877901266		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 2.7287312877901266 | validation: 3.7565010425647984]
	TIME [epoch: 9.75 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.778333371091677		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 2.778333371091677 | validation: 3.8745412485056874]
	TIME [epoch: 9.74 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.853976157444361		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 2.853976157444361 | validation: 3.81663373506721]
	TIME [epoch: 9.76 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6972525211431786		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 2.6972525211431786 | validation: 4.1653712677076635]
	TIME [epoch: 9.75 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.248534967229145		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.248534967229145 | validation: 3.8315733144950515]
	TIME [epoch: 9.75 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3691890580808526		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.3691890580808526 | validation: 4.636807354471143]
	TIME [epoch: 9.75 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9473321342919894		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 2.9473321342919894 | validation: 3.774065073596541]
	TIME [epoch: 9.76 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.871702234035001		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 2.871702234035001 | validation: 3.719783241533855]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.780160309024048		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 2.780160309024048 | validation: 3.749752083620689]
	TIME [epoch: 9.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9088928238303		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 2.9088928238303 | validation: 3.780630902493165]
	TIME [epoch: 9.76 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6721363030088527		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 2.6721363030088527 | validation: 4.114258598498966]
	TIME [epoch: 9.75 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8000317760880074		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 2.8000317760880074 | validation: 3.900068589761472]
	TIME [epoch: 9.74 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974445067795672		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 2.974445067795672 | validation: 3.795366529852844]
	TIME [epoch: 9.74 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3611526223212183		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 3.3611526223212183 | validation: 4.013476859791598]
	TIME [epoch: 9.76 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.769733670293082		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 2.769733670293082 | validation: 4.164325992618242]
	TIME [epoch: 9.73 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.878417585995904		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 2.878417585995904 | validation: 5.256609309553503]
	TIME [epoch: 9.73 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.139516792602908		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.139516792602908 | validation: 3.930331302443808]
	TIME [epoch: 9.75 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.787917464919969		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 2.787917464919969 | validation: 3.742494709248076]
	TIME [epoch: 9.74 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.596198826692247		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 2.596198826692247 | validation: 3.9655999310479446]
	TIME [epoch: 9.74 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.693815181156795		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 2.693815181156795 | validation: 3.8042509759881677]
	TIME [epoch: 9.75 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.751721768631678		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 2.751721768631678 | validation: 4.1902501332414035]
	TIME [epoch: 9.77 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.783690124317785		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 2.783690124317785 | validation: 3.8727412645064705]
	TIME [epoch: 9.74 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.707342175172786		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 2.707342175172786 | validation: 3.7928901776998725]
	TIME [epoch: 9.75 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.705399433620438		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 2.705399433620438 | validation: 3.9543100980786963]
	TIME [epoch: 9.75 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8155593288167693		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 2.8155593288167693 | validation: 3.9049893076307196]
	TIME [epoch: 9.75 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7032285709671315		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 2.7032285709671315 | validation: 3.8546710067123877]
	TIME [epoch: 9.75 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7383951343702733		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 2.7383951343702733 | validation: 3.8267640026143885]
	TIME [epoch: 9.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8348849998280974		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 2.8348849998280974 | validation: 4.07715608858336]
	TIME [epoch: 9.76 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.762772498239606		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 2.762772498239606 | validation: 3.7624239665718413]
	TIME [epoch: 9.75 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.683551843647421		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 2.683551843647421 | validation: 3.8657253387334625]
	TIME [epoch: 9.74 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6684244841867897		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 2.6684244841867897 | validation: 4.124597832054645]
	TIME [epoch: 9.74 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7486195326712495		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 2.7486195326712495 | validation: 3.9568588281550277]
	TIME [epoch: 9.76 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.575131048998453		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 2.575131048998453 | validation: 3.8728422455000078]
	TIME [epoch: 9.75 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.857032974000215		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 2.857032974000215 | validation: 3.753170773038324]
	TIME [epoch: 9.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.710713227647207		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 2.710713227647207 | validation: 3.824255789301882]
	TIME [epoch: 9.77 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.606399983474674		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 2.606399983474674 | validation: 4.136251037015151]
	TIME [epoch: 9.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.72458507572355		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 2.72458507572355 | validation: 4.027337579346004]
	TIME [epoch: 9.74 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.784310535901023		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 2.784310535901023 | validation: 3.8581333082664173]
	TIME [epoch: 9.74 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.656561340228004		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 2.656561340228004 | validation: 3.8273837396635257]
	TIME [epoch: 9.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7305975218044694		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 2.7305975218044694 | validation: 3.712285886988681]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6030214235478306		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 2.6030214235478306 | validation: 4.1635542582994525]
	TIME [epoch: 9.73 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.794554811961593		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 2.794554811961593 | validation: 3.6740324022727773]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6028967496213777		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 2.6028967496213777 | validation: 4.1450683927701855]
	TIME [epoch: 9.76 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7228826092327125		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 2.7228826092327125 | validation: 3.8522901264874174]
	TIME [epoch: 9.76 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.739532506660442		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 2.739532506660442 | validation: 3.74918431562792]
	TIME [epoch: 9.75 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7054660799449595		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 2.7054660799449595 | validation: 3.9856000229046558]
	TIME [epoch: 9.77 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.711376909133411		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 2.711376909133411 | validation: 3.654200298250994]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8111601559855464		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 2.8111601559855464 | validation: 4.056614020379859]
	TIME [epoch: 9.74 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7048245319050763		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 2.7048245319050763 | validation: 3.7561046535522133]
	TIME [epoch: 9.75 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6202776648351644		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 2.6202776648351644 | validation: 4.162486380457932]
	TIME [epoch: 9.75 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5911787176533134		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 2.5911787176533134 | validation: 3.8708540249557433]
	TIME [epoch: 9.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7304602423555124		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 2.7304602423555124 | validation: 4.16207136747471]
	TIME [epoch: 9.74 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6271457156409586		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 2.6271457156409586 | validation: 4.56356744345884]
	TIME [epoch: 9.76 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.996800644480415		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 2.996800644480415 | validation: 4.220404581013018]
	TIME [epoch: 9.75 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.656198305640026		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 2.656198305640026 | validation: 3.8375121174069564]
	TIME [epoch: 9.75 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8971167149953034		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 2.8971167149953034 | validation: 3.7567220688962033]
	TIME [epoch: 9.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6967029270921694		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 2.6967029270921694 | validation: 3.6734290068414874]
	TIME [epoch: 9.75 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7266518495573857		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 2.7266518495573857 | validation: 3.8053324906549166]
	TIME [epoch: 9.74 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6975474882007555		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 2.6975474882007555 | validation: 4.154600861710369]
	TIME [epoch: 9.75 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6181923860052185		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 2.6181923860052185 | validation: 3.658285608382581]
	TIME [epoch: 9.76 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7624779116540945		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 2.7624779116540945 | validation: 3.657406313205468]
	TIME [epoch: 9.75 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8534730744906143		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 2.8534730744906143 | validation: 3.7756474044325192]
	TIME [epoch: 9.75 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.600380604504349		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 2.600380604504349 | validation: 4.234585107318407]
	TIME [epoch: 9.74 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6294537172625274		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 2.6294537172625274 | validation: 4.010719544426159]
	TIME [epoch: 9.76 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7123049240086656		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 2.7123049240086656 | validation: 4.019171670108155]
	TIME [epoch: 9.75 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.719988491486732		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 2.719988491486732 | validation: 3.7749544863087774]
	TIME [epoch: 9.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6516091529953902		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 2.6516091529953902 | validation: 3.7726927093879157]
	TIME [epoch: 9.76 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.433763953600426		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.433763953600426 | validation: 3.990586594762938]
	TIME [epoch: 9.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.549095178516818		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 2.549095178516818 | validation: 4.104667342564064]
	TIME [epoch: 9.75 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7255359826980756		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 2.7255359826980756 | validation: 3.730212869550563]
	TIME [epoch: 9.75 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6488205470641377		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 2.6488205470641377 | validation: 4.361552988615016]
	TIME [epoch: 9.76 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.787428747292669		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 2.787428747292669 | validation: 3.771983115462955]
	TIME [epoch: 9.75 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.601571925433645		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 2.601571925433645 | validation: 3.7510660905549713]
	TIME [epoch: 9.75 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.68970030124431		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 2.68970030124431 | validation: 3.989433969466643]
	TIME [epoch: 9.75 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.644661075400694		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 2.644661075400694 | validation: 3.7560764334365495]
	TIME [epoch: 9.75 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6060538440774215		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 2.6060538440774215 | validation: 3.7218222601339046]
	TIME [epoch: 9.74 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.821052542991946		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 2.821052542991946 | validation: 3.615680059434168]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6127292353177265		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 2.6127292353177265 | validation: 3.724902587975564]
	TIME [epoch: 9.76 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.570037782170792		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 2.570037782170792 | validation: 3.640599050954943]
	TIME [epoch: 9.76 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8526171046462894		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 2.8526171046462894 | validation: 3.8870220338382215]
	TIME [epoch: 9.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6705996704217148		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 2.6705996704217148 | validation: 3.6667676862231837]
	TIME [epoch: 9.75 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.577362333074061		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 2.577362333074061 | validation: 4.494261051625751]
	TIME [epoch: 9.75 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6909778406209384		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 2.6909778406209384 | validation: 3.9688082159620413]
	TIME [epoch: 9.74 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.639226975752775		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 2.639226975752775 | validation: 3.8203553953027507]
	TIME [epoch: 9.74 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7576164732178476		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 2.7576164732178476 | validation: 3.7135906676866184]
	TIME [epoch: 9.74 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7870655502929718		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 2.7870655502929718 | validation: 3.643945024508687]
	TIME [epoch: 9.74 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6784596525160955		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 2.6784596525160955 | validation: 3.619137712176844]
	TIME [epoch: 9.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6992808867876166		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 2.6992808867876166 | validation: 3.6111694572296695]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.662455795335185		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 2.662455795335185 | validation: 3.8537702693982734]
	TIME [epoch: 9.77 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6232114824301185		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 2.6232114824301185 | validation: 3.903004346546659]
	TIME [epoch: 9.75 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6270536348097804		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 2.6270536348097804 | validation: 3.7271629932936827]
	TIME [epoch: 9.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6619867942674964		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 2.6619867942674964 | validation: 3.701763406350928]
	TIME [epoch: 9.76 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5765710245811944		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 2.5765710245811944 | validation: 3.709252503653937]
	TIME [epoch: 9.74 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7192036610988013		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 2.7192036610988013 | validation: 4.336825803661732]
	TIME [epoch: 9.74 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506452532127523		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 2.9506452532127523 | validation: 3.7570100370532917]
	TIME [epoch: 9.74 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6850635117129773		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 2.6850635117129773 | validation: 3.7221824657031597]
	TIME [epoch: 9.77 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6160611577471475		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 2.6160611577471475 | validation: 3.8081991727985485]
	TIME [epoch: 9.74 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.570287338130575		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 2.570287338130575 | validation: 3.936226553644353]
	TIME [epoch: 9.75 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6402245266863353		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 2.6402245266863353 | validation: 4.214173653944541]
	TIME [epoch: 9.76 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.674567471963295		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 2.674567471963295 | validation: 3.769183547637372]
	TIME [epoch: 9.75 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5707560270940464		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 2.5707560270940464 | validation: 4.360873455206173]
	TIME [epoch: 9.75 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7356145640306755		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 2.7356145640306755 | validation: 3.6636374349260628]
	TIME [epoch: 9.74 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5344453137676597		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 2.5344453137676597 | validation: 3.7222579093729666]
	TIME [epoch: 9.75 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6285178509860048		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 2.6285178509860048 | validation: 3.6574771491697193]
	TIME [epoch: 9.74 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.431464382869076		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 2.431464382869076 | validation: 4.510938173233162]
	TIME [epoch: 9.75 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6974849390815416		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 2.6974849390815416 | validation: 3.6333587614887235]
	TIME [epoch: 9.76 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5397940561911154		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 2.5397940561911154 | validation: 3.772954115726199]
	TIME [epoch: 9.75 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.558541093529446		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 2.558541093529446 | validation: 3.6455332116380856]
	TIME [epoch: 9.74 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5543664683045884		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 2.5543664683045884 | validation: 3.6350591761621933]
	TIME [epoch: 9.74 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.47951125651482		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 2.47951125651482 | validation: 3.7616333976001695]
	TIME [epoch: 9.77 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.575247897834171		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 2.575247897834171 | validation: 3.693461126200595]
	TIME [epoch: 9.75 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5257523945841744		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 2.5257523945841744 | validation: 3.66392767552813]
	TIME [epoch: 9.75 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4822644346888767		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 2.4822644346888767 | validation: 3.585819631281234]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5112236410001385		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 2.5112236410001385 | validation: 3.8079877813559233]
	TIME [epoch: 9.77 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.557430940480051		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 2.557430940480051 | validation: 3.659216094342503]
	TIME [epoch: 9.75 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.645776897056252		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 2.645776897056252 | validation: 3.8066718452588346]
	TIME [epoch: 9.75 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6887067028420533		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 2.6887067028420533 | validation: 3.851266855869797]
	TIME [epoch: 9.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.541394102359977		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 2.541394102359977 | validation: 3.6109752551896244]
	TIME [epoch: 9.74 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.526434824364766		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 2.526434824364766 | validation: 3.592698055208101]
	TIME [epoch: 9.74 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5923114528953173		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 2.5923114528953173 | validation: 3.7178176382295867]
	TIME [epoch: 9.74 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5454062389781202		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 2.5454062389781202 | validation: 3.6332172153771047]
	TIME [epoch: 9.76 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.474917737387687		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 2.474917737387687 | validation: 3.901427113632327]
	TIME [epoch: 9.74 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.660569579032846		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 2.660569579032846 | validation: 3.893405521715148]
	TIME [epoch: 9.75 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5857096748433084		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 2.5857096748433084 | validation: 3.5725313632704006]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5473258130653944		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 2.5473258130653944 | validation: 3.577215289403512]
	TIME [epoch: 9.74 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.542697473570969		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 2.542697473570969 | validation: 3.6343650899160522]
	TIME [epoch: 9.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5224929485854397		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 2.5224929485854397 | validation: 4.103850574717978]
	TIME [epoch: 9.74 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6668346651883788		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 2.6668346651883788 | validation: 3.7676253879417305]
	TIME [epoch: 9.77 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.507043515609979		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 2.507043515609979 | validation: 3.9002034931443146]
	TIME [epoch: 9.74 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.610961948265074		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 2.610961948265074 | validation: 3.753538397403391]
	TIME [epoch: 9.74 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5357389475284546		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 2.5357389475284546 | validation: 3.7461353811998435]
	TIME [epoch: 9.75 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.517332367094963		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 2.517332367094963 | validation: 3.7128904902139364]
	TIME [epoch: 9.75 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6217886961474965		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 2.6217886961474965 | validation: 3.7870295058335763]
	TIME [epoch: 9.74 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5337454941905393		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 2.5337454941905393 | validation: 3.5829015613189665]
	TIME [epoch: 9.73 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4393315094070216		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 2.4393315094070216 | validation: 3.6519856457157744]
	TIME [epoch: 9.76 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.458161710854216		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 2.458161710854216 | validation: 3.9572716206862397]
	TIME [epoch: 9.74 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5279154687800385		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 2.5279154687800385 | validation: 3.617511986283406]
	TIME [epoch: 9.73 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5319978966732783		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 2.5319978966732783 | validation: 3.9086807221317987]
	TIME [epoch: 9.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.701728056748941		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 2.701728056748941 | validation: 3.571646124995956]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.801080294057015		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 2.801080294057015 | validation: 4.439174247026607]
	TIME [epoch: 9.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.755771316962323		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 2.755771316962323 | validation: 3.5814983820421875]
	TIME [epoch: 9.74 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5322115712323376		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 2.5322115712323376 | validation: 3.7399780896726327]
	TIME [epoch: 9.76 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.498483927010288		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 2.498483927010288 | validation: 3.5655621464602008]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.509662386172739		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 2.509662386172739 | validation: 3.589334482957605]
	TIME [epoch: 9.74 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.442301069375973		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 2.442301069375973 | validation: 3.9014320919807894]
	TIME [epoch: 9.76 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6352714558237476		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 2.6352714558237476 | validation: 3.863327482569803]
	TIME [epoch: 9.76 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.498141177450367		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 2.498141177450367 | validation: 4.169808555081312]
	TIME [epoch: 9.75 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8112736037706894		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 2.8112736037706894 | validation: 3.7285051009892176]
	TIME [epoch: 9.75 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.533878267704175		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 2.533878267704175 | validation: 3.57944960209408]
	TIME [epoch: 9.76 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.422855418617684		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 2.422855418617684 | validation: 3.6885191547142533]
	TIME [epoch: 9.74 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4608534165631353		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 2.4608534165631353 | validation: 3.5838758129249526]
	TIME [epoch: 9.74 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.456658874450785		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 2.456658874450785 | validation: 3.7441521303216563]
	TIME [epoch: 9.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5219366677513366		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 2.5219366677513366 | validation: 3.9023031946104276]
	TIME [epoch: 9.77 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5273523865186096		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 2.5273523865186096 | validation: 3.921858349705193]
	TIME [epoch: 9.75 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.483285334054544		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 2.483285334054544 | validation: 3.8503759488909886]
	TIME [epoch: 9.75 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5048385374005417		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 2.5048385374005417 | validation: 4.399600181930898]
	TIME [epoch: 9.76 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7187153123249517		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 2.7187153123249517 | validation: 3.9290205241853933]
	TIME [epoch: 9.76 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4963956412457295		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 2.4963956412457295 | validation: 3.9916777505911725]
	TIME [epoch: 9.75 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8306691008430303		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 2.8306691008430303 | validation: 3.920948968898001]
	TIME [epoch: 9.75 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4878604194332796		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 2.4878604194332796 | validation: 3.711990276790699]
	TIME [epoch: 9.77 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4844153166521297		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 2.4844153166521297 | validation: 3.7446556761882053]
	TIME [epoch: 9.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6264035686691107		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 2.6264035686691107 | validation: 3.595011744567043]
	TIME [epoch: 9.74 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.668022712024211		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 2.668022712024211 | validation: 3.6348496846672016]
	TIME [epoch: 9.75 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5446491884065567		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 2.5446491884065567 | validation: 3.63233721365582]
	TIME [epoch: 9.76 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.468491311737909		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 2.468491311737909 | validation: 3.572265236495285]
	TIME [epoch: 9.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5406439901553552		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 2.5406439901553552 | validation: 3.821807389218518]
	TIME [epoch: 9.74 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.477456604415438		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 2.477456604415438 | validation: 3.6270003934361488]
	TIME [epoch: 9.77 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4376824308415053		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 2.4376824308415053 | validation: 3.5951583586472613]
	TIME [epoch: 9.75 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.484610957945004		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 2.484610957945004 | validation: 3.764920494339469]
	TIME [epoch: 9.74 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.604690946006065		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 2.604690946006065 | validation: 3.948419899928688]
	TIME [epoch: 9.75 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.579950096479026		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 2.579950096479026 | validation: 3.6005714561756883]
	TIME [epoch: 9.76 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5210612447101446		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 2.5210612447101446 | validation: 3.7571217777521144]
	TIME [epoch: 9.74 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5279272902362324		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 2.5279272902362324 | validation: 3.5707397160548444]
	TIME [epoch: 9.75 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.56406159668837		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 2.56406159668837 | validation: 3.7306759545607378]
	TIME [epoch: 9.77 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4392047727919994		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 2.4392047727919994 | validation: 3.611832333685638]
	TIME [epoch: 9.75 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4999654790418857		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 2.4999654790418857 | validation: 3.9888730548980376]
	TIME [epoch: 9.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.708529091048974		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 2.708529091048974 | validation: 3.9306932734447777]
	TIME [epoch: 9.74 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6223550791024635		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 2.6223550791024635 | validation: 3.6542325335649224]
	TIME [epoch: 9.77 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6732464648251013		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 2.6732464648251013 | validation: 4.039129878176462]
	TIME [epoch: 9.74 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6698950288458962		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 2.6698950288458962 | validation: 3.5622032240941017]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4098372624656124		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 2.4098372624656124 | validation: 3.6434118802313926]
	TIME [epoch: 9.77 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.444484552461728		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 2.444484552461728 | validation: 3.6298394862963375]
	TIME [epoch: 9.76 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.549495384903552		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 2.549495384903552 | validation: 3.8707268894629165]
	TIME [epoch: 9.75 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5212505901596374		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 2.5212505901596374 | validation: 3.947958940679485]
	TIME [epoch: 9.75 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6096309732611744		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 2.6096309732611744 | validation: 3.5581247153134927]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4469853434404127		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 2.4469853434404127 | validation: 4.403319808373744]
	TIME [epoch: 9.75 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.701308729341395		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 2.701308729341395 | validation: 3.603723318802588]
	TIME [epoch: 9.74 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.562667890573931		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 2.562667890573931 | validation: 3.6497176123907593]
	TIME [epoch: 9.76 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.441797618569148		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 2.441797618569148 | validation: 3.5499627684452095]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6843045558770675		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 2.6843045558770675 | validation: 4.093082709013069]
	TIME [epoch: 9.75 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6860189096801212		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 2.6860189096801212 | validation: 3.5456289662445464]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5475317711611836		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 2.5475317711611836 | validation: 3.7737791150011946]
	TIME [epoch: 9.77 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6052796223447126		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 2.6052796223447126 | validation: 3.6492541437066133]
	TIME [epoch: 9.75 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.582341843621659		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 2.582341843621659 | validation: 3.6646907230006933]
	TIME [epoch: 9.75 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.476118013727709		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 2.476118013727709 | validation: 3.6511839949509866]
	TIME [epoch: 9.76 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5227713925392727		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 2.5227713925392727 | validation: 3.5820066308867395]
	TIME [epoch: 9.77 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.481787753346481		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 2.481787753346481 | validation: 3.721460784690596]
	TIME [epoch: 9.76 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4645130048209314		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 2.4645130048209314 | validation: 3.659584368513614]
	TIME [epoch: 9.76 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.435372042624441		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 2.435372042624441 | validation: 3.7285595306043344]
	TIME [epoch: 9.78 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4516234886783144		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 2.4516234886783144 | validation: 3.685316351524178]
	TIME [epoch: 9.76 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3782677597085544		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 2.3782677597085544 | validation: 3.870098175557634]
	TIME [epoch: 9.76 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5125182248636504		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 2.5125182248636504 | validation: 3.542394372566319]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4118206715781882		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 2.4118206715781882 | validation: 3.582043497348993]
	TIME [epoch: 9.77 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.491347953762884		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 2.491347953762884 | validation: 3.5451383400517886]
	TIME [epoch: 9.76 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.461999057616409		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 2.461999057616409 | validation: 3.812629511682694]
	TIME [epoch: 9.76 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4132442582262783		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 2.4132442582262783 | validation: 3.6093953704004185]
	TIME [epoch: 9.78 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4510703846861768		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 2.4510703846861768 | validation: 3.6405906724520647]
	TIME [epoch: 9.76 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4455348261838243		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 2.4455348261838243 | validation: 3.7101412491365022]
	TIME [epoch: 9.75 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.391715631297096		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 2.391715631297096 | validation: 3.538939032700238]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4635489413395515		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 2.4635489413395515 | validation: 3.6393667202247912]
	TIME [epoch: 9.77 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4588275191547493		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 2.4588275191547493 | validation: 3.697519757964479]
	TIME [epoch: 9.75 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4534813863362848		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 2.4534813863362848 | validation: 4.115515128941608]
	TIME [epoch: 9.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6749375756124727		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 2.6749375756124727 | validation: 3.5468351627891934]
	TIME [epoch: 9.77 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4924009685328334		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 2.4924009685328334 | validation: 3.653572579421365]
	TIME [epoch: 9.76 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4777416027306725		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 2.4777416027306725 | validation: 3.688649320322349]
	TIME [epoch: 9.76 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.438769443935008		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 2.438769443935008 | validation: 3.5431063759233883]
	TIME [epoch: 9.76 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4964508979412092		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 2.4964508979412092 | validation: 3.657841216903311]
	TIME [epoch: 9.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.434143648632858		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 2.434143648632858 | validation: 3.536865630484603]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3829412946249158		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 2.3829412946249158 | validation: 3.5495015275648654]
	TIME [epoch: 9.75 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4228880370438564		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 2.4228880370438564 | validation: 3.5705513816457484]
	TIME [epoch: 9.78 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4231368183489086		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 2.4231368183489086 | validation: 3.54214008096855]
	TIME [epoch: 9.76 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4626757167626145		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 2.4626757167626145 | validation: 3.6042418698387504]
	TIME [epoch: 9.75 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.520182030589176		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 2.520182030589176 | validation: 3.7313377975826776]
	TIME [epoch: 9.75 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.431763683633137		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 2.431763683633137 | validation: 3.5326627969607918]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3891400068813047		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 2.3891400068813047 | validation: 3.490880647915624]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4448650792642823		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 2.4448650792642823 | validation: 3.900725821104968]
	TIME [epoch: 9.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4943690028342838		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 2.4943690028342838 | validation: 3.5029594286403656]
	TIME [epoch: 9.76 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4300232281244556		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 2.4300232281244556 | validation: 3.562511979997613]
	TIME [epoch: 9.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4162839138429626		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 2.4162839138429626 | validation: 3.5353186217891226]
	TIME [epoch: 9.74 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.42740940484903		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 2.42740940484903 | validation: 3.7399922964247287]
	TIME [epoch: 9.74 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4189616850275533		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 2.4189616850275533 | validation: 3.6439074479331586]
	TIME [epoch: 9.76 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3912138474190754		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 2.3912138474190754 | validation: 3.6356125410658824]
	TIME [epoch: 9.74 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4075689036302896		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 2.4075689036302896 | validation: 3.692746087804275]
	TIME [epoch: 9.74 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4132200716954992		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 2.4132200716954992 | validation: 3.6566677534835508]
	TIME [epoch: 9.76 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4283165222841765		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 2.4283165222841765 | validation: 3.530122904300906]
	TIME [epoch: 9.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.424768084114898		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 2.424768084114898 | validation: 3.5904676263731234]
	TIME [epoch: 9.75 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3971400070487454		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 2.3971400070487454 | validation: 3.524621998253199]
	TIME [epoch: 9.74 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.373390200725736		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 2.373390200725736 | validation: 3.540894741279597]
	TIME [epoch: 9.76 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3861810710908187		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 2.3861810710908187 | validation: 3.5932753388145735]
	TIME [epoch: 9.77 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4600274851371333		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 2.4600274851371333 | validation: 3.499409562504244]
	TIME [epoch: 9.73 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4478198020285484		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 2.4478198020285484 | validation: 3.5243738922081818]
	TIME [epoch: 9.74 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.376460455654802		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 2.376460455654802 | validation: 4.238056036753666]
	TIME [epoch: 9.76 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7305768084366235		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 2.7305768084366235 | validation: 3.577613577040078]
	TIME [epoch: 9.76 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4526123223912477		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 2.4526123223912477 | validation: 3.757059277821043]
	TIME [epoch: 9.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.461898610702917		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 2.461898610702917 | validation: 3.5416530711734153]
	TIME [epoch: 9.78 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.439449072773676		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 2.439449072773676 | validation: 3.557925832266282]
	TIME [epoch: 9.77 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3782656946941136		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 2.3782656946941136 | validation: 3.5053321879485466]
	TIME [epoch: 9.74 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3840904068269313		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 2.3840904068269313 | validation: 3.578982309759872]
	TIME [epoch: 9.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4806767440358044		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 2.4806767440358044 | validation: 3.5960386213459983]
	TIME [epoch: 9.76 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3673545035773627		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 2.3673545035773627 | validation: 3.596681545690224]
	TIME [epoch: 9.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4873258960319733		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 2.4873258960319733 | validation: 3.5320321687257525]
	TIME [epoch: 9.74 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4580355143784276		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 2.4580355143784276 | validation: 3.492261163576364]
	TIME [epoch: 9.77 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3652010295546093		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 2.3652010295546093 | validation: 3.5385048019872345]
	TIME [epoch: 9.74 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4180874074760688		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 2.4180874074760688 | validation: 3.507959040417596]
	TIME [epoch: 9.74 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4108998498930405		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 2.4108998498930405 | validation: 3.5087454726693306]
	TIME [epoch: 9.74 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4128770461408795		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 2.4128770461408795 | validation: 3.562277027005736]
	TIME [epoch: 9.76 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.418627188790324		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 2.418627188790324 | validation: 3.97045300870461]
	TIME [epoch: 9.74 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.48526627671247		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 2.48526627671247 | validation: 3.5087401454208895]
	TIME [epoch: 9.74 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.438134150288307		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 2.438134150288307 | validation: 3.5084717419704066]
	TIME [epoch: 9.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3581792734543234		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 2.3581792734543234 | validation: 3.5965720677588475]
	TIME [epoch: 9.74 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3839177130202343		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 2.3839177130202343 | validation: 3.5561921522974127]
	TIME [epoch: 9.74 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3450676767805834		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 2.3450676767805834 | validation: 3.52281726076519]
	TIME [epoch: 9.73 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4826913695803263		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 2.4826913695803263 | validation: 3.653633765666001]
	TIME [epoch: 9.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4788943873798734		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 2.4788943873798734 | validation: 3.5331874341594833]
	TIME [epoch: 9.75 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.339552941918769		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 2.339552941918769 | validation: 3.4855799903783886]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3699683370230553		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 2.3699683370230553 | validation: 3.500597379633431]
	TIME [epoch: 9.75 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3840778579499142		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 2.3840778579499142 | validation: 3.647434625023225]
	TIME [epoch: 9.76 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.352453299425725		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 2.352453299425725 | validation: 3.514349412689727]
	TIME [epoch: 9.75 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3401936222668454		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 2.3401936222668454 | validation: 3.9996737407961267]
	TIME [epoch: 9.76 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.435870124310161		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 2.435870124310161 | validation: 3.814117690442157]
	TIME [epoch: 9.77 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4042820769463704		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 2.4042820769463704 | validation: 3.6039593468843236]
	TIME [epoch: 9.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.372361802987768		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 2.372361802987768 | validation: 3.541104804352124]
	TIME [epoch: 9.74 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5250939841845197		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 2.5250939841845197 | validation: 3.90654678208222]
	TIME [epoch: 9.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4888144896382407		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 2.4888144896382407 | validation: 3.615926970637271]
	TIME [epoch: 9.76 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.421464193256635		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 2.421464193256635 | validation: 3.839273059023728]
	TIME [epoch: 9.74 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.387104934280887		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 2.387104934280887 | validation: 3.549409198912751]
	TIME [epoch: 9.74 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4535894453026788		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 2.4535894453026788 | validation: 3.508033181464689]
	TIME [epoch: 9.77 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3503274646229984		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 2.3503274646229984 | validation: 3.518787062904619]
	TIME [epoch: 9.75 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3513271363260815		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 2.3513271363260815 | validation: 3.5458431718269585]
	TIME [epoch: 9.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3365761128335167		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 2.3365761128335167 | validation: 3.4751110626174846]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3507871126859126		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 2.3507871126859126 | validation: 3.8471085256684554]
	TIME [epoch: 9.78 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5011085847902104		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 2.5011085847902104 | validation: 3.598959080961206]
	TIME [epoch: 9.75 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3663454132708823		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 2.3663454132708823 | validation: 3.7023636882835746]
	TIME [epoch: 9.75 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.39078322709721		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 2.39078322709721 | validation: 3.710960498146874]
	TIME [epoch: 9.75 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3854473495395623		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 2.3854473495395623 | validation: 3.5233903087674108]
	TIME [epoch: 9.76 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3686224446714865		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 2.3686224446714865 | validation: 3.6855228412798953]
	TIME [epoch: 9.75 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.399341889219072		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 2.399341889219072 | validation: 3.5367010312413734]
	TIME [epoch: 9.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.362190196577127		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 2.362190196577127 | validation: 3.5151420052890234]
	TIME [epoch: 9.76 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.352527038494929		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 2.352527038494929 | validation: 3.56879403026536]
	TIME [epoch: 9.76 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.370685075261508		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 2.370685075261508 | validation: 3.6097697314622463]
	TIME [epoch: 9.75 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3799642074385177		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 2.3799642074385177 | validation: 3.547334711971384]
	TIME [epoch: 9.76 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3910189143289227		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 2.3910189143289227 | validation: 3.621334180176409]
	TIME [epoch: 9.75 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3959955632253696		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 2.3959955632253696 | validation: 3.497823724270195]
	TIME [epoch: 9.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3503598116308613		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 2.3503598116308613 | validation: 3.639659269196579]
	TIME [epoch: 9.75 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3357233133189457		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 2.3357233133189457 | validation: 3.6087937481146612]
	TIME [epoch: 9.76 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4452488359436115		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 2.4452488359436115 | validation: 3.4706017174000197]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.47687321649224		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 2.47687321649224 | validation: 3.5894396431335247]
	TIME [epoch: 9.76 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.358127182554071		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 2.358127182554071 | validation: 3.5041807908348597]
	TIME [epoch: 9.74 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3400764881989184		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 2.3400764881989184 | validation: 3.48888177878284]
	TIME [epoch: 9.78 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3536034331862927		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 2.3536034331862927 | validation: 3.490816175279537]
	TIME [epoch: 9.74 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4589390201172536		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 2.4589390201172536 | validation: 3.533825392068916]
	TIME [epoch: 9.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4082934827429723		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 2.4082934827429723 | validation: 3.9957578982525135]
	TIME [epoch: 9.77 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4245293892215263		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 2.4245293892215263 | validation: 3.5232681670206274]
	TIME [epoch: 9.74 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3140510845156133		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 2.3140510845156133 | validation: 3.592889812832826]
	TIME [epoch: 9.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4164714970684695		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 2.4164714970684695 | validation: 3.678383569234536]
	TIME [epoch: 9.74 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3716144731748727		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 2.3716144731748727 | validation: 3.5071316772939065]
	TIME [epoch: 9.75 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3814279895449344		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 2.3814279895449344 | validation: 3.508292863875415]
	TIME [epoch: 9.74 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4070430715101963		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 2.4070430715101963 | validation: 3.802199283942247]
	TIME [epoch: 9.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3534785583125832		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 2.3534785583125832 | validation: 3.5179719441540045]
	TIME [epoch: 9.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3331981132440145		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 2.3331981132440145 | validation: 3.486760595038268]
	TIME [epoch: 9.76 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.361681869311684		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 2.361681869311684 | validation: 3.551976817731283]
	TIME [epoch: 9.75 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3448493611289556		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 2.3448493611289556 | validation: 3.594524662130266]
	TIME [epoch: 9.74 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3721853999135925		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 2.3721853999135925 | validation: 3.5803407325992165]
	TIME [epoch: 9.77 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.345937860779057		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 2.345937860779057 | validation: 3.4731083144775803]
	TIME [epoch: 9.76 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3464138944722723		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 2.3464138944722723 | validation: 3.505433807100755]
	TIME [epoch: 9.75 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3217177460143095		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 2.3217177460143095 | validation: 3.7693723974433384]
	TIME [epoch: 9.75 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.428992736438737		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 2.428992736438737 | validation: 3.561897292104604]
	TIME [epoch: 9.75 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3770121045469113		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 2.3770121045469113 | validation: 3.5054667209090447]
	TIME [epoch: 9.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3514606117390353		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 2.3514606117390353 | validation: 3.566263811582741]
	TIME [epoch: 9.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3221161387828744		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 2.3221161387828744 | validation: 3.470286972908914]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.343250521596732		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 2.343250521596732 | validation: 3.669216117360299]
	TIME [epoch: 9.74 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4847594072435557		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 2.4847594072435557 | validation: 3.6458097750980567]
	TIME [epoch: 9.74 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.36615642466782		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 2.36615642466782 | validation: 3.4896541042220033]
	TIME [epoch: 9.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.397942826936231		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 2.397942826936231 | validation: 3.8158512597820646]
	TIME [epoch: 9.76 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4289737020272		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 2.4289737020272 | validation: 3.529886276373948]
	TIME [epoch: 9.75 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3305514529393423		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 2.3305514529393423 | validation: 3.55378223614635]
	TIME [epoch: 9.74 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3426400988642806		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 2.3426400988642806 | validation: 3.500090978670365]
	TIME [epoch: 9.75 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3820460242862733		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 2.3820460242862733 | validation: 3.4731936567658366]
	TIME [epoch: 9.74 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.351725409552782		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 2.351725409552782 | validation: 3.522291564752076]
	TIME [epoch: 9.76 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3599344557880704		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 2.3599344557880704 | validation: 3.4757662165507135]
	TIME [epoch: 9.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.359667956781314		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 2.359667956781314 | validation: 3.4896477572940015]
	TIME [epoch: 9.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2856471293265015		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 2.2856471293265015 | validation: 3.5570331319991464]
	TIME [epoch: 9.74 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.394117381997164		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 2.394117381997164 | validation: 3.8167606835989627]
	TIME [epoch: 9.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3912874957692662		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 2.3912874957692662 | validation: 3.668998975639529]
	TIME [epoch: 9.76 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4445189202125874		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 2.4445189202125874 | validation: 3.5912501342600542]
	TIME [epoch: 9.74 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.381876369806592		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 2.381876369806592 | validation: 3.5994593245045086]
	TIME [epoch: 9.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3319177341395205		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 2.3319177341395205 | validation: 3.57679555390057]
	TIME [epoch: 9.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3248628261096047		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 2.3248628261096047 | validation: 3.671325364987662]
	TIME [epoch: 9.77 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3576890105366712		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 2.3576890105366712 | validation: 3.536728450343095]
	TIME [epoch: 9.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.377273315432604		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 2.377273315432604 | validation: 3.459757161252471]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_545.pth
	Model improved!!!
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3854209129485646		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 2.3854209129485646 | validation: 3.523017374754577]
	TIME [epoch: 9.78 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.384449888239647		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 2.384449888239647 | validation: 3.5237781050085397]
	TIME [epoch: 9.77 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.345343960369548		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 2.345343960369548 | validation: 3.4624873997900294]
	TIME [epoch: 9.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.314361864907853		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 2.314361864907853 | validation: 3.4843644539271628]
	TIME [epoch: 9.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.417860702272842		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 2.417860702272842 | validation: 3.5620242864669853]
	TIME [epoch: 9.78 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3650016665959126		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 2.3650016665959126 | validation: 3.4848765227777228]
	TIME [epoch: 9.76 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3111630648145685		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 2.3111630648145685 | validation: 3.4967817646824932]
	TIME [epoch: 9.76 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.282798317018738		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 2.282798317018738 | validation: 3.5367818690120694]
	TIME [epoch: 9.77 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3479935648420147		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 2.3479935648420147 | validation: 3.4818265790435357]
	TIME [epoch: 9.77 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3501394218396774		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 2.3501394218396774 | validation: 3.7945760685879204]
	TIME [epoch: 9.76 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4399624298957057		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 2.4399624298957057 | validation: 3.547806999882837]
	TIME [epoch: 9.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3682259626606625		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 2.3682259626606625 | validation: 3.561554331789529]
	TIME [epoch: 9.78 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3066763648205533		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 2.3066763648205533 | validation: 3.4496812477441647]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4114794767620653		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 2.4114794767620653 | validation: 3.4926585409906035]
	TIME [epoch: 9.76 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.311441957112048		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 2.311441957112048 | validation: 3.67138162566542]
	TIME [epoch: 9.76 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4417528630213745		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 2.4417528630213745 | validation: 3.6791044204512473]
	TIME [epoch: 9.78 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3509178859817874		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 2.3509178859817874 | validation: 3.476903266854605]
	TIME [epoch: 9.76 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.295877892293809		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 2.295877892293809 | validation: 3.4694223407717084]
	TIME [epoch: 9.77 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.306504311839668		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 2.306504311839668 | validation: 3.5141036821135714]
	TIME [epoch: 9.78 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3080492063671088		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 2.3080492063671088 | validation: 3.503671387282939]
	TIME [epoch: 9.77 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.317955416338253		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 2.317955416338253 | validation: 3.4983609642126807]
	TIME [epoch: 9.76 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.308056778480586		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 2.308056778480586 | validation: 3.5276410990755913]
	TIME [epoch: 9.76 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.376267847573692		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 2.376267847573692 | validation: 3.6227001413219497]
	TIME [epoch: 9.78 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3330323782970317		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 2.3330323782970317 | validation: 3.8370588398453336]
	TIME [epoch: 9.77 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.372172042788806		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 2.372172042788806 | validation: 3.46933703102861]
	TIME [epoch: 9.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2810260470874715		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 2.2810260470874715 | validation: 3.5768723012719277]
	TIME [epoch: 9.78 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3811560564542633		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 2.3811560564542633 | validation: 3.543575329593584]
	TIME [epoch: 9.76 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3286557626099182		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 2.3286557626099182 | validation: 3.5266911352358923]
	TIME [epoch: 9.75 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3273050747729656		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 2.3273050747729656 | validation: 3.7524764235507697]
	TIME [epoch: 9.76 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.364941601338124		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 2.364941601338124 | validation: 3.519855769200176]
	TIME [epoch: 9.77 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3048045320601966		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 2.3048045320601966 | validation: 3.4466150787978065]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.387207820110648		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 2.387207820110648 | validation: 3.5077693085404884]
	TIME [epoch: 9.75 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.363026966008103		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 2.363026966008103 | validation: 3.5045879515085163]
	TIME [epoch: 9.76 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3216227692496134		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 2.3216227692496134 | validation: 3.472486006959885]
	TIME [epoch: 9.76 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3100095713409727		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 2.3100095713409727 | validation: 3.5216586406874706]
	TIME [epoch: 9.75 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3297898355539655		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 2.3297898355539655 | validation: 3.4742543551608343]
	TIME [epoch: 9.75 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.346090450820031		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 2.346090450820031 | validation: 3.4703554278629576]
	TIME [epoch: 9.76 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3333359797174866		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 2.3333359797174866 | validation: 3.5451167796685796]
	TIME [epoch: 9.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3007199139489654		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 2.3007199139489654 | validation: 3.4518597140047937]
	TIME [epoch: 9.74 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3667830798657477		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 2.3667830798657477 | validation: 3.526771559452459]
	TIME [epoch: 9.74 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.311138993718898		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 2.311138993718898 | validation: 3.5726197924575525]
	TIME [epoch: 9.76 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3153560888570537		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 2.3153560888570537 | validation: 3.5353585550324556]
	TIME [epoch: 9.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.283420801102318		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 2.283420801102318 | validation: 3.504330548766027]
	TIME [epoch: 9.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3089447211581775		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 2.3089447211581775 | validation: 3.512949081446832]
	TIME [epoch: 9.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.269948464929151		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 2.269948464929151 | validation: 3.5077875494228006]
	TIME [epoch: 9.75 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3429934958148584		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 2.3429934958148584 | validation: 3.7255318680171277]
	TIME [epoch: 9.75 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.36903404602747		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 2.36903404602747 | validation: 3.4904604401818298]
	TIME [epoch: 9.74 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.307251288707232		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 2.307251288707232 | validation: 3.518736367345806]
	TIME [epoch: 9.76 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2891623133748435		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 2.2891623133748435 | validation: 3.566825086771705]
	TIME [epoch: 9.74 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2912974745579615		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 2.2912974745579615 | validation: 3.6315941798265476]
	TIME [epoch: 9.74 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3362036356380673		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 2.3362036356380673 | validation: 3.546894029341978]
	TIME [epoch: 9.76 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288177457828922		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 2.288177457828922 | validation: 3.465261420565189]
	TIME [epoch: 9.75 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2852209663177634		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 2.2852209663177634 | validation: 3.59072293252811]
	TIME [epoch: 9.75 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.320159027939334		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 2.320159027939334 | validation: 3.4589226952800094]
	TIME [epoch: 9.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3423835654802447		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 2.3423835654802447 | validation: 3.5885936121388955]
	TIME [epoch: 9.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.336115891294496		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 2.336115891294496 | validation: 3.444175035324379]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278885779306689		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 2.278885779306689 | validation: 3.457199099963595]
	TIME [epoch: 9.75 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2658336257013962		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 2.2658336257013962 | validation: 3.4971364762690382]
	TIME [epoch: 9.77 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.337829210189723		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 2.337829210189723 | validation: 3.540745660179605]
	TIME [epoch: 9.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.299094454915071		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 2.299094454915071 | validation: 3.679719418603105]
	TIME [epoch: 9.75 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3196849524364795		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 2.3196849524364795 | validation: 3.4716999082025035]
	TIME [epoch: 9.76 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3239028689574703		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 2.3239028689574703 | validation: 4.215791415569076]
	TIME [epoch: 9.75 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.593365026191777		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 2.593365026191777 | validation: 3.4484520690143405]
	TIME [epoch: 9.75 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.283966001451583		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 2.283966001451583 | validation: 3.443811770216131]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2917907649098725		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 2.2917907649098725 | validation: 3.51295475148233]
	TIME [epoch: 9.77 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.431645340899805		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 2.431645340899805 | validation: 3.437852601302995]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.307863888141724		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 2.307863888141724 | validation: 3.4834190393176505]
	TIME [epoch: 9.74 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3318535939719425		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 2.3318535939719425 | validation: 3.4876893198836023]
	TIME [epoch: 9.76 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.285413879844211		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 2.285413879844211 | validation: 3.5802500703712297]
	TIME [epoch: 9.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.316746800061194		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 2.316746800061194 | validation: 3.690448462276288]
	TIME [epoch: 9.74 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.36904711711935		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 2.36904711711935 | validation: 3.445119705249553]
	TIME [epoch: 9.76 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3083707356306404		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 2.3083707356306404 | validation: 3.4342077800395487]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25653721100389		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 2.25653721100389 | validation: 3.451792351530827]
	TIME [epoch: 9.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3316555403008747		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 2.3316555403008747 | validation: 3.44949294973669]
	TIME [epoch: 9.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.272797011498966		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 2.272797011498966 | validation: 3.488564624773697]
	TIME [epoch: 9.77 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.326942610455103		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 2.326942610455103 | validation: 3.518445433257306]
	TIME [epoch: 9.76 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.318096431933589		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 2.318096431933589 | validation: 3.4648506661666683]
	TIME [epoch: 9.75 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.291881643818719		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 2.291881643818719 | validation: 3.591711817919404]
	TIME [epoch: 9.76 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2719381760552846		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 2.2719381760552846 | validation: 3.4679500752476122]
	TIME [epoch: 9.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2945711127891		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 2.2945711127891 | validation: 3.444068424921298]
	TIME [epoch: 9.75 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2749242040074242		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 2.2749242040074242 | validation: 3.4591583247220683]
	TIME [epoch: 9.75 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.297060694802761		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 2.297060694802761 | validation: 3.522202000427381]
	TIME [epoch: 9.76 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.326497412442459		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 2.326497412442459 | validation: 3.461980972927262]
	TIME [epoch: 9.75 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.328466346427646		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 2.328466346427646 | validation: 3.561932181691515]
	TIME [epoch: 9.75 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2990321041707964		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 2.2990321041707964 | validation: 3.4633107162353385]
	TIME [epoch: 9.77 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3323093251534233		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 2.3323093251534233 | validation: 3.444134512594964]
	TIME [epoch: 9.74 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.28354610505978		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 2.28354610505978 | validation: 3.4381637704595267]
	TIME [epoch: 9.74 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3015698549270525		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 2.3015698549270525 | validation: 3.516877356532827]
	TIME [epoch: 9.76 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4165239587478036		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 2.4165239587478036 | validation: 3.4646956757211838]
	TIME [epoch: 9.75 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3206891695991283		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 2.3206891695991283 | validation: 3.540738915555244]
	TIME [epoch: 9.74 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.277072285709795		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 2.277072285709795 | validation: 3.499951707322612]
	TIME [epoch: 9.75 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273976734444095		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 2.273976734444095 | validation: 3.4399218922286674]
	TIME [epoch: 9.76 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3071242949827973		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 2.3071242949827973 | validation: 3.4573090514435116]
	TIME [epoch: 9.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2955716802939263		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 2.2955716802939263 | validation: 3.469407393037476]
	TIME [epoch: 9.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2556544624404875		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 2.2556544624404875 | validation: 3.478723833038356]
	TIME [epoch: 9.76 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.322096171812217		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 2.322096171812217 | validation: 3.5222112355964414]
	TIME [epoch: 9.75 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.331225769418996		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 2.331225769418996 | validation: 3.4595901873897503]
	TIME [epoch: 9.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2548969169572444		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 2.2548969169572444 | validation: 3.4986072619520763]
	TIME [epoch: 9.75 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3370954452068595		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 2.3370954452068595 | validation: 3.464304367188696]
	TIME [epoch: 9.76 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.299696633557779		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 2.299696633557779 | validation: 3.591024392853431]
	TIME [epoch: 9.75 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3565820721247595		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 2.3565820721247595 | validation: 3.4884482821760274]
	TIME [epoch: 9.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.303410624348543		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 2.303410624348543 | validation: 3.4413855526187955]
	TIME [epoch: 9.77 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.291603288335309		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 2.291603288335309 | validation: 3.449222123419019]
	TIME [epoch: 9.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2879810674576198		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 2.2879810674576198 | validation: 3.4689336301015814]
	TIME [epoch: 9.75 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261744182081313		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 2.261744182081313 | validation: 3.4271590134457472]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.321644123335293		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 2.321644123335293 | validation: 3.4202886367897163]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2559925911879772		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 2.2559925911879772 | validation: 3.5976486921393436]
	TIME [epoch: 9.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.328610809117094		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 2.328610809117094 | validation: 3.5088067555824556]
	TIME [epoch: 9.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3519977677388044		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 2.3519977677388044 | validation: 3.568107316387825]
	TIME [epoch: 9.76 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2934629349912092		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 2.2934629349912092 | validation: 3.488682325840691]
	TIME [epoch: 9.75 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2649427732258047		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 2.2649427732258047 | validation: 3.492957566827654]
	TIME [epoch: 9.74 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2504039562460205		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 2.2504039562460205 | validation: 3.479244542463956]
	TIME [epoch: 9.77 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260096060655316		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 2.260096060655316 | validation: 3.453306951853798]
	TIME [epoch: 9.74 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2862892662622145		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 2.2862892662622145 | validation: 3.5185795788774956]
	TIME [epoch: 9.74 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.329259596892977		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 2.329259596892977 | validation: 3.502349877569463]
	TIME [epoch: 9.77 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4006012291215946		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 2.4006012291215946 | validation: 3.637381078038303]
	TIME [epoch: 9.76 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.32145705101878		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 2.32145705101878 | validation: 3.5083300153806114]
	TIME [epoch: 9.75 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315193383347894		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 2.315193383347894 | validation: 3.495285742843803]
	TIME [epoch: 9.76 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2859034745298175		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 2.2859034745298175 | validation: 3.4356418776642648]
	TIME [epoch: 9.78 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2658143255566925		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 2.2658143255566925 | validation: 3.429495466333983]
	TIME [epoch: 9.75 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2606055820284596		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 2.2606055820284596 | validation: 3.4897733983401835]
	TIME [epoch: 9.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2802973635125308		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 2.2802973635125308 | validation: 3.4292595702800943]
	TIME [epoch: 9.77 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.289902201431044		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 2.289902201431044 | validation: 3.494619964193664]
	TIME [epoch: 9.76 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2862322042022276		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 2.2862322042022276 | validation: 3.558284238367487]
	TIME [epoch: 9.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.303925323216199		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 2.303925323216199 | validation: 3.4625452591688055]
	TIME [epoch: 9.75 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.302903036195869		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 2.302903036195869 | validation: 3.531744575755835]
	TIME [epoch: 9.76 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.321376038667002		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 2.321376038667002 | validation: 3.425431424543232]
	TIME [epoch: 9.75 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2856607119112593		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 2.2856607119112593 | validation: 3.4789206200112663]
	TIME [epoch: 9.75 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.285429784919753		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 2.285429784919753 | validation: 3.422494026180024]
	TIME [epoch: 9.77 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3062806842819255		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 2.3062806842819255 | validation: 3.473557889819886]
	TIME [epoch: 9.75 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.279767020722012		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 2.279767020722012 | validation: 3.4335026796799584]
	TIME [epoch: 9.75 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2991641552859234		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 2.2991641552859234 | validation: 3.492421748140423]
	TIME [epoch: 9.76 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2695635036097492		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 2.2695635036097492 | validation: 3.4457839302449687]
	TIME [epoch: 9.75 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4151023732239585		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 2.4151023732239585 | validation: 3.4533762373501307]
	TIME [epoch: 9.75 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.346567045757395		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 2.346567045757395 | validation: 3.472554673041634]
	TIME [epoch: 9.75 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2763513981133654		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 2.2763513981133654 | validation: 3.4502637759372723]
	TIME [epoch: 9.77 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2640607600111538		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 2.2640607600111538 | validation: 3.415748703216691]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288231119395948		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 2.288231119395948 | validation: 3.439766672534642]
	TIME [epoch: 9.75 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2681756864375315		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 2.2681756864375315 | validation: 3.444013579182506]
	TIME [epoch: 9.77 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.282925792969043		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 2.282925792969043 | validation: 3.4612434334899524]
	TIME [epoch: 9.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.30098271354669		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 2.30098271354669 | validation: 3.5309206489771463]
	TIME [epoch: 9.75 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.269908180449999		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 2.269908180449999 | validation: 3.431594910499097]
	TIME [epoch: 9.76 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2670050250576397		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 2.2670050250576397 | validation: 3.4887181456616894]
	TIME [epoch: 9.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.292643755417963		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 2.292643755417963 | validation: 3.4754350771545655]
	TIME [epoch: 9.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.293817701700627		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 2.293817701700627 | validation: 3.6597886935823376]
	TIME [epoch: 9.75 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2943195681207755		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 2.2943195681207755 | validation: 3.4574525137271253]
	TIME [epoch: 9.76 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.295704075565671		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 2.295704075565671 | validation: 3.460037240894786]
	TIME [epoch: 9.75 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255443021360543		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 2.255443021360543 | validation: 3.5873102966485364]
	TIME [epoch: 9.75 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3056375194763885		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 2.3056375194763885 | validation: 3.518477796847274]
	TIME [epoch: 9.76 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251258680417363		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 2.251258680417363 | validation: 3.519599002806343]
	TIME [epoch: 9.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259748763959679		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 2.259748763959679 | validation: 3.451262836921952]
	TIME [epoch: 9.74 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.330724365305083		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 2.330724365305083 | validation: 3.4390095927297795]
	TIME [epoch: 9.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2467624347727417		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 2.2467624347727417 | validation: 3.5210840661346414]
	TIME [epoch: 9.75 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2963232989113878		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 2.2963232989113878 | validation: 3.4271281205010657]
	TIME [epoch: 9.75 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2501538974330764		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 2.2501538974330764 | validation: 3.456152337983303]
	TIME [epoch: 9.75 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2460377474940927		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 2.2460377474940927 | validation: 3.4686579361075496]
	TIME [epoch: 9.77 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2901993281184234		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 2.2901993281184234 | validation: 3.4364737512705394]
	TIME [epoch: 9.75 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3108545323327507		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 2.3108545323327507 | validation: 3.5665626459380615]
	TIME [epoch: 9.74 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252766471611671		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 2.252766471611671 | validation: 3.4520357010529112]
	TIME [epoch: 9.75 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.511509489634225		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 2.511509489634225 | validation: 3.4888876614014226]
	TIME [epoch: 9.75 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266472456636754		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 2.266472456636754 | validation: 3.4385195860040962]
	TIME [epoch: 9.74 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253373205398193		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 2.253373205398193 | validation: 3.446357085466202]
	TIME [epoch: 9.75 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2977497123015267		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 2.2977497123015267 | validation: 3.4465899847872574]
	TIME [epoch: 9.76 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.252591710194795		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 2.252591710194795 | validation: 3.4926118155871615]
	TIME [epoch: 9.74 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2926576010702164		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 2.2926576010702164 | validation: 3.4739420850888703]
	TIME [epoch: 9.74 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433439580490337		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 2.2433439580490337 | validation: 3.4359659070787623]
	TIME [epoch: 9.76 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2618616546061308		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 2.2618616546061308 | validation: 3.438924454183989]
	TIME [epoch: 9.75 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2732363254424834		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 2.2732363254424834 | validation: 3.4258602913414795]
	TIME [epoch: 9.75 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2416723154969356		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 2.2416723154969356 | validation: 3.503380376176151]
	TIME [epoch: 9.76 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266055046133132		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 2.266055046133132 | validation: 3.4302032017145034]
	TIME [epoch: 9.75 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.369353366234697		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 2.369353366234697 | validation: 3.668955284456726]
	TIME [epoch: 9.75 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.298260082620587		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 2.298260082620587 | validation: 3.429967991014421]
	TIME [epoch: 9.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2674990767083303		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 2.2674990767083303 | validation: 3.4257191775181104]
	TIME [epoch: 9.77 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2469548344978945		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 2.2469548344978945 | validation: 3.484720231517108]
	TIME [epoch: 9.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253979854866171		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 2.253979854866171 | validation: 3.438385345968781]
	TIME [epoch: 9.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249897422743318		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 2.249897422743318 | validation: 3.4405225501176933]
	TIME [epoch: 9.77 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2463978767467827		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 2.2463978767467827 | validation: 3.4336380304675993]
	TIME [epoch: 9.75 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.257399297081621		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 2.257399297081621 | validation: 3.426274475162626]
	TIME [epoch: 9.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2598102563928504		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 2.2598102563928504 | validation: 3.5735455838162733]
	TIME [epoch: 9.75 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.297762259693739		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 2.297762259693739 | validation: 3.4361359322732223]
	TIME [epoch: 9.77 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240486476823554		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 2.240486476823554 | validation: 3.440222819430486]
	TIME [epoch: 9.75 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.348407011583968		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 2.348407011583968 | validation: 3.4187634206114836]
	TIME [epoch: 9.75 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273987377840077		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 2.273987377840077 | validation: 3.4784862196974426]
	TIME [epoch: 9.77 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2396816105782515		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 2.2396816105782515 | validation: 3.409618655071389]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_729.pth
	Model improved!!!
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2443006699837573		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 2.2443006699837573 | validation: 3.47994246159382]
	TIME [epoch: 9.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2458265797850916		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 2.2458265797850916 | validation: 3.4267233613939267]
	TIME [epoch: 9.77 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2490113038475767		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 2.2490113038475767 | validation: 3.4054969967982056]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2507906103402813		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 2.2507906103402813 | validation: 3.42075048896892]
	TIME [epoch: 9.75 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.295461023553753		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 2.295461023553753 | validation: 3.5967856824821514]
	TIME [epoch: 9.75 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.262529694023022		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 2.262529694023022 | validation: 3.4385232913562045]
	TIME [epoch: 9.76 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245005469163691		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 2.245005469163691 | validation: 3.430916137906073]
	TIME [epoch: 9.75 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273333470951003		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 2.273333470951003 | validation: 3.4232030281651395]
	TIME [epoch: 9.75 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2606157481312734		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 2.2606157481312734 | validation: 3.4781072898499703]
	TIME [epoch: 9.77 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2643972051075734		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 2.2643972051075734 | validation: 3.4667543943709846]
	TIME [epoch: 9.76 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2791595505808067		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 2.2791595505808067 | validation: 3.4564720924687196]
	TIME [epoch: 9.76 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.235647091406882		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 2.235647091406882 | validation: 3.466910888150573]
	TIME [epoch: 9.77 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.28116482254363		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 2.28116482254363 | validation: 3.5033354309807976]
	TIME [epoch: 9.77 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2909086778463843		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 2.2909086778463843 | validation: 3.4155581920749025]
	TIME [epoch: 9.76 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.263663583621003		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 2.263663583621003 | validation: 3.4658015526952015]
	TIME [epoch: 9.76 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.275473248594876		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 2.275473248594876 | validation: 3.414947126059756]
	TIME [epoch: 9.78 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254119689882069		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 2.254119689882069 | validation: 3.4377557026823315]
	TIME [epoch: 9.75 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3161363167346574		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 2.3161363167346574 | validation: 3.588792539794482]
	TIME [epoch: 9.76 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2774468242145742		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 2.2774468242145742 | validation: 3.4403987954229267]
	TIME [epoch: 9.77 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2435971307101816		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 2.2435971307101816 | validation: 3.5043602250976904]
	TIME [epoch: 9.76 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.312704539637102		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 2.312704539637102 | validation: 3.4091152792446464]
	TIME [epoch: 9.76 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2574608406023957		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 2.2574608406023957 | validation: 3.416778171182698]
	TIME [epoch: 9.75 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.26034073357717		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 2.26034073357717 | validation: 3.4052405740322875]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2792708836344873		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 2.2792708836344873 | validation: 3.46785813401842]
	TIME [epoch: 9.75 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2421390744690366		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 2.2421390744690366 | validation: 3.4182315861151165]
	TIME [epoch: 9.74 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23384316686516		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 2.23384316686516 | validation: 3.4093743106260086]
	TIME [epoch: 9.76 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2707706605241964		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 2.2707706605241964 | validation: 3.4510171950972723]
	TIME [epoch: 9.75 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259192699272602		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 2.259192699272602 | validation: 3.4074780760126986]
	TIME [epoch: 9.74 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479879433578493		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 2.2479879433578493 | validation: 3.4123930786450183]
	TIME [epoch: 9.75 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2479375251567726		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 2.2479375251567726 | validation: 3.4488894756474484]
	TIME [epoch: 9.75 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2823488722151386		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 2.2823488722151386 | validation: 3.4222801481296123]
	TIME [epoch: 9.74 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24566882610834		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 2.24566882610834 | validation: 3.407480876612359]
	TIME [epoch: 9.74 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2655976590407696		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 2.2655976590407696 | validation: 3.417321115893908]
	TIME [epoch: 9.76 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.231576885029848		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 2.231576885029848 | validation: 3.4339554158182715]
	TIME [epoch: 9.74 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.265103926736086		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 2.265103926736086 | validation: 3.414767863215605]
	TIME [epoch: 9.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2502265126181094		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 2.2502265126181094 | validation: 3.4229605041202285]
	TIME [epoch: 9.76 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254993376362598		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 2.254993376362598 | validation: 3.455858366492247]
	TIME [epoch: 9.74 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2566184138373844		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 2.2566184138373844 | validation: 3.430406723176763]
	TIME [epoch: 9.74 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.249993836619637		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 2.249993836619637 | validation: 3.43795951013424]
	TIME [epoch: 9.74 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3033154764876977		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 2.3033154764876977 | validation: 3.4760315505515345]
	TIME [epoch: 9.76 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2998041568678707		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 2.2998041568678707 | validation: 3.407946811014209]
	TIME [epoch: 9.74 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.232540898001589		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 2.232540898001589 | validation: 3.427400263607378]
	TIME [epoch: 9.74 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2652853622969964		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 2.2652853622969964 | validation: 3.6239654873419482]
	TIME [epoch: 9.75 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.298495464303586		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 2.298495464303586 | validation: 3.428094733580205]
	TIME [epoch: 9.75 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2551430280211973		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 2.2551430280211973 | validation: 3.460248786968876]
	TIME [epoch: 9.75 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2573688611319893		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 2.2573688611319893 | validation: 3.4303173980427983]
	TIME [epoch: 9.75 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248203760736363		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 2.248203760736363 | validation: 3.564680200070054]
	TIME [epoch: 9.75 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.266537094213278		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 2.266537094213278 | validation: 3.4223477529426476]
	TIME [epoch: 9.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240963652772558		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 2.240963652772558 | validation: 3.399895130663797]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2250356379308487		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 2.2250356379308487 | validation: 3.4894365956439253]
	TIME [epoch: 9.77 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2433352416586847		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 2.2433352416586847 | validation: 3.465447974550942]
	TIME [epoch: 9.75 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2804026980245027		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 2.2804026980245027 | validation: 3.413021418296229]
	TIME [epoch: 9.75 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2373499419517993		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 2.2373499419517993 | validation: 3.4586741435644717]
	TIME [epoch: 9.76 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478590821968543		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 2.2478590821968543 | validation: 3.474164143034773]
	TIME [epoch: 9.75 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3266210392535562		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 2.3266210392535562 | validation: 3.414579885833719]
	TIME [epoch: 9.74 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2290115064841327		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 2.2290115064841327 | validation: 3.40986626184103]
	TIME [epoch: 9.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.225314592282378		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 2.225314592282378 | validation: 3.4454033702314315]
	TIME [epoch: 9.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2339620735319925		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 2.2339620735319925 | validation: 3.425505514346171]
	TIME [epoch: 9.75 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.24157101425747		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 2.24157101425747 | validation: 3.4304679814467476]
	TIME [epoch: 9.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251650645851252		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 2.251650645851252 | validation: 3.4581889280337816]
	TIME [epoch: 9.77 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2522320474971265		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 2.2522320474971265 | validation: 3.4521328321314217]
	TIME [epoch: 9.75 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2741184263226084		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 2.2741184263226084 | validation: 3.408146025512364]
	TIME [epoch: 9.75 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.227379809204613		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 2.227379809204613 | validation: 3.408892740912843]
	TIME [epoch: 9.76 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2664082635955105		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 2.2664082635955105 | validation: 3.4053993656773316]
	TIME [epoch: 9.75 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.254608562719538		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 2.254608562719538 | validation: 3.568975235144003]
	TIME [epoch: 9.75 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2924156548621832		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 2.2924156548621832 | validation: 3.407024093107973]
	TIME [epoch: 9.75 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2297132460043905		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 2.2297132460043905 | validation: 3.519808903240612]
	TIME [epoch: 9.76 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3315684761058013		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 2.3315684761058013 | validation: 3.4245677036702875]
	TIME [epoch: 9.74 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.260391608041653		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 2.260391608041653 | validation: 3.3917593486270663]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_798.pth
	Model improved!!!
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234698152344104		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 2.234698152344104 | validation: 3.4772632628685383]
	TIME [epoch: 9.76 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259021466314409		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 2.259021466314409 | validation: 3.437738741647935]
	TIME [epoch: 9.75 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240789943231904		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 2.240789943231904 | validation: 3.4718301689563185]
	TIME [epoch: 9.75 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.332573669919411		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 2.332573669919411 | validation: 3.4030224166026892]
	TIME [epoch: 9.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2389185377777467		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 2.2389185377777467 | validation: 3.418758585420383]
	TIME [epoch: 9.75 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2460949928256975		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 2.2460949928256975 | validation: 3.4283006982925777]
	TIME [epoch: 9.75 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2490648124602606		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 2.2490648124602606 | validation: 3.449141106693562]
	TIME [epoch: 9.74 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2315873420842234		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 2.2315873420842234 | validation: 3.3919127824232023]
	TIME [epoch: 9.76 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2271510048912533		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 2.2271510048912533 | validation: 3.3729073152129594]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_807.pth
	Model improved!!!
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2318219013142384		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 2.2318219013142384 | validation: 3.4739381334746122]
	TIME [epoch: 9.76 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2467809113384165		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 2.2467809113384165 | validation: 3.418641478681592]
	TIME [epoch: 9.77 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.233226908970157		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 2.233226908970157 | validation: 3.44684058091624]
	TIME [epoch: 9.76 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2308661333392923		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 2.2308661333392923 | validation: 3.414570083565522]
	TIME [epoch: 9.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2294021923919		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 2.2294021923919 | validation: 3.5413128976902897]
	TIME [epoch: 9.75 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25482978793364		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 2.25482978793364 | validation: 3.4081731649314304]
	TIME [epoch: 9.76 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.239355829107952		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 2.239355829107952 | validation: 3.469260703656388]
	TIME [epoch: 9.76 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.244022494851656		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 2.244022494851656 | validation: 3.4349919119830004]
	TIME [epoch: 9.75 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3135543354133947		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 2.3135543354133947 | validation: 3.452306956412105]
	TIME [epoch: 9.78 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.290815942977091		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 2.290815942977091 | validation: 3.421922796064696]
	TIME [epoch: 9.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.225820946793811		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 2.225820946793811 | validation: 3.4168750373209567]
	TIME [epoch: 9.75 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2564855203895795		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 2.2564855203895795 | validation: 3.4479902448962805]
	TIME [epoch: 9.77 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2676690265594766		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 2.2676690265594766 | validation: 3.5090028738725376]
	TIME [epoch: 9.74 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.258963822174837		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 2.258963822174837 | validation: 3.4095955081946534]
	TIME [epoch: 9.74 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2363554886696555		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 2.2363554886696555 | validation: 3.3985265349695806]
	TIME [epoch: 9.74 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2239796631666855		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 2.2239796631666855 | validation: 3.398274638886621]
	TIME [epoch: 9.76 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2256744228130323		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 2.2256744228130323 | validation: 3.464045806486401]
	TIME [epoch: 9.75 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2448768912357933		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 2.2448768912357933 | validation: 3.4115086305949185]
	TIME [epoch: 9.75 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2618301513701495		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 2.2618301513701495 | validation: 3.494837094042548]
	TIME [epoch: 9.77 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2456142883571895		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 2.2456142883571895 | validation: 3.4077689233953574]
	TIME [epoch: 9.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2535980816935375		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 2.2535980816935375 | validation: 3.412143027143796]
	TIME [epoch: 9.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2148960508286812		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 2.2148960508286812 | validation: 3.4706346917654685]
	TIME [epoch: 9.77 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.261915663770295		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 2.261915663770295 | validation: 3.4042707188081547]
	TIME [epoch: 9.76 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.273539037701689		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 2.273539037701689 | validation: 3.4830912641664087]
	TIME [epoch: 9.75 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2371701921380054		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 2.2371701921380054 | validation: 3.468473581619114]
	TIME [epoch: 9.74 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234172383309251		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 2.234172383309251 | validation: 3.4514263614857112]
	TIME [epoch: 9.77 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2365912685825733		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 2.2365912685825733 | validation: 3.437502338175158]
	TIME [epoch: 9.75 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2233611839797387		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 2.2233611839797387 | validation: 3.439142685987722]
	TIME [epoch: 9.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.270880100853034		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 2.270880100853034 | validation: 3.4373700730479833]
	TIME [epoch: 9.77 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2246072451431442		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 2.2246072451431442 | validation: 3.4398013364263647]
	TIME [epoch: 9.76 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2478233679559136		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 2.2478233679559136 | validation: 3.411875112059064]
	TIME [epoch: 9.73 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245378967044682		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 2.245378967044682 | validation: 3.5752533911478475]
	TIME [epoch: 9.75 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3266233620493564		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 2.3266233620493564 | validation: 3.4289253688938]
	TIME [epoch: 9.78 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.239720176345753		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 2.239720176345753 | validation: 3.4023518071874]
	TIME [epoch: 9.76 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2202541975068097		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 2.2202541975068097 | validation: 3.408835775658795]
	TIME [epoch: 9.75 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2319842285050204		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 2.2319842285050204 | validation: 3.394216692575641]
	TIME [epoch: 9.77 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.225956606235431		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 2.225956606235431 | validation: 3.410617865935797]
	TIME [epoch: 9.75 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2215571008370225		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 2.2215571008370225 | validation: 3.4402167998532676]
	TIME [epoch: 9.75 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2347288565195322		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 2.2347288565195322 | validation: 3.4003903755481044]
	TIME [epoch: 9.76 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.229843151287946		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 2.229843151287946 | validation: 3.4523144698346333]
	TIME [epoch: 9.76 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.22141481010397		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 2.22141481010397 | validation: 3.4137993202887693]
	TIME [epoch: 9.74 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211344106618939		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 2.211344106618939 | validation: 3.4151094760376286]
	TIME [epoch: 9.74 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251357129522956		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 2.251357129522956 | validation: 3.4021469154267057]
	TIME [epoch: 9.76 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248262311921345		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 2.248262311921345 | validation: 3.403261289045235]
	TIME [epoch: 9.75 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2586948562877462		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 2.2586948562877462 | validation: 3.3927936986890734]
	TIME [epoch: 9.75 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2398985804500287		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 2.2398985804500287 | validation: 3.419720614282541]
	TIME [epoch: 9.77 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221888555414876		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 2.221888555414876 | validation: 3.4029459086820246]
	TIME [epoch: 9.75 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2321780412231043		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 2.2321780412231043 | validation: 3.421125729678979]
	TIME [epoch: 9.75 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221725691496612		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 2.221725691496612 | validation: 3.3887597953592]
	TIME [epoch: 9.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.219573570305958		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 2.219573570305958 | validation: 3.4588111569590025]
	TIME [epoch: 9.76 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251201913954619		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 2.251201913954619 | validation: 3.388724504112271]
	TIME [epoch: 9.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2130196839497653		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 2.2130196839497653 | validation: 3.5167291145204924]
	TIME [epoch: 9.75 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.253044057437647		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 2.253044057437647 | validation: 3.4350360594453653]
	TIME [epoch: 9.75 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2606019462636673		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 2.2606019462636673 | validation: 3.4186903177249977]
	TIME [epoch: 9.74 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.238635425175132		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 2.238635425175132 | validation: 3.460394398430761]
	TIME [epoch: 9.74 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.245717299777383		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 2.245717299777383 | validation: 3.4229324399632435]
	TIME [epoch: 9.74 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221196439836447		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 2.221196439836447 | validation: 3.422535865830984]
	TIME [epoch: 9.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2549566653511603		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 2.2549566653511603 | validation: 3.4540733708936013]
	TIME [epoch: 9.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2790611129474145		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 2.2790611129474145 | validation: 3.4162933041143013]
	TIME [epoch: 9.75 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2155873760830387		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 2.2155873760830387 | validation: 3.3868495381530024]
	TIME [epoch: 9.76 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.287860956163831		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 2.287860956163831 | validation: 3.4130666507505967]
	TIME [epoch: 9.75 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.228752346288062		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 2.228752346288062 | validation: 3.421979391460671]
	TIME [epoch: 9.75 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2357248452885017		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 2.2357248452885017 | validation: 3.3978990108113876]
	TIME [epoch: 9.76 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2168047275982055		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 2.2168047275982055 | validation: 3.4023503969831763]
	TIME [epoch: 9.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214453091761226		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 2.214453091761226 | validation: 3.4321766883604465]
	TIME [epoch: 9.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3042709015816154		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 2.3042709015816154 | validation: 3.3928974916452965]
	TIME [epoch: 9.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2135805841381813		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 2.2135805841381813 | validation: 3.4599681104086004]
	TIME [epoch: 9.76 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.288072326681748		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 2.288072326681748 | validation: 3.4170064557259763]
	TIME [epoch: 9.74 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.231717588557862		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 2.231717588557862 | validation: 3.437963722784402]
	TIME [epoch: 9.74 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23239247692334		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 2.23239247692334 | validation: 3.414493066651268]
	TIME [epoch: 9.76 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.218711521566246		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 2.218711521566246 | validation: 3.4115351395500295]
	TIME [epoch: 9.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2214616417222484		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 2.2214616417222484 | validation: 3.4450001796607284]
	TIME [epoch: 9.75 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.219510339358199		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 2.219510339358199 | validation: 3.490626171537183]
	TIME [epoch: 9.76 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2247466661883974		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 2.2247466661883974 | validation: 3.4136307794616094]
	TIME [epoch: 9.76 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2197187942711576		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 2.2197187942711576 | validation: 3.408638453368657]
	TIME [epoch: 9.74 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.229146038097994		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 2.229146038097994 | validation: 3.415846916851667]
	TIME [epoch: 9.75 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208514299283501		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 2.208514299283501 | validation: 3.4511939832587166]
	TIME [epoch: 9.75 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2303933569789303		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 2.2303933569789303 | validation: 3.4089121464674474]
	TIME [epoch: 9.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.259274399785666		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 2.259274399785666 | validation: 3.4048790044415354]
	TIME [epoch: 9.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2405461124346275		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 2.2405461124346275 | validation: 3.415907719490921]
	TIME [epoch: 9.76 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2336927572872805		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 2.2336927572872805 | validation: 3.448756837012567]
	TIME [epoch: 9.74 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2228837179658134		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 2.2228837179658134 | validation: 3.408202080968085]
	TIME [epoch: 9.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.207642298804895		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 2.207642298804895 | validation: 3.411047211981657]
	TIME [epoch: 9.74 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.229645431908253		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 2.229645431908253 | validation: 3.4014937727550785]
	TIME [epoch: 9.76 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2630517121462725		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 2.2630517121462725 | validation: 3.4026257682050742]
	TIME [epoch: 9.74 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.240945925858725		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 2.240945925858725 | validation: 3.413121841233878]
	TIME [epoch: 9.74 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.21014663187465		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 2.21014663187465 | validation: 3.407507212345941]
	TIME [epoch: 9.77 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2203373788877876		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 2.2203373788877876 | validation: 3.4146527959827866]
	TIME [epoch: 9.74 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.220259599636098		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 2.220259599636098 | validation: 3.4152677042784854]
	TIME [epoch: 9.75 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2263928150295085		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 2.2263928150295085 | validation: 3.436404937800997]
	TIME [epoch: 9.77 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2309502439593127		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 2.2309502439593127 | validation: 3.3765570867464385]
	TIME [epoch: 9.75 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.231166489797478		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 2.231166489797478 | validation: 3.3963516762004]
	TIME [epoch: 9.75 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2481798099814387		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 2.2481798099814387 | validation: 3.399604050009853]
	TIME [epoch: 9.75 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.223732181386853		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 2.223732181386853 | validation: 3.423296898423048]
	TIME [epoch: 9.77 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2270638551029416		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 2.2270638551029416 | validation: 3.412712529523058]
	TIME [epoch: 9.76 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2428198807412167		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 2.2428198807412167 | validation: 3.3946517858603373]
	TIME [epoch: 9.75 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.241047638847948		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 2.241047638847948 | validation: 3.4122994821136308]
	TIME [epoch: 9.76 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.220126022893125		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 2.220126022893125 | validation: 3.4500404008122123]
	TIME [epoch: 9.74 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2315663106035473		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 2.2315663106035473 | validation: 3.4098021675192918]
	TIME [epoch: 9.74 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2434934127440718		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 2.2434934127440718 | validation: 3.3995714307193228]
	TIME [epoch: 9.75 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2253086991714333		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 2.2253086991714333 | validation: 3.3796736690912472]
	TIME [epoch: 9.75 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.222220937554373		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 2.222220937554373 | validation: 3.42378184513591]
	TIME [epoch: 9.74 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2205535500046603		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 2.2205535500046603 | validation: 3.46990777512025]
	TIME [epoch: 9.74 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2315186034085976		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 2.2315186034085976 | validation: 3.434017677385571]
	TIME [epoch: 9.76 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2397366086860857		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 2.2397366086860857 | validation: 3.436384887324286]
	TIME [epoch: 9.74 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2364289963760555		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 2.2364289963760555 | validation: 3.4442388463436275]
	TIME [epoch: 9.75 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2331804632716734		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 2.2331804632716734 | validation: 3.410332105456354]
	TIME [epoch: 9.76 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.228517616040409		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 2.228517616040409 | validation: 3.3953520272493907]
	TIME [epoch: 9.74 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2165218731565615		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 2.2165218731565615 | validation: 3.4323849690549046]
	TIME [epoch: 9.73 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2131189470028274		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 2.2131189470028274 | validation: 3.451193493845284]
	TIME [epoch: 9.75 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23281409890751		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 2.23281409890751 | validation: 3.4016765122510852]
	TIME [epoch: 9.75 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.216225856879169		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 2.216225856879169 | validation: 3.399278333953438]
	TIME [epoch: 9.74 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.209933689457151		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 2.209933689457151 | validation: 3.3943633322597826]
	TIME [epoch: 9.74 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2119174113295292		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 2.2119174113295292 | validation: 3.445809304163493]
	TIME [epoch: 9.75 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.222163281852141		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 2.222163281852141 | validation: 3.3965745146671726]
	TIME [epoch: 9.75 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2321801800647494		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 2.2321801800647494 | validation: 3.394575714330058]
	TIME [epoch: 9.75 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2178442327364456		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 2.2178442327364456 | validation: 3.405753767088815]
	TIME [epoch: 9.76 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.251455287783247		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 2.251455287783247 | validation: 3.400417783247424]
	TIME [epoch: 9.76 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2107192977008583		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 2.2107192977008583 | validation: 3.3958142541025076]
	TIME [epoch: 9.75 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.215065580523699		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 2.215065580523699 | validation: 3.401759227339732]
	TIME [epoch: 9.75 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211599131578424		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 2.211599131578424 | validation: 3.4050047168857804]
	TIME [epoch: 9.78 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2162043660268216		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 2.2162043660268216 | validation: 3.4230600947978793]
	TIME [epoch: 9.74 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.237721328475412		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 2.237721328475412 | validation: 3.38477092841742]
	TIME [epoch: 9.75 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2102692097452987		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 2.2102692097452987 | validation: 3.3910620889658807]
	TIME [epoch: 9.76 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.206769280140345		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 2.206769280140345 | validation: 3.4248714307673573]
	TIME [epoch: 9.75 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242903570966599		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 2.242903570966599 | validation: 3.441593699330306]
	TIME [epoch: 9.75 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242980455439749		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 2.242980455439749 | validation: 3.399134554212153]
	TIME [epoch: 9.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2223413538095302		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 2.2223413538095302 | validation: 3.4072446650876533]
	TIME [epoch: 9.75 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2075267898822006		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 2.2075267898822006 | validation: 3.4055716861269474]
	TIME [epoch: 9.76 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.213791483027344		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 2.213791483027344 | validation: 3.440781242631351]
	TIME [epoch: 9.74 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.218349518249877		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 2.218349518249877 | validation: 3.4479445735950187]
	TIME [epoch: 9.76 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2202862691294234		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 2.2202862691294234 | validation: 3.3882914162370814]
	TIME [epoch: 9.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2389507243533564		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 2.2389507243533564 | validation: 3.406073674102604]
	TIME [epoch: 9.74 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2216805908252724		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 2.2216805908252724 | validation: 3.4135581386764104]
	TIME [epoch: 9.76 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2088026024217866		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 2.2088026024217866 | validation: 3.4296362077071927]
	TIME [epoch: 9.76 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2261216605703433		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 2.2261216605703433 | validation: 3.391924473916099]
	TIME [epoch: 9.74 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2198234566663286		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 2.2198234566663286 | validation: 3.4239833833904605]
	TIME [epoch: 9.74 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.217504651696813		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 2.217504651696813 | validation: 3.3721598088769555]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2094425839537797		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 2.2094425839537797 | validation: 3.395632273678189]
	TIME [epoch: 9.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2396028811677673		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 2.2396028811677673 | validation: 3.4060184947118373]
	TIME [epoch: 9.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201456718153658		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 2.201456718153658 | validation: 3.412794802480613]
	TIME [epoch: 9.74 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.228157121925327		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 2.228157121925327 | validation: 3.484969624390802]
	TIME [epoch: 9.75 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.238967395236318		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 2.238967395236318 | validation: 3.383277720828419]
	TIME [epoch: 9.72 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.203729476847072		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 2.203729476847072 | validation: 3.402589079322144]
	TIME [epoch: 9.73 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.225965479955544		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 2.225965479955544 | validation: 3.4062986533922293]
	TIME [epoch: 9.74 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2175475791814003		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 2.2175475791814003 | validation: 3.3865005511650454]
	TIME [epoch: 9.72 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.219258478897195		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 2.219258478897195 | validation: 3.416186190084425]
	TIME [epoch: 9.73 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2188576111896743		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 2.2188576111896743 | validation: 3.4065116630784154]
	TIME [epoch: 9.75 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2213675288602883		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 2.2213675288602883 | validation: 3.4052458382701234]
	TIME [epoch: 9.74 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2176540441983903		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 2.2176540441983903 | validation: 3.366946831209849]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.216883779934314		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 2.216883779934314 | validation: 3.3830116613301606]
	TIME [epoch: 9.76 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2294374621678954		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 2.2294374621678954 | validation: 3.408646146688667]
	TIME [epoch: 9.76 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2277191802312517		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 2.2277191802312517 | validation: 3.3982089861542004]
	TIME [epoch: 9.76 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.234223121783586		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 2.234223121783586 | validation: 3.4055753669165267]
	TIME [epoch: 9.76 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2046891369638475		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 2.2046891369638475 | validation: 3.442033261061398]
	TIME [epoch: 9.77 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2285411636468515		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 2.2285411636468515 | validation: 3.3757715007024314]
	TIME [epoch: 9.76 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2136108338377314		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 2.2136108338377314 | validation: 3.363196823673285]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210975610784426		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 2.210975610784426 | validation: 3.374028808477444]
	TIME [epoch: 9.77 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1961383735486293		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 2.1961383735486293 | validation: 3.370297979097777]
	TIME [epoch: 9.76 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2134686355032835		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 2.2134686355032835 | validation: 3.3541329430258773]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2130945855936752		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 2.2130945855936752 | validation: 3.3751883907524274]
	TIME [epoch: 9.78 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2107924001250385		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 2.2107924001250385 | validation: 3.396902320408855]
	TIME [epoch: 9.77 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2111949434781697		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 2.2111949434781697 | validation: 3.4010379937892985]
	TIME [epoch: 9.75 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2017577862398636		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 2.2017577862398636 | validation: 3.355755458027805]
	TIME [epoch: 9.75 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2157161341025438		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 2.2157161341025438 | validation: 3.4123808745476785]
	TIME [epoch: 9.77 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2309711409853357		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 2.2309711409853357 | validation: 3.477922747490083]
	TIME [epoch: 9.75 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2288992634402396		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 2.2288992634402396 | validation: 3.3803381904915057]
	TIME [epoch: 9.75 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200893984444037		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 2.200893984444037 | validation: 3.4237695902629133]
	TIME [epoch: 9.77 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210157087178208		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 2.210157087178208 | validation: 3.3670392665728115]
	TIME [epoch: 9.75 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2149145966544497		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 2.2149145966544497 | validation: 3.416145660014292]
	TIME [epoch: 9.75 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2383295632620985		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 2.2383295632620985 | validation: 3.399774348460996]
	TIME [epoch: 9.76 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2337322462171163		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 2.2337322462171163 | validation: 3.3631006015058076]
	TIME [epoch: 9.76 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.204989876718736		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 2.204989876718736 | validation: 3.4155920906016224]
	TIME [epoch: 9.75 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.222508551436665		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 2.222508551436665 | validation: 3.4199070630497905]
	TIME [epoch: 9.75 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211664284738524		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 2.211664284738524 | validation: 3.3597817854584338]
	TIME [epoch: 9.76 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2269389371274437		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 2.2269389371274437 | validation: 3.3567365444786765]
	TIME [epoch: 9.75 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2140704818453933		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 2.2140704818453933 | validation: 3.399810512773736]
	TIME [epoch: 9.74 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2287060491589665		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 2.2287060491589665 | validation: 3.363566706676518]
	TIME [epoch: 9.76 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2071646660072077		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 2.2071646660072077 | validation: 3.3953573205684835]
	TIME [epoch: 9.75 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2032336037785525		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 2.2032336037785525 | validation: 3.3947227559378934]
	TIME [epoch: 9.75 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211807700333141		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 2.211807700333141 | validation: 3.4349327216543886]
	TIME [epoch: 9.75 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.248472177419341		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 2.248472177419341 | validation: 3.3670053157907684]
	TIME [epoch: 9.77 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2142527007638995		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 2.2142527007638995 | validation: 3.3807240064048525]
	TIME [epoch: 9.75 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2168887356132836		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 2.2168887356132836 | validation: 3.378851052229767]
	TIME [epoch: 9.74 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2279432283329648		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 2.2279432283329648 | validation: 3.4107503698690587]
	TIME [epoch: 9.76 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2325533039542416		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 2.2325533039542416 | validation: 3.3646518227372613]
	TIME [epoch: 9.75 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2243276438814155		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 2.2243276438814155 | validation: 3.3643280991223157]
	TIME [epoch: 9.75 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20251171809461		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 2.20251171809461 | validation: 3.3464267302908937]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2125283291512234		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 2.2125283291512234 | validation: 3.3682507555674928]
	TIME [epoch: 9.75 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2104554076003082		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 2.2104554076003082 | validation: 3.4092602054575014]
	TIME [epoch: 9.74 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2069052815484036		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 2.2069052815484036 | validation: 3.37113788891802]
	TIME [epoch: 9.73 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208413766530415		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 2.208413766530415 | validation: 3.357100596643665]
	TIME [epoch: 9.76 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221952955283563		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 2.221952955283563 | validation: 3.372976279442247]
	TIME [epoch: 9.74 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.255414935658599		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 2.255414935658599 | validation: 3.4355831294198755]
	TIME [epoch: 9.74 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2255305235454097		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 2.2255305235454097 | validation: 3.373592507737853]
	TIME [epoch: 9.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2082913245791223		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 2.2082913245791223 | validation: 3.375734040877036]
	TIME [epoch: 9.75 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2271029224052294		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 2.2271029224052294 | validation: 3.356602639085212]
	TIME [epoch: 9.74 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221125845646089		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 2.221125845646089 | validation: 3.3514598092479555]
	TIME [epoch: 9.74 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2163916428992487		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 2.2163916428992487 | validation: 3.362394018526921]
	TIME [epoch: 9.75 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2261581928057		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 2.2261581928057 | validation: 3.3543474874372863]
	TIME [epoch: 9.74 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1987295967300566		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 2.1987295967300566 | validation: 3.37434749288533]
	TIME [epoch: 9.73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.215178401589698		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 2.215178401589698 | validation: 3.38077611988993]
	TIME [epoch: 9.75 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2337819244836203		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 2.2337819244836203 | validation: 3.4727474490717656]
	TIME [epoch: 9.74 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3232344739369126		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 2.3232344739369126 | validation: 3.3694691592194896]
	TIME [epoch: 9.74 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2053488405683632		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 2.2053488405683632 | validation: 3.347295958316859]
	TIME [epoch: 9.74 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200414271437709		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 2.200414271437709 | validation: 3.3788727223457817]
	TIME [epoch: 9.75 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208014920182632		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 2.208014920182632 | validation: 3.3592359907826936]
	TIME [epoch: 9.73 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2201886962085537		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 2.2201886962085537 | validation: 3.3677112572547108]
	TIME [epoch: 9.74 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.220368205001675		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 2.220368205001675 | validation: 3.407469354889461]
	TIME [epoch: 9.75 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.223374637065668		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 2.223374637065668 | validation: 3.349498254852423]
	TIME [epoch: 9.74 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.21356189198199		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 2.21356189198199 | validation: 3.3937146150048716]
	TIME [epoch: 9.74 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.242520489957225		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 2.242520489957225 | validation: 3.385865512080493]
	TIME [epoch: 9.75 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2107389583955723		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 2.2107389583955723 | validation: 3.338856665650816]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1020.pth
	Model improved!!!
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.209473541436805		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 2.209473541436805 | validation: 3.347979856278198]
	TIME [epoch: 9.74 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2178046503160744		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 2.2178046503160744 | validation: 3.502048186139347]
	TIME [epoch: 9.75 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2390690411152847		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 2.2390690411152847 | validation: 3.3912907659137623]
	TIME [epoch: 9.75 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2647227080971		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 2.2647227080971 | validation: 3.3922561959467474]
	TIME [epoch: 9.73 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2243464827898345		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 2.2243464827898345 | validation: 3.348902357059941]
	TIME [epoch: 9.74 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1989561875490904		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 2.1989561875490904 | validation: 3.3803011019885076]
	TIME [epoch: 9.76 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.213175945876785		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 2.213175945876785 | validation: 3.3868755856131147]
	TIME [epoch: 9.74 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.206686029481511		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 2.206686029481511 | validation: 3.3467135564677375]
	TIME [epoch: 9.74 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202900854445463		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 2.202900854445463 | validation: 3.3916417347527843]
	TIME [epoch: 9.76 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2085661260147282		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 2.2085661260147282 | validation: 3.378656602914982]
	TIME [epoch: 9.75 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2230496407154328		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 2.2230496407154328 | validation: 3.3912409458809907]
	TIME [epoch: 9.74 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2173524459319744		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 2.2173524459319744 | validation: 3.3589847590116633]
	TIME [epoch: 9.75 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197448291228608		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 2.197448291228608 | validation: 3.3578024043263714]
	TIME [epoch: 9.76 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2093285285553037		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 2.2093285285553037 | validation: 3.3790369071531097]
	TIME [epoch: 9.75 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1985299649060925		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 2.1985299649060925 | validation: 3.3750934927172156]
	TIME [epoch: 9.74 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.199229116246985		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 2.199229116246985 | validation: 3.44598410099008]
	TIME [epoch: 9.77 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2276830604367253		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 2.2276830604367253 | validation: 3.420754634072189]
	TIME [epoch: 9.75 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1991892008324294		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 2.1991892008324294 | validation: 3.36761435606261]
	TIME [epoch: 9.75 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2008644441577405		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 2.2008644441577405 | validation: 3.386018156604188]
	TIME [epoch: 9.76 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210485951944217		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 2.210485951944217 | validation: 3.4130557569933866]
	TIME [epoch: 9.75 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20833180748345		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 2.20833180748345 | validation: 3.430067486544863]
	TIME [epoch: 9.74 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2092443147555416		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 2.2092443147555416 | validation: 3.401577687931456]
	TIME [epoch: 9.74 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198776945763209		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 2.198776945763209 | validation: 3.3673811178141935]
	TIME [epoch: 9.76 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2283372237891745		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 2.2283372237891745 | validation: 3.3826649281001164]
	TIME [epoch: 9.75 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2195958820327317		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 2.2195958820327317 | validation: 3.414321552384531]
	TIME [epoch: 9.74 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2167902271937328		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 2.2167902271937328 | validation: 3.3808356094288388]
	TIME [epoch: 9.76 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2096724310748526		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 2.2096724310748526 | validation: 3.371421165908757]
	TIME [epoch: 9.74 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1972602109978348		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 2.1972602109978348 | validation: 3.355641496866993]
	TIME [epoch: 9.73 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201357843582557		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 2.201357843582557 | validation: 3.3590062071151827]
	TIME [epoch: 9.74 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2012080682158595		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 2.2012080682158595 | validation: 3.354352369358774]
	TIME [epoch: 9.75 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210925449244705		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 2.210925449244705 | validation: 3.3885052918490395]
	TIME [epoch: 9.73 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2258948193389885		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 2.2258948193389885 | validation: 3.4118891627851897]
	TIME [epoch: 9.73 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195552673851342		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 2.195552673851342 | validation: 3.355134791945785]
	TIME [epoch: 9.75 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1982527887647616		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 2.1982527887647616 | validation: 3.3469452312735593]
	TIME [epoch: 9.74 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2075406735779914		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 2.2075406735779914 | validation: 3.4885958880967998]
	TIME [epoch: 9.74 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2348554045566926		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 2.2348554045566926 | validation: 3.349163573801411]
	TIME [epoch: 9.74 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195881915227072		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 2.195881915227072 | validation: 3.357088735922214]
	TIME [epoch: 9.74 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1973376135231106		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 2.1973376135231106 | validation: 3.371208237048615]
	TIME [epoch: 9.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201229029106016		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 2.201229029106016 | validation: 3.418128353249458]
	TIME [epoch: 9.73 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2087252130569923		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 2.2087252130569923 | validation: 3.3426417426077477]
	TIME [epoch: 9.74 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208009106549995		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 2.208009106549995 | validation: 3.439752229546193]
	TIME [epoch: 9.73 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2085539297698		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 2.2085539297698 | validation: 3.3745941846436383]
	TIME [epoch: 9.74 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2016412630197513		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 2.2016412630197513 | validation: 3.3720485970183693]
	TIME [epoch: 9.75 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20188038463292		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 2.20188038463292 | validation: 3.3570964426127547]
	TIME [epoch: 9.74 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.213273691862258		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 2.213273691862258 | validation: 3.412209089936098]
	TIME [epoch: 9.73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205497378775267		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 2.205497378775267 | validation: 3.390511608177177]
	TIME [epoch: 9.74 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2033421981270283		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 2.2033421981270283 | validation: 3.425480412535932]
	TIME [epoch: 9.75 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2049627175992255		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 2.2049627175992255 | validation: 3.3864400692270022]
	TIME [epoch: 9.74 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201178749146796		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 2.201178749146796 | validation: 3.4061022588092507]
	TIME [epoch: 9.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205365868014785		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 2.205365868014785 | validation: 3.383386463576357]
	TIME [epoch: 9.75 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210594886808551		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 2.210594886808551 | validation: 3.3579087099360705]
	TIME [epoch: 9.74 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.209022007521577		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 2.209022007521577 | validation: 3.38007988098251]
	TIME [epoch: 9.74 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.212259374743018		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 2.212259374743018 | validation: 3.3586746061854127]
	TIME [epoch: 9.75 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.203279595833202		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 2.203279595833202 | validation: 3.392521789980053]
	TIME [epoch: 9.74 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1949628504850294		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 2.1949628504850294 | validation: 3.359942137380073]
	TIME [epoch: 9.74 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201659207095429		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 2.201659207095429 | validation: 3.4098020916520273]
	TIME [epoch: 9.73 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.218539698741701		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 2.218539698741701 | validation: 3.3526237748966983]
	TIME [epoch: 9.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2112715915829315		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 2.2112715915829315 | validation: 3.3709211834759527]
	TIME [epoch: 9.73 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198397185342361		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 2.198397185342361 | validation: 3.3604315219652943]
	TIME [epoch: 9.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2069672507164126		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 2.2069672507164126 | validation: 3.3490757037302625]
	TIME [epoch: 9.75 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.215857037346126		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 2.215857037346126 | validation: 3.35871405302155]
	TIME [epoch: 9.74 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2265416513905225		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 2.2265416513905225 | validation: 3.4500718467003253]
	TIME [epoch: 9.74 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221282743682207		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 2.221282743682207 | validation: 3.36570120055299]
	TIME [epoch: 9.73 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2179611714507486		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 2.2179611714507486 | validation: 3.4387696722256638]
	TIME [epoch: 9.75 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2178173382575292		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 2.2178173382575292 | validation: 3.367132443063322]
	TIME [epoch: 9.73 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202020742354776		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 2.202020742354776 | validation: 3.365705960565053]
	TIME [epoch: 9.74 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2034478337755212		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 2.2034478337755212 | validation: 3.3522976718408266]
	TIME [epoch: 9.74 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205932526723902		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 2.205932526723902 | validation: 3.423228087723714]
	TIME [epoch: 9.74 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2072411777411265		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 2.2072411777411265 | validation: 3.429955938521821]
	TIME [epoch: 9.73 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2165329102893443		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 2.2165329102893443 | validation: 3.3672716383181633]
	TIME [epoch: 9.74 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201485468332037		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 2.201485468332037 | validation: 3.383101342104995]
	TIME [epoch: 9.74 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.199552117340025		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 2.199552117340025 | validation: 3.3942193962357408]
	TIME [epoch: 9.73 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.207056106997596		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 2.207056106997596 | validation: 3.37572966710738]
	TIME [epoch: 9.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211448059567071		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 2.211448059567071 | validation: 3.36700877147652]
	TIME [epoch: 9.75 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214613431039001		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 2.214613431039001 | validation: 3.413445835750798]
	TIME [epoch: 9.74 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.224611759390106		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 2.224611759390106 | validation: 3.4409337895330383]
	TIME [epoch: 9.73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.208363320851313		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 2.208363320851313 | validation: 3.3743613674782753]
	TIME [epoch: 9.75 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2203534427201594		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 2.2203534427201594 | validation: 3.3785947235510077]
	TIME [epoch: 9.74 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195479229691956		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 2.195479229691956 | validation: 3.378108747638148]
	TIME [epoch: 9.74 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2013072121963404		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 2.2013072121963404 | validation: 3.3537170961434595]
	TIME [epoch: 9.74 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20566530146138		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 2.20566530146138 | validation: 3.3728484914386776]
	TIME [epoch: 9.75 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202230655957993		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 2.202230655957993 | validation: 3.390935944071131]
	TIME [epoch: 9.73 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211418047868091		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 2.211418047868091 | validation: 3.365355788985596]
	TIME [epoch: 9.73 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1970688142573405		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 2.1970688142573405 | validation: 3.387374471855492]
	TIME [epoch: 9.74 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2051910038048144		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 2.2051910038048144 | validation: 3.3905999950780914]
	TIME [epoch: 9.74 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1979877057641604		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 2.1979877057641604 | validation: 3.36225991982655]
	TIME [epoch: 9.73 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1950011204385182		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 2.1950011204385182 | validation: 3.389564407395411]
	TIME [epoch: 9.74 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.221267919942297		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 2.221267919942297 | validation: 3.4120607507332683]
	TIME [epoch: 9.74 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.22188986263444		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 2.22188986263444 | validation: 3.4021768493395697]
	TIME [epoch: 9.73 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200390967795059		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 2.200390967795059 | validation: 3.3787924674470946]
	TIME [epoch: 9.73 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210739985853899		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 2.210739985853899 | validation: 3.3928010989124084]
	TIME [epoch: 9.74 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2058379411205578		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 2.2058379411205578 | validation: 3.355530577697725]
	TIME [epoch: 9.74 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.204588718131101		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 2.204588718131101 | validation: 3.345559906361351]
	TIME [epoch: 9.73 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2113789662053582		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 2.2113789662053582 | validation: 3.4424372754176926]
	TIME [epoch: 9.75 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2072242524596413		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 2.2072242524596413 | validation: 3.4178252343192517]
	TIME [epoch: 9.73 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.207883727105482		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 2.207883727105482 | validation: 3.4255259529415976]
	TIME [epoch: 9.73 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.204431057091539		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 2.204431057091539 | validation: 3.353240519439425]
	TIME [epoch: 9.73 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194536163943422		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 2.194536163943422 | validation: 3.3633249105873597]
	TIME [epoch: 9.74 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2005927521658535		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 2.2005927521658535 | validation: 3.382042499358781]
	TIME [epoch: 9.74 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2097742506191898		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 2.2097742506191898 | validation: 3.4193671266686123]
	TIME [epoch: 9.74 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2062564271474066		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 2.2062564271474066 | validation: 3.3684881275124803]
	TIME [epoch: 9.75 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1924657770176537		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 2.1924657770176537 | validation: 3.3870943103008195]
	TIME [epoch: 9.73 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2086282762374543		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 2.2086282762374543 | validation: 3.394175808643652]
	TIME [epoch: 9.73 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.204815041936669		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 2.204815041936669 | validation: 3.3779910154414394]
	TIME [epoch: 9.74 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.204275586667422		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 2.204275586667422 | validation: 3.4311631240186307]
	TIME [epoch: 9.74 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2244748265975858		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 2.2244748265975858 | validation: 3.4131719365523487]
	TIME [epoch: 9.73 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201015347025189		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 2.201015347025189 | validation: 3.359191499593352]
	TIME [epoch: 9.74 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1979427514877754		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 2.1979427514877754 | validation: 3.384060203779597]
	TIME [epoch: 9.75 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1942638923911617		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 2.1942638923911617 | validation: 3.357335897340828]
	TIME [epoch: 9.73 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210962769143729		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 2.210962769143729 | validation: 3.39398188410986]
	TIME [epoch: 9.73 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205002793239297		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 2.205002793239297 | validation: 3.387473877340078]
	TIME [epoch: 9.74 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2027736447798825		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 2.2027736447798825 | validation: 3.373132625034922]
	TIME [epoch: 9.73 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214205987681598		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 2.214205987681598 | validation: 3.3540823281362986]
	TIME [epoch: 9.73 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.214990506892737		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 2.214990506892737 | validation: 3.3960252396703847]
	TIME [epoch: 9.74 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198810632525758		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 2.198810632525758 | validation: 3.3487866494144725]
	TIME [epoch: 9.75 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200933590031293		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 2.200933590031293 | validation: 3.3435305771114985]
	TIME [epoch: 9.73 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202976810378061		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 2.202976810378061 | validation: 3.3437461564993693]
	TIME [epoch: 9.73 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1981175602203367		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 2.1981175602203367 | validation: 3.340208022196939]
	TIME [epoch: 9.75 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1953629204691665		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 2.1953629204691665 | validation: 3.416284089022479]
	TIME [epoch: 9.74 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.213540070062371		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 2.213540070062371 | validation: 3.34688756802274]
	TIME [epoch: 9.73 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200843374106667		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 2.200843374106667 | validation: 3.3381478087976593]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194193055665626		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 2.194193055665626 | validation: 3.3483384273602814]
	TIME [epoch: 9.73 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1951828604622925		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 2.1951828604622925 | validation: 3.3943319922431967]
	TIME [epoch: 9.73 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200602562496441		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 2.200602562496441 | validation: 3.340785405243974]
	TIME [epoch: 9.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1996258676549245		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 2.1996258676549245 | validation: 3.345369878059772]
	TIME [epoch: 9.76 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1985376581886498		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 2.1985376581886498 | validation: 3.3675321762520465]
	TIME [epoch: 9.74 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200483280310176		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 2.200483280310176 | validation: 3.343026980356254]
	TIME [epoch: 9.74 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2209916128521727		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 2.2209916128521727 | validation: 3.3622219532067574]
	TIME [epoch: 9.75 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205773833122509		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 2.205773833122509 | validation: 3.39901396265811]
	TIME [epoch: 9.74 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1984205565893937		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 2.1984205565893937 | validation: 3.3522790129009206]
	TIME [epoch: 9.74 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1963051819929964		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 2.1963051819929964 | validation: 3.363534104473665]
	TIME [epoch: 9.74 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2039291993013914		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 2.2039291993013914 | validation: 3.377053522570886]
	TIME [epoch: 9.74 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1970831529481636		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 2.1970831529481636 | validation: 3.367339638968449]
	TIME [epoch: 9.74 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198739326940676		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 2.198739326940676 | validation: 3.355578075812992]
	TIME [epoch: 9.74 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1896477062678037		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 2.1896477062678037 | validation: 3.3527010287531973]
	TIME [epoch: 9.76 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1985238156971088		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 2.1985238156971088 | validation: 3.352473941588932]
	TIME [epoch: 9.74 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192283100168559		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 2.192283100168559 | validation: 3.3631998958856895]
	TIME [epoch: 9.74 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201308389864365		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 2.201308389864365 | validation: 3.359235711530074]
	TIME [epoch: 9.75 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197998940033208		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 2.197998940033208 | validation: 3.3473502777726334]
	TIME [epoch: 9.73 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200545999399793		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 2.200545999399793 | validation: 3.3807685037337984]
	TIME [epoch: 9.74 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1961575626113374		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 2.1961575626113374 | validation: 3.371093839508069]
	TIME [epoch: 9.74 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19229051389688		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 2.19229051389688 | validation: 3.340682061396551]
	TIME [epoch: 9.76 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1891077870802986		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 2.1891077870802986 | validation: 3.342300367694538]
	TIME [epoch: 9.74 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2161896423810363		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 2.2161896423810363 | validation: 3.3564723066972175]
	TIME [epoch: 9.74 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1990890196080572		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 2.1990890196080572 | validation: 3.3678060730812422]
	TIME [epoch: 9.76 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1956416312068683		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 2.1956416312068683 | validation: 3.3424676214857176]
	TIME [epoch: 9.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1966140711548077		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 2.1966140711548077 | validation: 3.351583850064339]
	TIME [epoch: 9.74 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197246828301728		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 2.197246828301728 | validation: 3.3582634206216877]
	TIME [epoch: 9.75 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2146228708373745		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 2.2146228708373745 | validation: 3.371567815553724]
	TIME [epoch: 9.75 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1981209717886396		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 2.1981209717886396 | validation: 3.342616298470054]
	TIME [epoch: 9.74 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2052496722110435		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 2.2052496722110435 | validation: 3.339805088345461]
	TIME [epoch: 9.74 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1965008317463868		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 2.1965008317463868 | validation: 3.346668748091021]
	TIME [epoch: 9.75 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.215290596973191		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 2.215290596973191 | validation: 3.345296495249966]
	TIME [epoch: 9.74 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.220086230072197		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 2.220086230072197 | validation: 3.3651520057700814]
	TIME [epoch: 9.74 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1942964199591684		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 2.1942964199591684 | validation: 3.347925896057393]
	TIME [epoch: 9.76 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846229584222767		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 2.1846229584222767 | validation: 3.3595519436629173]
	TIME [epoch: 9.73 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194677094010813		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 2.194677094010813 | validation: 3.3465719253688304]
	TIME [epoch: 9.74 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2046954239410006		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 2.2046954239410006 | validation: 3.350105755112399]
	TIME [epoch: 9.74 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2092112746906327		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 2.2092112746906327 | validation: 3.352748139293713]
	TIME [epoch: 9.75 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188244322890673		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 2.188244322890673 | validation: 3.3576353723967634]
	TIME [epoch: 9.74 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20349671583378		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 2.20349671583378 | validation: 3.3490709615528163]
	TIME [epoch: 9.74 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200951019889029		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 2.200951019889029 | validation: 3.351069867210328]
	TIME [epoch: 9.75 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1907831392315016		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 2.1907831392315016 | validation: 3.3484844148699846]
	TIME [epoch: 9.74 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197166181498838		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 2.197166181498838 | validation: 3.35858470837912]
	TIME [epoch: 9.74 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188885295352062		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 2.188885295352062 | validation: 3.330878628832271]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1185.pth
	Model improved!!!
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1927789465555017		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 2.1927789465555017 | validation: 3.3646029140859537]
	TIME [epoch: 9.74 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192578552468607		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 2.192578552468607 | validation: 3.347645562512661]
	TIME [epoch: 9.74 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1971056980713684		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 2.1971056980713684 | validation: 3.343658701538093]
	TIME [epoch: 9.73 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.201315743007405		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 2.201315743007405 | validation: 3.3689833635251456]
	TIME [epoch: 9.75 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2015793264897345		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 2.2015793264897345 | validation: 3.3408217547180983]
	TIME [epoch: 9.74 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1916664276932187		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 2.1916664276932187 | validation: 3.3330261075371372]
	TIME [epoch: 9.74 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1975260770579026		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 2.1975260770579026 | validation: 3.3485066239038317]
	TIME [epoch: 9.75 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1922427865294134		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 2.1922427865294134 | validation: 3.346986522476353]
	TIME [epoch: 9.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2016412067539597		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 2.2016412067539597 | validation: 3.342519431521235]
	TIME [epoch: 9.73 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1940615698162853		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 2.1940615698162853 | validation: 3.3458248123429906]
	TIME [epoch: 9.73 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198267624648745		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 2.198267624648745 | validation: 3.3369139218637884]
	TIME [epoch: 9.75 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1965770684109707		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 2.1965770684109707 | validation: 3.361343324865136]
	TIME [epoch: 9.74 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196971638839334		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 2.196971638839334 | validation: 3.370590291682687]
	TIME [epoch: 9.73 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1851995297527997		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 2.1851995297527997 | validation: 3.3820132599147246]
	TIME [epoch: 9.75 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187099630305844		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 2.187099630305844 | validation: 3.3616552483898]
	TIME [epoch: 9.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196160985751159		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 2.196160985751159 | validation: 3.368115225809134]
	TIME [epoch: 9.73 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190193053686162		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 2.190193053686162 | validation: 3.367476793127519]
	TIME [epoch: 9.75 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190225651280359		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 2.190225651280359 | validation: 3.353903076522156]
	TIME [epoch: 9.74 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1925571582431447		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 2.1925571582431447 | validation: 3.383092399875872]
	TIME [epoch: 9.73 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.227752714687163		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 2.227752714687163 | validation: 3.347130248935905]
	TIME [epoch: 9.74 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2071994217257296		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 2.2071994217257296 | validation: 3.346853681397011]
	TIME [epoch: 9.75 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1917115839119092		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 2.1917115839119092 | validation: 3.3432835333101845]
	TIME [epoch: 9.73 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195867046766409		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 2.195867046766409 | validation: 3.391707233400215]
	TIME [epoch: 9.73 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1965555430671673		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 2.1965555430671673 | validation: 3.3398872208185435]
	TIME [epoch: 9.75 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19813254966144		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 2.19813254966144 | validation: 3.345386852059751]
	TIME [epoch: 9.73 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.227067349414117		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 2.227067349414117 | validation: 3.4916418870855286]
	TIME [epoch: 9.73 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2426493307539372		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 2.2426493307539372 | validation: 3.3669600856244295]
	TIME [epoch: 9.74 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1959846380906085		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 2.1959846380906085 | validation: 3.3510346378356277]
	TIME [epoch: 9.75 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.218324169830062		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 2.218324169830062 | validation: 3.349286201346655]
	TIME [epoch: 9.74 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1962977799975176		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 2.1962977799975176 | validation: 3.345424555920683]
	TIME [epoch: 9.75 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200942273602419		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 2.200942273602419 | validation: 3.341614732487024]
	TIME [epoch: 9.76 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1899182270468187		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 2.1899182270468187 | validation: 3.352040357268961]
	TIME [epoch: 9.73 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1919801029219856		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 2.1919801029219856 | validation: 3.3297764385220154]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1218.pth
	Model improved!!!
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1975572266064427		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 2.1975572266064427 | validation: 3.3827394601663445]
	TIME [epoch: 9.75 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1985950041129905		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 2.1985950041129905 | validation: 3.349417226076389]
	TIME [epoch: 9.74 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196481534962429		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 2.196481534962429 | validation: 3.356290024495521]
	TIME [epoch: 9.73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2001707703893993		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 2.2001707703893993 | validation: 3.338995707203028]
	TIME [epoch: 9.74 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198012603920473		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 2.198012603920473 | validation: 3.3588415053509886]
	TIME [epoch: 9.76 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1904409779322895		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 2.1904409779322895 | validation: 3.3421311559472855]
	TIME [epoch: 9.73 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2008282543773428		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 2.2008282543773428 | validation: 3.3457628460399276]
	TIME [epoch: 9.74 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198817819486813		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 2.198817819486813 | validation: 3.35180095455211]
	TIME [epoch: 9.75 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.199035909559937		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 2.199035909559937 | validation: 3.3728073010863295]
	TIME [epoch: 9.74 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198879657555069		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 2.198879657555069 | validation: 3.3590491324589347]
	TIME [epoch: 9.73 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1924643387121203		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 2.1924643387121203 | validation: 3.327759181930426]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1229.pth
	Model improved!!!
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1925037490077792		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 2.1925037490077792 | validation: 3.341783855742503]
	TIME [epoch: 9.75 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19669437011706		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 2.19669437011706 | validation: 3.3569654081140783]
	TIME [epoch: 9.74 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1952923334217074		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 2.1952923334217074 | validation: 3.3598358473466]
	TIME [epoch: 9.73 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196216254112699		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 2.196216254112699 | validation: 3.351680160960914]
	TIME [epoch: 9.76 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2112305398978673		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 2.2112305398978673 | validation: 3.3505042222723938]
	TIME [epoch: 9.73 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184709031442196		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 2.184709031442196 | validation: 3.3617551042342964]
	TIME [epoch: 9.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196193967991881		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 2.196193967991881 | validation: 3.353483936879974]
	TIME [epoch: 9.75 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188271354735697		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 2.188271354735697 | validation: 3.3496360656178856]
	TIME [epoch: 9.74 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.199811940737889		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 2.199811940737889 | validation: 3.343465936278215]
	TIME [epoch: 9.74 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1908832335614266		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 2.1908832335614266 | validation: 3.356258636095108]
	TIME [epoch: 9.74 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1920965246046342		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 2.1920965246046342 | validation: 3.355220914050657]
	TIME [epoch: 9.76 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2002624841728027		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 2.2002624841728027 | validation: 3.3599995303216064]
	TIME [epoch: 9.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2008992283417643		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 2.2008992283417643 | validation: 3.3383789847367225]
	TIME [epoch: 9.74 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195972030233055		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 2.195972030233055 | validation: 3.3495549378311544]
	TIME [epoch: 9.76 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193578099360974		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 2.193578099360974 | validation: 3.3508868809242287]
	TIME [epoch: 9.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1969620094454583		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 2.1969620094454583 | validation: 3.3566421592091116]
	TIME [epoch: 9.74 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195423267208086		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 2.195423267208086 | validation: 3.38468659784443]
	TIME [epoch: 9.77 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.199583780471772		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 2.199583780471772 | validation: 3.357321340267026]
	TIME [epoch: 9.75 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192235563361818		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 2.192235563361818 | validation: 3.3505250851551795]
	TIME [epoch: 9.74 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1893412543532453		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 2.1893412543532453 | validation: 3.343707772194027]
	TIME [epoch: 9.74 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.200355527156882		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 2.200355527156882 | validation: 3.3474132052938]
	TIME [epoch: 9.75 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2028479595246155		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 2.2028479595246155 | validation: 3.352859832724355]
	TIME [epoch: 9.75 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195944212125004		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 2.195944212125004 | validation: 3.3488221210361404]
	TIME [epoch: 9.75 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1854752860402464		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 2.1854752860402464 | validation: 3.376084276054689]
	TIME [epoch: 9.77 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205070289263699		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 2.205070289263699 | validation: 3.388259265480424]
	TIME [epoch: 9.75 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.205342637905534		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 2.205342637905534 | validation: 3.3319885058672454]
	TIME [epoch: 9.75 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1936425597356175		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 2.1936425597356175 | validation: 3.356381452256325]
	TIME [epoch: 9.76 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1907170566827814		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 2.1907170566827814 | validation: 3.3584477323475865]
	TIME [epoch: 9.76 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192981533960658		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 2.192981533960658 | validation: 3.383982371943552]
	TIME [epoch: 9.73 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1891942100785946		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 2.1891942100785946 | validation: 3.3460852618600585]
	TIME [epoch: 9.74 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.203381896409737		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 2.203381896409737 | validation: 3.3448175419270183]
	TIME [epoch: 9.76 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2011032855099977		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 2.2011032855099977 | validation: 3.3732276543587445]
	TIME [epoch: 9.75 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198743054726509		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 2.198743054726509 | validation: 3.3615023617949955]
	TIME [epoch: 9.75 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1937404920788275		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 2.1937404920788275 | validation: 3.3507924214322737]
	TIME [epoch: 9.77 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1890216064560515		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 2.1890216064560515 | validation: 3.3651101894201414]
	TIME [epoch: 9.75 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1901597824487644		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 2.1901597824487644 | validation: 3.3502770117088163]
	TIME [epoch: 9.75 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197444366042794		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 2.197444366042794 | validation: 3.38094768233924]
	TIME [epoch: 9.75 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1920371822955644		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 2.1920371822955644 | validation: 3.391954446451829]
	TIME [epoch: 9.76 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197595709682722		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 2.197595709682722 | validation: 3.3489850074992136]
	TIME [epoch: 9.75 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2041204911015706		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 2.2041204911015706 | validation: 3.3601856162569335]
	TIME [epoch: 9.75 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2097790532187305		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 2.2097790532187305 | validation: 3.347430675539708]
	TIME [epoch: 9.76 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1884136235881977		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 2.1884136235881977 | validation: 3.3752742799080506]
	TIME [epoch: 9.76 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2070838813755427		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 2.2070838813755427 | validation: 3.3726198237649463]
	TIME [epoch: 9.75 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1912236831578484		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 2.1912236831578484 | validation: 3.3782619889929]
	TIME [epoch: 9.75 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194152614331606		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 2.194152614331606 | validation: 3.3430223226760987]
	TIME [epoch: 9.75 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1981814654102236		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 2.1981814654102236 | validation: 3.3713790821238367]
	TIME [epoch: 9.75 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191714131208651		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 2.191714131208651 | validation: 3.3662732709139727]
	TIME [epoch: 9.75 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195211839121835		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 2.195211839121835 | validation: 3.3562133457141377]
	TIME [epoch: 9.76 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1943413286284947		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 2.1943413286284947 | validation: 3.363131989120994]
	TIME [epoch: 9.75 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197848336935966		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 2.197848336935966 | validation: 3.38112950883302]
	TIME [epoch: 9.75 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2172679209082586		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 2.2172679209082586 | validation: 3.3602268739428696]
	TIME [epoch: 9.76 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1979819059502934		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 2.1979819059502934 | validation: 3.352086357871454]
	TIME [epoch: 9.75 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18885175072119		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 2.18885175072119 | validation: 3.3461033119633887]
	TIME [epoch: 9.75 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192472487140629		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 2.192472487140629 | validation: 3.33389727443917]
	TIME [epoch: 9.75 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1925182937617005		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 2.1925182937617005 | validation: 3.356759668541424]
	TIME [epoch: 9.76 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191767650578823		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 2.191767650578823 | validation: 3.341910394775523]
	TIME [epoch: 9.74 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188978874642623		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 2.188978874642623 | validation: 3.3702305282687037]
	TIME [epoch: 9.75 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2011591892262885		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 2.2011591892262885 | validation: 3.337933460659714]
	TIME [epoch: 9.76 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1911404591139103		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 2.1911404591139103 | validation: 3.3446812418453926]
	TIME [epoch: 9.76 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1944824562048706		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 2.1944824562048706 | validation: 3.335588030232791]
	TIME [epoch: 9.75 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1889535919850904		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 2.1889535919850904 | validation: 3.3483266917417223]
	TIME [epoch: 9.76 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1970278587690384		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 2.1970278587690384 | validation: 3.380294038873946]
	TIME [epoch: 9.76 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1914352274603286		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 2.1914352274603286 | validation: 3.344122490849901]
	TIME [epoch: 9.75 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197033749111803		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 2.197033749111803 | validation: 3.3427141250107533]
	TIME [epoch: 9.75 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202454462160295		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 2.202454462160295 | validation: 3.338983297040079]
	TIME [epoch: 9.77 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1925642669836494		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 2.1925642669836494 | validation: 3.3462750286877143]
	TIME [epoch: 9.75 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1950330829370377		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 2.1950330829370377 | validation: 3.35681539627882]
	TIME [epoch: 9.75 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189625750946574		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 2.189625750946574 | validation: 3.349127551656917]
	TIME [epoch: 9.77 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190650691137917		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 2.190650691137917 | validation: 3.3418870661022595]
	TIME [epoch: 9.74 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2018258435175353		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 2.2018258435175353 | validation: 3.341643462921563]
	TIME [epoch: 9.75 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1962237164959757		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 2.1962237164959757 | validation: 3.3413899552260236]
	TIME [epoch: 9.75 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19229902270623		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 2.19229902270623 | validation: 3.3485217861039893]
	TIME [epoch: 9.76 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198338700064342		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 2.198338700064342 | validation: 3.3624115881944885]
	TIME [epoch: 9.75 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1929228380140393		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 2.1929228380140393 | validation: 3.356084429732133]
	TIME [epoch: 9.75 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194322685449905		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 2.194322685449905 | validation: 3.3778952487022638]
	TIME [epoch: 9.77 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2015716535655407		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 2.2015716535655407 | validation: 3.3878254396639758]
	TIME [epoch: 9.75 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191039443301775		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 2.191039443301775 | validation: 3.3349833250840777]
	TIME [epoch: 9.75 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1892079796690416		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 2.1892079796690416 | validation: 3.3422311443828296]
	TIME [epoch: 9.77 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185908834479504		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 2.185908834479504 | validation: 3.342495768243765]
	TIME [epoch: 9.75 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195116736499248		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 2.195116736499248 | validation: 3.3623396720765726]
	TIME [epoch: 9.75 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1998326345543564		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 2.1998326345543564 | validation: 3.373225121466114]
	TIME [epoch: 9.75 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1918762625914985		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 2.1918762625914985 | validation: 3.360650494458962]
	TIME [epoch: 9.76 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1980864761564813		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 2.1980864761564813 | validation: 3.3396131031145364]
	TIME [epoch: 9.75 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194261992605596		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 2.194261992605596 | validation: 3.3363808105677766]
	TIME [epoch: 9.75 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1927795805403965		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 2.1927795805403965 | validation: 3.3504778290417505]
	TIME [epoch: 9.76 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1906369102103818		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 2.1906369102103818 | validation: 3.355626503533768]
	TIME [epoch: 9.75 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189896933220817		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 2.189896933220817 | validation: 3.359292564309857]
	TIME [epoch: 9.75 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1913144778584464		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 2.1913144778584464 | validation: 3.344563399252092]
	TIME [epoch: 9.76 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193630604676664		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 2.193630604676664 | validation: 3.3485093401242203]
	TIME [epoch: 9.76 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197756886913504		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 2.197756886913504 | validation: 3.395032478547893]
	TIME [epoch: 9.75 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2070847989519224		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 2.2070847989519224 | validation: 3.3683258537302345]
	TIME [epoch: 9.74 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1935128546251335		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 2.1935128546251335 | validation: 3.361205626251816]
	TIME [epoch: 9.77 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1858588547789077		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 2.1858588547789077 | validation: 3.335169970413708]
	TIME [epoch: 9.75 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197859158692995		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 2.197859158692995 | validation: 3.366892699499754]
	TIME [epoch: 9.73 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189040150081435		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 2.189040150081435 | validation: 3.3540721978564103]
	TIME [epoch: 9.76 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1905152439265057		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 2.1905152439265057 | validation: 3.371002658297798]
	TIME [epoch: 9.75 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1863969819312605		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 2.1863969819312605 | validation: 3.359033945340789]
	TIME [epoch: 9.75 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186630246317614		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 2.186630246317614 | validation: 3.387806001430935]
	TIME [epoch: 9.74 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1879664944735495		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 2.1879664944735495 | validation: 3.3462640581990377]
	TIME [epoch: 9.76 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.20005561304687		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 2.20005561304687 | validation: 3.344926391726158]
	TIME [epoch: 9.74 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191731228666994		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 2.191731228666994 | validation: 3.3401186768208277]
	TIME [epoch: 9.75 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1938484497442436		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 2.1938484497442436 | validation: 3.340836521445675]
	TIME [epoch: 9.76 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187891623176355		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 2.187891623176355 | validation: 3.3465301449215374]
	TIME [epoch: 9.75 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1863542757499097		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 2.1863542757499097 | validation: 3.3421378992638076]
	TIME [epoch: 9.74 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1923848708982696		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 2.1923848708982696 | validation: 3.3651460294388076]
	TIME [epoch: 9.76 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186390325400537		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 2.186390325400537 | validation: 3.343904102863596]
	TIME [epoch: 9.76 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1942632197578917		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 2.1942632197578917 | validation: 3.355212737447583]
	TIME [epoch: 9.74 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187745794194158		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 2.187745794194158 | validation: 3.3483933238411225]
	TIME [epoch: 9.74 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1910716339916227		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 2.1910716339916227 | validation: 3.3502167856240157]
	TIME [epoch: 9.76 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1898463045978374		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 2.1898463045978374 | validation: 3.3356180312844224]
	TIME [epoch: 9.74 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189532465069163		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 2.189532465069163 | validation: 3.342382517823495]
	TIME [epoch: 9.75 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1992130258751326		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 2.1992130258751326 | validation: 3.3455130872843335]
	TIME [epoch: 9.76 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197571482901837		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 2.197571482901837 | validation: 3.363433619261906]
	TIME [epoch: 9.74 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19447272609841		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 2.19447272609841 | validation: 3.349737892459899]
	TIME [epoch: 9.74 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1938272298095702		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 2.1938272298095702 | validation: 3.3441794548860635]
	TIME [epoch: 9.75 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1912781534979873		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 2.1912781534979873 | validation: 3.3392662492252523]
	TIME [epoch: 9.77 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190898731221792		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 2.190898731221792 | validation: 3.347081789345264]
	TIME [epoch: 9.75 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1921090181802487		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 2.1921090181802487 | validation: 3.365541303554863]
	TIME [epoch: 9.76 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1858259583436475		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 2.1858259583436475 | validation: 3.349068075735138]
	TIME [epoch: 9.77 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1936932450934803		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 2.1936932450934803 | validation: 3.347227171358461]
	TIME [epoch: 9.75 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1861250994964		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 2.1861250994964 | validation: 3.3336029140078676]
	TIME [epoch: 9.75 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196554508849272		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 2.196554508849272 | validation: 3.3371715231353223]
	TIME [epoch: 9.76 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190411126620105		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 2.190411126620105 | validation: 3.355182735621387]
	TIME [epoch: 9.75 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189182442375529		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 2.189182442375529 | validation: 3.3466883495626587]
	TIME [epoch: 9.75 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195795961959959		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 2.195795961959959 | validation: 3.406748066901154]
	TIME [epoch: 9.74 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1930976731288485		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 2.1930976731288485 | validation: 3.3644325152748866]
	TIME [epoch: 9.77 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1838299127105456		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 2.1838299127105456 | validation: 3.3525565088807894]
	TIME [epoch: 9.75 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1999931700280455		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 2.1999931700280455 | validation: 3.3375489568143415]
	TIME [epoch: 9.75 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1947371583394584		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 2.1947371583394584 | validation: 3.3820965723926646]
	TIME [epoch: 9.76 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198271246212671		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 2.198271246212671 | validation: 3.353466325344332]
	TIME [epoch: 9.75 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196819912303413		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 2.196819912303413 | validation: 3.3597645529294993]
	TIME [epoch: 9.73 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1881202768188333		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 2.1881202768188333 | validation: 3.3568618765735527]
	TIME [epoch: 9.74 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1969308917544663		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 2.1969308917544663 | validation: 3.347454645879909]
	TIME [epoch: 9.76 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1904188025162994		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 2.1904188025162994 | validation: 3.338003857435696]
	TIME [epoch: 9.75 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1847738173844062		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 2.1847738173844062 | validation: 3.349158746176215]
	TIME [epoch: 9.74 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1878558393326015		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 2.1878558393326015 | validation: 3.3517567500988164]
	TIME [epoch: 9.81 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1890641814502354		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 2.1890641814502354 | validation: 3.344599458065354]
	TIME [epoch: 9.75 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191588501505107		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 2.191588501505107 | validation: 3.3454283928934587]
	TIME [epoch: 9.75 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189143920796474		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 2.189143920796474 | validation: 3.343883713562733]
	TIME [epoch: 9.75 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1922369178235805		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 2.1922369178235805 | validation: 3.3520356776167253]
	TIME [epoch: 9.75 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1864505190292958		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 2.1864505190292958 | validation: 3.338552137204364]
	TIME [epoch: 9.75 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1980316759079206		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 2.1980316759079206 | validation: 3.375904228170267]
	TIME [epoch: 9.74 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19465204531621		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 2.19465204531621 | validation: 3.3691560109854537]
	TIME [epoch: 9.74 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189177929289387		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 2.189177929289387 | validation: 3.3389605350705382]
	TIME [epoch: 9.74 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1911985818972712		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 2.1911985818972712 | validation: 3.341602384090445]
	TIME [epoch: 9.74 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195877962702238		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 2.195877962702238 | validation: 3.3348670774857387]
	TIME [epoch: 9.75 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1822694678109062		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 2.1822694678109062 | validation: 3.3419302396136072]
	TIME [epoch: 9.75 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191262775112076		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 2.191262775112076 | validation: 3.353314205805403]
	TIME [epoch: 9.74 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1847954706009007		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 2.1847954706009007 | validation: 3.356846170573165]
	TIME [epoch: 9.75 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1958787814193697		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 2.1958787814193697 | validation: 3.3343624656093733]
	TIME [epoch: 9.75 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187171416892026		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 2.187171416892026 | validation: 3.337086376911408]
	TIME [epoch: 9.75 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846047861416444		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 2.1846047861416444 | validation: 3.336771240903154]
	TIME [epoch: 9.74 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1856917055316183		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 2.1856917055316183 | validation: 3.349487146411025]
	TIME [epoch: 9.76 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.192538390034712		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 2.192538390034712 | validation: 3.325903684133557]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1383.pth
	Model improved!!!
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.198851329919897		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 2.198851329919897 | validation: 3.3324050097631805]
	TIME [epoch: 9.74 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1890875582094287		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 2.1890875582094287 | validation: 3.355193156817104]
	TIME [epoch: 9.75 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1855663309462754		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 2.1855663309462754 | validation: 3.337023338086806]
	TIME [epoch: 9.75 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196429403147394		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 2.196429403147394 | validation: 3.351615952652893]
	TIME [epoch: 9.74 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187299525558008		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 2.187299525558008 | validation: 3.3448709722267274]
	TIME [epoch: 9.74 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190035949193691		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 2.190035949193691 | validation: 3.366060241502712]
	TIME [epoch: 9.75 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1826682166698363		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 2.1826682166698363 | validation: 3.3617043887070284]
	TIME [epoch: 9.74 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1931314055574855		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 2.1931314055574855 | validation: 3.3562492748596084]
	TIME [epoch: 9.74 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1892127065668205		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 2.1892127065668205 | validation: 3.345152000857402]
	TIME [epoch: 9.77 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1882352356186647		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 2.1882352356186647 | validation: 3.3593450914134513]
	TIME [epoch: 9.73 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1859428825027742		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 2.1859428825027742 | validation: 3.335813556623677]
	TIME [epoch: 9.72 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185883584505683		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 2.185883584505683 | validation: 3.3800039351238205]
	TIME [epoch: 9.75 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.202220568308631		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 2.202220568308631 | validation: 3.3631028536030714]
	TIME [epoch: 9.74 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1997368151759296		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 2.1997368151759296 | validation: 3.3888674934797653]
	TIME [epoch: 9.74 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1897941825016725		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 2.1897941825016725 | validation: 3.359370351474072]
	TIME [epoch: 9.74 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190331470152739		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 2.190331470152739 | validation: 3.3354253015332795]
	TIME [epoch: 9.75 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1861391138554422		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 2.1861391138554422 | validation: 3.3309153554151623]
	TIME [epoch: 9.74 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190844079733824		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 2.190844079733824 | validation: 3.3504064392233324]
	TIME [epoch: 9.73 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1969271578781178		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 2.1969271578781178 | validation: 3.339598961473929]
	TIME [epoch: 9.75 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196909711765823		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 2.196909711765823 | validation: 3.3566727665480602]
	TIME [epoch: 9.74 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832465581384186		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 2.1832465581384186 | validation: 3.347711447423046]
	TIME [epoch: 9.73 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844524418342344		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 2.1844524418342344 | validation: 3.3369719276809757]
	TIME [epoch: 9.73 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185489565704229		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 2.185489565704229 | validation: 3.3574765509076565]
	TIME [epoch: 9.75 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184023332882224		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 2.184023332882224 | validation: 3.3516298216176335]
	TIME [epoch: 9.73 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186732024932105		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 2.186732024932105 | validation: 3.361927700273974]
	TIME [epoch: 9.74 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188406612980007		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 2.188406612980007 | validation: 3.328941362353985]
	TIME [epoch: 9.75 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188663597090507		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 2.188663597090507 | validation: 3.3434920525080534]
	TIME [epoch: 9.74 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1910805723321287		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 2.1910805723321287 | validation: 3.344298823218743]
	TIME [epoch: 9.74 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1909395820868984		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 2.1909395820868984 | validation: 3.359228965953046]
	TIME [epoch: 9.74 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1950707432117005		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 2.1950707432117005 | validation: 3.3507844273081786]
	TIME [epoch: 9.75 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196219510154422		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 2.196219510154422 | validation: 3.3637834343523405]
	TIME [epoch: 9.74 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1956691403238295		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 2.1956691403238295 | validation: 3.3690763768142205]
	TIME [epoch: 9.73 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1851330617626323		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 2.1851330617626323 | validation: 3.367042957413852]
	TIME [epoch: 9.75 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188489190592094		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 2.188489190592094 | validation: 3.3411498357467497]
	TIME [epoch: 9.73 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189378076452281		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 2.189378076452281 | validation: 3.3380824609828337]
	TIME [epoch: 9.74 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1869005919338558		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 2.1869005919338558 | validation: 3.35872790916217]
	TIME [epoch: 9.75 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866505139550836		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 2.1866505139550836 | validation: 3.3531155065832303]
	TIME [epoch: 9.75 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183966738931815		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 2.183966738931815 | validation: 3.3470381698322917]
	TIME [epoch: 9.73 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190847327872014		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 2.190847327872014 | validation: 3.3394404948979]
	TIME [epoch: 9.74 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187196341992908		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 2.187196341992908 | validation: 3.3523253531364587]
	TIME [epoch: 9.75 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1963434355748497		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 2.1963434355748497 | validation: 3.337851837431549]
	TIME [epoch: 9.73 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2030142821035823		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 2.2030142821035823 | validation: 3.3494945718653315]
	TIME [epoch: 9.73 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846298943137956		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 2.1846298943137956 | validation: 3.3446173674877917]
	TIME [epoch: 9.75 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188469266755516		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 2.188469266755516 | validation: 3.3513641114797554]
	TIME [epoch: 9.73 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1856100726045713		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 2.1856100726045713 | validation: 3.3562452741337347]
	TIME [epoch: 9.73 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194162561513157		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 2.194162561513157 | validation: 3.3473595162648997]
	TIME [epoch: 9.75 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2015935400614923		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 2.2015935400614923 | validation: 3.3636833104481982]
	TIME [epoch: 9.73 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1974914984654705		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 2.1974914984654705 | validation: 3.3370492969598815]
	TIME [epoch: 9.74 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1950251005097314		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 2.1950251005097314 | validation: 3.334472909349745]
	TIME [epoch: 9.73 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866067193170076		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 2.1866067193170076 | validation: 3.384831289022392]
	TIME [epoch: 9.75 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194271045606386		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 2.194271045606386 | validation: 3.347954564227385]
	TIME [epoch: 9.75 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187551545082343		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 2.187551545082343 | validation: 3.354546607957318]
	TIME [epoch: 9.74 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185451195272868		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 2.185451195272868 | validation: 3.341554035068997]
	TIME [epoch: 9.75 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197011063003218		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 2.197011063003218 | validation: 3.345525603950763]
	TIME [epoch: 9.74 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.211475634219349		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 2.211475634219349 | validation: 3.328264284897683]
	TIME [epoch: 9.74 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1904029231900184		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 2.1904029231900184 | validation: 3.352631302223256]
	TIME [epoch: 9.75 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193842277568925		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 2.193842277568925 | validation: 3.3384895911453136]
	TIME [epoch: 9.74 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866481375439455		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 2.1866481375439455 | validation: 3.350227043103979]
	TIME [epoch: 9.73 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190200231956099		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 2.190200231956099 | validation: 3.3714589165669624]
	TIME [epoch: 9.74 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19653378274714		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 2.19653378274714 | validation: 3.367909659302878]
	TIME [epoch: 9.76 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1891464548427226		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 2.1891464548427226 | validation: 3.3669700422139943]
	TIME [epoch: 9.74 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1867702490735117		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 2.1867702490735117 | validation: 3.357551440390604]
	TIME [epoch: 9.73 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1916003802062884		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 2.1916003802062884 | validation: 3.354923346228776]
	TIME [epoch: 9.75 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1933241491905826		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 2.1933241491905826 | validation: 3.400296186385241]
	TIME [epoch: 9.75 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.197432072384382		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 2.197432072384382 | validation: 3.3336157866707654]
	TIME [epoch: 9.75 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184489547831728		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 2.184489547831728 | validation: 3.3617618846407704]
	TIME [epoch: 9.74 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185198873938216		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 2.185198873938216 | validation: 3.339547538194753]
	TIME [epoch: 9.76 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188512068649918		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 2.188512068649918 | validation: 3.337536259153053]
	TIME [epoch: 9.74 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191579982386121		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 2.191579982386121 | validation: 3.3561874951487125]
	TIME [epoch: 9.74 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1959598830073643		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 2.1959598830073643 | validation: 3.3488014381370363]
	TIME [epoch: 9.76 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1875865409459605		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 2.1875865409459605 | validation: 3.3464319251167605]
	TIME [epoch: 9.75 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1924317440430587		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 2.1924317440430587 | validation: 3.342159122141806]
	TIME [epoch: 9.74 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.196402713205927		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 2.196402713205927 | validation: 3.338467056778285]
	TIME [epoch: 9.74 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182529979458606		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 2.182529979458606 | validation: 3.364824438699476]
	TIME [epoch: 9.75 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191464300334382		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 2.191464300334382 | validation: 3.3509889601897296]
	TIME [epoch: 9.74 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1883123894835395		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 2.1883123894835395 | validation: 3.3665672472480788]
	TIME [epoch: 9.75 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186720596684417		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 2.186720596684417 | validation: 3.358989631608117]
	TIME [epoch: 9.75 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1875259219304004		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 2.1875259219304004 | validation: 3.3544743720892343]
	TIME [epoch: 9.75 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1854632156524025		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 2.1854632156524025 | validation: 3.3608488741594873]
	TIME [epoch: 9.73 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1933534116396936		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 2.1933534116396936 | validation: 3.3748719006337207]
	TIME [epoch: 9.74 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191814943293152		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 2.191814943293152 | validation: 3.354696867184072]
	TIME [epoch: 9.74 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.19013959924313		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 2.19013959924313 | validation: 3.3516359145886883]
	TIME [epoch: 9.73 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182215140815968		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 2.182215140815968 | validation: 3.333928001635237]
	TIME [epoch: 9.74 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1859070116889745		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 2.1859070116889745 | validation: 3.3622673221878463]
	TIME [epoch: 9.77 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1922997471997303		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 2.1922997471997303 | validation: 3.3463466543766356]
	TIME [epoch: 9.74 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1843136466041067		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 2.1843136466041067 | validation: 3.351309170524179]
	TIME [epoch: 9.73 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1843512095322573		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 2.1843512095322573 | validation: 3.3482632927733436]
	TIME [epoch: 9.76 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1863622973389734		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 2.1863622973389734 | validation: 3.3579567473066367]
	TIME [epoch: 9.73 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1911374729674122		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 2.1911374729674122 | validation: 3.3624696001353973]
	TIME [epoch: 9.74 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193190904192123		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 2.193190904192123 | validation: 3.347371729973947]
	TIME [epoch: 9.75 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1859129506475306		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 2.1859129506475306 | validation: 3.35526132191924]
	TIME [epoch: 9.75 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1942697794535815		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 2.1942697794535815 | validation: 3.3810694021936967]
	TIME [epoch: 9.74 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.193519060814039		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 2.193519060814039 | validation: 3.377622882713944]
	TIME [epoch: 9.74 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195100891244222		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 2.195100891244222 | validation: 3.3945566042097575]
	TIME [epoch: 9.75 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1938200642193784		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 2.1938200642193784 | validation: 3.3475757816152574]
	TIME [epoch: 9.75 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1852262347693747		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 2.1852262347693747 | validation: 3.342878323101069]
	TIME [epoch: 9.73 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184648951116583		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 2.184648951116583 | validation: 3.352801316938816]
	TIME [epoch: 9.76 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1888189016225725		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 2.1888189016225725 | validation: 3.356388449624693]
	TIME [epoch: 9.75 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1930096790404825		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 2.1930096790404825 | validation: 3.386459051283679]
	TIME [epoch: 9.74 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1913417295373527		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 2.1913417295373527 | validation: 3.343681323584333]
	TIME [epoch: 9.74 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.195076711802252		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 2.195076711802252 | validation: 3.331961427651346]
	TIME [epoch: 9.76 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1910271029576274		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 2.1910271029576274 | validation: 3.3576355311951787]
	TIME [epoch: 9.74 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18757649333165		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 2.18757649333165 | validation: 3.3513009390035813]
	TIME [epoch: 9.74 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184616698188898		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 2.184616698188898 | validation: 3.346204457497962]
	TIME [epoch: 9.75 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1931324427941417		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 2.1931324427941417 | validation: 3.3358254530591003]
	TIME [epoch: 9.74 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1946230285688646		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 2.1946230285688646 | validation: 3.3388915982386687]
	TIME [epoch: 9.74 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181780037587452		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 2.181780037587452 | validation: 3.3418446900278593]
	TIME [epoch: 9.75 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188189645984173		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 2.188189645984173 | validation: 3.3511430048950674]
	TIME [epoch: 9.74 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1842598905803206		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 2.1842598905803206 | validation: 3.3349525848312322]
	TIME [epoch: 9.74 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1898550136237764		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 2.1898550136237764 | validation: 3.332523115363557]
	TIME [epoch: 9.74 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1934180091257707		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 2.1934180091257707 | validation: 3.362154472448573]
	TIME [epoch: 9.76 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1865577522105477		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 2.1865577522105477 | validation: 3.350902090428156]
	TIME [epoch: 9.75 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190572217830267		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 2.190572217830267 | validation: 3.3615361497966503]
	TIME [epoch: 9.74 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1923230793215454		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 2.1923230793215454 | validation: 3.3501279372944004]
	TIME [epoch: 9.8 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1885632182282486		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 2.1885632182282486 | validation: 3.3429057939364384]
	TIME [epoch: 9.75 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1878445737714807		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 2.1878445737714807 | validation: 3.3651526793005972]
	TIME [epoch: 9.74 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185939588027151		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 2.185939588027151 | validation: 3.334009656843883]
	TIME [epoch: 9.75 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1857301274383483		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 2.1857301274383483 | validation: 3.3427094342736563]
	TIME [epoch: 9.76 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184977513541007		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 2.184977513541007 | validation: 3.3492151782744886]
	TIME [epoch: 9.74 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1836574447178116		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 2.1836574447178116 | validation: 3.3376999118378006]
	TIME [epoch: 9.74 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186534379587228		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 2.186534379587228 | validation: 3.3578737920090953]
	TIME [epoch: 9.75 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1906418834882033		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 2.1906418834882033 | validation: 3.374153438446374]
	TIME [epoch: 9.74 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.194332486648892		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 2.194332486648892 | validation: 3.359853560158224]
	TIME [epoch: 9.74 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1893853584187		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 2.1893853584187 | validation: 3.34656816176441]
	TIME [epoch: 9.76 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1834468908181095		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 2.1834468908181095 | validation: 3.3392872823745314]
	TIME [epoch: 9.74 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1905960622065335		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 2.1905960622065335 | validation: 3.352326275004947]
	TIME [epoch: 9.74 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1954480707495034		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 2.1954480707495034 | validation: 3.3372060375445187]
	TIME [epoch: 9.75 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1896447979116402		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 2.1896447979116402 | validation: 3.3385115651231003]
	TIME [epoch: 9.75 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1868191210202017		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 2.1868191210202017 | validation: 3.3538179736850657]
	TIME [epoch: 9.74 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1874324180040823		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 2.1874324180040823 | validation: 3.3499440232808753]
	TIME [epoch: 9.74 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1870652449632964		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 2.1870652449632964 | validation: 3.3392498747384516]
	TIME [epoch: 9.76 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1887538511406506		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 2.1887538511406506 | validation: 3.353381838668601]
	TIME [epoch: 9.75 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185752319866759		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 2.185752319866759 | validation: 3.343961288599244]
	TIME [epoch: 9.74 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191207822948797		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 2.191207822948797 | validation: 3.3353352441751314]
	TIME [epoch: 9.75 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1835163740346033		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 2.1835163740346033 | validation: 3.354954712378798]
	TIME [epoch: 9.75 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1874603377986572		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 2.1874603377986572 | validation: 3.3459357675612114]
	TIME [epoch: 9.74 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1873231553509895		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 2.1873231553509895 | validation: 3.344877151797199]
	TIME [epoch: 9.75 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820988485612576		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 2.1820988485612576 | validation: 3.3575098443566693]
	TIME [epoch: 9.77 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1871786427840307		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 2.1871786427840307 | validation: 3.352399852217823]
	TIME [epoch: 9.74 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866833526614182		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 2.1866833526614182 | validation: 3.332193326088783]
	TIME [epoch: 9.76 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1854847392766414		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 2.1854847392766414 | validation: 3.3511811326416474]
	TIME [epoch: 9.75 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1868864860560846		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 2.1868864860560846 | validation: 3.3466605117868493]
	TIME [epoch: 9.73 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846662154603247		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 2.1846662154603247 | validation: 3.3556563214930235]
	TIME [epoch: 9.74 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182670215683642		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 2.182670215683642 | validation: 3.3387444067688548]
	TIME [epoch: 9.73 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187254580456601		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 2.187254580456601 | validation: 3.3432367027889773]
	TIME [epoch: 9.76 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185464646037979		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 2.185464646037979 | validation: 3.348807856573535]
	TIME [epoch: 9.74 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182559476991652		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 2.182559476991652 | validation: 3.3485063004110076]
	TIME [epoch: 9.74 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1837078872632856		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 2.1837078872632856 | validation: 3.370705528593838]
	TIME [epoch: 9.74 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1837746030477603		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 2.1837746030477603 | validation: 3.372410275990096]
	TIME [epoch: 9.75 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1904553093431365		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 2.1904553093431365 | validation: 3.3564879671143104]
	TIME [epoch: 9.75 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1880307440551947		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 2.1880307440551947 | validation: 3.3483755044122643]
	TIME [epoch: 9.75 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189420228124164		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 2.189420228124164 | validation: 3.351261046155147]
	TIME [epoch: 9.74 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1864288807505745		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 2.1864288807505745 | validation: 3.357853150876232]
	TIME [epoch: 9.74 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188309691198734		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 2.188309691198734 | validation: 3.3666555213086404]
	TIME [epoch: 9.74 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189550846968536		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 2.189550846968536 | validation: 3.3471994542108745]
	TIME [epoch: 9.76 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182580514987931		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 2.182580514987931 | validation: 3.344316726659083]
	TIME [epoch: 9.74 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1892250810885328		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 2.1892250810885328 | validation: 3.357692269394844]
	TIME [epoch: 9.74 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1822851199447344		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 2.1822851199447344 | validation: 3.33226851839416]
	TIME [epoch: 9.75 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187258588223853		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 2.187258588223853 | validation: 3.328475681194712]
	TIME [epoch: 9.74 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1931873121630234		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 2.1931873121630234 | validation: 3.3425693536918515]
	TIME [epoch: 9.73 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1837055477232314		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 2.1837055477232314 | validation: 3.3754973400259196]
	TIME [epoch: 9.74 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190751189675666		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 2.190751189675666 | validation: 3.3447406029047317]
	TIME [epoch: 9.76 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1800662859231608		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 2.1800662859231608 | validation: 3.33047042440106]
	TIME [epoch: 9.73 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1833552518280834		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 2.1833552518280834 | validation: 3.348544094199829]
	TIME [epoch: 9.73 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846116303190244		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 2.1846116303190244 | validation: 3.3380637180582076]
	TIME [epoch: 9.76 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1806739784306166		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 2.1806739784306166 | validation: 3.351027549758029]
	TIME [epoch: 9.75 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.175996595884303		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 2.175996595884303 | validation: 3.3437105536292377]
	TIME [epoch: 9.75 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186773527301347		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 2.186773527301347 | validation: 3.3504122439922943]
	TIME [epoch: 9.75 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1895803726710854		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 2.1895803726710854 | validation: 3.3405152896199968]
	TIME [epoch: 9.74 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185272761777541		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 2.185272761777541 | validation: 3.3459908514229313]
	TIME [epoch: 9.73 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183194950700565		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 2.183194950700565 | validation: 3.3419886852489946]
	TIME [epoch: 9.74 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187115760569035		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 2.187115760569035 | validation: 3.3438119347479076]
	TIME [epoch: 9.75 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1878576838031285		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 2.1878576838031285 | validation: 3.3511179057671923]
	TIME [epoch: 9.75 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189897787296402		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 2.189897787296402 | validation: 3.3426729562492485]
	TIME [epoch: 9.74 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185309878240843		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 2.185309878240843 | validation: 3.344998997084041]
	TIME [epoch: 9.76 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180325299627184		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 2.180325299627184 | validation: 3.3406495730471217]
	TIME [epoch: 9.74 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186347977055335		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 2.186347977055335 | validation: 3.3567797726243658]
	TIME [epoch: 9.74 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1888722954675437		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 2.1888722954675437 | validation: 3.3652732814680997]
	TIME [epoch: 9.74 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1865593564120274		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 2.1865593564120274 | validation: 3.3442588752439075]
	TIME [epoch: 9.76 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1829203530970753		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 2.1829203530970753 | validation: 3.359913679650765]
	TIME [epoch: 9.74 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1900214393077553		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 2.1900214393077553 | validation: 3.3523163445795574]
	TIME [epoch: 9.74 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184650235571825		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 2.184650235571825 | validation: 3.348885003690026]
	TIME [epoch: 9.75 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18258753727128		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 2.18258753727128 | validation: 3.3468380045820023]
	TIME [epoch: 9.75 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1858488818528774		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 2.1858488818528774 | validation: 3.334878954619593]
	TIME [epoch: 9.74 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18678007497778		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 2.18678007497778 | validation: 3.343693577420253]
	TIME [epoch: 9.76 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1827864665008847		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 2.1827864665008847 | validation: 3.3558055503636366]
	TIME [epoch: 9.75 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1831223088115017		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 2.1831223088115017 | validation: 3.3653507762268333]
	TIME [epoch: 9.75 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1952439827494157		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 2.1952439827494157 | validation: 3.371095119006544]
	TIME [epoch: 9.74 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1915732756512014		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 2.1915732756512014 | validation: 3.3640502436918256]
	TIME [epoch: 9.77 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1893872662762948		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 2.1893872662762948 | validation: 3.351447255635412]
	TIME [epoch: 9.75 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190306045428215		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 2.190306045428215 | validation: 3.3522402016037445]
	TIME [epoch: 9.75 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1867081664533794		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 2.1867081664533794 | validation: 3.336335844513858]
	TIME [epoch: 9.76 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190326696970567		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 2.190326696970567 | validation: 3.346140797472539]
	TIME [epoch: 9.75 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1845602917008007		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 2.1845602917008007 | validation: 3.3500527415835823]
	TIME [epoch: 9.75 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1921993304881036		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 2.1921993304881036 | validation: 3.345036415785956]
	TIME [epoch: 9.76 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1829035971038433		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 2.1829035971038433 | validation: 3.3374736294798404]
	TIME [epoch: 9.75 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1864027009148175		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 2.1864027009148175 | validation: 3.357655602936958]
	TIME [epoch: 9.74 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866437218132857		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 2.1866437218132857 | validation: 3.3473945547345365]
	TIME [epoch: 9.74 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187022070998618		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 2.187022070998618 | validation: 3.3372152559411568]
	TIME [epoch: 9.77 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1875491193559844		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 2.1875491193559844 | validation: 3.348678592146649]
	TIME [epoch: 9.75 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1873151214442514		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 2.1873151214442514 | validation: 3.34373911487377]
	TIME [epoch: 9.75 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1887494014460613		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 2.1887494014460613 | validation: 3.3415419264542736]
	TIME [epoch: 9.76 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187378453034859		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 2.187378453034859 | validation: 3.3404205816180625]
	TIME [epoch: 9.75 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1918450419002484		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 2.1918450419002484 | validation: 3.3418667595494926]
	TIME [epoch: 9.75 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1823418598437505		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 2.1823418598437505 | validation: 3.3386938799237806]
	TIME [epoch: 9.75 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187730142747256		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 2.187730142747256 | validation: 3.368958919704196]
	TIME [epoch: 9.76 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1974866733058356		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 2.1974866733058356 | validation: 3.3550381988326867]
	TIME [epoch: 9.75 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1876966918051517		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 2.1876966918051517 | validation: 3.341293175231289]
	TIME [epoch: 9.74 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187468640306853		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 2.187468640306853 | validation: 3.3345196735671734]
	TIME [epoch: 9.76 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1830936953881923		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 2.1830936953881923 | validation: 3.3465414163840594]
	TIME [epoch: 9.73 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1827740770824127		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 2.1827740770824127 | validation: 3.362135482842957]
	TIME [epoch: 9.75 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181268662009936		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 2.181268662009936 | validation: 3.3512111400978233]
	TIME [epoch: 9.76 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1894158396447274		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 2.1894158396447274 | validation: 3.3571486269882054]
	TIME [epoch: 9.75 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1829890685228697		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 2.1829890685228697 | validation: 3.346496375172135]
	TIME [epoch: 9.74 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183152089922776		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 2.183152089922776 | validation: 3.3477074176877317]
	TIME [epoch: 9.74 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189234883074049		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 2.189234883074049 | validation: 3.3560851198851793]
	TIME [epoch: 9.76 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185663619601348		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 2.185663619601348 | validation: 3.359276589701863]
	TIME [epoch: 9.74 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1891316767130404		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 2.1891316767130404 | validation: 3.3435453269087976]
	TIME [epoch: 9.73 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185846968990588		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 2.185846968990588 | validation: 3.3452395932843406]
	TIME [epoch: 9.76 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1854930602094678		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 2.1854930602094678 | validation: 3.3486647978262516]
	TIME [epoch: 9.75 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1835358574841437		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 2.1835358574841437 | validation: 3.330710982154349]
	TIME [epoch: 9.74 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1882720419496384		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 2.1882720419496384 | validation: 3.341252125141108]
	TIME [epoch: 9.74 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1872543399411097		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 2.1872543399411097 | validation: 3.3384510952611777]
	TIME [epoch: 9.75 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1953160016382265		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 2.1953160016382265 | validation: 3.3497640520205767]
	TIME [epoch: 9.74 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1945383547046737		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 2.1945383547046737 | validation: 3.3514272353933774]
	TIME [epoch: 9.75 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191496492180412		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 2.191496492180412 | validation: 3.338036211475859]
	TIME [epoch: 9.77 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1872290452432557		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 2.1872290452432557 | validation: 3.3448229064338806]
	TIME [epoch: 9.74 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182666313036968		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 2.182666313036968 | validation: 3.346670830228013]
	TIME [epoch: 9.74 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18578730446592		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 2.18578730446592 | validation: 3.3460452986787574]
	TIME [epoch: 9.75 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181190305767042		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 2.181190305767042 | validation: 3.3493421171193676]
	TIME [epoch: 9.74 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1856021800063647		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 2.1856021800063647 | validation: 3.3430661880114725]
	TIME [epoch: 9.73 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180231675343035		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 2.180231675343035 | validation: 3.329341869866587]
	TIME [epoch: 9.74 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185158768705823		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 2.185158768705823 | validation: 3.3454139224954242]
	TIME [epoch: 9.76 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183546652228736		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 2.183546652228736 | validation: 3.34040451340884]
	TIME [epoch: 9.75 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844165360307817		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 2.1844165360307817 | validation: 3.3434269234020326]
	TIME [epoch: 9.74 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179024664841252		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 2.179024664841252 | validation: 3.341370898089729]
	TIME [epoch: 9.76 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180285667052779		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 2.180285667052779 | validation: 3.3492183963596642]
	TIME [epoch: 9.73 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190098624239514		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 2.190098624239514 | validation: 3.338497851912772]
	TIME [epoch: 9.74 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1911371267783273		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 2.1911371267783273 | validation: 3.3433096154857047]
	TIME [epoch: 9.74 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848307909037157		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 2.1848307909037157 | validation: 3.3441512003589806]
	TIME [epoch: 9.76 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189397652287451		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 2.189397652287451 | validation: 3.3416631323587773]
	TIME [epoch: 9.75 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1895362266618412		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 2.1895362266618412 | validation: 3.3418504489648875]
	TIME [epoch: 9.75 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1914356004980875		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 2.1914356004980875 | validation: 3.3352146341654]
	TIME [epoch: 9.76 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189621270497157		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 2.189621270497157 | validation: 3.3283367517393563]
	TIME [epoch: 9.74 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183067042493573		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 2.183067042493573 | validation: 3.3294827676175522]
	TIME [epoch: 9.75 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187443071643083		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 2.187443071643083 | validation: 3.358952430335445]
	TIME [epoch: 9.77 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185010505604364		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 2.185010505604364 | validation: 3.355731375197885]
	TIME [epoch: 9.75 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1885210101556423		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 2.1885210101556423 | validation: 3.357987573965719]
	TIME [epoch: 9.74 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1992622514367275		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 2.1992622514367275 | validation: 3.363123382297928]
	TIME [epoch: 9.74 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189544923687726		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 2.189544923687726 | validation: 3.3464455175746526]
	TIME [epoch: 9.77 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1875566565344124		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 2.1875566565344124 | validation: 3.368228288315503]
	TIME [epoch: 9.75 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1929065134649424		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 2.1929065134649424 | validation: 3.3431989306942147]
	TIME [epoch: 9.75 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846642402096736		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 2.1846642402096736 | validation: 3.343493776665013]
	TIME [epoch: 9.76 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1914426624143113		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 2.1914426624143113 | validation: 3.3257896822038435]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1637.pth
	Model improved!!!
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184224640926951		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 2.184224640926951 | validation: 3.3230710187221604]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study205/model_tr_study205_r0_20240219_205156/states/model_tr_study205_1638.pth
	Model improved!!!
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180664612956945		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 2.180664612956945 | validation: 3.3325581834915425]
	TIME [epoch: 9.75 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182669847009967		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 2.182669847009967 | validation: 3.3328923311137273]
	TIME [epoch: 9.76 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181190536558698		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 2.181190536558698 | validation: 3.339921209527868]
	TIME [epoch: 9.75 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832849499585403		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 2.1832849499585403 | validation: 3.345070749962297]
	TIME [epoch: 9.76 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832672239829005		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 2.1832672239829005 | validation: 3.3459537537007966]
	TIME [epoch: 9.78 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1887098514013674		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 2.1887098514013674 | validation: 3.332499682265291]
	TIME [epoch: 9.75 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187854173149179		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 2.187854173149179 | validation: 3.3495479671123833]
	TIME [epoch: 9.75 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18863360731265		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 2.18863360731265 | validation: 3.336629943105528]
	TIME [epoch: 9.77 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179818637825902		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 2.179818637825902 | validation: 3.3383134074637564]
	TIME [epoch: 9.76 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1839384986837684		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 2.1839384986837684 | validation: 3.341087833242274]
	TIME [epoch: 9.76 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185672524213492		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 2.185672524213492 | validation: 3.337544885521638]
	TIME [epoch: 9.76 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1871381627834405		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 2.1871381627834405 | validation: 3.3401868954419847]
	TIME [epoch: 9.76 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1874106093362564		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 2.1874106093362564 | validation: 3.345798537867271]
	TIME [epoch: 9.75 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180862066252767		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 2.180862066252767 | validation: 3.349628806592789]
	TIME [epoch: 9.75 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1815678585104874		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 2.1815678585104874 | validation: 3.337667174790173]
	TIME [epoch: 9.77 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844019592061765		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 2.1844019592061765 | validation: 3.338643622609244]
	TIME [epoch: 9.76 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180795330363994		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 2.180795330363994 | validation: 3.3435464431092603]
	TIME [epoch: 9.75 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1802879920393976		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 2.1802879920393976 | validation: 3.3497588307657615]
	TIME [epoch: 9.77 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189259322619898		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 2.189259322619898 | validation: 3.3324380417471238]
	TIME [epoch: 9.76 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185580245805768		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 2.185580245805768 | validation: 3.3346416316369063]
	TIME [epoch: 9.75 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179953745553779		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 2.179953745553779 | validation: 3.36167638137613]
	TIME [epoch: 9.75 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1839599226073223		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 2.1839599226073223 | validation: 3.3490527160580656]
	TIME [epoch: 9.77 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188962317167758		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 2.188962317167758 | validation: 3.3583571637332454]
	TIME [epoch: 9.75 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186454139576625		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 2.186454139576625 | validation: 3.3378842766787615]
	TIME [epoch: 9.76 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866786669179112		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 2.1866786669179112 | validation: 3.3427336633412]
	TIME [epoch: 9.77 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18897870246888		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 2.18897870246888 | validation: 3.3431958976870324]
	TIME [epoch: 9.76 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190651214063529		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 2.190651214063529 | validation: 3.352406599883774]
	TIME [epoch: 9.75 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189881350711952		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 2.189881350711952 | validation: 3.3477483465293503]
	TIME [epoch: 9.76 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180819656582329		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 2.180819656582329 | validation: 3.343361285734977]
	TIME [epoch: 9.76 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1853341159099093		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 2.1853341159099093 | validation: 3.3310351059155967]
	TIME [epoch: 9.75 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1778561821843865		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 2.1778561821843865 | validation: 3.352889956710003]
	TIME [epoch: 9.75 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1787996228643127		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 2.1787996228643127 | validation: 3.3456876379053044]
	TIME [epoch: 9.77 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181989235547357		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 2.181989235547357 | validation: 3.339811574128082]
	TIME [epoch: 9.75 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1786799108319554		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 2.1786799108319554 | validation: 3.352664465794671]
	TIME [epoch: 9.75 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1874989712559976		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 2.1874989712559976 | validation: 3.350371999713191]
	TIME [epoch: 9.76 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18654782018691		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 2.18654782018691 | validation: 3.3383462248713283]
	TIME [epoch: 9.76 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181543099257841		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 2.181543099257841 | validation: 3.3491877413926967]
	TIME [epoch: 9.76 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191258260682545		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 2.191258260682545 | validation: 3.3469322989499273]
	TIME [epoch: 9.75 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1829326561779405		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 2.1829326561779405 | validation: 3.3553634845580564]
	TIME [epoch: 9.77 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1847823374879725		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 2.1847823374879725 | validation: 3.3472290647600853]
	TIME [epoch: 9.75 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1833270653861283		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 2.1833270653861283 | validation: 3.340942918677314]
	TIME [epoch: 9.75 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188539590172092		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 2.188539590172092 | validation: 3.3446471177693162]
	TIME [epoch: 9.76 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1847132169510934		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 2.1847132169510934 | validation: 3.347530516495049]
	TIME [epoch: 9.76 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1873209323202323		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 2.1873209323202323 | validation: 3.362558720427744]
	TIME [epoch: 9.75 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183648707338869		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 2.183648707338869 | validation: 3.344494441183083]
	TIME [epoch: 9.76 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185581085280652		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 2.185581085280652 | validation: 3.3450739864456853]
	TIME [epoch: 9.76 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18316152165552		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 2.18316152165552 | validation: 3.348177007145945]
	TIME [epoch: 9.75 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1880684926063885		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 2.1880684926063885 | validation: 3.3445376809046237]
	TIME [epoch: 9.75 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189268697980657		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 2.189268697980657 | validation: 3.3480631208998797]
	TIME [epoch: 9.77 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1808597490960393		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 2.1808597490960393 | validation: 3.35034346035139]
	TIME [epoch: 9.75 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185872242436793		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 2.185872242436793 | validation: 3.348871224929973]
	TIME [epoch: 9.75 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848524317019247		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 2.1848524317019247 | validation: 3.347331041895519]
	TIME [epoch: 9.77 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1828209264119662		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 2.1828209264119662 | validation: 3.336119911209096]
	TIME [epoch: 9.75 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1793840082652407		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 2.1793840082652407 | validation: 3.343467546095599]
	TIME [epoch: 9.75 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187463911426225		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 2.187463911426225 | validation: 3.3558932103513532]
	TIME [epoch: 9.76 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179650739969207		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 2.179650739969207 | validation: 3.344954457411168]
	TIME [epoch: 9.76 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18627492876415		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 2.18627492876415 | validation: 3.3530532302401004]
	TIME [epoch: 9.75 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190081516089182		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 2.190081516089182 | validation: 3.3359575478997865]
	TIME [epoch: 9.75 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1840675116371826		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 2.1840675116371826 | validation: 3.3371730635229233]
	TIME [epoch: 9.78 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1899589820589944		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 2.1899589820589944 | validation: 3.3586462426893173]
	TIME [epoch: 9.75 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182814637691081		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 2.182814637691081 | validation: 3.3378417215754133]
	TIME [epoch: 9.75 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1849251135048666		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 2.1849251135048666 | validation: 3.3449133569214804]
	TIME [epoch: 9.77 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832232801803797		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 2.1832232801803797 | validation: 3.350634256734902]
	TIME [epoch: 9.75 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1887628332955686		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 2.1887628332955686 | validation: 3.3414112923031105]
	TIME [epoch: 9.75 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189887832448579		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 2.189887832448579 | validation: 3.3467454046255307]
	TIME [epoch: 9.75 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1894016176696853		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 2.1894016176696853 | validation: 3.3442521368824902]
	TIME [epoch: 9.77 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181998741778945		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 2.181998741778945 | validation: 3.355323609729486]
	TIME [epoch: 9.76 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1821514214986717		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 2.1821514214986717 | validation: 3.3566422547072934]
	TIME [epoch: 9.75 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182739389475693		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 2.182739389475693 | validation: 3.347663887514483]
	TIME [epoch: 9.77 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1861603132804013		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 2.1861603132804013 | validation: 3.3456918548318186]
	TIME [epoch: 9.76 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182283315253435		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 2.182283315253435 | validation: 3.3451817076169115]
	TIME [epoch: 9.75 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183513910382804		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 2.183513910382804 | validation: 3.3668093347998638]
	TIME [epoch: 9.77 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188518860679832		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 2.188518860679832 | validation: 3.3623286554519485]
	TIME [epoch: 9.76 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180121576869994		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 2.180121576869994 | validation: 3.350910541140174]
	TIME [epoch: 9.75 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183448812798946		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 2.183448812798946 | validation: 3.348960240093029]
	TIME [epoch: 9.76 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1787611064239534		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 2.1787611064239534 | validation: 3.3506236169163794]
	TIME [epoch: 9.77 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189525773776405		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 2.189525773776405 | validation: 3.3483017311661762]
	TIME [epoch: 9.75 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181174410116143		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 2.181174410116143 | validation: 3.3368312412107164]
	TIME [epoch: 9.75 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181795438965741		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 2.181795438965741 | validation: 3.345106890237508]
	TIME [epoch: 9.77 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820054644102713		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 2.1820054644102713 | validation: 3.3669554580902448]
	TIME [epoch: 9.76 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185285456129494		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 2.185285456129494 | validation: 3.347182591616665]
	TIME [epoch: 9.75 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.175813889569296		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 2.175813889569296 | validation: 3.353617359440554]
	TIME [epoch: 9.76 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1792685224288624		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 2.1792685224288624 | validation: 3.3434753953670144]
	TIME [epoch: 9.77 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182859719643662		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 2.182859719643662 | validation: 3.341758127332098]
	TIME [epoch: 9.75 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182758051762618		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 2.182758051762618 | validation: 3.361265574871313]
	TIME [epoch: 9.75 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189725772586736		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 2.189725772586736 | validation: 3.358094362582833]
	TIME [epoch: 9.77 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1885033351341163		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 2.1885033351341163 | validation: 3.3318557297746576]
	TIME [epoch: 9.75 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183939978000807		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 2.183939978000807 | validation: 3.3418260767746917]
	TIME [epoch: 9.75 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1808489174030785		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 2.1808489174030785 | validation: 3.3637124181849174]
	TIME [epoch: 9.76 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846486217569523		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 2.1846486217569523 | validation: 3.33261668220187]
	TIME [epoch: 9.76 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187902446342099		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 2.187902446342099 | validation: 3.3549274510533014]
	TIME [epoch: 9.75 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191946724079723		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 2.191946724079723 | validation: 3.358684018929308]
	TIME [epoch: 9.75 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1882581732397544		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 2.1882581732397544 | validation: 3.3549896412179834]
	TIME [epoch: 9.77 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187522966016596		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 2.187522966016596 | validation: 3.3549427930448004]
	TIME [epoch: 9.75 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185499842222579		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 2.185499842222579 | validation: 3.364599152934453]
	TIME [epoch: 9.75 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1812949419096883		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 2.1812949419096883 | validation: 3.345207404468193]
	TIME [epoch: 9.76 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1835868698250076		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 2.1835868698250076 | validation: 3.340123021207035]
	TIME [epoch: 9.75 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188203656577026		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 2.188203656577026 | validation: 3.338759995421994]
	TIME [epoch: 9.75 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180311142490928		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 2.180311142490928 | validation: 3.345348620979397]
	TIME [epoch: 9.75 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820170717519947		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 2.1820170717519947 | validation: 3.360047277794076]
	TIME [epoch: 9.76 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1897146109658663		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 2.1897146109658663 | validation: 3.356369336194423]
	TIME [epoch: 9.75 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187488017083421		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 2.187488017083421 | validation: 3.3529834031732797]
	TIME [epoch: 9.75 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1891378796489405		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 2.1891378796489405 | validation: 3.3446970117627415]
	TIME [epoch: 9.76 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1834239933917914		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 2.1834239933917914 | validation: 3.3532847206286935]
	TIME [epoch: 9.75 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1855239062049594		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 2.1855239062049594 | validation: 3.362159878981398]
	TIME [epoch: 9.75 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1796259504878863		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 2.1796259504878863 | validation: 3.344829847433051]
	TIME [epoch: 9.76 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1888570646178787		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 2.1888570646178787 | validation: 3.3394656209970037]
	TIME [epoch: 9.75 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1798658093757894		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 2.1798658093757894 | validation: 3.345317200421114]
	TIME [epoch: 9.74 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1862858636072535		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 2.1862858636072535 | validation: 3.349820979214555]
	TIME [epoch: 9.74 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1882396876889474		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 2.1882396876889474 | validation: 3.3440029047632516]
	TIME [epoch: 9.76 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1867014199113903		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 2.1867014199113903 | validation: 3.3478006723223626]
	TIME [epoch: 9.75 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187143565786072		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 2.187143565786072 | validation: 3.346942058594017]
	TIME [epoch: 9.74 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181005151578131		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 2.181005151578131 | validation: 3.3336154183531406]
	TIME [epoch: 9.76 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18119612959018		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 2.18119612959018 | validation: 3.3430239481303508]
	TIME [epoch: 9.74 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1899942583389254		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 2.1899942583389254 | validation: 3.335732846287921]
	TIME [epoch: 9.74 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1871551116370154		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 2.1871551116370154 | validation: 3.3440550142544403]
	TIME [epoch: 9.75 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184658027003103		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 2.184658027003103 | validation: 3.3358893553121223]
	TIME [epoch: 9.75 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184485193802302		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 2.184485193802302 | validation: 3.333076027481094]
	TIME [epoch: 9.75 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1860348999050565		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 2.1860348999050565 | validation: 3.335155113673299]
	TIME [epoch: 9.75 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187223221350855		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 2.187223221350855 | validation: 3.3568707813394214]
	TIME [epoch: 9.77 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186103760453947		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 2.186103760453947 | validation: 3.3490017954097553]
	TIME [epoch: 9.74 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.178026815199984		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 2.178026815199984 | validation: 3.342760728575456]
	TIME [epoch: 9.74 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182400639447591		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 2.182400639447591 | validation: 3.3440857405024405]
	TIME [epoch: 9.76 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832466166615685		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 2.1832466166615685 | validation: 3.3369006688745118]
	TIME [epoch: 9.75 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184968602035518		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 2.184968602035518 | validation: 3.332215550129014]
	TIME [epoch: 9.75 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1912810502566145		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 2.1912810502566145 | validation: 3.347554918292225]
	TIME [epoch: 9.75 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185831348774681		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 2.185831348774681 | validation: 3.3489976056951134]
	TIME [epoch: 9.76 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1899129331847873		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 2.1899129331847873 | validation: 3.342794250023455]
	TIME [epoch: 9.74 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180111993123807		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 2.180111993123807 | validation: 3.352315353051456]
	TIME [epoch: 9.75 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187623123614535		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 2.187623123614535 | validation: 3.3514589122737357]
	TIME [epoch: 9.76 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1828135006031055		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 2.1828135006031055 | validation: 3.3442161458774837]
	TIME [epoch: 9.75 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191201539731958		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 2.191201539731958 | validation: 3.335971830825838]
	TIME [epoch: 9.74 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1831293471318896		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 2.1831293471318896 | validation: 3.3379659377384536]
	TIME [epoch: 9.75 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182817169062315		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 2.182817169062315 | validation: 3.3408036690720238]
	TIME [epoch: 9.76 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182622832437656		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 2.182622832437656 | validation: 3.3460576937726594]
	TIME [epoch: 9.74 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18131252911535		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 2.18131252911535 | validation: 3.348573195301881]
	TIME [epoch: 9.75 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848377384240996		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 2.1848377384240996 | validation: 3.3540341962493616]
	TIME [epoch: 9.76 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189347583203967		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 2.189347583203967 | validation: 3.353381005291475]
	TIME [epoch: 9.75 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1817458909799767		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 2.1817458909799767 | validation: 3.3401861928625967]
	TIME [epoch: 9.74 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184488030416155		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 2.184488030416155 | validation: 3.342037289036806]
	TIME [epoch: 9.76 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1873353798706328		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 2.1873353798706328 | validation: 3.334727264016071]
	TIME [epoch: 9.75 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183023069581753		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 2.183023069581753 | validation: 3.340362791453582]
	TIME [epoch: 9.74 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1864947404002963		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 2.1864947404002963 | validation: 3.3359458380071807]
	TIME [epoch: 9.76 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190039291731954		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 2.190039291731954 | validation: 3.3432505517014035]
	TIME [epoch: 9.76 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1823875009889075		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 2.1823875009889075 | validation: 3.3438617020292276]
	TIME [epoch: 9.75 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18400910300842		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 2.18400910300842 | validation: 3.3469813091638474]
	TIME [epoch: 9.75 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182324978728468		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 2.182324978728468 | validation: 3.3432651493682157]
	TIME [epoch: 9.76 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1880589383865634		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 2.1880589383865634 | validation: 3.3328530993988057]
	TIME [epoch: 9.75 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182831818609304		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 2.182831818609304 | validation: 3.353505129794554]
	TIME [epoch: 9.74 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183753970772442		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 2.183753970772442 | validation: 3.3399590908315986]
	TIME [epoch: 9.76 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1788606085110027		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 2.1788606085110027 | validation: 3.3433348122034636]
	TIME [epoch: 9.75 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182645210786297		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 2.182645210786297 | validation: 3.3394470722585936]
	TIME [epoch: 9.74 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18028576116114		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 2.18028576116114 | validation: 3.3505921667201375]
	TIME [epoch: 9.74 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1822766720026516		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 2.1822766720026516 | validation: 3.323481401188029]
	TIME [epoch: 9.76 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844828281973854		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 2.1844828281973854 | validation: 3.3376665795921565]
	TIME [epoch: 9.74 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185795348072705		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 2.185795348072705 | validation: 3.335235764569968]
	TIME [epoch: 9.74 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1805959808419715		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 2.1805959808419715 | validation: 3.351925114864299]
	TIME [epoch: 9.76 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1805686338684835		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 2.1805686338684835 | validation: 3.352674425226608]
	TIME [epoch: 9.75 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1860651958175685		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 2.1860651958175685 | validation: 3.3475225848011485]
	TIME [epoch: 9.74 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179554789130983		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 2.179554789130983 | validation: 3.336447177737991]
	TIME [epoch: 9.75 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820449743337464		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 2.1820449743337464 | validation: 3.3374444035730337]
	TIME [epoch: 9.76 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1835854000005925		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 2.1835854000005925 | validation: 3.3428794636654504]
	TIME [epoch: 9.74 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1774484312925724		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 2.1774484312925724 | validation: 3.346910157116322]
	TIME [epoch: 9.74 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1884694132048494		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 2.1884694132048494 | validation: 3.3560978410350506]
	TIME [epoch: 9.76 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1833632297130046		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 2.1833632297130046 | validation: 3.344704947962853]
	TIME [epoch: 9.74 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1839278139703078		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 2.1839278139703078 | validation: 3.3428963772097133]
	TIME [epoch: 9.74 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1842253500890765		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 2.1842253500890765 | validation: 3.3418503035332914]
	TIME [epoch: 9.76 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185657282072019		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 2.185657282072019 | validation: 3.3470281005452556]
	TIME [epoch: 9.74 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1806203305506116		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 2.1806203305506116 | validation: 3.359896178411994]
	TIME [epoch: 9.74 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1796570875973265		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 2.1796570875973265 | validation: 3.3528952688354945]
	TIME [epoch: 9.75 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183034630024074		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 2.183034630024074 | validation: 3.3414510372714417]
	TIME [epoch: 9.76 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844461983720564		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 2.1844461983720564 | validation: 3.33989256674608]
	TIME [epoch: 9.75 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179618925297478		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 2.179618925297478 | validation: 3.3510556654894588]
	TIME [epoch: 9.74 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1803781880582838		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 2.1803781880582838 | validation: 3.3385618226219385]
	TIME [epoch: 9.76 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832006253174447		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 2.1832006253174447 | validation: 3.350594762164353]
	TIME [epoch: 9.75 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18634791102403		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 2.18634791102403 | validation: 3.3450717345114938]
	TIME [epoch: 9.74 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182794974296157		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 2.182794974296157 | validation: 3.353906984250514]
	TIME [epoch: 9.75 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1874350772940416		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 2.1874350772940416 | validation: 3.3560061937678336]
	TIME [epoch: 9.75 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179822833651621		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 2.179822833651621 | validation: 3.3522181798711963]
	TIME [epoch: 9.75 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844872828895783		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 2.1844872828895783 | validation: 3.3590084167652434]
	TIME [epoch: 9.74 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1863577730622974		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 2.1863577730622974 | validation: 3.3485620020410773]
	TIME [epoch: 9.76 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1836149954192265		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 2.1836149954192265 | validation: 3.3366372597998812]
	TIME [epoch: 9.74 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1858771502062604		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 2.1858771502062604 | validation: 3.335748378883029]
	TIME [epoch: 9.74 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183836175925638		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 2.183836175925638 | validation: 3.350268681519649]
	TIME [epoch: 9.76 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182765687619626		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 2.182765687619626 | validation: 3.3442593223841626]
	TIME [epoch: 9.75 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1839865307420796		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 2.1839865307420796 | validation: 3.3415464188862813]
	TIME [epoch: 9.74 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179578110427694		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 2.179578110427694 | validation: 3.338441569598945]
	TIME [epoch: 9.75 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179552394526782		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 2.179552394526782 | validation: 3.3458973547430526]
	TIME [epoch: 9.75 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1870769678437405		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 2.1870769678437405 | validation: 3.354642422968855]
	TIME [epoch: 9.75 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1850877330653957		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 2.1850877330653957 | validation: 3.3469367865992576]
	TIME [epoch: 9.74 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186879159620711		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 2.186879159620711 | validation: 3.351616802083929]
	TIME [epoch: 9.76 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1814233175116877		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 2.1814233175116877 | validation: 3.3497403398856056]
	TIME [epoch: 9.74 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1835578873482517		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 2.1835578873482517 | validation: 3.33337214202165]
	TIME [epoch: 9.74 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848384814256088		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 2.1848384814256088 | validation: 3.3390266436998304]
	TIME [epoch: 9.76 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187071905632947		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 2.187071905632947 | validation: 3.3383551450044333]
	TIME [epoch: 9.75 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183696874798333		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 2.183696874798333 | validation: 3.3322895785849274]
	TIME [epoch: 9.74 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187350142975919		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 2.187350142975919 | validation: 3.334941194060084]
	TIME [epoch: 9.75 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185458359583788		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 2.185458359583788 | validation: 3.326744264022707]
	TIME [epoch: 9.76 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.189313994486788		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 2.189313994486788 | validation: 3.337345387332255]
	TIME [epoch: 9.74 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187255185779104		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 2.187255185779104 | validation: 3.3383000109201095]
	TIME [epoch: 9.74 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1782429325365245		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 2.1782429325365245 | validation: 3.3479513518716066]
	TIME [epoch: 9.76 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1813060753347338		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 2.1813060753347338 | validation: 3.338013848876983]
	TIME [epoch: 9.75 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1829784900531464		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 2.1829784900531464 | validation: 3.339970978834922]
	TIME [epoch: 9.75 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183524757332519		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 2.183524757332519 | validation: 3.3329935252068976]
	TIME [epoch: 9.76 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182668018868289		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 2.182668018868289 | validation: 3.342740165089095]
	TIME [epoch: 9.76 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18329876663907		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 2.18329876663907 | validation: 3.347029715921185]
	TIME [epoch: 9.74 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1755601470197283		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 2.1755601470197283 | validation: 3.34174654208579]
	TIME [epoch: 9.75 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184551400145801		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 2.184551400145801 | validation: 3.3465218425682064]
	TIME [epoch: 9.77 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1784403985457925		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 2.1784403985457925 | validation: 3.338100624627108]
	TIME [epoch: 9.74 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18493062853583		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 2.18493062853583 | validation: 3.3537068988405485]
	TIME [epoch: 9.75 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1810627278863346		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 2.1810627278863346 | validation: 3.3448796130814813]
	TIME [epoch: 9.76 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1762923003541284		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 2.1762923003541284 | validation: 3.3468145143865207]
	TIME [epoch: 9.74 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1818949425438006		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 2.1818949425438006 | validation: 3.3497263336455503]
	TIME [epoch: 9.75 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1890907001374207		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 2.1890907001374207 | validation: 3.353314946898036]
	TIME [epoch: 9.75 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1824584154857307		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 2.1824584154857307 | validation: 3.3525424742519365]
	TIME [epoch: 9.76 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1836986549841293		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 2.1836986549841293 | validation: 3.356250456473373]
	TIME [epoch: 9.74 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.17787004906674		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 2.17787004906674 | validation: 3.354086584230338]
	TIME [epoch: 9.74 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1849801977183243		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 2.1849801977183243 | validation: 3.340469619221524]
	TIME [epoch: 9.75 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1753527894158085		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 2.1753527894158085 | validation: 3.340119578385339]
	TIME [epoch: 9.74 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1854381632941666		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 2.1854381632941666 | validation: 3.3606637585622074]
	TIME [epoch: 9.74 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1816846060142545		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 2.1816846060142545 | validation: 3.344887831715588]
	TIME [epoch: 9.76 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182946588821211		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 2.182946588821211 | validation: 3.3597918355159835]
	TIME [epoch: 9.76 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1872565251959712		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 2.1872565251959712 | validation: 3.355797410884916]
	TIME [epoch: 9.75 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182240914779018		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 2.182240914779018 | validation: 3.3517653275938906]
	TIME [epoch: 9.75 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182387724996078		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 2.182387724996078 | validation: 3.352878446228493]
	TIME [epoch: 9.76 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181397546494278		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 2.181397546494278 | validation: 3.3467879242420544]
	TIME [epoch: 9.74 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1825850168864895		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 2.1825850168864895 | validation: 3.358028465914484]
	TIME [epoch: 9.74 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1894454037675315		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 2.1894454037675315 | validation: 3.3577775869280577]
	TIME [epoch: 9.76 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1843957522929927		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 2.1843957522929927 | validation: 3.3510635078800988]
	TIME [epoch: 9.74 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1837220017070296		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 2.1837220017070296 | validation: 3.3472448428348276]
	TIME [epoch: 9.74 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187194999885939		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 2.187194999885939 | validation: 3.341825281466813]
	TIME [epoch: 9.75 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185608747867238		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 2.185608747867238 | validation: 3.340038595366258]
	TIME [epoch: 9.75 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1843120883839466		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 2.1843120883839466 | validation: 3.358122210008186]
	TIME [epoch: 9.74 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1871183540775605		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 2.1871183540775605 | validation: 3.3527356275593836]
	TIME [epoch: 9.75 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1819407113957956		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 2.1819407113957956 | validation: 3.356003122082053]
	TIME [epoch: 9.76 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1816245645180006		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 2.1816245645180006 | validation: 3.344714301957852]
	TIME [epoch: 9.74 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1876549666965945		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 2.1876549666965945 | validation: 3.3553120540103567]
	TIME [epoch: 9.74 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181134788873183		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 2.181134788873183 | validation: 3.354252598687666]
	TIME [epoch: 9.75 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182083388680609		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 2.182083388680609 | validation: 3.348952759396232]
	TIME [epoch: 9.75 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184460426124255		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 2.184460426124255 | validation: 3.343380164963666]
	TIME [epoch: 9.74 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1793866107548068		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 2.1793866107548068 | validation: 3.3379486493302135]
	TIME [epoch: 9.74 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1868311178467272		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 2.1868311178467272 | validation: 3.3314600501834746]
	TIME [epoch: 9.76 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1914132935475665		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 2.1914132935475665 | validation: 3.350081244927047]
	TIME [epoch: 9.72 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1816685830274007		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 2.1816685830274007 | validation: 3.3412455502770975]
	TIME [epoch: 9.73 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1851922071664567		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 2.1851922071664567 | validation: 3.3420750934519736]
	TIME [epoch: 9.75 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185873859138435		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 2.185873859138435 | validation: 3.3281192191018416]
	TIME [epoch: 9.75 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179712050709731		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 2.179712050709731 | validation: 3.330755661968698]
	TIME [epoch: 9.74 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1830211597029745		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 2.1830211597029745 | validation: 3.3351233812782977]
	TIME [epoch: 9.75 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1817145704232113		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 2.1817145704232113 | validation: 3.3362660554732924]
	TIME [epoch: 9.74 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1828405049178508		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 2.1828405049178508 | validation: 3.352266287302581]
	TIME [epoch: 9.73 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1828483871808975		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 2.1828483871808975 | validation: 3.3415397054872114]
	TIME [epoch: 9.74 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1795151454776245		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 2.1795151454776245 | validation: 3.34890767974283]
	TIME [epoch: 9.76 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848848195186177		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 2.1848848195186177 | validation: 3.355127901865539]
	TIME [epoch: 9.75 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185755229852125		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 2.185755229852125 | validation: 3.3514941253307047]
	TIME [epoch: 9.73 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1863455952861144		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 2.1863455952861144 | validation: 3.3390750895338055]
	TIME [epoch: 9.77 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1790061865582375		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 2.1790061865582375 | validation: 3.34438074045804]
	TIME [epoch: 9.75 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1837752037689326		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 2.1837752037689326 | validation: 3.335871992587709]
	TIME [epoch: 9.74 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.179737183066753		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 2.179737183066753 | validation: 3.3389363777527636]
	TIME [epoch: 9.75 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1754338520727563		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 2.1754338520727563 | validation: 3.3456358643776083]
	TIME [epoch: 9.77 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184415385739794		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 2.184415385739794 | validation: 3.3340597670904084]
	TIME [epoch: 9.75 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181061388646519		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 2.181061388646519 | validation: 3.339170742994809]
	TIME [epoch: 9.74 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186509397308324		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 2.186509397308324 | validation: 3.335445043242408]
	TIME [epoch: 9.76 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183796055177878		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 2.183796055177878 | validation: 3.349504410306267]
	TIME [epoch: 9.75 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1816897737278835		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 2.1816897737278835 | validation: 3.332173914398783]
	TIME [epoch: 9.75 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180354707357962		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 2.180354707357962 | validation: 3.339518859257165]
	TIME [epoch: 9.75 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1868905743141847		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 2.1868905743141847 | validation: 3.343471884281704]
	TIME [epoch: 9.75 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191058354878542		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 2.191058354878542 | validation: 3.334482357910912]
	TIME [epoch: 9.75 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1862515366805657		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 2.1862515366805657 | validation: 3.3483766258357437]
	TIME [epoch: 9.74 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1819254735419893		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 2.1819254735419893 | validation: 3.3487925380263426]
	TIME [epoch: 9.75 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1874219978335647		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 2.1874219978335647 | validation: 3.3515554137449635]
	TIME [epoch: 9.74 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184674260043603		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 2.184674260043603 | validation: 3.334018545409764]
	TIME [epoch: 9.74 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1919957845552878		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 2.1919957845552878 | validation: 3.35017130419249]
	TIME [epoch: 9.76 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181227224580989		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 2.181227224580989 | validation: 3.3459138940193975]
	TIME [epoch: 9.75 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1872867127554114		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 2.1872867127554114 | validation: 3.357616588013792]
	TIME [epoch: 9.75 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180824117813112		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 2.180824117813112 | validation: 3.3460823587797486]
	TIME [epoch: 9.75 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.186793757859429		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 2.186793757859429 | validation: 3.3389643257511348]
	TIME [epoch: 9.76 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1834636566232044		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 2.1834636566232044 | validation: 3.3503372348568656]
	TIME [epoch: 9.75 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1838598475453312		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 2.1838598475453312 | validation: 3.351635085237238]
	TIME [epoch: 9.75 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184844090840188		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 2.184844090840188 | validation: 3.3392275104769005]
	TIME [epoch: 9.76 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181510071319523		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 2.181510071319523 | validation: 3.346183742633359]
	TIME [epoch: 9.74 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180670260081246		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 2.180670260081246 | validation: 3.3441684234368387]
	TIME [epoch: 9.75 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1882026315767242		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 2.1882026315767242 | validation: 3.345241753591505]
	TIME [epoch: 9.76 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1847077184543084		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 2.1847077184543084 | validation: 3.3509802902535046]
	TIME [epoch: 9.75 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1845817528735187		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 2.1845817528735187 | validation: 3.3426146534658834]
	TIME [epoch: 9.74 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18029467718248		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 2.18029467718248 | validation: 3.3522876415881844]
	TIME [epoch: 9.74 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1797511165521355		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 2.1797511165521355 | validation: 3.3293712652553227]
	TIME [epoch: 9.76 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1862880316580773		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 2.1862880316580773 | validation: 3.345534165771161]
	TIME [epoch: 9.74 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1846497253236192		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 2.1846497253236192 | validation: 3.350280627384958]
	TIME [epoch: 9.73 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1834657905566157		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 2.1834657905566157 | validation: 3.3263794676698697]
	TIME [epoch: 9.75 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1785936820719867		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 2.1785936820719867 | validation: 3.3650225604419632]
	TIME [epoch: 9.74 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1827100262538353		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 2.1827100262538353 | validation: 3.3497128742539624]
	TIME [epoch: 9.74 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181225573613036		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 2.181225573613036 | validation: 3.348623112820884]
	TIME [epoch: 9.75 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185911469100592		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 2.185911469100592 | validation: 3.3352234347919216]
	TIME [epoch: 9.75 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.175236291984863		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 2.175236291984863 | validation: 3.344766157399163]
	TIME [epoch: 9.74 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180401373561035		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 2.180401373561035 | validation: 3.351232183153142]
	TIME [epoch: 9.74 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182708233479299		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 2.182708233479299 | validation: 3.341664531670201]
	TIME [epoch: 9.76 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1813303670233153		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 2.1813303670233153 | validation: 3.3510130147901087]
	TIME [epoch: 9.74 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1809987411631733		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 2.1809987411631733 | validation: 3.3450583354875643]
	TIME [epoch: 9.74 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1819742029433593		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 2.1819742029433593 | validation: 3.3623797283461556]
	TIME [epoch: 9.76 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190046149375752		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 2.190046149375752 | validation: 3.347751627518901]
	TIME [epoch: 9.74 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820587226101305		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 2.1820587226101305 | validation: 3.3395115873295613]
	TIME [epoch: 9.75 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190312415425736		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 2.190312415425736 | validation: 3.3378900601319215]
	TIME [epoch: 9.74 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188782409898334		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 2.188782409898334 | validation: 3.341547327277084]
	TIME [epoch: 9.77 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1885853176391263		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 2.1885853176391263 | validation: 3.348579013628357]
	TIME [epoch: 9.74 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.178712162581665		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 2.178712162581665 | validation: 3.351921365506647]
	TIME [epoch: 9.75 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182061633047124		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 2.182061633047124 | validation: 3.3322676524725283]
	TIME [epoch: 9.76 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184849170049567		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 2.184849170049567 | validation: 3.335934206105291]
	TIME [epoch: 9.75 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1829157736363713		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 2.1829157736363713 | validation: 3.34155582915496]
	TIME [epoch: 9.74 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1908126271139965		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 2.1908126271139965 | validation: 3.3398581534509426]
	TIME [epoch: 9.76 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181535286535164		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 2.181535286535164 | validation: 3.3496020004803495]
	TIME [epoch: 9.75 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184065734572383		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 2.184065734572383 | validation: 3.339197652769643]
	TIME [epoch: 9.75 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.191427195630084		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 2.191427195630084 | validation: 3.336511953244268]
	TIME [epoch: 9.74 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1802064070392517		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 2.1802064070392517 | validation: 3.344896297306101]
	TIME [epoch: 9.76 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.188293037760979		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 2.188293037760979 | validation: 3.329971203603139]
	TIME [epoch: 9.75 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1850881712204093		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 2.1850881712204093 | validation: 3.3332983096724864]
	TIME [epoch: 9.75 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1855515387937112		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 2.1855515387937112 | validation: 3.34291956714484]
	TIME [epoch: 9.76 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182561437099163		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 2.182561437099163 | validation: 3.3359155314708815]
	TIME [epoch: 9.74 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1826888414964936		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 2.1826888414964936 | validation: 3.3491119664500384]
	TIME [epoch: 9.73 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.182105366374939		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 2.182105366374939 | validation: 3.338399222085059]
	TIME [epoch: 9.74 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183620342989258		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 2.183620342989258 | validation: 3.351299502038147]
	TIME [epoch: 9.75 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820887215308082		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 2.1820887215308082 | validation: 3.3466080370449665]
	TIME [epoch: 9.75 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1837526357229136		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 2.1837526357229136 | validation: 3.349022226141577]
	TIME [epoch: 9.75 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185326737527682		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 2.185326737527682 | validation: 3.337043168204914]
	TIME [epoch: 9.76 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1808344061813556		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 2.1808344061813556 | validation: 3.351818384829408]
	TIME [epoch: 9.75 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181836370596531		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 2.181836370596531 | validation: 3.3278616129361924]
	TIME [epoch: 9.75 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.185463014741268		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 2.185463014741268 | validation: 3.3538371373802254]
	TIME [epoch: 9.76 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.180886848323238		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 2.180886848323238 | validation: 3.3380783314416407]
	TIME [epoch: 9.74 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.190174650427001		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 2.190174650427001 | validation: 3.323651282333175]
	TIME [epoch: 9.75 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1872629878672685		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 2.1872629878672685 | validation: 3.3411130070621176]
	TIME [epoch: 9.75 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1882201913827095		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 2.1882201913827095 | validation: 3.327964732680594]
	TIME [epoch: 9.76 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181172766998371		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 2.181172766998371 | validation: 3.3378901551436932]
	TIME [epoch: 9.73 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183819951953512		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 2.183819951953512 | validation: 3.334601486171055]
	TIME [epoch: 9.75 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1808304250799524		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 2.1808304250799524 | validation: 3.3376036665642803]
	TIME [epoch: 9.75 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184250781843747		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 2.184250781843747 | validation: 3.3278248415146554]
	TIME [epoch: 9.73 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18787053337199		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 2.18787053337199 | validation: 3.3300102387134185]
	TIME [epoch: 9.75 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1772965046358417		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 2.1772965046358417 | validation: 3.3432863008487015]
	TIME [epoch: 9.74 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1856617290472333		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 2.1856617290472333 | validation: 3.3486217211869946]
	TIME [epoch: 9.75 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1820904732941044		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 2.1820904732941044 | validation: 3.343475877284938]
	TIME [epoch: 9.74 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1866208832591156		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 2.1866208832591156 | validation: 3.3475730607973935]
	TIME [epoch: 9.74 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.181349462539887		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 2.181349462539887 | validation: 3.344075437273484]
	TIME [epoch: 9.75 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.18239854636721		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 2.18239854636721 | validation: 3.3408337481977752]
	TIME [epoch: 9.74 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1861291141369055		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 2.1861291141369055 | validation: 3.3331670797800714]
	TIME [epoch: 9.73 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.174804169111082		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 2.174804169111082 | validation: 3.3479455074235136]
	TIME [epoch: 9.76 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1844701070463		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 2.1844701070463 | validation: 3.3434277466864804]
	TIME [epoch: 9.74 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1875805621512345		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 2.1875805621512345 | validation: 3.3438075057144285]
	TIME [epoch: 9.74 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832033547342453		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 2.1832033547342453 | validation: 3.3555240011457563]
	TIME [epoch: 9.75 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.187552242481358		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 2.187552242481358 | validation: 3.3553207176969533]
	TIME [epoch: 9.76 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.183986985024409		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 2.183986985024409 | validation: 3.332126301969076]
	TIME [epoch: 9.73 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184518504331746		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 2.184518504331746 | validation: 3.333322875757955]
	TIME [epoch: 9.73 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1828487895920667		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 2.1828487895920667 | validation: 3.3398975305583685]
	TIME [epoch: 9.75 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.184817523824866		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 2.184817523824866 | validation: 3.3480455942895135]
	TIME [epoch: 9.74 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1869889912310816		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 2.1869889912310816 | validation: 3.3322061121059585]
	TIME [epoch: 9.73 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848295913611944		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 2.1848295913611944 | validation: 3.342129351920251]
	TIME [epoch: 9.75 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1832524426021935		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 2.1832524426021935 | validation: 3.3344381193700317]
	TIME [epoch: 9.75 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1848924897265003		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 2.1848924897265003 | validation: 3.3391874352178164]
	TIME [epoch: 9.74 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1802353963981322		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 2.1802353963981322 | validation: 3.34192135383895]
	TIME [epoch: 9.74 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1852817522933075		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 2.1852817522933075 | validation: 3.3396051681841867]
	TIME [epoch: 9.76 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1840041326347706		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 2.1840041326347706 | validation: 3.341397888063421]
	TIME [epoch: 9.75 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1857844335944825		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 2.1857844335944825 | validation: 3.3404549295311683]
	TIME [epoch: 9.73 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1788458997024223		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 2.1788458997024223 | validation: 3.3380250785941796]
	TIME [epoch: 9.76 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1811437237049227		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 2.1811437237049227 | validation: 3.3369962526316295]
	TIME [epoch: 9.74 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1827301944795474		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 2.1827301944795474 | validation: 3.346119891703411]
	TIME [epoch: 9.75 sec]
Finished training in 19633.569 seconds.
