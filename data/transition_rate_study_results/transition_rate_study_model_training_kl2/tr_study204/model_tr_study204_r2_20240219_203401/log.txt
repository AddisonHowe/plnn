Args:
Namespace(name='model_tr_study204', outdir='out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2', training_data='data/transition_rate_studies/tr_study204/tr_study204_training/r2', validation_data='data/transition_rate_studies/tr_study204/tr_study204_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2986357825

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/10] avg loss: 10.35313792954528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.35313792954528 | validation: 10.315882781892041]
	TIME [epoch: 52.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/10] avg loss: 8.964077654746376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.964077654746376 | validation: 7.970477519250056]
	TIME [epoch: 8.61 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/10] avg loss: 7.3591245590439325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.3591245590439325 | validation: 5.223372570698352]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/10] avg loss: 5.498442517909842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.498442517909842 | validation: 3.7952499631107237]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.940000102611579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.940000102611579 | validation: 3.786447115804533]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.675981569827789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.675981569827789 | validation: 4.111994843212851]
	TIME [epoch: 8.57 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.6293459138666275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6293459138666275 | validation: 3.6502520341419826]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.590494029473605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.590494029473605 | validation: 4.121771908157555]
	TIME [epoch: 8.54 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.584064365668892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.584064365668892 | validation: 3.5696006524853363]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.546129362926176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.546129362926176 | validation: 3.838263315136422]
	TIME [epoch: 8.56 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.5042256824152815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5042256824152815 | validation: 3.8561620838202244]
	TIME [epoch: 8.55 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.464040437298182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.464040437298182 | validation: 3.766434211049675]
	TIME [epoch: 8.55 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4152678230995095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4152678230995095 | validation: 4.02093528825559]
	TIME [epoch: 8.56 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.402703137682362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.402703137682362 | validation: 3.318643689272342]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.437055005194617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.437055005194617 | validation: 3.373071192930257]
	TIME [epoch: 8.54 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.282193809017961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.282193809017961 | validation: 3.4778636070065043]
	TIME [epoch: 8.56 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.310278482847513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.310278482847513 | validation: 3.448963685861587]
	TIME [epoch: 8.55 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.363624540300016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.363624540300016 | validation: 4.408578590393175]
	TIME [epoch: 8.56 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.320177059743149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.320177059743149 | validation: 3.5373803094101075]
	TIME [epoch: 8.54 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.22783280178595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.22783280178595 | validation: 4.4890416400998125]
	TIME [epoch: 8.56 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.29395220102281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.29395220102281 | validation: 3.7341558619655997]
	TIME [epoch: 8.54 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.224741402798983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.224741402798983 | validation: 3.755104708905153]
	TIME [epoch: 8.55 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.2232791104190825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2232791104190825 | validation: 3.4185502554625953]
	TIME [epoch: 8.54 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.192894284407318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.192894284407318 | validation: 3.6970670492711015]
	TIME [epoch: 8.56 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.222357739022572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.222357739022572 | validation: 3.432269495583072]
	TIME [epoch: 8.53 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1572243867108645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1572243867108645 | validation: 3.2380802459758566]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.228928578718972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.228928578718972 | validation: 3.233411560708158]
	TIME [epoch: 8.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1791866815150644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1791866815150644 | validation: 3.3004110582050026]
	TIME [epoch: 8.54 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.4045935875306075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4045935875306075 | validation: 3.229295002671611]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.025000709165849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025000709165849 | validation: 3.4774907613238035]
	TIME [epoch: 8.57 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.08488084783954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.08488084783954 | validation: 4.5258411315424905]
	TIME [epoch: 8.56 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.254659255085697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.254659255085697 | validation: 3.6622803711272907]
	TIME [epoch: 8.56 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.482587015231457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.482587015231457 | validation: 3.829019367788602]
	TIME [epoch: 8.53 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.143845292590964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.143845292590964 | validation: 3.5542936197040103]
	TIME [epoch: 8.57 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.988296833050365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.988296833050365 | validation: 2.9880412030698884]
	TIME [epoch: 8.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9560970988737454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9560970988737454 | validation: 3.0156525016505547]
	TIME [epoch: 8.55 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.129672952621111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.129672952621111 | validation: 3.1300917965733124]
	TIME [epoch: 8.54 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.973360540007777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.973360540007777 | validation: 2.9195310541012165]
	TIME [epoch: 8.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.144283408918299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144283408918299 | validation: 3.675594954386672]
	TIME [epoch: 8.57 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.034271291814587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.034271291814587 | validation: 2.9590771053160534]
	TIME [epoch: 8.54 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.887318062369956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.887318062369956 | validation: 3.4641598503189046]
	TIME [epoch: 8.54 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1147954839657945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1147954839657945 | validation: 3.3005161833895595]
	TIME [epoch: 8.58 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8794875934322284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8794875934322284 | validation: 3.744297356451182]
	TIME [epoch: 8.55 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9781536198908336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9781536198908336 | validation: 2.9709316070094096]
	TIME [epoch: 8.58 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9348019592770584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9348019592770584 | validation: 3.0589196238051075]
	TIME [epoch: 8.55 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.923302234184175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.923302234184175 | validation: 3.0165586094498895]
	TIME [epoch: 8.58 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.036553483922028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.036553483922028 | validation: 2.927835837463151]
	TIME [epoch: 8.54 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9637885436428197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9637885436428197 | validation: 3.0655361453160586]
	TIME [epoch: 8.55 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9115779012858676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9115779012858676 | validation: 2.873299188797793]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9178703942046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9178703942046 | validation: 3.5321347146525097]
	TIME [epoch: 8.58 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9534022777471556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9534022777471556 | validation: 3.102555101327006]
	TIME [epoch: 8.54 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9329932167627284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9329932167627284 | validation: 3.084994739210023]
	TIME [epoch: 8.54 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8874760874845746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8874760874845746 | validation: 2.891813114907785]
	TIME [epoch: 8.55 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.999059519387655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.999059519387655 | validation: 3.322917977166033]
	TIME [epoch: 8.57 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9550335478801415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9550335478801415 | validation: 3.0614760003171964]
	TIME [epoch: 8.54 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.038643345171031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.038643345171031 | validation: 3.0809337541598696]
	TIME [epoch: 8.54 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.858964343600987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.858964343600987 | validation: 2.852983594622984]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.892244463833012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.892244463833012 | validation: 2.862034933463959]
	TIME [epoch: 8.58 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.473099320901883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.473099320901883 | validation: 2.894624242938338]
	TIME [epoch: 8.55 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8795990271031364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8795990271031364 | validation: 2.98056019411079]
	TIME [epoch: 8.55 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8044981467124144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8044981467124144 | validation: 2.8209494145921545]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8674034224489873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8674034224489873 | validation: 3.3480825009305692]
	TIME [epoch: 8.6 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.952121945980256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.952121945980256 | validation: 3.49071895721032]
	TIME [epoch: 8.54 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.203049805813761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.203049805813761 | validation: 2.939271645236401]
	TIME [epoch: 8.54 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7811950274410777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7811950274410777 | validation: 2.8532298322074094]
	TIME [epoch: 8.56 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7155647489264325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7155647489264325 | validation: 3.1695085785687622]
	TIME [epoch: 8.56 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.8137939569914545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8137939569914545 | validation: 3.2010390708248866]
	TIME [epoch: 8.55 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.9227180661216594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9227180661216594 | validation: 2.903093705683761]
	TIME [epoch: 8.55 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.771966952315973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.771966952315973 | validation: 2.803940260603849]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.724878792670043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.724878792670043 | validation: 2.7426886293799364]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7785692893303997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7785692893303997 | validation: 2.842436306476081]
	TIME [epoch: 8.55 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.807473466503396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.807473466503396 | validation: 2.83948656687393]
	TIME [epoch: 8.56 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7588938437627704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7588938437627704 | validation: 3.4295218418945996]
	TIME [epoch: 8.55 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.850855152444683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.850855152444683 | validation: 2.988340821807883]
	TIME [epoch: 8.56 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.748504505211307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.748504505211307 | validation: 2.9313330629140357]
	TIME [epoch: 8.54 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.846752282590541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.846752282590541 | validation: 2.8113926401175107]
	TIME [epoch: 8.56 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.810453664629601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.810453664629601 | validation: 2.8600217103051677]
	TIME [epoch: 8.54 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.708524983799631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.708524983799631 | validation: 2.7831620333811355]
	TIME [epoch: 8.56 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7212412180430534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7212412180430534 | validation: 2.997434760893474]
	TIME [epoch: 8.56 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.717395638461297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.717395638461297 | validation: 2.9030392454356297]
	TIME [epoch: 8.57 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.780226429032325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.780226429032325 | validation: 3.1296943791053926]
	TIME [epoch: 8.54 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.720840135985747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.720840135985747 | validation: 3.076320169188347]
	TIME [epoch: 8.56 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.778782635788221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.778782635788221 | validation: 2.95050948224094]
	TIME [epoch: 8.57 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.741159157331615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.741159157331615 | validation: 4.730950476160445]
	TIME [epoch: 8.56 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.1212350917954135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1212350917954135 | validation: 3.172581526553582]
	TIME [epoch: 8.54 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7870248509402145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7870248509402145 | validation: 2.8043365074893667]
	TIME [epoch: 8.54 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.776093500363001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.776093500363001 | validation: 2.919186092959945]
	TIME [epoch: 8.58 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7041431146756603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7041431146756603 | validation: 2.768678115540289]
	TIME [epoch: 8.55 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.881032875258756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.881032875258756 | validation: 3.013939319555152]
	TIME [epoch: 8.54 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7143327968117417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7143327968117417 | validation: 2.8700190728788604]
	TIME [epoch: 8.54 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7351642140476384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7351642140476384 | validation: 3.620779156423778]
	TIME [epoch: 8.58 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.848008933117223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.848008933117223 | validation: 2.9787515248725636]
	TIME [epoch: 8.55 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7099476434338436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7099476434338436 | validation: 2.9578793509588066]
	TIME [epoch: 8.54 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.799709790116556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.799709790116556 | validation: 2.8306499751894334]
	TIME [epoch: 8.54 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.636488387971751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.636488387971751 | validation: 2.79316956184452]
	TIME [epoch: 8.58 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6802249092790476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6802249092790476 | validation: 2.824096774767378]
	TIME [epoch: 8.54 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.643396669761396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.643396669761396 | validation: 3.1156503655716796]
	TIME [epoch: 8.54 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7258832223215697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7258832223215697 | validation: 2.9627858368876003]
	TIME [epoch: 8.54 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.667052385378907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.667052385378907 | validation: 2.870087540632607]
	TIME [epoch: 8.57 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.743641590020342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.743641590020342 | validation: 3.5520713213194526]
	TIME [epoch: 8.54 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.768428418734421		[learning rate: 0.0099673]
	Learning Rate: 0.00996733
	LOSS [training: 3.768428418734421 | validation: 2.803512548430281]
	TIME [epoch: 8.53 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/10] avg loss: 4.083089191269848		[learning rate: 0.0099312]
	Learning Rate: 0.00993116
	LOSS [training: 4.083089191269848 | validation: 3.0773460078533]
	TIME [epoch: 8.55 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6367116355501112		[learning rate: 0.0098951]
	Learning Rate: 0.00989512
	LOSS [training: 3.6367116355501112 | validation: 2.7427442608838204]
	TIME [epoch: 8.56 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.716418390513612		[learning rate: 0.0098592]
	Learning Rate: 0.00985921
	LOSS [training: 3.716418390513612 | validation: 3.0288936666856254]
	TIME [epoch: 8.54 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.702774105797239		[learning rate: 0.0098234]
	Learning Rate: 0.00982343
	LOSS [training: 3.702774105797239 | validation: 2.7982225435460877]
	TIME [epoch: 8.54 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.633723248912927		[learning rate: 0.0097878]
	Learning Rate: 0.00978778
	LOSS [training: 3.633723248912927 | validation: 2.739546976148516]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6037857280046395		[learning rate: 0.0097523]
	Learning Rate: 0.00975226
	LOSS [training: 3.6037857280046395 | validation: 3.0275845020298764]
	TIME [epoch: 8.57 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6498359943571415		[learning rate: 0.0097169]
	Learning Rate: 0.00971687
	LOSS [training: 3.6498359943571415 | validation: 2.712124405436098]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7475177577927603		[learning rate: 0.0096816]
	Learning Rate: 0.0096816
	LOSS [training: 3.7475177577927603 | validation: 2.7906151687183547]
	TIME [epoch: 8.53 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.650778485138298		[learning rate: 0.0096465]
	Learning Rate: 0.00964647
	LOSS [training: 3.650778485138298 | validation: 2.934584866485742]
	TIME [epoch: 8.55 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6842315702486546		[learning rate: 0.0096115]
	Learning Rate: 0.00961146
	LOSS [training: 3.6842315702486546 | validation: 2.864135512406623]
	TIME [epoch: 8.56 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.714984466431404		[learning rate: 0.0095766]
	Learning Rate: 0.00957658
	LOSS [training: 3.714984466431404 | validation: 3.0611033322463683]
	TIME [epoch: 8.54 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7394728976262805		[learning rate: 0.0095418]
	Learning Rate: 0.00954183
	LOSS [training: 3.7394728976262805 | validation: 2.74567934677299]
	TIME [epoch: 8.53 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.787032975101277		[learning rate: 0.0095072]
	Learning Rate: 0.0095072
	LOSS [training: 3.787032975101277 | validation: 2.971843312605385]
	TIME [epoch: 8.56 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.696584009613666		[learning rate: 0.0094727]
	Learning Rate: 0.0094727
	LOSS [training: 3.696584009613666 | validation: 2.9697562571269183]
	TIME [epoch: 8.56 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.774108540816355		[learning rate: 0.0094383]
	Learning Rate: 0.00943832
	LOSS [training: 3.774108540816355 | validation: 3.346665994316709]
	TIME [epoch: 8.54 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.650533947183244		[learning rate: 0.0094041]
	Learning Rate: 0.00940407
	LOSS [training: 3.650533947183244 | validation: 2.822724467287764]
	TIME [epoch: 8.54 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.721577879954386		[learning rate: 0.0093699]
	Learning Rate: 0.00936994
	LOSS [training: 3.721577879954386 | validation: 3.1836869711605797]
	TIME [epoch: 8.55 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.640679938803731		[learning rate: 0.0093359]
	Learning Rate: 0.00933594
	LOSS [training: 3.640679938803731 | validation: 2.784285414541176]
	TIME [epoch: 8.56 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6091579543650547		[learning rate: 0.0093021]
	Learning Rate: 0.00930206
	LOSS [training: 3.6091579543650547 | validation: 3.0353535019165934]
	TIME [epoch: 8.54 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7475205816188755		[learning rate: 0.0092683]
	Learning Rate: 0.0092683
	LOSS [training: 3.7475205816188755 | validation: 2.7661885953986127]
	TIME [epoch: 8.56 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6000647611384693		[learning rate: 0.0092347]
	Learning Rate: 0.00923466
	LOSS [training: 3.6000647611384693 | validation: 2.852273837522947]
	TIME [epoch: 8.55 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.590799404056534		[learning rate: 0.0092011]
	Learning Rate: 0.00920115
	LOSS [training: 3.590799404056534 | validation: 2.8331094187834407]
	TIME [epoch: 8.56 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.7099360063591442		[learning rate: 0.0091678]
	Learning Rate: 0.00916776
	LOSS [training: 3.7099360063591442 | validation: 3.601587570712954]
	TIME [epoch: 8.53 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.735896282086962		[learning rate: 0.0091345]
	Learning Rate: 0.00913449
	LOSS [training: 3.735896282086962 | validation: 2.908615975669833]
	TIME [epoch: 8.56 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6345867386435073		[learning rate: 0.0091013]
	Learning Rate: 0.00910134
	LOSS [training: 3.6345867386435073 | validation: 3.2070373179318596]
	TIME [epoch: 8.54 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.696656225199233		[learning rate: 0.0090683]
	Learning Rate: 0.00906831
	LOSS [training: 3.696656225199233 | validation: 2.7970838132538898]
	TIME [epoch: 8.56 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.605037615093571		[learning rate: 0.0090354]
	Learning Rate: 0.0090354
	LOSS [training: 3.605037615093571 | validation: 3.1865212221503123]
	TIME [epoch: 8.54 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.690963160002494		[learning rate: 0.0090026]
	Learning Rate: 0.00900261
	LOSS [training: 3.690963160002494 | validation: 2.9169015324431817]
	TIME [epoch: 8.56 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6227288809302456		[learning rate: 0.0089699]
	Learning Rate: 0.00896994
	LOSS [training: 3.6227288809302456 | validation: 3.0665008612268334]
	TIME [epoch: 8.54 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.581691827882975		[learning rate: 0.0089374]
	Learning Rate: 0.00893739
	LOSS [training: 3.581691827882975 | validation: 2.7581955374499554]
	TIME [epoch: 8.56 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5762224768604396		[learning rate: 0.008905]
	Learning Rate: 0.00890495
	LOSS [training: 3.5762224768604396 | validation: 2.8770776424462747]
	TIME [epoch: 8.55 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.579773007243054		[learning rate: 0.0088726]
	Learning Rate: 0.00887263
	LOSS [training: 3.579773007243054 | validation: 2.9780135715298495]
	TIME [epoch: 8.55 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.609448940069062		[learning rate: 0.0088404]
	Learning Rate: 0.00884044
	LOSS [training: 3.609448940069062 | validation: 3.001082506889873]
	TIME [epoch: 8.54 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6320913716858554		[learning rate: 0.0088084]
	Learning Rate: 0.00880835
	LOSS [training: 3.6320913716858554 | validation: 2.8366653255053143]
	TIME [epoch: 8.55 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.576380123481022		[learning rate: 0.0087764]
	Learning Rate: 0.00877639
	LOSS [training: 3.576380123481022 | validation: 3.110837034520327]
	TIME [epoch: 8.56 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6264539982758572		[learning rate: 0.0087445]
	Learning Rate: 0.00874454
	LOSS [training: 3.6264539982758572 | validation: 2.842564665345197]
	TIME [epoch: 8.55 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6012185562900987		[learning rate: 0.0087128]
	Learning Rate: 0.0087128
	LOSS [training: 3.6012185562900987 | validation: 2.880026942485979]
	TIME [epoch: 8.54 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6387763886620084		[learning rate: 0.0086812]
	Learning Rate: 0.00868118
	LOSS [training: 3.6387763886620084 | validation: 2.7535531787647485]
	TIME [epoch: 8.55 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6548224752164287		[learning rate: 0.0086497]
	Learning Rate: 0.00864968
	LOSS [training: 3.6548224752164287 | validation: 2.8505846868893197]
	TIME [epoch: 8.56 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.540989501828348		[learning rate: 0.0086183]
	Learning Rate: 0.00861829
	LOSS [training: 3.540989501828348 | validation: 2.7659351100021983]
	TIME [epoch: 8.55 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.57825596426365		[learning rate: 0.008587]
	Learning Rate: 0.00858701
	LOSS [training: 3.57825596426365 | validation: 2.7819923195762692]
	TIME [epoch: 8.54 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5710934670316106		[learning rate: 0.0085559]
	Learning Rate: 0.00855585
	LOSS [training: 3.5710934670316106 | validation: 2.6815314866623745]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5908920707149496		[learning rate: 0.0085248]
	Learning Rate: 0.0085248
	LOSS [training: 3.5908920707149496 | validation: 2.7467579221603087]
	TIME [epoch: 8.58 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.63081894631148		[learning rate: 0.0084939]
	Learning Rate: 0.00849386
	LOSS [training: 3.63081894631148 | validation: 3.1039758738714713]
	TIME [epoch: 8.53 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6285995459043123		[learning rate: 0.008463]
	Learning Rate: 0.00846304
	LOSS [training: 3.6285995459043123 | validation: 2.7129535861402503]
	TIME [epoch: 8.54 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.692570102989611		[learning rate: 0.0084323]
	Learning Rate: 0.00843233
	LOSS [training: 3.692570102989611 | validation: 2.7553745612524985]
	TIME [epoch: 8.54 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6377996632118013		[learning rate: 0.0084017]
	Learning Rate: 0.00840172
	LOSS [training: 3.6377996632118013 | validation: 3.363658088859463]
	TIME [epoch: 8.58 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.654161131623728		[learning rate: 0.0083712]
	Learning Rate: 0.00837123
	LOSS [training: 3.654161131623728 | validation: 2.7233602128740126]
	TIME [epoch: 8.54 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.582722798527512		[learning rate: 0.0083409]
	Learning Rate: 0.00834085
	LOSS [training: 3.582722798527512 | validation: 2.958913894943687]
	TIME [epoch: 8.53 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6177695868921176		[learning rate: 0.0083106]
	Learning Rate: 0.00831059
	LOSS [training: 3.6177695868921176 | validation: 2.6942868289841204]
	TIME [epoch: 8.53 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.610760307016237		[learning rate: 0.0082804]
	Learning Rate: 0.00828043
	LOSS [training: 3.610760307016237 | validation: 3.0726327665533724]
	TIME [epoch: 8.58 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.571657867087829		[learning rate: 0.0082504]
	Learning Rate: 0.00825037
	LOSS [training: 3.571657867087829 | validation: 2.7239276968983273]
	TIME [epoch: 8.53 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.559423047130339		[learning rate: 0.0082204]
	Learning Rate: 0.00822043
	LOSS [training: 3.559423047130339 | validation: 3.1986400256377436]
	TIME [epoch: 8.52 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6102213576084212		[learning rate: 0.0081906]
	Learning Rate: 0.0081906
	LOSS [training: 3.6102213576084212 | validation: 3.0478369036027524]
	TIME [epoch: 8.54 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5903541173058864		[learning rate: 0.0081609]
	Learning Rate: 0.00816088
	LOSS [training: 3.5903541173058864 | validation: 2.762504387918638]
	TIME [epoch: 8.57 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5554965648554337		[learning rate: 0.0081313]
	Learning Rate: 0.00813126
	LOSS [training: 3.5554965648554337 | validation: 2.948438462400366]
	TIME [epoch: 8.54 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.757825471832949		[learning rate: 0.0081018]
	Learning Rate: 0.00810175
	LOSS [training: 3.757825471832949 | validation: 3.5690904430394963]
	TIME [epoch: 8.53 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6219371387436405		[learning rate: 0.0080724]
	Learning Rate: 0.00807235
	LOSS [training: 3.6219371387436405 | validation: 2.6967882832576993]
	TIME [epoch: 8.55 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5958528018238156		[learning rate: 0.0080431]
	Learning Rate: 0.00804305
	LOSS [training: 3.5958528018238156 | validation: 2.714362838650742]
	TIME [epoch: 8.57 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5673093750276834		[learning rate: 0.0080139]
	Learning Rate: 0.00801387
	LOSS [training: 3.5673093750276834 | validation: 2.729927243963469]
	TIME [epoch: 8.53 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.560672108389113		[learning rate: 0.0079848]
	Learning Rate: 0.00798478
	LOSS [training: 3.560672108389113 | validation: 2.969382683181097]
	TIME [epoch: 8.54 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.540997217208914		[learning rate: 0.0079558]
	Learning Rate: 0.00795581
	LOSS [training: 3.540997217208914 | validation: 2.8588184489348314]
	TIME [epoch: 8.55 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5069716609349486		[learning rate: 0.0079269]
	Learning Rate: 0.00792693
	LOSS [training: 3.5069716609349486 | validation: 2.8163033571708143]
	TIME [epoch: 8.56 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.500142313608902		[learning rate: 0.0078982]
	Learning Rate: 0.00789817
	LOSS [training: 3.500142313608902 | validation: 2.708386819106385]
	TIME [epoch: 8.53 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5762327104574005		[learning rate: 0.0078695]
	Learning Rate: 0.0078695
	LOSS [training: 3.5762327104574005 | validation: 2.881696965347772]
	TIME [epoch: 8.53 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5071314265147953		[learning rate: 0.0078409]
	Learning Rate: 0.00784094
	LOSS [training: 3.5071314265147953 | validation: 2.812998700262871]
	TIME [epoch: 8.55 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.522996581838364		[learning rate: 0.0078125]
	Learning Rate: 0.00781249
	LOSS [training: 3.522996581838364 | validation: 3.5582541136350025]
	TIME [epoch: 8.57 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6341335840288833		[learning rate: 0.0077841]
	Learning Rate: 0.00778414
	LOSS [training: 3.6341335840288833 | validation: 2.7336478698215947]
	TIME [epoch: 8.53 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.51250860580161		[learning rate: 0.0077559]
	Learning Rate: 0.00775589
	LOSS [training: 3.51250860580161 | validation: 2.997091512548113]
	TIME [epoch: 8.53 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5648262661458765		[learning rate: 0.0077277]
	Learning Rate: 0.00772774
	LOSS [training: 3.5648262661458765 | validation: 3.2169961611020206]
	TIME [epoch: 8.54 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6047855749815225		[learning rate: 0.0076997]
	Learning Rate: 0.0076997
	LOSS [training: 3.6047855749815225 | validation: 3.094747430027093]
	TIME [epoch: 8.56 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6454133261953094		[learning rate: 0.0076718]
	Learning Rate: 0.00767176
	LOSS [training: 3.6454133261953094 | validation: 2.74923539151505]
	TIME [epoch: 8.54 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5219999933476607		[learning rate: 0.0076439]
	Learning Rate: 0.00764391
	LOSS [training: 3.5219999933476607 | validation: 3.581727371728779]
	TIME [epoch: 8.52 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6581641171313812		[learning rate: 0.0076162]
	Learning Rate: 0.00761617
	LOSS [training: 3.6581641171313812 | validation: 2.8355514812358766]
	TIME [epoch: 8.55 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6330967454028724		[learning rate: 0.0075885]
	Learning Rate: 0.00758853
	LOSS [training: 3.6330967454028724 | validation: 3.360543462832725]
	TIME [epoch: 8.55 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.560849292587585		[learning rate: 0.007561]
	Learning Rate: 0.00756099
	LOSS [training: 3.560849292587585 | validation: 2.7085572929277104]
	TIME [epoch: 8.53 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5154496152674035		[learning rate: 0.0075336]
	Learning Rate: 0.00753356
	LOSS [training: 3.5154496152674035 | validation: 2.694107103525997]
	TIME [epoch: 8.53 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.52921416888564		[learning rate: 0.0075062]
	Learning Rate: 0.00750622
	LOSS [training: 3.52921416888564 | validation: 2.9530019370162206]
	TIME [epoch: 8.55 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6181699120657393		[learning rate: 0.007479]
	Learning Rate: 0.00747897
	LOSS [training: 3.6181699120657393 | validation: 2.931664912438567]
	TIME [epoch: 8.56 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.554800200355358		[learning rate: 0.0074518]
	Learning Rate: 0.00745183
	LOSS [training: 3.554800200355358 | validation: 2.819471504078038]
	TIME [epoch: 8.54 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.573827065121266		[learning rate: 0.0074248]
	Learning Rate: 0.00742479
	LOSS [training: 3.573827065121266 | validation: 2.7259843982995147]
	TIME [epoch: 8.53 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.566215405872446		[learning rate: 0.0073978]
	Learning Rate: 0.00739784
	LOSS [training: 3.566215405872446 | validation: 3.0340768068350537]
	TIME [epoch: 8.56 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5559054029459007		[learning rate: 0.007371]
	Learning Rate: 0.007371
	LOSS [training: 3.5559054029459007 | validation: 2.777596267309942]
	TIME [epoch: 8.54 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4991951434122215		[learning rate: 0.0073442]
	Learning Rate: 0.00734425
	LOSS [training: 3.4991951434122215 | validation: 2.6586142588889947]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.588106133934199		[learning rate: 0.0073176]
	Learning Rate: 0.0073176
	LOSS [training: 3.588106133934199 | validation: 2.836296441708356]
	TIME [epoch: 8.54 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5857135563299307		[learning rate: 0.007291]
	Learning Rate: 0.00729104
	LOSS [training: 3.5857135563299307 | validation: 2.696616993026291]
	TIME [epoch: 8.54 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.523938964310357		[learning rate: 0.0072646]
	Learning Rate: 0.00726458
	LOSS [training: 3.523938964310357 | validation: 2.7359211421674594]
	TIME [epoch: 8.55 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.560713642105598		[learning rate: 0.0072382]
	Learning Rate: 0.00723822
	LOSS [training: 3.560713642105598 | validation: 2.872030712110457]
	TIME [epoch: 8.55 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5608332534446774		[learning rate: 0.0072119]
	Learning Rate: 0.00721195
	LOSS [training: 3.5608332534446774 | validation: 2.652321173838219]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.528627909381297		[learning rate: 0.0071858]
	Learning Rate: 0.00718578
	LOSS [training: 3.528627909381297 | validation: 2.9701117978012674]
	TIME [epoch: 8.55 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.581333160449193		[learning rate: 0.0071597]
	Learning Rate: 0.0071597
	LOSS [training: 3.581333160449193 | validation: 2.8533784022074355]
	TIME [epoch: 8.55 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4781514721386193		[learning rate: 0.0071337]
	Learning Rate: 0.00713371
	LOSS [training: 3.4781514721386193 | validation: 2.6583268550496197]
	TIME [epoch: 8.55 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5108716590513387		[learning rate: 0.0071078]
	Learning Rate: 0.00710783
	LOSS [training: 3.5108716590513387 | validation: 2.8824129301348775]
	TIME [epoch: 8.55 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.544118526986531		[learning rate: 0.007082]
	Learning Rate: 0.00708203
	LOSS [training: 3.544118526986531 | validation: 2.6655918155517133]
	TIME [epoch: 8.55 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4959669103770836		[learning rate: 0.0070563]
	Learning Rate: 0.00705633
	LOSS [training: 3.4959669103770836 | validation: 2.693697472062701]
	TIME [epoch: 8.54 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.544055301362671		[learning rate: 0.0070307]
	Learning Rate: 0.00703072
	LOSS [training: 3.544055301362671 | validation: 3.0500468481447087]
	TIME [epoch: 8.55 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.56585918010732		[learning rate: 0.0070052]
	Learning Rate: 0.00700521
	LOSS [training: 3.56585918010732 | validation: 2.6941956399531612]
	TIME [epoch: 8.57 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5925191457672483		[learning rate: 0.0069798]
	Learning Rate: 0.00697979
	LOSS [training: 3.5925191457672483 | validation: 2.7835917430936647]
	TIME [epoch: 8.54 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5367674622551606		[learning rate: 0.0069545]
	Learning Rate: 0.00695446
	LOSS [training: 3.5367674622551606 | validation: 2.70791197494372]
	TIME [epoch: 8.53 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.571083127994225		[learning rate: 0.0069292]
	Learning Rate: 0.00692922
	LOSS [training: 3.571083127994225 | validation: 2.931515869423561]
	TIME [epoch: 8.56 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.538046773656993		[learning rate: 0.0069041]
	Learning Rate: 0.00690407
	LOSS [training: 3.538046773656993 | validation: 2.8113704626291303]
	TIME [epoch: 8.56 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.490947332692908		[learning rate: 0.006879]
	Learning Rate: 0.00687902
	LOSS [training: 3.490947332692908 | validation: 2.66248074859333]
	TIME [epoch: 8.54 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5281367123617016		[learning rate: 0.0068541]
	Learning Rate: 0.00685405
	LOSS [training: 3.5281367123617016 | validation: 2.840176316640592]
	TIME [epoch: 8.53 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5607571717226847		[learning rate: 0.0068292]
	Learning Rate: 0.00682918
	LOSS [training: 3.5607571717226847 | validation: 2.6438224267496557]
	TIME [epoch: 8.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5355088940985118		[learning rate: 0.0068044]
	Learning Rate: 0.00680439
	LOSS [training: 3.5355088940985118 | validation: 2.8425372167891947]
	TIME [epoch: 8.54 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4577090191276354		[learning rate: 0.0067797]
	Learning Rate: 0.0067797
	LOSS [training: 3.4577090191276354 | validation: 2.948256027170612]
	TIME [epoch: 8.54 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5117372581499824		[learning rate: 0.0067551]
	Learning Rate: 0.0067551
	LOSS [training: 3.5117372581499824 | validation: 2.689240791652358]
	TIME [epoch: 8.53 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.501021839481303		[learning rate: 0.0067306]
	Learning Rate: 0.00673058
	LOSS [training: 3.501021839481303 | validation: 2.845963239506098]
	TIME [epoch: 8.55 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.538134301405158		[learning rate: 0.0067062]
	Learning Rate: 0.00670616
	LOSS [training: 3.538134301405158 | validation: 2.684105596064373]
	TIME [epoch: 8.53 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.547600599558797		[learning rate: 0.0066818]
	Learning Rate: 0.00668182
	LOSS [training: 3.547600599558797 | validation: 2.6766347971535898]
	TIME [epoch: 8.54 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5306590543330616		[learning rate: 0.0066576]
	Learning Rate: 0.00665757
	LOSS [training: 3.5306590543330616 | validation: 2.677828825037092]
	TIME [epoch: 8.52 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5089667565295954		[learning rate: 0.0066334]
	Learning Rate: 0.00663341
	LOSS [training: 3.5089667565295954 | validation: 2.757380104396583]
	TIME [epoch: 8.56 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5584286090847295		[learning rate: 0.0066093]
	Learning Rate: 0.00660934
	LOSS [training: 3.5584286090847295 | validation: 2.6890171732006394]
	TIME [epoch: 8.53 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.492233292618342		[learning rate: 0.0065854]
	Learning Rate: 0.00658535
	LOSS [training: 3.492233292618342 | validation: 2.9259527084128036]
	TIME [epoch: 8.52 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.549661263625871		[learning rate: 0.0065615]
	Learning Rate: 0.00656145
	LOSS [training: 3.549661263625871 | validation: 2.7515964059716533]
	TIME [epoch: 8.53 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5094639721027114		[learning rate: 0.0065376]
	Learning Rate: 0.00653764
	LOSS [training: 3.5094639721027114 | validation: 2.8136082731333696]
	TIME [epoch: 8.56 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.466151112502639		[learning rate: 0.0065139]
	Learning Rate: 0.00651392
	LOSS [training: 3.466151112502639 | validation: 3.271803808209661]
	TIME [epoch: 8.54 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5252498564158046		[learning rate: 0.0064903]
	Learning Rate: 0.00649028
	LOSS [training: 3.5252498564158046 | validation: 2.6686565658154997]
	TIME [epoch: 8.53 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.470239467228577		[learning rate: 0.0064667]
	Learning Rate: 0.00646672
	LOSS [training: 3.470239467228577 | validation: 2.6645913266276615]
	TIME [epoch: 8.53 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4599645102974024		[learning rate: 0.0064433]
	Learning Rate: 0.00644325
	LOSS [training: 3.4599645102974024 | validation: 3.0296073860919446]
	TIME [epoch: 8.56 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5649592178824347		[learning rate: 0.0064199]
	Learning Rate: 0.00641987
	LOSS [training: 3.5649592178824347 | validation: 2.866346862974843]
	TIME [epoch: 8.54 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.528564239683255		[learning rate: 0.0063966]
	Learning Rate: 0.00639657
	LOSS [training: 3.528564239683255 | validation: 2.821348171822556]
	TIME [epoch: 8.53 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.525425914399457		[learning rate: 0.0063734]
	Learning Rate: 0.00637336
	LOSS [training: 3.525425914399457 | validation: 2.674495946505622]
	TIME [epoch: 8.53 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5046944255976746		[learning rate: 0.0063502]
	Learning Rate: 0.00635023
	LOSS [training: 3.5046944255976746 | validation: 2.657052188268512]
	TIME [epoch: 8.56 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5968555535643914		[learning rate: 0.0063272]
	Learning Rate: 0.00632718
	LOSS [training: 3.5968555535643914 | validation: 2.873254700728374]
	TIME [epoch: 8.53 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.502580377445527		[learning rate: 0.0063042]
	Learning Rate: 0.00630422
	LOSS [training: 3.502580377445527 | validation: 3.106675457471494]
	TIME [epoch: 8.52 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5290606460067218		[learning rate: 0.0062813]
	Learning Rate: 0.00628134
	LOSS [training: 3.5290606460067218 | validation: 2.6540770763678805]
	TIME [epoch: 8.52 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5091209677248343		[learning rate: 0.0062585]
	Learning Rate: 0.00625855
	LOSS [training: 3.5091209677248343 | validation: 2.65002700750631]
	TIME [epoch: 8.56 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.518224279390035		[learning rate: 0.0062358]
	Learning Rate: 0.00623584
	LOSS [training: 3.518224279390035 | validation: 2.6485309372872656]
	TIME [epoch: 8.53 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.457729515546297		[learning rate: 0.0062132]
	Learning Rate: 0.00621321
	LOSS [training: 3.457729515546297 | validation: 2.685919680168105]
	TIME [epoch: 8.52 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4714459754359113		[learning rate: 0.0061907]
	Learning Rate: 0.00619066
	LOSS [training: 3.4714459754359113 | validation: 2.9017868060226504]
	TIME [epoch: 8.53 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.489212170802164		[learning rate: 0.0061682]
	Learning Rate: 0.00616819
	LOSS [training: 3.489212170802164 | validation: 2.6547987212320607]
	TIME [epoch: 8.56 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.578242267416474		[learning rate: 0.0061458]
	Learning Rate: 0.00614581
	LOSS [training: 3.578242267416474 | validation: 3.0159296218626714]
	TIME [epoch: 8.53 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5445713701981467		[learning rate: 0.0061235]
	Learning Rate: 0.0061235
	LOSS [training: 3.5445713701981467 | validation: 2.7282910351552596]
	TIME [epoch: 8.52 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4456875158759432		[learning rate: 0.0061013]
	Learning Rate: 0.00610128
	LOSS [training: 3.4456875158759432 | validation: 2.9799601962616595]
	TIME [epoch: 8.53 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.506152869479881		[learning rate: 0.0060791]
	Learning Rate: 0.00607914
	LOSS [training: 3.506152869479881 | validation: 2.665584059645723]
	TIME [epoch: 8.56 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4752618680368705		[learning rate: 0.0060571]
	Learning Rate: 0.00605708
	LOSS [training: 3.4752618680368705 | validation: 2.790666998024065]
	TIME [epoch: 8.53 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.472238126542102		[learning rate: 0.0060351]
	Learning Rate: 0.0060351
	LOSS [training: 3.472238126542102 | validation: 2.665509710611498]
	TIME [epoch: 8.53 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4855609380909263		[learning rate: 0.0060132]
	Learning Rate: 0.00601319
	LOSS [training: 3.4855609380909263 | validation: 2.611484400854118]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.485006652008509		[learning rate: 0.0059914]
	Learning Rate: 0.00599137
	LOSS [training: 3.485006652008509 | validation: 2.658092168661214]
	TIME [epoch: 8.59 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5142938792622354		[learning rate: 0.0059696]
	Learning Rate: 0.00596963
	LOSS [training: 3.5142938792622354 | validation: 2.704384943485437]
	TIME [epoch: 8.55 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4451161990050787		[learning rate: 0.005948]
	Learning Rate: 0.00594797
	LOSS [training: 3.4451161990050787 | validation: 2.721915949564166]
	TIME [epoch: 8.54 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.462324066972885		[learning rate: 0.0059264]
	Learning Rate: 0.00592638
	LOSS [training: 3.462324066972885 | validation: 2.691008414381635]
	TIME [epoch: 8.57 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.500895244807942		[learning rate: 0.0059049]
	Learning Rate: 0.00590487
	LOSS [training: 3.500895244807942 | validation: 2.6257996244724633]
	TIME [epoch: 8.55 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.459563560755991		[learning rate: 0.0058834]
	Learning Rate: 0.00588344
	LOSS [training: 3.459563560755991 | validation: 2.675554858779611]
	TIME [epoch: 8.57 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4983022925083915		[learning rate: 0.0058621]
	Learning Rate: 0.00586209
	LOSS [training: 3.4983022925083915 | validation: 2.7443207382725117]
	TIME [epoch: 8.54 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4945246352560355		[learning rate: 0.0058408]
	Learning Rate: 0.00584082
	LOSS [training: 3.4945246352560355 | validation: 2.846386709181021]
	TIME [epoch: 8.54 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4921971677374506		[learning rate: 0.0058196]
	Learning Rate: 0.00581962
	LOSS [training: 3.4921971677374506 | validation: 2.811921119795974]
	TIME [epoch: 8.57 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4730586640518006		[learning rate: 0.0057985]
	Learning Rate: 0.0057985
	LOSS [training: 3.4730586640518006 | validation: 2.7256029366645613]
	TIME [epoch: 8.57 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4908241754562774		[learning rate: 0.0057775]
	Learning Rate: 0.00577746
	LOSS [training: 3.4908241754562774 | validation: 2.7150309387641514]
	TIME [epoch: 8.54 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4729265373529286		[learning rate: 0.0057565]
	Learning Rate: 0.00575649
	LOSS [training: 3.4729265373529286 | validation: 2.628564102415758]
	TIME [epoch: 8.55 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.441218292656866		[learning rate: 0.0057356]
	Learning Rate: 0.0057356
	LOSS [training: 3.441218292656866 | validation: 2.645452992688265]
	TIME [epoch: 8.56 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5480445659499353		[learning rate: 0.0057148]
	Learning Rate: 0.00571479
	LOSS [training: 3.5480445659499353 | validation: 2.6381513483031505]
	TIME [epoch: 8.56 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.463037095783208		[learning rate: 0.005694]
	Learning Rate: 0.00569405
	LOSS [training: 3.463037095783208 | validation: 2.7424241021645654]
	TIME [epoch: 8.54 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.456773443653046		[learning rate: 0.0056734]
	Learning Rate: 0.00567338
	LOSS [training: 3.456773443653046 | validation: 2.6173234219279364]
	TIME [epoch: 8.55 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.523463077031772		[learning rate: 0.0056528]
	Learning Rate: 0.00565279
	LOSS [training: 3.523463077031772 | validation: 2.6553745364372454]
	TIME [epoch: 8.56 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.441021119955336		[learning rate: 0.0056323]
	Learning Rate: 0.00563228
	LOSS [training: 3.441021119955336 | validation: 2.6712930403046244]
	TIME [epoch: 8.57 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.457080087112563		[learning rate: 0.0056118]
	Learning Rate: 0.00561184
	LOSS [training: 3.457080087112563 | validation: 2.6373163017132937]
	TIME [epoch: 8.54 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5127290319527367		[learning rate: 0.0055915]
	Learning Rate: 0.00559147
	LOSS [training: 3.5127290319527367 | validation: 2.664840368307963]
	TIME [epoch: 8.55 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.515072800480857		[learning rate: 0.0055712]
	Learning Rate: 0.00557118
	LOSS [training: 3.515072800480857 | validation: 2.649743613411928]
	TIME [epoch: 8.58 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5013701471187892		[learning rate: 0.005551]
	Learning Rate: 0.00555096
	LOSS [training: 3.5013701471187892 | validation: 2.66249619411778]
	TIME [epoch: 8.56 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.455050879476336		[learning rate: 0.0055308]
	Learning Rate: 0.00553082
	LOSS [training: 3.455050879476336 | validation: 2.637361799196042]
	TIME [epoch: 8.54 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4256107935384983		[learning rate: 0.0055107]
	Learning Rate: 0.00551075
	LOSS [training: 3.4256107935384983 | validation: 2.6851700529023197]
	TIME [epoch: 8.55 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.52773204879801		[learning rate: 0.0054907]
	Learning Rate: 0.00549075
	LOSS [training: 3.52773204879801 | validation: 2.6499429194700763]
	TIME [epoch: 8.58 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.450195815149383		[learning rate: 0.0054708]
	Learning Rate: 0.00547082
	LOSS [training: 3.450195815149383 | validation: 2.8998637263957554]
	TIME [epoch: 8.56 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.479870411987959		[learning rate: 0.005451]
	Learning Rate: 0.00545097
	LOSS [training: 3.479870411987959 | validation: 2.8417036158183437]
	TIME [epoch: 8.55 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4773786192093126		[learning rate: 0.0054312]
	Learning Rate: 0.00543119
	LOSS [training: 3.4773786192093126 | validation: 2.644769497135506]
	TIME [epoch: 8.55 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.444610061884158		[learning rate: 0.0054115]
	Learning Rate: 0.00541148
	LOSS [training: 3.444610061884158 | validation: 2.7640820983740344]
	TIME [epoch: 8.59 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4564488925954784		[learning rate: 0.0053918]
	Learning Rate: 0.00539184
	LOSS [training: 3.4564488925954784 | validation: 2.6558835899825683]
	TIME [epoch: 8.55 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.456940745840739		[learning rate: 0.0053723]
	Learning Rate: 0.00537227
	LOSS [training: 3.456940745840739 | validation: 2.956933037347458]
	TIME [epoch: 8.55 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5260416080077936		[learning rate: 0.0053528]
	Learning Rate: 0.00535277
	LOSS [training: 3.5260416080077936 | validation: 2.6459136998648347]
	TIME [epoch: 8.55 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4545103768362835		[learning rate: 0.0053333]
	Learning Rate: 0.00533335
	LOSS [training: 3.4545103768362835 | validation: 2.6222676736375767]
	TIME [epoch: 8.59 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5187525777024007		[learning rate: 0.005314]
	Learning Rate: 0.00531399
	LOSS [training: 3.5187525777024007 | validation: 2.701808698070157]
	TIME [epoch: 8.55 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4577750341257127		[learning rate: 0.0052947]
	Learning Rate: 0.00529471
	LOSS [training: 3.4577750341257127 | validation: 3.1407859685410395]
	TIME [epoch: 8.54 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4799941363906677		[learning rate: 0.0052755]
	Learning Rate: 0.00527549
	LOSS [training: 3.4799941363906677 | validation: 2.8539623862137837]
	TIME [epoch: 8.56 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.522454348409629		[learning rate: 0.0052563]
	Learning Rate: 0.00525635
	LOSS [training: 3.522454348409629 | validation: 2.8310132116703914]
	TIME [epoch: 8.57 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.432465446896033		[learning rate: 0.0052373]
	Learning Rate: 0.00523727
	LOSS [training: 3.432465446896033 | validation: 2.818488566701518]
	TIME [epoch: 8.56 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4531288860445395		[learning rate: 0.0052183]
	Learning Rate: 0.00521827
	LOSS [training: 3.4531288860445395 | validation: 2.6589370329944524]
	TIME [epoch: 8.55 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4301642764114546		[learning rate: 0.0051993]
	Learning Rate: 0.00519933
	LOSS [training: 3.4301642764114546 | validation: 2.9619359967561394]
	TIME [epoch: 8.55 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5118563953445516		[learning rate: 0.0051805]
	Learning Rate: 0.00518046
	LOSS [training: 3.5118563953445516 | validation: 2.671501947018391]
	TIME [epoch: 8.56 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4785138636251673		[learning rate: 0.0051617]
	Learning Rate: 0.00516166
	LOSS [training: 3.4785138636251673 | validation: 2.6383247520518203]
	TIME [epoch: 8.58 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.472012911491477		[learning rate: 0.0051429]
	Learning Rate: 0.00514293
	LOSS [training: 3.472012911491477 | validation: 2.6630484192643125]
	TIME [epoch: 8.55 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4499171524338217		[learning rate: 0.0051243]
	Learning Rate: 0.00512426
	LOSS [training: 3.4499171524338217 | validation: 2.658819417297899]
	TIME [epoch: 8.57 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4387560045470216		[learning rate: 0.0051057]
	Learning Rate: 0.00510567
	LOSS [training: 3.4387560045470216 | validation: 2.635430613042397]
	TIME [epoch: 8.55 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.476073025799734		[learning rate: 0.0050871]
	Learning Rate: 0.00508714
	LOSS [training: 3.476073025799734 | validation: 2.6515997797403332]
	TIME [epoch: 8.57 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4448361547970108		[learning rate: 0.0050687]
	Learning Rate: 0.00506868
	LOSS [training: 3.4448361547970108 | validation: 2.8093519120919965]
	TIME [epoch: 8.55 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.6936172546504147		[learning rate: 0.0050503]
	Learning Rate: 0.00505028
	LOSS [training: 3.6936172546504147 | validation: 2.9594488469885585]
	TIME [epoch: 8.56 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.461892984741204		[learning rate: 0.005032]
	Learning Rate: 0.00503196
	LOSS [training: 3.461892984741204 | validation: 2.6837155768462035]
	TIME [epoch: 8.56 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4542968852982647		[learning rate: 0.0050137]
	Learning Rate: 0.00501369
	LOSS [training: 3.4542968852982647 | validation: 2.8971627885767384]
	TIME [epoch: 8.55 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4529731168487814		[learning rate: 0.0049955]
	Learning Rate: 0.0049955
	LOSS [training: 3.4529731168487814 | validation: 2.780160383468675]
	TIME [epoch: 8.54 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.446252272557083		[learning rate: 0.0049774]
	Learning Rate: 0.00497737
	LOSS [training: 3.446252272557083 | validation: 2.677101750407485]
	TIME [epoch: 8.56 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5166477494973605		[learning rate: 0.0049593]
	Learning Rate: 0.00495931
	LOSS [training: 3.5166477494973605 | validation: 2.6636930874948517]
	TIME [epoch: 8.57 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.436764960206591		[learning rate: 0.0049413]
	Learning Rate: 0.00494131
	LOSS [training: 3.436764960206591 | validation: 2.717566214912409]
	TIME [epoch: 8.55 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4466672656352095		[learning rate: 0.0049234]
	Learning Rate: 0.00492338
	LOSS [training: 3.4466672656352095 | validation: 2.6662535260387066]
	TIME [epoch: 8.54 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4162990140642426		[learning rate: 0.0049055]
	Learning Rate: 0.00490551
	LOSS [training: 3.4162990140642426 | validation: 2.6265903279433513]
	TIME [epoch: 8.56 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4327068330191954		[learning rate: 0.0048877]
	Learning Rate: 0.00488771
	LOSS [training: 3.4327068330191954 | validation: 2.6470664342584507]
	TIME [epoch: 8.56 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.5107833230862227		[learning rate: 0.00487]
	Learning Rate: 0.00486997
	LOSS [training: 3.5107833230862227 | validation: 2.602393320716618]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.459027612196646		[learning rate: 0.0048523]
	Learning Rate: 0.0048523
	LOSS [training: 3.459027612196646 | validation: 2.774594759167006]
	TIME [epoch: 8.54 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.443118761367013		[learning rate: 0.0048347]
	Learning Rate: 0.00483469
	LOSS [training: 3.443118761367013 | validation: 2.6445658620609658]
	TIME [epoch: 8.57 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4430748388992676		[learning rate: 0.0048171]
	Learning Rate: 0.00481714
	LOSS [training: 3.4430748388992676 | validation: 2.6724955534078116]
	TIME [epoch: 8.56 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.467675658316194		[learning rate: 0.0047997]
	Learning Rate: 0.00479966
	LOSS [training: 3.467675658316194 | validation: 2.7420461692752838]
	TIME [epoch: 8.53 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4521658956064427		[learning rate: 0.0047822]
	Learning Rate: 0.00478224
	LOSS [training: 3.4521658956064427 | validation: 2.6964293156905663]
	TIME [epoch: 8.53 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.458238905649398		[learning rate: 0.0047649]
	Learning Rate: 0.00476489
	LOSS [training: 3.458238905649398 | validation: 2.618459187817166]
	TIME [epoch: 8.58 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4121135595085983		[learning rate: 0.0047476]
	Learning Rate: 0.00474759
	LOSS [training: 3.4121135595085983 | validation: 2.945313066096435]
	TIME [epoch: 8.54 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.505292251780763		[learning rate: 0.0047304]
	Learning Rate: 0.00473037
	LOSS [training: 3.505292251780763 | validation: 2.649317909629326]
	TIME [epoch: 8.53 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4079228041632397		[learning rate: 0.0047132]
	Learning Rate: 0.0047132
	LOSS [training: 3.4079228041632397 | validation: 2.7287210736884453]
	TIME [epoch: 8.55 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.485069514921052		[learning rate: 0.0046961]
	Learning Rate: 0.00469609
	LOSS [training: 3.485069514921052 | validation: 2.7866245134676486]
	TIME [epoch: 8.57 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4653436102560122		[learning rate: 0.0046791]
	Learning Rate: 0.00467905
	LOSS [training: 3.4653436102560122 | validation: 2.637945799589341]
	TIME [epoch: 8.54 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.494486761162931		[learning rate: 0.0046621]
	Learning Rate: 0.00466207
	LOSS [training: 3.494486761162931 | validation: 2.740876947882402]
	TIME [epoch: 8.54 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4741732061776522		[learning rate: 0.0046452]
	Learning Rate: 0.00464515
	LOSS [training: 3.4741732061776522 | validation: 2.670725529072617]
	TIME [epoch: 8.56 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4729809888993133		[learning rate: 0.0046283]
	Learning Rate: 0.0046283
	LOSS [training: 3.4729809888993133 | validation: 2.622986675932194]
	TIME [epoch: 8.55 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.544638156125727		[learning rate: 0.0046115]
	Learning Rate: 0.0046115
	LOSS [training: 3.544638156125727 | validation: 2.886579231829337]
	TIME [epoch: 8.54 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.454655282288089		[learning rate: 0.0045948]
	Learning Rate: 0.00459476
	LOSS [training: 3.454655282288089 | validation: 2.654885408016075]
	TIME [epoch: 8.54 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.459009680094295		[learning rate: 0.0045781]
	Learning Rate: 0.00457809
	LOSS [training: 3.459009680094295 | validation: 2.6515620329137772]
	TIME [epoch: 8.58 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4426321987968236		[learning rate: 0.0045615]
	Learning Rate: 0.00456148
	LOSS [training: 3.4426321987968236 | validation: 2.6583786589221408]
	TIME [epoch: 8.55 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4201933382464227		[learning rate: 0.0045449]
	Learning Rate: 0.00454492
	LOSS [training: 3.4201933382464227 | validation: 2.616310670439808]
	TIME [epoch: 8.54 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.431892550317175		[learning rate: 0.0045284]
	Learning Rate: 0.00452843
	LOSS [training: 3.431892550317175 | validation: 2.612645273643759]
	TIME [epoch: 8.56 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4687669358493616		[learning rate: 0.004512]
	Learning Rate: 0.00451199
	LOSS [training: 3.4687669358493616 | validation: 2.6198530578880264]
	TIME [epoch: 8.59 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.482509103874299		[learning rate: 0.0044956]
	Learning Rate: 0.00449562
	LOSS [training: 3.482509103874299 | validation: 2.7548967026232787]
	TIME [epoch: 8.55 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4792975602713208		[learning rate: 0.0044793]
	Learning Rate: 0.0044793
	LOSS [training: 3.4792975602713208 | validation: 2.6710582357837613]
	TIME [epoch: 8.54 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.424172924532442		[learning rate: 0.004463]
	Learning Rate: 0.00446305
	LOSS [training: 3.424172924532442 | validation: 2.6344586775115637]
	TIME [epoch: 8.56 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4421822023681896		[learning rate: 0.0044469]
	Learning Rate: 0.00444685
	LOSS [training: 3.4421822023681896 | validation: 2.6293568491747377]
	TIME [epoch: 8.58 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4498817373358177		[learning rate: 0.0044307]
	Learning Rate: 0.00443071
	LOSS [training: 3.4498817373358177 | validation: 2.6961653414739533]
	TIME [epoch: 8.55 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4549378880712474		[learning rate: 0.0044146]
	Learning Rate: 0.00441463
	LOSS [training: 3.4549378880712474 | validation: 2.6549415397597516]
	TIME [epoch: 8.54 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4216960972696233		[learning rate: 0.0043986]
	Learning Rate: 0.00439861
	LOSS [training: 3.4216960972696233 | validation: 2.7309645981331885]
	TIME [epoch: 8.56 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4295925686635735		[learning rate: 0.0043827]
	Learning Rate: 0.00438265
	LOSS [training: 3.4295925686635735 | validation: 2.705244910862617]
	TIME [epoch: 8.57 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.461084273536598		[learning rate: 0.0043667]
	Learning Rate: 0.00436675
	LOSS [training: 3.461084273536598 | validation: 2.8510765316118265]
	TIME [epoch: 8.55 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4698087194418847		[learning rate: 0.0043509]
	Learning Rate: 0.0043509
	LOSS [training: 3.4698087194418847 | validation: 2.6270447620927477]
	TIME [epoch: 8.55 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.428809418487803		[learning rate: 0.0043351]
	Learning Rate: 0.00433511
	LOSS [training: 3.428809418487803 | validation: 2.7824075131352766]
	TIME [epoch: 8.57 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.460394648897252		[learning rate: 0.0043194]
	Learning Rate: 0.00431938
	LOSS [training: 3.460394648897252 | validation: 2.6115555049084276]
	TIME [epoch: 8.57 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4101611370440224		[learning rate: 0.0043037]
	Learning Rate: 0.0043037
	LOSS [training: 3.4101611370440224 | validation: 2.7642805583735024]
	TIME [epoch: 8.54 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4421966288456005		[learning rate: 0.0042881]
	Learning Rate: 0.00428808
	LOSS [training: 3.4421966288456005 | validation: 2.652947087095511]
	TIME [epoch: 8.54 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4619104483810794		[learning rate: 0.0042725]
	Learning Rate: 0.00427252
	LOSS [training: 3.4619104483810794 | validation: 2.6561834785158003]
	TIME [epoch: 8.57 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4560268989192897		[learning rate: 0.004257]
	Learning Rate: 0.00425702
	LOSS [training: 3.4560268989192897 | validation: 2.6363002643428386]
	TIME [epoch: 8.57 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.410858741278733		[learning rate: 0.0042416]
	Learning Rate: 0.00424157
	LOSS [training: 3.410858741278733 | validation: 2.6704114298900135]
	TIME [epoch: 8.54 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.445000433931058		[learning rate: 0.0042262]
	Learning Rate: 0.00422617
	LOSS [training: 3.445000433931058 | validation: 2.6445970994844132]
	TIME [epoch: 8.55 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4472619336945756		[learning rate: 0.0042108]
	Learning Rate: 0.00421084
	LOSS [training: 3.4472619336945756 | validation: 2.6570525212454754]
	TIME [epoch: 8.57 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.430200889567888		[learning rate: 0.0041956]
	Learning Rate: 0.00419556
	LOSS [training: 3.430200889567888 | validation: 2.626339648690756]
	TIME [epoch: 8.56 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.428504490195943		[learning rate: 0.0041803]
	Learning Rate: 0.00418033
	LOSS [training: 3.428504490195943 | validation: 2.7379114322486595]
	TIME [epoch: 8.54 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.441158209554149		[learning rate: 0.0041652]
	Learning Rate: 0.00416516
	LOSS [training: 3.441158209554149 | validation: 2.6034171332495752]
	TIME [epoch: 8.56 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.423679760523762		[learning rate: 0.00415]
	Learning Rate: 0.00415004
	LOSS [training: 3.423679760523762 | validation: 2.6186915964486164]
	TIME [epoch: 8.57 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4246319920040236		[learning rate: 0.004135]
	Learning Rate: 0.00413498
	LOSS [training: 3.4246319920040236 | validation: 2.6513619978910152]
	TIME [epoch: 8.55 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.429387901219871		[learning rate: 0.00412]
	Learning Rate: 0.00411998
	LOSS [training: 3.429387901219871 | validation: 2.599767097256851]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_344.pth
	Model improved!!!
EPOCH 345/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.451832191733474		[learning rate: 0.004105]
	Learning Rate: 0.00410502
	LOSS [training: 3.451832191733474 | validation: 2.641175340897084]
	TIME [epoch: 8.55 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4007232180922893		[learning rate: 0.0040901]
	Learning Rate: 0.00409013
	LOSS [training: 3.4007232180922893 | validation: 2.594801667077146]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.40843151739069		[learning rate: 0.0040753]
	Learning Rate: 0.00407528
	LOSS [training: 3.40843151739069 | validation: 2.6413364823482945]
	TIME [epoch: 8.53 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.410445467176346		[learning rate: 0.0040605]
	Learning Rate: 0.00406049
	LOSS [training: 3.410445467176346 | validation: 2.7942834120673576]
	TIME [epoch: 8.54 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4119121108355372		[learning rate: 0.0040458]
	Learning Rate: 0.00404576
	LOSS [training: 3.4119121108355372 | validation: 2.6033282160697606]
	TIME [epoch: 8.54 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.441300462156993		[learning rate: 0.0040311]
	Learning Rate: 0.00403108
	LOSS [training: 3.441300462156993 | validation: 2.6855685739327027]
	TIME [epoch: 8.55 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4380052394698124		[learning rate: 0.0040164]
	Learning Rate: 0.00401645
	LOSS [training: 3.4380052394698124 | validation: 2.696969500613674]
	TIME [epoch: 8.54 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4584110219170094		[learning rate: 0.0040019]
	Learning Rate: 0.00400187
	LOSS [training: 3.4584110219170094 | validation: 2.592198068887983]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4177919944489155		[learning rate: 0.0039873]
	Learning Rate: 0.00398735
	LOSS [training: 3.4177919944489155 | validation: 2.6854293317228377]
	TIME [epoch: 8.53 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4361615565327392		[learning rate: 0.0039729]
	Learning Rate: 0.00397288
	LOSS [training: 3.4361615565327392 | validation: 2.6094199927143653]
	TIME [epoch: 8.55 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3999259317196673		[learning rate: 0.0039585]
	Learning Rate: 0.00395846
	LOSS [training: 3.3999259317196673 | validation: 2.6516276602273825]
	TIME [epoch: 8.52 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.396046358650814		[learning rate: 0.0039441]
	Learning Rate: 0.00394409
	LOSS [training: 3.396046358650814 | validation: 2.5862858816344256]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4255364053090167		[learning rate: 0.0039298]
	Learning Rate: 0.00392978
	LOSS [training: 3.4255364053090167 | validation: 2.6233369142488443]
	TIME [epoch: 8.54 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.450401050057245		[learning rate: 0.0039155]
	Learning Rate: 0.00391552
	LOSS [training: 3.450401050057245 | validation: 2.6024147974624743]
	TIME [epoch: 8.55 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4056455941349517		[learning rate: 0.0039013]
	Learning Rate: 0.00390131
	LOSS [training: 3.4056455941349517 | validation: 2.599778690351294]
	TIME [epoch: 8.54 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4153175528978865		[learning rate: 0.0038872]
	Learning Rate: 0.00388715
	LOSS [training: 3.4153175528978865 | validation: 2.631778572402763]
	TIME [epoch: 8.55 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.387573685862253		[learning rate: 0.003873]
	Learning Rate: 0.00387305
	LOSS [training: 3.387573685862253 | validation: 2.5996169139453595]
	TIME [epoch: 8.54 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4602827097132467		[learning rate: 0.003859]
	Learning Rate: 0.00385899
	LOSS [training: 3.4602827097132467 | validation: 2.6575826766479755]
	TIME [epoch: 8.54 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4074212018966534		[learning rate: 0.003845]
	Learning Rate: 0.00384499
	LOSS [training: 3.4074212018966534 | validation: 2.616641731449179]
	TIME [epoch: 8.54 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4122546261024356		[learning rate: 0.003831]
	Learning Rate: 0.00383103
	LOSS [training: 3.4122546261024356 | validation: 2.62294063455455]
	TIME [epoch: 8.54 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.42121899334945		[learning rate: 0.0038171]
	Learning Rate: 0.00381713
	LOSS [training: 3.42121899334945 | validation: 2.59716734098129]
	TIME [epoch: 8.55 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4046058208496612		[learning rate: 0.0038033]
	Learning Rate: 0.00380328
	LOSS [training: 3.4046058208496612 | validation: 2.610314633075429]
	TIME [epoch: 8.53 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4139697859934666		[learning rate: 0.0037895]
	Learning Rate: 0.00378947
	LOSS [training: 3.4139697859934666 | validation: 2.6846025630963477]
	TIME [epoch: 8.55 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3898522096345394		[learning rate: 0.0037757]
	Learning Rate: 0.00377572
	LOSS [training: 3.3898522096345394 | validation: 2.607176038019271]
	TIME [epoch: 8.53 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3960182064749453		[learning rate: 0.003762]
	Learning Rate: 0.00376202
	LOSS [training: 3.3960182064749453 | validation: 2.676365477340412]
	TIME [epoch: 8.54 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.388753523086378		[learning rate: 0.0037484]
	Learning Rate: 0.00374837
	LOSS [training: 3.388753523086378 | validation: 2.6516380542653675]
	TIME [epoch: 8.53 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4323845099590526		[learning rate: 0.0037348]
	Learning Rate: 0.00373476
	LOSS [training: 3.4323845099590526 | validation: 2.7047019261593634]
	TIME [epoch: 8.55 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4492595867838625		[learning rate: 0.0037212]
	Learning Rate: 0.00372121
	LOSS [training: 3.4492595867838625 | validation: 2.632952294514892]
	TIME [epoch: 8.53 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3901093204068617		[learning rate: 0.0037077]
	Learning Rate: 0.00370771
	LOSS [training: 3.3901093204068617 | validation: 2.7042006715681115]
	TIME [epoch: 8.55 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4301216291513668		[learning rate: 0.0036943]
	Learning Rate: 0.00369425
	LOSS [training: 3.4301216291513668 | validation: 2.6722592358136903]
	TIME [epoch: 8.54 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4266324103772816		[learning rate: 0.0036808]
	Learning Rate: 0.00368084
	LOSS [training: 3.4266324103772816 | validation: 2.6159954550806326]
	TIME [epoch: 8.54 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.39882361719449		[learning rate: 0.0036675]
	Learning Rate: 0.00366749
	LOSS [training: 3.39882361719449 | validation: 2.612286162371766]
	TIME [epoch: 8.53 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.374878278082374		[learning rate: 0.0036542]
	Learning Rate: 0.00365418
	LOSS [training: 3.374878278082374 | validation: 2.7181804383590213]
	TIME [epoch: 8.54 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.413387607501158		[learning rate: 0.0036409]
	Learning Rate: 0.00364092
	LOSS [training: 3.413387607501158 | validation: 2.701734454081722]
	TIME [epoch: 8.54 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4156345184758976		[learning rate: 0.0036277]
	Learning Rate: 0.0036277
	LOSS [training: 3.4156345184758976 | validation: 2.602754227048507]
	TIME [epoch: 8.53 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4718252181945717		[learning rate: 0.0036145]
	Learning Rate: 0.00361454
	LOSS [training: 3.4718252181945717 | validation: 2.6589770747315273]
	TIME [epoch: 8.52 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4203449373303494		[learning rate: 0.0036014]
	Learning Rate: 0.00360142
	LOSS [training: 3.4203449373303494 | validation: 2.6750809813336236]
	TIME [epoch: 8.55 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.417082495137689		[learning rate: 0.0035883]
	Learning Rate: 0.00358835
	LOSS [training: 3.417082495137689 | validation: 2.636250669785522]
	TIME [epoch: 8.56 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.425517597631837		[learning rate: 0.0035753]
	Learning Rate: 0.00357533
	LOSS [training: 3.425517597631837 | validation: 2.6354331180585184]
	TIME [epoch: 8.54 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4084226480446285		[learning rate: 0.0035624]
	Learning Rate: 0.00356235
	LOSS [training: 3.4084226480446285 | validation: 2.6412010781873576]
	TIME [epoch: 8.54 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3854185020141356		[learning rate: 0.0035494]
	Learning Rate: 0.00354942
	LOSS [training: 3.3854185020141356 | validation: 2.60881456388431]
	TIME [epoch: 8.56 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3678244092098453		[learning rate: 0.0035365]
	Learning Rate: 0.00353654
	LOSS [training: 3.3678244092098453 | validation: 2.6102365280601947]
	TIME [epoch: 8.55 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3898791629574694		[learning rate: 0.0035237]
	Learning Rate: 0.00352371
	LOSS [training: 3.3898791629574694 | validation: 2.680144479464144]
	TIME [epoch: 8.53 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.446734129208872		[learning rate: 0.0035109]
	Learning Rate: 0.00351092
	LOSS [training: 3.446734129208872 | validation: 2.630566088186717]
	TIME [epoch: 8.54 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.406381385969939		[learning rate: 0.0034982]
	Learning Rate: 0.00349818
	LOSS [training: 3.406381385969939 | validation: 2.6049034249336147]
	TIME [epoch: 8.55 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3945877603898493		[learning rate: 0.0034855]
	Learning Rate: 0.00348548
	LOSS [training: 3.3945877603898493 | validation: 2.590042763167346]
	TIME [epoch: 8.56 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.379412124895166		[learning rate: 0.0034728]
	Learning Rate: 0.00347284
	LOSS [training: 3.379412124895166 | validation: 2.6391196620874684]
	TIME [epoch: 8.53 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3843448988194518		[learning rate: 0.0034602]
	Learning Rate: 0.00346023
	LOSS [training: 3.3843448988194518 | validation: 2.573203800668839]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.423894755380514		[learning rate: 0.0034477]
	Learning Rate: 0.00344767
	LOSS [training: 3.423894755380514 | validation: 2.5980289499552947]
	TIME [epoch: 8.56 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.381196723057579		[learning rate: 0.0034352]
	Learning Rate: 0.00343516
	LOSS [training: 3.381196723057579 | validation: 2.5924524096791677]
	TIME [epoch: 8.53 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.376014109952841		[learning rate: 0.0034227]
	Learning Rate: 0.0034227
	LOSS [training: 3.376014109952841 | validation: 2.7330269149765893]
	TIME [epoch: 8.51 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.397767259103363		[learning rate: 0.0034103]
	Learning Rate: 0.00341028
	LOSS [training: 3.397767259103363 | validation: 2.818940129102036]
	TIME [epoch: 8.54 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4243068682533733		[learning rate: 0.0033979]
	Learning Rate: 0.0033979
	LOSS [training: 3.4243068682533733 | validation: 2.643909572410341]
	TIME [epoch: 8.54 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.368328969691363		[learning rate: 0.0033856]
	Learning Rate: 0.00338557
	LOSS [training: 3.368328969691363 | validation: 2.617602469463155]
	TIME [epoch: 8.53 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4074219981023637		[learning rate: 0.0033733]
	Learning Rate: 0.00337328
	LOSS [training: 3.4074219981023637 | validation: 2.796112521890316]
	TIME [epoch: 8.52 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4295055514156587		[learning rate: 0.003361]
	Learning Rate: 0.00336104
	LOSS [training: 3.4295055514156587 | validation: 2.654858540513087]
	TIME [epoch: 8.54 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.378142219043975		[learning rate: 0.0033488]
	Learning Rate: 0.00334884
	LOSS [training: 3.378142219043975 | validation: 2.596067099057205]
	TIME [epoch: 8.55 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.384578880604438		[learning rate: 0.0033367]
	Learning Rate: 0.00333669
	LOSS [training: 3.384578880604438 | validation: 2.5829005631551345]
	TIME [epoch: 8.52 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.406902472903275		[learning rate: 0.0033246]
	Learning Rate: 0.00332458
	LOSS [training: 3.406902472903275 | validation: 2.6402811831508872]
	TIME [epoch: 8.52 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3744833162768684		[learning rate: 0.0033125]
	Learning Rate: 0.00331252
	LOSS [training: 3.3744833162768684 | validation: 2.63963263884877]
	TIME [epoch: 8.54 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.363501980178209		[learning rate: 0.0033005]
	Learning Rate: 0.00330049
	LOSS [training: 3.363501980178209 | validation: 2.6220894673504236]
	TIME [epoch: 8.54 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.38540784181345		[learning rate: 0.0032885]
	Learning Rate: 0.00328852
	LOSS [training: 3.38540784181345 | validation: 2.5875318759040757]
	TIME [epoch: 8.52 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3856341151639717		[learning rate: 0.0032766]
	Learning Rate: 0.00327658
	LOSS [training: 3.3856341151639717 | validation: 2.6038332282405423]
	TIME [epoch: 8.52 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3909969854060895		[learning rate: 0.0032647]
	Learning Rate: 0.00326469
	LOSS [training: 3.3909969854060895 | validation: 2.5964518473026583]
	TIME [epoch: 8.55 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.374604820427998		[learning rate: 0.0032528]
	Learning Rate: 0.00325284
	LOSS [training: 3.374604820427998 | validation: 2.713871760405163]
	TIME [epoch: 8.52 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4376224993265416		[learning rate: 0.003241]
	Learning Rate: 0.00324104
	LOSS [training: 3.4376224993265416 | validation: 2.5974656910619]
	TIME [epoch: 8.53 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4138366814686925		[learning rate: 0.0032293]
	Learning Rate: 0.00322928
	LOSS [training: 3.4138366814686925 | validation: 2.584587393846388]
	TIME [epoch: 8.53 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.390181488840201		[learning rate: 0.0032176]
	Learning Rate: 0.00321756
	LOSS [training: 3.390181488840201 | validation: 2.5904668011534207]
	TIME [epoch: 8.54 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3908855833840845		[learning rate: 0.0032059]
	Learning Rate: 0.00320588
	LOSS [training: 3.3908855833840845 | validation: 2.597200506029754]
	TIME [epoch: 8.53 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.371314030313063		[learning rate: 0.0031942]
	Learning Rate: 0.00319425
	LOSS [training: 3.371314030313063 | validation: 2.6636592578496305]
	TIME [epoch: 8.54 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.386415183411457		[learning rate: 0.0031827]
	Learning Rate: 0.00318265
	LOSS [training: 3.386415183411457 | validation: 2.5743414600848533]
	TIME [epoch: 8.52 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4212374113250554		[learning rate: 0.0031711]
	Learning Rate: 0.0031711
	LOSS [training: 3.4212374113250554 | validation: 2.579590653848862]
	TIME [epoch: 8.55 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.408360386001084		[learning rate: 0.0031596]
	Learning Rate: 0.0031596
	LOSS [training: 3.408360386001084 | validation: 2.735052386629152]
	TIME [epoch: 8.52 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.385186352291803		[learning rate: 0.0031481]
	Learning Rate: 0.00314813
	LOSS [training: 3.385186352291803 | validation: 2.6359487375518817]
	TIME [epoch: 8.55 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4304956259651136		[learning rate: 0.0031367]
	Learning Rate: 0.00313671
	LOSS [training: 3.4304956259651136 | validation: 2.5946241386029736]
	TIME [epoch: 8.53 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3723059597093203		[learning rate: 0.0031253]
	Learning Rate: 0.00312532
	LOSS [training: 3.3723059597093203 | validation: 2.6315344155683555]
	TIME [epoch: 8.55 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3850983590970416		[learning rate: 0.003114]
	Learning Rate: 0.00311398
	LOSS [training: 3.3850983590970416 | validation: 2.6050007979602716]
	TIME [epoch: 8.53 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.38118629385835		[learning rate: 0.0031027]
	Learning Rate: 0.00310268
	LOSS [training: 3.38118629385835 | validation: 2.6726119399565094]
	TIME [epoch: 8.54 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.378921593735124		[learning rate: 0.0030914]
	Learning Rate: 0.00309142
	LOSS [training: 3.378921593735124 | validation: 3.300568688587977]
	TIME [epoch: 8.52 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.512404396089063		[learning rate: 0.0030802]
	Learning Rate: 0.0030802
	LOSS [training: 3.512404396089063 | validation: 2.6522083311117557]
	TIME [epoch: 8.55 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3764595852100756		[learning rate: 0.003069]
	Learning Rate: 0.00306902
	LOSS [training: 3.3764595852100756 | validation: 2.558290738601598]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.416371748846418		[learning rate: 0.0030579]
	Learning Rate: 0.00305788
	LOSS [training: 3.416371748846418 | validation: 2.6022528788029695]
	TIME [epoch: 8.55 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.397819813819661		[learning rate: 0.0030468]
	Learning Rate: 0.00304679
	LOSS [training: 3.397819813819661 | validation: 2.9502416745913322]
	TIME [epoch: 8.55 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4117523368668303		[learning rate: 0.0030357]
	Learning Rate: 0.00303573
	LOSS [training: 3.4117523368668303 | validation: 2.636685859048873]
	TIME [epoch: 8.53 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3540985684149676		[learning rate: 0.0030247]
	Learning Rate: 0.00302471
	LOSS [training: 3.3540985684149676 | validation: 2.5905924657864885]
	TIME [epoch: 8.54 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.383791159466726		[learning rate: 0.0030137]
	Learning Rate: 0.00301374
	LOSS [training: 3.383791159466726 | validation: 2.621466223029227]
	TIME [epoch: 8.53 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3660451715016984		[learning rate: 0.0030028]
	Learning Rate: 0.0030028
	LOSS [training: 3.3660451715016984 | validation: 2.5661115024929644]
	TIME [epoch: 8.56 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.4055870208484307		[learning rate: 0.0029919]
	Learning Rate: 0.0029919
	LOSS [training: 3.4055870208484307 | validation: 2.611637384323155]
	TIME [epoch: 8.53 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.373152218260055		[learning rate: 0.002981]
	Learning Rate: 0.00298104
	LOSS [training: 3.373152218260055 | validation: 2.5810717096771736]
	TIME [epoch: 8.53 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.3719844845302895		[learning rate: 0.0029702]
	Learning Rate: 0.00297023
	LOSS [training: 3.3719844845302895 | validation: 2.5936330253915827]
	TIME [epoch: 8.54 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/10] avg loss: 3.1071960063049344		[learning rate: 0.0029594]
	Learning Rate: 0.00295945
	LOSS [training: 3.1071960063049344 | validation: 1.8035296956363442]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.2989719216795974		[learning rate: 0.0029487]
	Learning Rate: 0.00294871
	LOSS [training: 2.2989719216795974 | validation: 1.926764907438193]
	TIME [epoch: 8.53 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.218698566136979		[learning rate: 0.002938]
	Learning Rate: 0.00293801
	LOSS [training: 2.218698566136979 | validation: 1.634689286377909]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1724387281008366		[learning rate: 0.0029273]
	Learning Rate: 0.00292734
	LOSS [training: 2.1724387281008366 | validation: 1.714643184007597]
	TIME [epoch: 8.55 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.225299784274143		[learning rate: 0.0029167]
	Learning Rate: 0.00291672
	LOSS [training: 2.225299784274143 | validation: 1.5952275495530581]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1092612350795386		[learning rate: 0.0029061]
	Learning Rate: 0.00290614
	LOSS [training: 2.1092612350795386 | validation: 1.6481180193660432]
	TIME [epoch: 8.54 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.1463857679607337		[learning rate: 0.0028956]
	Learning Rate: 0.00289559
	LOSS [training: 2.1463857679607337 | validation: 1.5989444368515544]
	TIME [epoch: 8.54 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.081266684342592		[learning rate: 0.0028851]
	Learning Rate: 0.00288508
	LOSS [training: 2.081266684342592 | validation: 1.6288162069716057]
	TIME [epoch: 8.57 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/10] avg loss: 2.0136250552951513		[learning rate: 0.0028746]
	Learning Rate: 0.00287461
	LOSS [training: 2.0136250552951513 | validation: 1.6525350572409145]
	TIME [epoch: 8.56 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.9401551212312937		[learning rate: 0.0028642]
	Learning Rate: 0.00286418
	LOSS [training: 1.9401551212312937 | validation: 1.560699603760273]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.8229888812894077		[learning rate: 0.0028538]
	Learning Rate: 0.00285378
	LOSS [training: 1.8229888812894077 | validation: 1.4890978775898092]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7333538387572651		[learning rate: 0.0028434]
	Learning Rate: 0.00284343
	LOSS [training: 1.7333538387572651 | validation: 1.5536517015143212]
	TIME [epoch: 8.56 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.703000968543666		[learning rate: 0.0028331]
	Learning Rate: 0.00283311
	LOSS [training: 1.703000968543666 | validation: 1.5580420481345405]
	TIME [epoch: 8.55 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.678662459323504		[learning rate: 0.0028228]
	Learning Rate: 0.00282283
	LOSS [training: 1.678662459323504 | validation: 1.5933231993678802]
	TIME [epoch: 8.54 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.7256609654948452		[learning rate: 0.0028126]
	Learning Rate: 0.00281258
	LOSS [training: 1.7256609654948452 | validation: 1.5614864432321038]
	TIME [epoch: 8.55 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6237104393455777		[learning rate: 0.0028024]
	Learning Rate: 0.00280238
	LOSS [training: 1.6237104393455777 | validation: 1.4699772029664262]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_450.pth
	Model improved!!!
EPOCH 451/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6242848587028305		[learning rate: 0.0027922]
	Learning Rate: 0.00279221
	LOSS [training: 1.6242848587028305 | validation: 1.5240837334321453]
	TIME [epoch: 8.54 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.6379891772963522		[learning rate: 0.0027821]
	Learning Rate: 0.00278207
	LOSS [training: 1.6379891772963522 | validation: 1.4650890786546704]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5710482732357054		[learning rate: 0.002772]
	Learning Rate: 0.00277198
	LOSS [training: 1.5710482732357054 | validation: 1.4207632768453737]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_453.pth
	Model improved!!!
EPOCH 454/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5717768973698019		[learning rate: 0.0027619]
	Learning Rate: 0.00276192
	LOSS [training: 1.5717768973698019 | validation: 1.479719365435189]
	TIME [epoch: 8.55 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5597312496563043		[learning rate: 0.0027519]
	Learning Rate: 0.00275189
	LOSS [training: 1.5597312496563043 | validation: 1.442068070928728]
	TIME [epoch: 8.53 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5494813365385078		[learning rate: 0.0027419]
	Learning Rate: 0.00274191
	LOSS [training: 1.5494813365385078 | validation: 1.3762722776582617]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5239961811564111		[learning rate: 0.002732]
	Learning Rate: 0.00273196
	LOSS [training: 1.5239961811564111 | validation: 1.4036443781974208]
	TIME [epoch: 8.54 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.5259846396419696		[learning rate: 0.002722]
	Learning Rate: 0.00272204
	LOSS [training: 1.5259846396419696 | validation: 1.3111981147144773]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4752776530239962		[learning rate: 0.0027122]
	Learning Rate: 0.00271216
	LOSS [training: 1.4752776530239962 | validation: 1.334450132742472]
	TIME [epoch: 8.52 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4535904723577369		[learning rate: 0.0027023]
	Learning Rate: 0.00270232
	LOSS [training: 1.4535904723577369 | validation: 1.2859994302033348]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4400910442106607		[learning rate: 0.0026925]
	Learning Rate: 0.00269251
	LOSS [training: 1.4400910442106607 | validation: 1.2727905463290825]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_461.pth
	Model improved!!!
EPOCH 462/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.4706022137985062		[learning rate: 0.0026827]
	Learning Rate: 0.00268274
	LOSS [training: 1.4706022137985062 | validation: 1.2686001733835575]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3995701384648418		[learning rate: 0.002673]
	Learning Rate: 0.00267301
	LOSS [training: 1.3995701384648418 | validation: 1.1804332881688775]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3852203112126271		[learning rate: 0.0026633]
	Learning Rate: 0.00266331
	LOSS [training: 1.3852203112126271 | validation: 1.2401149787254153]
	TIME [epoch: 8.54 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3758539960720455		[learning rate: 0.0026536]
	Learning Rate: 0.00265364
	LOSS [training: 1.3758539960720455 | validation: 1.2297196992127861]
	TIME [epoch: 8.57 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3637185842719712		[learning rate: 0.002644]
	Learning Rate: 0.00264401
	LOSS [training: 1.3637185842719712 | validation: 1.2435499061661133]
	TIME [epoch: 8.55 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.3537006867944847		[learning rate: 0.0026344]
	Learning Rate: 0.00263441
	LOSS [training: 1.3537006867944847 | validation: 1.1515765391016972]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.277796917043011		[learning rate: 0.0026249]
	Learning Rate: 0.00262485
	LOSS [training: 1.277796917043011 | validation: 1.143845286786721]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.27010609459388		[learning rate: 0.0026153]
	Learning Rate: 0.00261533
	LOSS [training: 1.27010609459388 | validation: 1.0545391691463653]
	TIME [epoch: 8.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.242756588055141		[learning rate: 0.0026058]
	Learning Rate: 0.00260584
	LOSS [training: 1.242756588055141 | validation: 1.0418602948120756]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2405272182894718		[learning rate: 0.0025964]
	Learning Rate: 0.00259638
	LOSS [training: 1.2405272182894718 | validation: 1.0109462371612161]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.2058245320044514		[learning rate: 0.002587]
	Learning Rate: 0.00258696
	LOSS [training: 1.2058245320044514 | validation: 0.9579998273059507]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1848611398617586		[learning rate: 0.0025776]
	Learning Rate: 0.00257757
	LOSS [training: 1.1848611398617586 | validation: 0.9805477745551001]
	TIME [epoch: 8.56 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.1978067975236506		[learning rate: 0.0025682]
	Learning Rate: 0.00256822
	LOSS [training: 1.1978067975236506 | validation: 0.8888762836181732]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0637829381499153		[learning rate: 0.0025589]
	Learning Rate: 0.0025589
	LOSS [training: 1.0637829381499153 | validation: 0.8765867150167805]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0707172140903392		[learning rate: 0.0025496]
	Learning Rate: 0.00254961
	LOSS [training: 1.0707172140903392 | validation: 0.8727985116253019]
	TIME [epoch: 8.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.012127317201879		[learning rate: 0.0025404]
	Learning Rate: 0.00254036
	LOSS [training: 1.012127317201879 | validation: 1.0271569259244482]
	TIME [epoch: 8.55 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.027792444799752		[learning rate: 0.0025311]
	Learning Rate: 0.00253114
	LOSS [training: 1.027792444799752 | validation: 0.8091157905811194]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9755821878062838		[learning rate: 0.002522]
	Learning Rate: 0.00252195
	LOSS [training: 0.9755821878062838 | validation: 0.8113302069243559]
	TIME [epoch: 8.55 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9467153978098534		[learning rate: 0.0025128]
	Learning Rate: 0.0025128
	LOSS [training: 0.9467153978098534 | validation: 0.9005109391851887]
	TIME [epoch: 8.58 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9528296401649682		[learning rate: 0.0025037]
	Learning Rate: 0.00250368
	LOSS [training: 0.9528296401649682 | validation: 0.8146441222300025]
	TIME [epoch: 8.54 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9473885904805723		[learning rate: 0.0024946]
	Learning Rate: 0.00249459
	LOSS [training: 0.9473885904805723 | validation: 0.8191870900266879]
	TIME [epoch: 8.54 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9403543769039387		[learning rate: 0.0024855]
	Learning Rate: 0.00248554
	LOSS [training: 0.9403543769039387 | validation: 0.7653988109148853]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.900748595956842		[learning rate: 0.0024765]
	Learning Rate: 0.00247652
	LOSS [training: 0.900748595956842 | validation: 0.7867196606921212]
	TIME [epoch: 8.57 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9178410007254628		[learning rate: 0.0024675]
	Learning Rate: 0.00246753
	LOSS [training: 0.9178410007254628 | validation: 0.678010345573281]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8722209807890943		[learning rate: 0.0024586]
	Learning Rate: 0.00245858
	LOSS [training: 0.8722209807890943 | validation: 0.6572157051152099]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8663415940708703		[learning rate: 0.0024497]
	Learning Rate: 0.00244966
	LOSS [training: 0.8663415940708703 | validation: 0.6805341862764155]
	TIME [epoch: 8.55 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8857618183979442		[learning rate: 0.0024408]
	Learning Rate: 0.00244077
	LOSS [training: 0.8857618183979442 | validation: 0.6756270713500362]
	TIME [epoch: 8.55 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8567159482618667		[learning rate: 0.0024319]
	Learning Rate: 0.00243191
	LOSS [training: 0.8567159482618667 | validation: 0.7305711614104747]
	TIME [epoch: 8.53 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8463098497763303		[learning rate: 0.0024231]
	Learning Rate: 0.00242308
	LOSS [training: 0.8463098497763303 | validation: 0.7892789320535878]
	TIME [epoch: 8.54 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8660367463009546		[learning rate: 0.0024143]
	Learning Rate: 0.00241429
	LOSS [training: 0.8660367463009546 | validation: 0.8200304226223994]
	TIME [epoch: 8.55 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8939055533657129		[learning rate: 0.0024055]
	Learning Rate: 0.00240553
	LOSS [training: 0.8939055533657129 | validation: 0.6454315565808281]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8411574285177007		[learning rate: 0.0023968]
	Learning Rate: 0.0023968
	LOSS [training: 0.8411574285177007 | validation: 0.709821370407759]
	TIME [epoch: 8.53 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8888786104049394		[learning rate: 0.0023881]
	Learning Rate: 0.0023881
	LOSS [training: 0.8888786104049394 | validation: 0.839675145941063]
	TIME [epoch: 8.54 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9070222311480333		[learning rate: 0.0023794]
	Learning Rate: 0.00237943
	LOSS [training: 0.9070222311480333 | validation: 0.7202713157969323]
	TIME [epoch: 8.55 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8419329221997257		[learning rate: 0.0023708]
	Learning Rate: 0.0023708
	LOSS [training: 0.8419329221997257 | validation: 0.7147924382652626]
	TIME [epoch: 8.54 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8464304252163327		[learning rate: 0.0023622]
	Learning Rate: 0.0023622
	LOSS [training: 0.8464304252163327 | validation: 0.7138656970995723]
	TIME [epoch: 8.53 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.848016329890274		[learning rate: 0.0023536]
	Learning Rate: 0.00235362
	LOSS [training: 0.848016329890274 | validation: 0.590728638292082]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.79327481151006		[learning rate: 0.0023451]
	Learning Rate: 0.00234508
	LOSS [training: 0.79327481151006 | validation: 0.6314921327639369]
	TIME [epoch: 8.55 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8104903002803772		[learning rate: 0.0023366]
	Learning Rate: 0.00233657
	LOSS [training: 0.8104903002803772 | validation: 0.7511630864108592]
	TIME [epoch: 8.53 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8498703487816643		[learning rate: 0.0023281]
	Learning Rate: 0.00232809
	LOSS [training: 0.8498703487816643 | validation: 0.6765255639405803]
	TIME [epoch: 8.53 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8366455714512583		[learning rate: 0.0023196]
	Learning Rate: 0.00231964
	LOSS [training: 0.8366455714512583 | validation: 0.6281356072381233]
	TIME [epoch: 8.55 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8633417060312141		[learning rate: 0.0023112]
	Learning Rate: 0.00231122
	LOSS [training: 0.8633417060312141 | validation: 0.7179886064209993]
	TIME [epoch: 8.55 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.825207166453764		[learning rate: 0.0023028]
	Learning Rate: 0.00230284
	LOSS [training: 0.825207166453764 | validation: 0.7121470784481677]
	TIME [epoch: 8.53 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8729401894244004		[learning rate: 0.0022945]
	Learning Rate: 0.00229448
	LOSS [training: 0.8729401894244004 | validation: 0.6442506134165548]
	TIME [epoch: 8.54 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9915028356740357		[learning rate: 0.0022862]
	Learning Rate: 0.00228615
	LOSS [training: 0.9915028356740357 | validation: 0.5970826274523686]
	TIME [epoch: 8.54 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8030868760027854		[learning rate: 0.0022779]
	Learning Rate: 0.00227786
	LOSS [training: 0.8030868760027854 | validation: 0.6683834792211086]
	TIME [epoch: 8.55 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.836832885541401		[learning rate: 0.0022696]
	Learning Rate: 0.00226959
	LOSS [training: 0.836832885541401 | validation: 0.6291969921482305]
	TIME [epoch: 8.53 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8309215338878607		[learning rate: 0.0022614]
	Learning Rate: 0.00226135
	LOSS [training: 0.8309215338878607 | validation: 0.7511486385389798]
	TIME [epoch: 8.54 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8465531278170323		[learning rate: 0.0022531]
	Learning Rate: 0.00225315
	LOSS [training: 0.8465531278170323 | validation: 0.6269386432064481]
	TIME [epoch: 8.54 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8178402838334538		[learning rate: 0.002245]
	Learning Rate: 0.00224497
	LOSS [training: 0.8178402838334538 | validation: 0.6437845254657861]
	TIME [epoch: 8.55 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.812840841012281		[learning rate: 0.0022368]
	Learning Rate: 0.00223682
	LOSS [training: 0.812840841012281 | validation: 0.6233220178704197]
	TIME [epoch: 8.53 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8243071808178962		[learning rate: 0.0022287]
	Learning Rate: 0.00222871
	LOSS [training: 0.8243071808178962 | validation: 0.6801961014346662]
	TIME [epoch: 8.54 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8678317546750243		[learning rate: 0.0022206]
	Learning Rate: 0.00222062
	LOSS [training: 0.8678317546750243 | validation: 0.6009213854723746]
	TIME [epoch: 8.53 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8576665033957752		[learning rate: 0.0022126]
	Learning Rate: 0.00221256
	LOSS [training: 0.8576665033957752 | validation: 0.6189079258619697]
	TIME [epoch: 8.55 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8111522381148939		[learning rate: 0.0022045]
	Learning Rate: 0.00220453
	LOSS [training: 0.8111522381148939 | validation: 0.6028254798069581]
	TIME [epoch: 8.53 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8060964759845748		[learning rate: 0.0021965]
	Learning Rate: 0.00219653
	LOSS [training: 0.8060964759845748 | validation: 0.5914938116904156]
	TIME [epoch: 8.55 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8092457085925566		[learning rate: 0.0021886]
	Learning Rate: 0.00218856
	LOSS [training: 0.8092457085925566 | validation: 0.5999696377405976]
	TIME [epoch: 8.53 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8914232752317466		[learning rate: 0.0021806]
	Learning Rate: 0.00218061
	LOSS [training: 0.8914232752317466 | validation: 0.5902406312260965]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8362973488528859		[learning rate: 0.0021727]
	Learning Rate: 0.0021727
	LOSS [training: 0.8362973488528859 | validation: 0.5960401714522933]
	TIME [epoch: 8.55 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8184344724837358		[learning rate: 0.0021648]
	Learning Rate: 0.00216482
	LOSS [training: 0.8184344724837358 | validation: 0.6024553523998715]
	TIME [epoch: 8.53 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.772170116017494		[learning rate: 0.002157]
	Learning Rate: 0.00215696
	LOSS [training: 0.772170116017494 | validation: 0.5633046837906556]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_522.pth
	Model improved!!!
EPOCH 523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.798324569402389		[learning rate: 0.0021491]
	Learning Rate: 0.00214913
	LOSS [training: 0.798324569402389 | validation: 0.6386021770106114]
	TIME [epoch: 8.54 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7741324063322204		[learning rate: 0.0021413]
	Learning Rate: 0.00214133
	LOSS [training: 0.7741324063322204 | validation: 0.5830958247996181]
	TIME [epoch: 8.53 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.777045467294079		[learning rate: 0.0021336]
	Learning Rate: 0.00213356
	LOSS [training: 0.777045467294079 | validation: 0.5572557883807846]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_525.pth
	Model improved!!!
EPOCH 526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8083946038832825		[learning rate: 0.0021258]
	Learning Rate: 0.00212582
	LOSS [training: 0.8083946038832825 | validation: 0.6769403942098602]
	TIME [epoch: 8.56 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.813523043783758		[learning rate: 0.0021181]
	Learning Rate: 0.0021181
	LOSS [training: 0.813523043783758 | validation: 0.5996808893237965]
	TIME [epoch: 8.56 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7911436048216898		[learning rate: 0.0021104]
	Learning Rate: 0.00211042
	LOSS [training: 0.7911436048216898 | validation: 0.6159209881327129]
	TIME [epoch: 8.54 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.9717924819144385		[learning rate: 0.0021028]
	Learning Rate: 0.00210276
	LOSS [training: 0.9717924819144385 | validation: 0.5162988535527964]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8262233930267282		[learning rate: 0.0020951]
	Learning Rate: 0.00209513
	LOSS [training: 0.8262233930267282 | validation: 0.6249325016985114]
	TIME [epoch: 8.57 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.832831542939098		[learning rate: 0.0020875]
	Learning Rate: 0.00208752
	LOSS [training: 0.832831542939098 | validation: 0.5660293748812424]
	TIME [epoch: 8.55 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7683886210038195		[learning rate: 0.0020799]
	Learning Rate: 0.00207995
	LOSS [training: 0.7683886210038195 | validation: 0.5545591922387265]
	TIME [epoch: 8.54 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8046524447315022		[learning rate: 0.0020724]
	Learning Rate: 0.0020724
	LOSS [training: 0.8046524447315022 | validation: 0.5834168275541058]
	TIME [epoch: 8.54 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7790080638337703		[learning rate: 0.0020649]
	Learning Rate: 0.00206488
	LOSS [training: 0.7790080638337703 | validation: 0.5782391335738669]
	TIME [epoch: 8.58 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7568907518023253		[learning rate: 0.0020574]
	Learning Rate: 0.00205739
	LOSS [training: 0.7568907518023253 | validation: 0.8141828945516224]
	TIME [epoch: 8.55 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/10] avg loss: 1.0319148626813706		[learning rate: 0.0020499]
	Learning Rate: 0.00204992
	LOSS [training: 1.0319148626813706 | validation: 0.6172719385030698]
	TIME [epoch: 8.54 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7715815037586007		[learning rate: 0.0020425]
	Learning Rate: 0.00204248
	LOSS [training: 0.7715815037586007 | validation: 0.5832580721058943]
	TIME [epoch: 8.54 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7872575022738104		[learning rate: 0.0020351]
	Learning Rate: 0.00203507
	LOSS [training: 0.7872575022738104 | validation: 0.6276797344177396]
	TIME [epoch: 8.59 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7873727505302537		[learning rate: 0.0020277]
	Learning Rate: 0.00202768
	LOSS [training: 0.7873727505302537 | validation: 0.6077425366095008]
	TIME [epoch: 8.54 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7692840071172286		[learning rate: 0.0020203]
	Learning Rate: 0.00202032
	LOSS [training: 0.7692840071172286 | validation: 0.5622279725091869]
	TIME [epoch: 8.54 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7914810468212037		[learning rate: 0.002013]
	Learning Rate: 0.00201299
	LOSS [training: 0.7914810468212037 | validation: 0.6231787149425467]
	TIME [epoch: 8.55 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7812102636933385		[learning rate: 0.0020057]
	Learning Rate: 0.00200569
	LOSS [training: 0.7812102636933385 | validation: 0.5939771472519197]
	TIME [epoch: 8.58 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7907383179062036		[learning rate: 0.0019984]
	Learning Rate: 0.00199841
	LOSS [training: 0.7907383179062036 | validation: 0.5622410707620032]
	TIME [epoch: 8.54 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7934738153292633		[learning rate: 0.0019912]
	Learning Rate: 0.00199116
	LOSS [training: 0.7934738153292633 | validation: 0.5267088713657857]
	TIME [epoch: 8.54 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7797279169545975		[learning rate: 0.0019839]
	Learning Rate: 0.00198393
	LOSS [training: 0.7797279169545975 | validation: 0.5822440977814043]
	TIME [epoch: 8.55 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7514190876086939		[learning rate: 0.0019767]
	Learning Rate: 0.00197673
	LOSS [training: 0.7514190876086939 | validation: 0.5718722584234576]
	TIME [epoch: 8.57 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8247414535086717		[learning rate: 0.0019696]
	Learning Rate: 0.00196956
	LOSS [training: 0.8247414535086717 | validation: 0.7517386477757144]
	TIME [epoch: 8.54 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7752920555339372		[learning rate: 0.0019624]
	Learning Rate: 0.00196241
	LOSS [training: 0.7752920555339372 | validation: 0.5301059061758583]
	TIME [epoch: 8.54 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7804873821720062		[learning rate: 0.0019553]
	Learning Rate: 0.00195529
	LOSS [training: 0.7804873821720062 | validation: 0.6860430465393251]
	TIME [epoch: 8.57 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8204429969730441		[learning rate: 0.0019482]
	Learning Rate: 0.00194819
	LOSS [training: 0.8204429969730441 | validation: 1.0130586981159513]
	TIME [epoch: 8.55 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8090740116137087		[learning rate: 0.0019411]
	Learning Rate: 0.00194112
	LOSS [training: 0.8090740116137087 | validation: 0.508594221056828]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_551.pth
	Model improved!!!
EPOCH 552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7237025751340289		[learning rate: 0.0019341]
	Learning Rate: 0.00193408
	LOSS [training: 0.7237025751340289 | validation: 0.604729825050427]
	TIME [epoch: 8.54 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7624780184404749		[learning rate: 0.0019271]
	Learning Rate: 0.00192706
	LOSS [training: 0.7624780184404749 | validation: 0.5111322660142759]
	TIME [epoch: 8.56 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7449932375237601		[learning rate: 0.0019201]
	Learning Rate: 0.00192006
	LOSS [training: 0.7449932375237601 | validation: 0.5938115917741744]
	TIME [epoch: 8.53 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7824666606187853		[learning rate: 0.0019131]
	Learning Rate: 0.0019131
	LOSS [training: 0.7824666606187853 | validation: 0.5706148958829527]
	TIME [epoch: 8.53 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7739761134407315		[learning rate: 0.0019062]
	Learning Rate: 0.00190615
	LOSS [training: 0.7739761134407315 | validation: 0.5741209387697077]
	TIME [epoch: 8.55 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8018035639190877		[learning rate: 0.0018992]
	Learning Rate: 0.00189924
	LOSS [training: 0.8018035639190877 | validation: 0.5751576887665932]
	TIME [epoch: 8.56 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7917273405528938		[learning rate: 0.0018923]
	Learning Rate: 0.00189234
	LOSS [training: 0.7917273405528938 | validation: 0.5821973039968987]
	TIME [epoch: 8.54 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7505537904544448		[learning rate: 0.0018855]
	Learning Rate: 0.00188548
	LOSS [training: 0.7505537904544448 | validation: 0.6080580669320085]
	TIME [epoch: 8.53 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7392975153948068		[learning rate: 0.0018786]
	Learning Rate: 0.00187863
	LOSS [training: 0.7392975153948068 | validation: 0.5185453832136268]
	TIME [epoch: 8.55 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8354456371428365		[learning rate: 0.0018718]
	Learning Rate: 0.00187182
	LOSS [training: 0.8354456371428365 | validation: 0.5427354598614992]
	TIME [epoch: 8.56 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7385746313749343		[learning rate: 0.001865]
	Learning Rate: 0.00186502
	LOSS [training: 0.7385746313749343 | validation: 0.5033495815769715]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7624859183247674		[learning rate: 0.0018583]
	Learning Rate: 0.00185825
	LOSS [training: 0.7624859183247674 | validation: 0.497700407908444]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6964011083469475		[learning rate: 0.0018515]
	Learning Rate: 0.00185151
	LOSS [training: 0.6964011083469475 | validation: 0.576841302603188]
	TIME [epoch: 8.54 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7253964318272479		[learning rate: 0.0018448]
	Learning Rate: 0.00184479
	LOSS [training: 0.7253964318272479 | validation: 0.6234744259353187]
	TIME [epoch: 8.55 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7398703994541829		[learning rate: 0.0018381]
	Learning Rate: 0.0018381
	LOSS [training: 0.7398703994541829 | validation: 0.5173185258860932]
	TIME [epoch: 8.53 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7713738940075137		[learning rate: 0.0018314]
	Learning Rate: 0.00183143
	LOSS [training: 0.7713738940075137 | validation: 0.5081805320577892]
	TIME [epoch: 8.54 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7102947483085105		[learning rate: 0.0018248]
	Learning Rate: 0.00182478
	LOSS [training: 0.7102947483085105 | validation: 0.6169528025049271]
	TIME [epoch: 8.54 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6995030274032167		[learning rate: 0.0018182]
	Learning Rate: 0.00181816
	LOSS [training: 0.6995030274032167 | validation: 0.4975140974193166]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.775259528988727		[learning rate: 0.0018116]
	Learning Rate: 0.00181156
	LOSS [training: 0.775259528988727 | validation: 0.5161175173425259]
	TIME [epoch: 8.54 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6981336063898397		[learning rate: 0.001805]
	Learning Rate: 0.00180499
	LOSS [training: 0.6981336063898397 | validation: 0.5490670639156017]
	TIME [epoch: 8.55 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7216499864263344		[learning rate: 0.0017984]
	Learning Rate: 0.00179844
	LOSS [training: 0.7216499864263344 | validation: 0.6926192113112654]
	TIME [epoch: 8.55 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7480445799471398		[learning rate: 0.0017919]
	Learning Rate: 0.00179191
	LOSS [training: 0.7480445799471398 | validation: 0.5755165712798945]
	TIME [epoch: 8.55 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.685338222960777		[learning rate: 0.0017854]
	Learning Rate: 0.00178541
	LOSS [training: 0.685338222960777 | validation: 0.5457038049036094]
	TIME [epoch: 8.54 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.691071595469275		[learning rate: 0.0017789]
	Learning Rate: 0.00177893
	LOSS [training: 0.691071595469275 | validation: 0.5804343997121175]
	TIME [epoch: 8.56 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.708413297706394		[learning rate: 0.0017725]
	Learning Rate: 0.00177247
	LOSS [training: 0.708413297706394 | validation: 0.6262778578878309]
	TIME [epoch: 8.57 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.742195718774361		[learning rate: 0.001766]
	Learning Rate: 0.00176604
	LOSS [training: 0.742195718774361 | validation: 0.49671313917684745]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7096586018673777		[learning rate: 0.0017596]
	Learning Rate: 0.00175963
	LOSS [training: 0.7096586018673777 | validation: 0.5108481023279937]
	TIME [epoch: 8.54 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7122799205569258		[learning rate: 0.0017532]
	Learning Rate: 0.00175324
	LOSS [training: 0.7122799205569258 | validation: 0.5102502505941726]
	TIME [epoch: 8.55 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6979763995420661		[learning rate: 0.0017469]
	Learning Rate: 0.00174688
	LOSS [training: 0.6979763995420661 | validation: 0.48858497489548935]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.726788944246137		[learning rate: 0.0017405]
	Learning Rate: 0.00174054
	LOSS [training: 0.726788944246137 | validation: 0.550524320745736]
	TIME [epoch: 8.54 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7003633486801834		[learning rate: 0.0017342]
	Learning Rate: 0.00173422
	LOSS [training: 0.7003633486801834 | validation: 0.5922308670201466]
	TIME [epoch: 8.55 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6628218124284633		[learning rate: 0.0017279]
	Learning Rate: 0.00172793
	LOSS [training: 0.6628218124284633 | validation: 0.47976408797637754]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6876046421234978		[learning rate: 0.0017217]
	Learning Rate: 0.00172166
	LOSS [training: 0.6876046421234978 | validation: 0.527608341037396]
	TIME [epoch: 8.56 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6841464228142942		[learning rate: 0.0017154]
	Learning Rate: 0.00171541
	LOSS [training: 0.6841464228142942 | validation: 0.5135966128219421]
	TIME [epoch: 8.53 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7198776278601121		[learning rate: 0.0017092]
	Learning Rate: 0.00170919
	LOSS [training: 0.7198776278601121 | validation: 0.5211713988473812]
	TIME [epoch: 8.55 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7196311394851974		[learning rate: 0.001703]
	Learning Rate: 0.00170298
	LOSS [training: 0.7196311394851974 | validation: 0.4782736569596706]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6951232514731605		[learning rate: 0.0016968]
	Learning Rate: 0.0016968
	LOSS [training: 0.6951232514731605 | validation: 0.49595255976225433]
	TIME [epoch: 8.56 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6881230510602219		[learning rate: 0.0016906]
	Learning Rate: 0.00169065
	LOSS [training: 0.6881230510602219 | validation: 0.5563874049003112]
	TIME [epoch: 8.54 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7309287939783056		[learning rate: 0.0016845]
	Learning Rate: 0.00168451
	LOSS [training: 0.7309287939783056 | validation: 0.5372178863890412]
	TIME [epoch: 8.56 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7086544634449046		[learning rate: 0.0016784]
	Learning Rate: 0.0016784
	LOSS [training: 0.7086544634449046 | validation: 0.599524897532345]
	TIME [epoch: 8.53 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6854288280607793		[learning rate: 0.0016723]
	Learning Rate: 0.00167231
	LOSS [training: 0.6854288280607793 | validation: 0.5438739781312429]
	TIME [epoch: 8.56 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6754446589898603		[learning rate: 0.0016662]
	Learning Rate: 0.00166624
	LOSS [training: 0.6754446589898603 | validation: 0.4899684465779691]
	TIME [epoch: 8.55 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6721419702387784		[learning rate: 0.0016602]
	Learning Rate: 0.00166019
	LOSS [training: 0.6721419702387784 | validation: 0.544299089789086]
	TIME [epoch: 8.55 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7712065393725056		[learning rate: 0.0016542]
	Learning Rate: 0.00165417
	LOSS [training: 0.7712065393725056 | validation: 0.5834161067819716]
	TIME [epoch: 8.55 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.684887477206556		[learning rate: 0.0016482]
	Learning Rate: 0.00164816
	LOSS [training: 0.684887477206556 | validation: 0.5416292291183004]
	TIME [epoch: 8.55 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8454309236058759		[learning rate: 0.0016422]
	Learning Rate: 0.00164218
	LOSS [training: 0.8454309236058759 | validation: 0.500447913534475]
	TIME [epoch: 8.55 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.646645292913208		[learning rate: 0.0016362]
	Learning Rate: 0.00163622
	LOSS [training: 0.646645292913208 | validation: 0.4717922474593866]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6489823189249003		[learning rate: 0.0016303]
	Learning Rate: 0.00163028
	LOSS [training: 0.6489823189249003 | validation: 0.45347083319705217]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7213154530933205		[learning rate: 0.0016244]
	Learning Rate: 0.00162437
	LOSS [training: 0.7213154530933205 | validation: 0.5901832736191367]
	TIME [epoch: 8.54 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.67084273382107		[learning rate: 0.0016185]
	Learning Rate: 0.00161847
	LOSS [training: 0.67084273382107 | validation: 0.4557905001234856]
	TIME [epoch: 8.55 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6802905103131475		[learning rate: 0.0016126]
	Learning Rate: 0.0016126
	LOSS [training: 0.6802905103131475 | validation: 0.5761052965529576]
	TIME [epoch: 8.53 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.7035085299020379		[learning rate: 0.0016067]
	Learning Rate: 0.00160675
	LOSS [training: 0.7035085299020379 | validation: 0.48404090223508095]
	TIME [epoch: 8.56 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6494109357777021		[learning rate: 0.0016009]
	Learning Rate: 0.00160092
	LOSS [training: 0.6494109357777021 | validation: 0.49609323881006795]
	TIME [epoch: 8.53 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.660071662225218		[learning rate: 0.0015951]
	Learning Rate: 0.00159511
	LOSS [training: 0.660071662225218 | validation: 0.4702013204824282]
	TIME [epoch: 8.56 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.672710819169712		[learning rate: 0.0015893]
	Learning Rate: 0.00158932
	LOSS [training: 0.672710819169712 | validation: 0.5029648481702088]
	TIME [epoch: 8.53 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.672279210038693		[learning rate: 0.0015835]
	Learning Rate: 0.00158355
	LOSS [training: 0.672279210038693 | validation: 0.5108130183668124]
	TIME [epoch: 8.56 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6743830180247359		[learning rate: 0.0015778]
	Learning Rate: 0.0015778
	LOSS [training: 0.6743830180247359 | validation: 0.4747976229076293]
	TIME [epoch: 8.56 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.692908224434433		[learning rate: 0.0015721]
	Learning Rate: 0.00157208
	LOSS [training: 0.692908224434433 | validation: 0.7641878266045556]
	TIME [epoch: 8.55 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6969446401917014		[learning rate: 0.0015664]
	Learning Rate: 0.00156637
	LOSS [training: 0.6969446401917014 | validation: 0.5005270471133442]
	TIME [epoch: 8.55 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6566666306174006		[learning rate: 0.0015607]
	Learning Rate: 0.00156069
	LOSS [training: 0.6566666306174006 | validation: 0.4856019287431764]
	TIME [epoch: 8.56 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6289142265458763		[learning rate: 0.001555]
	Learning Rate: 0.00155502
	LOSS [training: 0.6289142265458763 | validation: 0.46129873203338345]
	TIME [epoch: 8.56 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.8314607386591438		[learning rate: 0.0015494]
	Learning Rate: 0.00154938
	LOSS [training: 0.8314607386591438 | validation: 0.49480647926875443]
	TIME [epoch: 8.54 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6361463421633099		[learning rate: 0.0015438]
	Learning Rate: 0.00154376
	LOSS [training: 0.6361463421633099 | validation: 0.4811010060771858]
	TIME [epoch: 8.54 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6368842556036446		[learning rate: 0.0015382]
	Learning Rate: 0.00153815
	LOSS [training: 0.6368842556036446 | validation: 0.4963057853563456]
	TIME [epoch: 8.56 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6290740375478856		[learning rate: 0.0015326]
	Learning Rate: 0.00153257
	LOSS [training: 0.6290740375478856 | validation: 0.6407255064044224]
	TIME [epoch: 8.57 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6667590782207229		[learning rate: 0.001527]
	Learning Rate: 0.00152701
	LOSS [training: 0.6667590782207229 | validation: 0.46478028877563565]
	TIME [epoch: 8.54 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6284764891802692		[learning rate: 0.0015215]
	Learning Rate: 0.00152147
	LOSS [training: 0.6284764891802692 | validation: 0.44399141324650215]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_618.pth
	Model improved!!!
EPOCH 619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6112901527798428		[learning rate: 0.0015159]
	Learning Rate: 0.00151595
	LOSS [training: 0.6112901527798428 | validation: 0.46683303726718384]
	TIME [epoch: 8.57 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6287812576951691		[learning rate: 0.0015104]
	Learning Rate: 0.00151045
	LOSS [training: 0.6287812576951691 | validation: 0.47213643289854734]
	TIME [epoch: 8.54 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6327506147398264		[learning rate: 0.001505]
	Learning Rate: 0.00150496
	LOSS [training: 0.6327506147398264 | validation: 0.5347762781986569]
	TIME [epoch: 8.53 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6525581148357424		[learning rate: 0.0014995]
	Learning Rate: 0.0014995
	LOSS [training: 0.6525581148357424 | validation: 0.42330031023789183]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6295027145272287		[learning rate: 0.0014941]
	Learning Rate: 0.00149406
	LOSS [training: 0.6295027145272287 | validation: 0.479928527455807]
	TIME [epoch: 8.55 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5991035888616605		[learning rate: 0.0014886]
	Learning Rate: 0.00148864
	LOSS [training: 0.5991035888616605 | validation: 0.4458953945566977]
	TIME [epoch: 8.52 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6636702412109872		[learning rate: 0.0014832]
	Learning Rate: 0.00148324
	LOSS [training: 0.6636702412109872 | validation: 0.43751768878677777]
	TIME [epoch: 8.52 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6007006505708514		[learning rate: 0.0014779]
	Learning Rate: 0.00147785
	LOSS [training: 0.6007006505708514 | validation: 0.558393910950426]
	TIME [epoch: 8.54 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6163702456641718		[learning rate: 0.0014725]
	Learning Rate: 0.00147249
	LOSS [training: 0.6163702456641718 | validation: 0.5519334682778781]
	TIME [epoch: 8.55 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6680783814397279		[learning rate: 0.0014671]
	Learning Rate: 0.00146715
	LOSS [training: 0.6680783814397279 | validation: 0.4449883030709262]
	TIME [epoch: 8.52 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.642466812736866		[learning rate: 0.0014618]
	Learning Rate: 0.00146182
	LOSS [training: 0.642466812736866 | validation: 0.40771320745051887]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6659895213762048		[learning rate: 0.0014565]
	Learning Rate: 0.00145652
	LOSS [training: 0.6659895213762048 | validation: 0.5701951694647616]
	TIME [epoch: 8.57 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5952006233195525		[learning rate: 0.0014512]
	Learning Rate: 0.00145123
	LOSS [training: 0.5952006233195525 | validation: 0.4318425624932763]
	TIME [epoch: 8.54 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6015352341927064		[learning rate: 0.001446]
	Learning Rate: 0.00144597
	LOSS [training: 0.6015352341927064 | validation: 0.48631818817906075]
	TIME [epoch: 8.53 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6201569075582763		[learning rate: 0.0014407]
	Learning Rate: 0.00144072
	LOSS [training: 0.6201569075582763 | validation: 0.4900512358775049]
	TIME [epoch: 8.53 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6111053145722638		[learning rate: 0.0014355]
	Learning Rate: 0.00143549
	LOSS [training: 0.6111053145722638 | validation: 0.4223267034638894]
	TIME [epoch: 8.57 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6369113281912272		[learning rate: 0.0014303]
	Learning Rate: 0.00143028
	LOSS [training: 0.6369113281912272 | validation: 0.44925510057746865]
	TIME [epoch: 8.54 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6219412612343131		[learning rate: 0.0014251]
	Learning Rate: 0.00142509
	LOSS [training: 0.6219412612343131 | validation: 0.45614357936741146]
	TIME [epoch: 8.54 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5970697269060375		[learning rate: 0.0014199]
	Learning Rate: 0.00141992
	LOSS [training: 0.5970697269060375 | validation: 0.4710524340748296]
	TIME [epoch: 8.53 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6248252017424066		[learning rate: 0.0014148]
	Learning Rate: 0.00141476
	LOSS [training: 0.6248252017424066 | validation: 0.40587053175835275]
	TIME [epoch: 8.58 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5946468525235323		[learning rate: 0.0014096]
	Learning Rate: 0.00140963
	LOSS [training: 0.5946468525235323 | validation: 0.45565724618771875]
	TIME [epoch: 8.54 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6030617408759236		[learning rate: 0.0014045]
	Learning Rate: 0.00140451
	LOSS [training: 0.6030617408759236 | validation: 0.527957297388891]
	TIME [epoch: 8.53 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6215796966405528		[learning rate: 0.0013994]
	Learning Rate: 0.00139942
	LOSS [training: 0.6215796966405528 | validation: 0.5662131061520068]
	TIME [epoch: 8.54 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5736601299501314		[learning rate: 0.0013943]
	Learning Rate: 0.00139434
	LOSS [training: 0.5736601299501314 | validation: 0.5368204025799166]
	TIME [epoch: 8.56 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5884794920753617		[learning rate: 0.0013893]
	Learning Rate: 0.00138928
	LOSS [training: 0.5884794920753617 | validation: 0.5305851667105698]
	TIME [epoch: 8.53 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6246156798014233		[learning rate: 0.0013842]
	Learning Rate: 0.00138424
	LOSS [training: 0.6246156798014233 | validation: 0.5717946872925219]
	TIME [epoch: 8.53 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6089913489140767		[learning rate: 0.0013792]
	Learning Rate: 0.00137921
	LOSS [training: 0.6089913489140767 | validation: 0.4119405208778925]
	TIME [epoch: 8.55 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5855256553878556		[learning rate: 0.0013742]
	Learning Rate: 0.00137421
	LOSS [training: 0.5855256553878556 | validation: 0.42821290680844815]
	TIME [epoch: 8.55 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6108751657925496		[learning rate: 0.0013692]
	Learning Rate: 0.00136922
	LOSS [training: 0.6108751657925496 | validation: 0.5504087926268892]
	TIME [epoch: 8.53 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5921599599782785		[learning rate: 0.0013643]
	Learning Rate: 0.00136425
	LOSS [training: 0.5921599599782785 | validation: 0.5310666309227027]
	TIME [epoch: 8.52 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5673886464007856		[learning rate: 0.0013593]
	Learning Rate: 0.0013593
	LOSS [training: 0.5673886464007856 | validation: 0.4543459228032273]
	TIME [epoch: 8.57 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6050910472888417		[learning rate: 0.0013544]
	Learning Rate: 0.00135437
	LOSS [training: 0.6050910472888417 | validation: 0.4602478119473519]
	TIME [epoch: 8.53 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6044066229384633		[learning rate: 0.0013495]
	Learning Rate: 0.00134945
	LOSS [training: 0.6044066229384633 | validation: 0.4372429322385507]
	TIME [epoch: 8.52 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5794851582724432		[learning rate: 0.0013446]
	Learning Rate: 0.00134456
	LOSS [training: 0.5794851582724432 | validation: 0.44579308405276374]
	TIME [epoch: 8.53 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5651412867337314		[learning rate: 0.0013397]
	Learning Rate: 0.00133968
	LOSS [training: 0.5651412867337314 | validation: 0.5052752883522567]
	TIME [epoch: 8.57 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.582222318625354		[learning rate: 0.0013348]
	Learning Rate: 0.00133481
	LOSS [training: 0.582222318625354 | validation: 0.4621022329729588]
	TIME [epoch: 8.53 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5708201673143185		[learning rate: 0.00133]
	Learning Rate: 0.00132997
	LOSS [training: 0.5708201673143185 | validation: 0.602555537559592]
	TIME [epoch: 8.53 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6215078750351724		[learning rate: 0.0013251]
	Learning Rate: 0.00132514
	LOSS [training: 0.6215078750351724 | validation: 0.48228994089625665]
	TIME [epoch: 8.54 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.596309018605214		[learning rate: 0.0013203]
	Learning Rate: 0.00132034
	LOSS [training: 0.596309018605214 | validation: 0.5139246899387256]
	TIME [epoch: 8.56 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6365390165106797		[learning rate: 0.0013155]
	Learning Rate: 0.00131554
	LOSS [training: 0.6365390165106797 | validation: 0.4944745473980272]
	TIME [epoch: 8.53 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5799348752304467		[learning rate: 0.0013108]
	Learning Rate: 0.00131077
	LOSS [training: 0.5799348752304467 | validation: 0.5702722619210036]
	TIME [epoch: 8.53 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6137816776184927		[learning rate: 0.001306]
	Learning Rate: 0.00130601
	LOSS [training: 0.6137816776184927 | validation: 0.44486668128888757]
	TIME [epoch: 8.54 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6359249698447268		[learning rate: 0.0013013]
	Learning Rate: 0.00130127
	LOSS [training: 0.6359249698447268 | validation: 0.4729776803991549]
	TIME [epoch: 8.56 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.571772982027993		[learning rate: 0.0012966]
	Learning Rate: 0.00129655
	LOSS [training: 0.571772982027993 | validation: 0.4728923966850262]
	TIME [epoch: 8.53 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5669461104590938		[learning rate: 0.0012918]
	Learning Rate: 0.00129185
	LOSS [training: 0.5669461104590938 | validation: 0.49449328798778636]
	TIME [epoch: 8.53 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5685213582404386		[learning rate: 0.0012872]
	Learning Rate: 0.00128716
	LOSS [training: 0.5685213582404386 | validation: 0.5364330389091817]
	TIME [epoch: 8.55 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5718907331817225		[learning rate: 0.0012825]
	Learning Rate: 0.00128249
	LOSS [training: 0.5718907331817225 | validation: 0.48300346893339685]
	TIME [epoch: 8.55 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6023456041424825		[learning rate: 0.0012778]
	Learning Rate: 0.00127783
	LOSS [training: 0.6023456041424825 | validation: 0.5013623441956624]
	TIME [epoch: 8.53 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5577413609679273		[learning rate: 0.0012732]
	Learning Rate: 0.00127319
	LOSS [training: 0.5577413609679273 | validation: 0.39948348201785466]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5920318866069715		[learning rate: 0.0012686]
	Learning Rate: 0.00126857
	LOSS [training: 0.5920318866069715 | validation: 0.5705265115888651]
	TIME [epoch: 8.55 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6041033091231125		[learning rate: 0.001264]
	Learning Rate: 0.00126397
	LOSS [training: 0.6041033091231125 | validation: 0.5368496804374223]
	TIME [epoch: 8.55 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5381564896705486		[learning rate: 0.0012594]
	Learning Rate: 0.00125938
	LOSS [training: 0.5381564896705486 | validation: 0.4054956791716947]
	TIME [epoch: 8.53 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5279283172407029		[learning rate: 0.0012548]
	Learning Rate: 0.00125481
	LOSS [training: 0.5279283172407029 | validation: 0.3776264002250987]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5560467511409136		[learning rate: 0.0012503]
	Learning Rate: 0.00125026
	LOSS [training: 0.5560467511409136 | validation: 0.42518144225646404]
	TIME [epoch: 8.55 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5431732805437961		[learning rate: 0.0012457]
	Learning Rate: 0.00124572
	LOSS [training: 0.5431732805437961 | validation: 0.45339141817403833]
	TIME [epoch: 8.54 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5585110444209394		[learning rate: 0.0012412]
	Learning Rate: 0.0012412
	LOSS [training: 0.5585110444209394 | validation: 0.42250406647283745]
	TIME [epoch: 8.53 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5256958757798695		[learning rate: 0.0012367]
	Learning Rate: 0.0012367
	LOSS [training: 0.5256958757798695 | validation: 0.41675335914630196]
	TIME [epoch: 8.56 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5494252479947707		[learning rate: 0.0012322]
	Learning Rate: 0.00123221
	LOSS [training: 0.5494252479947707 | validation: 0.4091588859529023]
	TIME [epoch: 8.55 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5759913167528588		[learning rate: 0.0012277]
	Learning Rate: 0.00122774
	LOSS [training: 0.5759913167528588 | validation: 0.4155882431933271]
	TIME [epoch: 8.55 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5249074512641989		[learning rate: 0.0012233]
	Learning Rate: 0.00122328
	LOSS [training: 0.5249074512641989 | validation: 0.3938141320547589]
	TIME [epoch: 8.54 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5420644574514398		[learning rate: 0.0012188]
	Learning Rate: 0.00121884
	LOSS [training: 0.5420644574514398 | validation: 0.43985382140751395]
	TIME [epoch: 8.56 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5353537971167978		[learning rate: 0.0012144]
	Learning Rate: 0.00121442
	LOSS [training: 0.5353537971167978 | validation: 0.384027709769727]
	TIME [epoch: 8.56 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5505950880106213		[learning rate: 0.00121]
	Learning Rate: 0.00121001
	LOSS [training: 0.5505950880106213 | validation: 0.5295782562816904]
	TIME [epoch: 8.54 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.574404385725321		[learning rate: 0.0012056]
	Learning Rate: 0.00120562
	LOSS [training: 0.574404385725321 | validation: 0.3891707413391963]
	TIME [epoch: 8.53 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5501807773544898		[learning rate: 0.0012012]
	Learning Rate: 0.00120125
	LOSS [training: 0.5501807773544898 | validation: 0.39946086674849873]
	TIME [epoch: 8.55 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5483307025930412		[learning rate: 0.0011969]
	Learning Rate: 0.00119689
	LOSS [training: 0.5483307025930412 | validation: 0.4514678868366835]
	TIME [epoch: 8.56 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5249485250038541		[learning rate: 0.0011925]
	Learning Rate: 0.00119254
	LOSS [training: 0.5249485250038541 | validation: 0.43440296649997046]
	TIME [epoch: 8.54 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5863017795338701		[learning rate: 0.0011882]
	Learning Rate: 0.00118821
	LOSS [training: 0.5863017795338701 | validation: 0.48452810574915334]
	TIME [epoch: 8.55 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5211298626263907		[learning rate: 0.0011839]
	Learning Rate: 0.0011839
	LOSS [training: 0.5211298626263907 | validation: 0.382214719041325]
	TIME [epoch: 8.55 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5201873951319083		[learning rate: 0.0011796]
	Learning Rate: 0.00117961
	LOSS [training: 0.5201873951319083 | validation: 0.3940118162730175]
	TIME [epoch: 8.55 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5218220696464566		[learning rate: 0.0011753]
	Learning Rate: 0.00117532
	LOSS [training: 0.5218220696464566 | validation: 0.41818196856884593]
	TIME [epoch: 8.54 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5158356449145406		[learning rate: 0.0011711]
	Learning Rate: 0.00117106
	LOSS [training: 0.5158356449145406 | validation: 0.5730206806472672]
	TIME [epoch: 8.55 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5380223136332998		[learning rate: 0.0011668]
	Learning Rate: 0.00116681
	LOSS [training: 0.5380223136332998 | validation: 0.450572033732322]
	TIME [epoch: 8.55 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5494162194417628		[learning rate: 0.0011626]
	Learning Rate: 0.00116258
	LOSS [training: 0.5494162194417628 | validation: 0.4055617414929452]
	TIME [epoch: 8.56 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5194920295773546		[learning rate: 0.0011584]
	Learning Rate: 0.00115836
	LOSS [training: 0.5194920295773546 | validation: 0.4907377881882065]
	TIME [epoch: 8.54 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5514028656233725		[learning rate: 0.0011542]
	Learning Rate: 0.00115415
	LOSS [training: 0.5514028656233725 | validation: 0.3944472680314114]
	TIME [epoch: 8.55 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5226259483411083		[learning rate: 0.00115]
	Learning Rate: 0.00114996
	LOSS [training: 0.5226259483411083 | validation: 0.4365339393768424]
	TIME [epoch: 8.53 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5154759194392458		[learning rate: 0.0011458]
	Learning Rate: 0.00114579
	LOSS [training: 0.5154759194392458 | validation: 0.4611176558300168]
	TIME [epoch: 8.56 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5414518493573715		[learning rate: 0.0011416]
	Learning Rate: 0.00114163
	LOSS [training: 0.5414518493573715 | validation: 0.44491404566550635]
	TIME [epoch: 8.56 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5495792791861426		[learning rate: 0.0011375]
	Learning Rate: 0.00113749
	LOSS [training: 0.5495792791861426 | validation: 0.39377875123722694]
	TIME [epoch: 8.55 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5911619154196271		[learning rate: 0.0011334]
	Learning Rate: 0.00113336
	LOSS [training: 0.5911619154196271 | validation: 0.42513137611509744]
	TIME [epoch: 8.54 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5703080565377461		[learning rate: 0.0011292]
	Learning Rate: 0.00112925
	LOSS [training: 0.5703080565377461 | validation: 0.4396438236193779]
	TIME [epoch: 8.56 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5887343513696897		[learning rate: 0.0011252]
	Learning Rate: 0.00112515
	LOSS [training: 0.5887343513696897 | validation: 0.48035583084758904]
	TIME [epoch: 8.56 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5371076198296565		[learning rate: 0.0011211]
	Learning Rate: 0.00112107
	LOSS [training: 0.5371076198296565 | validation: 0.3906791449526042]
	TIME [epoch: 8.54 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5087756001322996		[learning rate: 0.001117]
	Learning Rate: 0.001117
	LOSS [training: 0.5087756001322996 | validation: 0.38389384877508426]
	TIME [epoch: 8.55 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.6517724301260301		[learning rate: 0.0011129]
	Learning Rate: 0.00111295
	LOSS [training: 0.6517724301260301 | validation: 0.42386837853542525]
	TIME [epoch: 8.54 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.533136485205908		[learning rate: 0.0011089]
	Learning Rate: 0.00110891
	LOSS [training: 0.533136485205908 | validation: 0.3756229914256327]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5438341231248527		[learning rate: 0.0011049]
	Learning Rate: 0.00110488
	LOSS [training: 0.5438341231248527 | validation: 0.36394685276274386]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5297565399588682		[learning rate: 0.0011009]
	Learning Rate: 0.00110087
	LOSS [training: 0.5297565399588682 | validation: 0.43026373216084157]
	TIME [epoch: 8.57 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5135538202439556		[learning rate: 0.0010969]
	Learning Rate: 0.00109688
	LOSS [training: 0.5135538202439556 | validation: 0.37790053548741376]
	TIME [epoch: 8.56 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.509307866669873		[learning rate: 0.0010929]
	Learning Rate: 0.0010929
	LOSS [training: 0.509307866669873 | validation: 0.36623406382009005]
	TIME [epoch: 8.55 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5462845181493539		[learning rate: 0.0010889]
	Learning Rate: 0.00108893
	LOSS [training: 0.5462845181493539 | validation: 0.36053621723774304]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49420269425065344		[learning rate: 0.001085]
	Learning Rate: 0.00108498
	LOSS [training: 0.49420269425065344 | validation: 0.41600366143625467]
	TIME [epoch: 8.56 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5130691129576544		[learning rate: 0.001081]
	Learning Rate: 0.00108104
	LOSS [training: 0.5130691129576544 | validation: 0.3498261315467538]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5182879825522756		[learning rate: 0.0010771]
	Learning Rate: 0.00107712
	LOSS [training: 0.5182879825522756 | validation: 0.6295821282407075]
	TIME [epoch: 8.53 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5688282313380126		[learning rate: 0.0010732]
	Learning Rate: 0.00107321
	LOSS [training: 0.5688282313380126 | validation: 0.40974721587498897]
	TIME [epoch: 8.53 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5274535915516446		[learning rate: 0.0010693]
	Learning Rate: 0.00106931
	LOSS [training: 0.5274535915516446 | validation: 0.42850280769389926]
	TIME [epoch: 8.55 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5166849314119912		[learning rate: 0.0010654]
	Learning Rate: 0.00106543
	LOSS [training: 0.5166849314119912 | validation: 0.3998664407294106]
	TIME [epoch: 8.56 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5168778783981799		[learning rate: 0.0010616]
	Learning Rate: 0.00106157
	LOSS [training: 0.5168778783981799 | validation: 0.35365191816917924]
	TIME [epoch: 8.54 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49592751920953615		[learning rate: 0.0010577]
	Learning Rate: 0.00105771
	LOSS [training: 0.49592751920953615 | validation: 0.397353734644988]
	TIME [epoch: 8.53 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5663583004194539		[learning rate: 0.0010539]
	Learning Rate: 0.00105388
	LOSS [training: 0.5663583004194539 | validation: 0.4397060452787902]
	TIME [epoch: 8.56 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5055505018098343		[learning rate: 0.0010501]
	Learning Rate: 0.00105005
	LOSS [training: 0.5055505018098343 | validation: 0.3959013486196444]
	TIME [epoch: 8.55 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5064025918074393		[learning rate: 0.0010462]
	Learning Rate: 0.00104624
	LOSS [training: 0.5064025918074393 | validation: 0.6334090198270649]
	TIME [epoch: 8.53 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5197668694258593		[learning rate: 0.0010424]
	Learning Rate: 0.00104244
	LOSS [training: 0.5197668694258593 | validation: 0.3718596023528269]
	TIME [epoch: 8.53 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5420531372553051		[learning rate: 0.0010387]
	Learning Rate: 0.00103866
	LOSS [training: 0.5420531372553051 | validation: 0.41257099494333593]
	TIME [epoch: 8.58 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5426860145279988		[learning rate: 0.0010349]
	Learning Rate: 0.00103489
	LOSS [training: 0.5426860145279988 | validation: 0.3631059271768616]
	TIME [epoch: 8.54 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4870671487566822		[learning rate: 0.0010311]
	Learning Rate: 0.00103114
	LOSS [training: 0.4870671487566822 | validation: 0.35683026191083006]
	TIME [epoch: 8.54 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5014826368066978		[learning rate: 0.0010274]
	Learning Rate: 0.00102739
	LOSS [training: 0.5014826368066978 | validation: 0.4548508651889213]
	TIME [epoch: 8.54 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5008411684340578		[learning rate: 0.0010237]
	Learning Rate: 0.00102367
	LOSS [training: 0.5008411684340578 | validation: 0.37050435665474846]
	TIME [epoch: 8.57 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48254320104996823		[learning rate: 0.00102]
	Learning Rate: 0.00101995
	LOSS [training: 0.48254320104996823 | validation: 0.37494203055588465]
	TIME [epoch: 8.53 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5028850277640516		[learning rate: 0.0010162]
	Learning Rate: 0.00101625
	LOSS [training: 0.5028850277640516 | validation: 0.3893860981679504]
	TIME [epoch: 8.54 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5218237677003559		[learning rate: 0.0010126]
	Learning Rate: 0.00101256
	LOSS [training: 0.5218237677003559 | validation: 0.5360557793551262]
	TIME [epoch: 8.57 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5108626904576113		[learning rate: 0.0010089]
	Learning Rate: 0.00100889
	LOSS [training: 0.5108626904576113 | validation: 0.41695756648073923]
	TIME [epoch: 8.55 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5150165714534936		[learning rate: 0.0010052]
	Learning Rate: 0.00100522
	LOSS [training: 0.5150165714534936 | validation: 0.37903335173033936]
	TIME [epoch: 8.54 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.496062368116385		[learning rate: 0.0010016]
	Learning Rate: 0.00100158
	LOSS [training: 0.496062368116385 | validation: 0.3674441580538196]
	TIME [epoch: 8.54 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4913303928988043		[learning rate: 0.00099794]
	Learning Rate: 0.000997942
	LOSS [training: 0.4913303928988043 | validation: 0.4456031768914013]
	TIME [epoch: 8.57 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5015841047295704		[learning rate: 0.00099432]
	Learning Rate: 0.00099432
	LOSS [training: 0.5015841047295704 | validation: 0.38700873226016874]
	TIME [epoch: 8.56 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5262059442678207		[learning rate: 0.00099071]
	Learning Rate: 0.000990712
	LOSS [training: 0.5262059442678207 | validation: 0.5051595114755825]
	TIME [epoch: 8.54 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5154757974029729		[learning rate: 0.00098712]
	Learning Rate: 0.000987116
	LOSS [training: 0.5154757974029729 | validation: 0.3864032883373659]
	TIME [epoch: 8.53 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47378299719632666		[learning rate: 0.00098353]
	Learning Rate: 0.000983534
	LOSS [training: 0.47378299719632666 | validation: 0.3756576839114345]
	TIME [epoch: 8.58 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5020944336704565		[learning rate: 0.00097996]
	Learning Rate: 0.000979965
	LOSS [training: 0.5020944336704565 | validation: 0.4470296505820157]
	TIME [epoch: 8.54 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5017110193706109		[learning rate: 0.00097641]
	Learning Rate: 0.000976409
	LOSS [training: 0.5017110193706109 | validation: 0.42239224990288116]
	TIME [epoch: 8.54 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5230022950971314		[learning rate: 0.00097287]
	Learning Rate: 0.000972865
	LOSS [training: 0.5230022950971314 | validation: 0.587085208179988]
	TIME [epoch: 8.54 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5102767314531612		[learning rate: 0.00096933]
	Learning Rate: 0.000969335
	LOSS [training: 0.5102767314531612 | validation: 0.6629333345720785]
	TIME [epoch: 8.58 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5249746461177314		[learning rate: 0.00096582]
	Learning Rate: 0.000965817
	LOSS [training: 0.5249746461177314 | validation: 0.3934805907495573]
	TIME [epoch: 8.53 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48403004742379807		[learning rate: 0.00096231]
	Learning Rate: 0.000962312
	LOSS [training: 0.48403004742379807 | validation: 0.3791632988498592]
	TIME [epoch: 8.54 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47879900949745674		[learning rate: 0.00095882]
	Learning Rate: 0.00095882
	LOSS [training: 0.47879900949745674 | validation: 0.4113583495821269]
	TIME [epoch: 8.55 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5120815914197993		[learning rate: 0.00095534]
	Learning Rate: 0.00095534
	LOSS [training: 0.5120815914197993 | validation: 0.3716999135512124]
	TIME [epoch: 8.56 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48022376636942976		[learning rate: 0.00095187]
	Learning Rate: 0.000951873
	LOSS [training: 0.48022376636942976 | validation: 0.45756792154252734]
	TIME [epoch: 8.54 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5085774481742124		[learning rate: 0.00094842]
	Learning Rate: 0.000948419
	LOSS [training: 0.5085774481742124 | validation: 0.42914561455172884]
	TIME [epoch: 8.54 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4878280404482429		[learning rate: 0.00094498]
	Learning Rate: 0.000944977
	LOSS [training: 0.4878280404482429 | validation: 0.3984665992193065]
	TIME [epoch: 8.56 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5431163320243688		[learning rate: 0.00094155]
	Learning Rate: 0.000941547
	LOSS [training: 0.5431163320243688 | validation: 0.4261851349199076]
	TIME [epoch: 8.56 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48248842855301943		[learning rate: 0.00093813]
	Learning Rate: 0.00093813
	LOSS [training: 0.48248842855301943 | validation: 0.4529103477253653]
	TIME [epoch: 8.53 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4911888126791516		[learning rate: 0.00093473]
	Learning Rate: 0.000934726
	LOSS [training: 0.4911888126791516 | validation: 0.3661731949696006]
	TIME [epoch: 8.54 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45688259925796837		[learning rate: 0.00093133]
	Learning Rate: 0.000931334
	LOSS [training: 0.45688259925796837 | validation: 0.43546394121653]
	TIME [epoch: 8.56 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46874770405448335		[learning rate: 0.00092795]
	Learning Rate: 0.000927954
	LOSS [training: 0.46874770405448335 | validation: 0.3967238344079043]
	TIME [epoch: 8.55 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49780053125170803		[learning rate: 0.00092459]
	Learning Rate: 0.000924586
	LOSS [training: 0.49780053125170803 | validation: 0.3563950016587889]
	TIME [epoch: 8.54 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5046525593831915		[learning rate: 0.00092123]
	Learning Rate: 0.000921231
	LOSS [training: 0.5046525593831915 | validation: 0.3841002805018778]
	TIME [epoch: 8.54 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5548786853598264		[learning rate: 0.00091789]
	Learning Rate: 0.000917888
	LOSS [training: 0.5548786853598264 | validation: 0.35728903560850855]
	TIME [epoch: 8.57 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49517618089870535		[learning rate: 0.00091456]
	Learning Rate: 0.000914556
	LOSS [training: 0.49517618089870535 | validation: 0.44103617319054705]
	TIME [epoch: 8.55 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48482134381781766		[learning rate: 0.00091124]
	Learning Rate: 0.000911237
	LOSS [training: 0.48482134381781766 | validation: 0.41391612703946656]
	TIME [epoch: 8.53 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4776459324748343		[learning rate: 0.00090793]
	Learning Rate: 0.000907931
	LOSS [training: 0.4776459324748343 | validation: 0.3909252580262128]
	TIME [epoch: 8.55 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5288033279532849		[learning rate: 0.00090464]
	Learning Rate: 0.000904636
	LOSS [training: 0.5288033279532849 | validation: 0.4448926480119908]
	TIME [epoch: 8.56 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.5104360343569095		[learning rate: 0.00090135]
	Learning Rate: 0.000901353
	LOSS [training: 0.5104360343569095 | validation: 0.3903845368715274]
	TIME [epoch: 8.53 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44867665415936087		[learning rate: 0.00089808]
	Learning Rate: 0.000898082
	LOSS [training: 0.44867665415936087 | validation: 0.45686588184662025]
	TIME [epoch: 8.53 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46740622972067064		[learning rate: 0.00089482]
	Learning Rate: 0.000894822
	LOSS [training: 0.46740622972067064 | validation: 0.3695026217760678]
	TIME [epoch: 8.55 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4658889657318565		[learning rate: 0.00089157]
	Learning Rate: 0.000891575
	LOSS [training: 0.4658889657318565 | validation: 0.37779020561521814]
	TIME [epoch: 8.54 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4690816465859422		[learning rate: 0.00088834]
	Learning Rate: 0.000888339
	LOSS [training: 0.4690816465859422 | validation: 0.3640546796067513]
	TIME [epoch: 8.54 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4844609934743917		[learning rate: 0.00088512]
	Learning Rate: 0.000885116
	LOSS [training: 0.4844609934743917 | validation: 0.41008728659339855]
	TIME [epoch: 8.53 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4834436346248726		[learning rate: 0.0008819]
	Learning Rate: 0.000881903
	LOSS [training: 0.4834436346248726 | validation: 0.3808035016512986]
	TIME [epoch: 8.56 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4651226048663818		[learning rate: 0.0008787]
	Learning Rate: 0.000878703
	LOSS [training: 0.4651226048663818 | validation: 0.35918513965699517]
	TIME [epoch: 8.56 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45482694211615		[learning rate: 0.00087551]
	Learning Rate: 0.000875514
	LOSS [training: 0.45482694211615 | validation: 0.3611719270013763]
	TIME [epoch: 8.54 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4586161743981917		[learning rate: 0.00087234]
	Learning Rate: 0.000872337
	LOSS [training: 0.4586161743981917 | validation: 0.3898839832745463]
	TIME [epoch: 8.54 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4541302203891088		[learning rate: 0.00086917]
	Learning Rate: 0.000869171
	LOSS [training: 0.4541302203891088 | validation: 0.5169460045650652]
	TIME [epoch: 8.55 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4978814829436519		[learning rate: 0.00086602]
	Learning Rate: 0.000866017
	LOSS [training: 0.4978814829436519 | validation: 0.3696097445810088]
	TIME [epoch: 8.56 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45953158467638655		[learning rate: 0.00086287]
	Learning Rate: 0.000862874
	LOSS [training: 0.45953158467638655 | validation: 0.33811733908624136]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46295057836290204		[learning rate: 0.00085974]
	Learning Rate: 0.000859743
	LOSS [training: 0.46295057836290204 | validation: 0.3966264475562554]
	TIME [epoch: 8.55 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4955558303827573		[learning rate: 0.00085662]
	Learning Rate: 0.000856623
	LOSS [training: 0.4955558303827573 | validation: 0.35194151640298604]
	TIME [epoch: 8.54 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49322120167553163		[learning rate: 0.00085351]
	Learning Rate: 0.000853514
	LOSS [training: 0.49322120167553163 | validation: 0.36571557180144576]
	TIME [epoch: 8.55 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47268305106908465		[learning rate: 0.00085042]
	Learning Rate: 0.000850416
	LOSS [training: 0.47268305106908465 | validation: 0.34788737380047613]
	TIME [epoch: 8.53 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4584869600608356		[learning rate: 0.00084733]
	Learning Rate: 0.00084733
	LOSS [training: 0.4584869600608356 | validation: 0.39118026852367804]
	TIME [epoch: 8.56 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4811544576354298		[learning rate: 0.00084426]
	Learning Rate: 0.000844255
	LOSS [training: 0.4811544576354298 | validation: 0.3671953158884707]
	TIME [epoch: 8.53 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4560396169191868		[learning rate: 0.00084119]
	Learning Rate: 0.000841191
	LOSS [training: 0.4560396169191868 | validation: 0.35152867812128874]
	TIME [epoch: 8.56 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4585807755452434		[learning rate: 0.00083814]
	Learning Rate: 0.000838139
	LOSS [training: 0.4585807755452434 | validation: 0.38928921664803456]
	TIME [epoch: 8.55 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4748074413070741		[learning rate: 0.0008351]
	Learning Rate: 0.000835097
	LOSS [training: 0.4748074413070741 | validation: 0.4923344269219545]
	TIME [epoch: 8.54 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48841099684631606		[learning rate: 0.00083207]
	Learning Rate: 0.000832066
	LOSS [training: 0.48841099684631606 | validation: 0.3266774791271281]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46388086879466295		[learning rate: 0.00082905]
	Learning Rate: 0.000829046
	LOSS [training: 0.46388086879466295 | validation: 0.37105324081568125]
	TIME [epoch: 8.55 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45240829495085083		[learning rate: 0.00082604]
	Learning Rate: 0.000826038
	LOSS [training: 0.45240829495085083 | validation: 0.4867479356451886]
	TIME [epoch: 8.55 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47380205621873017		[learning rate: 0.00082304]
	Learning Rate: 0.00082304
	LOSS [training: 0.47380205621873017 | validation: 0.35340318085840733]
	TIME [epoch: 8.53 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46891307431924406		[learning rate: 0.00082005]
	Learning Rate: 0.000820053
	LOSS [training: 0.46891307431924406 | validation: 0.3693036561856894]
	TIME [epoch: 8.55 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4612450010096503		[learning rate: 0.00081708]
	Learning Rate: 0.000817077
	LOSS [training: 0.4612450010096503 | validation: 0.3781531532940224]
	TIME [epoch: 8.53 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46944086277267943		[learning rate: 0.00081411]
	Learning Rate: 0.000814112
	LOSS [training: 0.46944086277267943 | validation: 0.4136766661422385]
	TIME [epoch: 8.55 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.47202292255162515		[learning rate: 0.00081116]
	Learning Rate: 0.000811158
	LOSS [training: 0.47202292255162515 | validation: 0.35453982032105946]
	TIME [epoch: 8.53 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46511941621540676		[learning rate: 0.00080821]
	Learning Rate: 0.000808214
	LOSS [training: 0.46511941621540676 | validation: 0.3947917761556579]
	TIME [epoch: 8.55 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46527700407100064		[learning rate: 0.00080528]
	Learning Rate: 0.000805281
	LOSS [training: 0.46527700407100064 | validation: 0.3679130723572613]
	TIME [epoch: 8.55 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46725324983760147		[learning rate: 0.00080236]
	Learning Rate: 0.000802358
	LOSS [training: 0.46725324983760147 | validation: 0.38907236099498566]
	TIME [epoch: 8.53 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48437025322196287		[learning rate: 0.00079945]
	Learning Rate: 0.000799447
	LOSS [training: 0.48437025322196287 | validation: 0.3600889722953699]
	TIME [epoch: 8.53 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45085922782525883		[learning rate: 0.00079655]
	Learning Rate: 0.000796545
	LOSS [training: 0.45085922782525883 | validation: 0.3678532300016248]
	TIME [epoch: 8.55 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4908142759294233		[learning rate: 0.00079365]
	Learning Rate: 0.000793655
	LOSS [training: 0.4908142759294233 | validation: 0.33981693586106854]
	TIME [epoch: 8.55 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4466884462170497		[learning rate: 0.00079077]
	Learning Rate: 0.000790775
	LOSS [training: 0.4466884462170497 | validation: 0.483583135546538]
	TIME [epoch: 8.54 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.49276238339361844		[learning rate: 0.0007879]
	Learning Rate: 0.000787905
	LOSS [training: 0.49276238339361844 | validation: 0.416472977938385]
	TIME [epoch: 8.53 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4778636128441488		[learning rate: 0.00078505]
	Learning Rate: 0.000785045
	LOSS [training: 0.4778636128441488 | validation: 0.4085610807091389]
	TIME [epoch: 8.55 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46418844356956257		[learning rate: 0.0007822]
	Learning Rate: 0.000782196
	LOSS [training: 0.46418844356956257 | validation: 0.3795093814032753]
	TIME [epoch: 8.55 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4483832401407673		[learning rate: 0.00077936]
	Learning Rate: 0.000779358
	LOSS [training: 0.4483832401407673 | validation: 0.3984431481672291]
	TIME [epoch: 8.53 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4561508508825245		[learning rate: 0.00077653]
	Learning Rate: 0.000776529
	LOSS [training: 0.4561508508825245 | validation: 0.34529289997573326]
	TIME [epoch: 8.53 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42482044880149655		[learning rate: 0.00077371]
	Learning Rate: 0.000773711
	LOSS [training: 0.42482044880149655 | validation: 0.4076952520431064]
	TIME [epoch: 8.55 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44482030207944706		[learning rate: 0.0007709]
	Learning Rate: 0.000770903
	LOSS [training: 0.44482030207944706 | validation: 0.3693261134367295]
	TIME [epoch: 8.56 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46095349814853187		[learning rate: 0.00076811]
	Learning Rate: 0.000768106
	LOSS [training: 0.46095349814853187 | validation: 0.4147255189452699]
	TIME [epoch: 8.53 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.44965557098810693		[learning rate: 0.00076532]
	Learning Rate: 0.000765318
	LOSS [training: 0.44965557098810693 | validation: 0.38654465601987453]
	TIME [epoch: 8.53 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48054865781643663		[learning rate: 0.00076254]
	Learning Rate: 0.000762541
	LOSS [training: 0.48054865781643663 | validation: 0.34970437388424863]
	TIME [epoch: 8.56 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48027063006859183		[learning rate: 0.00075977]
	Learning Rate: 0.000759774
	LOSS [training: 0.48027063006859183 | validation: 0.37450994559495066]
	TIME [epoch: 8.54 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46264971375104685		[learning rate: 0.00075702]
	Learning Rate: 0.000757016
	LOSS [training: 0.46264971375104685 | validation: 0.48025033096404535]
	TIME [epoch: 8.53 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4769797322243566		[learning rate: 0.00075427]
	Learning Rate: 0.000754269
	LOSS [training: 0.4769797322243566 | validation: 0.33983645099785276]
	TIME [epoch: 8.53 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4478391247280163		[learning rate: 0.00075153]
	Learning Rate: 0.000751532
	LOSS [training: 0.4478391247280163 | validation: 0.38340530009983614]
	TIME [epoch: 8.57 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45665949294771063		[learning rate: 0.0007488]
	Learning Rate: 0.000748804
	LOSS [training: 0.45665949294771063 | validation: 0.3399002735722165]
	TIME [epoch: 8.54 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4363828033763618		[learning rate: 0.00074609]
	Learning Rate: 0.000746087
	LOSS [training: 0.4363828033763618 | validation: 0.3151871096582021]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42822153721874107		[learning rate: 0.00074338]
	Learning Rate: 0.000743379
	LOSS [training: 0.42822153721874107 | validation: 0.34265715607777847]
	TIME [epoch: 8.56 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4544115164237833		[learning rate: 0.00074068]
	Learning Rate: 0.000740682
	LOSS [training: 0.4544115164237833 | validation: 0.3678784517616833]
	TIME [epoch: 8.57 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.453015272454515		[learning rate: 0.00073799]
	Learning Rate: 0.000737994
	LOSS [training: 0.453015272454515 | validation: 0.36928080404033325]
	TIME [epoch: 8.54 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.46767839447464954		[learning rate: 0.00073532]
	Learning Rate: 0.000735315
	LOSS [training: 0.46767839447464954 | validation: 0.3731339108359768]
	TIME [epoch: 8.54 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4369258738860828		[learning rate: 0.00073265]
	Learning Rate: 0.000732647
	LOSS [training: 0.4369258738860828 | validation: 0.3500636650394773]
	TIME [epoch: 8.57 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.432883414390408		[learning rate: 0.00072999]
	Learning Rate: 0.000729988
	LOSS [training: 0.432883414390408 | validation: 0.32474130093632314]
	TIME [epoch: 8.55 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4333146863500431		[learning rate: 0.00072734]
	Learning Rate: 0.000727339
	LOSS [training: 0.4333146863500431 | validation: 0.325711626807426]
	TIME [epoch: 8.54 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.447692546268318		[learning rate: 0.0007247]
	Learning Rate: 0.000724699
	LOSS [training: 0.447692546268318 | validation: 0.3628033743458504]
	TIME [epoch: 8.54 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42729456923352044		[learning rate: 0.00072207]
	Learning Rate: 0.000722069
	LOSS [training: 0.42729456923352044 | validation: 0.3409375833393136]
	TIME [epoch: 8.58 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43210869893346054		[learning rate: 0.00071945]
	Learning Rate: 0.000719449
	LOSS [training: 0.43210869893346054 | validation: 0.3231927487912228]
	TIME [epoch: 8.55 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45716118481447604		[learning rate: 0.00071684]
	Learning Rate: 0.000716838
	LOSS [training: 0.45716118481447604 | validation: 0.39794470207486965]
	TIME [epoch: 8.54 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43593348764480133		[learning rate: 0.00071424]
	Learning Rate: 0.000714237
	LOSS [training: 0.43593348764480133 | validation: 0.3098171493845066]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4319073764725103		[learning rate: 0.00071164]
	Learning Rate: 0.000711645
	LOSS [training: 0.4319073764725103 | validation: 0.34903406183752783]
	TIME [epoch: 8.58 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4331293770726484		[learning rate: 0.00070906]
	Learning Rate: 0.000709062
	LOSS [training: 0.4331293770726484 | validation: 0.33255821204452757]
	TIME [epoch: 8.54 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4385512106982409		[learning rate: 0.00070649]
	Learning Rate: 0.000706489
	LOSS [training: 0.4385512106982409 | validation: 0.32325589609278677]
	TIME [epoch: 8.53 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45050517397784623		[learning rate: 0.00070392]
	Learning Rate: 0.000703925
	LOSS [training: 0.45050517397784623 | validation: 0.41237250241869255]
	TIME [epoch: 8.53 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4364634245400348		[learning rate: 0.00070137]
	Learning Rate: 0.00070137
	LOSS [training: 0.4364634245400348 | validation: 0.3453537482369712]
	TIME [epoch: 8.58 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4100716955082898		[learning rate: 0.00069883]
	Learning Rate: 0.000698825
	LOSS [training: 0.4100716955082898 | validation: 0.32929989177981794]
	TIME [epoch: 8.53 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.404136020375965		[learning rate: 0.00069629]
	Learning Rate: 0.000696289
	LOSS [training: 0.404136020375965 | validation: 0.33816535568374567]
	TIME [epoch: 8.53 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4274388588158041		[learning rate: 0.00069376]
	Learning Rate: 0.000693762
	LOSS [training: 0.4274388588158041 | validation: 0.4021050679105419]
	TIME [epoch: 8.54 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4230070250276171		[learning rate: 0.00069124]
	Learning Rate: 0.000691244
	LOSS [training: 0.4230070250276171 | validation: 0.311856950534497]
	TIME [epoch: 8.56 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41053371577715225		[learning rate: 0.00068874]
	Learning Rate: 0.000688736
	LOSS [training: 0.41053371577715225 | validation: 0.3564808899429143]
	TIME [epoch: 8.53 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41986431687250425		[learning rate: 0.00068624]
	Learning Rate: 0.000686236
	LOSS [training: 0.41986431687250425 | validation: 0.3767223076851039]
	TIME [epoch: 8.53 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4207564098264842		[learning rate: 0.00068375]
	Learning Rate: 0.000683746
	LOSS [training: 0.4207564098264842 | validation: 0.32482213690031364]
	TIME [epoch: 8.55 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42600108118896624		[learning rate: 0.00068126]
	Learning Rate: 0.000681265
	LOSS [training: 0.42600108118896624 | validation: 0.35325281985037654]
	TIME [epoch: 8.57 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43518688937319183		[learning rate: 0.00067879]
	Learning Rate: 0.000678792
	LOSS [training: 0.43518688937319183 | validation: 0.35885861519455775]
	TIME [epoch: 8.54 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41378272305439473		[learning rate: 0.00067633]
	Learning Rate: 0.000676329
	LOSS [training: 0.41378272305439473 | validation: 0.3383587560160452]
	TIME [epoch: 8.53 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43248380227533617		[learning rate: 0.00067387]
	Learning Rate: 0.000673874
	LOSS [training: 0.43248380227533617 | validation: 0.3222773652366189]
	TIME [epoch: 8.56 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41816521278623153		[learning rate: 0.00067143]
	Learning Rate: 0.000671429
	LOSS [training: 0.41816521278623153 | validation: 0.33270768631910785]
	TIME [epoch: 8.54 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.48020965778558394		[learning rate: 0.00066899]
	Learning Rate: 0.000668992
	LOSS [training: 0.48020965778558394 | validation: 0.3304988285520503]
	TIME [epoch: 8.53 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.430018386589566		[learning rate: 0.00066656]
	Learning Rate: 0.000666564
	LOSS [training: 0.430018386589566 | validation: 0.3721641185150118]
	TIME [epoch: 8.53 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4649317775407383		[learning rate: 0.00066415]
	Learning Rate: 0.000664145
	LOSS [training: 0.4649317775407383 | validation: 0.3383068944088525]
	TIME [epoch: 8.57 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4271261968476475		[learning rate: 0.00066174]
	Learning Rate: 0.000661735
	LOSS [training: 0.4271261968476475 | validation: 0.364759224719139]
	TIME [epoch: 8.54 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4186922136017103		[learning rate: 0.00065933]
	Learning Rate: 0.000659334
	LOSS [training: 0.4186922136017103 | validation: 0.3871948186993056]
	TIME [epoch: 8.54 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45978030319587904		[learning rate: 0.00065694]
	Learning Rate: 0.000656941
	LOSS [training: 0.45978030319587904 | validation: 0.3668344030794759]
	TIME [epoch: 8.55 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4485151082963624		[learning rate: 0.00065456]
	Learning Rate: 0.000654557
	LOSS [training: 0.4485151082963624 | validation: 0.3582791410578693]
	TIME [epoch: 8.57 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42692418467642923		[learning rate: 0.00065218]
	Learning Rate: 0.000652182
	LOSS [training: 0.42692418467642923 | validation: 0.36038443448526414]
	TIME [epoch: 8.53 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42287248428680224		[learning rate: 0.00064981]
	Learning Rate: 0.000649815
	LOSS [training: 0.42287248428680224 | validation: 0.3617639388336328]
	TIME [epoch: 8.53 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41544261438480345		[learning rate: 0.00064746]
	Learning Rate: 0.000647456
	LOSS [training: 0.41544261438480345 | validation: 0.37205924304779636]
	TIME [epoch: 8.54 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45579833447261187		[learning rate: 0.00064511]
	Learning Rate: 0.000645107
	LOSS [training: 0.45579833447261187 | validation: 0.547177233980655]
	TIME [epoch: 8.56 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.436485336456885		[learning rate: 0.00064277]
	Learning Rate: 0.000642766
	LOSS [training: 0.436485336456885 | validation: 0.31823613052062516]
	TIME [epoch: 8.54 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40466292484947164		[learning rate: 0.00064043]
	Learning Rate: 0.000640433
	LOSS [training: 0.40466292484947164 | validation: 0.3165130014573927]
	TIME [epoch: 8.53 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41404544139769844		[learning rate: 0.00063811]
	Learning Rate: 0.000638109
	LOSS [training: 0.41404544139769844 | validation: 0.3211981719154027]
	TIME [epoch: 8.56 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4085320067218544		[learning rate: 0.00063579]
	Learning Rate: 0.000635793
	LOSS [training: 0.4085320067218544 | validation: 0.29608420384865325]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4223222553641241		[learning rate: 0.00063349]
	Learning Rate: 0.000633486
	LOSS [training: 0.4223222553641241 | validation: 0.32783893077177795]
	TIME [epoch: 8.54 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42283842100683255		[learning rate: 0.00063119]
	Learning Rate: 0.000631187
	LOSS [training: 0.42283842100683255 | validation: 0.34432567782417633]
	TIME [epoch: 8.54 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4215860772864232		[learning rate: 0.0006289]
	Learning Rate: 0.000628896
	LOSS [training: 0.4215860772864232 | validation: 0.40535083060983457]
	TIME [epoch: 8.54 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4032174187290587		[learning rate: 0.00062661]
	Learning Rate: 0.000626614
	LOSS [training: 0.4032174187290587 | validation: 0.3177930388487877]
	TIME [epoch: 8.56 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40961194401580847		[learning rate: 0.00062434]
	Learning Rate: 0.00062434
	LOSS [training: 0.40961194401580847 | validation: 0.3339914703003925]
	TIME [epoch: 8.53 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4119696691014645		[learning rate: 0.00062207]
	Learning Rate: 0.000622074
	LOSS [training: 0.4119696691014645 | validation: 0.36045405199739167]
	TIME [epoch: 8.54 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.45897793184410396		[learning rate: 0.00061982]
	Learning Rate: 0.000619816
	LOSS [training: 0.45897793184410396 | validation: 0.3380001478062639]
	TIME [epoch: 8.54 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38357466945278623		[learning rate: 0.00061757]
	Learning Rate: 0.000617567
	LOSS [training: 0.38357466945278623 | validation: 0.3121746357090036]
	TIME [epoch: 8.55 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39221257742266685		[learning rate: 0.00061533]
	Learning Rate: 0.000615326
	LOSS [training: 0.39221257742266685 | validation: 0.32959546833847064]
	TIME [epoch: 8.53 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4340639870954869		[learning rate: 0.00061309]
	Learning Rate: 0.000613093
	LOSS [training: 0.4340639870954869 | validation: 0.34093372490836227]
	TIME [epoch: 8.54 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4166242751865831		[learning rate: 0.00061087]
	Learning Rate: 0.000610868
	LOSS [training: 0.4166242751865831 | validation: 0.337757736976717]
	TIME [epoch: 8.55 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4155520592185944		[learning rate: 0.00060865]
	Learning Rate: 0.000608651
	LOSS [training: 0.4155520592185944 | validation: 0.32228094447795436]
	TIME [epoch: 8.54 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4093145343040425		[learning rate: 0.00060644]
	Learning Rate: 0.000606442
	LOSS [training: 0.4093145343040425 | validation: 0.2980906926455438]
	TIME [epoch: 8.52 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.429324387533822		[learning rate: 0.00060424]
	Learning Rate: 0.000604242
	LOSS [training: 0.429324387533822 | validation: 0.337359372299769]
	TIME [epoch: 8.55 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4001865018560564		[learning rate: 0.00060205]
	Learning Rate: 0.000602049
	LOSS [training: 0.4001865018560564 | validation: 0.3481520458116057]
	TIME [epoch: 8.55 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39668965225663283		[learning rate: 0.00059986]
	Learning Rate: 0.000599864
	LOSS [training: 0.39668965225663283 | validation: 0.30958281575361235]
	TIME [epoch: 8.53 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40912824003581		[learning rate: 0.00059769]
	Learning Rate: 0.000597687
	LOSS [training: 0.40912824003581 | validation: 0.32381577204275247]
	TIME [epoch: 8.54 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4222575653009305		[learning rate: 0.00059552]
	Learning Rate: 0.000595518
	LOSS [training: 0.4222575653009305 | validation: 0.3134242403092081]
	TIME [epoch: 8.54 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4251898666594598		[learning rate: 0.00059336]
	Learning Rate: 0.000593357
	LOSS [training: 0.4251898666594598 | validation: 0.3878663600103339]
	TIME [epoch: 8.55 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39042396748099806		[learning rate: 0.0005912]
	Learning Rate: 0.000591203
	LOSS [training: 0.39042396748099806 | validation: 0.29446023928871634]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4070264852910916		[learning rate: 0.00058906]
	Learning Rate: 0.000589058
	LOSS [training: 0.4070264852910916 | validation: 0.344710176020503]
	TIME [epoch: 8.54 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.399823226292536		[learning rate: 0.00058692]
	Learning Rate: 0.00058692
	LOSS [training: 0.399823226292536 | validation: 0.3260122389172052]
	TIME [epoch: 8.77 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4109295382472936		[learning rate: 0.00058479]
	Learning Rate: 0.00058479
	LOSS [training: 0.4109295382472936 | validation: 0.32170499385970647]
	TIME [epoch: 8.55 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40828514827679036		[learning rate: 0.00058267]
	Learning Rate: 0.000582668
	LOSS [training: 0.40828514827679036 | validation: 0.3339525837555626]
	TIME [epoch: 8.53 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40388602817873115		[learning rate: 0.00058055]
	Learning Rate: 0.000580553
	LOSS [training: 0.40388602817873115 | validation: 0.30787361682843256]
	TIME [epoch: 8.55 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39665203316545805		[learning rate: 0.00057845]
	Learning Rate: 0.000578446
	LOSS [training: 0.39665203316545805 | validation: 0.304426517613066]
	TIME [epoch: 8.53 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4012545272634023		[learning rate: 0.00057635]
	Learning Rate: 0.000576347
	LOSS [training: 0.4012545272634023 | validation: 0.34837619806530823]
	TIME [epoch: 8.55 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3953550123809184		[learning rate: 0.00057426]
	Learning Rate: 0.000574256
	LOSS [training: 0.3953550123809184 | validation: 0.3292731376418626]
	TIME [epoch: 8.53 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4100814231620536		[learning rate: 0.00057217]
	Learning Rate: 0.000572172
	LOSS [training: 0.4100814231620536 | validation: 0.39714771453362147]
	TIME [epoch: 8.55 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41142291595768343		[learning rate: 0.0005701]
	Learning Rate: 0.000570095
	LOSS [training: 0.41142291595768343 | validation: 0.3498278443369527]
	TIME [epoch: 8.53 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41007054149389877		[learning rate: 0.00056803]
	Learning Rate: 0.000568026
	LOSS [training: 0.41007054149389877 | validation: 0.35428133136256434]
	TIME [epoch: 8.55 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40526792774245457		[learning rate: 0.00056596]
	Learning Rate: 0.000565965
	LOSS [training: 0.40526792774245457 | validation: 0.3592848406106804]
	TIME [epoch: 8.53 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4397391972425712		[learning rate: 0.00056391]
	Learning Rate: 0.000563911
	LOSS [training: 0.4397391972425712 | validation: 0.33245805927586924]
	TIME [epoch: 8.54 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40493116129535034		[learning rate: 0.00056186]
	Learning Rate: 0.000561864
	LOSS [training: 0.40493116129535034 | validation: 0.3279687861026123]
	TIME [epoch: 8.53 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4452903591867766		[learning rate: 0.00055983]
	Learning Rate: 0.000559825
	LOSS [training: 0.4452903591867766 | validation: 0.41907669364906625]
	TIME [epoch: 8.55 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4217099812762827		[learning rate: 0.00055779]
	Learning Rate: 0.000557794
	LOSS [training: 0.4217099812762827 | validation: 0.3177340188317447]
	TIME [epoch: 8.55 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39462025311004456		[learning rate: 0.00055577]
	Learning Rate: 0.00055577
	LOSS [training: 0.39462025311004456 | validation: 0.32189277445755293]
	TIME [epoch: 8.55 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43462503030016214		[learning rate: 0.00055375]
	Learning Rate: 0.000553753
	LOSS [training: 0.43462503030016214 | validation: 0.3983313331556939]
	TIME [epoch: 8.53 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40632665965239906		[learning rate: 0.00055174]
	Learning Rate: 0.000551743
	LOSS [training: 0.40632665965239906 | validation: 0.3866836425577381]
	TIME [epoch: 8.55 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40792594511720537		[learning rate: 0.00054974]
	Learning Rate: 0.000549741
	LOSS [training: 0.40792594511720537 | validation: 0.33030412388219776]
	TIME [epoch: 8.55 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42863921099586444		[learning rate: 0.00054775]
	Learning Rate: 0.000547746
	LOSS [training: 0.42863921099586444 | validation: 0.3015944684849714]
	TIME [epoch: 8.53 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4441924653541477		[learning rate: 0.00054576]
	Learning Rate: 0.000545758
	LOSS [training: 0.4441924653541477 | validation: 0.3622865909228975]
	TIME [epoch: 8.54 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41127901515268306		[learning rate: 0.00054378]
	Learning Rate: 0.000543777
	LOSS [training: 0.41127901515268306 | validation: 0.30423405077804505]
	TIME [epoch: 8.54 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37943699201653064		[learning rate: 0.0005418]
	Learning Rate: 0.000541804
	LOSS [training: 0.37943699201653064 | validation: 0.30281999490105743]
	TIME [epoch: 8.56 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4567915199686413		[learning rate: 0.00053984]
	Learning Rate: 0.000539838
	LOSS [training: 0.4567915199686413 | validation: 0.37552180037713395]
	TIME [epoch: 8.53 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41356022440152607		[learning rate: 0.00053788]
	Learning Rate: 0.000537879
	LOSS [training: 0.41356022440152607 | validation: 0.4198288991415374]
	TIME [epoch: 8.55 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40861394192885026		[learning rate: 0.00053593]
	Learning Rate: 0.000535926
	LOSS [training: 0.40861394192885026 | validation: 0.30649087260654456]
	TIME [epoch: 8.55 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39715299208315114		[learning rate: 0.00053398]
	Learning Rate: 0.000533982
	LOSS [training: 0.39715299208315114 | validation: 0.42467445549216903]
	TIME [epoch: 8.54 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.43062066821115375		[learning rate: 0.00053204]
	Learning Rate: 0.000532044
	LOSS [training: 0.43062066821115375 | validation: 0.42444058200366885]
	TIME [epoch: 8.53 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3985416325910339		[learning rate: 0.00053011]
	Learning Rate: 0.000530113
	LOSS [training: 0.3985416325910339 | validation: 0.30026787414629913]
	TIME [epoch: 8.56 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3820527241838197		[learning rate: 0.00052819]
	Learning Rate: 0.000528189
	LOSS [training: 0.3820527241838197 | validation: 0.3173293417088525]
	TIME [epoch: 8.55 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38063602472013625		[learning rate: 0.00052627]
	Learning Rate: 0.000526272
	LOSS [training: 0.38063602472013625 | validation: 0.3166431184708404]
	TIME [epoch: 8.54 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.39255348636893955		[learning rate: 0.00052436]
	Learning Rate: 0.000524362
	LOSS [training: 0.39255348636893955 | validation: 0.3325528019154945]
	TIME [epoch: 8.53 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4191095992674475		[learning rate: 0.00052246]
	Learning Rate: 0.00052246
	LOSS [training: 0.4191095992674475 | validation: 0.3384763149872563]
	TIME [epoch: 8.55 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37513483531568365		[learning rate: 0.00052056]
	Learning Rate: 0.000520563
	LOSS [training: 0.37513483531568365 | validation: 0.38260319798629777]
	TIME [epoch: 8.56 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4080498364217259		[learning rate: 0.00051867]
	Learning Rate: 0.000518674
	LOSS [training: 0.4080498364217259 | validation: 0.3324274657065484]
	TIME [epoch: 8.53 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37696711446338227		[learning rate: 0.00051679]
	Learning Rate: 0.000516792
	LOSS [training: 0.37696711446338227 | validation: 0.3126485457854462]
	TIME [epoch: 8.53 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4025412075802908		[learning rate: 0.00051492]
	Learning Rate: 0.000514917
	LOSS [training: 0.4025412075802908 | validation: 0.30686811463928704]
	TIME [epoch: 8.56 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40671250744999005		[learning rate: 0.00051305]
	Learning Rate: 0.000513048
	LOSS [training: 0.40671250744999005 | validation: 0.32805254207129225]
	TIME [epoch: 8.56 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3876189942659584		[learning rate: 0.00051119]
	Learning Rate: 0.000511186
	LOSS [training: 0.3876189942659584 | validation: 0.3787483047333583]
	TIME [epoch: 8.54 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.42674719745858364		[learning rate: 0.00050933]
	Learning Rate: 0.000509331
	LOSS [training: 0.42674719745858364 | validation: 0.304887940529381]
	TIME [epoch: 8.53 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4041767164595907		[learning rate: 0.00050748]
	Learning Rate: 0.000507483
	LOSS [training: 0.4041767164595907 | validation: 0.2870602697787007]
	TIME [epoch: 8.57 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_920.pth
	Model improved!!!
EPOCH 921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37945519646576215		[learning rate: 0.00050564]
	Learning Rate: 0.000505641
	LOSS [training: 0.37945519646576215 | validation: 0.29493415759234576]
	TIME [epoch: 8.54 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4029702050274964		[learning rate: 0.00050381]
	Learning Rate: 0.000503806
	LOSS [training: 0.4029702050274964 | validation: 0.2918577699648903]
	TIME [epoch: 8.53 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38022183342203597		[learning rate: 0.00050198]
	Learning Rate: 0.000501977
	LOSS [training: 0.38022183342203597 | validation: 0.27913147300879326]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3719471192017689		[learning rate: 0.00050016]
	Learning Rate: 0.000500156
	LOSS [training: 0.3719471192017689 | validation: 0.331502614267449]
	TIME [epoch: 8.57 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4038152379309758		[learning rate: 0.00049834]
	Learning Rate: 0.000498341
	LOSS [training: 0.4038152379309758 | validation: 0.30007829100921846]
	TIME [epoch: 8.53 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3655450743890337		[learning rate: 0.00049653]
	Learning Rate: 0.000496532
	LOSS [training: 0.3655450743890337 | validation: 0.31310059201089263]
	TIME [epoch: 8.53 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38461080851404233		[learning rate: 0.00049473]
	Learning Rate: 0.00049473
	LOSS [training: 0.38461080851404233 | validation: 0.34928804598860425]
	TIME [epoch: 8.55 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3908994728530676		[learning rate: 0.00049293]
	Learning Rate: 0.000492935
	LOSS [training: 0.3908994728530676 | validation: 0.3473092870603448]
	TIME [epoch: 8.55 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3900724371897045		[learning rate: 0.00049115]
	Learning Rate: 0.000491146
	LOSS [training: 0.3900724371897045 | validation: 0.33230110315117684]
	TIME [epoch: 8.53 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38680830227410834		[learning rate: 0.00048936]
	Learning Rate: 0.000489364
	LOSS [training: 0.38680830227410834 | validation: 0.30949565177220373]
	TIME [epoch: 8.53 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38433215578310553		[learning rate: 0.00048759]
	Learning Rate: 0.000487588
	LOSS [training: 0.38433215578310553 | validation: 0.36236124794078245]
	TIME [epoch: 8.56 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37136397742781896		[learning rate: 0.00048582]
	Learning Rate: 0.000485818
	LOSS [training: 0.37136397742781896 | validation: 0.314193122265931]
	TIME [epoch: 8.54 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41081960332745887		[learning rate: 0.00048406]
	Learning Rate: 0.000484055
	LOSS [training: 0.41081960332745887 | validation: 0.5008368732147194]
	TIME [epoch: 8.53 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4233509576097304		[learning rate: 0.0004823]
	Learning Rate: 0.000482298
	LOSS [training: 0.4233509576097304 | validation: 0.28419623838237856]
	TIME [epoch: 8.53 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3752351017084643		[learning rate: 0.00048055]
	Learning Rate: 0.000480548
	LOSS [training: 0.3752351017084643 | validation: 0.2917739867836124]
	TIME [epoch: 8.57 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3874451798429652		[learning rate: 0.0004788]
	Learning Rate: 0.000478804
	LOSS [training: 0.3874451798429652 | validation: 0.31127138906444707]
	TIME [epoch: 8.54 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37985319407849555		[learning rate: 0.00047707]
	Learning Rate: 0.000477067
	LOSS [training: 0.37985319407849555 | validation: 0.3419452469750518]
	TIME [epoch: 8.53 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41541080164422983		[learning rate: 0.00047534]
	Learning Rate: 0.000475335
	LOSS [training: 0.41541080164422983 | validation: 0.32823818224039547]
	TIME [epoch: 8.53 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3806294813008732		[learning rate: 0.00047361]
	Learning Rate: 0.00047361
	LOSS [training: 0.3806294813008732 | validation: 0.33074782813330295]
	TIME [epoch: 8.57 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.41121229713548646		[learning rate: 0.00047189]
	Learning Rate: 0.000471891
	LOSS [training: 0.41121229713548646 | validation: 0.30887574564789744]
	TIME [epoch: 8.53 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38124514295313516		[learning rate: 0.00047018]
	Learning Rate: 0.000470179
	LOSS [training: 0.38124514295313516 | validation: 0.32278970498748705]
	TIME [epoch: 8.53 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3949874012267127		[learning rate: 0.00046847]
	Learning Rate: 0.000468473
	LOSS [training: 0.3949874012267127 | validation: 0.2988184267852529]
	TIME [epoch: 8.53 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37683011490737556		[learning rate: 0.00046677]
	Learning Rate: 0.000466773
	LOSS [training: 0.37683011490737556 | validation: 0.39769397004829476]
	TIME [epoch: 8.58 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.40857146698064406		[learning rate: 0.00046508]
	Learning Rate: 0.000465079
	LOSS [training: 0.40857146698064406 | validation: 0.3200725653527929]
	TIME [epoch: 8.53 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3662000084710444		[learning rate: 0.00046339]
	Learning Rate: 0.000463391
	LOSS [training: 0.3662000084710444 | validation: 0.27611936383499747]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3771260383740777		[learning rate: 0.00046171]
	Learning Rate: 0.000461709
	LOSS [training: 0.3771260383740777 | validation: 0.30409473621511973]
	TIME [epoch: 8.54 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3624165388335885		[learning rate: 0.00046003]
	Learning Rate: 0.000460034
	LOSS [training: 0.3624165388335885 | validation: 0.3595434290285594]
	TIME [epoch: 8.56 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37462785447739355		[learning rate: 0.00045836]
	Learning Rate: 0.000458364
	LOSS [training: 0.37462785447739355 | validation: 0.30332435383257217]
	TIME [epoch: 8.52 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3765515491751274		[learning rate: 0.0004567]
	Learning Rate: 0.000456701
	LOSS [training: 0.3765515491751274 | validation: 0.3169827824045337]
	TIME [epoch: 8.52 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36899101315820637		[learning rate: 0.00045504]
	Learning Rate: 0.000455043
	LOSS [training: 0.36899101315820637 | validation: 0.2891633694308174]
	TIME [epoch: 8.54 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3607468833347939		[learning rate: 0.00045339]
	Learning Rate: 0.000453392
	LOSS [training: 0.3607468833347939 | validation: 0.32978750461246037]
	TIME [epoch: 8.55 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3839056708857663		[learning rate: 0.00045175]
	Learning Rate: 0.000451746
	LOSS [training: 0.3839056708857663 | validation: 0.3811528780808524]
	TIME [epoch: 8.53 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3816192516464782		[learning rate: 0.00045011]
	Learning Rate: 0.000450107
	LOSS [training: 0.3816192516464782 | validation: 0.32211058438888085]
	TIME [epoch: 8.52 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3592848187964579		[learning rate: 0.00044847]
	Learning Rate: 0.000448474
	LOSS [training: 0.3592848187964579 | validation: 0.3122917852857987]
	TIME [epoch: 8.55 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3644703077710484		[learning rate: 0.00044685]
	Learning Rate: 0.000446846
	LOSS [training: 0.3644703077710484 | validation: 0.3514382524493216]
	TIME [epoch: 8.54 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3696856409457828		[learning rate: 0.00044522]
	Learning Rate: 0.000445224
	LOSS [training: 0.3696856409457828 | validation: 0.3131434180987408]
	TIME [epoch: 8.52 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.370227990205935		[learning rate: 0.00044361]
	Learning Rate: 0.000443609
	LOSS [training: 0.370227990205935 | validation: 0.40024918304368656]
	TIME [epoch: 8.52 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3765910319782495		[learning rate: 0.000442]
	Learning Rate: 0.000441999
	LOSS [training: 0.3765910319782495 | validation: 0.3395635629501141]
	TIME [epoch: 8.56 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36730315537034464		[learning rate: 0.00044039]
	Learning Rate: 0.000440395
	LOSS [training: 0.36730315537034464 | validation: 0.30423634609534345]
	TIME [epoch: 8.53 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3522577761086932		[learning rate: 0.0004388]
	Learning Rate: 0.000438797
	LOSS [training: 0.3522577761086932 | validation: 0.30356939963867413]
	TIME [epoch: 8.53 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3655238484902027		[learning rate: 0.0004372]
	Learning Rate: 0.000437204
	LOSS [training: 0.3655238484902027 | validation: 0.29037506289224874]
	TIME [epoch: 8.54 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36115048102706837		[learning rate: 0.00043562]
	Learning Rate: 0.000435617
	LOSS [training: 0.36115048102706837 | validation: 0.35742468924217197]
	TIME [epoch: 8.56 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35555216532829265		[learning rate: 0.00043404]
	Learning Rate: 0.000434037
	LOSS [training: 0.35555216532829265 | validation: 0.3021664339190942]
	TIME [epoch: 8.53 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3415445945091742		[learning rate: 0.00043246]
	Learning Rate: 0.000432461
	LOSS [training: 0.3415445945091742 | validation: 0.25935906946683546]
	TIME [epoch: 8.52 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_964.pth
	Model improved!!!
EPOCH 965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3722752064003858		[learning rate: 0.00043089]
	Learning Rate: 0.000430892
	LOSS [training: 0.3722752064003858 | validation: 0.2865653675937143]
	TIME [epoch: 8.55 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37451798334802644		[learning rate: 0.00042933]
	Learning Rate: 0.000429328
	LOSS [training: 0.37451798334802644 | validation: 0.3155616086916654]
	TIME [epoch: 8.55 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3528212671739678		[learning rate: 0.00042777]
	Learning Rate: 0.00042777
	LOSS [training: 0.3528212671739678 | validation: 0.3242155684769499]
	TIME [epoch: 8.52 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3644879576108926		[learning rate: 0.00042622]
	Learning Rate: 0.000426218
	LOSS [training: 0.3644879576108926 | validation: 0.2909181226502373]
	TIME [epoch: 8.53 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3610216379211975		[learning rate: 0.00042467]
	Learning Rate: 0.000424671
	LOSS [training: 0.3610216379211975 | validation: 0.3281983427223336]
	TIME [epoch: 8.54 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35923207284241065		[learning rate: 0.00042313]
	Learning Rate: 0.00042313
	LOSS [training: 0.35923207284241065 | validation: 0.29438164833457064]
	TIME [epoch: 8.55 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36329252129297573		[learning rate: 0.00042159]
	Learning Rate: 0.000421594
	LOSS [training: 0.36329252129297573 | validation: 0.2929711664727343]
	TIME [epoch: 8.52 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35534335561827474		[learning rate: 0.00042006]
	Learning Rate: 0.000420064
	LOSS [training: 0.35534335561827474 | validation: 0.3230206232026765]
	TIME [epoch: 8.53 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3787132503909395		[learning rate: 0.00041854]
	Learning Rate: 0.00041854
	LOSS [training: 0.3787132503909395 | validation: 0.3292276912709281]
	TIME [epoch: 8.54 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3530508693619964		[learning rate: 0.00041702]
	Learning Rate: 0.000417021
	LOSS [training: 0.3530508693619964 | validation: 0.34106515942920745]
	TIME [epoch: 8.54 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35508367176789296		[learning rate: 0.00041551]
	Learning Rate: 0.000415508
	LOSS [training: 0.35508367176789296 | validation: 0.29690834666574]
	TIME [epoch: 8.52 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3525530340937396		[learning rate: 0.000414]
	Learning Rate: 0.000414
	LOSS [training: 0.3525530340937396 | validation: 0.2700923237037073]
	TIME [epoch: 8.54 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.38402590378243506		[learning rate: 0.0004125]
	Learning Rate: 0.000412497
	LOSS [training: 0.38402590378243506 | validation: 0.345112798449504]
	TIME [epoch: 8.53 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3710846802699593		[learning rate: 0.000411]
	Learning Rate: 0.000411
	LOSS [training: 0.3710846802699593 | validation: 0.30636013680631197]
	TIME [epoch: 8.55 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.348866115932212		[learning rate: 0.00040951]
	Learning Rate: 0.000409509
	LOSS [training: 0.348866115932212 | validation: 0.3208587235400184]
	TIME [epoch: 8.53 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3677514920447117		[learning rate: 0.00040802]
	Learning Rate: 0.000408023
	LOSS [training: 0.3677514920447117 | validation: 0.2782411498425683]
	TIME [epoch: 8.54 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.4025211129683792		[learning rate: 0.00040654]
	Learning Rate: 0.000406542
	LOSS [training: 0.4025211129683792 | validation: 0.29727622448073804]
	TIME [epoch: 8.53 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3617059928155644		[learning rate: 0.00040507]
	Learning Rate: 0.000405067
	LOSS [training: 0.3617059928155644 | validation: 0.34475091811807845]
	TIME [epoch: 8.55 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3729797525647426		[learning rate: 0.0004036]
	Learning Rate: 0.000403597
	LOSS [training: 0.3729797525647426 | validation: 0.2953449455654643]
	TIME [epoch: 8.53 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3527032502476673		[learning rate: 0.00040213]
	Learning Rate: 0.000402132
	LOSS [training: 0.3527032502476673 | validation: 0.28788119489269304]
	TIME [epoch: 8.55 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3366497311429419		[learning rate: 0.00040067]
	Learning Rate: 0.000400672
	LOSS [training: 0.3366497311429419 | validation: 0.2908785746277517]
	TIME [epoch: 8.54 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3504705525101533		[learning rate: 0.00039922]
	Learning Rate: 0.000399218
	LOSS [training: 0.3504705525101533 | validation: 0.31001153794579717]
	TIME [epoch: 8.54 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3792961849830413		[learning rate: 0.00039777]
	Learning Rate: 0.00039777
	LOSS [training: 0.3792961849830413 | validation: 0.33867494856813324]
	TIME [epoch: 8.53 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37150607123648854		[learning rate: 0.00039633]
	Learning Rate: 0.000396326
	LOSS [training: 0.37150607123648854 | validation: 0.2743297117958598]
	TIME [epoch: 8.54 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3608126638438797		[learning rate: 0.00039489]
	Learning Rate: 0.000394888
	LOSS [training: 0.3608126638438797 | validation: 0.2804534891381221]
	TIME [epoch: 8.55 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35440337809134514		[learning rate: 0.00039345]
	Learning Rate: 0.000393455
	LOSS [training: 0.35440337809134514 | validation: 0.2868301121879543]
	TIME [epoch: 8.52 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3504307193617767		[learning rate: 0.00039203]
	Learning Rate: 0.000392027
	LOSS [training: 0.3504307193617767 | validation: 0.3105315798678523]
	TIME [epoch: 8.54 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.353239707508096		[learning rate: 0.0003906]
	Learning Rate: 0.000390604
	LOSS [training: 0.353239707508096 | validation: 0.27784106460530433]
	TIME [epoch: 8.53 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37773353770564977		[learning rate: 0.00038919]
	Learning Rate: 0.000389187
	LOSS [training: 0.37773353770564977 | validation: 0.2716103500351683]
	TIME [epoch: 8.54 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3721858570667327		[learning rate: 0.00038777]
	Learning Rate: 0.000387774
	LOSS [training: 0.3721858570667327 | validation: 0.30087576615060435]
	TIME [epoch: 8.53 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34434257519255473		[learning rate: 0.00038637]
	Learning Rate: 0.000386367
	LOSS [training: 0.34434257519255473 | validation: 0.3155933812564494]
	TIME [epoch: 8.54 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3363117772465332		[learning rate: 0.00038496]
	Learning Rate: 0.000384965
	LOSS [training: 0.3363117772465332 | validation: 0.2691149194558035]
	TIME [epoch: 8.53 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3468787033683052		[learning rate: 0.00038357]
	Learning Rate: 0.000383568
	LOSS [training: 0.3468787033683052 | validation: 0.29516372935006435]
	TIME [epoch: 8.55 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3490129433319046		[learning rate: 0.00038218]
	Learning Rate: 0.000382176
	LOSS [training: 0.3490129433319046 | validation: 0.2944252612132608]
	TIME [epoch: 8.53 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3362499742733676		[learning rate: 0.00038079]
	Learning Rate: 0.000380789
	LOSS [training: 0.3362499742733676 | validation: 0.2810351928117678]
	TIME [epoch: 8.55 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36073985605721814		[learning rate: 0.00037941]
	Learning Rate: 0.000379407
	LOSS [training: 0.36073985605721814 | validation: 0.3356719742134212]
	TIME [epoch: 8.52 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3637251100453143		[learning rate: 0.00037803]
	Learning Rate: 0.00037803
	LOSS [training: 0.3637251100453143 | validation: 0.2807820276448527]
	TIME [epoch: 8.55 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34927913168703273		[learning rate: 0.00037666]
	Learning Rate: 0.000376658
	LOSS [training: 0.34927913168703273 | validation: 0.2865000471777098]
	TIME [epoch: 8.53 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3392838374702875		[learning rate: 0.00037529]
	Learning Rate: 0.000375291
	LOSS [training: 0.3392838374702875 | validation: 0.33307960920189283]
	TIME [epoch: 8.53 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35806351469939257		[learning rate: 0.00037393]
	Learning Rate: 0.000373929
	LOSS [training: 0.35806351469939257 | validation: 0.3098221762298139]
	TIME [epoch: 8.52 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34195585268200396		[learning rate: 0.00037257]
	Learning Rate: 0.000372572
	LOSS [training: 0.34195585268200396 | validation: 0.3038889539032457]
	TIME [epoch: 8.55 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3531647071071095		[learning rate: 0.00037122]
	Learning Rate: 0.00037122
	LOSS [training: 0.3531647071071095 | validation: 0.2817910235431515]
	TIME [epoch: 8.54 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34603362134769833		[learning rate: 0.00036987]
	Learning Rate: 0.000369873
	LOSS [training: 0.34603362134769833 | validation: 0.27642619136001656]
	TIME [epoch: 8.53 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33991003158319416		[learning rate: 0.00036853]
	Learning Rate: 0.000368531
	LOSS [training: 0.33991003158319416 | validation: 0.3378950724853714]
	TIME [epoch: 8.53 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3741680861884135		[learning rate: 0.00036719]
	Learning Rate: 0.000367193
	LOSS [training: 0.3741680861884135 | validation: 0.27658117112825464]
	TIME [epoch: 8.54 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3281252651475891		[learning rate: 0.00036586]
	Learning Rate: 0.000365861
	LOSS [training: 0.3281252651475891 | validation: 0.2784148861814234]
	TIME [epoch: 8.55 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.37003984202951556		[learning rate: 0.00036453]
	Learning Rate: 0.000364533
	LOSS [training: 0.37003984202951556 | validation: 0.3141416880584097]
	TIME [epoch: 8.53 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33886408578198324		[learning rate: 0.00036321]
	Learning Rate: 0.00036321
	LOSS [training: 0.33886408578198324 | validation: 0.2764467152881433]
	TIME [epoch: 8.53 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3384306397000084		[learning rate: 0.00036189]
	Learning Rate: 0.000361892
	LOSS [training: 0.3384306397000084 | validation: 0.30115603922345086]
	TIME [epoch: 8.55 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34978016882416735		[learning rate: 0.00036058]
	Learning Rate: 0.000360579
	LOSS [training: 0.34978016882416735 | validation: 0.3103880359024186]
	TIME [epoch: 8.55 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35285496929396215		[learning rate: 0.00035927]
	Learning Rate: 0.00035927
	LOSS [training: 0.35285496929396215 | validation: 0.27271045984524955]
	TIME [epoch: 8.52 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3640039585912787		[learning rate: 0.00035797]
	Learning Rate: 0.000357966
	LOSS [training: 0.3640039585912787 | validation: 0.32375356318422677]
	TIME [epoch: 8.54 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3463732113259811		[learning rate: 0.00035667]
	Learning Rate: 0.000356667
	LOSS [training: 0.3463732113259811 | validation: 0.3126020303644413]
	TIME [epoch: 8.55 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3600692804878705		[learning rate: 0.00035537]
	Learning Rate: 0.000355373
	LOSS [training: 0.3600692804878705 | validation: 0.3125732479475474]
	TIME [epoch: 8.54 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3612674015579729		[learning rate: 0.00035408]
	Learning Rate: 0.000354083
	LOSS [training: 0.3612674015579729 | validation: 0.33337843397306915]
	TIME [epoch: 8.52 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3654024099566362		[learning rate: 0.0003528]
	Learning Rate: 0.000352798
	LOSS [training: 0.3654024099566362 | validation: 0.32117352614350814]
	TIME [epoch: 8.55 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34249167845681366		[learning rate: 0.00035152]
	Learning Rate: 0.000351518
	LOSS [training: 0.34249167845681366 | validation: 0.2897058475742094]
	TIME [epoch: 8.54 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34069616001853065		[learning rate: 0.00035024]
	Learning Rate: 0.000350242
	LOSS [training: 0.34069616001853065 | validation: 0.3076063088020479]
	TIME [epoch: 8.54 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3359765511568504		[learning rate: 0.00034897]
	Learning Rate: 0.000348971
	LOSS [training: 0.3359765511568504 | validation: 0.3002669224866161]
	TIME [epoch: 8.53 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.35086316764551734		[learning rate: 0.0003477]
	Learning Rate: 0.000347705
	LOSS [training: 0.35086316764551734 | validation: 0.2840687807397556]
	TIME [epoch: 8.55 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3407058508145169		[learning rate: 0.00034644]
	Learning Rate: 0.000346443
	LOSS [training: 0.3407058508145169 | validation: 0.31498788952335616]
	TIME [epoch: 8.55 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3516293625388264		[learning rate: 0.00034519]
	Learning Rate: 0.000345186
	LOSS [training: 0.3516293625388264 | validation: 0.3180876936945457]
	TIME [epoch: 8.53 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.36514482033951234		[learning rate: 0.00034393]
	Learning Rate: 0.000343933
	LOSS [training: 0.36514482033951234 | validation: 0.3256023697952243]
	TIME [epoch: 8.53 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34298433540464857		[learning rate: 0.00034268]
	Learning Rate: 0.000342685
	LOSS [training: 0.34298433540464857 | validation: 0.2961154982554132]
	TIME [epoch: 8.55 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32863260705858355		[learning rate: 0.00034144]
	Learning Rate: 0.000341441
	LOSS [training: 0.32863260705858355 | validation: 0.30494642922300186]
	TIME [epoch: 8.55 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32891537010364563		[learning rate: 0.0003402]
	Learning Rate: 0.000340202
	LOSS [training: 0.32891537010364563 | validation: 0.2853915771008038]
	TIME [epoch: 8.53 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34658769904253517		[learning rate: 0.00033897]
	Learning Rate: 0.000338967
	LOSS [training: 0.34658769904253517 | validation: 0.3151937046773691]
	TIME [epoch: 8.53 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34370907406584783		[learning rate: 0.00033774]
	Learning Rate: 0.000337737
	LOSS [training: 0.34370907406584783 | validation: 0.28815380566870724]
	TIME [epoch: 8.56 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33668373836015136		[learning rate: 0.00033651]
	Learning Rate: 0.000336512
	LOSS [training: 0.33668373836015136 | validation: 0.31321167091554497]
	TIME [epoch: 8.54 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3402324524970043		[learning rate: 0.00033529]
	Learning Rate: 0.00033529
	LOSS [training: 0.3402324524970043 | validation: 0.3008977009783716]
	TIME [epoch: 8.53 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33045795474526996		[learning rate: 0.00033407]
	Learning Rate: 0.000334074
	LOSS [training: 0.33045795474526996 | validation: 0.2842362280262539]
	TIME [epoch: 8.53 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3348819660964083		[learning rate: 0.00033286]
	Learning Rate: 0.000332861
	LOSS [training: 0.3348819660964083 | validation: 0.28804394985637183]
	TIME [epoch: 8.56 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3440423241987518		[learning rate: 0.00033165]
	Learning Rate: 0.000331653
	LOSS [training: 0.3440423241987518 | validation: 0.29604001526982904]
	TIME [epoch: 8.54 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34266547515635143		[learning rate: 0.00033045]
	Learning Rate: 0.00033045
	LOSS [training: 0.34266547515635143 | validation: 0.3184575404100292]
	TIME [epoch: 8.53 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33256647428769565		[learning rate: 0.00032925]
	Learning Rate: 0.00032925
	LOSS [training: 0.33256647428769565 | validation: 0.30415888943246916]
	TIME [epoch: 8.52 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34397101870796887		[learning rate: 0.00032806]
	Learning Rate: 0.000328056
	LOSS [training: 0.34397101870796887 | validation: 0.3163237878546664]
	TIME [epoch: 8.57 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3301270933156443		[learning rate: 0.00032686]
	Learning Rate: 0.000326865
	LOSS [training: 0.3301270933156443 | validation: 0.30966475806083527]
	TIME [epoch: 8.53 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3356788986833327		[learning rate: 0.00032568]
	Learning Rate: 0.000325679
	LOSS [training: 0.3356788986833327 | validation: 0.2782238949537831]
	TIME [epoch: 8.53 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3386226994670984		[learning rate: 0.0003245]
	Learning Rate: 0.000324497
	LOSS [training: 0.3386226994670984 | validation: 0.2899968532107096]
	TIME [epoch: 8.55 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32548802576020486		[learning rate: 0.00032332]
	Learning Rate: 0.000323319
	LOSS [training: 0.32548802576020486 | validation: 0.2833788405194554]
	TIME [epoch: 8.56 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3403495435609808		[learning rate: 0.00032215]
	Learning Rate: 0.000322146
	LOSS [training: 0.3403495435609808 | validation: 0.2879874571439729]
	TIME [epoch: 8.53 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3277056580777919		[learning rate: 0.00032098]
	Learning Rate: 0.000320977
	LOSS [training: 0.3277056580777919 | validation: 0.2706161009259751]
	TIME [epoch: 8.53 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3502306050976448		[learning rate: 0.00031981]
	Learning Rate: 0.000319812
	LOSS [training: 0.3502306050976448 | validation: 0.29016622791787916]
	TIME [epoch: 8.55 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32628524353347854		[learning rate: 0.00031865]
	Learning Rate: 0.000318651
	LOSS [training: 0.32628524353347854 | validation: 0.28107559984912805]
	TIME [epoch: 8.54 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3283301454837791		[learning rate: 0.0003175]
	Learning Rate: 0.000317495
	LOSS [training: 0.3283301454837791 | validation: 0.3056760674532166]
	TIME [epoch: 8.53 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3242708939247326		[learning rate: 0.00031634]
	Learning Rate: 0.000316343
	LOSS [training: 0.3242708939247326 | validation: 0.29635388769082865]
	TIME [epoch: 8.52 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32210090646535455		[learning rate: 0.00031519]
	Learning Rate: 0.000315195
	LOSS [training: 0.32210090646535455 | validation: 0.29274674441445375]
	TIME [epoch: 8.56 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34201681797538785		[learning rate: 0.00031405]
	Learning Rate: 0.000314051
	LOSS [training: 0.34201681797538785 | validation: 0.27112429011493644]
	TIME [epoch: 8.53 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32113865144650516		[learning rate: 0.00031291]
	Learning Rate: 0.000312911
	LOSS [training: 0.32113865144650516 | validation: 0.2850933407603302]
	TIME [epoch: 8.53 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3299140853012509		[learning rate: 0.00031178]
	Learning Rate: 0.000311776
	LOSS [training: 0.3299140853012509 | validation: 0.2873900284165214]
	TIME [epoch: 8.52 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32821476161949714		[learning rate: 0.00031064]
	Learning Rate: 0.000310644
	LOSS [training: 0.32821476161949714 | validation: 0.31742234022011495]
	TIME [epoch: 8.57 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3205167324319581		[learning rate: 0.00030952]
	Learning Rate: 0.000309517
	LOSS [training: 0.3205167324319581 | validation: 0.2660351056191167]
	TIME [epoch: 8.53 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32376715149578095		[learning rate: 0.00030839]
	Learning Rate: 0.000308394
	LOSS [training: 0.32376715149578095 | validation: 0.2633985003259671]
	TIME [epoch: 8.53 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33101437804776107		[learning rate: 0.00030727]
	Learning Rate: 0.000307274
	LOSS [training: 0.33101437804776107 | validation: 0.26343490651238577]
	TIME [epoch: 8.53 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3670796078352194		[learning rate: 0.00030616]
	Learning Rate: 0.000306159
	LOSS [training: 0.3670796078352194 | validation: 0.2781906350775612]
	TIME [epoch: 8.56 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31772570445481496		[learning rate: 0.00030505]
	Learning Rate: 0.000305048
	LOSS [training: 0.31772570445481496 | validation: 0.27120325220317765]
	TIME [epoch: 8.53 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3480394970786303		[learning rate: 0.00030394]
	Learning Rate: 0.000303941
	LOSS [training: 0.3480394970786303 | validation: 0.2941296165701534]
	TIME [epoch: 8.52 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3342599248274634		[learning rate: 0.00030284]
	Learning Rate: 0.000302838
	LOSS [training: 0.3342599248274634 | validation: 0.3032100576494312]
	TIME [epoch: 8.54 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31407058160491885		[learning rate: 0.00030174]
	Learning Rate: 0.000301739
	LOSS [training: 0.31407058160491885 | validation: 0.2631099902016944]
	TIME [epoch: 8.56 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3163391492832307		[learning rate: 0.00030064]
	Learning Rate: 0.000300644
	LOSS [training: 0.3163391492832307 | validation: 0.2653908320433965]
	TIME [epoch: 8.54 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32048988547383284		[learning rate: 0.00029955]
	Learning Rate: 0.000299553
	LOSS [training: 0.32048988547383284 | validation: 0.28467932231021725]
	TIME [epoch: 8.53 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3155131584311155		[learning rate: 0.00029847]
	Learning Rate: 0.000298466
	LOSS [training: 0.3155131584311155 | validation: 0.26387641961408037]
	TIME [epoch: 8.55 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3214188929281204		[learning rate: 0.00029738]
	Learning Rate: 0.000297383
	LOSS [training: 0.3214188929281204 | validation: 0.2551838551973442]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3187051234143427		[learning rate: 0.0002963]
	Learning Rate: 0.000296304
	LOSS [training: 0.3187051234143427 | validation: 0.29475878739705097]
	TIME [epoch: 8.54 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3351908442792406		[learning rate: 0.00029523]
	Learning Rate: 0.000295228
	LOSS [training: 0.3351908442792406 | validation: 0.2582975954354267]
	TIME [epoch: 8.52 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33228888471942064		[learning rate: 0.00029416]
	Learning Rate: 0.000294157
	LOSS [training: 0.33228888471942064 | validation: 0.3359231532522161]
	TIME [epoch: 8.54 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3452439052565711		[learning rate: 0.00029309]
	Learning Rate: 0.000293089
	LOSS [training: 0.3452439052565711 | validation: 0.2851224176136149]
	TIME [epoch: 8.54 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3273103087484516		[learning rate: 0.00029203]
	Learning Rate: 0.000292026
	LOSS [training: 0.3273103087484516 | validation: 0.2906459317263523]
	TIME [epoch: 8.52 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31985477710315197		[learning rate: 0.00029097]
	Learning Rate: 0.000290966
	LOSS [training: 0.31985477710315197 | validation: 0.25687939071314553]
	TIME [epoch: 8.53 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3475935808001653		[learning rate: 0.00028991]
	Learning Rate: 0.00028991
	LOSS [training: 0.3475935808001653 | validation: 0.2854469781926384]
	TIME [epoch: 8.54 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3420273739322052		[learning rate: 0.00028886]
	Learning Rate: 0.000288858
	LOSS [training: 0.3420273739322052 | validation: 0.29217738729115494]
	TIME [epoch: 8.54 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34780131750383325		[learning rate: 0.00028781]
	Learning Rate: 0.00028781
	LOSS [training: 0.34780131750383325 | validation: 0.2599733483637709]
	TIME [epoch: 8.52 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31697153357546337		[learning rate: 0.00028677]
	Learning Rate: 0.000286765
	LOSS [training: 0.31697153357546337 | validation: 0.2496882859548507]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1077.pth
	Model improved!!!
EPOCH 1078/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32299216593492697		[learning rate: 0.00028572]
	Learning Rate: 0.000285724
	LOSS [training: 0.32299216593492697 | validation: 0.2568290268731476]
	TIME [epoch: 8.55 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31630399083130933		[learning rate: 0.00028469]
	Learning Rate: 0.000284688
	LOSS [training: 0.31630399083130933 | validation: 0.2822858502768918]
	TIME [epoch: 8.53 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31535279382830844		[learning rate: 0.00028365]
	Learning Rate: 0.000283654
	LOSS [training: 0.31535279382830844 | validation: 0.2782213983080093]
	TIME [epoch: 8.53 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30981867150363374		[learning rate: 0.00028263]
	Learning Rate: 0.000282625
	LOSS [training: 0.30981867150363374 | validation: 0.29184252097640856]
	TIME [epoch: 8.55 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31553684965903306		[learning rate: 0.0002816]
	Learning Rate: 0.000281599
	LOSS [training: 0.31553684965903306 | validation: 0.2654136417237748]
	TIME [epoch: 8.55 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30926200910123997		[learning rate: 0.00028058]
	Learning Rate: 0.000280577
	LOSS [training: 0.30926200910123997 | validation: 0.27302515366376073]
	TIME [epoch: 8.54 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32020382052559543		[learning rate: 0.00027956]
	Learning Rate: 0.000279559
	LOSS [training: 0.32020382052559543 | validation: 0.2712865800147213]
	TIME [epoch: 8.52 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32438593622486717		[learning rate: 0.00027854]
	Learning Rate: 0.000278545
	LOSS [training: 0.32438593622486717 | validation: 0.2933339136517235]
	TIME [epoch: 8.55 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32685978706287644		[learning rate: 0.00027753]
	Learning Rate: 0.000277534
	LOSS [training: 0.32685978706287644 | validation: 0.28366662046028623]
	TIME [epoch: 8.54 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3218584124854748		[learning rate: 0.00027653]
	Learning Rate: 0.000276527
	LOSS [training: 0.3218584124854748 | validation: 0.24945809193946417]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1087.pth
	Model improved!!!
EPOCH 1088/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3074351253390296		[learning rate: 0.00027552]
	Learning Rate: 0.000275523
	LOSS [training: 0.3074351253390296 | validation: 0.2645588085843506]
	TIME [epoch: 8.54 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34028277015609953		[learning rate: 0.00027452]
	Learning Rate: 0.000274523
	LOSS [training: 0.34028277015609953 | validation: 0.27085597369824543]
	TIME [epoch: 8.53 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31334082592355866		[learning rate: 0.00027353]
	Learning Rate: 0.000273527
	LOSS [training: 0.31334082592355866 | validation: 0.2671943652342754]
	TIME [epoch: 8.55 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32914668930197155		[learning rate: 0.00027253]
	Learning Rate: 0.000272534
	LOSS [training: 0.32914668930197155 | validation: 0.26044115841488824]
	TIME [epoch: 8.54 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3092796194438389		[learning rate: 0.00027155]
	Learning Rate: 0.000271545
	LOSS [training: 0.3092796194438389 | validation: 0.2578761525199084]
	TIME [epoch: 8.55 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3159089607610136		[learning rate: 0.00027056]
	Learning Rate: 0.00027056
	LOSS [training: 0.3159089607610136 | validation: 0.27553963685672667]
	TIME [epoch: 8.53 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32672418462829933		[learning rate: 0.00026958]
	Learning Rate: 0.000269578
	LOSS [training: 0.32672418462829933 | validation: 0.29297258680696076]
	TIME [epoch: 8.55 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31784638134763765		[learning rate: 0.0002686]
	Learning Rate: 0.0002686
	LOSS [training: 0.31784638134763765 | validation: 0.32556266499172587]
	TIME [epoch: 8.52 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3293159403658098		[learning rate: 0.00026762]
	Learning Rate: 0.000267625
	LOSS [training: 0.3293159403658098 | validation: 0.29178809971621156]
	TIME [epoch: 8.55 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3105176680358716		[learning rate: 0.00026665]
	Learning Rate: 0.000266654
	LOSS [training: 0.3105176680358716 | validation: 0.2710026554928223]
	TIME [epoch: 8.54 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3132960523294809		[learning rate: 0.00026569]
	Learning Rate: 0.000265686
	LOSS [training: 0.3132960523294809 | validation: 0.2756291734522519]
	TIME [epoch: 8.56 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31825783346124575		[learning rate: 0.00026472]
	Learning Rate: 0.000264722
	LOSS [training: 0.31825783346124575 | validation: 0.28899934373708547]
	TIME [epoch: 8.54 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3135412426664715		[learning rate: 0.00026376]
	Learning Rate: 0.000263761
	LOSS [training: 0.3135412426664715 | validation: 0.2890026760435247]
	TIME [epoch: 8.55 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3170080446674203		[learning rate: 0.0002628]
	Learning Rate: 0.000262804
	LOSS [training: 0.3170080446674203 | validation: 0.28475763900744044]
	TIME [epoch: 8.54 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31118227863710785		[learning rate: 0.00026185]
	Learning Rate: 0.00026185
	LOSS [training: 0.31118227863710785 | validation: 0.27588771608437546]
	TIME [epoch: 8.53 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31545906345205915		[learning rate: 0.0002609]
	Learning Rate: 0.0002609
	LOSS [training: 0.31545906345205915 | validation: 0.25448403723453705]
	TIME [epoch: 8.55 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31104144858073374		[learning rate: 0.00025995]
	Learning Rate: 0.000259953
	LOSS [training: 0.31104144858073374 | validation: 0.27108136466895905]
	TIME [epoch: 8.53 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30938929354005495		[learning rate: 0.00025901]
	Learning Rate: 0.00025901
	LOSS [training: 0.30938929354005495 | validation: 0.28417251702832474]
	TIME [epoch: 8.55 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30842499926375255		[learning rate: 0.00025807]
	Learning Rate: 0.00025807
	LOSS [training: 0.30842499926375255 | validation: 0.26279259618177786]
	TIME [epoch: 8.53 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31522247933071057		[learning rate: 0.00025713]
	Learning Rate: 0.000257133
	LOSS [training: 0.31522247933071057 | validation: 0.28793141005718254]
	TIME [epoch: 8.55 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.309473081099375		[learning rate: 0.0002562]
	Learning Rate: 0.0002562
	LOSS [training: 0.309473081099375 | validation: 0.28635547863415717]
	TIME [epoch: 8.53 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3092571248555092		[learning rate: 0.00025527]
	Learning Rate: 0.00025527
	LOSS [training: 0.3092571248555092 | validation: 0.2808889784547162]
	TIME [epoch: 8.55 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.32017688555243645		[learning rate: 0.00025434]
	Learning Rate: 0.000254344
	LOSS [training: 0.32017688555243645 | validation: 0.3538206319537385]
	TIME [epoch: 8.54 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31607541456658855		[learning rate: 0.00025342]
	Learning Rate: 0.000253421
	LOSS [training: 0.31607541456658855 | validation: 0.28334795482551184]
	TIME [epoch: 8.55 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3055992679127964		[learning rate: 0.0002525]
	Learning Rate: 0.000252501
	LOSS [training: 0.3055992679127964 | validation: 0.2863060058833431]
	TIME [epoch: 8.53 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3035600472579176		[learning rate: 0.00025158]
	Learning Rate: 0.000251585
	LOSS [training: 0.3035600472579176 | validation: 0.28016041512000356]
	TIME [epoch: 8.55 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3195912175656269		[learning rate: 0.00025067]
	Learning Rate: 0.000250672
	LOSS [training: 0.3195912175656269 | validation: 0.27376755227541666]
	TIME [epoch: 8.54 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3129732917227144		[learning rate: 0.00024976]
	Learning Rate: 0.000249762
	LOSS [training: 0.3129732917227144 | validation: 0.28447555355665105]
	TIME [epoch: 8.53 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.310001446630414		[learning rate: 0.00024886]
	Learning Rate: 0.000248856
	LOSS [training: 0.310001446630414 | validation: 0.26295404523635896]
	TIME [epoch: 8.53 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30248098395024947		[learning rate: 0.00024795]
	Learning Rate: 0.000247952
	LOSS [training: 0.30248098395024947 | validation: 0.310974914140782]
	TIME [epoch: 8.55 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3148946143864542		[learning rate: 0.00024705]
	Learning Rate: 0.000247053
	LOSS [training: 0.3148946143864542 | validation: 0.2756409255604756]
	TIME [epoch: 8.55 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3089293960851939		[learning rate: 0.00024616]
	Learning Rate: 0.000246156
	LOSS [training: 0.3089293960851939 | validation: 0.2962125577993673]
	TIME [epoch: 8.53 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3091999585692862		[learning rate: 0.00024526]
	Learning Rate: 0.000245263
	LOSS [training: 0.3091999585692862 | validation: 0.28427685519687096]
	TIME [epoch: 8.54 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3120253103043114		[learning rate: 0.00024437]
	Learning Rate: 0.000244373
	LOSS [training: 0.3120253103043114 | validation: 0.29456164358924275]
	TIME [epoch: 8.55 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3149251040627231		[learning rate: 0.00024349]
	Learning Rate: 0.000243486
	LOSS [training: 0.3149251040627231 | validation: 0.29357326910668535]
	TIME [epoch: 8.56 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3055485476733685		[learning rate: 0.0002426]
	Learning Rate: 0.000242602
	LOSS [training: 0.3055485476733685 | validation: 0.2787238144315165]
	TIME [epoch: 8.53 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3011426816280496		[learning rate: 0.00024172]
	Learning Rate: 0.000241722
	LOSS [training: 0.3011426816280496 | validation: 0.28876813705569393]
	TIME [epoch: 8.53 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.34500948312263546		[learning rate: 0.00024084]
	Learning Rate: 0.000240845
	LOSS [training: 0.34500948312263546 | validation: 0.28251572339286674]
	TIME [epoch: 8.56 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3183676024204139		[learning rate: 0.00023997]
	Learning Rate: 0.000239971
	LOSS [training: 0.3183676024204139 | validation: 0.28610897443202776]
	TIME [epoch: 8.54 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3022524524469691		[learning rate: 0.0002391]
	Learning Rate: 0.0002391
	LOSS [training: 0.3022524524469691 | validation: 0.2547048531824779]
	TIME [epoch: 8.53 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30570916941979376		[learning rate: 0.00023823]
	Learning Rate: 0.000238232
	LOSS [training: 0.30570916941979376 | validation: 0.2662800133819291]
	TIME [epoch: 8.53 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3099316130927177		[learning rate: 0.00023737]
	Learning Rate: 0.000237367
	LOSS [training: 0.3099316130927177 | validation: 0.26014363562282544]
	TIME [epoch: 8.58 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3069912771217222		[learning rate: 0.00023651]
	Learning Rate: 0.000236506
	LOSS [training: 0.3069912771217222 | validation: 0.2565265079348136]
	TIME [epoch: 8.54 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2984414705158424		[learning rate: 0.00023565]
	Learning Rate: 0.000235648
	LOSS [training: 0.2984414705158424 | validation: 0.26253132948943325]
	TIME [epoch: 8.54 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3077395208594317		[learning rate: 0.00023479]
	Learning Rate: 0.000234793
	LOSS [training: 0.3077395208594317 | validation: 0.2618493091841816]
	TIME [epoch: 8.54 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3039481733225077		[learning rate: 0.00023394]
	Learning Rate: 0.00023394
	LOSS [training: 0.3039481733225077 | validation: 0.2618038458516983]
	TIME [epoch: 8.57 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2980383920102244		[learning rate: 0.00023309]
	Learning Rate: 0.000233091
	LOSS [training: 0.2980383920102244 | validation: 0.24702834879279323]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1134.pth
	Model improved!!!
EPOCH 1135/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29764172686179		[learning rate: 0.00023225]
	Learning Rate: 0.000232246
	LOSS [training: 0.29764172686179 | validation: 0.25407401204480534]
	TIME [epoch: 8.53 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29812022826545076		[learning rate: 0.0002314]
	Learning Rate: 0.000231403
	LOSS [training: 0.29812022826545076 | validation: 0.2691828430266988]
	TIME [epoch: 8.56 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2939087047851405		[learning rate: 0.00023056]
	Learning Rate: 0.000230563
	LOSS [training: 0.2939087047851405 | validation: 0.285256332405013]
	TIME [epoch: 8.55 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3124018374401527		[learning rate: 0.00022973]
	Learning Rate: 0.000229726
	LOSS [training: 0.3124018374401527 | validation: 0.28632117639295196]
	TIME [epoch: 8.53 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30802502625098144		[learning rate: 0.00022889]
	Learning Rate: 0.000228893
	LOSS [training: 0.30802502625098144 | validation: 0.2715495786909663]
	TIME [epoch: 8.53 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30308361202072837		[learning rate: 0.00022806]
	Learning Rate: 0.000228062
	LOSS [training: 0.30308361202072837 | validation: 0.2612567404909978]
	TIME [epoch: 8.57 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2933511426175771		[learning rate: 0.00022723]
	Learning Rate: 0.000227234
	LOSS [training: 0.2933511426175771 | validation: 0.2635124841306026]
	TIME [epoch: 8.54 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30398468473799395		[learning rate: 0.00022641]
	Learning Rate: 0.00022641
	LOSS [training: 0.30398468473799395 | validation: 0.2651968023443124]
	TIME [epoch: 8.54 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.303741818704317		[learning rate: 0.00022559]
	Learning Rate: 0.000225588
	LOSS [training: 0.303741818704317 | validation: 0.2631705485786428]
	TIME [epoch: 8.53 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3130129300286919		[learning rate: 0.00022477]
	Learning Rate: 0.000224769
	LOSS [training: 0.3130129300286919 | validation: 0.2890760390795696]
	TIME [epoch: 8.57 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29980768232353944		[learning rate: 0.00022395]
	Learning Rate: 0.000223954
	LOSS [training: 0.29980768232353944 | validation: 0.28846451412166896]
	TIME [epoch: 8.53 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3341350249944197		[learning rate: 0.00022314]
	Learning Rate: 0.000223141
	LOSS [training: 0.3341350249944197 | validation: 0.2764229270165077]
	TIME [epoch: 8.53 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3102174524553711		[learning rate: 0.00022233]
	Learning Rate: 0.000222331
	LOSS [training: 0.3102174524553711 | validation: 0.2504185508898419]
	TIME [epoch: 8.53 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29919416075545013		[learning rate: 0.00022152]
	Learning Rate: 0.000221524
	LOSS [training: 0.29919416075545013 | validation: 0.27290573119863754]
	TIME [epoch: 8.57 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2927562699051243		[learning rate: 0.00022072]
	Learning Rate: 0.00022072
	LOSS [training: 0.2927562699051243 | validation: 0.2660619178139079]
	TIME [epoch: 8.54 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.337507147370541		[learning rate: 0.00021992]
	Learning Rate: 0.000219919
	LOSS [training: 0.337507147370541 | validation: 0.2637382699878847]
	TIME [epoch: 8.54 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3022422421542518		[learning rate: 0.00021912]
	Learning Rate: 0.000219121
	LOSS [training: 0.3022422421542518 | validation: 0.28059192833554136]
	TIME [epoch: 8.54 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.326490416174984		[learning rate: 0.00021833]
	Learning Rate: 0.000218326
	LOSS [training: 0.326490416174984 | validation: 0.26444927987134564]
	TIME [epoch: 8.56 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3102683197269823		[learning rate: 0.00021753]
	Learning Rate: 0.000217534
	LOSS [training: 0.3102683197269823 | validation: 0.26344622878452506]
	TIME [epoch: 8.53 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30621060375028936		[learning rate: 0.00021674]
	Learning Rate: 0.000216744
	LOSS [training: 0.30621060375028936 | validation: 0.2712750676350866]
	TIME [epoch: 8.53 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3116818575402636		[learning rate: 0.00021596]
	Learning Rate: 0.000215958
	LOSS [training: 0.3116818575402636 | validation: 0.2616517458222212]
	TIME [epoch: 8.55 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3050933674515428		[learning rate: 0.00021517]
	Learning Rate: 0.000215174
	LOSS [training: 0.3050933674515428 | validation: 0.272485403538123]
	TIME [epoch: 8.55 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30601188118642864		[learning rate: 0.00021439]
	Learning Rate: 0.000214393
	LOSS [training: 0.30601188118642864 | validation: 0.24055648550670733]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1157.pth
	Model improved!!!
EPOCH 1158/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2891446234215746		[learning rate: 0.00021361]
	Learning Rate: 0.000213615
	LOSS [training: 0.2891446234215746 | validation: 0.2803509442155276]
	TIME [epoch: 8.53 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.31970870823094893		[learning rate: 0.00021284]
	Learning Rate: 0.00021284
	LOSS [training: 0.31970870823094893 | validation: 0.25146899067792894]
	TIME [epoch: 8.56 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3033050941208518		[learning rate: 0.00021207]
	Learning Rate: 0.000212067
	LOSS [training: 0.3033050941208518 | validation: 0.25768460809659444]
	TIME [epoch: 8.54 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29690217272737274		[learning rate: 0.0002113]
	Learning Rate: 0.000211298
	LOSS [training: 0.29690217272737274 | validation: 0.26678232908926175]
	TIME [epoch: 8.53 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30259340911354937		[learning rate: 0.00021053]
	Learning Rate: 0.000210531
	LOSS [training: 0.30259340911354937 | validation: 0.2660095532627029]
	TIME [epoch: 8.54 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30052401330096046		[learning rate: 0.00020977]
	Learning Rate: 0.000209767
	LOSS [training: 0.30052401330096046 | validation: 0.23322347524438944]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2911771511306201		[learning rate: 0.00020901]
	Learning Rate: 0.000209006
	LOSS [training: 0.2911771511306201 | validation: 0.2520422204450117]
	TIME [epoch: 8.55 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2924540222190748		[learning rate: 0.00020825]
	Learning Rate: 0.000208247
	LOSS [training: 0.2924540222190748 | validation: 0.25809112120707256]
	TIME [epoch: 8.54 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2975706560115673		[learning rate: 0.00020749]
	Learning Rate: 0.000207491
	LOSS [training: 0.2975706560115673 | validation: 0.2514564062529701]
	TIME [epoch: 8.55 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30842972175189975		[learning rate: 0.00020674]
	Learning Rate: 0.000206738
	LOSS [training: 0.30842972175189975 | validation: 0.2556802672299895]
	TIME [epoch: 8.58 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3112241424179588		[learning rate: 0.00020599]
	Learning Rate: 0.000205988
	LOSS [training: 0.3112241424179588 | validation: 0.2439817247898156]
	TIME [epoch: 8.54 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30699008066748223		[learning rate: 0.00020524]
	Learning Rate: 0.000205241
	LOSS [training: 0.30699008066748223 | validation: 0.24687104650100472]
	TIME [epoch: 8.54 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2942827692211344		[learning rate: 0.0002045]
	Learning Rate: 0.000204496
	LOSS [training: 0.2942827692211344 | validation: 0.2595270027914407]
	TIME [epoch: 8.56 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28388030725234287		[learning rate: 0.00020375]
	Learning Rate: 0.000203754
	LOSS [training: 0.28388030725234287 | validation: 0.2506279376255074]
	TIME [epoch: 8.57 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3015794705408354		[learning rate: 0.00020301]
	Learning Rate: 0.000203014
	LOSS [training: 0.3015794705408354 | validation: 0.2494236201589489]
	TIME [epoch: 8.54 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29178138975366624		[learning rate: 0.00020228]
	Learning Rate: 0.000202277
	LOSS [training: 0.29178138975366624 | validation: 0.2590642864240231]
	TIME [epoch: 8.53 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3014816145081286		[learning rate: 0.00020154]
	Learning Rate: 0.000201543
	LOSS [training: 0.3014816145081286 | validation: 0.2543311899548785]
	TIME [epoch: 8.56 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28826085198730195		[learning rate: 0.00020081]
	Learning Rate: 0.000200812
	LOSS [training: 0.28826085198730195 | validation: 0.25823404082469015]
	TIME [epoch: 8.56 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29896587189049284		[learning rate: 0.00020008]
	Learning Rate: 0.000200083
	LOSS [training: 0.29896587189049284 | validation: 0.25269509850722793]
	TIME [epoch: 8.54 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2865722970283096		[learning rate: 0.00019936]
	Learning Rate: 0.000199357
	LOSS [training: 0.2865722970283096 | validation: 0.26656582326319306]
	TIME [epoch: 8.55 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28761176452906745		[learning rate: 0.00019863]
	Learning Rate: 0.000198634
	LOSS [training: 0.28761176452906745 | validation: 0.2744441170609261]
	TIME [epoch: 8.55 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30123570948546574		[learning rate: 0.00019791]
	Learning Rate: 0.000197913
	LOSS [training: 0.30123570948546574 | validation: 0.2837742664644479]
	TIME [epoch: 8.56 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.33549206434656775		[learning rate: 0.00019719]
	Learning Rate: 0.000197194
	LOSS [training: 0.33549206434656775 | validation: 0.3172592522780888]
	TIME [epoch: 8.54 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3108989056236877		[learning rate: 0.00019648]
	Learning Rate: 0.000196479
	LOSS [training: 0.3108989056236877 | validation: 0.28804893078671023]
	TIME [epoch: 8.56 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3019382871104955		[learning rate: 0.00019577]
	Learning Rate: 0.000195766
	LOSS [training: 0.3019382871104955 | validation: 0.2601260342526751]
	TIME [epoch: 8.54 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29693074168769595		[learning rate: 0.00019506]
	Learning Rate: 0.000195055
	LOSS [training: 0.29693074168769595 | validation: 0.25736893467052085]
	TIME [epoch: 8.56 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29129625375684265		[learning rate: 0.00019435]
	Learning Rate: 0.000194347
	LOSS [training: 0.29129625375684265 | validation: 0.24417159114343834]
	TIME [epoch: 8.53 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.293167200591276		[learning rate: 0.00019364]
	Learning Rate: 0.000193642
	LOSS [training: 0.293167200591276 | validation: 0.24988750353805317]
	TIME [epoch: 8.56 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2862166868846843		[learning rate: 0.00019294]
	Learning Rate: 0.000192939
	LOSS [training: 0.2862166868846843 | validation: 0.25472850863630025]
	TIME [epoch: 8.55 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28199543034449126		[learning rate: 0.00019224]
	Learning Rate: 0.000192239
	LOSS [training: 0.28199543034449126 | validation: 0.24726294918325503]
	TIME [epoch: 8.55 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28227030919514906		[learning rate: 0.00019154]
	Learning Rate: 0.000191542
	LOSS [training: 0.28227030919514906 | validation: 0.26223561760491765]
	TIME [epoch: 8.54 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2997645080117585		[learning rate: 0.00019085]
	Learning Rate: 0.000190846
	LOSS [training: 0.2997645080117585 | validation: 0.2651572469530588]
	TIME [epoch: 8.56 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3103986533233344		[learning rate: 0.00019015]
	Learning Rate: 0.000190154
	LOSS [training: 0.3103986533233344 | validation: 0.239951628334991]
	TIME [epoch: 8.56 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2825485029746626		[learning rate: 0.00018946]
	Learning Rate: 0.000189464
	LOSS [training: 0.2825485029746626 | validation: 0.2526639604524093]
	TIME [epoch: 8.53 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2844090260966002		[learning rate: 0.00018878]
	Learning Rate: 0.000188776
	LOSS [training: 0.2844090260966002 | validation: 0.2592249864943137]
	TIME [epoch: 8.55 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2994717470473192		[learning rate: 0.00018809]
	Learning Rate: 0.000188091
	LOSS [training: 0.2994717470473192 | validation: 0.2571268545175291]
	TIME [epoch: 8.57 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30509470984594567		[learning rate: 0.00018741]
	Learning Rate: 0.000187409
	LOSS [training: 0.30509470984594567 | validation: 0.2891062393935287]
	TIME [epoch: 8.58 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28988630787145614		[learning rate: 0.00018673]
	Learning Rate: 0.000186729
	LOSS [training: 0.28988630787145614 | validation: 0.24688384034822342]
	TIME [epoch: 8.56 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30828456531377774		[learning rate: 0.00018605]
	Learning Rate: 0.000186051
	LOSS [training: 0.30828456531377774 | validation: 0.25911756691658083]
	TIME [epoch: 8.57 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2992884932563554		[learning rate: 0.00018538]
	Learning Rate: 0.000185376
	LOSS [training: 0.2992884932563554 | validation: 0.2596998112286032]
	TIME [epoch: 8.57 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.30634693098994986		[learning rate: 0.0001847]
	Learning Rate: 0.000184703
	LOSS [training: 0.30634693098994986 | validation: 0.2318228863036628]
	TIME [epoch: 8.59 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1198.pth
	Model improved!!!
EPOCH 1199/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29474601224623975		[learning rate: 0.00018403]
	Learning Rate: 0.000184033
	LOSS [training: 0.29474601224623975 | validation: 0.2248198720242643]
	TIME [epoch: 8.56 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2894926714836378		[learning rate: 0.00018336]
	Learning Rate: 0.000183365
	LOSS [training: 0.2894926714836378 | validation: 0.2563708023865608]
	TIME [epoch: 8.57 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2908948653177251		[learning rate: 0.0001827]
	Learning Rate: 0.000182699
	LOSS [training: 0.2908948653177251 | validation: 0.24047592255429645]
	TIME [epoch: 8.55 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29336066269681205		[learning rate: 0.00018204]
	Learning Rate: 0.000182036
	LOSS [training: 0.29336066269681205 | validation: 0.2502770352146078]
	TIME [epoch: 8.57 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3139739363793084		[learning rate: 0.00018138]
	Learning Rate: 0.000181376
	LOSS [training: 0.3139739363793084 | validation: 0.25989050255934826]
	TIME [epoch: 8.56 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2907812988989701		[learning rate: 0.00018072]
	Learning Rate: 0.000180717
	LOSS [training: 0.2907812988989701 | validation: 0.250257801199333]
	TIME [epoch: 8.55 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2858278684358896		[learning rate: 0.00018006]
	Learning Rate: 0.000180062
	LOSS [training: 0.2858278684358896 | validation: 0.24086623935524337]
	TIME [epoch: 8.55 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2776429549762963		[learning rate: 0.00017941]
	Learning Rate: 0.000179408
	LOSS [training: 0.2776429549762963 | validation: 0.25538812281279216]
	TIME [epoch: 8.58 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28832822223696486		[learning rate: 0.00017876]
	Learning Rate: 0.000178757
	LOSS [training: 0.28832822223696486 | validation: 0.2516185194042373]
	TIME [epoch: 8.58 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3079390081837067		[learning rate: 0.00017811]
	Learning Rate: 0.000178108
	LOSS [training: 0.3079390081837067 | validation: 0.24816923691689896]
	TIME [epoch: 8.55 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.298759041554844		[learning rate: 0.00017746]
	Learning Rate: 0.000177462
	LOSS [training: 0.298759041554844 | validation: 0.24532683103991043]
	TIME [epoch: 8.57 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.283972807117914		[learning rate: 0.00017682]
	Learning Rate: 0.000176818
	LOSS [training: 0.283972807117914 | validation: 0.24343731289363718]
	TIME [epoch: 8.56 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2826879843122353		[learning rate: 0.00017618]
	Learning Rate: 0.000176176
	LOSS [training: 0.2826879843122353 | validation: 0.24553266367493481]
	TIME [epoch: 8.57 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27721469550948463		[learning rate: 0.00017554]
	Learning Rate: 0.000175537
	LOSS [training: 0.27721469550948463 | validation: 0.2583209909805127]
	TIME [epoch: 8.55 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2737709397563081		[learning rate: 0.0001749]
	Learning Rate: 0.0001749
	LOSS [training: 0.2737709397563081 | validation: 0.2500082691106281]
	TIME [epoch: 8.57 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29771346052149295		[learning rate: 0.00017427]
	Learning Rate: 0.000174265
	LOSS [training: 0.29771346052149295 | validation: 0.2606078008168624]
	TIME [epoch: 8.56 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28339739995568747		[learning rate: 0.00017363]
	Learning Rate: 0.000173633
	LOSS [training: 0.28339739995568747 | validation: 0.2426501005827082]
	TIME [epoch: 8.57 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28558339319328035		[learning rate: 0.000173]
	Learning Rate: 0.000173003
	LOSS [training: 0.28558339319328035 | validation: 0.24108341403190664]
	TIME [epoch: 8.55 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28627879865736555		[learning rate: 0.00017237]
	Learning Rate: 0.000172375
	LOSS [training: 0.28627879865736555 | validation: 0.26202192538050906]
	TIME [epoch: 8.57 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29166039713167935		[learning rate: 0.00017175]
	Learning Rate: 0.000171749
	LOSS [training: 0.29166039713167935 | validation: 0.25110648393488877]
	TIME [epoch: 8.57 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2946220124726887		[learning rate: 0.00017113]
	Learning Rate: 0.000171126
	LOSS [training: 0.2946220124726887 | validation: 0.23978268529604108]
	TIME [epoch: 8.56 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2825878491017419		[learning rate: 0.0001705]
	Learning Rate: 0.000170505
	LOSS [training: 0.2825878491017419 | validation: 0.243544159478864]
	TIME [epoch: 8.55 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28617401078203375		[learning rate: 0.00016989]
	Learning Rate: 0.000169886
	LOSS [training: 0.28617401078203375 | validation: 0.26295730403757295]
	TIME [epoch: 8.57 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2863721803373192		[learning rate: 0.00016927]
	Learning Rate: 0.00016927
	LOSS [training: 0.2863721803373192 | validation: 0.24472811052721802]
	TIME [epoch: 8.57 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29188781032828864		[learning rate: 0.00016866]
	Learning Rate: 0.000168655
	LOSS [training: 0.29188781032828864 | validation: 0.25984716972289884]
	TIME [epoch: 8.55 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.281572491094901		[learning rate: 0.00016804]
	Learning Rate: 0.000168043
	LOSS [training: 0.281572491094901 | validation: 0.2600013133293829]
	TIME [epoch: 8.55 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2789142069570274		[learning rate: 0.00016743]
	Learning Rate: 0.000167433
	LOSS [training: 0.2789142069570274 | validation: 0.26187048627612325]
	TIME [epoch: 8.57 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27944844992232665		[learning rate: 0.00016683]
	Learning Rate: 0.000166826
	LOSS [training: 0.27944844992232665 | validation: 0.23911199320700038]
	TIME [epoch: 8.56 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2812630435934777		[learning rate: 0.00016622]
	Learning Rate: 0.00016622
	LOSS [training: 0.2812630435934777 | validation: 0.24388622462654824]
	TIME [epoch: 8.54 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28378542354188		[learning rate: 0.00016562]
	Learning Rate: 0.000165617
	LOSS [training: 0.28378542354188 | validation: 0.2385794240196073]
	TIME [epoch: 8.54 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28177795124128113		[learning rate: 0.00016502]
	Learning Rate: 0.000165016
	LOSS [training: 0.28177795124128113 | validation: 0.24973526077395575]
	TIME [epoch: 8.57 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2868811200569582		[learning rate: 0.00016442]
	Learning Rate: 0.000164417
	LOSS [training: 0.2868811200569582 | validation: 0.2370162079457238]
	TIME [epoch: 8.56 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27645270599972405		[learning rate: 0.00016382]
	Learning Rate: 0.000163821
	LOSS [training: 0.27645270599972405 | validation: 0.27122946434127027]
	TIME [epoch: 8.55 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2826802615187353		[learning rate: 0.00016323]
	Learning Rate: 0.000163226
	LOSS [training: 0.2826802615187353 | validation: 0.25984917030975463]
	TIME [epoch: 8.55 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2882039690609051		[learning rate: 0.00016263]
	Learning Rate: 0.000162634
	LOSS [training: 0.2882039690609051 | validation: 0.247226667628487]
	TIME [epoch: 8.58 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2760684458965544		[learning rate: 0.00016204]
	Learning Rate: 0.000162043
	LOSS [training: 0.2760684458965544 | validation: 0.2688940923132835]
	TIME [epoch: 8.56 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29032960737134716		[learning rate: 0.00016146]
	Learning Rate: 0.000161455
	LOSS [training: 0.29032960737134716 | validation: 0.24651851711167477]
	TIME [epoch: 8.55 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28262845527373204		[learning rate: 0.00016087]
	Learning Rate: 0.000160869
	LOSS [training: 0.28262845527373204 | validation: 0.2857780942054853]
	TIME [epoch: 8.55 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29564443595299095		[learning rate: 0.00016029]
	Learning Rate: 0.000160286
	LOSS [training: 0.29564443595299095 | validation: 0.24289094985099446]
	TIME [epoch: 8.59 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28152519516236413		[learning rate: 0.0001597]
	Learning Rate: 0.000159704
	LOSS [training: 0.28152519516236413 | validation: 0.22055213072619756]
	TIME [epoch: 8.55 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2778170271332706		[learning rate: 0.00015912]
	Learning Rate: 0.000159124
	LOSS [training: 0.2778170271332706 | validation: 0.2742607139943731]
	TIME [epoch: 8.55 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27604090172506146		[learning rate: 0.00015855]
	Learning Rate: 0.000158547
	LOSS [training: 0.27604090172506146 | validation: 0.2502392154441017]
	TIME [epoch: 8.57 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2816525564440303		[learning rate: 0.00015797]
	Learning Rate: 0.000157972
	LOSS [training: 0.2816525564440303 | validation: 0.2515127595982218]
	TIME [epoch: 8.58 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2707234452315216		[learning rate: 0.0001574]
	Learning Rate: 0.000157398
	LOSS [training: 0.2707234452315216 | validation: 0.2397786001064306]
	TIME [epoch: 8.54 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27777663331544855		[learning rate: 0.00015683]
	Learning Rate: 0.000156827
	LOSS [training: 0.27777663331544855 | validation: 0.24664858726787572]
	TIME [epoch: 8.54 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27489520366727965		[learning rate: 0.00015626]
	Learning Rate: 0.000156258
	LOSS [training: 0.27489520366727965 | validation: 0.25647835877168856]
	TIME [epoch: 8.57 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2758103470929155		[learning rate: 0.00015569]
	Learning Rate: 0.000155691
	LOSS [training: 0.2758103470929155 | validation: 0.24477291512369903]
	TIME [epoch: 8.55 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29908424239463993		[learning rate: 0.00015513]
	Learning Rate: 0.000155126
	LOSS [training: 0.29908424239463993 | validation: 0.2383312719416626]
	TIME [epoch: 8.53 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27599319110896836		[learning rate: 0.00015456]
	Learning Rate: 0.000154563
	LOSS [training: 0.27599319110896836 | validation: 0.23312480960632803]
	TIME [epoch: 8.54 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27083823025296716		[learning rate: 0.000154]
	Learning Rate: 0.000154002
	LOSS [training: 0.27083823025296716 | validation: 0.2650315266614677]
	TIME [epoch: 8.58 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2825875940985141		[learning rate: 0.00015344]
	Learning Rate: 0.000153443
	LOSS [training: 0.2825875940985141 | validation: 0.24926193364975305]
	TIME [epoch: 8.56 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2748029424423405		[learning rate: 0.00015289]
	Learning Rate: 0.000152886
	LOSS [training: 0.2748029424423405 | validation: 0.25283784718628466]
	TIME [epoch: 8.54 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28876012987767474		[learning rate: 0.00015233]
	Learning Rate: 0.000152331
	LOSS [training: 0.28876012987767474 | validation: 0.24696559215706593]
	TIME [epoch: 8.54 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2826286471058938		[learning rate: 0.00015178]
	Learning Rate: 0.000151779
	LOSS [training: 0.2826286471058938 | validation: 0.25663190147382015]
	TIME [epoch: 8.57 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28631461699063715		[learning rate: 0.00015123]
	Learning Rate: 0.000151228
	LOSS [training: 0.28631461699063715 | validation: 0.2791459008301729]
	TIME [epoch: 8.54 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2958016111656647		[learning rate: 0.00015068]
	Learning Rate: 0.000150679
	LOSS [training: 0.2958016111656647 | validation: 0.2386949395980379]
	TIME [epoch: 8.54 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2787383298038044		[learning rate: 0.00015013]
	Learning Rate: 0.000150132
	LOSS [training: 0.2787383298038044 | validation: 0.24942814925075232]
	TIME [epoch: 8.54 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27579325623910916		[learning rate: 0.00014959]
	Learning Rate: 0.000149587
	LOSS [training: 0.27579325623910916 | validation: 0.245009167524047]
	TIME [epoch: 8.59 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2814086378277212		[learning rate: 0.00014904]
	Learning Rate: 0.000149044
	LOSS [training: 0.2814086378277212 | validation: 0.2544505817698969]
	TIME [epoch: 8.54 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2738730761904156		[learning rate: 0.0001485]
	Learning Rate: 0.000148504
	LOSS [training: 0.2738730761904156 | validation: 0.24086603799502382]
	TIME [epoch: 8.54 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27531999617729114		[learning rate: 0.00014796]
	Learning Rate: 0.000147965
	LOSS [training: 0.27531999617729114 | validation: 0.2650166775376565]
	TIME [epoch: 8.55 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2682589422872429		[learning rate: 0.00014743]
	Learning Rate: 0.000147428
	LOSS [training: 0.2682589422872429 | validation: 0.24690733451297914]
	TIME [epoch: 8.57 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2738407991615037		[learning rate: 0.00014689]
	Learning Rate: 0.000146893
	LOSS [training: 0.2738407991615037 | validation: 0.24657984201740357]
	TIME [epoch: 8.54 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2731609266138095		[learning rate: 0.00014636]
	Learning Rate: 0.00014636
	LOSS [training: 0.2731609266138095 | validation: 0.2567378730277393]
	TIME [epoch: 8.55 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27429177658368925		[learning rate: 0.00014583]
	Learning Rate: 0.000145828
	LOSS [training: 0.27429177658368925 | validation: 0.23313385281858084]
	TIME [epoch: 8.55 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2765334201640955		[learning rate: 0.0001453]
	Learning Rate: 0.000145299
	LOSS [training: 0.2765334201640955 | validation: 0.23174593575077182]
	TIME [epoch: 8.57 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2751386235324066		[learning rate: 0.00014477]
	Learning Rate: 0.000144772
	LOSS [training: 0.2751386235324066 | validation: 0.2864094056535629]
	TIME [epoch: 8.52 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2804878870808912		[learning rate: 0.00014425]
	Learning Rate: 0.000144247
	LOSS [training: 0.2804878870808912 | validation: 0.2325472029330553]
	TIME [epoch: 8.51 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27117624699309173		[learning rate: 0.00014372]
	Learning Rate: 0.000143723
	LOSS [training: 0.27117624699309173 | validation: 0.22952951123525347]
	TIME [epoch: 8.55 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2710996441515349		[learning rate: 0.0001432]
	Learning Rate: 0.000143201
	LOSS [training: 0.2710996441515349 | validation: 0.24874005157568846]
	TIME [epoch: 8.57 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26941203192160645		[learning rate: 0.00014268]
	Learning Rate: 0.000142682
	LOSS [training: 0.26941203192160645 | validation: 0.23064044343363893]
	TIME [epoch: 8.55 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27809001138281586		[learning rate: 0.00014216]
	Learning Rate: 0.000142164
	LOSS [training: 0.27809001138281586 | validation: 0.24345078115570953]
	TIME [epoch: 8.54 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2742394401252652		[learning rate: 0.00014165]
	Learning Rate: 0.000141648
	LOSS [training: 0.2742394401252652 | validation: 0.2527293382756273]
	TIME [epoch: 8.57 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2825109502794685		[learning rate: 0.00014113]
	Learning Rate: 0.000141134
	LOSS [training: 0.2825109502794685 | validation: 0.2550720690722597]
	TIME [epoch: 8.54 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2790924895819623		[learning rate: 0.00014062]
	Learning Rate: 0.000140622
	LOSS [training: 0.2790924895819623 | validation: 0.2812540411884199]
	TIME [epoch: 8.54 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.3010320554119679		[learning rate: 0.00014011]
	Learning Rate: 0.000140112
	LOSS [training: 0.3010320554119679 | validation: 0.24982445156804706]
	TIME [epoch: 8.56 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27137112928863066		[learning rate: 0.0001396]
	Learning Rate: 0.000139603
	LOSS [training: 0.27137112928863066 | validation: 0.23198468662099164]
	TIME [epoch: 8.56 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27676016495841704		[learning rate: 0.0001391]
	Learning Rate: 0.000139096
	LOSS [training: 0.27676016495841704 | validation: 0.24660527982567937]
	TIME [epoch: 8.54 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2747107378395944		[learning rate: 0.00013859]
	Learning Rate: 0.000138592
	LOSS [training: 0.2747107378395944 | validation: 0.24672952023950573]
	TIME [epoch: 8.54 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2695020701787329		[learning rate: 0.00013809]
	Learning Rate: 0.000138089
	LOSS [training: 0.2695020701787329 | validation: 0.2624355469083762]
	TIME [epoch: 8.55 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27493457277596045		[learning rate: 0.00013759]
	Learning Rate: 0.000137587
	LOSS [training: 0.27493457277596045 | validation: 0.25588518938108284]
	TIME [epoch: 8.56 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2748394534452564		[learning rate: 0.00013709]
	Learning Rate: 0.000137088
	LOSS [training: 0.2748394534452564 | validation: 0.238615653741061]
	TIME [epoch: 8.53 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26222250544934156		[learning rate: 0.00013659]
	Learning Rate: 0.000136591
	LOSS [training: 0.26222250544934156 | validation: 0.2466330079646724]
	TIME [epoch: 8.53 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27345379210906173		[learning rate: 0.00013609]
	Learning Rate: 0.000136095
	LOSS [training: 0.27345379210906173 | validation: 0.24675627835239766]
	TIME [epoch: 8.54 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2799324114560897		[learning rate: 0.0001356]
	Learning Rate: 0.000135601
	LOSS [training: 0.2799324114560897 | validation: 0.22578015263635393]
	TIME [epoch: 8.55 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2703240183903202		[learning rate: 0.00013511]
	Learning Rate: 0.000135109
	LOSS [training: 0.2703240183903202 | validation: 0.245130993810898]
	TIME [epoch: 8.53 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27841338574876373		[learning rate: 0.00013462]
	Learning Rate: 0.000134619
	LOSS [training: 0.27841338574876373 | validation: 0.23632378632386292]
	TIME [epoch: 8.53 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2701426179029093		[learning rate: 0.00013413]
	Learning Rate: 0.00013413
	LOSS [training: 0.2701426179029093 | validation: 0.22522301853727722]
	TIME [epoch: 8.55 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.28576929967380826		[learning rate: 0.00013364]
	Learning Rate: 0.000133643
	LOSS [training: 0.28576929967380826 | validation: 0.22658098639410562]
	TIME [epoch: 8.56 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.268027050004695		[learning rate: 0.00013316]
	Learning Rate: 0.000133158
	LOSS [training: 0.268027050004695 | validation: 0.23101911449364887]
	TIME [epoch: 8.53 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27726786596238806		[learning rate: 0.00013268]
	Learning Rate: 0.000132675
	LOSS [training: 0.27726786596238806 | validation: 0.24981697363302213]
	TIME [epoch: 8.53 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27154548221536434		[learning rate: 0.00013219]
	Learning Rate: 0.000132194
	LOSS [training: 0.27154548221536434 | validation: 0.23332095084132262]
	TIME [epoch: 8.53 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2710190104564381		[learning rate: 0.00013171]
	Learning Rate: 0.000131714
	LOSS [training: 0.2710190104564381 | validation: 0.2668380839242332]
	TIME [epoch: 8.55 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26825635098892636		[learning rate: 0.00013124]
	Learning Rate: 0.000131236
	LOSS [training: 0.26825635098892636 | validation: 0.2405860129507974]
	TIME [epoch: 8.52 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26992197167917		[learning rate: 0.00013076]
	Learning Rate: 0.00013076
	LOSS [training: 0.26992197167917 | validation: 0.2605078748793618]
	TIME [epoch: 8.54 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2753166419693521		[learning rate: 0.00013029]
	Learning Rate: 0.000130285
	LOSS [training: 0.2753166419693521 | validation: 0.24331718243235326]
	TIME [epoch: 8.53 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26478461551578875		[learning rate: 0.00012981]
	Learning Rate: 0.000129812
	LOSS [training: 0.26478461551578875 | validation: 0.2382430481160483]
	TIME [epoch: 8.54 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2673049515244874		[learning rate: 0.00012934]
	Learning Rate: 0.000129341
	LOSS [training: 0.2673049515244874 | validation: 0.24473963194119738]
	TIME [epoch: 8.52 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2637510346971069		[learning rate: 0.00012887]
	Learning Rate: 0.000128872
	LOSS [training: 0.2637510346971069 | validation: 0.24646366346567247]
	TIME [epoch: 8.54 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2717897260425918		[learning rate: 0.0001284]
	Learning Rate: 0.000128404
	LOSS [training: 0.2717897260425918 | validation: 0.26857506434841316]
	TIME [epoch: 8.54 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2740886515950309		[learning rate: 0.00012794]
	Learning Rate: 0.000127938
	LOSS [training: 0.2740886515950309 | validation: 0.2468014258489185]
	TIME [epoch: 8.53 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2721086161909986		[learning rate: 0.00012747]
	Learning Rate: 0.000127474
	LOSS [training: 0.2721086161909986 | validation: 0.2475710079983849]
	TIME [epoch: 8.53 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27159742494240585		[learning rate: 0.00012701]
	Learning Rate: 0.000127011
	LOSS [training: 0.27159742494240585 | validation: 0.24560632044255987]
	TIME [epoch: 8.53 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26857069999189		[learning rate: 0.00012655]
	Learning Rate: 0.00012655
	LOSS [training: 0.26857069999189 | validation: 0.23810276186708795]
	TIME [epoch: 8.55 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26478623084429875		[learning rate: 0.00012609]
	Learning Rate: 0.000126091
	LOSS [training: 0.26478623084429875 | validation: 0.23575251405021114]
	TIME [epoch: 8.53 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2636296972186006		[learning rate: 0.00012563]
	Learning Rate: 0.000125633
	LOSS [training: 0.2636296972186006 | validation: 0.24234138578823916]
	TIME [epoch: 8.54 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26639413249058597		[learning rate: 0.00012518]
	Learning Rate: 0.000125178
	LOSS [training: 0.26639413249058597 | validation: 0.23581926119206775]
	TIME [epoch: 8.53 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2697992158757583		[learning rate: 0.00012472]
	Learning Rate: 0.000124723
	LOSS [training: 0.2697992158757583 | validation: 0.2372288920875199]
	TIME [epoch: 8.54 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2663363460683166		[learning rate: 0.00012427]
	Learning Rate: 0.000124271
	LOSS [training: 0.2663363460683166 | validation: 0.25495303052074414]
	TIME [epoch: 8.52 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27538268574555064		[learning rate: 0.00012382]
	Learning Rate: 0.00012382
	LOSS [training: 0.27538268574555064 | validation: 0.2442266533486743]
	TIME [epoch: 8.54 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2688554376628859		[learning rate: 0.00012337]
	Learning Rate: 0.00012337
	LOSS [training: 0.2688554376628859 | validation: 0.2512653256089735]
	TIME [epoch: 8.53 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2706359643188505		[learning rate: 0.00012292]
	Learning Rate: 0.000122923
	LOSS [training: 0.2706359643188505 | validation: 0.2453739013083704]
	TIME [epoch: 8.54 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2670842982718202		[learning rate: 0.00012248]
	Learning Rate: 0.000122476
	LOSS [training: 0.2670842982718202 | validation: 0.27569292354657876]
	TIME [epoch: 8.52 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2676846852464596		[learning rate: 0.00012203]
	Learning Rate: 0.000122032
	LOSS [training: 0.2676846852464596 | validation: 0.25664254435360384]
	TIME [epoch: 8.55 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2655695261971772		[learning rate: 0.00012159]
	Learning Rate: 0.000121589
	LOSS [training: 0.2655695261971772 | validation: 0.23111400428593482]
	TIME [epoch: 8.52 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26639786821846195		[learning rate: 0.00012115]
	Learning Rate: 0.000121148
	LOSS [training: 0.26639786821846195 | validation: 0.23705295339310475]
	TIME [epoch: 8.54 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2646330323431486		[learning rate: 0.00012071]
	Learning Rate: 0.000120708
	LOSS [training: 0.2646330323431486 | validation: 0.24927170129000403]
	TIME [epoch: 8.53 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2593931456122699		[learning rate: 0.00012027]
	Learning Rate: 0.00012027
	LOSS [training: 0.2593931456122699 | validation: 0.2541463859472614]
	TIME [epoch: 8.54 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26938292713262385		[learning rate: 0.00011983]
	Learning Rate: 0.000119834
	LOSS [training: 0.26938292713262385 | validation: 0.24545454974877823]
	TIME [epoch: 8.52 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2662972221624792		[learning rate: 0.0001194]
	Learning Rate: 0.000119399
	LOSS [training: 0.2662972221624792 | validation: 0.23879444371423542]
	TIME [epoch: 8.54 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2663860383157338		[learning rate: 0.00011897]
	Learning Rate: 0.000118966
	LOSS [training: 0.2663860383157338 | validation: 0.2397876536037192]
	TIME [epoch: 8.54 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2697652597159116		[learning rate: 0.00011853]
	Learning Rate: 0.000118534
	LOSS [training: 0.2697652597159116 | validation: 0.2240226707086328]
	TIME [epoch: 8.53 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26705716625917175		[learning rate: 0.0001181]
	Learning Rate: 0.000118104
	LOSS [training: 0.26705716625917175 | validation: 0.25215996384575423]
	TIME [epoch: 8.52 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2699836846227722		[learning rate: 0.00011767]
	Learning Rate: 0.000117675
	LOSS [training: 0.2699836846227722 | validation: 0.24097276924402747]
	TIME [epoch: 8.54 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2569996498705867		[learning rate: 0.00011725]
	Learning Rate: 0.000117248
	LOSS [training: 0.2569996498705867 | validation: 0.24831367302002647]
	TIME [epoch: 8.54 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2637448953978726		[learning rate: 0.00011682]
	Learning Rate: 0.000116822
	LOSS [training: 0.2637448953978726 | validation: 0.22888790514339208]
	TIME [epoch: 8.53 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26468449747385653		[learning rate: 0.0001164]
	Learning Rate: 0.000116398
	LOSS [training: 0.26468449747385653 | validation: 0.2429288356869701]
	TIME [epoch: 8.52 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2673912209317551		[learning rate: 0.00011598]
	Learning Rate: 0.000115976
	LOSS [training: 0.2673912209317551 | validation: 0.24237874659079978]
	TIME [epoch: 8.54 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26704142790779134		[learning rate: 0.00011556]
	Learning Rate: 0.000115555
	LOSS [training: 0.26704142790779134 | validation: 0.2673949379894338]
	TIME [epoch: 8.54 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2684087814638073		[learning rate: 0.00011514]
	Learning Rate: 0.000115136
	LOSS [training: 0.2684087814638073 | validation: 0.24088635527877472]
	TIME [epoch: 8.52 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2739187010248997		[learning rate: 0.00011472]
	Learning Rate: 0.000114718
	LOSS [training: 0.2739187010248997 | validation: 0.23359264079915856]
	TIME [epoch: 8.53 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26198055487313393		[learning rate: 0.0001143]
	Learning Rate: 0.000114302
	LOSS [training: 0.26198055487313393 | validation: 0.2268447991435396]
	TIME [epoch: 8.54 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2698279036889263		[learning rate: 0.00011389]
	Learning Rate: 0.000113887
	LOSS [training: 0.2698279036889263 | validation: 0.23768620624399134]
	TIME [epoch: 8.53 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27422082865777536		[learning rate: 0.00011347]
	Learning Rate: 0.000113474
	LOSS [training: 0.27422082865777536 | validation: 0.23903021191836094]
	TIME [epoch: 8.53 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2704898013721289		[learning rate: 0.00011306]
	Learning Rate: 0.000113062
	LOSS [training: 0.2704898013721289 | validation: 0.23399457837494658]
	TIME [epoch: 8.55 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2724835324559758		[learning rate: 0.00011265]
	Learning Rate: 0.000112651
	LOSS [training: 0.2724835324559758 | validation: 0.22834268425370174]
	TIME [epoch: 8.54 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2684523721303479		[learning rate: 0.00011224]
	Learning Rate: 0.000112243
	LOSS [training: 0.2684523721303479 | validation: 0.2197767957415549]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1335.pth
	Model improved!!!
EPOCH 1336/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2584589850380533		[learning rate: 0.00011184]
	Learning Rate: 0.000111835
	LOSS [training: 0.2584589850380533 | validation: 0.22867709833904962]
	TIME [epoch: 8.53 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26439452189000473		[learning rate: 0.00011143]
	Learning Rate: 0.000111429
	LOSS [training: 0.26439452189000473 | validation: 0.22840823494029097]
	TIME [epoch: 8.55 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26234854114069256		[learning rate: 0.00011103]
	Learning Rate: 0.000111025
	LOSS [training: 0.26234854114069256 | validation: 0.2507303481057947]
	TIME [epoch: 8.54 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2671455623798633		[learning rate: 0.00011062]
	Learning Rate: 0.000110622
	LOSS [training: 0.2671455623798633 | validation: 0.23427458915196098]
	TIME [epoch: 8.52 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26145223472775686		[learning rate: 0.00011022]
	Learning Rate: 0.000110221
	LOSS [training: 0.26145223472775686 | validation: 0.2512721125533607]
	TIME [epoch: 8.53 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25943948672799444		[learning rate: 0.00010982]
	Learning Rate: 0.000109821
	LOSS [training: 0.25943948672799444 | validation: 0.23413198019999582]
	TIME [epoch: 8.55 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2670924355344808		[learning rate: 0.00010942]
	Learning Rate: 0.000109422
	LOSS [training: 0.2670924355344808 | validation: 0.22627884777209448]
	TIME [epoch: 8.54 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2727636587632774		[learning rate: 0.00010903]
	Learning Rate: 0.000109025
	LOSS [training: 0.2727636587632774 | validation: 0.22690680347829278]
	TIME [epoch: 8.52 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2633216167432375		[learning rate: 0.00010863]
	Learning Rate: 0.000108629
	LOSS [training: 0.2633216167432375 | validation: 0.22445956204183196]
	TIME [epoch: 8.53 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2640953158166683		[learning rate: 0.00010824]
	Learning Rate: 0.000108235
	LOSS [training: 0.2640953158166683 | validation: 0.2237786893285774]
	TIME [epoch: 8.56 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2678417180682689		[learning rate: 0.00010784]
	Learning Rate: 0.000107842
	LOSS [training: 0.2678417180682689 | validation: 0.2223833680140836]
	TIME [epoch: 8.54 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26083013657717274		[learning rate: 0.00010745]
	Learning Rate: 0.000107451
	LOSS [training: 0.26083013657717274 | validation: 0.23413496056436972]
	TIME [epoch: 8.53 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2795703480749636		[learning rate: 0.00010706]
	Learning Rate: 0.000107061
	LOSS [training: 0.2795703480749636 | validation: 0.26935460987007187]
	TIME [epoch: 8.52 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27025333045515837		[learning rate: 0.00010667]
	Learning Rate: 0.000106673
	LOSS [training: 0.27025333045515837 | validation: 0.23175988560312585]
	TIME [epoch: 8.56 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26319331916120675		[learning rate: 0.00010629]
	Learning Rate: 0.000106285
	LOSS [training: 0.26319331916120675 | validation: 0.24742817734772923]
	TIME [epoch: 8.53 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2650875509031766		[learning rate: 0.0001059]
	Learning Rate: 0.0001059
	LOSS [training: 0.2650875509031766 | validation: 0.24033895139790026]
	TIME [epoch: 8.52 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2666747870104242		[learning rate: 0.00010552]
	Learning Rate: 0.000105515
	LOSS [training: 0.2666747870104242 | validation: 0.2571595029164126]
	TIME [epoch: 8.52 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2884217060332047		[learning rate: 0.00010513]
	Learning Rate: 0.000105132
	LOSS [training: 0.2884217060332047 | validation: 0.22208579281967625]
	TIME [epoch: 8.56 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27017698944746105		[learning rate: 0.00010475]
	Learning Rate: 0.000104751
	LOSS [training: 0.27017698944746105 | validation: 0.23680847398563493]
	TIME [epoch: 8.52 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.266687400803616		[learning rate: 0.00010437]
	Learning Rate: 0.000104371
	LOSS [training: 0.266687400803616 | validation: 0.25058638214926665]
	TIME [epoch: 8.52 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26481217948154373		[learning rate: 0.00010399]
	Learning Rate: 0.000103992
	LOSS [training: 0.26481217948154373 | validation: 0.2306427832798416]
	TIME [epoch: 8.54 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26157318430686816		[learning rate: 0.00010361]
	Learning Rate: 0.000103615
	LOSS [training: 0.26157318430686816 | validation: 0.25525003288043613]
	TIME [epoch: 8.55 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2656680400133758		[learning rate: 0.00010324]
	Learning Rate: 0.000103239
	LOSS [training: 0.2656680400133758 | validation: 0.23164931170715913]
	TIME [epoch: 8.52 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26497994932765384		[learning rate: 0.00010286]
	Learning Rate: 0.000102864
	LOSS [training: 0.26497994932765384 | validation: 0.24936934202483937]
	TIME [epoch: 8.52 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25770648396281803		[learning rate: 0.00010249]
	Learning Rate: 0.000102491
	LOSS [training: 0.25770648396281803 | validation: 0.22017388912255273]
	TIME [epoch: 8.55 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25665798096496845		[learning rate: 0.00010212]
	Learning Rate: 0.000102119
	LOSS [training: 0.25665798096496845 | validation: 0.23294230836285046]
	TIME [epoch: 8.54 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26204272550800495		[learning rate: 0.00010175]
	Learning Rate: 0.000101748
	LOSS [training: 0.26204272550800495 | validation: 0.23829271007357739]
	TIME [epoch: 8.52 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26683015198066656		[learning rate: 0.00010138]
	Learning Rate: 0.000101379
	LOSS [training: 0.26683015198066656 | validation: 0.24692356144097782]
	TIME [epoch: 8.52 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26794598937066083		[learning rate: 0.00010101]
	Learning Rate: 0.000101011
	LOSS [training: 0.26794598937066083 | validation: 0.23821814965996874]
	TIME [epoch: 8.56 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26290194299377523		[learning rate: 0.00010064]
	Learning Rate: 0.000100644
	LOSS [training: 0.26290194299377523 | validation: 0.24207207469588093]
	TIME [epoch: 8.53 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2654725378114776		[learning rate: 0.00010028]
	Learning Rate: 0.000100279
	LOSS [training: 0.2654725378114776 | validation: 0.24540553761202588]
	TIME [epoch: 8.52 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2759773476945607		[learning rate: 9.9915e-05]
	Learning Rate: 9.99152e-05
	LOSS [training: 0.2759773476945607 | validation: 0.24010678272077374]
	TIME [epoch: 8.53 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2623987219200926		[learning rate: 9.9553e-05]
	Learning Rate: 9.95526e-05
	LOSS [training: 0.2623987219200926 | validation: 0.24146650095115219]
	TIME [epoch: 8.57 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2562535560759615		[learning rate: 9.9191e-05]
	Learning Rate: 9.91913e-05
	LOSS [training: 0.2562535560759615 | validation: 0.24207899466504196]
	TIME [epoch: 8.53 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2660075673485809		[learning rate: 9.8831e-05]
	Learning Rate: 9.88314e-05
	LOSS [training: 0.2660075673485809 | validation: 0.23636581250309818]
	TIME [epoch: 8.52 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2649433039772502		[learning rate: 9.8473e-05]
	Learning Rate: 9.84727e-05
	LOSS [training: 0.2649433039772502 | validation: 0.23061334248086696]
	TIME [epoch: 8.53 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2670160051407936		[learning rate: 9.8115e-05]
	Learning Rate: 9.81153e-05
	LOSS [training: 0.2670160051407936 | validation: 0.24900306444037074]
	TIME [epoch: 8.55 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2609969059074933		[learning rate: 9.7759e-05]
	Learning Rate: 9.77592e-05
	LOSS [training: 0.2609969059074933 | validation: 0.2414882888540453]
	TIME [epoch: 8.53 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2614418024967734		[learning rate: 9.7404e-05]
	Learning Rate: 9.74045e-05
	LOSS [training: 0.2614418024967734 | validation: 0.2553808374220108]
	TIME [epoch: 8.52 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25865291581095584		[learning rate: 9.7051e-05]
	Learning Rate: 9.7051e-05
	LOSS [training: 0.25865291581095584 | validation: 0.23773816949585458]
	TIME [epoch: 8.54 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25530814610177666		[learning rate: 9.6699e-05]
	Learning Rate: 9.66988e-05
	LOSS [training: 0.25530814610177666 | validation: 0.23398704600195105]
	TIME [epoch: 8.55 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2632309578552149		[learning rate: 9.6348e-05]
	Learning Rate: 9.63479e-05
	LOSS [training: 0.2632309578552149 | validation: 0.23142801114424166]
	TIME [epoch: 8.53 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26197271197250716		[learning rate: 9.5998e-05]
	Learning Rate: 9.59982e-05
	LOSS [training: 0.26197271197250716 | validation: 0.23341488353829812]
	TIME [epoch: 8.52 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26411924062794323		[learning rate: 9.565e-05]
	Learning Rate: 9.56499e-05
	LOSS [training: 0.26411924062794323 | validation: 0.24395347996848454]
	TIME [epoch: 8.54 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.29104767354923994		[learning rate: 9.5303e-05]
	Learning Rate: 9.53027e-05
	LOSS [training: 0.29104767354923994 | validation: 0.27536278964322247]
	TIME [epoch: 8.55 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2847889831765468		[learning rate: 9.4957e-05]
	Learning Rate: 9.49569e-05
	LOSS [training: 0.2847889831765468 | validation: 0.25582415264902897]
	TIME [epoch: 8.53 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26813404476658115		[learning rate: 9.4612e-05]
	Learning Rate: 9.46122e-05
	LOSS [training: 0.26813404476658115 | validation: 0.225623337438077]
	TIME [epoch: 8.52 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26551595016613366		[learning rate: 9.4269e-05]
	Learning Rate: 9.42689e-05
	LOSS [training: 0.26551595016613366 | validation: 0.22722057476014193]
	TIME [epoch: 8.54 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26712930469390944		[learning rate: 9.3927e-05]
	Learning Rate: 9.39268e-05
	LOSS [training: 0.26712930469390944 | validation: 0.2467566978622957]
	TIME [epoch: 8.55 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2611875344198769		[learning rate: 9.3586e-05]
	Learning Rate: 9.35859e-05
	LOSS [training: 0.2611875344198769 | validation: 0.241561521322153]
	TIME [epoch: 8.52 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2699961462851189		[learning rate: 9.3246e-05]
	Learning Rate: 9.32463e-05
	LOSS [training: 0.2699961462851189 | validation: 0.254813781291923]
	TIME [epoch: 8.54 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26155441991522055		[learning rate: 9.2908e-05]
	Learning Rate: 9.29079e-05
	LOSS [training: 0.26155441991522055 | validation: 0.23386340838028413]
	TIME [epoch: 8.54 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2600252293578236		[learning rate: 9.2571e-05]
	Learning Rate: 9.25707e-05
	LOSS [training: 0.2600252293578236 | validation: 0.22795810028707425]
	TIME [epoch: 8.53 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2558809021682747		[learning rate: 9.2235e-05]
	Learning Rate: 9.22348e-05
	LOSS [training: 0.2558809021682747 | validation: 0.24545183747024568]
	TIME [epoch: 8.52 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2569007929062766		[learning rate: 9.19e-05]
	Learning Rate: 9.19001e-05
	LOSS [training: 0.2569007929062766 | validation: 0.22902746041581767]
	TIME [epoch: 8.54 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25798090023633236		[learning rate: 9.1567e-05]
	Learning Rate: 9.15665e-05
	LOSS [training: 0.25798090023633236 | validation: 0.2476511882842839]
	TIME [epoch: 8.55 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2556617496979995		[learning rate: 9.1234e-05]
	Learning Rate: 9.12343e-05
	LOSS [training: 0.2556617496979995 | validation: 0.24233796239838734]
	TIME [epoch: 8.53 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25773533885691874		[learning rate: 9.0903e-05]
	Learning Rate: 9.09031e-05
	LOSS [training: 0.25773533885691874 | validation: 0.2430844895291973]
	TIME [epoch: 8.52 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2611478882252924		[learning rate: 9.0573e-05]
	Learning Rate: 9.05733e-05
	LOSS [training: 0.2611478882252924 | validation: 0.23426046625352606]
	TIME [epoch: 8.54 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2649069282893839		[learning rate: 9.0245e-05]
	Learning Rate: 9.02446e-05
	LOSS [training: 0.2649069282893839 | validation: 0.23731322672718416]
	TIME [epoch: 8.54 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2530770670838619		[learning rate: 8.9917e-05]
	Learning Rate: 8.99171e-05
	LOSS [training: 0.2530770670838619 | validation: 0.23590201570748937]
	TIME [epoch: 8.53 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26245848012920087		[learning rate: 8.9591e-05]
	Learning Rate: 8.95908e-05
	LOSS [training: 0.26245848012920087 | validation: 0.23042791159049708]
	TIME [epoch: 8.53 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2642434249504512		[learning rate: 8.9266e-05]
	Learning Rate: 8.92656e-05
	LOSS [training: 0.2642434249504512 | validation: 0.21847023221311745]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1398.pth
	Model improved!!!
EPOCH 1399/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26552944453324506		[learning rate: 8.8942e-05]
	Learning Rate: 8.89417e-05
	LOSS [training: 0.26552944453324506 | validation: 0.2317421676943351]
	TIME [epoch: 8.55 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25673856277394025		[learning rate: 8.8619e-05]
	Learning Rate: 8.86189e-05
	LOSS [training: 0.25673856277394025 | validation: 0.23376245834151238]
	TIME [epoch: 8.53 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2663568112101214		[learning rate: 8.8297e-05]
	Learning Rate: 8.82973e-05
	LOSS [training: 0.2663568112101214 | validation: 0.2287548425086879]
	TIME [epoch: 8.53 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.263539511752897		[learning rate: 8.7977e-05]
	Learning Rate: 8.79769e-05
	LOSS [training: 0.263539511752897 | validation: 0.23376016161491459]
	TIME [epoch: 8.53 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25533679913695806		[learning rate: 8.7658e-05]
	Learning Rate: 8.76576e-05
	LOSS [training: 0.25533679913695806 | validation: 0.24630435752209257]
	TIME [epoch: 8.54 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2597477432973909		[learning rate: 8.7339e-05]
	Learning Rate: 8.73395e-05
	LOSS [training: 0.2597477432973909 | validation: 0.23756885972534597]
	TIME [epoch: 8.53 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2543470650194723		[learning rate: 8.7023e-05]
	Learning Rate: 8.70225e-05
	LOSS [training: 0.2543470650194723 | validation: 0.23207740159058166]
	TIME [epoch: 8.54 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2528000683639998		[learning rate: 8.6707e-05]
	Learning Rate: 8.67067e-05
	LOSS [training: 0.2528000683639998 | validation: 0.21760855480017055]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1406.pth
	Model improved!!!
EPOCH 1407/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25551272823128884		[learning rate: 8.6392e-05]
	Learning Rate: 8.63921e-05
	LOSS [training: 0.25551272823128884 | validation: 0.23189833699331935]
	TIME [epoch: 8.55 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.256686978498318		[learning rate: 8.6079e-05]
	Learning Rate: 8.60785e-05
	LOSS [training: 0.256686978498318 | validation: 0.21983958819835778]
	TIME [epoch: 8.53 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26000459065426007		[learning rate: 8.5766e-05]
	Learning Rate: 8.57661e-05
	LOSS [training: 0.26000459065426007 | validation: 0.23401752104350843]
	TIME [epoch: 8.54 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.259796825597802		[learning rate: 8.5455e-05]
	Learning Rate: 8.54549e-05
	LOSS [training: 0.259796825597802 | validation: 0.2293077781941505]
	TIME [epoch: 8.53 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2537565258086348		[learning rate: 8.5145e-05]
	Learning Rate: 8.51448e-05
	LOSS [training: 0.2537565258086348 | validation: 0.23675339616803265]
	TIME [epoch: 8.55 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26103696515778696		[learning rate: 8.4836e-05]
	Learning Rate: 8.48358e-05
	LOSS [training: 0.26103696515778696 | validation: 0.2260688725431959]
	TIME [epoch: 8.54 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25884102364702116		[learning rate: 8.4528e-05]
	Learning Rate: 8.45279e-05
	LOSS [training: 0.25884102364702116 | validation: 0.2409376386551228]
	TIME [epoch: 8.54 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2636126543409066		[learning rate: 8.4221e-05]
	Learning Rate: 8.42211e-05
	LOSS [training: 0.2636126543409066 | validation: 0.2540632102288058]
	TIME [epoch: 8.54 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25876298385652774		[learning rate: 8.3916e-05]
	Learning Rate: 8.39155e-05
	LOSS [training: 0.25876298385652774 | validation: 0.2395858522421897]
	TIME [epoch: 8.54 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26492974019625487		[learning rate: 8.3611e-05]
	Learning Rate: 8.3611e-05
	LOSS [training: 0.26492974019625487 | validation: 0.23923554182682533]
	TIME [epoch: 8.54 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25346553466255756		[learning rate: 8.3308e-05]
	Learning Rate: 8.33075e-05
	LOSS [training: 0.25346553466255756 | validation: 0.23388656528525203]
	TIME [epoch: 8.54 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2517260616022416		[learning rate: 8.3005e-05]
	Learning Rate: 8.30052e-05
	LOSS [training: 0.2517260616022416 | validation: 0.22813122578989342]
	TIME [epoch: 8.55 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.27217146836526074		[learning rate: 8.2704e-05]
	Learning Rate: 8.2704e-05
	LOSS [training: 0.27217146836526074 | validation: 0.2354902082687538]
	TIME [epoch: 8.53 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26630811183911673		[learning rate: 8.2404e-05]
	Learning Rate: 8.24038e-05
	LOSS [training: 0.26630811183911673 | validation: 0.2360634600724483]
	TIME [epoch: 8.55 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26040030409854664		[learning rate: 8.2105e-05]
	Learning Rate: 8.21048e-05
	LOSS [training: 0.26040030409854664 | validation: 0.23957408682837694]
	TIME [epoch: 8.53 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2523365623114067		[learning rate: 8.1807e-05]
	Learning Rate: 8.18068e-05
	LOSS [training: 0.2523365623114067 | validation: 0.23793490989822688]
	TIME [epoch: 8.55 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2597771948989582		[learning rate: 8.151e-05]
	Learning Rate: 8.15099e-05
	LOSS [training: 0.2597771948989582 | validation: 0.2270288623876539]
	TIME [epoch: 8.53 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2613280349391069		[learning rate: 8.1214e-05]
	Learning Rate: 8.12142e-05
	LOSS [training: 0.2613280349391069 | validation: 0.22712752907623524]
	TIME [epoch: 8.55 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25604141849431084		[learning rate: 8.0919e-05]
	Learning Rate: 8.09194e-05
	LOSS [training: 0.25604141849431084 | validation: 0.23477561490094573]
	TIME [epoch: 8.53 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25786647691009984		[learning rate: 8.0626e-05]
	Learning Rate: 8.06257e-05
	LOSS [training: 0.25786647691009984 | validation: 0.25464379100797474]
	TIME [epoch: 8.55 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25584227478478294		[learning rate: 8.0333e-05]
	Learning Rate: 8.03331e-05
	LOSS [training: 0.25584227478478294 | validation: 0.244609054825801]
	TIME [epoch: 8.54 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2564372437157407		[learning rate: 8.0042e-05]
	Learning Rate: 8.00416e-05
	LOSS [training: 0.2564372437157407 | validation: 0.23069392641155606]
	TIME [epoch: 8.54 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25868625190346733		[learning rate: 7.9751e-05]
	Learning Rate: 7.97511e-05
	LOSS [training: 0.25868625190346733 | validation: 0.24088114372035777]
	TIME [epoch: 8.53 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25788166809181157		[learning rate: 7.9462e-05]
	Learning Rate: 7.94617e-05
	LOSS [training: 0.25788166809181157 | validation: 0.2330980234772906]
	TIME [epoch: 8.55 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25820108588826524		[learning rate: 7.9173e-05]
	Learning Rate: 7.91733e-05
	LOSS [training: 0.25820108588826524 | validation: 0.23862089089644573]
	TIME [epoch: 8.55 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25725944868134004		[learning rate: 7.8886e-05]
	Learning Rate: 7.8886e-05
	LOSS [training: 0.25725944868134004 | validation: 0.244486457130007]
	TIME [epoch: 8.53 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26624014923937595		[learning rate: 7.86e-05]
	Learning Rate: 7.85997e-05
	LOSS [training: 0.26624014923937595 | validation: 0.25045748608477975]
	TIME [epoch: 8.53 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2625830951373393		[learning rate: 7.8315e-05]
	Learning Rate: 7.83145e-05
	LOSS [training: 0.2625830951373393 | validation: 0.2321968996383318]
	TIME [epoch: 8.55 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2596274337697831		[learning rate: 7.803e-05]
	Learning Rate: 7.80303e-05
	LOSS [training: 0.2596274337697831 | validation: 0.23466779361754586]
	TIME [epoch: 8.54 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2596452848295765		[learning rate: 7.7747e-05]
	Learning Rate: 7.77471e-05
	LOSS [training: 0.2596452848295765 | validation: 0.24251264000581668]
	TIME [epoch: 8.53 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2651832905927975		[learning rate: 7.7465e-05]
	Learning Rate: 7.7465e-05
	LOSS [training: 0.2651832905927975 | validation: 0.23939252785686777]
	TIME [epoch: 8.53 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26009179253756975		[learning rate: 7.7184e-05]
	Learning Rate: 7.71838e-05
	LOSS [training: 0.26009179253756975 | validation: 0.23003947065991284]
	TIME [epoch: 8.55 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25397764394237404		[learning rate: 7.6904e-05]
	Learning Rate: 7.69037e-05
	LOSS [training: 0.25397764394237404 | validation: 0.24919810980830584]
	TIME [epoch: 8.54 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25711251642530997		[learning rate: 7.6625e-05]
	Learning Rate: 7.66246e-05
	LOSS [training: 0.25711251642530997 | validation: 0.24160079147036]
	TIME [epoch: 8.53 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25013274756697224		[learning rate: 7.6347e-05]
	Learning Rate: 7.63466e-05
	LOSS [training: 0.25013274756697224 | validation: 0.24404471823835633]
	TIME [epoch: 8.53 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2578943711238003		[learning rate: 7.607e-05]
	Learning Rate: 7.60695e-05
	LOSS [training: 0.2578943711238003 | validation: 0.24623673795872775]
	TIME [epoch: 8.55 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2567264929024107		[learning rate: 7.5793e-05]
	Learning Rate: 7.57935e-05
	LOSS [training: 0.2567264929024107 | validation: 0.2304404439499852]
	TIME [epoch: 8.54 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2518589855686496		[learning rate: 7.5518e-05]
	Learning Rate: 7.55184e-05
	LOSS [training: 0.2518589855686496 | validation: 0.24402496358407522]
	TIME [epoch: 8.53 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25506664870920215		[learning rate: 7.5244e-05]
	Learning Rate: 7.52443e-05
	LOSS [training: 0.25506664870920215 | validation: 0.23020050607600606]
	TIME [epoch: 8.54 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2520736439612807		[learning rate: 7.4971e-05]
	Learning Rate: 7.49712e-05
	LOSS [training: 0.2520736439612807 | validation: 0.24269878017670599]
	TIME [epoch: 8.56 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25930885034357887		[learning rate: 7.4699e-05]
	Learning Rate: 7.46992e-05
	LOSS [training: 0.25930885034357887 | validation: 0.23923549893018997]
	TIME [epoch: 8.53 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2585103494163287		[learning rate: 7.4428e-05]
	Learning Rate: 7.44281e-05
	LOSS [training: 0.2585103494163287 | validation: 0.23911772128378067]
	TIME [epoch: 8.53 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2668930201570151		[learning rate: 7.4158e-05]
	Learning Rate: 7.4158e-05
	LOSS [training: 0.2668930201570151 | validation: 0.23697669421236978]
	TIME [epoch: 8.55 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25454520395452407		[learning rate: 7.3889e-05]
	Learning Rate: 7.38889e-05
	LOSS [training: 0.25454520395452407 | validation: 0.22742739300964004]
	TIME [epoch: 8.55 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26295415980313347		[learning rate: 7.3621e-05]
	Learning Rate: 7.36207e-05
	LOSS [training: 0.26295415980313347 | validation: 0.22930183726029896]
	TIME [epoch: 8.52 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2678660014069277		[learning rate: 7.3354e-05]
	Learning Rate: 7.33536e-05
	LOSS [training: 0.2678660014069277 | validation: 0.22524261885246472]
	TIME [epoch: 8.53 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26349160567123436		[learning rate: 7.3087e-05]
	Learning Rate: 7.30873e-05
	LOSS [training: 0.26349160567123436 | validation: 0.24801101758884797]
	TIME [epoch: 8.56 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2640970640528959		[learning rate: 7.2822e-05]
	Learning Rate: 7.28221e-05
	LOSS [training: 0.2640970640528959 | validation: 0.25381913343449214]
	TIME [epoch: 8.53 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2579068549938871		[learning rate: 7.2558e-05]
	Learning Rate: 7.25578e-05
	LOSS [training: 0.2579068549938871 | validation: 0.23516474082139593]
	TIME [epoch: 8.53 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2571449542119981		[learning rate: 7.2295e-05]
	Learning Rate: 7.22945e-05
	LOSS [training: 0.2571449542119981 | validation: 0.2454108579976635]
	TIME [epoch: 8.53 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2589184416212226		[learning rate: 7.2032e-05]
	Learning Rate: 7.20321e-05
	LOSS [training: 0.2589184416212226 | validation: 0.25043737212554196]
	TIME [epoch: 8.57 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2574207939335543		[learning rate: 7.1771e-05]
	Learning Rate: 7.17707e-05
	LOSS [training: 0.2574207939335543 | validation: 0.24813536866073294]
	TIME [epoch: 8.53 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2572536468415649		[learning rate: 7.151e-05]
	Learning Rate: 7.15103e-05
	LOSS [training: 0.2572536468415649 | validation: 0.24831249503706068]
	TIME [epoch: 8.53 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2641527979659252		[learning rate: 7.1251e-05]
	Learning Rate: 7.12508e-05
	LOSS [training: 0.2641527979659252 | validation: 0.24004054121406615]
	TIME [epoch: 8.53 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2626085688003703		[learning rate: 7.0992e-05]
	Learning Rate: 7.09922e-05
	LOSS [training: 0.2626085688003703 | validation: 0.25626251762198654]
	TIME [epoch: 8.56 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2576779732214714		[learning rate: 7.0735e-05]
	Learning Rate: 7.07345e-05
	LOSS [training: 0.2576779732214714 | validation: 0.2380983807507776]
	TIME [epoch: 8.53 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2631632246083259		[learning rate: 7.0478e-05]
	Learning Rate: 7.04778e-05
	LOSS [training: 0.2631632246083259 | validation: 0.25211736121505285]
	TIME [epoch: 8.53 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2611192534532668		[learning rate: 7.0222e-05]
	Learning Rate: 7.02221e-05
	LOSS [training: 0.2611192534532668 | validation: 0.25113436374657366]
	TIME [epoch: 8.54 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2748992915586754		[learning rate: 6.9967e-05]
	Learning Rate: 6.99672e-05
	LOSS [training: 0.2748992915586754 | validation: 0.26023675897671156]
	TIME [epoch: 8.56 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26562372025881803		[learning rate: 6.9713e-05]
	Learning Rate: 6.97133e-05
	LOSS [training: 0.26562372025881803 | validation: 0.22200726448734087]
	TIME [epoch: 8.53 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2587396765823965		[learning rate: 6.946e-05]
	Learning Rate: 6.94603e-05
	LOSS [training: 0.2587396765823965 | validation: 0.2376506610627328]
	TIME [epoch: 8.53 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2512031735770155		[learning rate: 6.9208e-05]
	Learning Rate: 6.92083e-05
	LOSS [training: 0.2512031735770155 | validation: 0.24666637367346783]
	TIME [epoch: 8.54 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.259528612035133		[learning rate: 6.8957e-05]
	Learning Rate: 6.89571e-05
	LOSS [training: 0.259528612035133 | validation: 0.2455493269245882]
	TIME [epoch: 8.56 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2522061857428093		[learning rate: 6.8707e-05]
	Learning Rate: 6.87068e-05
	LOSS [training: 0.2522061857428093 | validation: 0.24070010797000763]
	TIME [epoch: 8.53 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2555648421790345		[learning rate: 6.8457e-05]
	Learning Rate: 6.84575e-05
	LOSS [training: 0.2555648421790345 | validation: 0.23800458578903044]
	TIME [epoch: 8.53 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2621379296341138		[learning rate: 6.8209e-05]
	Learning Rate: 6.82091e-05
	LOSS [training: 0.2621379296341138 | validation: 0.23038657023825587]
	TIME [epoch: 8.56 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26207876550712633		[learning rate: 6.7962e-05]
	Learning Rate: 6.79615e-05
	LOSS [training: 0.26207876550712633 | validation: 0.23426481630642473]
	TIME [epoch: 8.54 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2572206205903109		[learning rate: 6.7715e-05]
	Learning Rate: 6.77149e-05
	LOSS [training: 0.2572206205903109 | validation: 0.2364769049671638]
	TIME [epoch: 8.53 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25717022209665596		[learning rate: 6.7469e-05]
	Learning Rate: 6.74692e-05
	LOSS [training: 0.25717022209665596 | validation: 0.24372981330662374]
	TIME [epoch: 8.53 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.253627154950476		[learning rate: 6.7224e-05]
	Learning Rate: 6.72243e-05
	LOSS [training: 0.253627154950476 | validation: 0.2324758785296832]
	TIME [epoch: 8.56 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26112369175413364		[learning rate: 6.698e-05]
	Learning Rate: 6.69804e-05
	LOSS [training: 0.26112369175413364 | validation: 0.232155813832483]
	TIME [epoch: 8.53 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24922066774080318		[learning rate: 6.6737e-05]
	Learning Rate: 6.67373e-05
	LOSS [training: 0.24922066774080318 | validation: 0.2555019789103582]
	TIME [epoch: 8.52 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26022526680634156		[learning rate: 6.6495e-05]
	Learning Rate: 6.64951e-05
	LOSS [training: 0.26022526680634156 | validation: 0.24058793940775672]
	TIME [epoch: 8.54 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2597832012726205		[learning rate: 6.6254e-05]
	Learning Rate: 6.62538e-05
	LOSS [training: 0.2597832012726205 | validation: 0.2350244195250231]
	TIME [epoch: 8.56 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2537501645333328		[learning rate: 6.6013e-05]
	Learning Rate: 6.60133e-05
	LOSS [training: 0.2537501645333328 | validation: 0.23661403415791935]
	TIME [epoch: 8.53 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2528652587551098		[learning rate: 6.5774e-05]
	Learning Rate: 6.57738e-05
	LOSS [training: 0.2528652587551098 | validation: 0.242769745917833]
	TIME [epoch: 8.53 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25943260391947087		[learning rate: 6.5535e-05]
	Learning Rate: 6.55351e-05
	LOSS [training: 0.25943260391947087 | validation: 0.2295343868198721]
	TIME [epoch: 8.54 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2551015605052994		[learning rate: 6.5297e-05]
	Learning Rate: 6.52972e-05
	LOSS [training: 0.2551015605052994 | validation: 0.23509346782453658]
	TIME [epoch: 8.55 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2522555498896174		[learning rate: 6.506e-05]
	Learning Rate: 6.50603e-05
	LOSS [training: 0.2522555498896174 | validation: 0.2395771577767119]
	TIME [epoch: 8.54 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25678060642209777		[learning rate: 6.4824e-05]
	Learning Rate: 6.48242e-05
	LOSS [training: 0.25678060642209777 | validation: 0.23830472417617546]
	TIME [epoch: 8.53 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2539566188930312		[learning rate: 6.4589e-05]
	Learning Rate: 6.45889e-05
	LOSS [training: 0.2539566188930312 | validation: 0.23742668044027151]
	TIME [epoch: 8.54 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26432047053527974		[learning rate: 6.4354e-05]
	Learning Rate: 6.43545e-05
	LOSS [training: 0.26432047053527974 | validation: 0.226152890825331]
	TIME [epoch: 8.55 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.251143324690234		[learning rate: 6.4121e-05]
	Learning Rate: 6.4121e-05
	LOSS [training: 0.251143324690234 | validation: 0.2540552111881732]
	TIME [epoch: 8.53 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2534367494179957		[learning rate: 6.3888e-05]
	Learning Rate: 6.38883e-05
	LOSS [training: 0.2534367494179957 | validation: 0.2590719328500568]
	TIME [epoch: 8.53 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25844827483588373		[learning rate: 6.3656e-05]
	Learning Rate: 6.36564e-05
	LOSS [training: 0.25844827483588373 | validation: 0.24208756408331364]
	TIME [epoch: 8.55 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2645696415339318		[learning rate: 6.3425e-05]
	Learning Rate: 6.34254e-05
	LOSS [training: 0.2645696415339318 | validation: 0.24137248429658975]
	TIME [epoch: 8.55 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2519266005728704		[learning rate: 6.3195e-05]
	Learning Rate: 6.31952e-05
	LOSS [training: 0.2519266005728704 | validation: 0.22981617647114522]
	TIME [epoch: 8.53 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2546290238290684		[learning rate: 6.2966e-05]
	Learning Rate: 6.29659e-05
	LOSS [training: 0.2546290238290684 | validation: 0.24370966303871372]
	TIME [epoch: 8.54 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25238572841976786		[learning rate: 6.2737e-05]
	Learning Rate: 6.27374e-05
	LOSS [training: 0.25238572841976786 | validation: 0.21554819517100965]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1495.pth
	Model improved!!!
EPOCH 1496/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2501000149957239		[learning rate: 6.251e-05]
	Learning Rate: 6.25097e-05
	LOSS [training: 0.2501000149957239 | validation: 0.22002385532832258]
	TIME [epoch: 8.55 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2559915463883079		[learning rate: 6.2283e-05]
	Learning Rate: 6.22828e-05
	LOSS [training: 0.2559915463883079 | validation: 0.22859305864843907]
	TIME [epoch: 8.53 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2518230044899149		[learning rate: 6.2057e-05]
	Learning Rate: 6.20568e-05
	LOSS [training: 0.2518230044899149 | validation: 0.24228353457552051]
	TIME [epoch: 8.54 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2516150768814117		[learning rate: 6.1832e-05]
	Learning Rate: 6.18316e-05
	LOSS [training: 0.2516150768814117 | validation: 0.22123893775389736]
	TIME [epoch: 8.53 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25050293710814625		[learning rate: 6.1607e-05]
	Learning Rate: 6.16072e-05
	LOSS [training: 0.25050293710814625 | validation: 0.24844325132142467]
	TIME [epoch: 8.54 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2551638707878428		[learning rate: 6.1384e-05]
	Learning Rate: 6.13836e-05
	LOSS [training: 0.2551638707878428 | validation: 0.2412815043399942]
	TIME [epoch: 8.53 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2502928416248259		[learning rate: 6.1161e-05]
	Learning Rate: 6.11609e-05
	LOSS [training: 0.2502928416248259 | validation: 0.22465166371760178]
	TIME [epoch: 8.54 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25070247483830493		[learning rate: 6.0939e-05]
	Learning Rate: 6.09389e-05
	LOSS [training: 0.25070247483830493 | validation: 0.21519178141238218]
	TIME [epoch: 8.54 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1503.pth
	Model improved!!!
EPOCH 1504/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25010318849465907		[learning rate: 6.0718e-05]
	Learning Rate: 6.07178e-05
	LOSS [training: 0.25010318849465907 | validation: 0.24279354420906168]
	TIME [epoch: 8.54 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2533169293483367		[learning rate: 6.0497e-05]
	Learning Rate: 6.04974e-05
	LOSS [training: 0.2533169293483367 | validation: 0.24710283742646055]
	TIME [epoch: 8.77 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25141948634396577		[learning rate: 6.0278e-05]
	Learning Rate: 6.02779e-05
	LOSS [training: 0.25141948634396577 | validation: 0.23047557331986807]
	TIME [epoch: 8.54 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25125689622626135		[learning rate: 6.0059e-05]
	Learning Rate: 6.00591e-05
	LOSS [training: 0.25125689622626135 | validation: 0.22521258493080698]
	TIME [epoch: 8.55 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2536278087729683		[learning rate: 5.9841e-05]
	Learning Rate: 5.98412e-05
	LOSS [training: 0.2536278087729683 | validation: 0.22933930789525458]
	TIME [epoch: 8.53 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25237652699026025		[learning rate: 5.9624e-05]
	Learning Rate: 5.9624e-05
	LOSS [training: 0.25237652699026025 | validation: 0.25739723620683336]
	TIME [epoch: 8.54 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25329151710659104		[learning rate: 5.9408e-05]
	Learning Rate: 5.94076e-05
	LOSS [training: 0.25329151710659104 | validation: 0.23397889135233424]
	TIME [epoch: 8.54 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2504881722949607		[learning rate: 5.9192e-05]
	Learning Rate: 5.9192e-05
	LOSS [training: 0.2504881722949607 | validation: 0.2390269933033371]
	TIME [epoch: 8.56 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25464384101723725		[learning rate: 5.8977e-05]
	Learning Rate: 5.89772e-05
	LOSS [training: 0.25464384101723725 | validation: 0.231148561569472]
	TIME [epoch: 8.53 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25275068219002683		[learning rate: 5.8763e-05]
	Learning Rate: 5.87632e-05
	LOSS [training: 0.25275068219002683 | validation: 0.23039546555221602]
	TIME [epoch: 8.55 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24577674568298544		[learning rate: 5.855e-05]
	Learning Rate: 5.85499e-05
	LOSS [training: 0.24577674568298544 | validation: 0.237965071065988]
	TIME [epoch: 8.53 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25372730135097876		[learning rate: 5.8337e-05]
	Learning Rate: 5.83374e-05
	LOSS [training: 0.25372730135097876 | validation: 0.23578262656518711]
	TIME [epoch: 8.55 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25138473254862104		[learning rate: 5.8126e-05]
	Learning Rate: 5.81257e-05
	LOSS [training: 0.25138473254862104 | validation: 0.25056892504479006]
	TIME [epoch: 8.54 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2600038904246395		[learning rate: 5.7915e-05]
	Learning Rate: 5.79148e-05
	LOSS [training: 0.2600038904246395 | validation: 0.23148265057924025]
	TIME [epoch: 8.55 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2523485228443965		[learning rate: 5.7705e-05]
	Learning Rate: 5.77046e-05
	LOSS [training: 0.2523485228443965 | validation: 0.22790570831146]
	TIME [epoch: 8.54 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25039729367953406		[learning rate: 5.7495e-05]
	Learning Rate: 5.74952e-05
	LOSS [training: 0.25039729367953406 | validation: 0.2333791348161689]
	TIME [epoch: 8.56 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24783976292001073		[learning rate: 5.7287e-05]
	Learning Rate: 5.72866e-05
	LOSS [training: 0.24783976292001073 | validation: 0.23557422478044862]
	TIME [epoch: 8.55 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24880514053526412		[learning rate: 5.7079e-05]
	Learning Rate: 5.70787e-05
	LOSS [training: 0.24880514053526412 | validation: 0.22857257341379836]
	TIME [epoch: 8.54 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25464651492606905		[learning rate: 5.6872e-05]
	Learning Rate: 5.68715e-05
	LOSS [training: 0.25464651492606905 | validation: 0.22245188876509195]
	TIME [epoch: 8.54 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26305593197757826		[learning rate: 5.6665e-05]
	Learning Rate: 5.66651e-05
	LOSS [training: 0.26305593197757826 | validation: 0.24252275875102397]
	TIME [epoch: 8.55 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2538321606751096		[learning rate: 5.6459e-05]
	Learning Rate: 5.64595e-05
	LOSS [training: 0.2538321606751096 | validation: 0.22370219567699007]
	TIME [epoch: 8.56 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2501529285714656		[learning rate: 5.6255e-05]
	Learning Rate: 5.62546e-05
	LOSS [training: 0.2501529285714656 | validation: 0.23956775941968628]
	TIME [epoch: 8.54 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2552597843159882		[learning rate: 5.605e-05]
	Learning Rate: 5.60504e-05
	LOSS [training: 0.2552597843159882 | validation: 0.23760783758516207]
	TIME [epoch: 8.54 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25443440761082897		[learning rate: 5.5847e-05]
	Learning Rate: 5.5847e-05
	LOSS [training: 0.25443440761082897 | validation: 0.2438305465344219]
	TIME [epoch: 8.55 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26156259937247883		[learning rate: 5.5644e-05]
	Learning Rate: 5.56444e-05
	LOSS [training: 0.26156259937247883 | validation: 0.2455813213161751]
	TIME [epoch: 8.56 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25996733061125565		[learning rate: 5.5442e-05]
	Learning Rate: 5.54424e-05
	LOSS [training: 0.25996733061125565 | validation: 0.2476016831180356]
	TIME [epoch: 8.53 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26152559807501574		[learning rate: 5.5241e-05]
	Learning Rate: 5.52412e-05
	LOSS [training: 0.26152559807501574 | validation: 0.22037212227202663]
	TIME [epoch: 8.55 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25199330368507505		[learning rate: 5.5041e-05]
	Learning Rate: 5.50407e-05
	LOSS [training: 0.25199330368507505 | validation: 0.21765134230620364]
	TIME [epoch: 8.54 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24913896444077946		[learning rate: 5.4841e-05]
	Learning Rate: 5.4841e-05
	LOSS [training: 0.24913896444077946 | validation: 0.23882280855349935]
	TIME [epoch: 8.56 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25444106924774645		[learning rate: 5.4642e-05]
	Learning Rate: 5.4642e-05
	LOSS [training: 0.25444106924774645 | validation: 0.24483482825949682]
	TIME [epoch: 8.53 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25223665888508673		[learning rate: 5.4444e-05]
	Learning Rate: 5.44437e-05
	LOSS [training: 0.25223665888508673 | validation: 0.23643470917051215]
	TIME [epoch: 8.56 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25515981677822575		[learning rate: 5.4246e-05]
	Learning Rate: 5.42461e-05
	LOSS [training: 0.25515981677822575 | validation: 0.22570573759649445]
	TIME [epoch: 8.55 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2500259061537443		[learning rate: 5.4049e-05]
	Learning Rate: 5.40492e-05
	LOSS [training: 0.2500259061537443 | validation: 0.21875193807031165]
	TIME [epoch: 8.56 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25740951448881494		[learning rate: 5.3853e-05]
	Learning Rate: 5.38531e-05
	LOSS [training: 0.25740951448881494 | validation: 0.24017527596385496]
	TIME [epoch: 8.53 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2517327311869936		[learning rate: 5.3658e-05]
	Learning Rate: 5.36577e-05
	LOSS [training: 0.2517327311869936 | validation: 0.23095802639878343]
	TIME [epoch: 8.55 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2506823819423406		[learning rate: 5.3463e-05]
	Learning Rate: 5.34629e-05
	LOSS [training: 0.2506823819423406 | validation: 0.23315715686821847]
	TIME [epoch: 8.55 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24827085248577233		[learning rate: 5.3269e-05]
	Learning Rate: 5.32689e-05
	LOSS [training: 0.24827085248577233 | validation: 0.23363748839764198]
	TIME [epoch: 8.55 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2491451066093624		[learning rate: 5.3076e-05]
	Learning Rate: 5.30756e-05
	LOSS [training: 0.2491451066093624 | validation: 0.24744783834204642]
	TIME [epoch: 8.54 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.253274252152918		[learning rate: 5.2883e-05]
	Learning Rate: 5.2883e-05
	LOSS [training: 0.253274252152918 | validation: 0.24043980489204397]
	TIME [epoch: 8.56 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25218454973234017		[learning rate: 5.2691e-05]
	Learning Rate: 5.26911e-05
	LOSS [training: 0.25218454973234017 | validation: 0.22632448572959019]
	TIME [epoch: 8.55 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25039123447262224		[learning rate: 5.25e-05]
	Learning Rate: 5.24998e-05
	LOSS [training: 0.25039123447262224 | validation: 0.2222862565852358]
	TIME [epoch: 8.54 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25237200318956143		[learning rate: 5.2309e-05]
	Learning Rate: 5.23093e-05
	LOSS [training: 0.25237200318956143 | validation: 0.2339468873581375]
	TIME [epoch: 8.54 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25144756521532524		[learning rate: 5.2119e-05]
	Learning Rate: 5.21195e-05
	LOSS [training: 0.25144756521532524 | validation: 0.2431504322148364]
	TIME [epoch: 8.55 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24798919853308718		[learning rate: 5.193e-05]
	Learning Rate: 5.19303e-05
	LOSS [training: 0.24798919853308718 | validation: 0.22989979628952567]
	TIME [epoch: 8.55 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2501190973521512		[learning rate: 5.1742e-05]
	Learning Rate: 5.17419e-05
	LOSS [training: 0.2501190973521512 | validation: 0.2384404694129983]
	TIME [epoch: 8.53 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.252481412170909		[learning rate: 5.1554e-05]
	Learning Rate: 5.15541e-05
	LOSS [training: 0.252481412170909 | validation: 0.22975567775016972]
	TIME [epoch: 8.53 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24422970842606162		[learning rate: 5.1367e-05]
	Learning Rate: 5.1367e-05
	LOSS [training: 0.24422970842606162 | validation: 0.23834798771687427]
	TIME [epoch: 8.56 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25669928074473736		[learning rate: 5.1181e-05]
	Learning Rate: 5.11806e-05
	LOSS [training: 0.25669928074473736 | validation: 0.24014719798856343]
	TIME [epoch: 8.54 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24569511883756237		[learning rate: 5.0995e-05]
	Learning Rate: 5.09949e-05
	LOSS [training: 0.24569511883756237 | validation: 0.23332928174376039]
	TIME [epoch: 8.53 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2523597685173921		[learning rate: 5.081e-05]
	Learning Rate: 5.08098e-05
	LOSS [training: 0.2523597685173921 | validation: 0.23433937855882328]
	TIME [epoch: 8.53 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24888198490839222		[learning rate: 5.0625e-05]
	Learning Rate: 5.06254e-05
	LOSS [training: 0.24888198490839222 | validation: 0.22231230112871883]
	TIME [epoch: 8.56 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24911456093837794		[learning rate: 5.0442e-05]
	Learning Rate: 5.04417e-05
	LOSS [training: 0.24911456093837794 | validation: 0.22256745974139489]
	TIME [epoch: 8.54 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24853102784693132		[learning rate: 5.0259e-05]
	Learning Rate: 5.02586e-05
	LOSS [training: 0.24853102784693132 | validation: 0.22245981544125035]
	TIME [epoch: 8.53 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2506207991931748		[learning rate: 5.0076e-05]
	Learning Rate: 5.00762e-05
	LOSS [training: 0.2506207991931748 | validation: 0.22036814528626963]
	TIME [epoch: 8.54 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24733051627477387		[learning rate: 4.9894e-05]
	Learning Rate: 4.98945e-05
	LOSS [training: 0.24733051627477387 | validation: 0.2344875869778047]
	TIME [epoch: 8.56 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24725676699260904		[learning rate: 4.9713e-05]
	Learning Rate: 4.97134e-05
	LOSS [training: 0.24725676699260904 | validation: 0.23958746504192852]
	TIME [epoch: 8.53 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25307441332868164		[learning rate: 4.9533e-05]
	Learning Rate: 4.9533e-05
	LOSS [training: 0.25307441332868164 | validation: 0.24107660170030126]
	TIME [epoch: 8.53 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24536796284545695		[learning rate: 4.9353e-05]
	Learning Rate: 4.93533e-05
	LOSS [training: 0.24536796284545695 | validation: 0.24189970518931553]
	TIME [epoch: 8.55 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25341385435513264		[learning rate: 4.9174e-05]
	Learning Rate: 4.91742e-05
	LOSS [training: 0.25341385435513264 | validation: 0.23528175969853551]
	TIME [epoch: 8.56 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2544858389104988		[learning rate: 4.8996e-05]
	Learning Rate: 4.89957e-05
	LOSS [training: 0.2544858389104988 | validation: 0.23332759727470762]
	TIME [epoch: 8.53 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.26053876906104295		[learning rate: 4.8818e-05]
	Learning Rate: 4.88179e-05
	LOSS [training: 0.26053876906104295 | validation: 0.2506633604585376]
	TIME [epoch: 8.53 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24616112912097804		[learning rate: 4.8641e-05]
	Learning Rate: 4.86407e-05
	LOSS [training: 0.24616112912097804 | validation: 0.23611098812571385]
	TIME [epoch: 8.56 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24817715119026076		[learning rate: 4.8464e-05]
	Learning Rate: 4.84642e-05
	LOSS [training: 0.24817715119026076 | validation: 0.24128611305265757]
	TIME [epoch: 8.55 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2504951964350842		[learning rate: 4.8288e-05]
	Learning Rate: 4.82883e-05
	LOSS [training: 0.2504951964350842 | validation: 0.2332491269404882]
	TIME [epoch: 8.53 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2506380912021847		[learning rate: 4.8113e-05]
	Learning Rate: 4.81131e-05
	LOSS [training: 0.2506380912021847 | validation: 0.22157343790359474]
	TIME [epoch: 8.53 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2457617618782595		[learning rate: 4.7938e-05]
	Learning Rate: 4.79385e-05
	LOSS [training: 0.2457617618782595 | validation: 0.2577987499012278]
	TIME [epoch: 8.56 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24512278108963786		[learning rate: 4.7765e-05]
	Learning Rate: 4.77645e-05
	LOSS [training: 0.24512278108963786 | validation: 0.23762872014123118]
	TIME [epoch: 8.54 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25120351691791165		[learning rate: 4.7591e-05]
	Learning Rate: 4.75912e-05
	LOSS [training: 0.25120351691791165 | validation: 0.22454751285295912]
	TIME [epoch: 8.53 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24552800365314392		[learning rate: 4.7418e-05]
	Learning Rate: 4.74185e-05
	LOSS [training: 0.24552800365314392 | validation: 0.23254615005104132]
	TIME [epoch: 8.53 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2536096790105577		[learning rate: 4.7246e-05]
	Learning Rate: 4.72464e-05
	LOSS [training: 0.2536096790105577 | validation: 0.2289116049842695]
	TIME [epoch: 8.57 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24575910568109047		[learning rate: 4.7075e-05]
	Learning Rate: 4.70749e-05
	LOSS [training: 0.24575910568109047 | validation: 0.24299796424556186]
	TIME [epoch: 8.53 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2491908687412999		[learning rate: 4.6904e-05]
	Learning Rate: 4.69041e-05
	LOSS [training: 0.2491908687412999 | validation: 0.22736126394961087]
	TIME [epoch: 8.53 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2502856605159047		[learning rate: 4.6734e-05]
	Learning Rate: 4.67339e-05
	LOSS [training: 0.2502856605159047 | validation: 0.2354397412575316]
	TIME [epoch: 8.53 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2511700049558516		[learning rate: 4.6564e-05]
	Learning Rate: 4.65643e-05
	LOSS [training: 0.2511700049558516 | validation: 0.2291970879986345]
	TIME [epoch: 8.57 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24703925580786984		[learning rate: 4.6395e-05]
	Learning Rate: 4.63953e-05
	LOSS [training: 0.24703925580786984 | validation: 0.23072706581383887]
	TIME [epoch: 8.53 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24709235858387096		[learning rate: 4.6227e-05]
	Learning Rate: 4.62269e-05
	LOSS [training: 0.24709235858387096 | validation: 0.21258880449216455]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1579.pth
	Model improved!!!
EPOCH 1580/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24395641868645565		[learning rate: 4.6059e-05]
	Learning Rate: 4.60591e-05
	LOSS [training: 0.24395641868645565 | validation: 0.23559750965645448]
	TIME [epoch: 8.54 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24824472574792336		[learning rate: 4.5892e-05]
	Learning Rate: 4.5892e-05
	LOSS [training: 0.24824472574792336 | validation: 0.2312261899169589]
	TIME [epoch: 8.55 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24886542477944373		[learning rate: 4.5725e-05]
	Learning Rate: 4.57254e-05
	LOSS [training: 0.24886542477944373 | validation: 0.22822518731636404]
	TIME [epoch: 8.53 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2548595124578303		[learning rate: 4.556e-05]
	Learning Rate: 4.55595e-05
	LOSS [training: 0.2548595124578303 | validation: 0.23442215386105889]
	TIME [epoch: 8.53 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25288736746548995		[learning rate: 4.5394e-05]
	Learning Rate: 4.53942e-05
	LOSS [training: 0.25288736746548995 | validation: 0.2552496651978309]
	TIME [epoch: 8.55 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25226694559318363		[learning rate: 4.5229e-05]
	Learning Rate: 4.52294e-05
	LOSS [training: 0.25226694559318363 | validation: 0.22491735155544065]
	TIME [epoch: 8.54 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25073184561353856		[learning rate: 4.5065e-05]
	Learning Rate: 4.50653e-05
	LOSS [training: 0.25073184561353856 | validation: 0.21712410542338237]
	TIME [epoch: 8.53 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2518504932174225		[learning rate: 4.4902e-05]
	Learning Rate: 4.49017e-05
	LOSS [training: 0.2518504932174225 | validation: 0.22172670904460112]
	TIME [epoch: 8.52 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24443899265831992		[learning rate: 4.4739e-05]
	Learning Rate: 4.47388e-05
	LOSS [training: 0.24443899265831992 | validation: 0.23899450105073916]
	TIME [epoch: 8.55 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2515311320028656		[learning rate: 4.4576e-05]
	Learning Rate: 4.45764e-05
	LOSS [training: 0.2515311320028656 | validation: 0.22687469799155294]
	TIME [epoch: 8.54 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2552145261241938		[learning rate: 4.4415e-05]
	Learning Rate: 4.44147e-05
	LOSS [training: 0.2552145261241938 | validation: 0.2574570851537189]
	TIME [epoch: 8.52 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2501762509756301		[learning rate: 4.4253e-05]
	Learning Rate: 4.42535e-05
	LOSS [training: 0.2501762509756301 | validation: 0.22283318572775496]
	TIME [epoch: 8.52 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2443968657485339		[learning rate: 4.4093e-05]
	Learning Rate: 4.40929e-05
	LOSS [training: 0.2443968657485339 | validation: 0.22450095500228245]
	TIME [epoch: 8.57 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24973975147717792		[learning rate: 4.3933e-05]
	Learning Rate: 4.39329e-05
	LOSS [training: 0.24973975147717792 | validation: 0.2420578162986059]
	TIME [epoch: 8.53 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24785278707908973		[learning rate: 4.3773e-05]
	Learning Rate: 4.37734e-05
	LOSS [training: 0.24785278707908973 | validation: 0.24318152810460755]
	TIME [epoch: 8.53 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25341404119555205		[learning rate: 4.3615e-05]
	Learning Rate: 4.36146e-05
	LOSS [training: 0.25341404119555205 | validation: 0.22824991596837735]
	TIME [epoch: 8.52 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24517951066355073		[learning rate: 4.3456e-05]
	Learning Rate: 4.34563e-05
	LOSS [training: 0.24517951066355073 | validation: 0.23751126319773225]
	TIME [epoch: 8.56 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2503703623307821		[learning rate: 4.3299e-05]
	Learning Rate: 4.32986e-05
	LOSS [training: 0.2503703623307821 | validation: 0.24195860931960866]
	TIME [epoch: 8.53 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25136529971970845		[learning rate: 4.3141e-05]
	Learning Rate: 4.31415e-05
	LOSS [training: 0.25136529971970845 | validation: 0.22908527828869382]
	TIME [epoch: 8.52 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2480725838751101		[learning rate: 4.2985e-05]
	Learning Rate: 4.29849e-05
	LOSS [training: 0.2480725838751101 | validation: 0.22802240060383663]
	TIME [epoch: 8.54 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24832770278472932		[learning rate: 4.2829e-05]
	Learning Rate: 4.28289e-05
	LOSS [training: 0.24832770278472932 | validation: 0.234131188916054]
	TIME [epoch: 8.55 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25162537794941164		[learning rate: 4.2673e-05]
	Learning Rate: 4.26735e-05
	LOSS [training: 0.25162537794941164 | validation: 0.23767354511871167]
	TIME [epoch: 8.53 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2489213605820631		[learning rate: 4.2519e-05]
	Learning Rate: 4.25186e-05
	LOSS [training: 0.2489213605820631 | validation: 0.24312558424095865]
	TIME [epoch: 8.52 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2461610838153962		[learning rate: 4.2364e-05]
	Learning Rate: 4.23643e-05
	LOSS [training: 0.2461610838153962 | validation: 0.22151910678771608]
	TIME [epoch: 8.53 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25415021379953046		[learning rate: 4.2211e-05]
	Learning Rate: 4.22106e-05
	LOSS [training: 0.25415021379953046 | validation: 0.23273862132753503]
	TIME [epoch: 8.56 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2499217320259542		[learning rate: 4.2057e-05]
	Learning Rate: 4.20574e-05
	LOSS [training: 0.2499217320259542 | validation: 0.22169647293383138]
	TIME [epoch: 8.53 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2537502795472358		[learning rate: 4.1905e-05]
	Learning Rate: 4.19047e-05
	LOSS [training: 0.2537502795472358 | validation: 0.2305929011622463]
	TIME [epoch: 8.52 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2464561750228326		[learning rate: 4.1753e-05]
	Learning Rate: 4.17527e-05
	LOSS [training: 0.2464561750228326 | validation: 0.24247294966224225]
	TIME [epoch: 8.54 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24415853125665307		[learning rate: 4.1601e-05]
	Learning Rate: 4.16012e-05
	LOSS [training: 0.24415853125665307 | validation: 0.23131088810946573]
	TIME [epoch: 8.55 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25566982615410966		[learning rate: 4.145e-05]
	Learning Rate: 4.14502e-05
	LOSS [training: 0.25566982615410966 | validation: 0.23546465673475941]
	TIME [epoch: 8.53 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2476583024084388		[learning rate: 4.13e-05]
	Learning Rate: 4.12998e-05
	LOSS [training: 0.2476583024084388 | validation: 0.2262048333306029]
	TIME [epoch: 8.52 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24928780408792237		[learning rate: 4.115e-05]
	Learning Rate: 4.11499e-05
	LOSS [training: 0.24928780408792237 | validation: 0.23304731768570708]
	TIME [epoch: 8.54 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2570782881892376		[learning rate: 4.1001e-05]
	Learning Rate: 4.10005e-05
	LOSS [training: 0.2570782881892376 | validation: 0.22787629444442076]
	TIME [epoch: 8.55 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2445195847669243		[learning rate: 4.0852e-05]
	Learning Rate: 4.08517e-05
	LOSS [training: 0.2445195847669243 | validation: 0.24720134572272207]
	TIME [epoch: 8.52 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24648142087487632		[learning rate: 4.0703e-05]
	Learning Rate: 4.07035e-05
	LOSS [training: 0.24648142087487632 | validation: 0.22360248110759862]
	TIME [epoch: 8.52 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25058077798721723		[learning rate: 4.0556e-05]
	Learning Rate: 4.05558e-05
	LOSS [training: 0.25058077798721723 | validation: 0.23841282102508202]
	TIME [epoch: 8.55 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.246292700373552		[learning rate: 4.0409e-05]
	Learning Rate: 4.04086e-05
	LOSS [training: 0.246292700373552 | validation: 0.24040572960480905]
	TIME [epoch: 8.54 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24779393191990162		[learning rate: 4.0262e-05]
	Learning Rate: 4.0262e-05
	LOSS [training: 0.24779393191990162 | validation: 0.23420803299807913]
	TIME [epoch: 8.52 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2468472262402433		[learning rate: 4.0116e-05]
	Learning Rate: 4.01158e-05
	LOSS [training: 0.2468472262402433 | validation: 0.23407164023763904]
	TIME [epoch: 8.52 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2500378402034315		[learning rate: 3.997e-05]
	Learning Rate: 3.99703e-05
	LOSS [training: 0.2500378402034315 | validation: 0.22704545137381174]
	TIME [epoch: 8.56 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2470875421050815		[learning rate: 3.9825e-05]
	Learning Rate: 3.98252e-05
	LOSS [training: 0.2470875421050815 | validation: 0.23658106087940395]
	TIME [epoch: 8.53 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2437030752222913		[learning rate: 3.9681e-05]
	Learning Rate: 3.96807e-05
	LOSS [training: 0.2437030752222913 | validation: 0.2294250738071078]
	TIME [epoch: 8.53 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24894201731388113		[learning rate: 3.9537e-05]
	Learning Rate: 3.95367e-05
	LOSS [training: 0.24894201731388113 | validation: 0.23596580445664547]
	TIME [epoch: 8.52 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25047786683204476		[learning rate: 3.9393e-05]
	Learning Rate: 3.93932e-05
	LOSS [training: 0.25047786683204476 | validation: 0.2391113512465673]
	TIME [epoch: 8.57 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2441578572617102		[learning rate: 3.925e-05]
	Learning Rate: 3.92502e-05
	LOSS [training: 0.2441578572617102 | validation: 0.23321193239467097]
	TIME [epoch: 8.53 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24711328920204947		[learning rate: 3.9108e-05]
	Learning Rate: 3.91078e-05
	LOSS [training: 0.24711328920204947 | validation: 0.2332845892987796]
	TIME [epoch: 8.53 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24586686360367457		[learning rate: 3.8966e-05]
	Learning Rate: 3.89659e-05
	LOSS [training: 0.24586686360367457 | validation: 0.2274159581875502]
	TIME [epoch: 8.53 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24281128024520124		[learning rate: 3.8824e-05]
	Learning Rate: 3.88245e-05
	LOSS [training: 0.24281128024520124 | validation: 0.2237605007034546]
	TIME [epoch: 8.56 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24452377687142982		[learning rate: 3.8684e-05]
	Learning Rate: 3.86836e-05
	LOSS [training: 0.24452377687142982 | validation: 0.24508613685285072]
	TIME [epoch: 8.53 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24664611221888114		[learning rate: 3.8543e-05]
	Learning Rate: 3.85432e-05
	LOSS [training: 0.24664611221888114 | validation: 0.2274705254625272]
	TIME [epoch: 8.53 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24825070928402546		[learning rate: 3.8403e-05]
	Learning Rate: 3.84033e-05
	LOSS [training: 0.24825070928402546 | validation: 0.23696554602568942]
	TIME [epoch: 8.54 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25286851992598186		[learning rate: 3.8264e-05]
	Learning Rate: 3.82639e-05
	LOSS [training: 0.25286851992598186 | validation: 0.2493537654340836]
	TIME [epoch: 8.56 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25030302826369455		[learning rate: 3.8125e-05]
	Learning Rate: 3.81251e-05
	LOSS [training: 0.25030302826369455 | validation: 0.22575302009339407]
	TIME [epoch: 8.53 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2457104130823918		[learning rate: 3.7987e-05]
	Learning Rate: 3.79867e-05
	LOSS [training: 0.2457104130823918 | validation: 0.2291063496288442]
	TIME [epoch: 8.53 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24895447216562197		[learning rate: 3.7849e-05]
	Learning Rate: 3.78489e-05
	LOSS [training: 0.24895447216562197 | validation: 0.22778754953438696]
	TIME [epoch: 8.54 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24972642869087158		[learning rate: 3.7711e-05]
	Learning Rate: 3.77115e-05
	LOSS [training: 0.24972642869087158 | validation: 0.23140049310091604]
	TIME [epoch: 8.55 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24410192509732537		[learning rate: 3.7575e-05]
	Learning Rate: 3.75746e-05
	LOSS [training: 0.24410192509732537 | validation: 0.24305764587476927]
	TIME [epoch: 8.53 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24540207180256654		[learning rate: 3.7438e-05]
	Learning Rate: 3.74383e-05
	LOSS [training: 0.24540207180256654 | validation: 0.22991064812084339]
	TIME [epoch: 8.53 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24139867969726514		[learning rate: 3.7302e-05]
	Learning Rate: 3.73024e-05
	LOSS [training: 0.24139867969726514 | validation: 0.22346444441008056]
	TIME [epoch: 8.55 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2447500762053434		[learning rate: 3.7167e-05]
	Learning Rate: 3.7167e-05
	LOSS [training: 0.2447500762053434 | validation: 0.24004177219280334]
	TIME [epoch: 8.55 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23915897531260968		[learning rate: 3.7032e-05]
	Learning Rate: 3.70322e-05
	LOSS [training: 0.23915897531260968 | validation: 0.23604487365195412]
	TIME [epoch: 8.52 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24980434614873012		[learning rate: 3.6898e-05]
	Learning Rate: 3.68978e-05
	LOSS [training: 0.24980434614873012 | validation: 0.22715315681796722]
	TIME [epoch: 8.53 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24326535608653618		[learning rate: 3.6764e-05]
	Learning Rate: 3.67639e-05
	LOSS [training: 0.24326535608653618 | validation: 0.23232716434872125]
	TIME [epoch: 8.55 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24769858051790744		[learning rate: 3.663e-05]
	Learning Rate: 3.66304e-05
	LOSS [training: 0.24769858051790744 | validation: 0.23674379236771764]
	TIME [epoch: 8.54 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25194305336895534		[learning rate: 3.6498e-05]
	Learning Rate: 3.64975e-05
	LOSS [training: 0.25194305336895534 | validation: 0.25237313938009903]
	TIME [epoch: 8.53 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24701809480867337		[learning rate: 3.6365e-05]
	Learning Rate: 3.63651e-05
	LOSS [training: 0.24701809480867337 | validation: 0.2391418133100494]
	TIME [epoch: 8.53 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2473150750358699		[learning rate: 3.6233e-05]
	Learning Rate: 3.62331e-05
	LOSS [training: 0.2473150750358699 | validation: 0.2308583296236828]
	TIME [epoch: 8.56 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24432658106636218		[learning rate: 3.6102e-05]
	Learning Rate: 3.61016e-05
	LOSS [training: 0.24432658106636218 | validation: 0.2425031257639581]
	TIME [epoch: 8.53 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25213441613402715		[learning rate: 3.5971e-05]
	Learning Rate: 3.59706e-05
	LOSS [training: 0.25213441613402715 | validation: 0.2199853144914724]
	TIME [epoch: 8.53 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2488259115751399		[learning rate: 3.584e-05]
	Learning Rate: 3.584e-05
	LOSS [training: 0.2488259115751399 | validation: 0.2323712953880258]
	TIME [epoch: 8.54 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24521972443111384		[learning rate: 3.571e-05]
	Learning Rate: 3.571e-05
	LOSS [training: 0.24521972443111384 | validation: 0.22910124500688966]
	TIME [epoch: 8.55 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2497818431433286		[learning rate: 3.558e-05]
	Learning Rate: 3.55804e-05
	LOSS [training: 0.2497818431433286 | validation: 0.23157375208771797]
	TIME [epoch: 8.53 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24734503213087136		[learning rate: 3.5451e-05]
	Learning Rate: 3.54513e-05
	LOSS [training: 0.24734503213087136 | validation: 0.22003928609334913]
	TIME [epoch: 8.53 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2440975553312445		[learning rate: 3.5323e-05]
	Learning Rate: 3.53226e-05
	LOSS [training: 0.2440975553312445 | validation: 0.23588926246713143]
	TIME [epoch: 8.54 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24830196162480483		[learning rate: 3.5194e-05]
	Learning Rate: 3.51944e-05
	LOSS [training: 0.24830196162480483 | validation: 0.23875634017215308]
	TIME [epoch: 8.55 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2521299296408689		[learning rate: 3.5067e-05]
	Learning Rate: 3.50667e-05
	LOSS [training: 0.2521299296408689 | validation: 0.22839066854133583]
	TIME [epoch: 8.53 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24542224841993426		[learning rate: 3.4939e-05]
	Learning Rate: 3.49394e-05
	LOSS [training: 0.24542224841993426 | validation: 0.23711961644032692]
	TIME [epoch: 8.53 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2483960412119819		[learning rate: 3.4813e-05]
	Learning Rate: 3.48126e-05
	LOSS [training: 0.2483960412119819 | validation: 0.25290043457027783]
	TIME [epoch: 8.54 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2501132569385289		[learning rate: 3.4686e-05]
	Learning Rate: 3.46863e-05
	LOSS [training: 0.2501132569385289 | validation: 0.23246392426321233]
	TIME [epoch: 8.56 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25196026658354576		[learning rate: 3.456e-05]
	Learning Rate: 3.45604e-05
	LOSS [training: 0.25196026658354576 | validation: 0.2256931067812843]
	TIME [epoch: 8.53 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25556058478695765		[learning rate: 3.4435e-05]
	Learning Rate: 3.4435e-05
	LOSS [training: 0.25556058478695765 | validation: 0.25152305569198063]
	TIME [epoch: 8.52 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25399718412154326		[learning rate: 3.431e-05]
	Learning Rate: 3.431e-05
	LOSS [training: 0.25399718412154326 | validation: 0.2382340946739746]
	TIME [epoch: 8.54 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2491770538002446		[learning rate: 3.4186e-05]
	Learning Rate: 3.41855e-05
	LOSS [training: 0.2491770538002446 | validation: 0.2326330370685123]
	TIME [epoch: 8.55 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24538435621543334		[learning rate: 3.4061e-05]
	Learning Rate: 3.40615e-05
	LOSS [training: 0.24538435621543334 | validation: 0.22757044341813001]
	TIME [epoch: 8.53 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24668760454440583		[learning rate: 3.3938e-05]
	Learning Rate: 3.39378e-05
	LOSS [training: 0.24668760454440583 | validation: 0.23342548614539196]
	TIME [epoch: 8.53 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25429215896120333		[learning rate: 3.3815e-05]
	Learning Rate: 3.38147e-05
	LOSS [training: 0.25429215896120333 | validation: 0.2163899008895443]
	TIME [epoch: 8.55 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24689547737596457		[learning rate: 3.3692e-05]
	Learning Rate: 3.3692e-05
	LOSS [training: 0.24689547737596457 | validation: 0.2329013353328856]
	TIME [epoch: 8.55 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24365248162087444		[learning rate: 3.357e-05]
	Learning Rate: 3.35697e-05
	LOSS [training: 0.24365248162087444 | validation: 0.23776588669060117]
	TIME [epoch: 8.53 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2462863378418357		[learning rate: 3.3448e-05]
	Learning Rate: 3.34479e-05
	LOSS [training: 0.2462863378418357 | validation: 0.232269317504961]
	TIME [epoch: 8.54 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24756213345358669		[learning rate: 3.3326e-05]
	Learning Rate: 3.33265e-05
	LOSS [training: 0.24756213345358669 | validation: 0.24451413478605938]
	TIME [epoch: 8.54 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24588681080303793		[learning rate: 3.3206e-05]
	Learning Rate: 3.32055e-05
	LOSS [training: 0.24588681080303793 | validation: 0.2355874169188687]
	TIME [epoch: 8.55 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24768308755932295		[learning rate: 3.3085e-05]
	Learning Rate: 3.3085e-05
	LOSS [training: 0.24768308755932295 | validation: 0.2323970774134823]
	TIME [epoch: 8.52 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2488132623567277		[learning rate: 3.2965e-05]
	Learning Rate: 3.2965e-05
	LOSS [training: 0.2488132623567277 | validation: 0.23905060138365694]
	TIME [epoch: 8.54 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2448675628689132		[learning rate: 3.2845e-05]
	Learning Rate: 3.28453e-05
	LOSS [training: 0.2448675628689132 | validation: 0.23250311564104148]
	TIME [epoch: 8.53 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24384096254622026		[learning rate: 3.2726e-05]
	Learning Rate: 3.27261e-05
	LOSS [training: 0.24384096254622026 | validation: 0.23840995819678096]
	TIME [epoch: 8.55 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25389594757813094		[learning rate: 3.2607e-05]
	Learning Rate: 3.26074e-05
	LOSS [training: 0.25389594757813094 | validation: 0.22805190406990564]
	TIME [epoch: 8.52 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2471275787238695		[learning rate: 3.2489e-05]
	Learning Rate: 3.2489e-05
	LOSS [training: 0.2471275787238695 | validation: 0.24707523497895784]
	TIME [epoch: 8.54 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24658284265899777		[learning rate: 3.2371e-05]
	Learning Rate: 3.23711e-05
	LOSS [training: 0.24658284265899777 | validation: 0.24047049915963303]
	TIME [epoch: 8.55 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2451988914049946		[learning rate: 3.2254e-05]
	Learning Rate: 3.22537e-05
	LOSS [training: 0.2451988914049946 | validation: 0.2349728306233586]
	TIME [epoch: 8.54 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24865063742735724		[learning rate: 3.2137e-05]
	Learning Rate: 3.21366e-05
	LOSS [training: 0.24865063742735724 | validation: 0.2396711003376037]
	TIME [epoch: 8.53 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24538516892956647		[learning rate: 3.202e-05]
	Learning Rate: 3.202e-05
	LOSS [training: 0.24538516892956647 | validation: 0.22926244644383242]
	TIME [epoch: 8.54 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24835290216689482		[learning rate: 3.1904e-05]
	Learning Rate: 3.19038e-05
	LOSS [training: 0.24835290216689482 | validation: 0.22932642530574704]
	TIME [epoch: 8.55 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25136402075228936		[learning rate: 3.1788e-05]
	Learning Rate: 3.1788e-05
	LOSS [training: 0.25136402075228936 | validation: 0.23264968748124143]
	TIME [epoch: 8.53 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2472755597875851		[learning rate: 3.1673e-05]
	Learning Rate: 3.16726e-05
	LOSS [training: 0.2472755597875851 | validation: 0.23489936447675663]
	TIME [epoch: 8.53 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24570714251450965		[learning rate: 3.1558e-05]
	Learning Rate: 3.15577e-05
	LOSS [training: 0.24570714251450965 | validation: 0.22361005864291003]
	TIME [epoch: 8.54 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25085363683847844		[learning rate: 3.1443e-05]
	Learning Rate: 3.14432e-05
	LOSS [training: 0.25085363683847844 | validation: 0.2282169530359513]
	TIME [epoch: 8.54 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24618612171078486		[learning rate: 3.1329e-05]
	Learning Rate: 3.13291e-05
	LOSS [training: 0.24618612171078486 | validation: 0.24157663259909407]
	TIME [epoch: 8.53 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25423155346950377		[learning rate: 3.1215e-05]
	Learning Rate: 3.12154e-05
	LOSS [training: 0.25423155346950377 | validation: 0.2531781557944155]
	TIME [epoch: 8.53 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2541163252465861		[learning rate: 3.1102e-05]
	Learning Rate: 3.11021e-05
	LOSS [training: 0.2541163252465861 | validation: 0.24867857409866317]
	TIME [epoch: 8.54 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24796896677314276		[learning rate: 3.0989e-05]
	Learning Rate: 3.09892e-05
	LOSS [training: 0.24796896677314276 | validation: 0.24055059931381786]
	TIME [epoch: 8.55 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24696608298699543		[learning rate: 3.0877e-05]
	Learning Rate: 3.08768e-05
	LOSS [training: 0.24696608298699543 | validation: 0.24195476506369296]
	TIME [epoch: 8.52 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24552553303159014		[learning rate: 3.0765e-05]
	Learning Rate: 3.07647e-05
	LOSS [training: 0.24552553303159014 | validation: 0.24940864049369976]
	TIME [epoch: 8.53 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2441174310654953		[learning rate: 3.0653e-05]
	Learning Rate: 3.0653e-05
	LOSS [training: 0.2441174310654953 | validation: 0.23136504944565012]
	TIME [epoch: 8.54 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24298325648842303		[learning rate: 3.0542e-05]
	Learning Rate: 3.05418e-05
	LOSS [training: 0.24298325648842303 | validation: 0.24817358562195402]
	TIME [epoch: 8.55 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24837781554859237		[learning rate: 3.0431e-05]
	Learning Rate: 3.0431e-05
	LOSS [training: 0.24837781554859237 | validation: 0.24019722278937689]
	TIME [epoch: 8.53 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2461850066638418		[learning rate: 3.0321e-05]
	Learning Rate: 3.03205e-05
	LOSS [training: 0.2461850066638418 | validation: 0.23932143804877748]
	TIME [epoch: 8.54 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24700051234447473		[learning rate: 3.0211e-05]
	Learning Rate: 3.02105e-05
	LOSS [training: 0.24700051234447473 | validation: 0.23941366615893436]
	TIME [epoch: 8.53 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24480707802788065		[learning rate: 3.0101e-05]
	Learning Rate: 3.01009e-05
	LOSS [training: 0.24480707802788065 | validation: 0.23443482612223643]
	TIME [epoch: 8.55 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24354206174474008		[learning rate: 2.9992e-05]
	Learning Rate: 2.99916e-05
	LOSS [training: 0.24354206174474008 | validation: 0.2369506306110663]
	TIME [epoch: 8.53 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2419267599668804		[learning rate: 2.9883e-05]
	Learning Rate: 2.98828e-05
	LOSS [training: 0.2419267599668804 | validation: 0.23975673285312454]
	TIME [epoch: 8.54 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24925687602109847		[learning rate: 2.9774e-05]
	Learning Rate: 2.97743e-05
	LOSS [training: 0.24925687602109847 | validation: 0.23416281436558434]
	TIME [epoch: 8.54 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24387095994645874		[learning rate: 2.9666e-05]
	Learning Rate: 2.96663e-05
	LOSS [training: 0.24387095994645874 | validation: 0.23481496509154673]
	TIME [epoch: 8.55 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24867430273100224		[learning rate: 2.9559e-05]
	Learning Rate: 2.95586e-05
	LOSS [training: 0.24867430273100224 | validation: 0.2360110314550774]
	TIME [epoch: 8.53 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24791946718023455		[learning rate: 2.9451e-05]
	Learning Rate: 2.94514e-05
	LOSS [training: 0.24791946718023455 | validation: 0.22865515845290582]
	TIME [epoch: 8.54 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25016940596564136		[learning rate: 2.9344e-05]
	Learning Rate: 2.93445e-05
	LOSS [training: 0.25016940596564136 | validation: 0.23011365803442504]
	TIME [epoch: 8.53 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2439037258688082		[learning rate: 2.9238e-05]
	Learning Rate: 2.9238e-05
	LOSS [training: 0.2439037258688082 | validation: 0.25085752592859023]
	TIME [epoch: 8.54 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2506265954731918		[learning rate: 2.9132e-05]
	Learning Rate: 2.91319e-05
	LOSS [training: 0.2506265954731918 | validation: 0.2409995339855257]
	TIME [epoch: 8.53 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24208188969326533		[learning rate: 2.9026e-05]
	Learning Rate: 2.90262e-05
	LOSS [training: 0.24208188969326533 | validation: 0.24316975617512476]
	TIME [epoch: 8.54 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.245112785117029		[learning rate: 2.8921e-05]
	Learning Rate: 2.89208e-05
	LOSS [training: 0.245112785117029 | validation: 0.23153673179120338]
	TIME [epoch: 8.55 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24418894749762457		[learning rate: 2.8816e-05]
	Learning Rate: 2.88159e-05
	LOSS [training: 0.24418894749762457 | validation: 0.24393272258219711]
	TIME [epoch: 8.52 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24534687965340893		[learning rate: 2.8711e-05]
	Learning Rate: 2.87113e-05
	LOSS [training: 0.24534687965340893 | validation: 0.24914636771221332]
	TIME [epoch: 8.53 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24492530255194378		[learning rate: 2.8607e-05]
	Learning Rate: 2.86071e-05
	LOSS [training: 0.24492530255194378 | validation: 0.24141990524952878]
	TIME [epoch: 8.54 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2430383332058475		[learning rate: 2.8503e-05]
	Learning Rate: 2.85033e-05
	LOSS [training: 0.2430383332058475 | validation: 0.2484723734213115]
	TIME [epoch: 8.55 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2418753993807187		[learning rate: 2.84e-05]
	Learning Rate: 2.83998e-05
	LOSS [training: 0.2418753993807187 | validation: 0.2485707480417239]
	TIME [epoch: 8.53 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24322387896022796		[learning rate: 2.8297e-05]
	Learning Rate: 2.82968e-05
	LOSS [training: 0.24322387896022796 | validation: 0.24969095331597763]
	TIME [epoch: 8.53 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2367569668657999		[learning rate: 2.8194e-05]
	Learning Rate: 2.81941e-05
	LOSS [training: 0.2367569668657999 | validation: 0.23686679674865688]
	TIME [epoch: 8.54 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2429845566067575		[learning rate: 2.8092e-05]
	Learning Rate: 2.80918e-05
	LOSS [training: 0.2429845566067575 | validation: 0.21739362400223394]
	TIME [epoch: 8.55 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2418541358684362		[learning rate: 2.799e-05]
	Learning Rate: 2.79898e-05
	LOSS [training: 0.2418541358684362 | validation: 0.22870780042112376]
	TIME [epoch: 8.53 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24306704324794232		[learning rate: 2.7888e-05]
	Learning Rate: 2.78882e-05
	LOSS [training: 0.24306704324794232 | validation: 0.22855112011806816]
	TIME [epoch: 8.53 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24949593375260468		[learning rate: 2.7787e-05]
	Learning Rate: 2.7787e-05
	LOSS [training: 0.24949593375260468 | validation: 0.22185236187980062]
	TIME [epoch: 8.54 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24750969501877723		[learning rate: 2.7686e-05]
	Learning Rate: 2.76862e-05
	LOSS [training: 0.24750969501877723 | validation: 0.24365905983684888]
	TIME [epoch: 8.54 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24602067131024685		[learning rate: 2.7586e-05]
	Learning Rate: 2.75857e-05
	LOSS [training: 0.24602067131024685 | validation: 0.23511855674338725]
	TIME [epoch: 8.53 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2524155488469232		[learning rate: 2.7486e-05]
	Learning Rate: 2.74856e-05
	LOSS [training: 0.2524155488469232 | validation: 0.23553443454312467]
	TIME [epoch: 8.54 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2450519960871361		[learning rate: 2.7386e-05]
	Learning Rate: 2.73859e-05
	LOSS [training: 0.2450519960871361 | validation: 0.22957968803522955]
	TIME [epoch: 8.52 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25122155992210504		[learning rate: 2.7286e-05]
	Learning Rate: 2.72865e-05
	LOSS [training: 0.25122155992210504 | validation: 0.2323586597925011]
	TIME [epoch: 8.55 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2494334761373203		[learning rate: 2.7187e-05]
	Learning Rate: 2.71875e-05
	LOSS [training: 0.2494334761373203 | validation: 0.23163833459254501]
	TIME [epoch: 8.53 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24145695430369588		[learning rate: 2.7089e-05]
	Learning Rate: 2.70888e-05
	LOSS [training: 0.24145695430369588 | validation: 0.23482212813307285]
	TIME [epoch: 8.54 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24184735027489848		[learning rate: 2.699e-05]
	Learning Rate: 2.69905e-05
	LOSS [training: 0.24184735027489848 | validation: 0.24632156026127838]
	TIME [epoch: 8.53 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24229298331123		[learning rate: 2.6893e-05]
	Learning Rate: 2.68925e-05
	LOSS [training: 0.24229298331123 | validation: 0.22936019412472838]
	TIME [epoch: 8.55 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2379635668646089		[learning rate: 2.6795e-05]
	Learning Rate: 2.67949e-05
	LOSS [training: 0.2379635668646089 | validation: 0.2288251356295672]
	TIME [epoch: 8.54 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24003204390629582		[learning rate: 2.6698e-05]
	Learning Rate: 2.66977e-05
	LOSS [training: 0.24003204390629582 | validation: 0.2314638002013994]
	TIME [epoch: 8.54 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25221177755581403		[learning rate: 2.6601e-05]
	Learning Rate: 2.66008e-05
	LOSS [training: 0.25221177755581403 | validation: 0.23837868507857832]
	TIME [epoch: 8.52 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24778310253400543		[learning rate: 2.6504e-05]
	Learning Rate: 2.65043e-05
	LOSS [training: 0.24778310253400543 | validation: 0.23045096051293049]
	TIME [epoch: 8.55 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24561301894522117		[learning rate: 2.6408e-05]
	Learning Rate: 2.64081e-05
	LOSS [training: 0.24561301894522117 | validation: 0.23572284898109447]
	TIME [epoch: 8.54 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25341884647736335		[learning rate: 2.6312e-05]
	Learning Rate: 2.63123e-05
	LOSS [training: 0.25341884647736335 | validation: 0.2310967672962525]
	TIME [epoch: 8.53 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2507273907492082		[learning rate: 2.6217e-05]
	Learning Rate: 2.62168e-05
	LOSS [training: 0.2507273907492082 | validation: 0.2425158640751936]
	TIME [epoch: 8.54 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24471795118624368		[learning rate: 2.6122e-05]
	Learning Rate: 2.61216e-05
	LOSS [training: 0.24471795118624368 | validation: 0.2434537386314254]
	TIME [epoch: 8.53 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2512287918574157		[learning rate: 2.6027e-05]
	Learning Rate: 2.60268e-05
	LOSS [training: 0.2512287918574157 | validation: 0.24576997055281904]
	TIME [epoch: 8.55 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24563305710367475		[learning rate: 2.5932e-05]
	Learning Rate: 2.59324e-05
	LOSS [training: 0.24563305710367475 | validation: 0.24752319242519194]
	TIME [epoch: 8.53 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25034960691267216		[learning rate: 2.5838e-05]
	Learning Rate: 2.58383e-05
	LOSS [training: 0.25034960691267216 | validation: 0.24410411657378278]
	TIME [epoch: 8.55 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24535218162852793		[learning rate: 2.5744e-05]
	Learning Rate: 2.57445e-05
	LOSS [training: 0.24535218162852793 | validation: 0.23419681501991557]
	TIME [epoch: 8.53 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2519885361051687		[learning rate: 2.5651e-05]
	Learning Rate: 2.56511e-05
	LOSS [training: 0.2519885361051687 | validation: 0.24129344997823154]
	TIME [epoch: 8.55 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25297661977207975		[learning rate: 2.5558e-05]
	Learning Rate: 2.5558e-05
	LOSS [training: 0.25297661977207975 | validation: 0.23585303332303176]
	TIME [epoch: 8.53 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25140173988527537		[learning rate: 2.5465e-05]
	Learning Rate: 2.54652e-05
	LOSS [training: 0.25140173988527537 | validation: 0.2372382420379666]
	TIME [epoch: 8.54 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2512627062793882		[learning rate: 2.5373e-05]
	Learning Rate: 2.53728e-05
	LOSS [training: 0.2512627062793882 | validation: 0.242985780970703]
	TIME [epoch: 8.53 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24680240501559753		[learning rate: 2.5281e-05]
	Learning Rate: 2.52807e-05
	LOSS [training: 0.24680240501559753 | validation: 0.2442374263729395]
	TIME [epoch: 8.54 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2528186229177657		[learning rate: 2.5189e-05]
	Learning Rate: 2.5189e-05
	LOSS [training: 0.2528186229177657 | validation: 0.24336090552256606]
	TIME [epoch: 8.53 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24321708499822753		[learning rate: 2.5098e-05]
	Learning Rate: 2.50976e-05
	LOSS [training: 0.24321708499822753 | validation: 0.24158594170526684]
	TIME [epoch: 8.54 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24445496669033734		[learning rate: 2.5006e-05]
	Learning Rate: 2.50065e-05
	LOSS [training: 0.24445496669033734 | validation: 0.2421121648985352]
	TIME [epoch: 8.53 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2399396401778256		[learning rate: 2.4916e-05]
	Learning Rate: 2.49157e-05
	LOSS [training: 0.2399396401778256 | validation: 0.2286909849313753]
	TIME [epoch: 8.54 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24611198597196898		[learning rate: 2.4825e-05]
	Learning Rate: 2.48253e-05
	LOSS [training: 0.24611198597196898 | validation: 0.2403383027938942]
	TIME [epoch: 8.53 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23953230445673618		[learning rate: 2.4735e-05]
	Learning Rate: 2.47352e-05
	LOSS [training: 0.23953230445673618 | validation: 0.2309073424603039]
	TIME [epoch: 8.54 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24014388117428115		[learning rate: 2.4645e-05]
	Learning Rate: 2.46455e-05
	LOSS [training: 0.24014388117428115 | validation: 0.22693253989112844]
	TIME [epoch: 8.54 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24256976899265922		[learning rate: 2.4556e-05]
	Learning Rate: 2.4556e-05
	LOSS [training: 0.24256976899265922 | validation: 0.2317963518638332]
	TIME [epoch: 8.54 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2478024456636163		[learning rate: 2.4467e-05]
	Learning Rate: 2.44669e-05
	LOSS [training: 0.2478024456636163 | validation: 0.24544832402049074]
	TIME [epoch: 8.53 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24341811116313306		[learning rate: 2.4378e-05]
	Learning Rate: 2.43781e-05
	LOSS [training: 0.24341811116313306 | validation: 0.23212228108707372]
	TIME [epoch: 8.55 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24455863089699034		[learning rate: 2.429e-05]
	Learning Rate: 2.42896e-05
	LOSS [training: 0.24455863089699034 | validation: 0.2366836434084368]
	TIME [epoch: 8.55 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2427025500843294		[learning rate: 2.4201e-05]
	Learning Rate: 2.42015e-05
	LOSS [training: 0.2427025500843294 | validation: 0.24757951399751627]
	TIME [epoch: 8.53 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24511558704306763		[learning rate: 2.4114e-05]
	Learning Rate: 2.41137e-05
	LOSS [training: 0.24511558704306763 | validation: 0.2526929195969748]
	TIME [epoch: 8.53 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23890061796327897		[learning rate: 2.4026e-05]
	Learning Rate: 2.40262e-05
	LOSS [training: 0.23890061796327897 | validation: 0.24390735196316632]
	TIME [epoch: 8.55 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24904151265992053		[learning rate: 2.3939e-05]
	Learning Rate: 2.3939e-05
	LOSS [training: 0.24904151265992053 | validation: 0.24157734962074476]
	TIME [epoch: 8.55 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24785506949184452		[learning rate: 2.3852e-05]
	Learning Rate: 2.38521e-05
	LOSS [training: 0.24785506949184452 | validation: 0.2323295062786677]
	TIME [epoch: 8.53 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24440632495155334		[learning rate: 2.3766e-05]
	Learning Rate: 2.37655e-05
	LOSS [training: 0.24440632495155334 | validation: 0.23325347666991164]
	TIME [epoch: 8.53 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24454783319290524		[learning rate: 2.3679e-05]
	Learning Rate: 2.36793e-05
	LOSS [training: 0.24454783319290524 | validation: 0.23116359540508508]
	TIME [epoch: 8.55 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24256663921172555		[learning rate: 2.3593e-05]
	Learning Rate: 2.35933e-05
	LOSS [training: 0.24256663921172555 | validation: 0.242283825190933]
	TIME [epoch: 8.55 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24621403239222656		[learning rate: 2.3508e-05]
	Learning Rate: 2.35077e-05
	LOSS [training: 0.24621403239222656 | validation: 0.24339122797637708]
	TIME [epoch: 8.53 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24261950725557266		[learning rate: 2.3422e-05]
	Learning Rate: 2.34224e-05
	LOSS [training: 0.24261950725557266 | validation: 0.2397202321950562]
	TIME [epoch: 8.55 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2437318712277964		[learning rate: 2.3337e-05]
	Learning Rate: 2.33374e-05
	LOSS [training: 0.2437318712277964 | validation: 0.22915413844212934]
	TIME [epoch: 8.55 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2485274171688748		[learning rate: 2.3253e-05]
	Learning Rate: 2.32527e-05
	LOSS [training: 0.2485274171688748 | validation: 0.2337492176146354]
	TIME [epoch: 8.54 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2428932316986463		[learning rate: 2.3168e-05]
	Learning Rate: 2.31683e-05
	LOSS [training: 0.2428932316986463 | validation: 0.22967984752654333]
	TIME [epoch: 8.52 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2463074020681602		[learning rate: 2.3084e-05]
	Learning Rate: 2.30843e-05
	LOSS [training: 0.2463074020681602 | validation: 0.2446527296078497]
	TIME [epoch: 8.54 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24312003980057756		[learning rate: 2.3e-05]
	Learning Rate: 2.30005e-05
	LOSS [training: 0.24312003980057756 | validation: 0.22862055022165623]
	TIME [epoch: 8.55 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24097139180385208		[learning rate: 2.2917e-05]
	Learning Rate: 2.2917e-05
	LOSS [training: 0.24097139180385208 | validation: 0.24492397933882698]
	TIME [epoch: 8.54 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24457996290844738		[learning rate: 2.2834e-05]
	Learning Rate: 2.28338e-05
	LOSS [training: 0.24457996290844738 | validation: 0.2361625010161752]
	TIME [epoch: 8.53 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24580610544971254		[learning rate: 2.2751e-05]
	Learning Rate: 2.2751e-05
	LOSS [training: 0.24580610544971254 | validation: 0.22994461919138312]
	TIME [epoch: 8.54 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24349526913738856		[learning rate: 2.2668e-05]
	Learning Rate: 2.26684e-05
	LOSS [training: 0.24349526913738856 | validation: 0.22515119837700576]
	TIME [epoch: 8.55 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2421619315778749		[learning rate: 2.2586e-05]
	Learning Rate: 2.25862e-05
	LOSS [training: 0.2421619315778749 | validation: 0.23116962635557764]
	TIME [epoch: 8.53 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24612898652092294		[learning rate: 2.2504e-05]
	Learning Rate: 2.25042e-05
	LOSS [training: 0.24612898652092294 | validation: 0.24301897337758793]
	TIME [epoch: 8.53 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2436281160896357		[learning rate: 2.2423e-05]
	Learning Rate: 2.24225e-05
	LOSS [training: 0.2436281160896357 | validation: 0.22833083546025268]
	TIME [epoch: 8.55 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2434426409637548		[learning rate: 2.2341e-05]
	Learning Rate: 2.23411e-05
	LOSS [training: 0.2434426409637548 | validation: 0.23971330624989934]
	TIME [epoch: 8.55 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24657226057199938		[learning rate: 2.226e-05]
	Learning Rate: 2.22601e-05
	LOSS [training: 0.24657226057199938 | validation: 0.2289352106623106]
	TIME [epoch: 8.53 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24348507403929465		[learning rate: 2.2179e-05]
	Learning Rate: 2.21793e-05
	LOSS [training: 0.24348507403929465 | validation: 0.23685647926068862]
	TIME [epoch: 8.53 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24399990835435412		[learning rate: 2.2099e-05]
	Learning Rate: 2.20988e-05
	LOSS [training: 0.24399990835435412 | validation: 0.23942033197029794]
	TIME [epoch: 8.56 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24949814612850751		[learning rate: 2.2019e-05]
	Learning Rate: 2.20186e-05
	LOSS [training: 0.24949814612850751 | validation: 0.2394907860884019]
	TIME [epoch: 8.54 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24710679281532527		[learning rate: 2.1939e-05]
	Learning Rate: 2.19387e-05
	LOSS [training: 0.24710679281532527 | validation: 0.23461045408140593]
	TIME [epoch: 8.53 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25078985717052005		[learning rate: 2.1859e-05]
	Learning Rate: 2.18591e-05
	LOSS [training: 0.25078985717052005 | validation: 0.2373774948797524]
	TIME [epoch: 8.53 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24792427174732407		[learning rate: 2.178e-05]
	Learning Rate: 2.17797e-05
	LOSS [training: 0.24792427174732407 | validation: 0.2502453612183302]
	TIME [epoch: 8.56 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24095553546214338		[learning rate: 2.1701e-05]
	Learning Rate: 2.17007e-05
	LOSS [training: 0.24095553546214338 | validation: 0.23953712193712132]
	TIME [epoch: 8.54 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2440038178222753		[learning rate: 2.1622e-05]
	Learning Rate: 2.16219e-05
	LOSS [training: 0.2440038178222753 | validation: 0.22698910016782212]
	TIME [epoch: 8.53 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2402296701739317		[learning rate: 2.1543e-05]
	Learning Rate: 2.15435e-05
	LOSS [training: 0.2402296701739317 | validation: 0.22946250003797758]
	TIME [epoch: 8.53 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24042570975701696		[learning rate: 2.1465e-05]
	Learning Rate: 2.14653e-05
	LOSS [training: 0.24042570975701696 | validation: 0.2362721322899253]
	TIME [epoch: 8.57 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24183052523321846		[learning rate: 2.1387e-05]
	Learning Rate: 2.13874e-05
	LOSS [training: 0.24183052523321846 | validation: 0.23586490478371902]
	TIME [epoch: 8.54 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24164713642696672		[learning rate: 2.131e-05]
	Learning Rate: 2.13098e-05
	LOSS [training: 0.24164713642696672 | validation: 0.2378379036324241]
	TIME [epoch: 8.53 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.240724691393289		[learning rate: 2.1232e-05]
	Learning Rate: 2.12325e-05
	LOSS [training: 0.240724691393289 | validation: 0.2294858831415204]
	TIME [epoch: 8.55 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2474298840884431		[learning rate: 2.1155e-05]
	Learning Rate: 2.11554e-05
	LOSS [training: 0.2474298840884431 | validation: 0.22457425368792097]
	TIME [epoch: 8.56 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24580325745240367		[learning rate: 2.1079e-05]
	Learning Rate: 2.10786e-05
	LOSS [training: 0.24580325745240367 | validation: 0.24277698738415915]
	TIME [epoch: 8.53 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24409580169206904		[learning rate: 2.1002e-05]
	Learning Rate: 2.10021e-05
	LOSS [training: 0.24409580169206904 | validation: 0.22674688865865036]
	TIME [epoch: 8.53 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24362441421246567		[learning rate: 2.0926e-05]
	Learning Rate: 2.09259e-05
	LOSS [training: 0.24362441421246567 | validation: 0.22269009095926834]
	TIME [epoch: 8.56 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24528490419428067		[learning rate: 2.085e-05]
	Learning Rate: 2.085e-05
	LOSS [training: 0.24528490419428067 | validation: 0.22882666584549627]
	TIME [epoch: 8.55 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24336899860118963		[learning rate: 2.0774e-05]
	Learning Rate: 2.07743e-05
	LOSS [training: 0.24336899860118963 | validation: 0.23774086803336542]
	TIME [epoch: 8.53 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24861843758337904		[learning rate: 2.0699e-05]
	Learning Rate: 2.06989e-05
	LOSS [training: 0.24861843758337904 | validation: 0.23565709093306608]
	TIME [epoch: 8.53 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24207967319834872		[learning rate: 2.0624e-05]
	Learning Rate: 2.06238e-05
	LOSS [training: 0.24207967319834872 | validation: 0.22893364060083002]
	TIME [epoch: 8.56 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24511395829938962		[learning rate: 2.0549e-05]
	Learning Rate: 2.05489e-05
	LOSS [training: 0.24511395829938962 | validation: 0.24016764653166867]
	TIME [epoch: 8.54 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24643269667687645		[learning rate: 2.0474e-05]
	Learning Rate: 2.04744e-05
	LOSS [training: 0.24643269667687645 | validation: 0.2404683569540942]
	TIME [epoch: 8.53 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.240267797948193		[learning rate: 2.04e-05]
	Learning Rate: 2.04001e-05
	LOSS [training: 0.240267797948193 | validation: 0.229369062829022]
	TIME [epoch: 8.53 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24477420582723047		[learning rate: 2.0326e-05]
	Learning Rate: 2.0326e-05
	LOSS [training: 0.24477420582723047 | validation: 0.22310317073348057]
	TIME [epoch: 8.56 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24543779196869067		[learning rate: 2.0252e-05]
	Learning Rate: 2.02523e-05
	LOSS [training: 0.24543779196869067 | validation: 0.22055785302376923]
	TIME [epoch: 8.53 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2417759841737705		[learning rate: 2.0179e-05]
	Learning Rate: 2.01788e-05
	LOSS [training: 0.2417759841737705 | validation: 0.22364816455728326]
	TIME [epoch: 8.52 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.247322097081104		[learning rate: 2.0106e-05]
	Learning Rate: 2.01055e-05
	LOSS [training: 0.247322097081104 | validation: 0.23262857413290303]
	TIME [epoch: 8.53 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2400984763329696		[learning rate: 2.0033e-05]
	Learning Rate: 2.00326e-05
	LOSS [training: 0.2400984763329696 | validation: 0.23877366671927291]
	TIME [epoch: 8.57 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2435925761074175		[learning rate: 1.996e-05]
	Learning Rate: 1.99599e-05
	LOSS [training: 0.2435925761074175 | validation: 0.2405612527904944]
	TIME [epoch: 8.53 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2419362605077799		[learning rate: 1.9887e-05]
	Learning Rate: 1.98874e-05
	LOSS [training: 0.2419362605077799 | validation: 0.22927729551894716]
	TIME [epoch: 8.53 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2409068439838053		[learning rate: 1.9815e-05]
	Learning Rate: 1.98153e-05
	LOSS [training: 0.2409068439838053 | validation: 0.23004671612250457]
	TIME [epoch: 8.53 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24445622060684738		[learning rate: 1.9743e-05]
	Learning Rate: 1.97434e-05
	LOSS [training: 0.24445622060684738 | validation: 0.23798202279481723]
	TIME [epoch: 8.56 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24364779746540477		[learning rate: 1.9672e-05]
	Learning Rate: 1.96717e-05
	LOSS [training: 0.24364779746540477 | validation: 0.21532995004572464]
	TIME [epoch: 8.53 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24106655817992334		[learning rate: 1.96e-05]
	Learning Rate: 1.96003e-05
	LOSS [training: 0.24106655817992334 | validation: 0.23742792966056614]
	TIME [epoch: 8.53 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24560019359977098		[learning rate: 1.9529e-05]
	Learning Rate: 1.95292e-05
	LOSS [training: 0.24560019359977098 | validation: 0.24491016946464347]
	TIME [epoch: 8.54 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23949290558835065		[learning rate: 1.9458e-05]
	Learning Rate: 1.94583e-05
	LOSS [training: 0.23949290558835065 | validation: 0.23319809153561252]
	TIME [epoch: 8.55 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23710597416431978		[learning rate: 1.9388e-05]
	Learning Rate: 1.93877e-05
	LOSS [training: 0.23710597416431978 | validation: 0.22724594934425965]
	TIME [epoch: 8.53 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24428112935569893		[learning rate: 1.9317e-05]
	Learning Rate: 1.93173e-05
	LOSS [training: 0.24428112935569893 | validation: 0.2215579203423984]
	TIME [epoch: 8.53 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24302424315758472		[learning rate: 1.9247e-05]
	Learning Rate: 1.92472e-05
	LOSS [training: 0.24302424315758472 | validation: 0.2293707360745195]
	TIME [epoch: 8.55 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2449205818537644		[learning rate: 1.9177e-05]
	Learning Rate: 1.91774e-05
	LOSS [training: 0.2449205818537644 | validation: 0.22886962059122146]
	TIME [epoch: 8.55 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24425485314234083		[learning rate: 1.9108e-05]
	Learning Rate: 1.91078e-05
	LOSS [training: 0.24425485314234083 | validation: 0.23515883473303859]
	TIME [epoch: 8.53 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24197980811934577		[learning rate: 1.9038e-05]
	Learning Rate: 1.90384e-05
	LOSS [training: 0.24197980811934577 | validation: 0.23760453880437093]
	TIME [epoch: 8.53 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24624851332116543		[learning rate: 1.8969e-05]
	Learning Rate: 1.89694e-05
	LOSS [training: 0.24624851332116543 | validation: 0.23328467202689968]
	TIME [epoch: 8.56 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23941063197465356		[learning rate: 1.8901e-05]
	Learning Rate: 1.89005e-05
	LOSS [training: 0.23941063197465356 | validation: 0.22743830828961886]
	TIME [epoch: 8.54 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24017826895983033		[learning rate: 1.8832e-05]
	Learning Rate: 1.88319e-05
	LOSS [training: 0.24017826895983033 | validation: 0.22281079664157477]
	TIME [epoch: 8.53 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23304655107481703		[learning rate: 1.8764e-05]
	Learning Rate: 1.87636e-05
	LOSS [training: 0.23304655107481703 | validation: 0.23521522971016576]
	TIME [epoch: 8.53 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.25005191540527366		[learning rate: 1.8695e-05]
	Learning Rate: 1.86955e-05
	LOSS [training: 0.25005191540527366 | validation: 0.2252341802294879]
	TIME [epoch: 8.57 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2479122129509331		[learning rate: 1.8628e-05]
	Learning Rate: 1.86276e-05
	LOSS [training: 0.2479122129509331 | validation: 0.2158549083655304]
	TIME [epoch: 8.53 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2379346195280323		[learning rate: 1.856e-05]
	Learning Rate: 1.856e-05
	LOSS [training: 0.2379346195280323 | validation: 0.23316018949946457]
	TIME [epoch: 8.53 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23940735100883476		[learning rate: 1.8493e-05]
	Learning Rate: 1.84927e-05
	LOSS [training: 0.23940735100883476 | validation: 0.23888536194930726]
	TIME [epoch: 8.53 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2411185937537018		[learning rate: 1.8426e-05]
	Learning Rate: 1.84256e-05
	LOSS [training: 0.2411185937537018 | validation: 0.23516106334971393]
	TIME [epoch: 8.56 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24787165123181487		[learning rate: 1.8359e-05]
	Learning Rate: 1.83587e-05
	LOSS [training: 0.24787165123181487 | validation: 0.23194872375486986]
	TIME [epoch: 8.53 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23679184749686147		[learning rate: 1.8292e-05]
	Learning Rate: 1.82921e-05
	LOSS [training: 0.23679184749686147 | validation: 0.21820366474179417]
	TIME [epoch: 8.53 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2408100707912672		[learning rate: 1.8226e-05]
	Learning Rate: 1.82257e-05
	LOSS [training: 0.2408100707912672 | validation: 0.22931640345704257]
	TIME [epoch: 8.53 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24320747518483438		[learning rate: 1.816e-05]
	Learning Rate: 1.81596e-05
	LOSS [training: 0.24320747518483438 | validation: 0.23305338066063727]
	TIME [epoch: 8.55 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23935149096826022		[learning rate: 1.8094e-05]
	Learning Rate: 1.80937e-05
	LOSS [training: 0.23935149096826022 | validation: 0.22385573658680283]
	TIME [epoch: 8.53 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2441845850237095		[learning rate: 1.8028e-05]
	Learning Rate: 1.8028e-05
	LOSS [training: 0.2441845850237095 | validation: 0.23080507244858317]
	TIME [epoch: 8.52 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24002827498934715		[learning rate: 1.7963e-05]
	Learning Rate: 1.79626e-05
	LOSS [training: 0.24002827498934715 | validation: 0.22426140348205287]
	TIME [epoch: 8.53 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23564553633351176		[learning rate: 1.7897e-05]
	Learning Rate: 1.78974e-05
	LOSS [training: 0.23564553633351176 | validation: 0.2160284760430985]
	TIME [epoch: 8.56 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24336276989074496		[learning rate: 1.7832e-05]
	Learning Rate: 1.78324e-05
	LOSS [training: 0.24336276989074496 | validation: 0.22709991997104662]
	TIME [epoch: 8.53 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24552338661009268		[learning rate: 1.7768e-05]
	Learning Rate: 1.77677e-05
	LOSS [training: 0.24552338661009268 | validation: 0.23190444467685367]
	TIME [epoch: 8.52 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24392842152703137		[learning rate: 1.7703e-05]
	Learning Rate: 1.77032e-05
	LOSS [training: 0.24392842152703137 | validation: 0.23835385873559978]
	TIME [epoch: 8.54 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24793966662703038		[learning rate: 1.7639e-05]
	Learning Rate: 1.7639e-05
	LOSS [training: 0.24793966662703038 | validation: 0.22229449943527277]
	TIME [epoch: 8.54 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24270643959629598		[learning rate: 1.7575e-05]
	Learning Rate: 1.7575e-05
	LOSS [training: 0.24270643959629598 | validation: 0.2215571585975361]
	TIME [epoch: 8.53 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24275825295975273		[learning rate: 1.7511e-05]
	Learning Rate: 1.75112e-05
	LOSS [training: 0.24275825295975273 | validation: 0.23801258900734212]
	TIME [epoch: 8.52 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24634077758084133		[learning rate: 1.7448e-05]
	Learning Rate: 1.74476e-05
	LOSS [training: 0.24634077758084133 | validation: 0.23126478405135611]
	TIME [epoch: 8.55 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2426366453323973		[learning rate: 1.7384e-05]
	Learning Rate: 1.73843e-05
	LOSS [training: 0.2426366453323973 | validation: 0.23806644433477187]
	TIME [epoch: 8.55 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24115637982874696		[learning rate: 1.7321e-05]
	Learning Rate: 1.73212e-05
	LOSS [training: 0.24115637982874696 | validation: 0.21989631552996639]
	TIME [epoch: 8.52 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23979479006410936		[learning rate: 1.7258e-05]
	Learning Rate: 1.72584e-05
	LOSS [training: 0.23979479006410936 | validation: 0.23884316437042413]
	TIME [epoch: 8.53 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.245391609415831		[learning rate: 1.7196e-05]
	Learning Rate: 1.71957e-05
	LOSS [training: 0.245391609415831 | validation: 0.22941496602157083]
	TIME [epoch: 8.55 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24151813627976879		[learning rate: 1.7133e-05]
	Learning Rate: 1.71333e-05
	LOSS [training: 0.24151813627976879 | validation: 0.22429589992282828]
	TIME [epoch: 8.54 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2489729831580577		[learning rate: 1.7071e-05]
	Learning Rate: 1.70712e-05
	LOSS [training: 0.2489729831580577 | validation: 0.22444077008210947]
	TIME [epoch: 8.52 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24386109305710407		[learning rate: 1.7009e-05]
	Learning Rate: 1.70092e-05
	LOSS [training: 0.24386109305710407 | validation: 0.22785422474185543]
	TIME [epoch: 8.54 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24989783584242464		[learning rate: 1.6947e-05]
	Learning Rate: 1.69475e-05
	LOSS [training: 0.24989783584242464 | validation: 0.23708742470524002]
	TIME [epoch: 8.55 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2399274784055872		[learning rate: 1.6886e-05]
	Learning Rate: 1.6886e-05
	LOSS [training: 0.2399274784055872 | validation: 0.21687619638201036]
	TIME [epoch: 8.53 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24177171169429582		[learning rate: 1.6825e-05]
	Learning Rate: 1.68247e-05
	LOSS [training: 0.24177171169429582 | validation: 0.2141229161918313]
	TIME [epoch: 8.53 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24427562702425792		[learning rate: 1.6764e-05]
	Learning Rate: 1.67636e-05
	LOSS [training: 0.24427562702425792 | validation: 0.2253598946456977]
	TIME [epoch: 8.54 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23807956466724273		[learning rate: 1.6703e-05]
	Learning Rate: 1.67028e-05
	LOSS [training: 0.23807956466724273 | validation: 0.2317841812523012]
	TIME [epoch: 8.55 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23816851421551927		[learning rate: 1.6642e-05]
	Learning Rate: 1.66422e-05
	LOSS [training: 0.23816851421551927 | validation: 0.23204545264834806]
	TIME [epoch: 8.52 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2365824145748233		[learning rate: 1.6582e-05]
	Learning Rate: 1.65818e-05
	LOSS [training: 0.2365824145748233 | validation: 0.22363205281067128]
	TIME [epoch: 8.53 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23880559988062325		[learning rate: 1.6522e-05]
	Learning Rate: 1.65216e-05
	LOSS [training: 0.23880559988062325 | validation: 0.2169049266711139]
	TIME [epoch: 8.54 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24632643901503976		[learning rate: 1.6462e-05]
	Learning Rate: 1.64617e-05
	LOSS [training: 0.24632643901503976 | validation: 0.2278331104520592]
	TIME [epoch: 8.55 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2375125688680301		[learning rate: 1.6402e-05]
	Learning Rate: 1.64019e-05
	LOSS [training: 0.2375125688680301 | validation: 0.2351271269507953]
	TIME [epoch: 8.53 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2406978032232731		[learning rate: 1.6342e-05]
	Learning Rate: 1.63424e-05
	LOSS [training: 0.2406978032232731 | validation: 0.22767876003613324]
	TIME [epoch: 8.53 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24220845758411552		[learning rate: 1.6283e-05]
	Learning Rate: 1.62831e-05
	LOSS [training: 0.24220845758411552 | validation: 0.23638233838726175]
	TIME [epoch: 8.54 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23718901867048023		[learning rate: 1.6224e-05]
	Learning Rate: 1.6224e-05
	LOSS [training: 0.23718901867048023 | validation: 0.22638494670400372]
	TIME [epoch: 8.55 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2370994666665398		[learning rate: 1.6165e-05]
	Learning Rate: 1.61651e-05
	LOSS [training: 0.2370994666665398 | validation: 0.23226948709928905]
	TIME [epoch: 8.53 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2490321278519881		[learning rate: 1.6106e-05]
	Learning Rate: 1.61065e-05
	LOSS [training: 0.2490321278519881 | validation: 0.21997834747612013]
	TIME [epoch: 8.53 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2485597225419502		[learning rate: 1.6048e-05]
	Learning Rate: 1.6048e-05
	LOSS [training: 0.2485597225419502 | validation: 0.22322626111778862]
	TIME [epoch: 8.54 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24049700188877415		[learning rate: 1.599e-05]
	Learning Rate: 1.59898e-05
	LOSS [training: 0.24049700188877415 | validation: 0.2283229476303898]
	TIME [epoch: 8.55 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24211200778867434		[learning rate: 1.5932e-05]
	Learning Rate: 1.59317e-05
	LOSS [training: 0.24211200778867434 | validation: 0.2276237234676294]
	TIME [epoch: 8.54 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24216414037374068		[learning rate: 1.5874e-05]
	Learning Rate: 1.58739e-05
	LOSS [training: 0.24216414037374068 | validation: 0.2233977110901806]
	TIME [epoch: 8.54 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23932336691920533		[learning rate: 1.5816e-05]
	Learning Rate: 1.58163e-05
	LOSS [training: 0.23932336691920533 | validation: 0.2226158260798749]
	TIME [epoch: 8.54 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2407618161406337		[learning rate: 1.5759e-05]
	Learning Rate: 1.57589e-05
	LOSS [training: 0.2407618161406337 | validation: 0.2216765991577339]
	TIME [epoch: 8.54 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24887771235530254		[learning rate: 1.5702e-05]
	Learning Rate: 1.57017e-05
	LOSS [training: 0.24887771235530254 | validation: 0.21669015864812582]
	TIME [epoch: 8.53 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2406314471999989		[learning rate: 1.5645e-05]
	Learning Rate: 1.56447e-05
	LOSS [training: 0.2406314471999989 | validation: 0.22805261255761566]
	TIME [epoch: 8.54 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24043471043891645		[learning rate: 1.5588e-05]
	Learning Rate: 1.5588e-05
	LOSS [training: 0.24043471043891645 | validation: 0.23090801743863687]
	TIME [epoch: 8.54 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24599323984257593		[learning rate: 1.5531e-05]
	Learning Rate: 1.55314e-05
	LOSS [training: 0.24599323984257593 | validation: 0.22648085132232174]
	TIME [epoch: 8.55 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24683667559475916		[learning rate: 1.5475e-05]
	Learning Rate: 1.5475e-05
	LOSS [training: 0.24683667559475916 | validation: 0.21998408151556648]
	TIME [epoch: 8.53 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24215644069542602		[learning rate: 1.5419e-05]
	Learning Rate: 1.54189e-05
	LOSS [training: 0.24215644069542602 | validation: 0.22348382906836714]
	TIME [epoch: 8.54 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24506480567498207		[learning rate: 1.5363e-05]
	Learning Rate: 1.53629e-05
	LOSS [training: 0.24506480567498207 | validation: 0.23343959630339833]
	TIME [epoch: 8.54 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24595090831937244		[learning rate: 1.5307e-05]
	Learning Rate: 1.53072e-05
	LOSS [training: 0.24595090831937244 | validation: 0.22336241986860672]
	TIME [epoch: 8.53 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23665958125423203		[learning rate: 1.5252e-05]
	Learning Rate: 1.52516e-05
	LOSS [training: 0.23665958125423203 | validation: 0.2192928752131556]
	TIME [epoch: 8.52 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24337695112733906		[learning rate: 1.5196e-05]
	Learning Rate: 1.51963e-05
	LOSS [training: 0.24337695112733906 | validation: 0.23277478594818984]
	TIME [epoch: 8.54 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2418876469768152		[learning rate: 1.5141e-05]
	Learning Rate: 1.51411e-05
	LOSS [training: 0.2418876469768152 | validation: 0.2173418537055052]
	TIME [epoch: 8.56 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24183240845246617		[learning rate: 1.5086e-05]
	Learning Rate: 1.50862e-05
	LOSS [training: 0.24183240845246617 | validation: 0.23416785763690656]
	TIME [epoch: 8.53 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24285824162885988		[learning rate: 1.5031e-05]
	Learning Rate: 1.50314e-05
	LOSS [training: 0.24285824162885988 | validation: 0.22036619498201887]
	TIME [epoch: 8.52 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24272111928166779		[learning rate: 1.4977e-05]
	Learning Rate: 1.49769e-05
	LOSS [training: 0.24272111928166779 | validation: 0.23050447855613637]
	TIME [epoch: 8.54 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24484013191863774		[learning rate: 1.4923e-05]
	Learning Rate: 1.49225e-05
	LOSS [training: 0.24484013191863774 | validation: 0.218555166634524]
	TIME [epoch: 8.55 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2429158466446959		[learning rate: 1.4868e-05]
	Learning Rate: 1.48684e-05
	LOSS [training: 0.2429158466446959 | validation: 0.2204752124218368]
	TIME [epoch: 8.52 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24613636635412917		[learning rate: 1.4814e-05]
	Learning Rate: 1.48144e-05
	LOSS [training: 0.24613636635412917 | validation: 0.21892873290419743]
	TIME [epoch: 8.53 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24567976011832343		[learning rate: 1.4761e-05]
	Learning Rate: 1.47606e-05
	LOSS [training: 0.24567976011832343 | validation: 0.229614462495823]
	TIME [epoch: 8.55 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2521344070709324		[learning rate: 1.4707e-05]
	Learning Rate: 1.47071e-05
	LOSS [training: 0.2521344070709324 | validation: 0.2142298128994745]
	TIME [epoch: 8.54 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23741172339303077		[learning rate: 1.4654e-05]
	Learning Rate: 1.46537e-05
	LOSS [training: 0.23741172339303077 | validation: 0.22308111138982523]
	TIME [epoch: 8.53 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24385340435807162		[learning rate: 1.4601e-05]
	Learning Rate: 1.46005e-05
	LOSS [training: 0.24385340435807162 | validation: 0.2196994528620663]
	TIME [epoch: 8.53 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24183869461534185		[learning rate: 1.4548e-05]
	Learning Rate: 1.45475e-05
	LOSS [training: 0.24183869461534185 | validation: 0.23622570996274034]
	TIME [epoch: 8.54 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24099444442017087		[learning rate: 1.4495e-05]
	Learning Rate: 1.44947e-05
	LOSS [training: 0.24099444442017087 | validation: 0.22165722108803082]
	TIME [epoch: 8.54 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2446799665057963		[learning rate: 1.4442e-05]
	Learning Rate: 1.44421e-05
	LOSS [training: 0.2446799665057963 | validation: 0.23316677687192935]
	TIME [epoch: 8.53 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24582854525333248		[learning rate: 1.439e-05]
	Learning Rate: 1.43897e-05
	LOSS [training: 0.24582854525333248 | validation: 0.22353999773969135]
	TIME [epoch: 8.53 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24407942985361836		[learning rate: 1.4338e-05]
	Learning Rate: 1.43375e-05
	LOSS [training: 0.24407942985361836 | validation: 0.2198260921025372]
	TIME [epoch: 8.54 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24424129589151797		[learning rate: 1.4285e-05]
	Learning Rate: 1.42855e-05
	LOSS [training: 0.24424129589151797 | validation: 0.24185548860157294]
	TIME [epoch: 8.54 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24154944937881498		[learning rate: 1.4234e-05]
	Learning Rate: 1.42336e-05
	LOSS [training: 0.24154944937881498 | validation: 0.23156916798635435]
	TIME [epoch: 8.52 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2412504749895145		[learning rate: 1.4182e-05]
	Learning Rate: 1.4182e-05
	LOSS [training: 0.2412504749895145 | validation: 0.23467171595173228]
	TIME [epoch: 8.54 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2411331279039775		[learning rate: 1.4131e-05]
	Learning Rate: 1.41305e-05
	LOSS [training: 0.2411331279039775 | validation: 0.22498884041994416]
	TIME [epoch: 8.53 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24589238588299794		[learning rate: 1.4079e-05]
	Learning Rate: 1.40792e-05
	LOSS [training: 0.24589238588299794 | validation: 0.22182133782447272]
	TIME [epoch: 8.55 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24241079031782506		[learning rate: 1.4028e-05]
	Learning Rate: 1.40281e-05
	LOSS [training: 0.24241079031782506 | validation: 0.23451429383755973]
	TIME [epoch: 8.52 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24214106160938434		[learning rate: 1.3977e-05]
	Learning Rate: 1.39772e-05
	LOSS [training: 0.24214106160938434 | validation: 0.22699987008966707]
	TIME [epoch: 8.54 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24236293392854885		[learning rate: 1.3927e-05]
	Learning Rate: 1.39265e-05
	LOSS [training: 0.24236293392854885 | validation: 0.21813910483566482]
	TIME [epoch: 8.52 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23806015536279662		[learning rate: 1.3876e-05]
	Learning Rate: 1.3876e-05
	LOSS [training: 0.23806015536279662 | validation: 0.23928966930159598]
	TIME [epoch: 8.54 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23970999050275993		[learning rate: 1.3826e-05]
	Learning Rate: 1.38256e-05
	LOSS [training: 0.23970999050275993 | validation: 0.21557824034640943]
	TIME [epoch: 8.52 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24183004205550723		[learning rate: 1.3775e-05]
	Learning Rate: 1.37754e-05
	LOSS [training: 0.24183004205550723 | validation: 0.2289377649887409]
	TIME [epoch: 8.54 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24567876589481347		[learning rate: 1.3725e-05]
	Learning Rate: 1.37254e-05
	LOSS [training: 0.24567876589481347 | validation: 0.22930576956900584]
	TIME [epoch: 8.54 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24259172478571425		[learning rate: 1.3676e-05]
	Learning Rate: 1.36756e-05
	LOSS [training: 0.24259172478571425 | validation: 0.23202628203944348]
	TIME [epoch: 8.54 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24714595311158924		[learning rate: 1.3626e-05]
	Learning Rate: 1.3626e-05
	LOSS [training: 0.24714595311158924 | validation: 0.21567795259496492]
	TIME [epoch: 8.53 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24091922990457215		[learning rate: 1.3577e-05]
	Learning Rate: 1.35766e-05
	LOSS [training: 0.24091922990457215 | validation: 0.23849803280827206]
	TIME [epoch: 8.55 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2433725706310265		[learning rate: 1.3527e-05]
	Learning Rate: 1.35273e-05
	LOSS [training: 0.2433725706310265 | validation: 0.225799015289107]
	TIME [epoch: 8.55 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23979593391226856		[learning rate: 1.3478e-05]
	Learning Rate: 1.34782e-05
	LOSS [training: 0.23979593391226856 | validation: 0.22974641595249118]
	TIME [epoch: 8.53 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2426558842084668		[learning rate: 1.3429e-05]
	Learning Rate: 1.34293e-05
	LOSS [training: 0.2426558842084668 | validation: 0.24256460505572186]
	TIME [epoch: 8.53 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23918691168361667		[learning rate: 1.3381e-05]
	Learning Rate: 1.33805e-05
	LOSS [training: 0.23918691168361667 | validation: 0.2286087558548884]
	TIME [epoch: 8.54 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24176663035835935		[learning rate: 1.3332e-05]
	Learning Rate: 1.3332e-05
	LOSS [training: 0.24176663035835935 | validation: 0.2342346266118197]
	TIME [epoch: 8.55 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24291047422711026		[learning rate: 1.3284e-05]
	Learning Rate: 1.32836e-05
	LOSS [training: 0.24291047422711026 | validation: 0.22405970075832754]
	TIME [epoch: 8.53 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23859705193606556		[learning rate: 1.3235e-05]
	Learning Rate: 1.32354e-05
	LOSS [training: 0.23859705193606556 | validation: 0.22784865168302867]
	TIME [epoch: 8.53 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23780561943676734		[learning rate: 1.3187e-05]
	Learning Rate: 1.31874e-05
	LOSS [training: 0.23780561943676734 | validation: 0.2250053488591982]
	TIME [epoch: 8.53 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2379782170458192		[learning rate: 1.314e-05]
	Learning Rate: 1.31395e-05
	LOSS [training: 0.2379782170458192 | validation: 0.2336393883518167]
	TIME [epoch: 8.55 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24681731290207942		[learning rate: 1.3092e-05]
	Learning Rate: 1.30918e-05
	LOSS [training: 0.24681731290207942 | validation: 0.22932006646161623]
	TIME [epoch: 8.53 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24230681804532977		[learning rate: 1.3044e-05]
	Learning Rate: 1.30443e-05
	LOSS [training: 0.24230681804532977 | validation: 0.22565065005678225]
	TIME [epoch: 8.54 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23788308983714357		[learning rate: 1.2997e-05]
	Learning Rate: 1.2997e-05
	LOSS [training: 0.23788308983714357 | validation: 0.22888344027548552]
	TIME [epoch: 8.53 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24513162394669155		[learning rate: 1.295e-05]
	Learning Rate: 1.29498e-05
	LOSS [training: 0.24513162394669155 | validation: 0.21668983759031477]
	TIME [epoch: 8.54 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24189573579806325		[learning rate: 1.2903e-05]
	Learning Rate: 1.29028e-05
	LOSS [training: 0.24189573579806325 | validation: 0.22230772189204967]
	TIME [epoch: 8.53 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24294531289039814		[learning rate: 1.2856e-05]
	Learning Rate: 1.2856e-05
	LOSS [training: 0.24294531289039814 | validation: 0.22366512650618575]
	TIME [epoch: 8.54 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2428916717891178		[learning rate: 1.2809e-05]
	Learning Rate: 1.28093e-05
	LOSS [training: 0.2428916717891178 | validation: 0.2206527256882225]
	TIME [epoch: 8.52 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23985701736846893		[learning rate: 1.2763e-05]
	Learning Rate: 1.27628e-05
	LOSS [training: 0.23985701736846893 | validation: 0.22288405123114108]
	TIME [epoch: 8.55 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24489158671963432		[learning rate: 1.2717e-05]
	Learning Rate: 1.27165e-05
	LOSS [training: 0.24489158671963432 | validation: 0.22309221271791593]
	TIME [epoch: 8.53 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23864591458265325		[learning rate: 1.267e-05]
	Learning Rate: 1.26704e-05
	LOSS [training: 0.23864591458265325 | validation: 0.2429727095734574]
	TIME [epoch: 8.54 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2388192004845741		[learning rate: 1.2624e-05]
	Learning Rate: 1.26244e-05
	LOSS [training: 0.2388192004845741 | validation: 0.23872870219613668]
	TIME [epoch: 8.52 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23973228691917972		[learning rate: 1.2579e-05]
	Learning Rate: 1.25786e-05
	LOSS [training: 0.23973228691917972 | validation: 0.22168052755137826]
	TIME [epoch: 8.54 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2496216909099506		[learning rate: 1.2533e-05]
	Learning Rate: 1.25329e-05
	LOSS [training: 0.2496216909099506 | validation: 0.2192961619010126]
	TIME [epoch: 8.54 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2456287263672574		[learning rate: 1.2487e-05]
	Learning Rate: 1.24874e-05
	LOSS [training: 0.2456287263672574 | validation: 0.22220593326272914]
	TIME [epoch: 8.53 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24281385750614515		[learning rate: 1.2442e-05]
	Learning Rate: 1.24421e-05
	LOSS [training: 0.24281385750614515 | validation: 0.24217530734268405]
	TIME [epoch: 8.53 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24224155710801512		[learning rate: 1.2397e-05]
	Learning Rate: 1.2397e-05
	LOSS [training: 0.24224155710801512 | validation: 0.23452061327973356]
	TIME [epoch: 8.54 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24303006681133338		[learning rate: 1.2352e-05]
	Learning Rate: 1.2352e-05
	LOSS [training: 0.24303006681133338 | validation: 0.23919319592080182]
	TIME [epoch: 8.54 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24275674678057507		[learning rate: 1.2307e-05]
	Learning Rate: 1.23072e-05
	LOSS [training: 0.24275674678057507 | validation: 0.23319875015981273]
	TIME [epoch: 8.53 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24268815395440263		[learning rate: 1.2263e-05]
	Learning Rate: 1.22625e-05
	LOSS [training: 0.24268815395440263 | validation: 0.22318028953525454]
	TIME [epoch: 8.54 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24308301574779848		[learning rate: 1.2218e-05]
	Learning Rate: 1.2218e-05
	LOSS [training: 0.24308301574779848 | validation: 0.23105032130315237]
	TIME [epoch: 8.52 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24585909458410718		[learning rate: 1.2174e-05]
	Learning Rate: 1.21737e-05
	LOSS [training: 0.24585909458410718 | validation: 0.22799592013957898]
	TIME [epoch: 8.54 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2443757500379624		[learning rate: 1.2129e-05]
	Learning Rate: 1.21295e-05
	LOSS [training: 0.2443757500379624 | validation: 0.23246442971517295]
	TIME [epoch: 8.52 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2424427055032738		[learning rate: 1.2085e-05]
	Learning Rate: 1.20855e-05
	LOSS [training: 0.2424427055032738 | validation: 0.23473517840336067]
	TIME [epoch: 8.54 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24137138715630863		[learning rate: 1.2042e-05]
	Learning Rate: 1.20416e-05
	LOSS [training: 0.24137138715630863 | validation: 0.22388173181849613]
	TIME [epoch: 8.53 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24518751845861467		[learning rate: 1.1998e-05]
	Learning Rate: 1.19979e-05
	LOSS [training: 0.24518751845861467 | validation: 0.23342162384179238]
	TIME [epoch: 8.55 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24341359503592783		[learning rate: 1.1954e-05]
	Learning Rate: 1.19544e-05
	LOSS [training: 0.24341359503592783 | validation: 0.2279438654388981]
	TIME [epoch: 8.52 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24260345473589523		[learning rate: 1.1911e-05]
	Learning Rate: 1.1911e-05
	LOSS [training: 0.24260345473589523 | validation: 0.22981286083971042]
	TIME [epoch: 8.54 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23955835500812128		[learning rate: 1.1868e-05]
	Learning Rate: 1.18678e-05
	LOSS [training: 0.23955835500812128 | validation: 0.23893284952760074]
	TIME [epoch: 8.53 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23773957131175746		[learning rate: 1.1825e-05]
	Learning Rate: 1.18247e-05
	LOSS [training: 0.23773957131175746 | validation: 0.2269594343619954]
	TIME [epoch: 8.54 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24437386053417565		[learning rate: 1.1782e-05]
	Learning Rate: 1.17818e-05
	LOSS [training: 0.24437386053417565 | validation: 0.23263454082902674]
	TIME [epoch: 8.53 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24028584924492685		[learning rate: 1.1739e-05]
	Learning Rate: 1.1739e-05
	LOSS [training: 0.24028584924492685 | validation: 0.21921014074679565]
	TIME [epoch: 8.55 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2419982103624682		[learning rate: 1.1696e-05]
	Learning Rate: 1.16964e-05
	LOSS [training: 0.2419982103624682 | validation: 0.23746071787579953]
	TIME [epoch: 8.54 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.237826222803278		[learning rate: 1.1654e-05]
	Learning Rate: 1.1654e-05
	LOSS [training: 0.237826222803278 | validation: 0.2359807519890549]
	TIME [epoch: 8.54 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24634327466256742		[learning rate: 1.1612e-05]
	Learning Rate: 1.16117e-05
	LOSS [training: 0.24634327466256742 | validation: 0.2208420236108609]
	TIME [epoch: 8.52 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2424884295532964		[learning rate: 1.157e-05]
	Learning Rate: 1.15695e-05
	LOSS [training: 0.2424884295532964 | validation: 0.2283837387028499]
	TIME [epoch: 8.55 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23537520526255165		[learning rate: 1.1528e-05]
	Learning Rate: 1.15275e-05
	LOSS [training: 0.23537520526255165 | validation: 0.23069978076809616]
	TIME [epoch: 8.54 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23782704888596046		[learning rate: 1.1486e-05]
	Learning Rate: 1.14857e-05
	LOSS [training: 0.23782704888596046 | validation: 0.21569424057577982]
	TIME [epoch: 8.54 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24405672062619216		[learning rate: 1.1444e-05]
	Learning Rate: 1.1444e-05
	LOSS [training: 0.24405672062619216 | validation: 0.21057382043987136]
	TIME [epoch: 8.53 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r2_20240219_203401/states/model_tr_study204_1963.pth
	Model improved!!!
EPOCH 1964/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24601996000022316		[learning rate: 1.1402e-05]
	Learning Rate: 1.14025e-05
	LOSS [training: 0.24601996000022316 | validation: 0.22760981067350283]
	TIME [epoch: 8.55 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23776790354837923		[learning rate: 1.1361e-05]
	Learning Rate: 1.13611e-05
	LOSS [training: 0.23776790354837923 | validation: 0.2300921301165927]
	TIME [epoch: 8.54 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24002295402444912		[learning rate: 1.132e-05]
	Learning Rate: 1.13199e-05
	LOSS [training: 0.24002295402444912 | validation: 0.22581369490013595]
	TIME [epoch: 8.53 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24166051676410277		[learning rate: 1.1279e-05]
	Learning Rate: 1.12788e-05
	LOSS [training: 0.24166051676410277 | validation: 0.23298605139626738]
	TIME [epoch: 8.54 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24328501962836704		[learning rate: 1.1238e-05]
	Learning Rate: 1.12379e-05
	LOSS [training: 0.24328501962836704 | validation: 0.22351805089051832]
	TIME [epoch: 8.54 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.244356068212137		[learning rate: 1.1197e-05]
	Learning Rate: 1.11971e-05
	LOSS [training: 0.244356068212137 | validation: 0.23884985686578442]
	TIME [epoch: 8.54 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24328652359783334		[learning rate: 1.1156e-05]
	Learning Rate: 1.11565e-05
	LOSS [training: 0.24328652359783334 | validation: 0.23631177411517068]
	TIME [epoch: 8.53 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24002525025510554		[learning rate: 1.1116e-05]
	Learning Rate: 1.1116e-05
	LOSS [training: 0.24002525025510554 | validation: 0.2286132712070733]
	TIME [epoch: 8.54 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24103739026644896		[learning rate: 1.1076e-05]
	Learning Rate: 1.10756e-05
	LOSS [training: 0.24103739026644896 | validation: 0.21270056610817356]
	TIME [epoch: 8.54 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24160976035726392		[learning rate: 1.1035e-05]
	Learning Rate: 1.10354e-05
	LOSS [training: 0.24160976035726392 | validation: 0.2248258672393315]
	TIME [epoch: 8.54 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23944045534582656		[learning rate: 1.0995e-05]
	Learning Rate: 1.09954e-05
	LOSS [training: 0.23944045534582656 | validation: 0.2320929547596523]
	TIME [epoch: 8.53 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24557057306499122		[learning rate: 1.0955e-05]
	Learning Rate: 1.09555e-05
	LOSS [training: 0.24557057306499122 | validation: 0.2347208635208249]
	TIME [epoch: 8.54 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24021039038288716		[learning rate: 1.0916e-05]
	Learning Rate: 1.09157e-05
	LOSS [training: 0.24021039038288716 | validation: 0.2295649071216866]
	TIME [epoch: 8.54 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24411737467964092		[learning rate: 1.0876e-05]
	Learning Rate: 1.08761e-05
	LOSS [training: 0.24411737467964092 | validation: 0.22363782738314592]
	TIME [epoch: 8.54 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24420965102991218		[learning rate: 1.0837e-05]
	Learning Rate: 1.08366e-05
	LOSS [training: 0.24420965102991218 | validation: 0.22573655115835944]
	TIME [epoch: 8.53 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2490758818534772		[learning rate: 1.0797e-05]
	Learning Rate: 1.07973e-05
	LOSS [training: 0.2490758818534772 | validation: 0.22401730931626296]
	TIME [epoch: 8.54 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23614553992311088		[learning rate: 1.0758e-05]
	Learning Rate: 1.07581e-05
	LOSS [training: 0.23614553992311088 | validation: 0.2233268261047263]
	TIME [epoch: 8.55 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2411089904762335		[learning rate: 1.0719e-05]
	Learning Rate: 1.07191e-05
	LOSS [training: 0.2411089904762335 | validation: 0.22792817290650016]
	TIME [epoch: 8.53 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23920188816730867		[learning rate: 1.068e-05]
	Learning Rate: 1.06802e-05
	LOSS [training: 0.23920188816730867 | validation: 0.2286428086559542]
	TIME [epoch: 8.53 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24125785698671373		[learning rate: 1.0641e-05]
	Learning Rate: 1.06414e-05
	LOSS [training: 0.24125785698671373 | validation: 0.22548178300959182]
	TIME [epoch: 8.56 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23757323737877178		[learning rate: 1.0603e-05]
	Learning Rate: 1.06028e-05
	LOSS [training: 0.23757323737877178 | validation: 0.21899748634785235]
	TIME [epoch: 8.55 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23945894555761912		[learning rate: 1.0564e-05]
	Learning Rate: 1.05643e-05
	LOSS [training: 0.23945894555761912 | validation: 0.2273672549442748]
	TIME [epoch: 8.53 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23805071996508795		[learning rate: 1.0526e-05]
	Learning Rate: 1.0526e-05
	LOSS [training: 0.23805071996508795 | validation: 0.23254650240701621]
	TIME [epoch: 8.53 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24641934050429753		[learning rate: 1.0488e-05]
	Learning Rate: 1.04878e-05
	LOSS [training: 0.24641934050429753 | validation: 0.22920282180319024]
	TIME [epoch: 8.56 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24394999501635875		[learning rate: 1.045e-05]
	Learning Rate: 1.04497e-05
	LOSS [training: 0.24394999501635875 | validation: 0.23151131671288863]
	TIME [epoch: 8.53 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24516153833323276		[learning rate: 1.0412e-05]
	Learning Rate: 1.04118e-05
	LOSS [training: 0.24516153833323276 | validation: 0.23186623185491795]
	TIME [epoch: 8.52 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2451851390264333		[learning rate: 1.0374e-05]
	Learning Rate: 1.0374e-05
	LOSS [training: 0.2451851390264333 | validation: 0.23896105754771926]
	TIME [epoch: 8.53 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23758408329026864		[learning rate: 1.0336e-05]
	Learning Rate: 1.03364e-05
	LOSS [training: 0.23758408329026864 | validation: 0.23056833961418988]
	TIME [epoch: 8.55 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23689389059035088		[learning rate: 1.0299e-05]
	Learning Rate: 1.02989e-05
	LOSS [training: 0.23689389059035088 | validation: 0.22669636382258898]
	TIME [epoch: 8.54 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2425223705417296		[learning rate: 1.0261e-05]
	Learning Rate: 1.02615e-05
	LOSS [training: 0.2425223705417296 | validation: 0.21883911340081216]
	TIME [epoch: 8.52 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2379525917801141		[learning rate: 1.0224e-05]
	Learning Rate: 1.02243e-05
	LOSS [training: 0.2379525917801141 | validation: 0.23919510049706627]
	TIME [epoch: 8.52 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24232958250117148		[learning rate: 1.0187e-05]
	Learning Rate: 1.01872e-05
	LOSS [training: 0.24232958250117148 | validation: 0.23794861768011177]
	TIME [epoch: 8.56 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.2405341966744055		[learning rate: 1.015e-05]
	Learning Rate: 1.01502e-05
	LOSS [training: 0.2405341966744055 | validation: 0.22256355438814956]
	TIME [epoch: 8.54 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24293490898802603		[learning rate: 1.0113e-05]
	Learning Rate: 1.01133e-05
	LOSS [training: 0.24293490898802603 | validation: 0.23581015956424356]
	TIME [epoch: 8.53 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.24264227780790232		[learning rate: 1.0077e-05]
	Learning Rate: 1.00766e-05
	LOSS [training: 0.24264227780790232 | validation: 0.24074551475109862]
	TIME [epoch: 8.54 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23927296525892539		[learning rate: 1.004e-05]
	Learning Rate: 1.00401e-05
	LOSS [training: 0.23927296525892539 | validation: 0.24147398972276773]
	TIME [epoch: 8.55 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/10] avg loss: 0.23785268215745897		[learning rate: 1.0004e-05]
	Learning Rate: 1.00036e-05
	LOSS [training: 0.23785268215745897 | validation: 0.2290203700964335]
	TIME [epoch: 8.53 sec]
Finished training in 17246.405 seconds.
