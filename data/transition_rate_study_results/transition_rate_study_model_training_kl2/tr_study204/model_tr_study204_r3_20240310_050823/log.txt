Args:
Namespace(name='model_tr_study204', outdir='out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3', training_data='data/transition_rate_studies/tr_study204/tr_study204_training/r3', validation_data='data/transition_rate_studies/tr_study204/tr_study204_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1058647527

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.984168747485558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.984168747485558 | validation: 11.651505264672338]
	TIME [epoch: 99.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.956084637930434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.956084637930434 | validation: 11.851667149961418]
	TIME [epoch: 11.6 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.036268313626252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.036268313626252 | validation: 9.371294057131063]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.112305974129004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.112305974129004 | validation: 10.982506773094755]
	TIME [epoch: 11.5 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.479872434554347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.479872434554347 | validation: 10.615210572266003]
	TIME [epoch: 11.5 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.830240705507503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.830240705507503 | validation: 7.596853465524847]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.7334362809169725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.7334362809169725 | validation: 7.978731208704025]
	TIME [epoch: 11.5 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.303850816212609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.303850816212609 | validation: 7.18295103632786]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.820641904555202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.820641904555202 | validation: 6.755429295418757]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.689642046681169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.689642046681169 | validation: 5.574837740747557]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.140616161345491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.140616161345491 | validation: 5.715940706871333]
	TIME [epoch: 11.6 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.082873993454857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.082873993454857 | validation: 6.346953789326147]
	TIME [epoch: 11.5 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.959729464027527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.959729464027527 | validation: 5.393376908902193]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.732233604581248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.732233604581248 | validation: 5.731762341875688]
	TIME [epoch: 11.6 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.947033151375153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.947033151375153 | validation: 5.482771222893473]
	TIME [epoch: 11.5 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.632172863981641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.632172863981641 | validation: 5.362111516416133]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.966177907020962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.966177907020962 | validation: 5.424323914605281]
	TIME [epoch: 11.6 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.714762505199051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.714762505199051 | validation: 5.1086374032268]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.871215081734158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.871215081734158 | validation: 5.8265318053645005]
	TIME [epoch: 11.5 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.729570460573573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.729570460573573 | validation: 5.526166457091995]
	TIME [epoch: 11.6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.750796590300894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.750796590300894 | validation: 5.251887307320374]
	TIME [epoch: 11.5 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.566708815679391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.566708815679391 | validation: 6.432618768770131]
	TIME [epoch: 11.5 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.9903707284160665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9903707284160665 | validation: 5.044744327890564]
	TIME [epoch: 11.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.457425921818534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.457425921818534 | validation: 5.299346228147875]
	TIME [epoch: 11.5 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.634329781858222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.634329781858222 | validation: 5.146339581964087]
	TIME [epoch: 11.5 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5770426699097575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5770426699097575 | validation: 5.19696533734799]
	TIME [epoch: 11.6 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.311814653392792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.311814653392792 | validation: 6.022126864099694]
	TIME [epoch: 11.5 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.919625336216484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.919625336216484 | validation: 5.173181662850856]
	TIME [epoch: 11.5 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.5169693122655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5169693122655 | validation: 5.095698467302174]
	TIME [epoch: 11.5 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.727192533810223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.727192533810223 | validation: 6.086172924340865]
	TIME [epoch: 11.5 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.877295429227036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.877295429227036 | validation: 5.132921773587169]
	TIME [epoch: 11.5 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.999869123120908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.999869123120908 | validation: 5.626944635865773]
	TIME [epoch: 11.6 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.908674731813457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.908674731813457 | validation: 5.020187035365176]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.390633781746643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.390633781746643 | validation: 4.982542527061114]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.550852591996624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.550852591996624 | validation: 5.388530645018473]
	TIME [epoch: 11.5 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.260940381421741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.260940381421741 | validation: 5.129682623896465]
	TIME [epoch: 11.5 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.247772973817899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.247772973817899 | validation: 4.918896439617281]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.729508237501286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.729508237501286 | validation: 4.8647771837584495]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.905619575707516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.905619575707516 | validation: 4.974587333745884]
	TIME [epoch: 11.5 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.3469187212397475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3469187212397475 | validation: 4.896389312619923]
	TIME [epoch: 11.5 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.344871853568552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.344871853568552 | validation: 4.938043042139419]
	TIME [epoch: 11.5 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.206409539291495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.206409539291495 | validation: 5.363187768186658]
	TIME [epoch: 11.5 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.744237493993398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.744237493993398 | validation: 6.573404425486681]
	TIME [epoch: 11.5 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.696813834415199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.696813834415199 | validation: 5.8834207048033385]
	TIME [epoch: 11.5 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.661144521522452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.661144521522452 | validation: 5.352172294428344]
	TIME [epoch: 11.5 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.241862230990076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.241862230990076 | validation: 6.064673150049702]
	TIME [epoch: 11.5 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.536961976278344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.536961976278344 | validation: 4.9221399137306685]
	TIME [epoch: 11.5 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.163354693342365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163354693342365 | validation: 5.132159381647307]
	TIME [epoch: 11.5 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.094004561894914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.094004561894914 | validation: 5.062115278909856]
	TIME [epoch: 11.5 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1599844100048955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1599844100048955 | validation: 4.677927522462438]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.224897248660304		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 4.224897248660304 | validation: 5.026267776512505]
	TIME [epoch: 11.6 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.264374933822755		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 4.264374933822755 | validation: 5.162719623244709]
	TIME [epoch: 11.5 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.07891111378109		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 4.07891111378109 | validation: 4.816430243072553]
	TIME [epoch: 11.5 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.071526647935918		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 4.071526647935918 | validation: 5.041178789882881]
	TIME [epoch: 11.6 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.123974655271053		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 4.123974655271053 | validation: 5.538385989649354]
	TIME [epoch: 11.5 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.314293243866245		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 4.314293243866245 | validation: 4.905757992234344]
	TIME [epoch: 11.5 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.0892855579732075		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 4.0892855579732075 | validation: 5.332895981210649]
	TIME [epoch: 11.6 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.125100067015444		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 4.125100067015444 | validation: 5.012267107105429]
	TIME [epoch: 11.5 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.952963042253863		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.952963042253863 | validation: 4.899787953508292]
	TIME [epoch: 11.5 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.195659845746947		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 4.195659845746947 | validation: 4.8738150256707184]
	TIME [epoch: 11.5 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8537733605545155		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.8537733605545155 | validation: 5.245248218765873]
	TIME [epoch: 11.5 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.963057952243738		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.963057952243738 | validation: 4.868635528365741]
	TIME [epoch: 11.5 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8610731186557397		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.8610731186557397 | validation: 4.900153525500416]
	TIME [epoch: 11.5 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.467225418835879		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 4.467225418835879 | validation: 4.549189728029253]
	TIME [epoch: 11.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8749399337030384		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 3.8749399337030384 | validation: 5.005613721042769]
	TIME [epoch: 11.5 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.039253391238483		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 4.039253391238483 | validation: 4.691530923812535]
	TIME [epoch: 11.5 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8973357625525957		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 3.8973357625525957 | validation: 4.804611949553074]
	TIME [epoch: 11.5 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980208714299037		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.980208714299037 | validation: 4.899401468831843]
	TIME [epoch: 11.5 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7852042272799085		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 3.7852042272799085 | validation: 5.334841596583917]
	TIME [epoch: 11.5 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.993063642563581		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.993063642563581 | validation: 4.5799531475922555]
	TIME [epoch: 11.5 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.491358712446187		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 4.491358712446187 | validation: 4.624227701648517]
	TIME [epoch: 11.5 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9180660838951207		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.9180660838951207 | validation: 4.591393904943232]
	TIME [epoch: 11.5 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7586716934093647		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 3.7586716934093647 | validation: 4.769095272272068]
	TIME [epoch: 11.5 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9229826241289616		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.9229826241289616 | validation: 5.026076704412904]
	TIME [epoch: 11.5 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.980059667826902		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 3.980059667826902 | validation: 7.600050614712504]
	TIME [epoch: 11.5 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.841056934322112		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 4.841056934322112 | validation: 5.651212367988364]
	TIME [epoch: 11.5 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2589983191754595		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 4.2589983191754595 | validation: 5.064872164474653]
	TIME [epoch: 11.5 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.872718850305528		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 3.872718850305528 | validation: 4.903230807261523]
	TIME [epoch: 11.5 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.926077685097362		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 3.926077685097362 | validation: 4.628077482867403]
	TIME [epoch: 11.5 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.088682357295303		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 4.088682357295303 | validation: 4.571557094547776]
	TIME [epoch: 11.5 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.268865084874948		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 4.268865084874948 | validation: 4.723623777749749]
	TIME [epoch: 11.5 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.750615820863909		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.750615820863909 | validation: 5.316916557778334]
	TIME [epoch: 11.5 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.075001852378474		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 4.075001852378474 | validation: 4.651441517640844]
	TIME [epoch: 11.5 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.888571874857283		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.888571874857283 | validation: 4.773235422774231]
	TIME [epoch: 11.5 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.929096678284121		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 3.929096678284121 | validation: 5.26701043233629]
	TIME [epoch: 11.5 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9745096724494466		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.9745096724494466 | validation: 4.640412297406904]
	TIME [epoch: 11.5 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.861073161129811		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.861073161129811 | validation: 4.539333568404216]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8674940901075106		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 3.8674940901075106 | validation: 4.577875611307255]
	TIME [epoch: 11.5 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8313129102120103		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 3.8313129102120103 | validation: 4.576840271663884]
	TIME [epoch: 11.5 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.894878814335792		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 3.894878814335792 | validation: 4.575165039972107]
	TIME [epoch: 11.5 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8311091369771155		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 3.8311091369771155 | validation: 4.900504465408671]
	TIME [epoch: 11.5 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.775354407571939		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 3.775354407571939 | validation: 4.514160759088165]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.103395181729304		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 4.103395181729304 | validation: 4.51102380040129]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.622492642855972		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 3.622492642855972 | validation: 4.673515976538611]
	TIME [epoch: 11.5 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.839437808790821		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 3.839437808790821 | validation: 4.555754918348517]
	TIME [epoch: 11.5 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7470195741651597		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 3.7470195741651597 | validation: 4.514073103009912]
	TIME [epoch: 11.5 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7461731716684255		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 3.7461731716684255 | validation: 6.841121302755214]
	TIME [epoch: 11.5 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.619241453438455		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 4.619241453438455 | validation: 4.702682734050881]
	TIME [epoch: 11.5 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.907522624470535		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 3.907522624470535 | validation: 4.5574307696599545]
	TIME [epoch: 11.5 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7098304339447488		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 3.7098304339447488 | validation: 4.594131874261542]
	TIME [epoch: 11.5 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8151482634381257		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 3.8151482634381257 | validation: 4.56757003249502]
	TIME [epoch: 11.5 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.71475826059899		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 3.71475826059899 | validation: 4.432683224948444]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6455549388758093		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 3.6455549388758093 | validation: 4.5474537493078335]
	TIME [epoch: 11.5 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6965374480260254		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 3.6965374480260254 | validation: 4.487376258826008]
	TIME [epoch: 11.5 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.650235142516087		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 3.650235142516087 | validation: 4.830987470829049]
	TIME [epoch: 11.5 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.735704500432602		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 3.735704500432602 | validation: 4.639517331476495]
	TIME [epoch: 11.5 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.601846290592419		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 3.601846290592419 | validation: 5.042287440577899]
	TIME [epoch: 11.5 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8026144625858995		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.8026144625858995 | validation: 4.477693833238897]
	TIME [epoch: 11.5 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7960356484398057		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 3.7960356484398057 | validation: 4.393632759977148]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7106458619018636		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 3.7106458619018636 | validation: 4.9737969087181675]
	TIME [epoch: 11.5 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7553162485762943		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 3.7553162485762943 | validation: 4.506220134242865]
	TIME [epoch: 11.5 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.628680917804967		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 3.628680917804967 | validation: 4.492559868553618]
	TIME [epoch: 11.5 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.55935497295955		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 3.55935497295955 | validation: 4.864525650106868]
	TIME [epoch: 11.5 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.699024676747302		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 3.699024676747302 | validation: 4.408900939076379]
	TIME [epoch: 11.5 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6867832106906424		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 3.6867832106906424 | validation: 4.4642267173677554]
	TIME [epoch: 11.5 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6357497532556207		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 3.6357497532556207 | validation: 4.601263313820669]
	TIME [epoch: 11.5 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6218575313581622		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 3.6218575313581622 | validation: 4.873502269331188]
	TIME [epoch: 11.5 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.69561847083083		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 3.69561847083083 | validation: 4.7777467621926535]
	TIME [epoch: 11.5 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6664457466126033		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 3.6664457466126033 | validation: 4.69107504492905]
	TIME [epoch: 11.5 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.565273944887123		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 3.565273944887123 | validation: 4.623322220725116]
	TIME [epoch: 11.5 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6261699438885175		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 3.6261699438885175 | validation: 4.446414387818167]
	TIME [epoch: 11.5 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.574582314207612		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 3.574582314207612 | validation: 4.621792048027864]
	TIME [epoch: 11.5 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6737956731263273		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 3.6737956731263273 | validation: 4.362099345840119]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6258796753002764		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 3.6258796753002764 | validation: 4.859648305537093]
	TIME [epoch: 11.5 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6388679004233113		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 3.6388679004233113 | validation: 4.423972860720567]
	TIME [epoch: 11.5 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.545184510628026		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 3.545184510628026 | validation: 4.396187489789777]
	TIME [epoch: 11.5 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6587893944907504		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 3.6587893944907504 | validation: 5.093488912943022]
	TIME [epoch: 11.5 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6275798341877983		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 3.6275798341877983 | validation: 4.6068285577756285]
	TIME [epoch: 11.5 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.573061143130477		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 3.573061143130477 | validation: 4.588954633572741]
	TIME [epoch: 11.5 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6100160378509525		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 3.6100160378509525 | validation: 4.317163593100491]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5306969312372614		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 3.5306969312372614 | validation: 4.3356252125423795]
	TIME [epoch: 11.5 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5888825385166574		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 3.5888825385166574 | validation: 4.883668823296939]
	TIME [epoch: 11.5 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5952838352769088		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 3.5952838352769088 | validation: 4.9098483214704345]
	TIME [epoch: 11.5 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6973798735098002		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 3.6973798735098002 | validation: 4.314012185168094]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.2745482500615015		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 4.2745482500615015 | validation: 4.3762624169953295]
	TIME [epoch: 11.5 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4802257895680753		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 3.4802257895680753 | validation: 4.395003248138074]
	TIME [epoch: 11.5 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.890995087247126		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 3.890995087247126 | validation: 4.396152769963127]
	TIME [epoch: 11.5 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5487560842208774		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 3.5487560842208774 | validation: 4.643842031770942]
	TIME [epoch: 11.5 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.546401917467753		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 3.546401917467753 | validation: 5.32114716776586]
	TIME [epoch: 11.5 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.013385904388245		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 4.013385904388245 | validation: 5.03754566670143]
	TIME [epoch: 11.5 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.790092386907349		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 3.790092386907349 | validation: 4.809099495765425]
	TIME [epoch: 11.5 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6100779651454933		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 3.6100779651454933 | validation: 5.041872044439469]
	TIME [epoch: 11.5 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7466776928450014		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 3.7466776928450014 | validation: 4.578263016070845]
	TIME [epoch: 11.5 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5758740858602125		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 3.5758740858602125 | validation: 4.930814212099419]
	TIME [epoch: 11.5 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8940166037313784		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 3.8940166037313784 | validation: 4.499407342646816]
	TIME [epoch: 11.5 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6671126785607226		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 3.6671126785607226 | validation: 4.40273285779922]
	TIME [epoch: 11.5 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4871554051820954		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 3.4871554051820954 | validation: 4.488097864535253]
	TIME [epoch: 11.5 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.499684068992979		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 3.499684068992979 | validation: 4.637285049099754]
	TIME [epoch: 11.5 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.497664077610819		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 3.497664077610819 | validation: 4.654497540261172]
	TIME [epoch: 11.5 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8961733852238973		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 3.8961733852238973 | validation: 5.150416772523073]
	TIME [epoch: 11.5 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.871181821808896		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 3.871181821808896 | validation: 4.782696832406285]
	TIME [epoch: 11.5 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7368838686254793		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 3.7368838686254793 | validation: 4.630383433064764]
	TIME [epoch: 11.5 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5846080672606435		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 3.5846080672606435 | validation: 4.292241634078062]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4376948748115694		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 3.4376948748115694 | validation: 4.476457095200722]
	TIME [epoch: 11.5 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.520827876162395		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 3.520827876162395 | validation: 4.4441224647721205]
	TIME [epoch: 11.5 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5795738564261805		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 3.5795738564261805 | validation: 4.385736749694869]
	TIME [epoch: 11.5 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4123423662436982		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 3.4123423662436982 | validation: 4.376059588027516]
	TIME [epoch: 11.5 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.48536542351549		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 3.48536542351549 | validation: 4.582446463624319]
	TIME [epoch: 11.5 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.522114175867908		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 3.522114175867908 | validation: 4.390176483490193]
	TIME [epoch: 11.5 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4364514458824877		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 3.4364514458824877 | validation: 4.378686316507232]
	TIME [epoch: 11.5 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.558691994728722		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 3.558691994728722 | validation: 4.3134795642230355]
	TIME [epoch: 11.5 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.431307307217556		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 3.431307307217556 | validation: 4.517874474136815]
	TIME [epoch: 11.5 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6058750696910074		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 3.6058750696910074 | validation: 4.296740662336476]
	TIME [epoch: 11.6 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.532073660220709		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 3.532073660220709 | validation: 4.269542885442779]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.507283610474917		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 3.507283610474917 | validation: 4.2919267496027524]
	TIME [epoch: 11.5 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6103914474966343		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 3.6103914474966343 | validation: 4.2924629166683]
	TIME [epoch: 11.5 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.709834882425188		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 3.709834882425188 | validation: 4.510760673006937]
	TIME [epoch: 11.5 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6528633471141925		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 3.6528633471141925 | validation: 4.346233625777272]
	TIME [epoch: 11.5 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.452411428391007		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 3.452411428391007 | validation: 4.461051474577401]
	TIME [epoch: 11.5 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4030062506778114		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 3.4030062506778114 | validation: 4.285614203585794]
	TIME [epoch: 11.5 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.423868153487353		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 3.423868153487353 | validation: 4.342773307773384]
	TIME [epoch: 11.5 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5052791328784076		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 3.5052791328784076 | validation: 4.2540607926208684]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.395455576919546		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 3.395455576919546 | validation: 4.354951526811117]
	TIME [epoch: 11.5 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.437110588913299		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 3.437110588913299 | validation: 4.588100702794557]
	TIME [epoch: 11.5 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.644548576000198		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 3.644548576000198 | validation: 4.439954056147529]
	TIME [epoch: 11.5 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.463570302970271		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 3.463570302970271 | validation: 4.627245220826877]
	TIME [epoch: 11.5 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.526344821482553		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 3.526344821482553 | validation: 4.263088892484141]
	TIME [epoch: 11.5 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.448249363014738		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 3.448249363014738 | validation: 4.26223096574886]
	TIME [epoch: 11.5 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4009085875466627		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 3.4009085875466627 | validation: 4.359249621255003]
	TIME [epoch: 11.5 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4375086024058925		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 3.4375086024058925 | validation: 4.291318426999323]
	TIME [epoch: 11.5 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3781910588273694		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 3.3781910588273694 | validation: 4.432544312729989]
	TIME [epoch: 11.5 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.452921533020171		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 3.452921533020171 | validation: 4.430917184338235]
	TIME [epoch: 11.5 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3913939704758906		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 3.3913939704758906 | validation: 4.515344263333341]
	TIME [epoch: 11.5 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.378691580362949		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 3.378691580362949 | validation: 4.628451126091374]
	TIME [epoch: 11.5 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.558242208847907		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 3.558242208847907 | validation: 4.949116077864379]
	TIME [epoch: 11.5 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7935528923002266		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 3.7935528923002266 | validation: 4.411905081082836]
	TIME [epoch: 11.5 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.358429189793496		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 3.358429189793496 | validation: 4.320255440493104]
	TIME [epoch: 11.5 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4307621880529995		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 3.4307621880529995 | validation: 4.613401621911834]
	TIME [epoch: 11.5 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5504775236077255		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 3.5504775236077255 | validation: 4.652374887155015]
	TIME [epoch: 11.6 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4837495146683817		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 3.4837495146683817 | validation: 4.488756974672185]
	TIME [epoch: 11.5 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.529638334265356		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 3.529638334265356 | validation: 4.282471745974059]
	TIME [epoch: 11.5 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4040675075364715		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 3.4040675075364715 | validation: 4.3461027397720144]
	TIME [epoch: 11.5 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.383486366784175		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 3.383486366784175 | validation: 4.39578156623266]
	TIME [epoch: 11.5 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6133191078853013		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 3.6133191078853013 | validation: 4.312952540540493]
	TIME [epoch: 11.5 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3814255506128963		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 3.3814255506128963 | validation: 4.457628576500424]
	TIME [epoch: 11.5 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.410471157141317		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 3.410471157141317 | validation: 4.293980929701491]
	TIME [epoch: 11.5 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366322219989664		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 3.366322219989664 | validation: 4.319822028480675]
	TIME [epoch: 11.5 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3426601350644303		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 3.3426601350644303 | validation: 4.6084948826039405]
	TIME [epoch: 11.5 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5292395209929794		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 3.5292395209929794 | validation: 4.266344386253687]
	TIME [epoch: 11.5 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3401981945527055		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 3.3401981945527055 | validation: 4.278753550199475]
	TIME [epoch: 11.5 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4399061543026157		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 3.4399061543026157 | validation: 4.296053259437666]
	TIME [epoch: 11.5 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5428342858222317		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 3.5428342858222317 | validation: 4.312155511559836]
	TIME [epoch: 11.5 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.468105838879734		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 3.468105838879734 | validation: 4.268807981748956]
	TIME [epoch: 11.5 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.444822779026367		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 3.444822779026367 | validation: 4.336772227604157]
	TIME [epoch: 11.5 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.404490450894408		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 3.404490450894408 | validation: 4.351758442821713]
	TIME [epoch: 11.5 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.394316381205548		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 3.394316381205548 | validation: 4.460558088212627]
	TIME [epoch: 11.5 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.437598102421811		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 3.437598102421811 | validation: 4.441428642796838]
	TIME [epoch: 11.5 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.382837039614253		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 3.382837039614253 | validation: 4.426975615147674]
	TIME [epoch: 11.5 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.419673013793614		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 3.419673013793614 | validation: 4.484609561164064]
	TIME [epoch: 11.5 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4139090363377846		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 3.4139090363377846 | validation: 4.286429732124057]
	TIME [epoch: 11.5 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3694364947147752		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 3.3694364947147752 | validation: 4.362825981081896]
	TIME [epoch: 11.5 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.420595982179262		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 3.420595982179262 | validation: 4.326343168894547]
	TIME [epoch: 11.5 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3689119299144554		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 3.3689119299144554 | validation: 4.2665318178291685]
	TIME [epoch: 11.5 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4359026740722465		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 3.4359026740722465 | validation: 4.415270507871865]
	TIME [epoch: 11.5 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3421450445762892		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 3.3421450445762892 | validation: 4.252865961298886]
	TIME [epoch: 11.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.35214278854752		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 3.35214278854752 | validation: 4.250654499936586]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.306356641585483		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 3.306356641585483 | validation: 4.421289649037213]
	TIME [epoch: 11.5 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.183331683274008		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 4.183331683274008 | validation: 4.592661396521645]
	TIME [epoch: 11.6 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4742439036611943		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 3.4742439036611943 | validation: 4.257552640413865]
	TIME [epoch: 11.5 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3448697353026464		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 3.3448697353026464 | validation: 4.235424954306196]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.319532648244915		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 3.319532648244915 | validation: 4.223290536085933]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366851232081619		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 3.366851232081619 | validation: 4.462592187549231]
	TIME [epoch: 11.5 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3701276653667662		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 3.3701276653667662 | validation: 4.221078484087055]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.770068507905825		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 3.770068507905825 | validation: 4.4329712344141505]
	TIME [epoch: 11.5 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.46018170348733		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 3.46018170348733 | validation: 4.324256712271243]
	TIME [epoch: 11.5 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.313278603403874		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 3.313278603403874 | validation: 4.254771559746084]
	TIME [epoch: 11.5 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.360099395234853		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 3.360099395234853 | validation: 4.258498089844431]
	TIME [epoch: 11.5 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320919208106366		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 3.320919208106366 | validation: 4.23206919582754]
	TIME [epoch: 11.5 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3157267800308654		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 3.3157267800308654 | validation: 4.289991263264929]
	TIME [epoch: 11.5 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3684782196168728		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 3.3684782196168728 | validation: 4.429609285116211]
	TIME [epoch: 11.5 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3629316356435925		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 3.3629316356435925 | validation: 4.262336778203607]
	TIME [epoch: 11.5 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5054434137551245		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 3.5054434137551245 | validation: 4.299325641636728]
	TIME [epoch: 11.5 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4231192314433763		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 3.4231192314433763 | validation: 4.288319503482471]
	TIME [epoch: 11.5 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.372720284235743		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 3.372720284235743 | validation: 4.23769152037658]
	TIME [epoch: 11.5 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4359305715550343		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 3.4359305715550343 | validation: 4.269596316019468]
	TIME [epoch: 11.5 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3194415113283386		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 3.3194415113283386 | validation: 4.241415021853312]
	TIME [epoch: 11.5 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5290918240443654		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 3.5290918240443654 | validation: 4.311651094736145]
	TIME [epoch: 11.5 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3628553362207496		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 3.3628553362207496 | validation: 4.31169153156001]
	TIME [epoch: 11.5 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.354376515632712		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 3.354376515632712 | validation: 4.242466533241499]
	TIME [epoch: 11.5 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3547797139741196		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 3.3547797139741196 | validation: 4.244917390167068]
	TIME [epoch: 11.5 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.426345147253347		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 3.426345147253347 | validation: 4.246827500061108]
	TIME [epoch: 11.5 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3203853564736834		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 3.3203853564736834 | validation: 4.24499748274645]
	TIME [epoch: 11.5 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3228940308260526		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 3.3228940308260526 | validation: 4.249078846347407]
	TIME [epoch: 11.5 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.299670104145		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 3.299670104145 | validation: 4.387879482934154]
	TIME [epoch: 11.5 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4584469762798307		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 3.4584469762798307 | validation: 4.544961327061952]
	TIME [epoch: 11.5 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.506293727863124		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 3.506293727863124 | validation: 4.2962529501373]
	TIME [epoch: 11.5 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345155261759136		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 3.345155261759136 | validation: 4.286857972973571]
	TIME [epoch: 11.5 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3334164524265475		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 3.3334164524265475 | validation: 4.243448844120435]
	TIME [epoch: 11.5 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.65853955920704		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 3.65853955920704 | validation: 4.79642518149746]
	TIME [epoch: 11.5 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.611490892553207		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 3.611490892553207 | validation: 4.338077004592223]
	TIME [epoch: 11.5 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3292725930957388		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 3.3292725930957388 | validation: 4.491610818468234]
	TIME [epoch: 11.5 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3804317137775417		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 3.3804317137775417 | validation: 4.234594799508283]
	TIME [epoch: 11.5 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3095478587246863		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 3.3095478587246863 | validation: 4.543573709009512]
	TIME [epoch: 11.5 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3869679887967545		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 3.3869679887967545 | validation: 4.293199394988376]
	TIME [epoch: 11.5 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3335888551891992		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 3.3335888551891992 | validation: 4.258911098036964]
	TIME [epoch: 11.5 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5085314641448937		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 3.5085314641448937 | validation: 4.319856212298148]
	TIME [epoch: 11.5 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3892227230932543		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 3.3892227230932543 | validation: 4.285118756760339]
	TIME [epoch: 11.5 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.387833536850819		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 3.387833536850819 | validation: 4.257193258396422]
	TIME [epoch: 11.5 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3179811430341974		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 3.3179811430341974 | validation: 4.23992473372914]
	TIME [epoch: 11.5 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3972831454315946		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 3.3972831454315946 | validation: 4.2449410453531495]
	TIME [epoch: 11.5 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.345404634967318		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 3.345404634967318 | validation: 4.243085755075105]
	TIME [epoch: 11.5 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4111757026108593		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 3.4111757026108593 | validation: 4.259010602384198]
	TIME [epoch: 11.5 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.460057897854692		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 3.460057897854692 | validation: 4.273959838585802]
	TIME [epoch: 11.5 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3030582500249945		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 3.3030582500249945 | validation: 4.2509597851117515]
	TIME [epoch: 11.5 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.306666203816226		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 3.306666203816226 | validation: 4.4138849033754175]
	TIME [epoch: 11.5 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3958845169236462		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 3.3958845169236462 | validation: 4.227579702648298]
	TIME [epoch: 11.5 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.315899807612265		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 3.315899807612265 | validation: 4.591449165060734]
	TIME [epoch: 11.5 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5170086631263677		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 3.5170086631263677 | validation: 4.277361604656846]
	TIME [epoch: 11.5 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4494443824030494		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 3.4494443824030494 | validation: 4.28121070148665]
	TIME [epoch: 11.5 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329291971965744		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 3.329291971965744 | validation: 4.243528957802993]
	TIME [epoch: 11.5 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340895246149062		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 3.340895246149062 | validation: 4.304094994697512]
	TIME [epoch: 11.5 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3316451651925725		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 3.3316451651925725 | validation: 4.2773560873683065]
	TIME [epoch: 11.5 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288474641702571		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 3.288474641702571 | validation: 4.32857320727531]
	TIME [epoch: 11.5 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3793673068197756		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 3.3793673068197756 | validation: 4.2280650927864665]
	TIME [epoch: 11.5 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.475532614823848		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 3.475532614823848 | validation: 4.357524750713515]
	TIME [epoch: 11.5 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3119043315252736		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 3.3119043315252736 | validation: 4.2492780134554184]
	TIME [epoch: 11.5 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.29401796410343		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 3.29401796410343 | validation: 4.241332349783719]
	TIME [epoch: 11.5 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3033590309282377		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 3.3033590309282377 | validation: 4.30951501562989]
	TIME [epoch: 11.5 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4504114268613524		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 3.4504114268613524 | validation: 4.3570541732229415]
	TIME [epoch: 11.5 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.415307459957937		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 3.415307459957937 | validation: 4.276604757792943]
	TIME [epoch: 11.5 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3492721276543502		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 3.3492721276543502 | validation: 4.369500487762279]
	TIME [epoch: 11.5 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336079389778825		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 3.336079389778825 | validation: 4.307238740016081]
	TIME [epoch: 11.5 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3380902039613045		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 3.3380902039613045 | validation: 4.243260653530503]
	TIME [epoch: 11.5 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3358860297726767		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 3.3358860297726767 | validation: 4.242561963112543]
	TIME [epoch: 11.5 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.324610757681998		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 3.324610757681998 | validation: 4.305996577441262]
	TIME [epoch: 11.5 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.369224846863361		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 3.369224846863361 | validation: 4.27349456785261]
	TIME [epoch: 11.5 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4250180777903694		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 3.4250180777903694 | validation: 4.2413993850768446]
	TIME [epoch: 11.5 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4476219728990403		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 3.4476219728990403 | validation: 4.276830809174719]
	TIME [epoch: 11.5 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330781899100483		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 3.330781899100483 | validation: 4.270531489761984]
	TIME [epoch: 11.5 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3070952353927776		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 3.3070952353927776 | validation: 4.3209573827810805]
	TIME [epoch: 11.5 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.427297800456401		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 3.427297800456401 | validation: 4.316142156424256]
	TIME [epoch: 11.5 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.322890471003624		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 3.322890471003624 | validation: 4.271079635600675]
	TIME [epoch: 11.5 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4793521355803705		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 3.4793521355803705 | validation: 4.291261937134267]
	TIME [epoch: 11.5 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.317236235435552		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 3.317236235435552 | validation: 4.324867731937921]
	TIME [epoch: 11.5 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2854807681187577		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 3.2854807681187577 | validation: 4.264602886674746]
	TIME [epoch: 11.5 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3690776750549336		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 3.3690776750549336 | validation: 4.251182880270534]
	TIME [epoch: 11.5 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3236338672048005		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 3.3236338672048005 | validation: 4.338014522012608]
	TIME [epoch: 11.5 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.445239040569494		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 3.445239040569494 | validation: 4.29240343201425]
	TIME [epoch: 11.5 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28907610612728		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 3.28907610612728 | validation: 4.2950887802650035]
	TIME [epoch: 11.5 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3337376449516367		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 3.3337376449516367 | validation: 4.300302462620205]
	TIME [epoch: 11.5 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331652678068371		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 3.331652678068371 | validation: 4.236673488805527]
	TIME [epoch: 11.5 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3865776969353174		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 3.3865776969353174 | validation: 4.256708803854692]
	TIME [epoch: 11.5 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3387990079123067		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 3.3387990079123067 | validation: 4.294217338400116]
	TIME [epoch: 11.5 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.329055436000665		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 3.329055436000665 | validation: 4.237542348608523]
	TIME [epoch: 11.5 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.311278301217745		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 3.311278301217745 | validation: 4.231228223277754]
	TIME [epoch: 11.5 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.286054747673835		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 3.286054747673835 | validation: 4.231156428937807]
	TIME [epoch: 11.5 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3112394927232516		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 3.3112394927232516 | validation: 4.2401681442209735]
	TIME [epoch: 11.5 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.347280878417927		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 3.347280878417927 | validation: 4.497415989266845]
	TIME [epoch: 11.5 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.403047265015289		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 3.403047265015289 | validation: 4.273290201281129]
	TIME [epoch: 11.5 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.303192729942994		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 3.303192729942994 | validation: 4.340512267145921]
	TIME [epoch: 11.5 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.327583670405532		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 3.327583670405532 | validation: 4.327608536688366]
	TIME [epoch: 11.5 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3015914171886838		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 3.3015914171886838 | validation: 4.342222150193511]
	TIME [epoch: 11.5 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7566206157726865		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 3.7566206157726865 | validation: 4.326874880661483]
	TIME [epoch: 11.5 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.311083019339636		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 3.311083019339636 | validation: 4.230328280914322]
	TIME [epoch: 11.5 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2763542840084545		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 3.2763542840084545 | validation: 4.360746770609897]
	TIME [epoch: 11.5 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.400641651715026		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 3.400641651715026 | validation: 4.325958984998549]
	TIME [epoch: 11.5 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.314943709395938		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 3.314943709395938 | validation: 4.26211148921327]
	TIME [epoch: 11.5 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2800484340367966		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 3.2800484340367966 | validation: 4.2452556198701075]
	TIME [epoch: 11.5 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3398524119001616		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 3.3398524119001616 | validation: 4.548221638798425]
	TIME [epoch: 11.5 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.421368489267231		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 3.421368489267231 | validation: 4.307603754234198]
	TIME [epoch: 11.5 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3123439891255586		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 3.3123439891255586 | validation: 4.452390455548303]
	TIME [epoch: 11.5 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.37281066096832		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 3.37281066096832 | validation: 4.413128345321041]
	TIME [epoch: 11.5 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.340179478405816		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 3.340179478405816 | validation: 4.2618543134599385]
	TIME [epoch: 11.5 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336164306716788		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 3.336164306716788 | validation: 4.294703805633167]
	TIME [epoch: 11.5 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295465273265624		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 3.295465273265624 | validation: 4.247555095165391]
	TIME [epoch: 11.5 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.326834528559707		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 3.326834528559707 | validation: 4.471502306400863]
	TIME [epoch: 11.5 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3539679517189898		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 3.3539679517189898 | validation: 4.245697189409259]
	TIME [epoch: 11.5 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.330399753883161		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 3.330399753883161 | validation: 4.476350157644972]
	TIME [epoch: 11.5 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3355256166880363		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 3.3355256166880363 | validation: 4.36730242362632]
	TIME [epoch: 11.5 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.312772125213552		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 3.312772125213552 | validation: 4.291815234820844]
	TIME [epoch: 11.5 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3786864888322143		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 3.3786864888322143 | validation: 4.255538971251975]
	TIME [epoch: 11.5 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.335345381807216		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 3.335345381807216 | validation: 4.339972308102169]
	TIME [epoch: 11.5 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2819002561931354		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 3.2819002561931354 | validation: 4.225830814971993]
	TIME [epoch: 11.5 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.300906095480401		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 3.300906095480401 | validation: 4.235138382600111]
	TIME [epoch: 11.5 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3240429996528182		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 3.3240429996528182 | validation: 4.239495581561177]
	TIME [epoch: 11.5 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2841423329998896		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 3.2841423329998896 | validation: 4.241180879902646]
	TIME [epoch: 11.5 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288191903898854		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 3.288191903898854 | validation: 4.248519009942473]
	TIME [epoch: 11.5 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3021975493227664		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 3.3021975493227664 | validation: 4.3290051035523405]
	TIME [epoch: 11.5 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280634855622944		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 3.280634855622944 | validation: 4.285916890818451]
	TIME [epoch: 11.5 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.273442875468543		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 3.273442875468543 | validation: 4.2878225884109815]
	TIME [epoch: 11.5 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2808863761375884		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 3.2808863761375884 | validation: 4.297369607541616]
	TIME [epoch: 11.5 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3248212548380445		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 3.3248212548380445 | validation: 4.249014049290955]
	TIME [epoch: 11.5 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363949360586994		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 3.363949360586994 | validation: 4.233265493367498]
	TIME [epoch: 11.5 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2969515705046515		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 3.2969515705046515 | validation: 4.253059875302204]
	TIME [epoch: 11.5 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30493846407366		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 3.30493846407366 | validation: 4.239086670607328]
	TIME [epoch: 11.5 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2663666471651935		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 3.2663666471651935 | validation: 4.300663987069712]
	TIME [epoch: 11.5 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3110293757410734		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 3.3110293757410734 | validation: 4.321806424342696]
	TIME [epoch: 11.5 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2888946471602134		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 3.2888946471602134 | validation: 4.256403508102986]
	TIME [epoch: 11.5 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3035191035310194		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 3.3035191035310194 | validation: 4.387016031665929]
	TIME [epoch: 11.5 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3434633468704904		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 3.3434633468704904 | validation: 4.313483133947842]
	TIME [epoch: 11.5 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3241999911783298		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 3.3241999911783298 | validation: 4.348206102808718]
	TIME [epoch: 11.5 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3574048966914303		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 3.3574048966914303 | validation: 4.3601447813438465]
	TIME [epoch: 11.5 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.434906383845515		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 3.434906383845515 | validation: 4.707813619280706]
	TIME [epoch: 11.5 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.478103595444159		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 3.478103595444159 | validation: 4.370762254096351]
	TIME [epoch: 11.5 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.341919269722003		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 3.341919269722003 | validation: 4.239475148960047]
	TIME [epoch: 11.5 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.291537658502534		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 3.291537658502534 | validation: 4.326645525541978]
	TIME [epoch: 11.5 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3018657991337683		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 3.3018657991337683 | validation: 4.373515104112933]
	TIME [epoch: 11.5 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.308347974377151		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 3.308347974377151 | validation: 4.260389698879688]
	TIME [epoch: 11.5 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2798550638149377		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 3.2798550638149377 | validation: 4.306931181550369]
	TIME [epoch: 11.5 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285810788249106		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 3.285810788249106 | validation: 4.267099116094146]
	TIME [epoch: 11.5 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.273716565935286		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 3.273716565935286 | validation: 4.236230371015749]
	TIME [epoch: 11.5 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2744267985076587		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 3.2744267985076587 | validation: 4.324307108849186]
	TIME [epoch: 11.5 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3026129997813487		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 3.3026129997813487 | validation: 4.238665201865035]
	TIME [epoch: 11.5 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.278871481665515		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 3.278871481665515 | validation: 4.294271651926469]
	TIME [epoch: 11.5 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288306558538097		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 3.288306558538097 | validation: 4.298667019723831]
	TIME [epoch: 11.5 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5379271347251846		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 3.5379271347251846 | validation: 4.230134449426216]
	TIME [epoch: 11.5 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2413435330898985		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 3.2413435330898985 | validation: 4.413650470317191]
	TIME [epoch: 11.5 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3079836703088406		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 3.3079836703088406 | validation: 4.290095010478817]
	TIME [epoch: 11.6 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2953536927008664		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 3.2953536927008664 | validation: 4.237995709199425]
	TIME [epoch: 11.5 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2916105505534707		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 3.2916105505534707 | validation: 4.284286602303399]
	TIME [epoch: 11.5 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.294119399796908		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 3.294119399796908 | validation: 4.235117180195828]
	TIME [epoch: 11.5 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2767421311305673		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 3.2767421311305673 | validation: 4.272845746042974]
	TIME [epoch: 11.5 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2747906818643204		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 3.2747906818643204 | validation: 4.307436429260362]
	TIME [epoch: 11.5 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3173638953846787		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 3.3173638953846787 | validation: 4.262253506314076]
	TIME [epoch: 11.5 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.281252828047387		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 3.281252828047387 | validation: 4.262645200388476]
	TIME [epoch: 11.5 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30641414168668		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 3.30641414168668 | validation: 4.354568488579223]
	TIME [epoch: 11.5 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3044164401466163		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 3.3044164401466163 | validation: 4.337236672011472]
	TIME [epoch: 11.5 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3006093126602756		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 3.3006093126602756 | validation: 4.30533187190604]
	TIME [epoch: 11.5 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3467370529025153		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 3.3467370529025153 | validation: 4.278260172142057]
	TIME [epoch: 11.5 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2974421380859966		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 3.2974421380859966 | validation: 4.3045953516000175]
	TIME [epoch: 11.5 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2905859777637962		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 3.2905859777637962 | validation: 4.250437796365715]
	TIME [epoch: 11.5 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3025967784080623		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 3.3025967784080623 | validation: 4.343808041435217]
	TIME [epoch: 11.5 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2771304585077394		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 3.2771304585077394 | validation: 4.300699315263126]
	TIME [epoch: 11.5 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274197358622201		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 3.274197358622201 | validation: 4.360164927409785]
	TIME [epoch: 11.5 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.37683249929618		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 3.37683249929618 | validation: 4.253592908682281]
	TIME [epoch: 11.5 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253959938078302		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 3.253959938078302 | validation: 4.368777099998144]
	TIME [epoch: 11.5 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3920519778820544		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 3.3920519778820544 | validation: 4.286562638225403]
	TIME [epoch: 11.6 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28675589847799		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 3.28675589847799 | validation: 4.235433606998743]
	TIME [epoch: 11.5 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2889514800512187		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 3.2889514800512187 | validation: 4.394978655317915]
	TIME [epoch: 11.5 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.331733999366984		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 3.331733999366984 | validation: 4.365486655401853]
	TIME [epoch: 11.5 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3122394186797406		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 3.3122394186797406 | validation: 4.244107583602468]
	TIME [epoch: 11.5 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253055500489345		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 3.253055500489345 | validation: 4.235920811161154]
	TIME [epoch: 11.5 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253062232365233		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 3.253062232365233 | validation: 4.2427103726241056]
	TIME [epoch: 11.5 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2998364895801955		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 3.2998364895801955 | validation: 4.497157890522011]
	TIME [epoch: 11.5 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3455043953467265		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 3.3455043953467265 | validation: 4.24260634321375]
	TIME [epoch: 11.5 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.344196119675322		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 3.344196119675322 | validation: 4.346983159717326]
	TIME [epoch: 11.5 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3315102429431596		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 3.3315102429431596 | validation: 4.363095842072973]
	TIME [epoch: 11.5 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3348167101859323		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 3.3348167101859323 | validation: 4.2454644105085775]
	TIME [epoch: 11.5 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2764542122945146		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 3.2764542122945146 | validation: 4.255834538977967]
	TIME [epoch: 11.5 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2520862321908384		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 3.2520862321908384 | validation: 4.2653039151420735]
	TIME [epoch: 11.5 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2780332184657484		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 3.2780332184657484 | validation: 4.242449389022136]
	TIME [epoch: 11.5 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4771778118264765		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 3.4771778118264765 | validation: 4.511204176304241]
	TIME [epoch: 11.5 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.386804029872948		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 3.386804029872948 | validation: 4.280460873478909]
	TIME [epoch: 11.5 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2715860350777937		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 3.2715860350777937 | validation: 4.254388715124426]
	TIME [epoch: 11.5 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2717520060828433		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 3.2717520060828433 | validation: 4.252679686885119]
	TIME [epoch: 11.5 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2681244030900003		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 3.2681244030900003 | validation: 4.2367350508548025]
	TIME [epoch: 11.5 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2664137170442054		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 3.2664137170442054 | validation: 4.239358406683807]
	TIME [epoch: 11.5 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2821934813089344		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 3.2821934813089344 | validation: 4.2374974188091095]
	TIME [epoch: 11.5 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2770872373112727		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 3.2770872373112727 | validation: 4.2721957482254]
	TIME [epoch: 11.5 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2845142144476993		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 3.2845142144476993 | validation: 4.259935992777564]
	TIME [epoch: 11.5 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.280155614427743		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 3.280155614427743 | validation: 4.2562187254078]
	TIME [epoch: 11.5 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2722723987477242		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 3.2722723987477242 | validation: 4.227188099130252]
	TIME [epoch: 11.5 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.248466395386165		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 3.248466395386165 | validation: 4.2351922014629215]
	TIME [epoch: 11.5 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2804935595402975		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 3.2804935595402975 | validation: 4.345737804813016]
	TIME [epoch: 11.5 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.298102308463949		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 3.298102308463949 | validation: 4.239241628326467]
	TIME [epoch: 11.5 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.306257632950591		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 3.306257632950591 | validation: 4.237206365664379]
	TIME [epoch: 11.5 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2933944182153434		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 3.2933944182153434 | validation: 4.250341903530383]
	TIME [epoch: 11.5 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2567891348136895		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 3.2567891348136895 | validation: 4.244823999707844]
	TIME [epoch: 11.5 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.274802302816216		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 3.274802302816216 | validation: 4.2339957324982915]
	TIME [epoch: 11.5 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2758255981655537		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 3.2758255981655537 | validation: 4.265477290379384]
	TIME [epoch: 11.5 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2661923277377096		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 3.2661923277377096 | validation: 4.2299796584029625]
	TIME [epoch: 11.5 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2463097133454		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 3.2463097133454 | validation: 4.396938145338639]
	TIME [epoch: 11.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.323762963648173		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 3.323762963648173 | validation: 4.31490574731941]
	TIME [epoch: 11.5 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.308897131493054		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 3.308897131493054 | validation: 4.254092701224282]
	TIME [epoch: 11.5 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2835237375549307		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 3.2835237375549307 | validation: 4.225663492254506]
	TIME [epoch: 11.5 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2600735295571384		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 3.2600735295571384 | validation: 4.254161316987578]
	TIME [epoch: 11.5 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3021417735856033		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 3.3021417735856033 | validation: 4.279973735589184]
	TIME [epoch: 11.5 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3000891062425377		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 3.3000891062425377 | validation: 4.232215252701455]
	TIME [epoch: 11.5 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.270017374027692		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 3.270017374027692 | validation: 4.238702864728242]
	TIME [epoch: 11.5 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2904920175289654		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 3.2904920175289654 | validation: 4.260458596277978]
	TIME [epoch: 11.5 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2813559841918205		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 3.2813559841918205 | validation: 4.236553915087619]
	TIME [epoch: 11.5 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292250268808877		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 3.292250268808877 | validation: 4.311469399107584]
	TIME [epoch: 11.5 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2845700880171216		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 3.2845700880171216 | validation: 4.321198079054008]
	TIME [epoch: 11.6 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.282917464340917		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 3.282917464340917 | validation: 4.296866104036916]
	TIME [epoch: 11.5 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292577342803461		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 3.292577342803461 | validation: 4.23398644204024]
	TIME [epoch: 11.5 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.279521503887487		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 3.279521503887487 | validation: 4.363072701439044]
	TIME [epoch: 11.6 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3465221475423474		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 3.3465221475423474 | validation: 4.258638944075009]
	TIME [epoch: 11.5 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.264268983917341		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 3.264268983917341 | validation: 4.241035218414598]
	TIME [epoch: 11.5 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260816653771967		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 3.260816653771967 | validation: 4.236442761846807]
	TIME [epoch: 11.5 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2542431984857307		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 3.2542431984857307 | validation: 4.246675463311192]
	TIME [epoch: 11.5 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.264342474902529		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 3.264342474902529 | validation: 4.29709146289887]
	TIME [epoch: 11.5 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.25804722057139		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 3.25804722057139 | validation: 4.305130946355302]
	TIME [epoch: 11.5 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2905342585876776		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 3.2905342585876776 | validation: 4.305628645687537]
	TIME [epoch: 11.5 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2840667766271534		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 3.2840667766271534 | validation: 4.221877523306859]
	TIME [epoch: 11.5 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2617622813988993		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 3.2617622813988993 | validation: 4.246713353875918]
	TIME [epoch: 11.5 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.276276858908835		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 3.276276858908835 | validation: 4.267625679617996]
	TIME [epoch: 11.5 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.295682420609323		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 3.295682420609323 | validation: 4.241873395841802]
	TIME [epoch: 11.5 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3048650024886332		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 3.3048650024886332 | validation: 4.2315198095987885]
	TIME [epoch: 11.5 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2800501686709307		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 3.2800501686709307 | validation: 4.323648995089392]
	TIME [epoch: 11.5 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.271941112480962		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 3.271941112480962 | validation: 4.229150802672464]
	TIME [epoch: 11.5 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2684026776261534		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 3.2684026776261534 | validation: 4.589008816516487]
	TIME [epoch: 11.5 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.422340868284918		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 3.422340868284918 | validation: 4.252799863082816]
	TIME [epoch: 11.5 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269742927878456		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 3.269742927878456 | validation: 4.3107125026404685]
	TIME [epoch: 11.5 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2756616462697234		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 3.2756616462697234 | validation: 4.25680020349486]
	TIME [epoch: 11.5 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.262911174513702		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 3.262911174513702 | validation: 4.364120980508139]
	TIME [epoch: 11.5 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.320658900457312		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 3.320658900457312 | validation: 4.232866455906545]
	TIME [epoch: 11.5 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2889028914130862		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 3.2889028914130862 | validation: 4.387756114965345]
	TIME [epoch: 11.5 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.336184064875144		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 3.336184064875144 | validation: 4.538927997239691]
	TIME [epoch: 11.5 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.435057411047276		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 3.435057411047276 | validation: 4.292331418907366]
	TIME [epoch: 11.5 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2703488131775584		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 3.2703488131775584 | validation: 4.238409993815359]
	TIME [epoch: 11.5 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2715640455130703		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 3.2715640455130703 | validation: 4.2571420189828935]
	TIME [epoch: 11.5 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.266461075475771		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 3.266461075475771 | validation: 4.320432193080595]
	TIME [epoch: 11.5 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288002130444762		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 3.288002130444762 | validation: 4.224680980603133]
	TIME [epoch: 11.5 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3035511926752257		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 3.3035511926752257 | validation: 4.265598500560361]
	TIME [epoch: 11.5 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2665235613479693		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 3.2665235613479693 | validation: 4.2314101311580785]
	TIME [epoch: 11.6 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2489786041333537		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 3.2489786041333537 | validation: 4.249510340844777]
	TIME [epoch: 11.5 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243255274198925		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 3.243255274198925 | validation: 4.222307236682032]
	TIME [epoch: 11.5 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240920260501319		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 3.240920260501319 | validation: 4.314943570261132]
	TIME [epoch: 11.5 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3058036077614874		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 3.3058036077614874 | validation: 4.23698063957992]
	TIME [epoch: 11.5 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2398390779376607		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 3.2398390779376607 | validation: 4.235264226411009]
	TIME [epoch: 11.5 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3142491524902837		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 3.3142491524902837 | validation: 4.29438844527804]
	TIME [epoch: 11.5 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3068305531498035		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 3.3068305531498035 | validation: 4.299674409178171]
	TIME [epoch: 11.5 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.30320994195926		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 3.30320994195926 | validation: 4.221274283519233]
	TIME [epoch: 11.5 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251990364236894		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 3.251990364236894 | validation: 4.247703075778942]
	TIME [epoch: 11.5 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.254004450867866		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 3.254004450867866 | validation: 4.251579082381982]
	TIME [epoch: 11.5 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2609438837514837		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 3.2609438837514837 | validation: 4.233869987054707]
	TIME [epoch: 11.5 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2830776396449384		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 3.2830776396449384 | validation: 4.271824255477483]
	TIME [epoch: 11.5 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265286195188473		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 3.265286195188473 | validation: 4.222145852252431]
	TIME [epoch: 11.6 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.267712448659827		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 3.267712448659827 | validation: 4.262953252852126]
	TIME [epoch: 11.5 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2572998878523207		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 3.2572998878523207 | validation: 4.223634283791611]
	TIME [epoch: 11.5 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.27075256739748		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 3.27075256739748 | validation: 4.250117243868428]
	TIME [epoch: 11.5 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.25050209909051		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 3.25050209909051 | validation: 4.23931450016205]
	TIME [epoch: 11.5 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.259795171963157		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 3.259795171963157 | validation: 4.2389198103333765]
	TIME [epoch: 11.5 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2554933153262464		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 3.2554933153262464 | validation: 4.249989622398325]
	TIME [epoch: 11.5 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.284066007678811		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 3.284066007678811 | validation: 4.257346962985892]
	TIME [epoch: 11.5 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2576097218818525		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 3.2576097218818525 | validation: 4.248274572902181]
	TIME [epoch: 11.5 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2539068145286536		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 3.2539068145286536 | validation: 4.2403360856184085]
	TIME [epoch: 11.5 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2801861681407765		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 3.2801861681407765 | validation: 4.248683326770705]
	TIME [epoch: 11.5 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2633238000169085		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 3.2633238000169085 | validation: 4.250529188241821]
	TIME [epoch: 11.5 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247956539978855		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 3.247956539978855 | validation: 4.277377565306239]
	TIME [epoch: 11.5 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2724193209969616		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 3.2724193209969616 | validation: 4.252450258837724]
	TIME [epoch: 11.6 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2696055329243823		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 3.2696055329243823 | validation: 4.236496343695386]
	TIME [epoch: 11.5 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258921997306232		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 3.258921997306232 | validation: 4.329742869048373]
	TIME [epoch: 11.5 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2964355396732117		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 3.2964355396732117 | validation: 4.282023423430765]
	TIME [epoch: 11.6 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2381541844099773		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 3.2381541844099773 | validation: 4.261910967725955]
	TIME [epoch: 11.5 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2865449409316954		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 3.2865449409316954 | validation: 4.246158415874021]
	TIME [epoch: 11.5 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2801587202585236		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 3.2801587202585236 | validation: 4.230922660827595]
	TIME [epoch: 11.5 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2663748264113948		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 3.2663748264113948 | validation: 4.2629435163129905]
	TIME [epoch: 11.5 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2645117645234456		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 3.2645117645234456 | validation: 4.237670242636498]
	TIME [epoch: 11.5 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2674251836064596		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 3.2674251836064596 | validation: 4.271873334318518]
	TIME [epoch: 11.5 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2628770653013737		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 3.2628770653013737 | validation: 4.271132131873353]
	TIME [epoch: 11.5 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3072696177839576		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 3.3072696177839576 | validation: 4.238047393267622]
	TIME [epoch: 11.5 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.26292998313827		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 3.26292998313827 | validation: 4.235614986187363]
	TIME [epoch: 11.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2528754323345215		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 3.2528754323345215 | validation: 4.224458453607144]
	TIME [epoch: 11.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240795379982224		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 3.240795379982224 | validation: 4.2921658922058965]
	TIME [epoch: 11.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2604364617283657		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 3.2604364617283657 | validation: 4.267079957872114]
	TIME [epoch: 11.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2570045773418603		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 3.2570045773418603 | validation: 4.2413370995933874]
	TIME [epoch: 11.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3120812149732792		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 3.3120812149732792 | validation: 4.268335925350613]
	TIME [epoch: 11.5 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260207367182106		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 3.260207367182106 | validation: 4.224212438557709]
	TIME [epoch: 11.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2680042608814626		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 3.2680042608814626 | validation: 4.262768149024124]
	TIME [epoch: 11.5 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283373577784485		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 3.283373577784485 | validation: 4.244159922691787]
	TIME [epoch: 11.5 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2645468511751052		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 3.2645468511751052 | validation: 4.239588894767411]
	TIME [epoch: 11.5 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.238617039211138		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 3.238617039211138 | validation: 4.225693435021108]
	TIME [epoch: 11.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2459226576443716		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 3.2459226576443716 | validation: 4.25089259449041]
	TIME [epoch: 11.5 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.28146566346917		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 3.28146566346917 | validation: 4.3232214532630655]
	TIME [epoch: 11.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.27296880043336		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 3.27296880043336 | validation: 4.287522438023737]
	TIME [epoch: 11.5 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2545306596026236		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 3.2545306596026236 | validation: 4.2590666666290895]
	TIME [epoch: 11.5 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2635795426716174		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 3.2635795426716174 | validation: 4.29659322255681]
	TIME [epoch: 11.5 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.258411474795635		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 3.258411474795635 | validation: 4.234023607092388]
	TIME [epoch: 11.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239093975033583		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 3.239093975033583 | validation: 4.242443830937872]
	TIME [epoch: 11.5 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2660514655925645		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 3.2660514655925645 | validation: 4.256724372605841]
	TIME [epoch: 11.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2895872612494674		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 3.2895872612494674 | validation: 4.3147292803810995]
	TIME [epoch: 11.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292423677759056		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 3.292423677759056 | validation: 4.227168857435762]
	TIME [epoch: 11.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2584734397607127		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 3.2584734397607127 | validation: 4.26330407010595]
	TIME [epoch: 11.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2514354763371016		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 3.2514354763371016 | validation: 4.273221467909561]
	TIME [epoch: 11.5 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2679119149347096		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 3.2679119149347096 | validation: 4.237798586625003]
	TIME [epoch: 11.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253823078541926		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 3.253823078541926 | validation: 4.2337568813964985]
	TIME [epoch: 11.5 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.245452083420529		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 3.245452083420529 | validation: 4.238610975166531]
	TIME [epoch: 11.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.246829200591339		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 3.246829200591339 | validation: 4.2765380954839465]
	TIME [epoch: 11.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2653012685339933		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 3.2653012685339933 | validation: 4.280244017984312]
	TIME [epoch: 11.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2737768339730797		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 3.2737768339730797 | validation: 4.241148747713896]
	TIME [epoch: 11.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2510421547753086		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 3.2510421547753086 | validation: 4.229549533086632]
	TIME [epoch: 11.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24228104030981		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 3.24228104030981 | validation: 4.225153090942814]
	TIME [epoch: 11.5 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.248000078866716		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 3.248000078866716 | validation: 4.241011321293909]
	TIME [epoch: 11.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.257167957590419		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 3.257167957590419 | validation: 4.25777247621071]
	TIME [epoch: 11.5 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2520544952411052		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 3.2520544952411052 | validation: 4.232724075287774]
	TIME [epoch: 11.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2420606483466026		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 3.2420606483466026 | validation: 4.259358807954809]
	TIME [epoch: 11.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2627256388088055		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 3.2627256388088055 | validation: 4.2664924556308845]
	TIME [epoch: 11.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255109048524411		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 3.255109048524411 | validation: 4.260783687989016]
	TIME [epoch: 11.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.262935126801492		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 3.262935126801492 | validation: 4.224465751922129]
	TIME [epoch: 11.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2385168374012063		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 3.2385168374012063 | validation: 4.315333663193718]
	TIME [epoch: 11.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2630300149746976		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 3.2630300149746976 | validation: 4.224658459215371]
	TIME [epoch: 11.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2601937167920108		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 3.2601937167920108 | validation: 4.212993693878554]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2270159210897926		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 3.2270159210897926 | validation: 4.220825362969522]
	TIME [epoch: 11.5 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232839750680042		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 3.232839750680042 | validation: 4.22358812555626]
	TIME [epoch: 11.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.283283188470662		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 3.283283188470662 | validation: 4.228954529822287]
	TIME [epoch: 11.5 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247677043852731		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 3.247677043852731 | validation: 4.273324181474509]
	TIME [epoch: 11.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2794746119841904		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 3.2794746119841904 | validation: 4.363171701918492]
	TIME [epoch: 11.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3038982927532112		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 3.3038982927532112 | validation: 4.247409876780309]
	TIME [epoch: 11.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.250409256628313		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 3.250409256628313 | validation: 4.240707894111008]
	TIME [epoch: 11.5 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243281102608588		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 3.243281102608588 | validation: 4.22482457216081]
	TIME [epoch: 11.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2403950019900627		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 3.2403950019900627 | validation: 4.228111083981518]
	TIME [epoch: 11.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2776528160063703		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 3.2776528160063703 | validation: 4.241158411232332]
	TIME [epoch: 11.5 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2377214423769938		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 3.2377214423769938 | validation: 4.2584711072377335]
	TIME [epoch: 11.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2670225310684247		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 3.2670225310684247 | validation: 4.251850286632682]
	TIME [epoch: 11.5 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2526558887528423		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 3.2526558887528423 | validation: 4.238939685497387]
	TIME [epoch: 11.5 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2423934737185585		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 3.2423934737185585 | validation: 4.216792131545792]
	TIME [epoch: 11.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2251102176668525		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 3.2251102176668525 | validation: 4.245228032955868]
	TIME [epoch: 11.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2477194782428427		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 3.2477194782428427 | validation: 4.221888456545907]
	TIME [epoch: 11.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3049871942064937		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 3.3049871942064937 | validation: 4.215154576325573]
	TIME [epoch: 11.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261603538772664		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 3.261603538772664 | validation: 4.215063241360586]
	TIME [epoch: 11.5 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2336912105516866		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 3.2336912105516866 | validation: 4.231216410087089]
	TIME [epoch: 11.5 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2543615191243194		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 3.2543615191243194 | validation: 4.244753283755228]
	TIME [epoch: 11.5 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2842749477776545		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 3.2842749477776545 | validation: 4.282994988818261]
	TIME [epoch: 11.5 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253031412992458		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 3.253031412992458 | validation: 4.30347563929758]
	TIME [epoch: 11.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2601318316465835		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 3.2601318316465835 | validation: 4.218651931089348]
	TIME [epoch: 11.5 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.241575215988648		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 3.241575215988648 | validation: 4.272537899078794]
	TIME [epoch: 11.5 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251009470447354		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 3.251009470447354 | validation: 4.222097368928075]
	TIME [epoch: 11.5 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2659836588816042		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 3.2659836588816042 | validation: 4.2401222438544055]
	TIME [epoch: 11.5 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251081456982135		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 3.251081456982135 | validation: 4.2674822072726055]
	TIME [epoch: 11.5 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2397733053846056		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 3.2397733053846056 | validation: 4.259779618557809]
	TIME [epoch: 11.5 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251111021937425		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 3.251111021937425 | validation: 4.237184387121816]
	TIME [epoch: 11.5 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2384863794813237		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 3.2384863794813237 | validation: 4.235058420549451]
	TIME [epoch: 11.5 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2575383194680017		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 3.2575383194680017 | validation: 4.2580320116077734]
	TIME [epoch: 11.5 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.275971972515091		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 3.275971972515091 | validation: 4.236662285830367]
	TIME [epoch: 11.5 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.26580841305506		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 3.26580841305506 | validation: 4.230923354005837]
	TIME [epoch: 11.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2466819665240623		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 3.2466819665240623 | validation: 4.245563625055341]
	TIME [epoch: 11.5 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.253967497260718		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 3.253967497260718 | validation: 4.375416148177584]
	TIME [epoch: 11.5 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2964238762451767		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 3.2964238762451767 | validation: 4.226204556590161]
	TIME [epoch: 11.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2479750595428416		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 3.2479750595428416 | validation: 4.235932355644364]
	TIME [epoch: 11.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2342876870587447		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 3.2342876870587447 | validation: 4.231918490970042]
	TIME [epoch: 11.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239887929784929		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 3.239887929784929 | validation: 4.243288036141753]
	TIME [epoch: 11.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2367882036343016		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 3.2367882036343016 | validation: 4.268558209739464]
	TIME [epoch: 11.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2865523729489006		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 3.2865523729489006 | validation: 4.266896590792164]
	TIME [epoch: 11.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.254454715588813		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 3.254454715588813 | validation: 4.287260560009131]
	TIME [epoch: 11.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.257194156837106		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 3.257194156837106 | validation: 4.244556410019047]
	TIME [epoch: 11.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2470554165250416		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 3.2470554165250416 | validation: 4.234098034519354]
	TIME [epoch: 11.5 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2966648434770187		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 3.2966648434770187 | validation: 4.2404433991257475]
	TIME [epoch: 11.5 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265266531913438		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 3.265266531913438 | validation: 4.235777244995081]
	TIME [epoch: 11.5 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.23998228108299		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 3.23998228108299 | validation: 4.214104288001044]
	TIME [epoch: 11.5 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2439762012636413		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 3.2439762012636413 | validation: 4.221797687210183]
	TIME [epoch: 11.5 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2292964605782037		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 3.2292964605782037 | validation: 4.226587189917361]
	TIME [epoch: 11.5 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.238117599821731		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 3.238117599821731 | validation: 4.220892131963083]
	TIME [epoch: 11.5 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2704726083208318		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 3.2704726083208318 | validation: 4.2522861670500065]
	TIME [epoch: 11.5 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2471629131130912		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 3.2471629131130912 | validation: 4.2120009550696516]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2595837774024945		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 3.2595837774024945 | validation: 4.307047041455452]
	TIME [epoch: 11.5 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3485331204161155		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 3.3485331204161155 | validation: 4.407007442341959]
	TIME [epoch: 11.5 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.288725803202034		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 3.288725803202034 | validation: 4.224562350000652]
	TIME [epoch: 11.5 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2445957011120106		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 3.2445957011120106 | validation: 4.222834925202808]
	TIME [epoch: 11.5 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239517686570032		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 3.239517686570032 | validation: 4.250929813172196]
	TIME [epoch: 11.5 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.245309681846561		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 3.245309681846561 | validation: 4.214477779905372]
	TIME [epoch: 11.5 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2328879970008844		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 3.2328879970008844 | validation: 4.22860990502724]
	TIME [epoch: 11.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227106081046875		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 3.227106081046875 | validation: 4.255697664233644]
	TIME [epoch: 11.5 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2742868981091964		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 3.2742868981091964 | validation: 4.221876644093146]
	TIME [epoch: 11.5 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2408745113310546		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 3.2408745113310546 | validation: 4.236769528285995]
	TIME [epoch: 11.5 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2390739144989578		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 3.2390739144989578 | validation: 4.255066154743858]
	TIME [epoch: 11.5 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243639594699113		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 3.243639594699113 | validation: 4.23907892034863]
	TIME [epoch: 11.5 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2443695603963834		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 3.2443695603963834 | validation: 4.2380394207734176]
	TIME [epoch: 11.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.265962561282488		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 3.265962561282488 | validation: 4.236092714337575]
	TIME [epoch: 11.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2275470320001975		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 3.2275470320001975 | validation: 4.253912824958189]
	TIME [epoch: 11.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.249290217200113		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 3.249290217200113 | validation: 4.249534982450331]
	TIME [epoch: 11.5 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.27318189519639		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 3.27318189519639 | validation: 4.235084644029037]
	TIME [epoch: 11.5 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2410797084407785		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 3.2410797084407785 | validation: 4.217022428416887]
	TIME [epoch: 11.5 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2312425277051773		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 3.2312425277051773 | validation: 4.254777769726635]
	TIME [epoch: 11.5 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2698701377993706		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 3.2698701377993706 | validation: 4.226411659980233]
	TIME [epoch: 11.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2336655718984186		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 3.2336655718984186 | validation: 4.288600383403905]
	TIME [epoch: 11.5 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.260994643393091		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 3.260994643393091 | validation: 4.224139960706948]
	TIME [epoch: 11.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226820645339598		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 3.226820645339598 | validation: 4.231496166896465]
	TIME [epoch: 11.5 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2354468362851554		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 3.2354468362851554 | validation: 4.238699458741485]
	TIME [epoch: 11.5 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2388844823848455		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 3.2388844823848455 | validation: 4.2294565788147045]
	TIME [epoch: 11.5 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2332951766286353		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 3.2332951766286353 | validation: 4.216754067724704]
	TIME [epoch: 11.5 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2363501706621034		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 3.2363501706621034 | validation: 4.234206355485339]
	TIME [epoch: 11.5 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2644289967426823		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 3.2644289967426823 | validation: 4.213296408063142]
	TIME [epoch: 11.5 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2339209250365255		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 3.2339209250365255 | validation: 4.224491888595784]
	TIME [epoch: 11.5 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.235058632998761		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 3.235058632998761 | validation: 4.219372263378031]
	TIME [epoch: 11.5 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.238346917898062		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 3.238346917898062 | validation: 4.232817211398847]
	TIME [epoch: 11.5 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2483038417935837		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 3.2483038417935837 | validation: 4.214323669325601]
	TIME [epoch: 11.5 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228283798642634		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 3.228283798642634 | validation: 4.251696486461067]
	TIME [epoch: 11.5 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2560766463860515		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 3.2560766463860515 | validation: 4.227438670013606]
	TIME [epoch: 11.5 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252476947325441		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 3.252476947325441 | validation: 4.229659783442095]
	TIME [epoch: 11.5 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.26472210703179		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 3.26472210703179 | validation: 4.221820733272753]
	TIME [epoch: 11.5 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228835134432611		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 3.228835134432611 | validation: 4.2440859448821655]
	TIME [epoch: 11.5 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2386679717923212		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 3.2386679717923212 | validation: 4.234646075388736]
	TIME [epoch: 11.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2332393471322085		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 3.2332393471322085 | validation: 4.260376846088939]
	TIME [epoch: 11.5 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2310712142028803		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 3.2310712142028803 | validation: 4.259740730455873]
	TIME [epoch: 11.5 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2502694424261547		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 3.2502694424261547 | validation: 4.252092595065966]
	TIME [epoch: 11.5 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.241573482120205		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 3.241573482120205 | validation: 4.221234269744874]
	TIME [epoch: 11.5 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227702060004435		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 3.227702060004435 | validation: 4.233745355731922]
	TIME [epoch: 11.5 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2393604320714386		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 3.2393604320714386 | validation: 4.227036649524329]
	TIME [epoch: 11.5 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233716112272725		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 3.233716112272725 | validation: 4.273580797327601]
	TIME [epoch: 11.5 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243152691283561		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 3.243152691283561 | validation: 4.224592906098112]
	TIME [epoch: 11.5 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218812720641713		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 3.218812720641713 | validation: 4.231284382334663]
	TIME [epoch: 11.5 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2507577730716744		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 3.2507577730716744 | validation: 4.225312748586942]
	TIME [epoch: 11.5 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232058046897712		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 3.232058046897712 | validation: 4.2450088669364865]
	TIME [epoch: 11.5 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2463239613822483		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 3.2463239613822483 | validation: 4.243219859442279]
	TIME [epoch: 11.5 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2625968372846845		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 3.2625968372846845 | validation: 4.227481965242577]
	TIME [epoch: 11.5 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230604570526803		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 3.230604570526803 | validation: 4.243990632235808]
	TIME [epoch: 11.5 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.248814421399297		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 3.248814421399297 | validation: 4.254901102662031]
	TIME [epoch: 11.5 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2408605530384094		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 3.2408605530384094 | validation: 4.219412261490455]
	TIME [epoch: 11.5 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2662807165978234		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 3.2662807165978234 | validation: 4.395130610081341]
	TIME [epoch: 11.5 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.292586634730691		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 3.292586634730691 | validation: 4.21788138755684]
	TIME [epoch: 11.5 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2464668475132563		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 3.2464668475132563 | validation: 4.2416792112241115]
	TIME [epoch: 11.5 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.239650543063802		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 3.239650543063802 | validation: 4.227221844618988]
	TIME [epoch: 11.5 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.238006569298918		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 3.238006569298918 | validation: 4.2179793159007595]
	TIME [epoch: 11.5 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2279773289660993		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 3.2279773289660993 | validation: 4.214764159842587]
	TIME [epoch: 11.5 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.252234427279582		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 3.252234427279582 | validation: 4.262122109207028]
	TIME [epoch: 11.5 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2366232805280584		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 3.2366232805280584 | validation: 4.235106426919832]
	TIME [epoch: 11.5 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2464484188117364		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 3.2464484188117364 | validation: 4.223108542164011]
	TIME [epoch: 11.5 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2299119946960637		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 3.2299119946960637 | validation: 4.251577771041491]
	TIME [epoch: 11.5 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.231988354134155		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 3.231988354134155 | validation: 4.234477969202841]
	TIME [epoch: 11.5 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.237739926388856		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 3.237739926388856 | validation: 4.221190198118003]
	TIME [epoch: 11.5 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233368848120501		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 3.233368848120501 | validation: 4.20741337335676]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233650032681089		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 3.233650032681089 | validation: 4.219837863451139]
	TIME [epoch: 11.5 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222127248822929		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 3.222127248822929 | validation: 4.212402506382023]
	TIME [epoch: 11.5 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2536480110225305		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 3.2536480110225305 | validation: 4.238089916311063]
	TIME [epoch: 11.5 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2264664514451384		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 3.2264664514451384 | validation: 4.218851266920565]
	TIME [epoch: 11.5 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.245001710752917		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 3.245001710752917 | validation: 4.218954339556857]
	TIME [epoch: 11.5 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.23481882572142		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 3.23481882572142 | validation: 4.2213576050870305]
	TIME [epoch: 11.5 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229207828020461		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 3.229207828020461 | validation: 4.233096556766164]
	TIME [epoch: 11.5 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2414381149099265		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 3.2414381149099265 | validation: 4.219771940021806]
	TIME [epoch: 11.5 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2341985163961757		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 3.2341985163961757 | validation: 4.232997951423323]
	TIME [epoch: 11.5 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.255710775619011		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 3.255710775619011 | validation: 4.210522757836816]
	TIME [epoch: 11.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2425607192380865		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 3.2425607192380865 | validation: 4.211864311404007]
	TIME [epoch: 11.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2444442854679063		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 3.2444442854679063 | validation: 4.270719313261256]
	TIME [epoch: 11.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2571511241508526		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 3.2571511241508526 | validation: 4.241176763092458]
	TIME [epoch: 11.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2328079247097175		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 3.2328079247097175 | validation: 4.2423022029613255]
	TIME [epoch: 11.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2611745373363563		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 3.2611745373363563 | validation: 4.2370591154120625]
	TIME [epoch: 11.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2399196949913205		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 3.2399196949913205 | validation: 4.239398387636831]
	TIME [epoch: 11.5 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.232785369960719		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 3.232785369960719 | validation: 4.237380543701956]
	TIME [epoch: 11.5 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236786697250108		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 3.236786697250108 | validation: 4.237803059609412]
	TIME [epoch: 11.5 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225512858491213		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 3.225512858491213 | validation: 4.208856634991349]
	TIME [epoch: 11.5 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2265441415446903		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 3.2265441415446903 | validation: 4.215251350687331]
	TIME [epoch: 11.5 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.285918222989693		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 3.285918222989693 | validation: 4.204987333579054]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240125109057922		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 3.240125109057922 | validation: 4.231112424554093]
	TIME [epoch: 11.5 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2421140022287616		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 3.2421140022287616 | validation: 4.2229808732846505]
	TIME [epoch: 11.5 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2289549902466974		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 3.2289549902466974 | validation: 4.205364443033044]
	TIME [epoch: 11.5 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223610966201711		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 3.223610966201711 | validation: 4.213480635104851]
	TIME [epoch: 11.5 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2626090785122392		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 3.2626090785122392 | validation: 4.2344398324601435]
	TIME [epoch: 11.5 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2410044584648814		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 3.2410044584648814 | validation: 4.246414942322698]
	TIME [epoch: 11.5 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2367024012963435		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 3.2367024012963435 | validation: 4.2134155817602394]
	TIME [epoch: 11.5 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.244640090140445		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 3.244640090140445 | validation: 4.217466217800913]
	TIME [epoch: 11.5 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.245672707495565		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 3.245672707495565 | validation: 4.221940276269033]
	TIME [epoch: 11.5 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223390673103199		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 3.223390673103199 | validation: 4.234933372274071]
	TIME [epoch: 11.5 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2417322047142174		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 3.2417322047142174 | validation: 4.215267049813995]
	TIME [epoch: 11.5 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2257001599943433		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 3.2257001599943433 | validation: 4.260125996662709]
	TIME [epoch: 11.5 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.263862673682977		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 3.263862673682977 | validation: 4.246071246643642]
	TIME [epoch: 11.5 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.261396289349043		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 3.261396289349043 | validation: 4.2132793257531755]
	TIME [epoch: 11.5 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2233908768085784		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 3.2233908768085784 | validation: 4.20872102391912]
	TIME [epoch: 11.5 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222400704594313		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 3.222400704594313 | validation: 4.225002990388975]
	TIME [epoch: 11.5 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.264719017435744		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 3.264719017435744 | validation: 4.2331610682550505]
	TIME [epoch: 11.5 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225877081300977		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 3.225877081300977 | validation: 4.233018748332449]
	TIME [epoch: 11.5 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2307213631569778		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 3.2307213631569778 | validation: 4.221570814380289]
	TIME [epoch: 11.5 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220763060182283		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 3.220763060182283 | validation: 4.21085459283922]
	TIME [epoch: 11.5 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2352839671021467		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 3.2352839671021467 | validation: 4.227336881596726]
	TIME [epoch: 11.5 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2325516752949843		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 3.2325516752949843 | validation: 4.21232261114396]
	TIME [epoch: 11.5 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222790105889005		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 3.222790105889005 | validation: 4.230807105268948]
	TIME [epoch: 11.5 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2273393285078487		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 3.2273393285078487 | validation: 4.226605332120794]
	TIME [epoch: 11.5 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226569490048441		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 3.226569490048441 | validation: 4.226732401245447]
	TIME [epoch: 11.5 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228862093711698		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 3.228862093711698 | validation: 4.218527324134064]
	TIME [epoch: 11.5 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2270666817183296		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 3.2270666817183296 | validation: 4.247964401703602]
	TIME [epoch: 11.5 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243830198278053		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 3.243830198278053 | validation: 4.213804700368689]
	TIME [epoch: 11.5 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2394139600385268		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 3.2394139600385268 | validation: 4.233286910466249]
	TIME [epoch: 11.5 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233988534402311		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 3.233988534402311 | validation: 4.206025997269789]
	TIME [epoch: 11.5 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236942454788494		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 3.236942454788494 | validation: 4.210750558333264]
	TIME [epoch: 11.5 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2214868454928363		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 3.2214868454928363 | validation: 4.254294800801485]
	TIME [epoch: 11.5 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.243974490585196		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 3.243974490585196 | validation: 4.214814710860123]
	TIME [epoch: 11.5 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225804535575654		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 3.225804535575654 | validation: 4.2051351065178215]
	TIME [epoch: 11.5 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2329693722498827		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 3.2329693722498827 | validation: 4.210408052228769]
	TIME [epoch: 11.5 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2439621213855028		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 3.2439621213855028 | validation: 4.209497628672585]
	TIME [epoch: 11.5 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2645726202394494		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 3.2645726202394494 | validation: 4.212520423009993]
	TIME [epoch: 11.5 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2329044561558744		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 3.2329044561558744 | validation: 4.207389056033855]
	TIME [epoch: 11.5 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.23152380548863		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 3.23152380548863 | validation: 4.286545483385579]
	TIME [epoch: 11.5 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2661623925932792		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 3.2661623925932792 | validation: 4.209273589911628]
	TIME [epoch: 11.5 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233048811047304		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 3.233048811047304 | validation: 4.222959307160091]
	TIME [epoch: 11.5 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2267872162672373		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 3.2267872162672373 | validation: 4.211425973163056]
	TIME [epoch: 11.5 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2252656931076595		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 3.2252656931076595 | validation: 4.244672082013565]
	TIME [epoch: 11.5 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.240651895849866		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 3.240651895849866 | validation: 4.234536435385203]
	TIME [epoch: 11.5 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2839542967774404		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 3.2839542967774404 | validation: 4.215342962620099]
	TIME [epoch: 11.5 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2545277898396314		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 3.2545277898396314 | validation: 4.216443721435582]
	TIME [epoch: 11.5 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221222928058263		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 3.221222928058263 | validation: 4.229184321381046]
	TIME [epoch: 11.5 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2273171568349124		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 3.2273171568349124 | validation: 4.239352012368996]
	TIME [epoch: 11.5 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236714037786617		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 3.236714037786617 | validation: 4.202178692222641]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2164764837263102		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 3.2164764837263102 | validation: 4.214273241462908]
	TIME [epoch: 11.5 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2199513115259064		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 3.2199513115259064 | validation: 4.2636873494663545]
	TIME [epoch: 11.5 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2710486143347204		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 3.2710486143347204 | validation: 4.209517840579446]
	TIME [epoch: 11.5 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.234416941904877		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 3.234416941904877 | validation: 4.220113032712986]
	TIME [epoch: 11.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2200506329736505		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 3.2200506329736505 | validation: 4.211319114817293]
	TIME [epoch: 11.5 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2433189554398094		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 3.2433189554398094 | validation: 4.236240242209574]
	TIME [epoch: 11.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22777618061729		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 3.22777618061729 | validation: 4.269561454969882]
	TIME [epoch: 11.5 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2610962323178336		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 3.2610962323178336 | validation: 4.221104336450787]
	TIME [epoch: 11.5 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.231361309560914		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 3.231361309560914 | validation: 4.21969839739566]
	TIME [epoch: 11.5 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2233317657409737		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 3.2233317657409737 | validation: 4.2163527693972584]
	TIME [epoch: 11.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2251623585661267		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 3.2251623585661267 | validation: 4.21273849730164]
	TIME [epoch: 11.5 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2400783840099443		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 3.2400783840099443 | validation: 4.225395483145983]
	TIME [epoch: 11.5 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2275071559014883		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 3.2275071559014883 | validation: 4.2492645184606905]
	TIME [epoch: 11.5 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2361989257381176		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 3.2361989257381176 | validation: 4.221801685096924]
	TIME [epoch: 11.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2237434749561964		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 3.2237434749561964 | validation: 4.219942452520276]
	TIME [epoch: 11.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223529392954622		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 3.223529392954622 | validation: 4.230673328867044]
	TIME [epoch: 11.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226823149581874		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 3.226823149581874 | validation: 4.246663613513089]
	TIME [epoch: 11.5 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.233656863203899		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 3.233656863203899 | validation: 4.232233957203144]
	TIME [epoch: 11.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222077942293639		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 3.222077942293639 | validation: 4.213663113681807]
	TIME [epoch: 11.5 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21909995426066		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 3.21909995426066 | validation: 4.216541506077408]
	TIME [epoch: 11.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216209309888438		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 3.216209309888438 | validation: 4.275719512951514]
	TIME [epoch: 11.5 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.269208635339292		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 3.269208635339292 | validation: 4.2107665249834945]
	TIME [epoch: 11.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2245702765116566		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 3.2245702765116566 | validation: 4.245042874251082]
	TIME [epoch: 11.5 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2305258178296588		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 3.2305258178296588 | validation: 4.243461906696833]
	TIME [epoch: 11.5 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2418829963761424		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 3.2418829963761424 | validation: 4.212311764657532]
	TIME [epoch: 11.5 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2375914668062906		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 3.2375914668062906 | validation: 4.21788050603719]
	TIME [epoch: 11.5 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247461744818671		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 3.247461744818671 | validation: 4.210804930007555]
	TIME [epoch: 11.5 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2374233708280507		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 3.2374233708280507 | validation: 4.215877095670269]
	TIME [epoch: 11.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2201373816234313		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 3.2201373816234313 | validation: 4.215117262647664]
	TIME [epoch: 11.5 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225886688367214		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 3.225886688367214 | validation: 4.213205221443688]
	TIME [epoch: 11.5 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2180574947263736		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 3.2180574947263736 | validation: 4.209360659360972]
	TIME [epoch: 11.5 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2159411798861184		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 3.2159411798861184 | validation: 4.240892863706577]
	TIME [epoch: 11.5 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.268997093548623		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 3.268997093548623 | validation: 4.218335167224392]
	TIME [epoch: 11.5 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226064141230161		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 3.226064141230161 | validation: 4.239886550335008]
	TIME [epoch: 11.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.268435943376194		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 3.268435943376194 | validation: 4.215775862311514]
	TIME [epoch: 11.5 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2237791873726245		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 3.2237791873726245 | validation: 4.21325264305397]
	TIME [epoch: 11.5 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2166904879473153		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 3.2166904879473153 | validation: 4.273152725451235]
	TIME [epoch: 11.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2534028544446087		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 3.2534028544446087 | validation: 4.237248438616207]
	TIME [epoch: 11.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226503087055031		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 3.226503087055031 | validation: 4.2238819133743]
	TIME [epoch: 11.5 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224432434204971		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 3.224432434204971 | validation: 4.220434256819765]
	TIME [epoch: 11.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2206168213632296		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 3.2206168213632296 | validation: 4.223045255738602]
	TIME [epoch: 11.5 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.237294023889148		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 3.237294023889148 | validation: 4.2100458750233045]
	TIME [epoch: 11.5 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2255311282573214		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 3.2255311282573214 | validation: 4.217357206706041]
	TIME [epoch: 11.5 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220138475060919		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 3.220138475060919 | validation: 4.222238117071044]
	TIME [epoch: 11.5 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2341751407042487		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 3.2341751407042487 | validation: 4.228287156344834]
	TIME [epoch: 11.5 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2307182705522326		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 3.2307182705522326 | validation: 4.214268213859298]
	TIME [epoch: 11.5 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2192624175788116		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 3.2192624175788116 | validation: 4.217442312073233]
	TIME [epoch: 11.5 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220406212881528		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 3.220406212881528 | validation: 4.211128411865227]
	TIME [epoch: 11.5 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217876822267076		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 3.217876822267076 | validation: 4.235407634561827]
	TIME [epoch: 11.5 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2436050726598062		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 3.2436050726598062 | validation: 4.221495625255362]
	TIME [epoch: 11.5 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216542181600103		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 3.216542181600103 | validation: 4.203552820454555]
	TIME [epoch: 11.5 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2245081252404986		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 3.2245081252404986 | validation: 4.259966492629841]
	TIME [epoch: 11.5 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.24843550357482		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 3.24843550357482 | validation: 4.209426131332545]
	TIME [epoch: 11.5 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2246345312091846		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 3.2246345312091846 | validation: 4.212940953196299]
	TIME [epoch: 11.5 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2218598804526732		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 3.2218598804526732 | validation: 4.208199089880324]
	TIME [epoch: 11.5 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2346445811026037		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 3.2346445811026037 | validation: 4.207705089161013]
	TIME [epoch: 11.5 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227983271044199		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 3.227983271044199 | validation: 4.290042979402533]
	TIME [epoch: 11.5 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2754007753631247		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 3.2754007753631247 | validation: 4.2034212330881076]
	TIME [epoch: 11.5 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2179880475081664		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 3.2179880475081664 | validation: 4.223152177664175]
	TIME [epoch: 11.5 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223210948999158		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 3.223210948999158 | validation: 4.20630226050106]
	TIME [epoch: 11.5 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2193867729782975		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 3.2193867729782975 | validation: 4.216212134473819]
	TIME [epoch: 11.5 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2257022432315314		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 3.2257022432315314 | validation: 4.213256606886523]
	TIME [epoch: 11.5 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2151035243389776		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 3.2151035243389776 | validation: 4.208628217599908]
	TIME [epoch: 11.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220155735252089		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 3.220155735252089 | validation: 4.202273385323628]
	TIME [epoch: 11.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2429969597924675		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 3.2429969597924675 | validation: 4.22276861041786]
	TIME [epoch: 11.5 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236996270976944		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 3.236996270976944 | validation: 4.2188617316865376]
	TIME [epoch: 11.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229472824303888		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 3.229472824303888 | validation: 4.227706413764524]
	TIME [epoch: 11.5 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2246307343132603		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 3.2246307343132603 | validation: 4.216396871699739]
	TIME [epoch: 11.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2132877156212674		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 3.2132877156212674 | validation: 4.206740338303916]
	TIME [epoch: 11.5 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215451165050144		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 3.215451165050144 | validation: 4.2109459188949145]
	TIME [epoch: 11.5 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2379162747452064		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 3.2379162747452064 | validation: 4.212987172712702]
	TIME [epoch: 11.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2222429082757573		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 3.2222429082757573 | validation: 4.222127657394544]
	TIME [epoch: 11.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2174169206312975		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 3.2174169206312975 | validation: 4.212561282388391]
	TIME [epoch: 11.5 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212404424212373		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 3.212404424212373 | validation: 4.214478429530957]
	TIME [epoch: 11.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221491179836336		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 3.221491179836336 | validation: 4.204246948322919]
	TIME [epoch: 11.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2327205197861684		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 3.2327205197861684 | validation: 4.215949628588227]
	TIME [epoch: 11.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217402187094879		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 3.217402187094879 | validation: 4.212471750156456]
	TIME [epoch: 11.5 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216227323465607		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 3.216227323465607 | validation: 4.207459172404713]
	TIME [epoch: 11.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230514971815106		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 3.230514971815106 | validation: 4.2138445198307215]
	TIME [epoch: 11.5 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219560842108442		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 3.219560842108442 | validation: 4.219794446203592]
	TIME [epoch: 11.5 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2246302975635244		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 3.2246302975635244 | validation: 4.23835573872464]
	TIME [epoch: 11.5 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216568384102254		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 3.216568384102254 | validation: 4.214600357703705]
	TIME [epoch: 11.5 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2186633019438515		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 3.2186633019438515 | validation: 4.22914220378191]
	TIME [epoch: 11.5 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2330366768218255		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 3.2330366768218255 | validation: 4.207342909082296]
	TIME [epoch: 11.5 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2233670148082045		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 3.2233670148082045 | validation: 4.201437789991751]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223835025322185		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 3.223835025322185 | validation: 4.215736075480388]
	TIME [epoch: 11.5 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2212966400157366		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 3.2212966400157366 | validation: 4.213961054543305]
	TIME [epoch: 11.5 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2136945088739766		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 3.2136945088739766 | validation: 4.213537354447099]
	TIME [epoch: 11.5 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226830935581236		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 3.226830935581236 | validation: 4.219444357379387]
	TIME [epoch: 11.5 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222880681566446		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 3.222880681566446 | validation: 4.218137482552347]
	TIME [epoch: 11.5 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2141913519559466		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 3.2141913519559466 | validation: 4.206043232448792]
	TIME [epoch: 11.5 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216210615283635		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 3.216210615283635 | validation: 4.2189763616809035]
	TIME [epoch: 11.5 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217514005868201		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 3.217514005868201 | validation: 4.2238239737931105]
	TIME [epoch: 11.5 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2162079018780645		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 3.2162079018780645 | validation: 4.221903996747231]
	TIME [epoch: 11.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.251352171824805		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 3.251352171824805 | validation: 4.20426866752711]
	TIME [epoch: 11.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2182453355735023		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 3.2182453355735023 | validation: 4.218218033957824]
	TIME [epoch: 11.5 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218172793609127		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 3.218172793609127 | validation: 4.216147379792311]
	TIME [epoch: 11.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2151434246622617		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 3.2151434246622617 | validation: 4.218513188304746]
	TIME [epoch: 11.5 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2233231670929907		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 3.2233231670929907 | validation: 4.233017658213812]
	TIME [epoch: 11.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2301029579695326		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 3.2301029579695326 | validation: 4.202317754912855]
	TIME [epoch: 11.5 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213047747336922		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 3.213047747336922 | validation: 4.209799199932019]
	TIME [epoch: 11.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2287062735793732		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 3.2287062735793732 | validation: 4.213571798871115]
	TIME [epoch: 11.5 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222121685648784		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 3.222121685648784 | validation: 4.206026036235285]
	TIME [epoch: 11.5 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209510246461589		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 3.209510246461589 | validation: 4.213216781559092]
	TIME [epoch: 11.5 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226195279056092		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 3.226195279056092 | validation: 4.213818260726576]
	TIME [epoch: 11.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21212093955992		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 3.21212093955992 | validation: 4.209205181976198]
	TIME [epoch: 11.5 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2281777436653125		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 3.2281777436653125 | validation: 4.227611534537192]
	TIME [epoch: 11.5 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2184680037890803		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 3.2184680037890803 | validation: 4.215374532638845]
	TIME [epoch: 11.5 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2133565475658425		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 3.2133565475658425 | validation: 4.221799756400959]
	TIME [epoch: 11.5 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2440295488567035		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 3.2440295488567035 | validation: 4.219600844421077]
	TIME [epoch: 11.5 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219682441734095		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 3.219682441734095 | validation: 4.202473998793937]
	TIME [epoch: 11.5 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218415509421234		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 3.218415509421234 | validation: 4.2133852099262254]
	TIME [epoch: 11.5 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215411647601373		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 3.215411647601373 | validation: 4.210932552631367]
	TIME [epoch: 11.5 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2239379797221512		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 3.2239379797221512 | validation: 4.214608670402824]
	TIME [epoch: 11.5 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215888157763137		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 3.215888157763137 | validation: 4.212186337572117]
	TIME [epoch: 11.5 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2371806834086883		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 3.2371806834086883 | validation: 4.201560151074315]
	TIME [epoch: 11.5 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217982735618275		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 3.217982735618275 | validation: 4.212079984537954]
	TIME [epoch: 11.5 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218408338778228		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 3.218408338778228 | validation: 4.209731466558128]
	TIME [epoch: 11.5 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2190773901565297		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 3.2190773901565297 | validation: 4.230239279067229]
	TIME [epoch: 11.5 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2272162292622757		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 3.2272162292622757 | validation: 4.210153167850616]
	TIME [epoch: 11.5 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21515568252409		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 3.21515568252409 | validation: 4.2260324975689825]
	TIME [epoch: 11.5 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2180632668672446		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 3.2180632668672446 | validation: 4.205815268397251]
	TIME [epoch: 11.5 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2190558515100927		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 3.2190558515100927 | validation: 4.234485082474557]
	TIME [epoch: 11.5 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225029329297212		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 3.225029329297212 | validation: 4.210131753066551]
	TIME [epoch: 11.5 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21445503420842		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 3.21445503420842 | validation: 4.211804947837428]
	TIME [epoch: 11.5 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221568269714115		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 3.221568269714115 | validation: 4.209773863434329]
	TIME [epoch: 11.5 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213991519486341		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 3.213991519486341 | validation: 4.211279843552242]
	TIME [epoch: 11.5 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2126116764110715		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 3.2126116764110715 | validation: 4.215570462102312]
	TIME [epoch: 11.5 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2378958545308967		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 3.2378958545308967 | validation: 4.247614384543426]
	TIME [epoch: 11.5 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2408810483309543		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 3.2408810483309543 | validation: 4.210893669113211]
	TIME [epoch: 11.5 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22239013035675		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 3.22239013035675 | validation: 4.219577344510559]
	TIME [epoch: 11.5 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217285644237174		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 3.217285644237174 | validation: 4.226202258114993]
	TIME [epoch: 11.5 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2432774272799594		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 3.2432774272799594 | validation: 4.218280709647359]
	TIME [epoch: 11.5 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2186857564846605		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 3.2186857564846605 | validation: 4.205278560463482]
	TIME [epoch: 11.5 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105991542546057		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 3.2105991542546057 | validation: 4.204729736039996]
	TIME [epoch: 11.5 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215605214572638		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 3.215605214572638 | validation: 4.20515719585137]
	TIME [epoch: 11.5 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2181086684561873		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 3.2181086684561873 | validation: 4.210966589379915]
	TIME [epoch: 11.5 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2161127595928587		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 3.2161127595928587 | validation: 4.216334850107523]
	TIME [epoch: 11.5 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222475408375985		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 3.222475408375985 | validation: 4.234061066625664]
	TIME [epoch: 11.5 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2355829643330782		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 3.2355829643330782 | validation: 4.209413294373411]
	TIME [epoch: 11.5 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2350471751531993		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 3.2350471751531993 | validation: 4.232307170524217]
	TIME [epoch: 11.5 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224436851600014		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 3.224436851600014 | validation: 4.222949950595449]
	TIME [epoch: 11.5 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22279540235414		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 3.22279540235414 | validation: 4.210619429480098]
	TIME [epoch: 11.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216469470522202		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 3.216469470522202 | validation: 4.234395354912212]
	TIME [epoch: 11.5 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.246801970729431		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 3.246801970729431 | validation: 4.21768840284771]
	TIME [epoch: 11.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2167258359142936		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 3.2167258359142936 | validation: 4.21613909160151]
	TIME [epoch: 11.5 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21704574739209		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 3.21704574739209 | validation: 4.201364877195479]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215724198570549		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 3.215724198570549 | validation: 4.218504516002962]
	TIME [epoch: 11.5 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220211309342035		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 3.220211309342035 | validation: 4.208821342854652]
	TIME [epoch: 11.5 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213445135510791		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 3.213445135510791 | validation: 4.219416270527887]
	TIME [epoch: 11.5 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2197796101823544		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 3.2197796101823544 | validation: 4.211452485949332]
	TIME [epoch: 11.5 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216087113244954		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 3.216087113244954 | validation: 4.211101889408867]
	TIME [epoch: 11.5 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215113112533028		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 3.215113112533028 | validation: 4.208699269723257]
	TIME [epoch: 11.5 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219318910944595		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 3.219318910944595 | validation: 4.219328043433406]
	TIME [epoch: 11.5 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2346110246924433		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 3.2346110246924433 | validation: 4.2229174527687094]
	TIME [epoch: 11.5 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.231274297076632		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 3.231274297076632 | validation: 4.232848595933627]
	TIME [epoch: 11.5 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224805016774149		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 3.224805016774149 | validation: 4.21827135936843]
	TIME [epoch: 11.5 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216086022844525		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 3.216086022844525 | validation: 4.216393165107178]
	TIME [epoch: 11.5 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2170711027121555		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 3.2170711027121555 | validation: 4.206306123868835]
	TIME [epoch: 11.5 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224053240038458		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 3.224053240038458 | validation: 4.243128169736356]
	TIME [epoch: 11.5 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222906532297541		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 3.222906532297541 | validation: 4.229995955103164]
	TIME [epoch: 11.5 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22845046167627		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 3.22845046167627 | validation: 4.218160539383683]
	TIME [epoch: 11.5 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2162334034923075		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 3.2162334034923075 | validation: 4.208049990301955]
	TIME [epoch: 11.5 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2165561693209006		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 3.2165561693209006 | validation: 4.218356730875143]
	TIME [epoch: 11.5 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.241085146867136		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 3.241085146867136 | validation: 4.243521136880938]
	TIME [epoch: 11.5 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2162912598693896		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 3.2162912598693896 | validation: 4.211914081731841]
	TIME [epoch: 11.5 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2132070180785086		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 3.2132070180785086 | validation: 4.234271440139144]
	TIME [epoch: 11.5 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226098896483416		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 3.226098896483416 | validation: 4.207986214877095]
	TIME [epoch: 11.5 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2168477909243784		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 3.2168477909243784 | validation: 4.220978365128697]
	TIME [epoch: 11.5 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228753933052552		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 3.228753933052552 | validation: 4.236795877455924]
	TIME [epoch: 11.5 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2220960376499304		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 3.2220960376499304 | validation: 4.213977205171662]
	TIME [epoch: 11.5 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212312379974758		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 3.212312379974758 | validation: 4.209314778330513]
	TIME [epoch: 11.5 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210108945075467		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 3.210108945075467 | validation: 4.219387754011628]
	TIME [epoch: 11.5 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214735268931351		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 3.214735268931351 | validation: 4.221466178387943]
	TIME [epoch: 11.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222137032242366		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 3.222137032242366 | validation: 4.2245819223157675]
	TIME [epoch: 11.5 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222637114028738		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 3.222637114028738 | validation: 4.215066893608813]
	TIME [epoch: 11.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2191373915157886		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 3.2191373915157886 | validation: 4.204641916207153]
	TIME [epoch: 11.5 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217297812555559		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 3.217297812555559 | validation: 4.216150218577938]
	TIME [epoch: 11.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229438057125323		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 3.229438057125323 | validation: 4.209489573116229]
	TIME [epoch: 11.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2109105848113817		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 3.2109105848113817 | validation: 4.215915252844726]
	TIME [epoch: 11.5 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2282952185588107		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 3.2282952185588107 | validation: 4.2493146943039575]
	TIME [epoch: 11.5 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.236685628957967		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 3.236685628957967 | validation: 4.22078067665246]
	TIME [epoch: 11.5 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2355441365337745		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 3.2355441365337745 | validation: 4.216014090850429]
	TIME [epoch: 11.5 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222409654626964		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 3.222409654626964 | validation: 4.222859530537212]
	TIME [epoch: 11.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2289495205468097		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 3.2289495205468097 | validation: 4.203905087628577]
	TIME [epoch: 11.5 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2123925717282917		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 3.2123925717282917 | validation: 4.2057109601261375]
	TIME [epoch: 11.5 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2141586786306817		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 3.2141586786306817 | validation: 4.212059039787609]
	TIME [epoch: 11.5 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213560542891937		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 3.213560542891937 | validation: 4.212729973704779]
	TIME [epoch: 11.5 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2110878908350826		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 3.2110878908350826 | validation: 4.207754675752433]
	TIME [epoch: 11.5 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2221938126112946		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 3.2221938126112946 | validation: 4.22383567076034]
	TIME [epoch: 11.5 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2227404819408347		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 3.2227404819408347 | validation: 4.202526813060873]
	TIME [epoch: 11.5 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2101307954768648		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 3.2101307954768648 | validation: 4.201888355074452]
	TIME [epoch: 11.5 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2155186836511205		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 3.2155186836511205 | validation: 4.214749668898793]
	TIME [epoch: 11.5 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2136712053777057		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 3.2136712053777057 | validation: 4.2188570408379835]
	TIME [epoch: 11.5 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227089483646391		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 3.227089483646391 | validation: 4.219480836776322]
	TIME [epoch: 11.5 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217548188923884		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 3.217548188923884 | validation: 4.22373756351319]
	TIME [epoch: 11.5 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2217741625785807		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 3.2217741625785807 | validation: 4.212460052223261]
	TIME [epoch: 11.5 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216418444591552		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 3.216418444591552 | validation: 4.212860048902721]
	TIME [epoch: 11.5 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2133264112336737		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 3.2133264112336737 | validation: 4.21845344705752]
	TIME [epoch: 11.5 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.247529320828353		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 3.247529320828353 | validation: 4.251803539067116]
	TIME [epoch: 11.5 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2297408672794834		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 3.2297408672794834 | validation: 4.2128295801573055]
	TIME [epoch: 11.5 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2197781844818674		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 3.2197781844818674 | validation: 4.205466050484223]
	TIME [epoch: 11.5 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216991035931142		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 3.216991035931142 | validation: 4.217826096016138]
	TIME [epoch: 11.5 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21603303865371		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 3.21603303865371 | validation: 4.211592931125296]
	TIME [epoch: 11.5 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2213599378848503		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 3.2213599378848503 | validation: 4.221260770826668]
	TIME [epoch: 11.5 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227403583095557		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 3.227403583095557 | validation: 4.229508582762348]
	TIME [epoch: 11.5 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2225873994910614		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 3.2225873994910614 | validation: 4.2247800589070374]
	TIME [epoch: 11.5 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2360932477386553		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 3.2360932477386553 | validation: 4.2359805657139535]
	TIME [epoch: 11.5 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2354371645452487		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 3.2354371645452487 | validation: 4.217140133985302]
	TIME [epoch: 11.5 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218145951117358		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 3.218145951117358 | validation: 4.21568901164602]
	TIME [epoch: 11.5 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2186187225602514		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 3.2186187225602514 | validation: 4.224805376796284]
	TIME [epoch: 11.5 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.223251303505382		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 3.223251303505382 | validation: 4.212641599109878]
	TIME [epoch: 11.5 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21243212071205		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 3.21243212071205 | validation: 4.2137934123208955]
	TIME [epoch: 11.5 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097154838087514		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 3.2097154838087514 | validation: 4.199507515222416]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2226749746001477		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 3.2226749746001477 | validation: 4.217349981425456]
	TIME [epoch: 11.5 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2189924688385414		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 3.2189924688385414 | validation: 4.215152045848911]
	TIME [epoch: 11.5 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2174649062435976		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 3.2174649062435976 | validation: 4.235175125992657]
	TIME [epoch: 11.5 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2303767133673853		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 3.2303767133673853 | validation: 4.211338706275479]
	TIME [epoch: 11.5 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210018337663213		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 3.210018337663213 | validation: 4.2043205382005215]
	TIME [epoch: 11.5 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2359107954428135		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 3.2359107954428135 | validation: 4.2509637355075585]
	TIME [epoch: 11.5 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2319231328569105		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 3.2319231328569105 | validation: 4.2024165338718875]
	TIME [epoch: 11.5 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107364149201527		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 3.2107364149201527 | validation: 4.201809692293396]
	TIME [epoch: 11.5 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208634100524387		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 3.208634100524387 | validation: 4.209864810086966]
	TIME [epoch: 11.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224949274078074		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 3.224949274078074 | validation: 4.216294418671681]
	TIME [epoch: 11.5 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222004045184208		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 3.222004045184208 | validation: 4.201990399629288]
	TIME [epoch: 11.5 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217356358525894		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 3.217356358525894 | validation: 4.211799469056606]
	TIME [epoch: 11.5 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2258007648155074		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 3.2258007648155074 | validation: 4.2055760210439335]
	TIME [epoch: 11.5 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224584281833241		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 3.224584281833241 | validation: 4.220386078110162]
	TIME [epoch: 11.5 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2204077831307365		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 3.2204077831307365 | validation: 4.2155864946408315]
	TIME [epoch: 11.5 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2301192195491306		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 3.2301192195491306 | validation: 4.229832756513207]
	TIME [epoch: 11.5 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222718037606851		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 3.222718037606851 | validation: 4.206067211791976]
	TIME [epoch: 11.5 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.222105226768884		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 3.222105226768884 | validation: 4.215888028130526]
	TIME [epoch: 11.5 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2143699602067692		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 3.2143699602067692 | validation: 4.194573024355403]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_965.pth
	Model improved!!!
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208148868672285		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 3.208148868672285 | validation: 4.215545542513124]
	TIME [epoch: 11.5 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.225406934719176		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 3.225406934719176 | validation: 4.209131685837636]
	TIME [epoch: 11.5 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2203392737219563		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 3.2203392737219563 | validation: 4.208571777274401]
	TIME [epoch: 11.5 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2196861606355864		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 3.2196861606355864 | validation: 4.208104321215731]
	TIME [epoch: 11.5 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2114503667544128		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 3.2114503667544128 | validation: 4.208264923126132]
	TIME [epoch: 11.5 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2140638986746954		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 3.2140638986746954 | validation: 4.209213634048918]
	TIME [epoch: 11.5 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22046439543405		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 3.22046439543405 | validation: 4.20841884752712]
	TIME [epoch: 11.5 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215024195803408		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 3.215024195803408 | validation: 4.208113240452977]
	TIME [epoch: 11.5 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211939694590651		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 3.211939694590651 | validation: 4.211482016866245]
	TIME [epoch: 11.5 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2153962646831333		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 3.2153962646831333 | validation: 4.218302830892646]
	TIME [epoch: 11.5 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2169108648761826		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 3.2169108648761826 | validation: 4.216868662779781]
	TIME [epoch: 11.5 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220245879294599		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 3.220245879294599 | validation: 4.21102982979472]
	TIME [epoch: 11.5 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218641773611914		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 3.218641773611914 | validation: 4.2136494215389675]
	TIME [epoch: 11.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2166506024770904		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 3.2166506024770904 | validation: 4.212651899919994]
	TIME [epoch: 11.5 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2127346158902883		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 3.2127346158902883 | validation: 4.207147925580083]
	TIME [epoch: 11.5 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2217296901351857		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 3.2217296901351857 | validation: 4.216718991696012]
	TIME [epoch: 11.5 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2134955576566187		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 3.2134955576566187 | validation: 4.205426323887207]
	TIME [epoch: 11.5 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2127040954108934		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 3.2127040954108934 | validation: 4.216908515253023]
	TIME [epoch: 11.5 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21228669525283		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 3.21228669525283 | validation: 4.2142515284490125]
	TIME [epoch: 11.5 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2196077344412064		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 3.2196077344412064 | validation: 4.215485947370091]
	TIME [epoch: 11.5 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2175847089676313		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 3.2175847089676313 | validation: 4.217354882870411]
	TIME [epoch: 11.5 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2431773825855323		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 3.2431773825855323 | validation: 4.2327359072295705]
	TIME [epoch: 11.5 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2245524792595903		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 3.2245524792595903 | validation: 4.207982074355826]
	TIME [epoch: 11.5 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107475809297417		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 3.2107475809297417 | validation: 4.2192339734026785]
	TIME [epoch: 11.5 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2269868858524053		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 3.2269868858524053 | validation: 4.2366938708320445]
	TIME [epoch: 11.5 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2193294941613084		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 3.2193294941613084 | validation: 4.206715327911232]
	TIME [epoch: 11.5 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097112320629546		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 3.2097112320629546 | validation: 4.203914298241334]
	TIME [epoch: 11.5 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217432376321845		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 3.217432376321845 | validation: 4.217094451699799]
	TIME [epoch: 11.5 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212640548139938		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 3.212640548139938 | validation: 4.208499141964561]
	TIME [epoch: 11.5 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2120266114620426		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 3.2120266114620426 | validation: 4.2103141181184105]
	TIME [epoch: 11.5 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2251616675181385		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 3.2251616675181385 | validation: 4.19731442339707]
	TIME [epoch: 11.5 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210490759073817		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 3.210490759073817 | validation: 4.2084623481665195]
	TIME [epoch: 11.5 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221296805382288		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 3.221296805382288 | validation: 4.219402533215948]
	TIME [epoch: 11.5 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2190861389542818		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 3.2190861389542818 | validation: 4.21500856040812]
	TIME [epoch: 11.5 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2160969997032725		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 3.2160969997032725 | validation: 4.208843541782023]
	TIME [epoch: 11.5 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2156116557228467		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 3.2156116557228467 | validation: 4.207192278896089]
	TIME [epoch: 11.5 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2124343688661288		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 3.2124343688661288 | validation: 4.202841505542715]
	TIME [epoch: 11.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2149219086655654		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 3.2149219086655654 | validation: 4.207410859115828]
	TIME [epoch: 11.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2244280500035503		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 3.2244280500035503 | validation: 4.215935987571611]
	TIME [epoch: 11.5 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2164200869989186		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 3.2164200869989186 | validation: 4.21393044933196]
	TIME [epoch: 11.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220801652664512		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 3.220801652664512 | validation: 4.241982830235161]
	TIME [epoch: 11.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.230811932805497		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 3.230811932805497 | validation: 4.205558961930841]
	TIME [epoch: 11.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2189289135237944		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 3.2189289135237944 | validation: 4.200710535873379]
	TIME [epoch: 11.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2121521324211253		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 3.2121521324211253 | validation: 4.2161895446261966]
	TIME [epoch: 11.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2291987058195253		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 3.2291987058195253 | validation: 4.213558808287954]
	TIME [epoch: 11.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216214140642106		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 3.216214140642106 | validation: 4.21047661524156]
	TIME [epoch: 11.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.228929719187083		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 3.228929719187083 | validation: 4.215721217228109]
	TIME [epoch: 11.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211581025158085		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 3.211581025158085 | validation: 4.198958734976969]
	TIME [epoch: 11.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213415948243381		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 3.213415948243381 | validation: 4.219560387598255]
	TIME [epoch: 11.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2282319766055054		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 3.2282319766055054 | validation: 4.204747562528522]
	TIME [epoch: 11.5 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2151712018480136		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 3.2151712018480136 | validation: 4.198922290303549]
	TIME [epoch: 11.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2114780186767478		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 3.2114780186767478 | validation: 4.205260655068476]
	TIME [epoch: 11.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209537780930564		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 3.209537780930564 | validation: 4.205246761483447]
	TIME [epoch: 11.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2106163993912267		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 3.2106163993912267 | validation: 4.210422754123867]
	TIME [epoch: 11.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2106600857577634		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 3.2106600857577634 | validation: 4.203781217269924]
	TIME [epoch: 11.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2119807431078584		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 3.2119807431078584 | validation: 4.212068785354532]
	TIME [epoch: 11.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21487105586065		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 3.21487105586065 | validation: 4.203415554773826]
	TIME [epoch: 11.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217116825132928		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 3.217116825132928 | validation: 4.2038892041093945]
	TIME [epoch: 11.5 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217945197173694		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 3.217945197173694 | validation: 4.219586885923556]
	TIME [epoch: 11.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216473168128898		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 3.216473168128898 | validation: 4.201919001573156]
	TIME [epoch: 11.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2143457488316853		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 3.2143457488316853 | validation: 4.211739990176998]
	TIME [epoch: 11.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216640292983045		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 3.216640292983045 | validation: 4.2077379659621625]
	TIME [epoch: 11.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2146720376453572		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 3.2146720376453572 | validation: 4.206193686324701]
	TIME [epoch: 11.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211181614613875		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 3.211181614613875 | validation: 4.199801901737802]
	TIME [epoch: 11.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210952892991398		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 3.210952892991398 | validation: 4.200410067435697]
	TIME [epoch: 11.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208545810140927		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 3.208545810140927 | validation: 4.2002180280010135]
	TIME [epoch: 11.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2145593127981575		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 3.2145593127981575 | validation: 4.2191889944591425]
	TIME [epoch: 11.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2227993403488053		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 3.2227993403488053 | validation: 4.218664207212668]
	TIME [epoch: 11.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221816426339565		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 3.221816426339565 | validation: 4.2149270240165]
	TIME [epoch: 11.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2152525480892677		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 3.2152525480892677 | validation: 4.205267230855725]
	TIME [epoch: 11.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219125301273153		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 3.219125301273153 | validation: 4.209902925574973]
	TIME [epoch: 11.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089949492651373		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 3.2089949492651373 | validation: 4.209435142341212]
	TIME [epoch: 11.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211416284962264		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 3.211416284962264 | validation: 4.199866167932782]
	TIME [epoch: 11.5 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093627503716946		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 3.2093627503716946 | validation: 4.219995233879853]
	TIME [epoch: 11.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2160546532619745		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 3.2160546532619745 | validation: 4.213648831093185]
	TIME [epoch: 11.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2177223905110433		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 3.2177223905110433 | validation: 4.208826535918724]
	TIME [epoch: 11.5 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.227035701906347		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 3.227035701906347 | validation: 4.219295719804807]
	TIME [epoch: 11.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2122853838684606		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 3.2122853838684606 | validation: 4.211090421512386]
	TIME [epoch: 11.5 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210202771877439		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 3.210202771877439 | validation: 4.2092669148567765]
	TIME [epoch: 11.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212674412811491		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 3.212674412811491 | validation: 4.211364480939419]
	TIME [epoch: 11.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213532793483716		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 3.213532793483716 | validation: 4.2112730476725275]
	TIME [epoch: 11.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2175158363882463		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 3.2175158363882463 | validation: 4.217676181407807]
	TIME [epoch: 11.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2121443476591454		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 3.2121443476591454 | validation: 4.208123977543402]
	TIME [epoch: 11.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211127204258789		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 3.211127204258789 | validation: 4.207691597653629]
	TIME [epoch: 11.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208742102187434		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 3.208742102187434 | validation: 4.209272723503684]
	TIME [epoch: 11.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212460571617985		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 3.212460571617985 | validation: 4.219026464675638]
	TIME [epoch: 11.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2281315058313096		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 3.2281315058313096 | validation: 4.2036954177058705]
	TIME [epoch: 11.5 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2130241799586226		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 3.2130241799586226 | validation: 4.209484340107427]
	TIME [epoch: 11.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2128927614210157		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 3.2128927614210157 | validation: 4.199354571602782]
	TIME [epoch: 11.5 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079791335267718		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 3.2079791335267718 | validation: 4.2113793843188105]
	TIME [epoch: 11.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2197070905160814		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 3.2197070905160814 | validation: 4.212579951251379]
	TIME [epoch: 11.5 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2130832783092957		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 3.2130832783092957 | validation: 4.215544934153479]
	TIME [epoch: 11.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20997848870131		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 3.20997848870131 | validation: 4.205701737403139]
	TIME [epoch: 11.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2090251368309364		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 3.2090251368309364 | validation: 4.203130220561902]
	TIME [epoch: 11.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073649872764873		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 3.2073649872764873 | validation: 4.209599598735647]
	TIME [epoch: 11.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2236308114196994		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 3.2236308114196994 | validation: 4.2229603475855155]
	TIME [epoch: 11.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.220809580616013		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 3.220809580616013 | validation: 4.202568470711729]
	TIME [epoch: 11.5 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214130675500048		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 3.214130675500048 | validation: 4.20637653774789]
	TIME [epoch: 11.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2126402193963353		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 3.2126402193963353 | validation: 4.2030351818996845]
	TIME [epoch: 11.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107569608454556		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 3.2107569608454556 | validation: 4.2012730514072185]
	TIME [epoch: 11.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206950516340318		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 3.206950516340318 | validation: 4.205841293181565]
	TIME [epoch: 11.5 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221109447889818		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 3.221109447889818 | validation: 4.211420601259428]
	TIME [epoch: 11.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218376370851135		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 3.218376370851135 | validation: 4.204539831980809]
	TIME [epoch: 11.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2106382179226443		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 3.2106382179226443 | validation: 4.203003256244712]
	TIME [epoch: 11.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210152208568038		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 3.210152208568038 | validation: 4.200722762637898]
	TIME [epoch: 11.5 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2157539854419093		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 3.2157539854419093 | validation: 4.196690857822721]
	TIME [epoch: 11.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2118528803577586		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 3.2118528803577586 | validation: 4.207705223054719]
	TIME [epoch: 11.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210504112826757		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 3.210504112826757 | validation: 4.1995651562344465]
	TIME [epoch: 11.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209378470008477		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 3.209378470008477 | validation: 4.198929365026888]
	TIME [epoch: 11.5 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073677287124487		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 3.2073677287124487 | validation: 4.207627287733694]
	TIME [epoch: 11.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2086328660666554		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 3.2086328660666554 | validation: 4.21186284855713]
	TIME [epoch: 11.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2300183030275496		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 3.2300183030275496 | validation: 4.228176290547003]
	TIME [epoch: 11.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224054934464713		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 3.224054934464713 | validation: 4.205160530288555]
	TIME [epoch: 11.5 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211028899372744		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 3.211028899372744 | validation: 4.205591218054198]
	TIME [epoch: 11.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2099400957327378		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 3.2099400957327378 | validation: 4.203019822088715]
	TIME [epoch: 11.5 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208614891787952		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 3.208614891787952 | validation: 4.204731398840814]
	TIME [epoch: 11.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216929291177401		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 3.216929291177401 | validation: 4.2045447633727875]
	TIME [epoch: 11.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208378024948646		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 3.208378024948646 | validation: 4.205258349740778]
	TIME [epoch: 11.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107163285045797		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 3.2107163285045797 | validation: 4.2075664342750585]
	TIME [epoch: 11.5 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2135277175221324		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 3.2135277175221324 | validation: 4.195212269943929]
	TIME [epoch: 11.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2108757947242568		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 3.2108757947242568 | validation: 4.202936489591953]
	TIME [epoch: 11.5 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2171901360680994		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 3.2171901360680994 | validation: 4.214216840561416]
	TIME [epoch: 11.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210034530098266		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 3.210034530098266 | validation: 4.204039173632526]
	TIME [epoch: 11.5 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213400019312197		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 3.213400019312197 | validation: 4.210463823006011]
	TIME [epoch: 11.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221532064200634		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 3.221532064200634 | validation: 4.206178410727557]
	TIME [epoch: 11.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218036942054335		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 3.218036942054335 | validation: 4.211371282708576]
	TIME [epoch: 11.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219713834868321		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 3.219713834868321 | validation: 4.225222432826737]
	TIME [epoch: 11.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.224563565928576		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 3.224563565928576 | validation: 4.2041900768987785]
	TIME [epoch: 11.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2258251244490967		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 3.2258251244490967 | validation: 4.222022962223905]
	TIME [epoch: 11.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2218565296069253		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 3.2218565296069253 | validation: 4.219781595998232]
	TIME [epoch: 11.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2143637101059586		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 3.2143637101059586 | validation: 4.201684495676524]
	TIME [epoch: 11.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203924279915821		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 3.203924279915821 | validation: 4.208924358709284]
	TIME [epoch: 11.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065299623512606		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 3.2065299623512606 | validation: 4.2157283465694455]
	TIME [epoch: 11.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212136633422583		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 3.212136633422583 | validation: 4.208802459292273]
	TIME [epoch: 11.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2137217825923696		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 3.2137217825923696 | validation: 4.224882557739522]
	TIME [epoch: 11.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2229195820888217		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 3.2229195820888217 | validation: 4.213868960825371]
	TIME [epoch: 11.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213361277633279		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 3.213361277633279 | validation: 4.206896906341692]
	TIME [epoch: 11.5 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215738615980023		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 3.215738615980023 | validation: 4.217706705557451]
	TIME [epoch: 11.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226431632794175		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 3.226431632794175 | validation: 4.216963160620182]
	TIME [epoch: 11.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2221050412408845		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 3.2221050412408845 | validation: 4.207964824332228]
	TIME [epoch: 11.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066763067767328		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 3.2066763067767328 | validation: 4.211167253959012]
	TIME [epoch: 11.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2129896380558876		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 3.2129896380558876 | validation: 4.21114830921428]
	TIME [epoch: 11.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2104006308736093		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 3.2104006308736093 | validation: 4.197815023925022]
	TIME [epoch: 11.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2131004933496694		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 3.2131004933496694 | validation: 4.204956424540325]
	TIME [epoch: 11.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210030376128221		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 3.210030376128221 | validation: 4.205483464174177]
	TIME [epoch: 11.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2111082755879297		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 3.2111082755879297 | validation: 4.204252427192733]
	TIME [epoch: 11.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2044108492963423		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 3.2044108492963423 | validation: 4.205396309900967]
	TIME [epoch: 11.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2138295434184934		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 3.2138295434184934 | validation: 4.207767826222753]
	TIME [epoch: 11.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.226856414691936		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 3.226856414691936 | validation: 4.20240073334393]
	TIME [epoch: 11.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211685761461158		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 3.211685761461158 | validation: 4.207817271546296]
	TIME [epoch: 11.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2206396678066573		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 3.2206396678066573 | validation: 4.21843918806852]
	TIME [epoch: 11.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2149610442536		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 3.2149610442536 | validation: 4.207911075549309]
	TIME [epoch: 11.5 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2101423107120373		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 3.2101423107120373 | validation: 4.206725126010205]
	TIME [epoch: 11.5 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2098999227320597		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 3.2098999227320597 | validation: 4.207120837306125]
	TIME [epoch: 11.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216055254498244		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 3.216055254498244 | validation: 4.20701125045694]
	TIME [epoch: 11.5 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.221062367611897		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 3.221062367611897 | validation: 4.206418411684268]
	TIME [epoch: 11.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085849113624185		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 3.2085849113624185 | validation: 4.207050646646037]
	TIME [epoch: 11.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204675459707951		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 3.204675459707951 | validation: 4.209983302369515]
	TIME [epoch: 11.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216381803079485		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 3.216381803079485 | validation: 4.2045513474357294]
	TIME [epoch: 11.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061370251376493		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 3.2061370251376493 | validation: 4.204706764808718]
	TIME [epoch: 11.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107349484026884		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 3.2107349484026884 | validation: 4.204294283635238]
	TIME [epoch: 11.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208432630572725		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 3.208432630572725 | validation: 4.207760725652428]
	TIME [epoch: 11.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2165049862778003		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 3.2165049862778003 | validation: 4.216142568241963]
	TIME [epoch: 11.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2155267842873685		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 3.2155267842873685 | validation: 4.206831272483363]
	TIME [epoch: 11.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210912316273465		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 3.210912316273465 | validation: 4.205902485964723]
	TIME [epoch: 11.5 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050180552391305		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 3.2050180552391305 | validation: 4.209964997691536]
	TIME [epoch: 11.5 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2131656750269095		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 3.2131656750269095 | validation: 4.200907403457355]
	TIME [epoch: 11.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2135963534317415		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 3.2135963534317415 | validation: 4.205637165277462]
	TIME [epoch: 11.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213975401090817		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 3.213975401090817 | validation: 4.204682198658813]
	TIME [epoch: 11.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2133866707316185		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 3.2133866707316185 | validation: 4.206215365377902]
	TIME [epoch: 11.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.218004708249661		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 3.218004708249661 | validation: 4.20671897071478]
	TIME [epoch: 11.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212873037886923		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 3.212873037886923 | validation: 4.214558619210578]
	TIME [epoch: 11.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107876262483077		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 3.2107876262483077 | validation: 4.207763918129073]
	TIME [epoch: 11.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210256989159058		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 3.210256989159058 | validation: 4.200589843367844]
	TIME [epoch: 11.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2113701315121954		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 3.2113701315121954 | validation: 4.209466732544754]
	TIME [epoch: 11.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206907035038456		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 3.206907035038456 | validation: 4.212356802896343]
	TIME [epoch: 11.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208925465036677		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 3.208925465036677 | validation: 4.203683928812783]
	TIME [epoch: 11.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2115896229847984		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 3.2115896229847984 | validation: 4.22004610626127]
	TIME [epoch: 11.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216491988263508		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 3.216491988263508 | validation: 4.213422760229707]
	TIME [epoch: 11.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2165624542584696		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 3.2165624542584696 | validation: 4.206082315689738]
	TIME [epoch: 11.5 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210461173774333		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 3.210461173774333 | validation: 4.210192997327908]
	TIME [epoch: 11.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2159428198123705		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 3.2159428198123705 | validation: 4.2212703700861995]
	TIME [epoch: 11.5 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215970433958526		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 3.215970433958526 | validation: 4.217244534762656]
	TIME [epoch: 11.5 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216678951294692		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 3.216678951294692 | validation: 4.2217595992885695]
	TIME [epoch: 11.5 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.229330823602092		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 3.229330823602092 | validation: 4.215757274647907]
	TIME [epoch: 11.5 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209205508096826		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 3.209205508096826 | validation: 4.204228735131975]
	TIME [epoch: 11.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209806846774039		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 3.209806846774039 | validation: 4.202349407776043]
	TIME [epoch: 11.5 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093557860057014		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 3.2093557860057014 | validation: 4.197367546921119]
	TIME [epoch: 11.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2109979941792384		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 3.2109979941792384 | validation: 4.204832517522063]
	TIME [epoch: 11.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2087547569446215		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 3.2087547569446215 | validation: 4.207959407057244]
	TIME [epoch: 11.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213650705293737		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 3.213650705293737 | validation: 4.211419811075769]
	TIME [epoch: 11.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215584511702014		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 3.215584511702014 | validation: 4.198911149927094]
	TIME [epoch: 11.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2217402812234606		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 3.2217402812234606 | validation: 4.207178658152982]
	TIME [epoch: 11.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064570473481107		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 3.2064570473481107 | validation: 4.200877614154069]
	TIME [epoch: 11.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208342926537801		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 3.208342926537801 | validation: 4.209539792919467]
	TIME [epoch: 11.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21291682914293		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 3.21291682914293 | validation: 4.1999682397480775]
	TIME [epoch: 11.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20964554727394		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 3.20964554727394 | validation: 4.2079009675427645]
	TIME [epoch: 11.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205592317337855		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 3.205592317337855 | validation: 4.198558625756349]
	TIME [epoch: 11.5 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211320631991204		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 3.211320631991204 | validation: 4.19865419013671]
	TIME [epoch: 11.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209937950414363		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 3.209937950414363 | validation: 4.20023231708043]
	TIME [epoch: 11.5 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219477745799832		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 3.219477745799832 | validation: 4.210819500688518]
	TIME [epoch: 11.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2197291904026013		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 3.2197291904026013 | validation: 4.198102099022278]
	TIME [epoch: 11.5 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210815342617228		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 3.210815342617228 | validation: 4.203138427919361]
	TIME [epoch: 11.5 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211306743652882		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 3.211306743652882 | validation: 4.201453063164029]
	TIME [epoch: 11.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207003327547574		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 3.207003327547574 | validation: 4.20411156409898]
	TIME [epoch: 11.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105490535612597		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 3.2105490535612597 | validation: 4.206182436605007]
	TIME [epoch: 11.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2114915976745424		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 3.2114915976745424 | validation: 4.201240548990066]
	TIME [epoch: 11.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088754021021844		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 3.2088754021021844 | validation: 4.201625454709803]
	TIME [epoch: 11.5 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2092371162289446		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 3.2092371162289446 | validation: 4.207980042160868]
	TIME [epoch: 11.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093469863785584		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 3.2093469863785584 | validation: 4.20169032116472]
	TIME [epoch: 11.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2156462263982157		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 3.2156462263982157 | validation: 4.204438674384197]
	TIME [epoch: 11.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2100270864424636		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 3.2100270864424636 | validation: 4.206567246157185]
	TIME [epoch: 11.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2086273406710313		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 3.2086273406710313 | validation: 4.200771215456143]
	TIME [epoch: 11.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207224189252552		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 3.207224189252552 | validation: 4.2052563980291175]
	TIME [epoch: 11.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2118861137662194		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 3.2118861137662194 | validation: 4.2105284444796585]
	TIME [epoch: 11.5 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105636417345282		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 3.2105636417345282 | validation: 4.2174615534027495]
	TIME [epoch: 11.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2217199754185257		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 3.2217199754185257 | validation: 4.212718769440662]
	TIME [epoch: 11.5 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209952111310017		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 3.209952111310017 | validation: 4.202344407586826]
	TIME [epoch: 11.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077295682174753		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 3.2077295682174753 | validation: 4.2052835541196325]
	TIME [epoch: 11.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208255943487388		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 3.208255943487388 | validation: 4.20933391581224]
	TIME [epoch: 11.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085833006743467		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 3.2085833006743467 | validation: 4.202771984771891]
	TIME [epoch: 11.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062652628099357		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 3.2062652628099357 | validation: 4.211693470948764]
	TIME [epoch: 11.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088621232658197		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 3.2088621232658197 | validation: 4.207407387931366]
	TIME [epoch: 11.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2086231844619353		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 3.2086231844619353 | validation: 4.207397832198164]
	TIME [epoch: 11.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213603633084225		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 3.213603633084225 | validation: 4.219998962978739]
	TIME [epoch: 11.5 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2171688455791214		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 3.2171688455791214 | validation: 4.201997335981914]
	TIME [epoch: 11.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071369634975913		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 3.2071369634975913 | validation: 4.2079868421615]
	TIME [epoch: 11.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212597360509718		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 3.212597360509718 | validation: 4.208615139735392]
	TIME [epoch: 11.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209843877925206		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 3.209843877925206 | validation: 4.2018983361022375]
	TIME [epoch: 11.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2109148234465836		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 3.2109148234465836 | validation: 4.211689721437181]
	TIME [epoch: 11.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216407292288966		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 3.216407292288966 | validation: 4.217674604708515]
	TIME [epoch: 11.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2183709872808453		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 3.2183709872808453 | validation: 4.2041704514014935]
	TIME [epoch: 11.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2156141881693925		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 3.2156141881693925 | validation: 4.212740792964934]
	TIME [epoch: 11.5 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214805701046327		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 3.214805701046327 | validation: 4.201709454058222]
	TIME [epoch: 11.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205253357546407		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 3.205253357546407 | validation: 4.193464393829718]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_1200.pth
	Model improved!!!
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21220541261167		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 3.21220541261167 | validation: 4.207223846038253]
	TIME [epoch: 11.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2103272698124656		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 3.2103272698124656 | validation: 4.19676465635662]
	TIME [epoch: 11.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207419241376572		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 3.207419241376572 | validation: 4.204638748416461]
	TIME [epoch: 11.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22110477599359		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 3.22110477599359 | validation: 4.202568454142657]
	TIME [epoch: 11.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2155645800651476		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 3.2155645800651476 | validation: 4.212880706569597]
	TIME [epoch: 11.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214228640378934		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 3.214228640378934 | validation: 4.203654748292551]
	TIME [epoch: 11.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209446029080209		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 3.209446029080209 | validation: 4.206757100310987]
	TIME [epoch: 11.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2080260515785786		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 3.2080260515785786 | validation: 4.205702489188914]
	TIME [epoch: 11.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2118688678839504		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 3.2118688678839504 | validation: 4.206755966347191]
	TIME [epoch: 11.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2199204775833223		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 3.2199204775833223 | validation: 4.206017265431136]
	TIME [epoch: 11.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2084273366675884		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 3.2084273366675884 | validation: 4.200814825512333]
	TIME [epoch: 11.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2111373445432423		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 3.2111373445432423 | validation: 4.210094804353022]
	TIME [epoch: 11.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2151166446513804		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 3.2151166446513804 | validation: 4.213170667493708]
	TIME [epoch: 11.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105653391811106		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 3.2105653391811106 | validation: 4.199670177208518]
	TIME [epoch: 11.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2091808632605665		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 3.2091808632605665 | validation: 4.202048277059533]
	TIME [epoch: 11.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.22059205353196		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 3.22059205353196 | validation: 4.206202382489092]
	TIME [epoch: 11.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214170476098318		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 3.214170476098318 | validation: 4.204507035231552]
	TIME [epoch: 11.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2138372690017114		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 3.2138372690017114 | validation: 4.206849595474313]
	TIME [epoch: 11.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070691872337993		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 3.2070691872337993 | validation: 4.203804653416788]
	TIME [epoch: 11.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209884734660406		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 3.209884734660406 | validation: 4.202060724953297]
	TIME [epoch: 11.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21139361655362		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 3.21139361655362 | validation: 4.204829310522734]
	TIME [epoch: 11.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207690187360278		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 3.207690187360278 | validation: 4.204604915256723]
	TIME [epoch: 11.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2112730667866076		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 3.2112730667866076 | validation: 4.200887141025393]
	TIME [epoch: 11.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085400497378567		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 3.2085400497378567 | validation: 4.201032275964849]
	TIME [epoch: 11.5 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2087433521790207		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 3.2087433521790207 | validation: 4.2145993656929415]
	TIME [epoch: 11.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213975358011033		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 3.213975358011033 | validation: 4.204289255500781]
	TIME [epoch: 11.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083232640031625		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 3.2083232640031625 | validation: 4.202911265119336]
	TIME [epoch: 11.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207854667443067		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 3.207854667443067 | validation: 4.200333172295173]
	TIME [epoch: 11.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105290055179942		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 3.2105290055179942 | validation: 4.204524631537433]
	TIME [epoch: 11.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207510875657194		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 3.207510875657194 | validation: 4.194415432314945]
	TIME [epoch: 11.5 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204628446768005		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 3.204628446768005 | validation: 4.204761899903614]
	TIME [epoch: 11.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062502737532923		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 3.2062502737532923 | validation: 4.203101548266615]
	TIME [epoch: 11.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2114774635320606		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 3.2114774635320606 | validation: 4.200547902520773]
	TIME [epoch: 11.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205967890321192		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 3.205967890321192 | validation: 4.2065783595343476]
	TIME [epoch: 11.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093723589974226		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 3.2093723589974226 | validation: 4.202459021345805]
	TIME [epoch: 11.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.216002409156352		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 3.216002409156352 | validation: 4.2068037435705445]
	TIME [epoch: 11.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076387021281616		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 3.2076387021281616 | validation: 4.20002781060484]
	TIME [epoch: 11.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207123960505698		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 3.207123960505698 | validation: 4.202127212967955]
	TIME [epoch: 11.5 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205258732787927		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 3.205258732787927 | validation: 4.2017684528543295]
	TIME [epoch: 11.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211670817651304		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 3.211670817651304 | validation: 4.196107204939162]
	TIME [epoch: 11.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2147833204579275		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 3.2147833204579275 | validation: 4.204356783327031]
	TIME [epoch: 11.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211813219773181		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 3.211813219773181 | validation: 4.209209707740852]
	TIME [epoch: 11.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211521972368316		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 3.211521972368316 | validation: 4.21101057677642]
	TIME [epoch: 11.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21159381370633		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 3.21159381370633 | validation: 4.206564371603368]
	TIME [epoch: 11.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213945022828556		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 3.213945022828556 | validation: 4.199904022894075]
	TIME [epoch: 11.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2058823725729417		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 3.2058823725729417 | validation: 4.20007241033565]
	TIME [epoch: 11.5 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209094878452072		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 3.209094878452072 | validation: 4.200905405072915]
	TIME [epoch: 11.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207428024099391		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 3.207428024099391 | validation: 4.203941055181997]
	TIME [epoch: 11.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210529738127639		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 3.210529738127639 | validation: 4.202049779788315]
	TIME [epoch: 11.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207057629196914		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 3.207057629196914 | validation: 4.198954715983123]
	TIME [epoch: 11.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061483439633585		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 3.2061483439633585 | validation: 4.200886944616303]
	TIME [epoch: 11.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208684694628131		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 3.208684694628131 | validation: 4.2000370219175185]
	TIME [epoch: 11.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204913702488213		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 3.204913702488213 | validation: 4.203165837254671]
	TIME [epoch: 11.5 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2139506779685107		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 3.2139506779685107 | validation: 4.200190734866372]
	TIME [epoch: 11.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209446343711652		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 3.209446343711652 | validation: 4.195597348162693]
	TIME [epoch: 11.5 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2101777181014333		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 3.2101777181014333 | validation: 4.207884675582099]
	TIME [epoch: 11.5 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2151723454514882		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 3.2151723454514882 | validation: 4.203222365289251]
	TIME [epoch: 11.5 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2148714632632687		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 3.2148714632632687 | validation: 4.20416621239772]
	TIME [epoch: 11.5 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2110872664766985		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 3.2110872664766985 | validation: 4.2058463326325395]
	TIME [epoch: 11.5 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2101287916523624		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 3.2101287916523624 | validation: 4.195541622562397]
	TIME [epoch: 11.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210294713211125		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 3.210294713211125 | validation: 4.204989640855208]
	TIME [epoch: 11.5 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214366942855962		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 3.214366942855962 | validation: 4.214117292047264]
	TIME [epoch: 11.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215128021885642		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 3.215128021885642 | validation: 4.207947422617348]
	TIME [epoch: 11.5 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2116025833131734		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 3.2116025833131734 | validation: 4.202709472039407]
	TIME [epoch: 11.5 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210898938216922		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 3.210898938216922 | validation: 4.195389250901189]
	TIME [epoch: 11.5 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208759517262353		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 3.208759517262353 | validation: 4.202097718775694]
	TIME [epoch: 11.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2104001242605453		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 3.2104001242605453 | validation: 4.204994099043522]
	TIME [epoch: 11.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079934981495453		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 3.2079934981495453 | validation: 4.210148253655345]
	TIME [epoch: 11.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097657012446716		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 3.2097657012446716 | validation: 4.201673227729864]
	TIME [epoch: 11.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2094026564119784		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 3.2094026564119784 | validation: 4.197148137455062]
	TIME [epoch: 11.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078997714645676		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 3.2078997714645676 | validation: 4.204742140971643]
	TIME [epoch: 11.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2100922060833104		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 3.2100922060833104 | validation: 4.209139466571863]
	TIME [epoch: 11.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2261370091294834		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 3.2261370091294834 | validation: 4.218581170165922]
	TIME [epoch: 11.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2212560894716087		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 3.2212560894716087 | validation: 4.203065894197151]
	TIME [epoch: 11.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2132111398466674		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 3.2132111398466674 | validation: 4.211557546296653]
	TIME [epoch: 11.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214109447257035		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 3.214109447257035 | validation: 4.190657695567891]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_1276.pth
	Model improved!!!
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209027508864126		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 3.209027508864126 | validation: 4.196077000576056]
	TIME [epoch: 11.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066961629292283		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 3.2066961629292283 | validation: 4.2035625758594275]
	TIME [epoch: 11.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207217909635782		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 3.207217909635782 | validation: 4.20974171592836]
	TIME [epoch: 11.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079893596429843		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 3.2079893596429843 | validation: 4.202116082844826]
	TIME [epoch: 11.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051437593387986		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 3.2051437593387986 | validation: 4.203594633259317]
	TIME [epoch: 11.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210178011834715		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 3.210178011834715 | validation: 4.2045320131986195]
	TIME [epoch: 11.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20777285773591		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 3.20777285773591 | validation: 4.2086704596843205]
	TIME [epoch: 11.5 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093594378874997		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 3.2093594378874997 | validation: 4.202361174814195]
	TIME [epoch: 11.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215983341773331		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 3.215983341773331 | validation: 4.207623991688424]
	TIME [epoch: 11.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097311386742757		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 3.2097311386742757 | validation: 4.20824492989204]
	TIME [epoch: 11.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21214449439251		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 3.21214449439251 | validation: 4.21053544338147]
	TIME [epoch: 11.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056219483064803		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 3.2056219483064803 | validation: 4.202683999486924]
	TIME [epoch: 11.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205355232049267		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 3.205355232049267 | validation: 4.209193628933741]
	TIME [epoch: 11.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2107676619463206		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 3.2107676619463206 | validation: 4.216207191409764]
	TIME [epoch: 11.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2147673887329242		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 3.2147673887329242 | validation: 4.209372273918551]
	TIME [epoch: 11.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207813986135476		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 3.207813986135476 | validation: 4.196190845253375]
	TIME [epoch: 11.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204887657902064		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 3.204887657902064 | validation: 4.204708475480973]
	TIME [epoch: 11.5 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210796115148044		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 3.210796115148044 | validation: 4.205829695278941]
	TIME [epoch: 11.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208959930687053		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 3.208959930687053 | validation: 4.199173224105302]
	TIME [epoch: 11.5 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20913593751044		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 3.20913593751044 | validation: 4.21414149291058]
	TIME [epoch: 11.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2111029070648276		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 3.2111029070648276 | validation: 4.213430229941323]
	TIME [epoch: 11.5 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.219780315435848		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 3.219780315435848 | validation: 4.215145434967212]
	TIME [epoch: 11.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2147035933245323		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 3.2147035933245323 | validation: 4.205323147749532]
	TIME [epoch: 11.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210564088798059		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 3.210564088798059 | validation: 4.201905575094326]
	TIME [epoch: 11.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085037404971537		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 3.2085037404971537 | validation: 4.200782685195098]
	TIME [epoch: 11.5 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205932331027085		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 3.205932331027085 | validation: 4.1984394383307855]
	TIME [epoch: 11.5 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20306849315447		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 3.20306849315447 | validation: 4.2002707887433965]
	TIME [epoch: 11.5 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206590509552096		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 3.206590509552096 | validation: 4.200884109386679]
	TIME [epoch: 11.5 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208975775176824		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 3.208975775176824 | validation: 4.19813201629992]
	TIME [epoch: 11.5 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204827994136788		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 3.204827994136788 | validation: 4.211268446166383]
	TIME [epoch: 11.5 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2091021293329574		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 3.2091021293329574 | validation: 4.1974288238543505]
	TIME [epoch: 11.5 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205386174705665		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 3.205386174705665 | validation: 4.203979874870122]
	TIME [epoch: 11.5 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054643096217745		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 3.2054643096217745 | validation: 4.208767964682939]
	TIME [epoch: 11.5 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066631155345138		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 3.2066631155345138 | validation: 4.206148631431143]
	TIME [epoch: 11.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093123341689394		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 3.2093123341689394 | validation: 4.2074716833457995]
	TIME [epoch: 11.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2132787244297143		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 3.2132787244297143 | validation: 4.21413885911883]
	TIME [epoch: 11.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2104687813682333		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 3.2104687813682333 | validation: 4.203273076123669]
	TIME [epoch: 11.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209400822887517		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 3.209400822887517 | validation: 4.200348483355767]
	TIME [epoch: 11.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105375986874063		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 3.2105375986874063 | validation: 4.2125898483076565]
	TIME [epoch: 11.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067755122765167		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 3.2067755122765167 | validation: 4.2050189499807225]
	TIME [epoch: 11.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2090399039164996		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 3.2090399039164996 | validation: 4.201078813224503]
	TIME [epoch: 11.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093394308283623		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 3.2093394308283623 | validation: 4.204395303441189]
	TIME [epoch: 11.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2104190196955935		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 3.2104190196955935 | validation: 4.203511172732687]
	TIME [epoch: 11.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2119244316818345		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 3.2119244316818345 | validation: 4.206880462161579]
	TIME [epoch: 11.5 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2087913222178948		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 3.2087913222178948 | validation: 4.205500352586242]
	TIME [epoch: 11.5 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2052687503039428		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 3.2052687503039428 | validation: 4.203148234136297]
	TIME [epoch: 11.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2086665041313442		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 3.2086665041313442 | validation: 4.204957934635085]
	TIME [epoch: 11.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213462295653639		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 3.213462295653639 | validation: 4.211729260922877]
	TIME [epoch: 11.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206898887488358		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 3.206898887488358 | validation: 4.205687721841116]
	TIME [epoch: 11.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20860190985978		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 3.20860190985978 | validation: 4.210073053471622]
	TIME [epoch: 11.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206493975050097		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 3.206493975050097 | validation: 4.202152801390845]
	TIME [epoch: 11.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077284147073106		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 3.2077284147073106 | validation: 4.206213173883865]
	TIME [epoch: 11.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065892393349755		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 3.2065892393349755 | validation: 4.20431500753382]
	TIME [epoch: 11.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209266314932564		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 3.209266314932564 | validation: 4.200826117297869]
	TIME [epoch: 11.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2109647535546135		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 3.2109647535546135 | validation: 4.199191339993593]
	TIME [epoch: 11.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069348666286417		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 3.2069348666286417 | validation: 4.207136870268835]
	TIME [epoch: 11.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2109036101851496		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 3.2109036101851496 | validation: 4.202807825097408]
	TIME [epoch: 11.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2159946693007417		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 3.2159946693007417 | validation: 4.211404549927583]
	TIME [epoch: 11.5 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.217531355029315		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 3.217531355029315 | validation: 4.207449993652468]
	TIME [epoch: 11.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215300742999122		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 3.215300742999122 | validation: 4.201967584622301]
	TIME [epoch: 11.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2109569177229904		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 3.2109569177229904 | validation: 4.210300798789763]
	TIME [epoch: 11.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2121780025292574		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 3.2121780025292574 | validation: 4.211203884933678]
	TIME [epoch: 11.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214562121140548		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 3.214562121140548 | validation: 4.2025273207719485]
	TIME [epoch: 11.5 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209978805559962		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 3.209978805559962 | validation: 4.204116090789407]
	TIME [epoch: 11.5 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206956523325073		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 3.206956523325073 | validation: 4.2041087272267825]
	TIME [epoch: 11.5 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2081803970558798		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 3.2081803970558798 | validation: 4.20872828903576]
	TIME [epoch: 11.5 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065463327828025		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 3.2065463327828025 | validation: 4.202679945803424]
	TIME [epoch: 11.5 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2060987026214764		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 3.2060987026214764 | validation: 4.201724567773813]
	TIME [epoch: 11.5 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207116690791143		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 3.207116690791143 | validation: 4.205756079981492]
	TIME [epoch: 11.5 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069919544307006		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 3.2069919544307006 | validation: 4.204435870801266]
	TIME [epoch: 11.5 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078064110418207		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 3.2078064110418207 | validation: 4.203971119512224]
	TIME [epoch: 11.5 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065727187969166		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 3.2065727187969166 | validation: 4.2040759239174745]
	TIME [epoch: 11.5 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204831443298492		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 3.204831443298492 | validation: 4.201976305848491]
	TIME [epoch: 11.5 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210671977441109		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 3.210671977441109 | validation: 4.19281015663121]
	TIME [epoch: 11.5 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062601484299837		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 3.2062601484299837 | validation: 4.204972878481628]
	TIME [epoch: 11.5 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068251675773016		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 3.2068251675773016 | validation: 4.2018186705736635]
	TIME [epoch: 11.5 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061282749960354		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 3.2061282749960354 | validation: 4.200463968822178]
	TIME [epoch: 11.5 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2084088076902266		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 3.2084088076902266 | validation: 4.205482947967927]
	TIME [epoch: 11.5 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2146485300830014		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 3.2146485300830014 | validation: 4.2007499126004175]
	TIME [epoch: 11.5 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210895735231783		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 3.210895735231783 | validation: 4.21188972981518]
	TIME [epoch: 11.5 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212384678370853		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 3.212384678370853 | validation: 4.202919226215873]
	TIME [epoch: 11.5 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2096597642932707		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 3.2096597642932707 | validation: 4.201680770188976]
	TIME [epoch: 11.5 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204519153952387		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 3.204519153952387 | validation: 4.2007770128982465]
	TIME [epoch: 11.5 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207813132786092		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 3.207813132786092 | validation: 4.200533878343113]
	TIME [epoch: 11.5 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209227064597694		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 3.209227064597694 | validation: 4.201767081861952]
	TIME [epoch: 11.5 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207701447669775		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 3.207701447669775 | validation: 4.208552084436473]
	TIME [epoch: 11.5 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205081163603087		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 3.205081163603087 | validation: 4.200642014941028]
	TIME [epoch: 11.5 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21006885408024		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 3.21006885408024 | validation: 4.216053474784525]
	TIME [epoch: 11.5 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2086245977670935		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 3.2086245977670935 | validation: 4.197077164108082]
	TIME [epoch: 11.5 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206518035744611		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 3.206518035744611 | validation: 4.201169259403951]
	TIME [epoch: 11.5 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070124495412458		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 3.2070124495412458 | validation: 4.2008714447582625]
	TIME [epoch: 11.5 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206839756831802		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 3.206839756831802 | validation: 4.199577113997621]
	TIME [epoch: 11.5 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097497195128786		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 3.2097497195128786 | validation: 4.207430151892237]
	TIME [epoch: 11.5 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059087465145986		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 3.2059087465145986 | validation: 4.201732976921415]
	TIME [epoch: 11.5 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2099515709379025		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 3.2099515709379025 | validation: 4.204091823072482]
	TIME [epoch: 11.5 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2041235323508714		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 3.2041235323508714 | validation: 4.209547920888678]
	TIME [epoch: 11.5 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2084970213913464		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 3.2084970213913464 | validation: 4.197810382910494]
	TIME [epoch: 11.5 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2091127939633783		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 3.2091127939633783 | validation: 4.201738951918999]
	TIME [epoch: 11.5 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2075937957591023		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 3.2075937957591023 | validation: 4.204004911124522]
	TIME [epoch: 11.5 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078454939282284		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 3.2078454939282284 | validation: 4.204530215693708]
	TIME [epoch: 11.5 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073167381128442		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 3.2073167381128442 | validation: 4.199102548601364]
	TIME [epoch: 11.5 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2031535100272235		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 3.2031535100272235 | validation: 4.205134675824569]
	TIME [epoch: 11.5 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089722503794866		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 3.2089722503794866 | validation: 4.202984124646417]
	TIME [epoch: 11.5 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055148196141996		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 3.2055148196141996 | validation: 4.206261596374242]
	TIME [epoch: 11.5 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073624826686493		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 3.2073624826686493 | validation: 4.209979937176687]
	TIME [epoch: 11.5 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2131020598312667		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 3.2131020598312667 | validation: 4.2064573078862875]
	TIME [epoch: 11.5 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2121682544752495		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 3.2121682544752495 | validation: 4.198459148703752]
	TIME [epoch: 11.5 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208090704773421		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 3.208090704773421 | validation: 4.199487398654443]
	TIME [epoch: 11.5 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204121341246757		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 3.204121341246757 | validation: 4.206991344132559]
	TIME [epoch: 11.5 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2133976343912733		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 3.2133976343912733 | validation: 4.211856435926235]
	TIME [epoch: 11.5 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2138099684792722		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 3.2138099684792722 | validation: 4.201370578352717]
	TIME [epoch: 11.5 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055428454169266		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 3.2055428454169266 | validation: 4.199319943010217]
	TIME [epoch: 11.5 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069029793611055		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 3.2069029793611055 | validation: 4.2065812552287145]
	TIME [epoch: 11.5 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212111546464232		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 3.212111546464232 | validation: 4.204498289760816]
	TIME [epoch: 11.5 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2103359516227554		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 3.2103359516227554 | validation: 4.206800911790163]
	TIME [epoch: 11.5 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077808328262476		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 3.2077808328262476 | validation: 4.2019577749983394]
	TIME [epoch: 11.5 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208696001893801		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 3.208696001893801 | validation: 4.205861770050813]
	TIME [epoch: 11.5 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070685913438473		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 3.2070685913438473 | validation: 4.197264920617173]
	TIME [epoch: 11.5 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210037985625428		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 3.210037985625428 | validation: 4.208770940803685]
	TIME [epoch: 11.5 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083449437747635		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 3.2083449437747635 | validation: 4.204724777489835]
	TIME [epoch: 11.5 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056221566342433		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 3.2056221566342433 | validation: 4.198040575756995]
	TIME [epoch: 11.5 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206000113953612		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 3.206000113953612 | validation: 4.20183347270123]
	TIME [epoch: 11.5 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209528263466733		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 3.209528263466733 | validation: 4.204744876169579]
	TIME [epoch: 11.5 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205961481041453		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 3.205961481041453 | validation: 4.202681520081035]
	TIME [epoch: 11.5 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20782270286541		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 3.20782270286541 | validation: 4.198681551036693]
	TIME [epoch: 11.5 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085935326154535		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 3.2085935326154535 | validation: 4.203754196046167]
	TIME [epoch: 11.5 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097760337794288		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 3.2097760337794288 | validation: 4.203136667663911]
	TIME [epoch: 11.5 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061673967923277		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 3.2061673967923277 | validation: 4.198279136507328]
	TIME [epoch: 11.5 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208212568345325		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 3.208212568345325 | validation: 4.200792672611955]
	TIME [epoch: 11.5 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205734637955581		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 3.205734637955581 | validation: 4.209454574061664]
	TIME [epoch: 11.5 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056130191979837		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 3.2056130191979837 | validation: 4.204084431896698]
	TIME [epoch: 11.5 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204079905027882		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 3.204079905027882 | validation: 4.210703740163047]
	TIME [epoch: 11.5 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2041393590843676		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 3.2041393590843676 | validation: 4.205072277329277]
	TIME [epoch: 11.5 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066179599568487		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 3.2066179599568487 | validation: 4.203384480002603]
	TIME [epoch: 11.5 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069742104912065		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 3.2069742104912065 | validation: 4.19838761391326]
	TIME [epoch: 11.5 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050929046534677		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 3.2050929046534677 | validation: 4.20176263554393]
	TIME [epoch: 11.5 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207195193072115		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 3.207195193072115 | validation: 4.208135967098711]
	TIME [epoch: 11.5 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207747875097183		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 3.207747875097183 | validation: 4.2069054358820805]
	TIME [epoch: 11.5 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2100413776684267		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 3.2100413776684267 | validation: 4.215450270333056]
	TIME [epoch: 11.5 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212635814488954		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 3.212635814488954 | validation: 4.210544778970734]
	TIME [epoch: 11.5 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214770679498269		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 3.214770679498269 | validation: 4.2081762024437]
	TIME [epoch: 11.5 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208549518480973		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 3.208549518480973 | validation: 4.197814226026334]
	TIME [epoch: 11.5 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078099104974376		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 3.2078099104974376 | validation: 4.205773844904857]
	TIME [epoch: 11.5 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206396218181184		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 3.206396218181184 | validation: 4.206295503932048]
	TIME [epoch: 11.5 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206575021834222		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 3.206575021834222 | validation: 4.201199335752224]
	TIME [epoch: 11.5 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205872016648186		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 3.205872016648186 | validation: 4.201625587528083]
	TIME [epoch: 11.5 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212868864478146		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 3.212868864478146 | validation: 4.20849106097242]
	TIME [epoch: 11.5 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210825225687597		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 3.210825225687597 | validation: 4.203875401340386]
	TIME [epoch: 11.5 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065189074695413		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 3.2065189074695413 | validation: 4.206344665796889]
	TIME [epoch: 11.5 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089084036955553		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 3.2089084036955553 | validation: 4.197602647479258]
	TIME [epoch: 11.5 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2094279488686883		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 3.2094279488686883 | validation: 4.2000339511052776]
	TIME [epoch: 11.5 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214717453699561		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 3.214717453699561 | validation: 4.207683363345549]
	TIME [epoch: 11.5 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.215761812858643		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 3.215761812858643 | validation: 4.205187081422588]
	TIME [epoch: 11.5 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2116820134252886		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 3.2116820134252886 | validation: 4.212832181749942]
	TIME [epoch: 11.5 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212317299459452		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 3.212317299459452 | validation: 4.21018326390804]
	TIME [epoch: 11.5 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077607148251945		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 3.2077607148251945 | validation: 4.206436201439536]
	TIME [epoch: 11.5 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077773326988135		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 3.2077773326988135 | validation: 4.203273340922534]
	TIME [epoch: 11.5 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207637721396557		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 3.207637721396557 | validation: 4.209968856342017]
	TIME [epoch: 11.5 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2058267102334717		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 3.2058267102334717 | validation: 4.2071352775592485]
	TIME [epoch: 11.5 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206965765936699		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 3.206965765936699 | validation: 4.2068476670632675]
	TIME [epoch: 11.5 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206207644557895		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 3.206207644557895 | validation: 4.196583050804319]
	TIME [epoch: 11.5 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214499902280034		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 3.214499902280034 | validation: 4.209066134533389]
	TIME [epoch: 11.5 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210568016010779		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 3.210568016010779 | validation: 4.198983300383212]
	TIME [epoch: 11.5 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2099491146477632		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 3.2099491146477632 | validation: 4.201811683256952]
	TIME [epoch: 11.5 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064918907130764		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 3.2064918907130764 | validation: 4.209119281010785]
	TIME [epoch: 11.5 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066046136935826		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 3.2066046136935826 | validation: 4.204260829361002]
	TIME [epoch: 11.5 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208647394079308		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 3.208647394079308 | validation: 4.202518680691405]
	TIME [epoch: 11.5 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088576056409255		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 3.2088576056409255 | validation: 4.205487167193187]
	TIME [epoch: 11.5 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208185306497018		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 3.208185306497018 | validation: 4.198199182921967]
	TIME [epoch: 11.5 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207015989305856		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 3.207015989305856 | validation: 4.2043007735669145]
	TIME [epoch: 11.5 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209115832755661		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 3.209115832755661 | validation: 4.200509522115211]
	TIME [epoch: 11.5 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208422845587578		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 3.208422845587578 | validation: 4.209912802437811]
	TIME [epoch: 11.5 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2103841615084354		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 3.2103841615084354 | validation: 4.201170440615895]
	TIME [epoch: 11.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083511592832203		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 3.2083511592832203 | validation: 4.203686170258045]
	TIME [epoch: 11.5 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2110783028419694		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 3.2110783028419694 | validation: 4.203631735620468]
	TIME [epoch: 11.5 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.21184936698595		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 3.21184936698595 | validation: 4.200335152982135]
	TIME [epoch: 11.5 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089022161202108		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 3.2089022161202108 | validation: 4.196761360874996]
	TIME [epoch: 11.5 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207191104654336		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 3.207191104654336 | validation: 4.195843966492568]
	TIME [epoch: 11.5 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20842416239085		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 3.20842416239085 | validation: 4.197868519703852]
	TIME [epoch: 11.5 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078184930883786		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 3.2078184930883786 | validation: 4.205011997197056]
	TIME [epoch: 11.5 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208237294089392		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 3.208237294089392 | validation: 4.203516146192]
	TIME [epoch: 11.5 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2092755952219094		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 3.2092755952219094 | validation: 4.200886093789774]
	TIME [epoch: 11.5 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211312433904563		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 3.211312433904563 | validation: 4.205596794545182]
	TIME [epoch: 11.5 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205615367661494		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 3.205615367661494 | validation: 4.2060065595535825]
	TIME [epoch: 11.5 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2101437255979395		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 3.2101437255979395 | validation: 4.19935948888632]
	TIME [epoch: 11.5 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208073574153643		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 3.208073574153643 | validation: 4.200783760786424]
	TIME [epoch: 11.5 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205449416138417		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 3.205449416138417 | validation: 4.203105929224859]
	TIME [epoch: 11.5 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2095974117179353		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 3.2095974117179353 | validation: 4.211333443821153]
	TIME [epoch: 11.5 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20968323878431		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 3.20968323878431 | validation: 4.205418983828385]
	TIME [epoch: 11.5 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209224613578256		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 3.209224613578256 | validation: 4.203813525714489]
	TIME [epoch: 11.5 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205804702710488		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 3.205804702710488 | validation: 4.2070659534743395]
	TIME [epoch: 11.5 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2034891690201355		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 3.2034891690201355 | validation: 4.202270337387597]
	TIME [epoch: 11.5 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056857535225305		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 3.2056857535225305 | validation: 4.21021322921504]
	TIME [epoch: 11.5 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068324425834778		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 3.2068324425834778 | validation: 4.20883631617983]
	TIME [epoch: 11.5 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205604512728544		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 3.205604512728544 | validation: 4.203214453430933]
	TIME [epoch: 11.5 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069138375725696		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 3.2069138375725696 | validation: 4.2014144631409645]
	TIME [epoch: 11.5 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205930805787361		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 3.205930805787361 | validation: 4.202229703206423]
	TIME [epoch: 11.5 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205645653959423		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 3.205645653959423 | validation: 4.203422801328642]
	TIME [epoch: 11.5 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066641294971374		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 3.2066641294971374 | validation: 4.200305623490616]
	TIME [epoch: 11.5 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2108316864129627		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 3.2108316864129627 | validation: 4.203884424266629]
	TIME [epoch: 11.5 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2122530972935928		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 3.2122530972935928 | validation: 4.207957036295441]
	TIME [epoch: 11.5 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073147716621624		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 3.2073147716621624 | validation: 4.2043259885278745]
	TIME [epoch: 11.5 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2105250734261768		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 3.2105250734261768 | validation: 4.20283297534593]
	TIME [epoch: 11.5 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211218584590392		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 3.211218584590392 | validation: 4.205089380771872]
	TIME [epoch: 11.5 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208471376394167		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 3.208471376394167 | validation: 4.206593226681026]
	TIME [epoch: 11.5 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057814162182225		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 3.2057814162182225 | validation: 4.200291773388447]
	TIME [epoch: 11.5 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076272278276825		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 3.2076272278276825 | validation: 4.199413565721792]
	TIME [epoch: 11.5 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2027330303810553		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 3.2027330303810553 | validation: 4.208098064997541]
	TIME [epoch: 11.5 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20673220811972		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 3.20673220811972 | validation: 4.200645716004598]
	TIME [epoch: 11.5 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205980836724871		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 3.205980836724871 | validation: 4.199014475805775]
	TIME [epoch: 11.5 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2040197482924953		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 3.2040197482924953 | validation: 4.202536554779298]
	TIME [epoch: 11.5 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205473822618472		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 3.205473822618472 | validation: 4.202481745544672]
	TIME [epoch: 11.5 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083097546638553		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 3.2083097546638553 | validation: 4.197883703154041]
	TIME [epoch: 11.5 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207234615556058		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 3.207234615556058 | validation: 4.205494642258747]
	TIME [epoch: 11.5 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208120955950407		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 3.208120955950407 | validation: 4.196976831904685]
	TIME [epoch: 11.5 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207604758618798		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 3.207604758618798 | validation: 4.207469994730899]
	TIME [epoch: 11.5 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208164440377687		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 3.208164440377687 | validation: 4.211257234297754]
	TIME [epoch: 11.5 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20480677390394		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 3.20480677390394 | validation: 4.2110461269995145]
	TIME [epoch: 11.5 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2106496404901526		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 3.2106496404901526 | validation: 4.212419981743907]
	TIME [epoch: 11.5 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088475088449924		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 3.2088475088449924 | validation: 4.203702971991227]
	TIME [epoch: 11.5 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2123795449325683		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 3.2123795449325683 | validation: 4.205396989486003]
	TIME [epoch: 11.5 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20747975699817		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 3.20747975699817 | validation: 4.204164971386889]
	TIME [epoch: 11.5 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079797584780083		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 3.2079797584780083 | validation: 4.2105225541800175]
	TIME [epoch: 11.5 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065588483100624		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 3.2065588483100624 | validation: 4.203276988674445]
	TIME [epoch: 11.5 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205147280212532		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 3.205147280212532 | validation: 4.210250987273731]
	TIME [epoch: 11.5 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062280943369936		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 3.2062280943369936 | validation: 4.207120948430725]
	TIME [epoch: 11.5 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209718902555768		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 3.209718902555768 | validation: 4.2038903910739105]
	TIME [epoch: 11.5 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057208772070647		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 3.2057208772070647 | validation: 4.200576462130763]
	TIME [epoch: 11.5 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2121140823139935		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 3.2121140823139935 | validation: 4.197677880449036]
	TIME [epoch: 11.5 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207672014254524		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 3.207672014254524 | validation: 4.199314099370048]
	TIME [epoch: 11.5 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204365215110319		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 3.204365215110319 | validation: 4.202453222791155]
	TIME [epoch: 11.5 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206834690403505		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 3.206834690403505 | validation: 4.2034558579752845]
	TIME [epoch: 11.5 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203058427714484		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 3.203058427714484 | validation: 4.202205058413344]
	TIME [epoch: 11.5 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2075650014630903		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 3.2075650014630903 | validation: 4.208807671525129]
	TIME [epoch: 11.5 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20934719004389		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 3.20934719004389 | validation: 4.205759319035175]
	TIME [epoch: 11.5 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209057019567769		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 3.209057019567769 | validation: 4.199181851394401]
	TIME [epoch: 11.5 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2053225879072476		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 3.2053225879072476 | validation: 4.204434842482414]
	TIME [epoch: 11.5 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050620171201616		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 3.2050620171201616 | validation: 4.201890672316938]
	TIME [epoch: 11.5 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208123162514311		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 3.208123162514311 | validation: 4.206374393795451]
	TIME [epoch: 11.5 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093593716619875		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 3.2093593716619875 | validation: 4.201193325747385]
	TIME [epoch: 11.5 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.214107580861923		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 3.214107580861923 | validation: 4.207675606919468]
	TIME [epoch: 11.5 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085440236504663		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 3.2085440236504663 | validation: 4.20281216632206]
	TIME [epoch: 11.5 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076161536109518		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 3.2076161536109518 | validation: 4.200654406004174]
	TIME [epoch: 11.5 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208551647896521		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 3.208551647896521 | validation: 4.204812098226942]
	TIME [epoch: 11.5 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208764503728907		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 3.208764503728907 | validation: 4.2005433720914445]
	TIME [epoch: 11.5 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208377093420666		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 3.208377093420666 | validation: 4.203962563910607]
	TIME [epoch: 11.5 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2095375603950553		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 3.2095375603950553 | validation: 4.201004541835513]
	TIME [epoch: 11.5 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206997708621082		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 3.206997708621082 | validation: 4.200896982816557]
	TIME [epoch: 11.5 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2025513703969373		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 3.2025513703969373 | validation: 4.2018955763518635]
	TIME [epoch: 11.5 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2082668056804513		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 3.2082668056804513 | validation: 4.201268210248203]
	TIME [epoch: 11.5 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068506548855433		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 3.2068506548855433 | validation: 4.20148148770078]
	TIME [epoch: 11.5 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2048594576101608		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 3.2048594576101608 | validation: 4.201934040346428]
	TIME [epoch: 11.5 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204200312412185		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 3.204200312412185 | validation: 4.2089090169714884]
	TIME [epoch: 11.5 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076507894067587		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 3.2076507894067587 | validation: 4.19932211880689]
	TIME [epoch: 11.5 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077686256965814		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 3.2077686256965814 | validation: 4.196157190613373]
	TIME [epoch: 11.5 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205273722466236		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 3.205273722466236 | validation: 4.202949050443352]
	TIME [epoch: 11.5 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045934115973127		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 3.2045934115973127 | validation: 4.2041369115337766]
	TIME [epoch: 11.5 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207438053271579		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 3.207438053271579 | validation: 4.206065562311487]
	TIME [epoch: 11.5 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207363770309286		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 3.207363770309286 | validation: 4.203698434682232]
	TIME [epoch: 11.5 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068116201260386		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 3.2068116201260386 | validation: 4.200121783679142]
	TIME [epoch: 11.5 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206711147819501		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 3.206711147819501 | validation: 4.203770462776344]
	TIME [epoch: 11.5 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206017915388606		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 3.206017915388606 | validation: 4.207507767737259]
	TIME [epoch: 11.5 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2037638766941656		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 3.2037638766941656 | validation: 4.207550656858274]
	TIME [epoch: 11.5 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073968392526004		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 3.2073968392526004 | validation: 4.208255186773967]
	TIME [epoch: 11.5 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207200402609608		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 3.207200402609608 | validation: 4.205112660120723]
	TIME [epoch: 11.5 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206645818103741		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 3.206645818103741 | validation: 4.202855269782189]
	TIME [epoch: 11.5 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2081359770805613		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 3.2081359770805613 | validation: 4.196187244554272]
	TIME [epoch: 11.5 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206581974976465		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 3.206581974976465 | validation: 4.204732796113965]
	TIME [epoch: 11.5 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054420667334487		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 3.2054420667334487 | validation: 4.198749135545419]
	TIME [epoch: 11.5 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207849244979186		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 3.207849244979186 | validation: 4.206388781082848]
	TIME [epoch: 11.5 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207487563256528		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 3.207487563256528 | validation: 4.200365694351169]
	TIME [epoch: 11.5 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207212081045319		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 3.207212081045319 | validation: 4.20068608699472]
	TIME [epoch: 11.5 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088543011135533		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 3.2088543011135533 | validation: 4.204366288098323]
	TIME [epoch: 11.5 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207621372502733		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 3.207621372502733 | validation: 4.204356463720623]
	TIME [epoch: 11.5 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2084620727254354		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 3.2084620727254354 | validation: 4.202604263468353]
	TIME [epoch: 11.5 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071792282582074		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 3.2071792282582074 | validation: 4.208202255838423]
	TIME [epoch: 11.5 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204438763804262		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 3.204438763804262 | validation: 4.201339252484192]
	TIME [epoch: 11.5 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203130242501369		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 3.203130242501369 | validation: 4.203069579289514]
	TIME [epoch: 11.5 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2038187009680272		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 3.2038187009680272 | validation: 4.1989380222085755]
	TIME [epoch: 11.5 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076447715687912		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 3.2076447715687912 | validation: 4.200084252993781]
	TIME [epoch: 11.5 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204739527319654		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 3.204739527319654 | validation: 4.202315269082982]
	TIME [epoch: 11.5 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2081892481882903		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 3.2081892481882903 | validation: 4.205404641522894]
	TIME [epoch: 11.5 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2113474705503284		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 3.2113474705503284 | validation: 4.199917651310119]
	TIME [epoch: 11.5 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067869463074534		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 3.2067869463074534 | validation: 4.202724724355219]
	TIME [epoch: 11.5 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085188867030636		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 3.2085188867030636 | validation: 4.206562422223999]
	TIME [epoch: 11.5 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079663733977304		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 3.2079663733977304 | validation: 4.203076551392756]
	TIME [epoch: 11.5 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210524986563091		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 3.210524986563091 | validation: 4.205588057437143]
	TIME [epoch: 11.5 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089450736950305		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 3.2089450736950305 | validation: 4.206540854604639]
	TIME [epoch: 11.5 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051049274517514		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 3.2051049274517514 | validation: 4.205921753245657]
	TIME [epoch: 11.5 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2047106257737346		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 3.2047106257737346 | validation: 4.198003297400316]
	TIME [epoch: 11.5 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209337516701929		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 3.209337516701929 | validation: 4.204107318956934]
	TIME [epoch: 11.5 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206045171103227		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 3.206045171103227 | validation: 4.1970535966085185]
	TIME [epoch: 11.5 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067125343976066		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 3.2067125343976066 | validation: 4.208335698323047]
	TIME [epoch: 11.5 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2041995708045086		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 3.2041995708045086 | validation: 4.2037049064146]
	TIME [epoch: 11.5 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2100954731773994		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 3.2100954731773994 | validation: 4.1990105996679885]
	TIME [epoch: 11.5 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205126938999431		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 3.205126938999431 | validation: 4.209352905876062]
	TIME [epoch: 11.5 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2081164945450245		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 3.2081164945450245 | validation: 4.204360046646196]
	TIME [epoch: 11.5 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20426618505611		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 3.20426618505611 | validation: 4.203638919748303]
	TIME [epoch: 11.5 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203377178790357		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 3.203377178790357 | validation: 4.201797552531126]
	TIME [epoch: 11.5 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071150866333284		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 3.2071150866333284 | validation: 4.203076760432652]
	TIME [epoch: 11.5 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2099482225710205		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 3.2099482225710205 | validation: 4.212803074619582]
	TIME [epoch: 11.5 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208321808542072		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 3.208321808542072 | validation: 4.19961976396544]
	TIME [epoch: 11.5 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204994658639924		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 3.204994658639924 | validation: 4.203020563863924]
	TIME [epoch: 11.5 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2095917990272613		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 3.2095917990272613 | validation: 4.1937538531578]
	TIME [epoch: 11.5 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077009287230247		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 3.2077009287230247 | validation: 4.205538134154926]
	TIME [epoch: 11.5 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069903114053635		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 3.2069903114053635 | validation: 4.199102203622841]
	TIME [epoch: 11.5 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210230871431942		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 3.210230871431942 | validation: 4.201228283605274]
	TIME [epoch: 11.5 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2024823681911276		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 3.2024823681911276 | validation: 4.207044965190699]
	TIME [epoch: 11.5 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203924841460752		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 3.203924841460752 | validation: 4.199898426262711]
	TIME [epoch: 11.5 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2090438040572917		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 3.2090438040572917 | validation: 4.201259002262413]
	TIME [epoch: 11.5 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204889086798743		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 3.204889086798743 | validation: 4.202607215384207]
	TIME [epoch: 11.5 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207353112314834		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 3.207353112314834 | validation: 4.201468898563007]
	TIME [epoch: 11.5 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209538000839259		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 3.209538000839259 | validation: 4.206180184418549]
	TIME [epoch: 11.5 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203909331507831		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 3.203909331507831 | validation: 4.194473058793708]
	TIME [epoch: 11.5 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205057450344455		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 3.205057450344455 | validation: 4.20505904813229]
	TIME [epoch: 11.5 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2052924307630413		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 3.2052924307630413 | validation: 4.200004798751482]
	TIME [epoch: 11.5 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059180882009297		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 3.2059180882009297 | validation: 4.2000389452021425]
	TIME [epoch: 11.5 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207546775864355		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 3.207546775864355 | validation: 4.201192975594746]
	TIME [epoch: 11.5 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083455491571726		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 3.2083455491571726 | validation: 4.207527417724722]
	TIME [epoch: 11.5 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056377621190864		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 3.2056377621190864 | validation: 4.198200418728702]
	TIME [epoch: 11.5 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2035602209046914		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 3.2035602209046914 | validation: 4.197122605329362]
	TIME [epoch: 11.5 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061337792842175		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 3.2061337792842175 | validation: 4.204939410765087]
	TIME [epoch: 11.5 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066098450947598		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 3.2066098450947598 | validation: 4.201319572012286]
	TIME [epoch: 11.5 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207218430966859		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 3.207218430966859 | validation: 4.203148762579871]
	TIME [epoch: 11.5 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2058437628067105		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 3.2058437628067105 | validation: 4.200618870774331]
	TIME [epoch: 11.5 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2096604374904567		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 3.2096604374904567 | validation: 4.211813711223288]
	TIME [epoch: 11.5 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070389375512933		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 3.2070389375512933 | validation: 4.202127602061555]
	TIME [epoch: 11.5 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093410157110718		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 3.2093410157110718 | validation: 4.205000164507779]
	TIME [epoch: 11.5 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206583603047201		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 3.206583603047201 | validation: 4.210209875723294]
	TIME [epoch: 11.5 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2082811429198976		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 3.2082811429198976 | validation: 4.19752384169232]
	TIME [epoch: 11.5 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2060228046611123		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 3.2060228046611123 | validation: 4.20329718533795]
	TIME [epoch: 11.5 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206868229249792		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 3.206868229249792 | validation: 4.196438150455856]
	TIME [epoch: 11.5 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077477248960298		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 3.2077477248960298 | validation: 4.200849642679304]
	TIME [epoch: 11.5 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206963292166467		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 3.206963292166467 | validation: 4.204302295092437]
	TIME [epoch: 11.5 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063987254014465		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 3.2063987254014465 | validation: 4.198648400051359]
	TIME [epoch: 11.5 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2072842966756108		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 3.2072842966756108 | validation: 4.199749253026368]
	TIME [epoch: 11.5 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066028397445807		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 3.2066028397445807 | validation: 4.206116662244627]
	TIME [epoch: 11.5 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069156182277205		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 3.2069156182277205 | validation: 4.2069682925628165]
	TIME [epoch: 11.5 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2075832536753124		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 3.2075832536753124 | validation: 4.202875389173561]
	TIME [epoch: 11.5 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208264866563008		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 3.208264866563008 | validation: 4.2071862711326125]
	TIME [epoch: 11.5 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206168161385243		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 3.206168161385243 | validation: 4.203749372981908]
	TIME [epoch: 11.5 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206622686501408		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 3.206622686501408 | validation: 4.204490347682353]
	TIME [epoch: 11.5 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2072545857358685		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 3.2072545857358685 | validation: 4.198209407858537]
	TIME [epoch: 11.5 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204669188097426		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 3.204669188097426 | validation: 4.2000602915009475]
	TIME [epoch: 11.5 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076713389385096		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 3.2076713389385096 | validation: 4.1984508411950445]
	TIME [epoch: 11.5 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062812801147045		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 3.2062812801147045 | validation: 4.2079484007975445]
	TIME [epoch: 11.5 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204554166599652		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 3.204554166599652 | validation: 4.199706985334572]
	TIME [epoch: 11.5 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2038806932155035		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 3.2038806932155035 | validation: 4.194563452152828]
	TIME [epoch: 11.5 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206654275053207		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 3.206654275053207 | validation: 4.208830866010311]
	TIME [epoch: 11.5 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207541053456835		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 3.207541053456835 | validation: 4.200668453267708]
	TIME [epoch: 11.5 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2031863112157475		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 3.2031863112157475 | validation: 4.201221937339509]
	TIME [epoch: 11.5 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207720004322036		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 3.207720004322036 | validation: 4.204841959177519]
	TIME [epoch: 11.5 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208334810899916		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 3.208334810899916 | validation: 4.2037344756649695]
	TIME [epoch: 11.5 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202839576287345		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 3.202839576287345 | validation: 4.199283041999595]
	TIME [epoch: 11.5 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2090827731457336		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 3.2090827731457336 | validation: 4.198815343187658]
	TIME [epoch: 11.5 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063366408412097		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 3.2063366408412097 | validation: 4.202256603570447]
	TIME [epoch: 11.5 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064437071744454		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 3.2064437071744454 | validation: 4.199250718293532]
	TIME [epoch: 11.5 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2049086587406714		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 3.2049086587406714 | validation: 4.204266766936063]
	TIME [epoch: 11.5 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2092715248542447		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 3.2092715248542447 | validation: 4.201074973289624]
	TIME [epoch: 11.5 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203277753546852		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 3.203277753546852 | validation: 4.204644522996471]
	TIME [epoch: 11.5 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209077541645887		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 3.209077541645887 | validation: 4.203254374307584]
	TIME [epoch: 11.5 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067203237463295		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 3.2067203237463295 | validation: 4.201117208999084]
	TIME [epoch: 11.5 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2038480004158885		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 3.2038480004158885 | validation: 4.201482786103832]
	TIME [epoch: 11.5 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2075492402015633		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 3.2075492402015633 | validation: 4.207430179470225]
	TIME [epoch: 11.5 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202649577227611		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 3.202649577227611 | validation: 4.202894886283038]
	TIME [epoch: 11.5 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203602784026585		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 3.203602784026585 | validation: 4.205337074354146]
	TIME [epoch: 11.5 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205700309146147		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 3.205700309146147 | validation: 4.2024594627015555]
	TIME [epoch: 11.5 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051392666020266		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 3.2051392666020266 | validation: 4.205241117182919]
	TIME [epoch: 11.5 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202662073882642		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 3.202662073882642 | validation: 4.200494170615957]
	TIME [epoch: 11.5 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206347652920713		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 3.206347652920713 | validation: 4.196277531223263]
	TIME [epoch: 11.5 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2088834999486977		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 3.2088834999486977 | validation: 4.1962116932174]
	TIME [epoch: 11.5 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067638833025573		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 3.2067638833025573 | validation: 4.202443465729206]
	TIME [epoch: 11.5 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057784919687617		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 3.2057784919687617 | validation: 4.206152736657463]
	TIME [epoch: 11.5 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208810631274412		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 3.208810631274412 | validation: 4.206247552950404]
	TIME [epoch: 11.5 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079354551277626		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 3.2079354551277626 | validation: 4.204890468339478]
	TIME [epoch: 11.5 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051488603838822		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 3.2051488603838822 | validation: 4.207519225083648]
	TIME [epoch: 11.5 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2122764378737863		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 3.2122764378737863 | validation: 4.205258960764942]
	TIME [epoch: 11.5 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211373862423904		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 3.211373862423904 | validation: 4.204684220534506]
	TIME [epoch: 11.5 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.212818446599167		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 3.212818446599167 | validation: 4.208115716842039]
	TIME [epoch: 11.5 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.213520238005792		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 3.213520238005792 | validation: 4.2035919538007365]
	TIME [epoch: 11.5 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208527729431211		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 3.208527729431211 | validation: 4.204788387124774]
	TIME [epoch: 11.5 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206046019752519		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 3.206046019752519 | validation: 4.203109595108893]
	TIME [epoch: 11.5 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206514509112741		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 3.206514509112741 | validation: 4.197057642434082]
	TIME [epoch: 11.5 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069306235131125		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 3.2069306235131125 | validation: 4.198532101498651]
	TIME [epoch: 11.5 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20250569137936		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 3.20250569137936 | validation: 4.205382834241743]
	TIME [epoch: 11.5 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2017573369099477		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 3.2017573369099477 | validation: 4.204340413122514]
	TIME [epoch: 11.5 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2097616788575616		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 3.2097616788575616 | validation: 4.198317861585322]
	TIME [epoch: 11.5 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2035358435612715		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 3.2035358435612715 | validation: 4.199596573773901]
	TIME [epoch: 11.5 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2111659250324354		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 3.2111659250324354 | validation: 4.1973325452496155]
	TIME [epoch: 11.5 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2074009376887513		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 3.2074009376887513 | validation: 4.201199343459603]
	TIME [epoch: 11.5 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207704385675849		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 3.207704385675849 | validation: 4.204009552120702]
	TIME [epoch: 11.5 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2096581067562617		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 3.2096581067562617 | validation: 4.209357440375245]
	TIME [epoch: 11.5 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204935202502276		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 3.204935202502276 | validation: 4.206714665621084]
	TIME [epoch: 11.5 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057662901505464		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 3.2057662901505464 | validation: 4.2009274549260605]
	TIME [epoch: 11.5 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061458034135604		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 3.2061458034135604 | validation: 4.198877612031959]
	TIME [epoch: 11.5 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2080610746259053		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 3.2080610746259053 | validation: 4.202217316746199]
	TIME [epoch: 11.5 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2042812378416166		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 3.2042812378416166 | validation: 4.202222677634384]
	TIME [epoch: 11.5 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057085321708816		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 3.2057085321708816 | validation: 4.200760305037177]
	TIME [epoch: 11.5 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206719995415182		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 3.206719995415182 | validation: 4.206345931749767]
	TIME [epoch: 11.5 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071428723797437		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 3.2071428723797437 | validation: 4.204446597162697]
	TIME [epoch: 11.5 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2047285801940086		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 3.2047285801940086 | validation: 4.203947317048967]
	TIME [epoch: 11.5 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085048101415126		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 3.2085048101415126 | validation: 4.2043985792369245]
	TIME [epoch: 11.5 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2087161237713966		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 3.2087161237713966 | validation: 4.197316399226444]
	TIME [epoch: 11.5 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050115775916934		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 3.2050115775916934 | validation: 4.198490001610343]
	TIME [epoch: 11.5 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2042627221526416		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 3.2042627221526416 | validation: 4.2008446825338135]
	TIME [epoch: 11.5 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045311234641543		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 3.2045311234641543 | validation: 4.203163212250517]
	TIME [epoch: 11.5 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209822950282886		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 3.209822950282886 | validation: 4.196128909496876]
	TIME [epoch: 11.5 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204258318494159		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 3.204258318494159 | validation: 4.1987303006163845]
	TIME [epoch: 11.5 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064927793470623		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 3.2064927793470623 | validation: 4.202022793144123]
	TIME [epoch: 11.5 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207138406825443		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 3.207138406825443 | validation: 4.19972401150635]
	TIME [epoch: 11.5 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2060925991966074		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 3.2060925991966074 | validation: 4.199039754170175]
	TIME [epoch: 11.5 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045426300566384		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 3.2045426300566384 | validation: 4.20629243431214]
	TIME [epoch: 11.5 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059816550149254		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 3.2059816550149254 | validation: 4.20120891212015]
	TIME [epoch: 11.5 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204054004766207		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 3.204054004766207 | validation: 4.200508355702413]
	TIME [epoch: 11.5 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055872063928676		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 3.2055872063928676 | validation: 4.192702779686561]
	TIME [epoch: 11.5 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2049420010711858		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 3.2049420010711858 | validation: 4.1983333501950835]
	TIME [epoch: 11.5 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076766168212685		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 3.2076766168212685 | validation: 4.2006733585825256]
	TIME [epoch: 11.5 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2039671040886093		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 3.2039671040886093 | validation: 4.199027379383715]
	TIME [epoch: 11.5 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205563124621086		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 3.205563124621086 | validation: 4.202223758993569]
	TIME [epoch: 11.5 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076071619527937		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 3.2076071619527937 | validation: 4.196292225039668]
	TIME [epoch: 11.5 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2041288817683466		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 3.2041288817683466 | validation: 4.201829657128189]
	TIME [epoch: 11.5 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1998114599776546		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 3.1998114599776546 | validation: 4.196686350990467]
	TIME [epoch: 11.5 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2091834631584155		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 3.2091834631584155 | validation: 4.199653677925213]
	TIME [epoch: 11.5 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207555710452963		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 3.207555710452963 | validation: 4.20073746747468]
	TIME [epoch: 11.5 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059568821081728		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 3.2059568821081728 | validation: 4.200016885220054]
	TIME [epoch: 11.5 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204614553700884		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 3.204614553700884 | validation: 4.204403326728884]
	TIME [epoch: 11.5 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.200330138241727		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 3.200330138241727 | validation: 4.196787211080597]
	TIME [epoch: 11.5 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207362914308817		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 3.207362914308817 | validation: 4.194268626479517]
	TIME [epoch: 11.5 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207929874122582		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 3.207929874122582 | validation: 4.196264308395542]
	TIME [epoch: 11.5 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208744607962889		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 3.208744607962889 | validation: 4.196393109824157]
	TIME [epoch: 11.5 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061392163662537		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 3.2061392163662537 | validation: 4.200405933369907]
	TIME [epoch: 11.5 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063175936591124		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 3.2063175936591124 | validation: 4.2085834638228015]
	TIME [epoch: 11.5 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206856228587342		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 3.206856228587342 | validation: 4.197313439038549]
	TIME [epoch: 11.5 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208159500616694		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 3.208159500616694 | validation: 4.202475965987477]
	TIME [epoch: 11.5 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205696158608617		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 3.205696158608617 | validation: 4.202306689798411]
	TIME [epoch: 11.5 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045121807415673		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 3.2045121807415673 | validation: 4.204932085280575]
	TIME [epoch: 11.5 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2023225977555994		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 3.2023225977555994 | validation: 4.204602751118731]
	TIME [epoch: 11.5 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2007697323863065		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 3.2007697323863065 | validation: 4.204553082060853]
	TIME [epoch: 11.5 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206440864891678		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 3.206440864891678 | validation: 4.200963070134027]
	TIME [epoch: 11.5 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064431133753475		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 3.2064431133753475 | validation: 4.202403325588618]
	TIME [epoch: 11.5 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067817917930483		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 3.2067817917930483 | validation: 4.199342088270659]
	TIME [epoch: 11.5 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205018944824436		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 3.205018944824436 | validation: 4.201510292760291]
	TIME [epoch: 11.5 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2034018638356465		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 3.2034018638356465 | validation: 4.208038550093727]
	TIME [epoch: 11.5 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2034649659283803		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 3.2034649659283803 | validation: 4.2046541657212515]
	TIME [epoch: 11.5 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062960357510963		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 3.2062960357510963 | validation: 4.206666420782374]
	TIME [epoch: 11.5 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203848803923314		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 3.203848803923314 | validation: 4.197039832643058]
	TIME [epoch: 11.5 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2075651346148915		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 3.2075651346148915 | validation: 4.1985234016903155]
	TIME [epoch: 11.5 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207760671683997		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 3.207760671683997 | validation: 4.202209967529624]
	TIME [epoch: 11.5 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2034533632864894		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 3.2034533632864894 | validation: 4.202202459710686]
	TIME [epoch: 11.5 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205354621753165		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 3.205354621753165 | validation: 4.200326512803628]
	TIME [epoch: 11.5 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050089475305983		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 3.2050089475305983 | validation: 4.201680457062942]
	TIME [epoch: 11.5 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062108558324183		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 3.2062108558324183 | validation: 4.201161425362433]
	TIME [epoch: 11.5 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203975880711048		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 3.203975880711048 | validation: 4.198542366226522]
	TIME [epoch: 11.5 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2074212695033344		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 3.2074212695033344 | validation: 4.197622609986977]
	TIME [epoch: 11.5 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059418220291587		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 3.2059418220291587 | validation: 4.202343372320721]
	TIME [epoch: 11.5 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205713960568294		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 3.205713960568294 | validation: 4.20034219465496]
	TIME [epoch: 11.5 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204143141060703		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 3.204143141060703 | validation: 4.208282260816396]
	TIME [epoch: 11.5 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205438534957208		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 3.205438534957208 | validation: 4.204161160675526]
	TIME [epoch: 11.5 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2024133992369843		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 3.2024133992369843 | validation: 4.207611502457576]
	TIME [epoch: 11.5 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077530127294134		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 3.2077530127294134 | validation: 4.20157469062866]
	TIME [epoch: 11.5 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070886974595627		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 3.2070886974595627 | validation: 4.208180904103844]
	TIME [epoch: 11.5 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208512056954859		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 3.208512056954859 | validation: 4.19818204135673]
	TIME [epoch: 11.5 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073181031770424		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 3.2073181031770424 | validation: 4.199878579288586]
	TIME [epoch: 11.5 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083550165544326		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 3.2083550165544326 | validation: 4.204346331446528]
	TIME [epoch: 11.5 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2065778683200454		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 3.2065778683200454 | validation: 4.202896931002602]
	TIME [epoch: 11.5 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208555455014123		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 3.208555455014123 | validation: 4.206865637304324]
	TIME [epoch: 11.5 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2037510906158086		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 3.2037510906158086 | validation: 4.197545126840198]
	TIME [epoch: 11.5 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206716859148674		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 3.206716859148674 | validation: 4.2063983889073455]
	TIME [epoch: 11.5 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206906123636263		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 3.206906123636263 | validation: 4.208495278877061]
	TIME [epoch: 11.5 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20789628604264		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 3.20789628604264 | validation: 4.203436683920991]
	TIME [epoch: 11.5 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056544579070474		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 3.2056544579070474 | validation: 4.202786827417324]
	TIME [epoch: 11.5 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204684722899606		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 3.204684722899606 | validation: 4.196559639809583]
	TIME [epoch: 11.5 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2098260272455934		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 3.2098260272455934 | validation: 4.195143234899133]
	TIME [epoch: 11.5 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062953828247425		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 3.2062953828247425 | validation: 4.19926095108298]
	TIME [epoch: 11.5 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210224101669353		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 3.210224101669353 | validation: 4.2092202402420495]
	TIME [epoch: 11.5 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055542666479617		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 3.2055542666479617 | validation: 4.206012778895741]
	TIME [epoch: 11.5 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2075774522699776		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 3.2075774522699776 | validation: 4.197633277453964]
	TIME [epoch: 11.5 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2041134643635316		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 3.2041134643635316 | validation: 4.201133103789025]
	TIME [epoch: 11.5 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068679167574623		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 3.2068679167574623 | validation: 4.206694296172623]
	TIME [epoch: 11.5 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2091495341171914		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 3.2091495341171914 | validation: 4.210178048202078]
	TIME [epoch: 11.5 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2084308232681504		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 3.2084308232681504 | validation: 4.204360969595137]
	TIME [epoch: 11.5 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203173005901022		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 3.203173005901022 | validation: 4.2039512037553415]
	TIME [epoch: 11.5 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206150533435684		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 3.206150533435684 | validation: 4.208379063681427]
	TIME [epoch: 11.5 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207115434786054		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 3.207115434786054 | validation: 4.2044381967167626]
	TIME [epoch: 11.5 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207341853413155		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 3.207341853413155 | validation: 4.206499215582372]
	TIME [epoch: 11.5 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055954034000633		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 3.2055954034000633 | validation: 4.198417979209102]
	TIME [epoch: 11.5 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206620668753112		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 3.206620668753112 | validation: 4.203974019536076]
	TIME [epoch: 11.5 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067506557214274		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 3.2067506557214274 | validation: 4.204281835103605]
	TIME [epoch: 11.5 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2046078774995657		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 3.2046078774995657 | validation: 4.2000443824771745]
	TIME [epoch: 11.5 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203102665370457		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 3.203102665370457 | validation: 4.209856406199889]
	TIME [epoch: 11.5 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2060981280474947		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 3.2060981280474947 | validation: 4.1944516282530335]
	TIME [epoch: 11.5 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054234239872113		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 3.2054234239872113 | validation: 4.199638747546187]
	TIME [epoch: 11.5 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205625076595869		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 3.205625076595869 | validation: 4.2014230718367775]
	TIME [epoch: 11.5 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20349805755781		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 3.20349805755781 | validation: 4.204335435906808]
	TIME [epoch: 11.5 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2044341789003425		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 3.2044341789003425 | validation: 4.208986673132609]
	TIME [epoch: 11.5 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2068198404699073		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 3.2068198404699073 | validation: 4.2013511586588645]
	TIME [epoch: 11.5 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2093896873399923		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 3.2093896873399923 | validation: 4.204265052773604]
	TIME [epoch: 11.5 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203946463712578		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 3.203946463712578 | validation: 4.19997531418125]
	TIME [epoch: 11.5 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203663948461148		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 3.203663948461148 | validation: 4.20121865271345]
	TIME [epoch: 11.5 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050413085416105		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 3.2050413085416105 | validation: 4.198000090740865]
	TIME [epoch: 11.5 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2037719881901934		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 3.2037719881901934 | validation: 4.200503425153441]
	TIME [epoch: 11.5 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2015278487655214		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 3.2015278487655214 | validation: 4.196570758290377]
	TIME [epoch: 11.5 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064576709566217		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 3.2064576709566217 | validation: 4.199382562708365]
	TIME [epoch: 11.5 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062752215996406		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 3.2062752215996406 | validation: 4.204632683544096]
	TIME [epoch: 11.5 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057887716392255		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 3.2057887716392255 | validation: 4.201272687368977]
	TIME [epoch: 11.5 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2043053186714316		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 3.2043053186714316 | validation: 4.203696051723311]
	TIME [epoch: 11.5 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2021649243641783		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 3.2021649243641783 | validation: 4.197954101429385]
	TIME [epoch: 11.5 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203905364618306		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 3.203905364618306 | validation: 4.208374607719476]
	TIME [epoch: 11.5 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207557150580171		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 3.207557150580171 | validation: 4.204671491367988]
	TIME [epoch: 11.5 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2052942019804256		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 3.2052942019804256 | validation: 4.202377813187745]
	TIME [epoch: 11.5 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20782855725685		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 3.20782855725685 | validation: 4.199605850567082]
	TIME [epoch: 11.5 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2021686058794017		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 3.2021686058794017 | validation: 4.20695186602534]
	TIME [epoch: 11.5 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20662779768991		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 3.20662779768991 | validation: 4.1999366940346095]
	TIME [epoch: 11.5 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056380181842785		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 3.2056380181842785 | validation: 4.20292816593022]
	TIME [epoch: 11.5 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051883862627286		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 3.2051883862627286 | validation: 4.195914192287238]
	TIME [epoch: 11.5 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207618539962225		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 3.207618539962225 | validation: 4.199770953833157]
	TIME [epoch: 11.5 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20694212807952		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 3.20694212807952 | validation: 4.200711402887302]
	TIME [epoch: 11.5 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2053091528652553		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 3.2053091528652553 | validation: 4.202151031273629]
	TIME [epoch: 11.5 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073982467020183		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 3.2073982467020183 | validation: 4.202318656433687]
	TIME [epoch: 11.5 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204464129721964		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 3.204464129721964 | validation: 4.198964440166076]
	TIME [epoch: 11.5 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063480273723286		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 3.2063480273723286 | validation: 4.196005897599426]
	TIME [epoch: 11.5 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206685054119769		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 3.206685054119769 | validation: 4.199007817972721]
	TIME [epoch: 11.5 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2039892150112483		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 3.2039892150112483 | validation: 4.200555968048759]
	TIME [epoch: 11.5 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204514979601463		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 3.204514979601463 | validation: 4.202134596496735]
	TIME [epoch: 11.5 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204580866816645		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 3.204580866816645 | validation: 4.199843016487099]
	TIME [epoch: 11.5 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056006162185255		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 3.2056006162185255 | validation: 4.201440876384424]
	TIME [epoch: 11.5 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078521617282023		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 3.2078521617282023 | validation: 4.198416888161758]
	TIME [epoch: 11.5 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2077292302635234		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 3.2077292302635234 | validation: 4.199468325879552]
	TIME [epoch: 11.5 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209248601502304		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 3.209248601502304 | validation: 4.199221164893657]
	TIME [epoch: 11.5 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2021261243679193		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 3.2021261243679193 | validation: 4.200151040993706]
	TIME [epoch: 11.5 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2035414736369803		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 3.2035414736369803 | validation: 4.206443556802025]
	TIME [epoch: 11.5 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204517810383249		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 3.204517810383249 | validation: 4.196513732522864]
	TIME [epoch: 11.5 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045064347581635		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 3.2045064347581635 | validation: 4.1942499997402365]
	TIME [epoch: 11.5 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2072187152024316		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 3.2072187152024316 | validation: 4.192158909074651]
	TIME [epoch: 11.5 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2049954949240584		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 3.2049954949240584 | validation: 4.199897804537577]
	TIME [epoch: 11.5 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205308878043663		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 3.205308878043663 | validation: 4.19692714446845]
	TIME [epoch: 11.5 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2038579167631625		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 3.2038579167631625 | validation: 4.1977919889409625]
	TIME [epoch: 11.5 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209076280829925		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 3.209076280829925 | validation: 4.2006466799977815]
	TIME [epoch: 11.5 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2044250787150856		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 3.2044250787150856 | validation: 4.2048192108845255]
	TIME [epoch: 11.5 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073865891171667		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 3.2073865891171667 | validation: 4.2070928972739425]
	TIME [epoch: 11.5 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207793630218377		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 3.207793630218377 | validation: 4.204583025532476]
	TIME [epoch: 11.5 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203924035813838		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 3.203924035813838 | validation: 4.197590720338564]
	TIME [epoch: 11.5 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2090157399802575		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 3.2090157399802575 | validation: 4.202601677763412]
	TIME [epoch: 11.5 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2042674179898443		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 3.2042674179898443 | validation: 4.205959593720437]
	TIME [epoch: 11.5 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206446159199991		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 3.206446159199991 | validation: 4.2063381165630345]
	TIME [epoch: 11.5 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205617329329657		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 3.205617329329657 | validation: 4.2072377401509815]
	TIME [epoch: 11.5 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066410630374267		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 3.2066410630374267 | validation: 4.206662898060307]
	TIME [epoch: 11.5 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207093812270659		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 3.207093812270659 | validation: 4.206215276987818]
	TIME [epoch: 11.5 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073874056318825		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 3.2073874056318825 | validation: 4.207437365769024]
	TIME [epoch: 11.5 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.211137168870973		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 3.211137168870973 | validation: 4.208167246312302]
	TIME [epoch: 11.5 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063877159569234		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 3.2063877159569234 | validation: 4.201379207139331]
	TIME [epoch: 11.5 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2080836358642384		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 3.2080836358642384 | validation: 4.201350332640702]
	TIME [epoch: 11.5 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2080855685270215		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 3.2080855685270215 | validation: 4.204856395459486]
	TIME [epoch: 11.5 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205015804499075		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 3.205015804499075 | validation: 4.2044619554735805]
	TIME [epoch: 11.5 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2042467215329395		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 3.2042467215329395 | validation: 4.2077379650575795]
	TIME [epoch: 11.5 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204180850038855		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 3.204180850038855 | validation: 4.201249006039502]
	TIME [epoch: 11.5 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206462603329867		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 3.206462603329867 | validation: 4.207946829190638]
	TIME [epoch: 11.5 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203789344376329		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 3.203789344376329 | validation: 4.199582841705467]
	TIME [epoch: 11.5 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066972437414916		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 3.2066972437414916 | validation: 4.201210281678754]
	TIME [epoch: 11.5 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204870440628266		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 3.204870440628266 | validation: 4.208658128553536]
	TIME [epoch: 11.5 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2053130300931385		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 3.2053130300931385 | validation: 4.201942781320666]
	TIME [epoch: 11.5 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062389724049596		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 3.2062389724049596 | validation: 4.197052856544072]
	TIME [epoch: 11.5 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057692257917703		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 3.2057692257917703 | validation: 4.206544362684788]
	TIME [epoch: 11.5 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205081026484517		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 3.205081026484517 | validation: 4.198788882531426]
	TIME [epoch: 11.5 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20577369744148		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 3.20577369744148 | validation: 4.195766596705053]
	TIME [epoch: 11.5 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059834469375073		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 3.2059834469375073 | validation: 4.201037241444214]
	TIME [epoch: 11.5 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076543544864564		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 3.2076543544864564 | validation: 4.192570928655088]
	TIME [epoch: 11.5 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076817027307927		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 3.2076817027307927 | validation: 4.2041983640916305]
	TIME [epoch: 11.5 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207005946783283		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 3.207005946783283 | validation: 4.204393852262585]
	TIME [epoch: 11.5 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054538291780554		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 3.2054538291780554 | validation: 4.202652217622874]
	TIME [epoch: 11.5 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045426409749753		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 3.2045426409749753 | validation: 4.204440025309051]
	TIME [epoch: 11.5 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205608109277216		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 3.205608109277216 | validation: 4.201076574875043]
	TIME [epoch: 11.5 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204333144009458		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 3.204333144009458 | validation: 4.200761016568122]
	TIME [epoch: 11.5 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2073510343831857		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 3.2073510343831857 | validation: 4.20580345869901]
	TIME [epoch: 11.5 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2040269226852605		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 3.2040269226852605 | validation: 4.196221606566213]
	TIME [epoch: 11.5 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2033138222786848		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 3.2033138222786848 | validation: 4.202053551551556]
	TIME [epoch: 11.5 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205031062671112		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 3.205031062671112 | validation: 4.205153071337227]
	TIME [epoch: 11.5 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204435094378234		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 3.204435094378234 | validation: 4.199346051040908]
	TIME [epoch: 11.5 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204845737011447		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 3.204845737011447 | validation: 4.200156897474519]
	TIME [epoch: 11.5 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064349942226515		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 3.2064349942226515 | validation: 4.198137439257782]
	TIME [epoch: 11.5 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2032254380741723		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 3.2032254380741723 | validation: 4.199680731514512]
	TIME [epoch: 11.5 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064839732567965		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 3.2064839732567965 | validation: 4.1986674530710495]
	TIME [epoch: 11.5 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205918947108124		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 3.205918947108124 | validation: 4.205563316116193]
	TIME [epoch: 11.5 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2039172409056516		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 3.2039172409056516 | validation: 4.1994452380022285]
	TIME [epoch: 11.5 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2072969961952014		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 3.2072969961952014 | validation: 4.198205810477372]
	TIME [epoch: 11.5 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2079068388555605		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 3.2079068388555605 | validation: 4.199949074721762]
	TIME [epoch: 11.5 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203867898141361		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 3.203867898141361 | validation: 4.20456418945629]
	TIME [epoch: 11.5 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2070364117224104		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 3.2070364117224104 | validation: 4.198868418312811]
	TIME [epoch: 11.5 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2048685137465736		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 3.2048685137465736 | validation: 4.199271252575995]
	TIME [epoch: 11.5 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207759187341761		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 3.207759187341761 | validation: 4.200197879095189]
	TIME [epoch: 11.5 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204325919081455		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 3.204325919081455 | validation: 4.200938065815164]
	TIME [epoch: 11.5 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2044160223706544		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 3.2044160223706544 | validation: 4.2106142478348785]
	TIME [epoch: 11.5 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203425537079079		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 3.203425537079079 | validation: 4.201862703201239]
	TIME [epoch: 11.5 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051021008413634		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 3.2051021008413634 | validation: 4.1972635253179735]
	TIME [epoch: 11.5 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2058856614044116		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 3.2058856614044116 | validation: 4.20293029467213]
	TIME [epoch: 11.5 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203823733588667		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 3.203823733588667 | validation: 4.206779631363425]
	TIME [epoch: 11.5 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207416397669713		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 3.207416397669713 | validation: 4.206882004086294]
	TIME [epoch: 11.5 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202818523376578		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 3.202818523376578 | validation: 4.2032704353365355]
	TIME [epoch: 11.5 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2048289996344828		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 3.2048289996344828 | validation: 4.200381612259706]
	TIME [epoch: 11.5 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206856235044707		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 3.206856235044707 | validation: 4.199993045990162]
	TIME [epoch: 11.5 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2039230289216016		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 3.2039230289216016 | validation: 4.202224375705013]
	TIME [epoch: 11.5 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2051182055784233		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 3.2051182055784233 | validation: 4.200933550483106]
	TIME [epoch: 11.5 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056103623761114		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 3.2056103623761114 | validation: 4.205643992625374]
	TIME [epoch: 11.5 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2043274514165376		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 3.2043274514165376 | validation: 4.207513480458055]
	TIME [epoch: 11.5 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2046165255802266		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 3.2046165255802266 | validation: 4.202495721222447]
	TIME [epoch: 11.5 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057325509412085		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 3.2057325509412085 | validation: 4.200588594724163]
	TIME [epoch: 11.5 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2049049117332276		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 3.2049049117332276 | validation: 4.201002905929701]
	TIME [epoch: 11.5 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205081547750533		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 3.205081547750533 | validation: 4.199571960712647]
	TIME [epoch: 11.5 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2062135048758083		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 3.2062135048758083 | validation: 4.208907985842671]
	TIME [epoch: 11.5 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207377389825797		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 3.207377389825797 | validation: 4.2024967095714025]
	TIME [epoch: 11.5 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20446184662217		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 3.20446184662217 | validation: 4.198293875087877]
	TIME [epoch: 11.5 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207117289601573		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 3.207117289601573 | validation: 4.200609228240284]
	TIME [epoch: 11.5 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20816067195495		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 3.20816067195495 | validation: 4.204097691848678]
	TIME [epoch: 11.5 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205452823419065		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 3.205452823419065 | validation: 4.2005614726964495]
	TIME [epoch: 11.5 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205959112890629		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 3.205959112890629 | validation: 4.197323932004935]
	TIME [epoch: 11.5 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2036033907052204		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 3.2036033907052204 | validation: 4.1993273324011735]
	TIME [epoch: 11.5 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2089430684117106		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 3.2089430684117106 | validation: 4.2085379812986625]
	TIME [epoch: 11.5 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071708124996916		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 3.2071708124996916 | validation: 4.202645428595125]
	TIME [epoch: 11.5 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2047414027077026		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 3.2047414027077026 | validation: 4.200308676872937]
	TIME [epoch: 11.5 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208040250509593		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 3.208040250509593 | validation: 4.203123408606768]
	TIME [epoch: 11.5 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2032926466090164		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 3.2032926466090164 | validation: 4.198966526426246]
	TIME [epoch: 11.5 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204654203138923		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 3.204654203138923 | validation: 4.195247154016792]
	TIME [epoch: 11.5 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2035290268224625		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 3.2035290268224625 | validation: 4.202717845929903]
	TIME [epoch: 11.5 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2074932133669254		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 3.2074932133669254 | validation: 4.189918504309482]
	TIME [epoch: 11.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study204/model_tr_study204_r3_20240310_050823/states/model_tr_study204_1900.pth
	Model improved!!!
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057719572424688		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 3.2057719572424688 | validation: 4.203979736118464]
	TIME [epoch: 11.5 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066410449651563		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 3.2066410449651563 | validation: 4.200841496024262]
	TIME [epoch: 11.5 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061338631670915		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 3.2061338631670915 | validation: 4.20998615402113]
	TIME [epoch: 11.5 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2111205030971517		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 3.2111205030971517 | validation: 4.201091547428595]
	TIME [epoch: 11.5 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069074091102996		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 3.2069074091102996 | validation: 4.204939153066489]
	TIME [epoch: 11.5 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204557937500658		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 3.204557937500658 | validation: 4.203060909595616]
	TIME [epoch: 11.5 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2064668942341332		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 3.2064668942341332 | validation: 4.200935560524822]
	TIME [epoch: 11.5 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205793790932901		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 3.205793790932901 | validation: 4.197906041636178]
	TIME [epoch: 11.5 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066903520028722		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 3.2066903520028722 | validation: 4.207531875416395]
	TIME [epoch: 11.5 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.210445128359229		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 3.210445128359229 | validation: 4.203092410714442]
	TIME [epoch: 11.5 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207760377974044		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 3.207760377974044 | validation: 4.2024830085105656]
	TIME [epoch: 11.5 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2043911547794206		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 3.2043911547794206 | validation: 4.198448692378334]
	TIME [epoch: 11.5 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204769956233833		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 3.204769956233833 | validation: 4.208247686210286]
	TIME [epoch: 11.5 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2085328345834623		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 3.2085328345834623 | validation: 4.202773716288863]
	TIME [epoch: 11.5 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2047006809126186		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 3.2047006809126186 | validation: 4.204678786353628]
	TIME [epoch: 11.5 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2028032651846994		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 3.2028032651846994 | validation: 4.205638249973271]
	TIME [epoch: 11.5 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206022585718088		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 3.206022585718088 | validation: 4.201215452689189]
	TIME [epoch: 11.5 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2076627400944506		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 3.2076627400944506 | validation: 4.208421163076167]
	TIME [epoch: 11.5 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206238469678867		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 3.206238469678867 | validation: 4.200891756184009]
	TIME [epoch: 11.5 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2092652597386486		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 3.2092652597386486 | validation: 4.199705157430473]
	TIME [epoch: 11.5 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204263140718434		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 3.204263140718434 | validation: 4.207045346809544]
	TIME [epoch: 11.5 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2084691442006923		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 3.2084691442006923 | validation: 4.207152653767209]
	TIME [epoch: 11.5 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209352915617671		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 3.209352915617671 | validation: 4.190167826188445]
	TIME [epoch: 11.5 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206698958984076		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 3.206698958984076 | validation: 4.197887250014888]
	TIME [epoch: 11.5 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203184983251011		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 3.203184983251011 | validation: 4.205077119416569]
	TIME [epoch: 11.5 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067709346441		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 3.2067709346441 | validation: 4.201363678281363]
	TIME [epoch: 11.5 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2050327028305468		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 3.2050327028305468 | validation: 4.207760631076087]
	TIME [epoch: 11.5 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2042913288248447		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 3.2042913288248447 | validation: 4.20430201962805]
	TIME [epoch: 11.5 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2083803534561453		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 3.2083803534561453 | validation: 4.201746827242427]
	TIME [epoch: 11.5 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.209954380462653		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 3.209954380462653 | validation: 4.200461381848079]
	TIME [epoch: 11.5 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202279626722995		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 3.202279626722995 | validation: 4.201461926647291]
	TIME [epoch: 11.5 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205705406571585		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 3.205705406571585 | validation: 4.20299514306131]
	TIME [epoch: 11.5 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205916542210246		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 3.205916542210246 | validation: 4.200635481653136]
	TIME [epoch: 11.5 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206398688190701		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 3.206398688190701 | validation: 4.19883843932594]
	TIME [epoch: 11.5 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205377076592245		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 3.205377076592245 | validation: 4.202198968664699]
	TIME [epoch: 11.5 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205437031625447		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 3.205437031625447 | validation: 4.196336767013603]
	TIME [epoch: 11.5 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205305327086077		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 3.205305327086077 | validation: 4.207582158652957]
	TIME [epoch: 11.5 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067128945101775		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 3.2067128945101775 | validation: 4.192211406099969]
	TIME [epoch: 11.5 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2100209619330977		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 3.2100209619330977 | validation: 4.201151685042555]
	TIME [epoch: 11.5 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202030909296402		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 3.202030909296402 | validation: 4.199716854258161]
	TIME [epoch: 11.5 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2063642182858		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 3.2063642182858 | validation: 4.1999337866567386]
	TIME [epoch: 11.5 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205146163133834		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 3.205146163133834 | validation: 4.2034375477942865]
	TIME [epoch: 11.5 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207040619678616		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 3.207040619678616 | validation: 4.200114184262662]
	TIME [epoch: 11.5 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207543334059566		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 3.207543334059566 | validation: 4.202857266172816]
	TIME [epoch: 11.5 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204291849702492		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 3.204291849702492 | validation: 4.201985500029036]
	TIME [epoch: 11.5 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204960641437324		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 3.204960641437324 | validation: 4.200331335929092]
	TIME [epoch: 11.5 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2056422718722155		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 3.2056422718722155 | validation: 4.201493677555799]
	TIME [epoch: 11.5 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206346297083846		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 3.206346297083846 | validation: 4.2014559815334325]
	TIME [epoch: 11.5 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205466700865177		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 3.205466700865177 | validation: 4.206418488534576]
	TIME [epoch: 11.5 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205672063704683		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 3.205672063704683 | validation: 4.199136714624603]
	TIME [epoch: 11.5 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078668590129267		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 3.2078668590129267 | validation: 4.203871309181924]
	TIME [epoch: 11.5 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2033638925176717		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 3.2033638925176717 | validation: 4.2011476007874835]
	TIME [epoch: 11.5 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20588507485405		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 3.20588507485405 | validation: 4.197818868980089]
	TIME [epoch: 11.5 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2041905752466695		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 3.2041905752466695 | validation: 4.19780426019683]
	TIME [epoch: 11.5 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204064289763826		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 3.204064289763826 | validation: 4.200859743137064]
	TIME [epoch: 11.5 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2071250881968374		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 3.2071250881968374 | validation: 4.209328071724826]
	TIME [epoch: 11.5 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2053921558629432		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 3.2053921558629432 | validation: 4.204908219984841]
	TIME [epoch: 11.5 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2018364900440806		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 3.2018364900440806 | validation: 4.197354132661888]
	TIME [epoch: 11.5 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2059132975631712		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 3.2059132975631712 | validation: 4.199526798797264]
	TIME [epoch: 11.5 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204626459155606		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 3.204626459155606 | validation: 4.2055476468420645]
	TIME [epoch: 11.5 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.20627105941665		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 3.20627105941665 | validation: 4.202673286300935]
	TIME [epoch: 11.5 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067960133864015		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 3.2067960133864015 | validation: 4.202183105235008]
	TIME [epoch: 11.5 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205573498961803		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 3.205573498961803 | validation: 4.202240255131307]
	TIME [epoch: 11.5 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207976045200335		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 3.207976045200335 | validation: 4.205957920051723]
	TIME [epoch: 11.5 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2054875019832068		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 3.2054875019832068 | validation: 4.204360024563844]
	TIME [epoch: 11.5 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205445029841484		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 3.205445029841484 | validation: 4.197926146411755]
	TIME [epoch: 11.5 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207192990752379		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 3.207192990752379 | validation: 4.2023511936870666]
	TIME [epoch: 11.5 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2069183354399513		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 3.2069183354399513 | validation: 4.2071912985257836]
	TIME [epoch: 11.5 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.208264904270146		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 3.208264904270146 | validation: 4.19967037028547]
	TIME [epoch: 11.5 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2060640824985116		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 3.2060640824985116 | validation: 4.201360615481887]
	TIME [epoch: 11.5 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057949142632194		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 3.2057949142632194 | validation: 4.2052494014821935]
	TIME [epoch: 11.5 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206321122396645		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 3.206321122396645 | validation: 4.197672935538135]
	TIME [epoch: 11.5 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205776268097903		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 3.205776268097903 | validation: 4.206363564170131]
	TIME [epoch: 11.5 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207360434285928		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 3.207360434285928 | validation: 4.208379060308344]
	TIME [epoch: 11.5 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203649527640965		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 3.203649527640965 | validation: 4.2040035847523765]
	TIME [epoch: 11.5 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204546026075109		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 3.204546026075109 | validation: 4.202002740641077]
	TIME [epoch: 11.5 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2055833213867926		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 3.2055833213867926 | validation: 4.2040521267746325]
	TIME [epoch: 11.5 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2028703141276695		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 3.2028703141276695 | validation: 4.201527713418544]
	TIME [epoch: 11.5 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2066609872703475		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 3.2066609872703475 | validation: 4.200412254279643]
	TIME [epoch: 11.5 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2045048503346196		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 3.2045048503346196 | validation: 4.207456060337204]
	TIME [epoch: 11.5 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.206399798285396		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 3.206399798285396 | validation: 4.203528765985345]
	TIME [epoch: 11.5 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2053797731421314		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 3.2053797731421314 | validation: 4.204225547892107]
	TIME [epoch: 11.5 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2021752469124722		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 3.2021752469124722 | validation: 4.196792109703673]
	TIME [epoch: 11.5 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2052803621640242		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 3.2052803621640242 | validation: 4.200933418288245]
	TIME [epoch: 11.5 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2067909432377166		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 3.2067909432377166 | validation: 4.204241746346108]
	TIME [epoch: 11.5 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203264121553682		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 3.203264121553682 | validation: 4.199208515391997]
	TIME [epoch: 11.5 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.203809076586111		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 3.203809076586111 | validation: 4.2003142172189785]
	TIME [epoch: 11.5 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.207261413551068		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 3.207261413551068 | validation: 4.197851439904907]
	TIME [epoch: 11.5 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2031069027783863		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 3.2031069027783863 | validation: 4.198925261410102]
	TIME [epoch: 11.5 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2052657839247374		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 3.2052657839247374 | validation: 4.212304562045627]
	TIME [epoch: 11.5 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2057291531944463		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 3.2057291531944463 | validation: 4.195379321107751]
	TIME [epoch: 11.5 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2078885333938105		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 3.2078885333938105 | validation: 4.203579679424725]
	TIME [epoch: 11.5 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2031431900799405		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 3.2031431900799405 | validation: 4.199771366654378]
	TIME [epoch: 11.5 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205727576154743		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 3.205727576154743 | validation: 4.20490457573907]
	TIME [epoch: 11.5 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2048181352014464		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 3.2048181352014464 | validation: 4.200043222152021]
	TIME [epoch: 11.5 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204101947457418		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 3.204101947457418 | validation: 4.200466672134739]
	TIME [epoch: 11.5 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.205907833311437		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 3.205907833311437 | validation: 4.203939995211135]
	TIME [epoch: 11.5 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2061150208325992		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 3.2061150208325992 | validation: 4.204744218560546]
	TIME [epoch: 11.5 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.204599598500247		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 3.204599598500247 | validation: 4.20204801610848]
	TIME [epoch: 11.5 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2039721643404713		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 3.2039721643404713 | validation: 4.200179227768362]
	TIME [epoch: 11.5 sec]
Finished training in 23185.549 seconds.
