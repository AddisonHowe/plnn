Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r0', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1070031074

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.62552411517784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.62552411517784 | validation: 9.617545567378894]
	TIME [epoch: 53.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.07974921513249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.07974921513249 | validation: 8.170103565483597]
	TIME [epoch: 9.78 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.092916009250235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.092916009250235 | validation: 6.359410693093353]
	TIME [epoch: 9.79 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.629948767984326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.629948767984326 | validation: 5.321328722642007]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.823063087388526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.823063087388526 | validation: 6.111970101530232]
	TIME [epoch: 9.75 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.706503247957386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.706503247957386 | validation: 5.2949111762432075]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.636701017190602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.636701017190602 | validation: 5.727526124194473]
	TIME [epoch: 9.76 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.747829548169686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.747829548169686 | validation: 5.080594525177151]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.527483290618332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.527483290618332 | validation: 5.660269125580738]
	TIME [epoch: 9.77 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.509627690291508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.509627690291508 | validation: 5.494512801039445]
	TIME [epoch: 9.75 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.514573422889748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.514573422889748 | validation: 5.155547359339278]
	TIME [epoch: 9.74 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.546464940686972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.546464940686972 | validation: 5.042459311610487]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.467690985623233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.467690985623233 | validation: 5.355836710197131]
	TIME [epoch: 9.76 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.43029242436919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.43029242436919 | validation: 5.114273984019042]
	TIME [epoch: 9.75 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.4560696970670675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.4560696970670675 | validation: 5.452715320741837]
	TIME [epoch: 9.75 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.630185247322035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.630185247322035 | validation: 5.012089323501859]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.298239594957142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.298239594957142 | validation: 4.893986005202275]
	TIME [epoch: 9.77 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.267589751544461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.267589751544461 | validation: 5.498310658556436]
	TIME [epoch: 9.75 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.329653676648697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.329653676648697 | validation: 5.4788305427028625]
	TIME [epoch: 9.75 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.2609599434390075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2609599434390075 | validation: 4.995033519548868]
	TIME [epoch: 9.76 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1479624821993415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1479624821993415 | validation: 4.66975407709278]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.464236580215033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.464236580215033 | validation: 5.919402929886258]
	TIME [epoch: 9.75 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3655819759087535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3655819759087535 | validation: 4.755895256645164]
	TIME [epoch: 9.74 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.176061973205452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.176061973205452 | validation: 4.961507309066085]
	TIME [epoch: 9.77 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.214240281875684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.214240281875684 | validation: 4.713352772111373]
	TIME [epoch: 9.74 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.078009315108085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.078009315108085 | validation: 4.562278830289066]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.113285872205556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.113285872205556 | validation: 4.454144165278753]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3188255747491455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3188255747491455 | validation: 4.74055301783696]
	TIME [epoch: 9.77 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.109925157379395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.109925157379395 | validation: 4.49851039753542]
	TIME [epoch: 9.75 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.988436008259453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.988436008259453 | validation: 4.437320071244027]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.970049930654191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.970049930654191 | validation: 5.326047571933928]
	TIME [epoch: 9.74 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.902123927910354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.902123927910354 | validation: 5.557695860946392]
	TIME [epoch: 9.76 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.119297699256589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.119297699256589 | validation: 4.196821189330369]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.686243624272392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686243624272392 | validation: 5.273799788286767]
	TIME [epoch: 9.74 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.1755218010231765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1755218010231765 | validation: 4.8366756269503615]
	TIME [epoch: 9.73 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7676245317860255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7676245317860255 | validation: 4.1975786933934085]
	TIME [epoch: 9.77 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.544047719878288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.544047719878288 | validation: 4.859689370397063]
	TIME [epoch: 9.73 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.691852699647864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.691852699647864 | validation: 3.9916428755473494]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.417356297559321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.417356297559321 | validation: 3.9113320808757512]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.430934929610882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.430934929610882 | validation: 3.785343389744119]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.467440470895685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.467440470895685 | validation: 3.9847053555691048]
	TIME [epoch: 9.74 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.295654618631451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.295654618631451 | validation: 3.938606274773751]
	TIME [epoch: 9.73 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.435037248168771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.435037248168771 | validation: 3.7010283291850117]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.357247035739947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.357247035739947 | validation: 3.850732248035846]
	TIME [epoch: 9.74 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.396413867975916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.396413867975916 | validation: 3.947749150112749]
	TIME [epoch: 9.74 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.309187597913167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.309187597913167 | validation: 3.839896740255004]
	TIME [epoch: 9.74 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.441416350812778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.441416350812778 | validation: 4.36489565114774]
	TIME [epoch: 9.76 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.37892432858528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.37892432858528 | validation: 3.6499989119236553]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.274935493792937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.274935493792937 | validation: 3.861773433851553]
	TIME [epoch: 9.74 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.305476356216625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.305476356216625 | validation: 3.6227245858512127]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2738503362202325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2738503362202325 | validation: 3.6650492977575095]
	TIME [epoch: 9.76 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.432915518223441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.432915518223441 | validation: 3.681140771842548]
	TIME [epoch: 9.73 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.360809432698021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.360809432698021 | validation: 4.405023683211229]
	TIME [epoch: 9.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.918463066710546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.918463066710546 | validation: 4.595218158042869]
	TIME [epoch: 9.73 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.528978590254926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.528978590254926 | validation: 5.578349392098943]
	TIME [epoch: 9.75 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.792632830684423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.792632830684423 | validation: 3.7004940791474286]
	TIME [epoch: 9.74 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.269994362258325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.269994362258325 | validation: 3.6492142762408255]
	TIME [epoch: 9.74 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.988150598130664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.988150598130664 | validation: 4.270372273994546]
	TIME [epoch: 9.73 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5130739490382155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5130739490382155 | validation: 3.7805787171889422]
	TIME [epoch: 9.75 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.35271025430872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.35271025430872 | validation: 3.714903241522151]
	TIME [epoch: 9.73 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2491929520664025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2491929520664025 | validation: 3.5949565134832278]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.755577770480627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.755577770480627 | validation: 3.822211315167877]
	TIME [epoch: 9.75 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.442743914483258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.442743914483258 | validation: 3.770441167199581]
	TIME [epoch: 9.73 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.35386925697331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.35386925697331 | validation: 3.940340454424037]
	TIME [epoch: 9.72 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.314496312143781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.314496312143781 | validation: 3.6765098211165754]
	TIME [epoch: 9.72 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.193626381303828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.193626381303828 | validation: 3.8734255833765405]
	TIME [epoch: 9.74 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.238871169863556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.238871169863556 | validation: 3.5643887015912674]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.104143358829324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.104143358829324 | validation: 4.647449304231831]
	TIME [epoch: 9.73 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.741562373980929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.741562373980929 | validation: 4.784250051410766]
	TIME [epoch: 9.74 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.796470870081763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.796470870081763 | validation: 3.6082950121859425]
	TIME [epoch: 9.76 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.134067630683734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.134067630683734 | validation: 3.769357789446477]
	TIME [epoch: 9.74 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.075417810783457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.075417810783457 | validation: 3.5526683658911242]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.119008995906728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.119008995906728 | validation: 3.5221493717807824]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.17071487602574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.17071487602574 | validation: 3.456464436830322]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.026546262481769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.026546262481769 | validation: 3.506620503013827]
	TIME [epoch: 9.75 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9771202527494385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9771202527494385 | validation: 3.656205578405377]
	TIME [epoch: 9.74 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.075003796215991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.075003796215991 | validation: 3.4227013797982684]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.165966570954448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.165966570954448 | validation: 4.249214209911769]
	TIME [epoch: 9.76 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.151771162451782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.151771162451782 | validation: 3.533616409516404]
	TIME [epoch: 9.74 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0113828525800885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0113828525800885 | validation: 3.693714077993632]
	TIME [epoch: 9.74 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.979303258106109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.979303258106109 | validation: 4.258757265477594]
	TIME [epoch: 9.74 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2328118578539655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2328118578539655 | validation: 3.7945908843214147]
	TIME [epoch: 9.78 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.085942556998161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085942556998161 | validation: 3.3843811797042]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.026856089555907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.026856089555907 | validation: 3.5417292578610966]
	TIME [epoch: 9.74 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.022727348832085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.022727348832085 | validation: 3.4723651493860817]
	TIME [epoch: 9.75 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.978947298683937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.978947298683937 | validation: 3.5256711161992325]
	TIME [epoch: 9.74 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.10183523513339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.10183523513339 | validation: 4.464433352348353]
	TIME [epoch: 9.74 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.069164128424871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.069164128424871 | validation: 3.651407320120743]
	TIME [epoch: 9.74 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.084559508392109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.084559508392109 | validation: 3.350999869712207]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0228135082133845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0228135082133845 | validation: 3.45483371978527]
	TIME [epoch: 9.74 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.078067106769623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.078067106769623 | validation: 3.898176624555678]
	TIME [epoch: 9.73 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.027818996005269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027818996005269 | validation: 3.3331778960411347]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.179523036823165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.179523036823165 | validation: 3.835098197344787]
	TIME [epoch: 9.74 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.054252099177311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.054252099177311 | validation: 3.460736859457999]
	TIME [epoch: 9.74 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9282644626498175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9282644626498175 | validation: 3.4888508293647185]
	TIME [epoch: 9.74 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.052489791714359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.052489791714359 | validation: 3.3569223606870118]
	TIME [epoch: 9.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.072720600997383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.072720600997383 | validation: 3.3272855459671677]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.215584602414054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.215584602414054 | validation: 3.7679018477325577]
	TIME [epoch: 9.74 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.246941327276934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.246941327276934 | validation: 4.76306918355936]
	TIME [epoch: 9.73 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2604558571512845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2604558571512845 | validation: 4.209737772602419]
	TIME [epoch: 9.74 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.211362827401549		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.211362827401549 | validation: 3.7844024357467583]
	TIME [epoch: 9.73 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.038553667037036		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 4.038553667037036 | validation: 3.907595915506868]
	TIME [epoch: 9.73 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.042591380092636		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 4.042591380092636 | validation: 3.4675736338944603]
	TIME [epoch: 9.74 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9326687088366747		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.9326687088366747 | validation: 4.113595172310738]
	TIME [epoch: 9.73 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.075490550426462		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 4.075490550426462 | validation: 3.565017859036989]
	TIME [epoch: 9.72 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.94587793434401		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 3.94587793434401 | validation: 3.4734360317447908]
	TIME [epoch: 9.72 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8867110202487574		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.8867110202487574 | validation: 3.678924423926034]
	TIME [epoch: 9.76 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.153981064291754		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 4.153981064291754 | validation: 3.4289125809944387]
	TIME [epoch: 9.73 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.976847247709311		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.976847247709311 | validation: 3.49035867356839]
	TIME [epoch: 9.72 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.886468090111817		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.886468090111817 | validation: 3.4570298995902475]
	TIME [epoch: 9.74 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.930978977046562		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.930978977046562 | validation: 4.009871494895295]
	TIME [epoch: 9.73 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.952385488633065		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.952385488633065 | validation: 3.4430323255535003]
	TIME [epoch: 9.72 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.938138999751433		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.938138999751433 | validation: 3.552827166431463]
	TIME [epoch: 9.73 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.089842638905564		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 4.089842638905564 | validation: 3.9741785524007085]
	TIME [epoch: 9.75 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.291599883578799		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 4.291599883578799 | validation: 3.4354232481636418]
	TIME [epoch: 9.73 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.897905430849915		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.897905430849915 | validation: 4.151396536993162]
	TIME [epoch: 9.73 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.114489820860304		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 4.114489820860304 | validation: 3.4318185369186494]
	TIME [epoch: 9.75 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.064542621511171		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 4.064542621511171 | validation: 3.350249150278044]
	TIME [epoch: 9.73 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9205413474422373		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.9205413474422373 | validation: 3.7050123825356813]
	TIME [epoch: 9.72 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.054833870308509		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 4.054833870308509 | validation: 3.4915537716707057]
	TIME [epoch: 9.73 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862479334467242		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 3.862479334467242 | validation: 4.418932784295551]
	TIME [epoch: 9.73 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.327452965532823		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 4.327452965532823 | validation: 3.520538809750257]
	TIME [epoch: 9.72 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.189612307468282		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 4.189612307468282 | validation: 3.3475084691650467]
	TIME [epoch: 9.73 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.040379565726643		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 4.040379565726643 | validation: 3.47506628026468]
	TIME [epoch: 9.75 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.981424556441631		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 3.981424556441631 | validation: 3.694411452645521]
	TIME [epoch: 9.73 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.936118233140099		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 3.936118233140099 | validation: 3.809503695657018]
	TIME [epoch: 9.72 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.856040377569399		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.856040377569399 | validation: 3.865462387733925]
	TIME [epoch: 9.74 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9748849841971774		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.9748849841971774 | validation: 3.632207109682057]
	TIME [epoch: 9.74 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.918724437521945		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 3.918724437521945 | validation: 3.488020463798896]
	TIME [epoch: 9.73 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8677992283574225		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.8677992283574225 | validation: 3.3292971922110213]
	TIME [epoch: 9.72 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9544287085229906		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.9544287085229906 | validation: 3.296654663626404]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9042413384014716		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.9042413384014716 | validation: 3.3531668401790298]
	TIME [epoch: 9.73 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9283885657898594		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.9283885657898594 | validation: 3.2364931547495805]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.837390810490361		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.837390810490361 | validation: 3.455956745137204]
	TIME [epoch: 9.76 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.003227317919356		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 4.003227317919356 | validation: 3.90899871634883]
	TIME [epoch: 9.75 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9678389168841632		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.9678389168841632 | validation: 3.3612564309661037]
	TIME [epoch: 9.75 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.884508379359622		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.884508379359622 | validation: 3.3037295526145773]
	TIME [epoch: 9.74 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.078170166144881		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 4.078170166144881 | validation: 4.1588621018264185]
	TIME [epoch: 9.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.154823467801679		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 4.154823467801679 | validation: 3.3238981208263523]
	TIME [epoch: 9.76 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.021515956364182		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 4.021515956364182 | validation: 3.3640482618535574]
	TIME [epoch: 9.74 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9899770528548815		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.9899770528548815 | validation: 3.274132737373719]
	TIME [epoch: 9.77 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.099155593733566		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 4.099155593733566 | validation: 3.4167301400059467]
	TIME [epoch: 9.75 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.055929525617691		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 4.055929525617691 | validation: 3.34426739542115]
	TIME [epoch: 9.75 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.640822957869597		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 4.640822957869597 | validation: 3.349466136445965]
	TIME [epoch: 9.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.026563842284996		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 4.026563842284996 | validation: 3.385845422058374]
	TIME [epoch: 9.76 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8945570400093863		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.8945570400093863 | validation: 3.339267889513848]
	TIME [epoch: 9.73 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827563055999142		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.827563055999142 | validation: 3.335271737352663]
	TIME [epoch: 9.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.854843657038489		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.854843657038489 | validation: 3.4724910738557004]
	TIME [epoch: 9.75 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.887206238640116		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.887206238640116 | validation: 3.400387989253462]
	TIME [epoch: 9.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8481163828182567		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.8481163828182567 | validation: 3.269591985788178]
	TIME [epoch: 9.75 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8303462840862013		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.8303462840862013 | validation: 3.8410696440134626]
	TIME [epoch: 9.75 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8559959101274037		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.8559959101274037 | validation: 4.019574306477648]
	TIME [epoch: 9.75 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1142408657510385		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 4.1142408657510385 | validation: 3.7898945920035305]
	TIME [epoch: 9.74 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.964979305381734		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.964979305381734 | validation: 3.615120726065384]
	TIME [epoch: 9.75 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.909411393833178		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.909411393833178 | validation: 3.445593054674008]
	TIME [epoch: 9.78 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8771305912198075		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 3.8771305912198075 | validation: 3.2667724480825413]
	TIME [epoch: 9.75 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848492875823591		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.848492875823591 | validation: 3.266679572459909]
	TIME [epoch: 9.75 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8656909521422897		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 3.8656909521422897 | validation: 3.358832663533539]
	TIME [epoch: 9.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.852359034461844		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 3.852359034461844 | validation: 3.2820509381468312]
	TIME [epoch: 9.75 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.193511619797136		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 4.193511619797136 | validation: 3.355892989844299]
	TIME [epoch: 9.74 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8304971288746494		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.8304971288746494 | validation: 3.2777859177028836]
	TIME [epoch: 9.76 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8757331106312476		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.8757331106312476 | validation: 3.271651415905015]
	TIME [epoch: 9.77 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.812865214238083		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.812865214238083 | validation: 3.483953639301966]
	TIME [epoch: 9.75 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.982072559006047		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.982072559006047 | validation: 3.3081123471009035]
	TIME [epoch: 9.73 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.203105816581808		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 4.203105816581808 | validation: 3.541313032002993]
	TIME [epoch: 9.75 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9026469619717767		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 3.9026469619717767 | validation: 3.406566639389559]
	TIME [epoch: 9.75 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.982220719153903		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.982220719153903 | validation: 3.219375286772183]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.134170845036014		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 4.134170845036014 | validation: 3.4779114271575873]
	TIME [epoch: 9.73 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.062033448296624		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 4.062033448296624 | validation: 4.725843120251244]
	TIME [epoch: 9.77 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.123201102253104		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 4.123201102253104 | validation: 3.414387483270151]
	TIME [epoch: 9.72 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8291787392408785		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.8291787392408785 | validation: 3.4845401094851765]
	TIME [epoch: 9.73 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8078521055646832		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 3.8078521055646832 | validation: 3.5781104974918936]
	TIME [epoch: 9.73 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9942214329757917		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 3.9942214329757917 | validation: 3.48009176860716]
	TIME [epoch: 9.74 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8124395963791953		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 3.8124395963791953 | validation: 4.032359107767722]
	TIME [epoch: 9.73 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0200119243038515		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 4.0200119243038515 | validation: 3.648789792229802]
	TIME [epoch: 9.73 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9353069989274787		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.9353069989274787 | validation: 4.061360945363627]
	TIME [epoch: 9.75 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.021506625285282		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 4.021506625285282 | validation: 3.3433468828769004]
	TIME [epoch: 9.74 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8169436637119176		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 3.8169436637119176 | validation: 3.2597682442656066]
	TIME [epoch: 9.72 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8158938204251576		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 3.8158938204251576 | validation: 3.2062604916653203]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827461356874103		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 3.827461356874103 | validation: 3.630789000953594]
	TIME [epoch: 9.75 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8739590996871565		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 3.8739590996871565 | validation: 3.935448712769629]
	TIME [epoch: 9.73 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.04905716996373		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 4.04905716996373 | validation: 3.946130431701655]
	TIME [epoch: 9.73 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.048214982684537		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 4.048214982684537 | validation: 3.325981744217585]
	TIME [epoch: 9.75 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7749872458449536		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 3.7749872458449536 | validation: 3.857446459156563]
	TIME [epoch: 9.75 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.162434644200086		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 4.162434644200086 | validation: 3.493070117487935]
	TIME [epoch: 9.74 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.028533089782947		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 4.028533089782947 | validation: 3.755026452192982]
	TIME [epoch: 9.73 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9097205977432923		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.9097205977432923 | validation: 3.263980769303072]
	TIME [epoch: 9.74 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783361813033957		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 3.783361813033957 | validation: 3.619157740691037]
	TIME [epoch: 9.73 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.099289781207774		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 4.099289781207774 | validation: 3.2021963848206556]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8719761228467062		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 3.8719761228467062 | validation: 3.2611096229608747]
	TIME [epoch: 9.77 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810873199750231		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 3.810873199750231 | validation: 3.237345872360604]
	TIME [epoch: 9.75 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.817460623000489		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.817460623000489 | validation: 3.3451906588653824]
	TIME [epoch: 9.75 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9102617182133463		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 3.9102617182133463 | validation: 3.2341766174468143]
	TIME [epoch: 9.76 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.940661985700907		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.940661985700907 | validation: 3.7872750364455903]
	TIME [epoch: 9.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9168723496407436		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 3.9168723496407436 | validation: 3.3148741382711537]
	TIME [epoch: 9.76 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7627958087098476		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 3.7627958087098476 | validation: 3.300669883031885]
	TIME [epoch: 9.75 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7870843342275577		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.7870843342275577 | validation: 3.1629574976398356]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8139596038937826		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 3.8139596038937826 | validation: 3.4302951348395205]
	TIME [epoch: 9.76 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013007140581905		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 4.013007140581905 | validation: 3.5548848562931847]
	TIME [epoch: 9.75 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.002387433213556		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 4.002387433213556 | validation: 3.421725008766286]
	TIME [epoch: 9.75 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7815168825846093		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 3.7815168825846093 | validation: 3.198770757494683]
	TIME [epoch: 9.76 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8516152919194773		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 3.8516152919194773 | validation: 3.516578560387548]
	TIME [epoch: 9.75 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8007359439875197		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 3.8007359439875197 | validation: 3.5133392596395217]
	TIME [epoch: 9.75 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013960273101184		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 4.013960273101184 | validation: 3.2239373800565296]
	TIME [epoch: 9.75 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832054378269395		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 3.832054378269395 | validation: 3.5455903317512854]
	TIME [epoch: 9.77 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803505756355764		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 3.803505756355764 | validation: 3.2880529250541963]
	TIME [epoch: 9.75 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.867984943176951		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.867984943176951 | validation: 3.3091571511300466]
	TIME [epoch: 9.75 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7819723065940933		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.7819723065940933 | validation: 3.1910764211249503]
	TIME [epoch: 9.75 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9010622071024263		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 3.9010622071024263 | validation: 3.165012287901427]
	TIME [epoch: 9.76 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.873168495173201		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.873168495173201 | validation: 4.0165642729896875]
	TIME [epoch: 9.75 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9576374950985262		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.9576374950985262 | validation: 3.2748143591252474]
	TIME [epoch: 9.75 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.735000898289678		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.735000898289678 | validation: 3.220386162427623]
	TIME [epoch: 9.78 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7595765424040755		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 3.7595765424040755 | validation: 4.256127286201621]
	TIME [epoch: 9.76 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.061192769106835		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 4.061192769106835 | validation: 3.3520275630807186]
	TIME [epoch: 9.76 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9106848606615316		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.9106848606615316 | validation: 3.488782364264261]
	TIME [epoch: 9.76 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7610932446767267		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.7610932446767267 | validation: 3.204206412394858]
	TIME [epoch: 9.79 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9609139716881274		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 3.9609139716881274 | validation: 3.838132566650926]
	TIME [epoch: 9.77 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.994413052130523		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 3.994413052130523 | validation: 3.4231905743113464]
	TIME [epoch: 9.76 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.871808471876198		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 3.871808471876198 | validation: 3.485063071030561]
	TIME [epoch: 9.79 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848922531054183		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.848922531054183 | validation: 3.277814942639707]
	TIME [epoch: 9.77 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7644511018371754		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 3.7644511018371754 | validation: 3.2483147289653687]
	TIME [epoch: 9.77 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823360322898675		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.823360322898675 | validation: 3.240560083407538]
	TIME [epoch: 9.76 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.778522571370721		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 3.778522571370721 | validation: 3.203744830154811]
	TIME [epoch: 9.77 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8678271046755066		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 3.8678271046755066 | validation: 3.501698855989839]
	TIME [epoch: 9.75 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.831771607464643		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 3.831771607464643 | validation: 3.4369400766458114]
	TIME [epoch: 9.76 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.739909416465788		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.739909416465788 | validation: 3.46014549557735]
	TIME [epoch: 9.78 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7492800877523877		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 3.7492800877523877 | validation: 3.3455052909175875]
	TIME [epoch: 9.77 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8146095185191817		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.8146095185191817 | validation: 3.2787508759314767]
	TIME [epoch: 9.75 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.781672317165692		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.781672317165692 | validation: 3.257988251572641]
	TIME [epoch: 9.78 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.684889469239086		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.684889469239086 | validation: 3.1978265716377123]
	TIME [epoch: 9.75 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7004977537311903		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 3.7004977537311903 | validation: 3.3912989379605887]
	TIME [epoch: 9.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8802976486907683		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 3.8802976486907683 | validation: 3.168221426499721]
	TIME [epoch: 9.73 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.825266355617232		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 3.825266355617232 | validation: 3.2592593313110787]
	TIME [epoch: 9.75 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827423775562501		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 3.827423775562501 | validation: 3.2850267730752023]
	TIME [epoch: 9.73 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.741054555083333		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 3.741054555083333 | validation: 3.31843876962799]
	TIME [epoch: 9.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.854070612927741		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.854070612927741 | validation: 3.240157047241096]
	TIME [epoch: 9.75 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8276526166041265		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.8276526166041265 | validation: 3.427386737577807]
	TIME [epoch: 9.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8441505886105167		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 3.8441505886105167 | validation: 3.1477641585123126]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.798121716184152		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 3.798121716184152 | validation: 3.6093014231974285]
	TIME [epoch: 9.76 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.899225008334077		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 3.899225008334077 | validation: 3.2422317153077778]
	TIME [epoch: 9.75 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8003993327878325		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.8003993327878325 | validation: 3.300558191175964]
	TIME [epoch: 9.75 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7106799695158754		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 3.7106799695158754 | validation: 3.8693864513265295]
	TIME [epoch: 9.75 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.906497839966245		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.906497839966245 | validation: 3.213959530688746]
	TIME [epoch: 9.77 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8590736661905276		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 3.8590736661905276 | validation: 3.462477150701232]
	TIME [epoch: 9.75 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.780986224555188		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 3.780986224555188 | validation: 3.212260079822514]
	TIME [epoch: 9.75 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7061902703817253		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 3.7061902703817253 | validation: 3.1802596380858255]
	TIME [epoch: 9.75 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827935181693821		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 3.827935181693821 | validation: 3.1880648574948927]
	TIME [epoch: 9.75 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.757900601863889		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 3.757900601863889 | validation: 3.3209242527427354]
	TIME [epoch: 9.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.71845996602151		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 3.71845996602151 | validation: 3.4613893042607207]
	TIME [epoch: 9.74 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.777836153763149		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 3.777836153763149 | validation: 3.1173355026908176]
	TIME [epoch: 9.76 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7098679805163313		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 3.7098679805163313 | validation: 3.3277417835617182]
	TIME [epoch: 9.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7088969125503426		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 3.7088969125503426 | validation: 3.1879334281093326]
	TIME [epoch: 9.73 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.675425796289339		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.675425796289339 | validation: 3.113963215800829]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662161170271395		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 3.662161170271395 | validation: 3.124704000354293]
	TIME [epoch: 9.75 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.661098568954729		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 3.661098568954729 | validation: 3.255768171096608]
	TIME [epoch: 9.74 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.702382160632781		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 3.702382160632781 | validation: 3.4073523685970475]
	TIME [epoch: 9.74 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7341790492457294		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.7341790492457294 | validation: 3.2656443618675723]
	TIME [epoch: 9.77 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.731763924678931		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 3.731763924678931 | validation: 3.162659862843155]
	TIME [epoch: 9.74 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6759438256302603		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.6759438256302603 | validation: 3.093752352577116]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.700062523376455		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 3.700062523376455 | validation: 3.132578037269106]
	TIME [epoch: 9.74 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6992265849009116		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 3.6992265849009116 | validation: 3.2358073282487543]
	TIME [epoch: 9.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6493759620527784		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 3.6493759620527784 | validation: 3.131374802015042]
	TIME [epoch: 9.72 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.67222121977478		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 3.67222121977478 | validation: 3.0931501570229636]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.634459839300584		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.634459839300584 | validation: 3.2621243682852543]
	TIME [epoch: 9.75 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.725156402269863		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.725156402269863 | validation: 3.115234265161927]
	TIME [epoch: 9.75 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7260713840569393		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.7260713840569393 | validation: 3.1718891465063566]
	TIME [epoch: 9.73 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.738161023039203		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.738161023039203 | validation: 3.24663917465333]
	TIME [epoch: 9.73 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.723703332346376		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 3.723703332346376 | validation: 3.1699560795589874]
	TIME [epoch: 9.75 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.764599925780695		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.764599925780695 | validation: 3.235991742895957]
	TIME [epoch: 9.74 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7078263251664816		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.7078263251664816 | validation: 3.3165220007346057]
	TIME [epoch: 9.73 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.750361693608326		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 3.750361693608326 | validation: 3.320193058272679]
	TIME [epoch: 9.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6764099812025535		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 3.6764099812025535 | validation: 3.1806475626514357]
	TIME [epoch: 9.76 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7947867103913175		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.7947867103913175 | validation: 3.211458035159767]
	TIME [epoch: 9.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6629608546414474		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 3.6629608546414474 | validation: 3.0981074189805127]
	TIME [epoch: 9.75 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7454755627724268		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.7454755627724268 | validation: 3.1862576841930226]
	TIME [epoch: 9.75 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.655654617886698		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 3.655654617886698 | validation: 3.350817123903692]
	TIME [epoch: 9.74 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7068368616810035		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 3.7068368616810035 | validation: 3.1472939570356244]
	TIME [epoch: 9.73 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.650253560606651		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 3.650253560606651 | validation: 3.1281189834154226]
	TIME [epoch: 9.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.756892996596112		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 3.756892996596112 | validation: 3.2812073530739463]
	TIME [epoch: 9.76 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.645562319296542		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.645562319296542 | validation: 3.210721015316858]
	TIME [epoch: 9.73 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7806981492635643		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.7806981492635643 | validation: 3.198344427387111]
	TIME [epoch: 9.72 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.688857091671513		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.688857091671513 | validation: 3.1619966134818]
	TIME [epoch: 9.74 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.63473076784568		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.63473076784568 | validation: 3.546903237490941]
	TIME [epoch: 9.72 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8273026474076937		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 3.8273026474076937 | validation: 3.124259439381143]
	TIME [epoch: 9.74 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6874941161362584		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 3.6874941161362584 | validation: 3.164033966160206]
	TIME [epoch: 9.74 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6527531475821347		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.6527531475821347 | validation: 3.536382379024477]
	TIME [epoch: 9.75 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.768403501244623		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 3.768403501244623 | validation: 3.3644150662897765]
	TIME [epoch: 9.74 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6814549555711102		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.6814549555711102 | validation: 3.5343597215709663]
	TIME [epoch: 9.72 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6678425165503703		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 3.6678425165503703 | validation: 3.226777785236479]
	TIME [epoch: 9.76 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.697997710025085		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.697997710025085 | validation: 3.3671224877541284]
	TIME [epoch: 9.73 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7638853694831824		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.7638853694831824 | validation: 3.3140863391533415]
	TIME [epoch: 9.73 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7488937248021896		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 3.7488937248021896 | validation: 3.236080878152041]
	TIME [epoch: 9.76 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.683088016159323		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.683088016159323 | validation: 3.3868203954116782]
	TIME [epoch: 9.75 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6650058583932776		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.6650058583932776 | validation: 3.2604008799599327]
	TIME [epoch: 9.74 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.75515857743583		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.75515857743583 | validation: 3.101510424944396]
	TIME [epoch: 9.74 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7080771272840694		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.7080771272840694 | validation: 3.2004979399688693]
	TIME [epoch: 9.75 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6588318254384427		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 3.6588318254384427 | validation: 3.3098273526639526]
	TIME [epoch: 9.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.698693315414249		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 3.698693315414249 | validation: 3.109991548867753]
	TIME [epoch: 9.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6788434190591532		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.6788434190591532 | validation: 3.1499153283225843]
	TIME [epoch: 9.75 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7025521237824237		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.7025521237824237 | validation: 3.1663929486640394]
	TIME [epoch: 9.74 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60586999793873		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.60586999793873 | validation: 3.152245019679409]
	TIME [epoch: 9.74 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6397632180036874		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.6397632180036874 | validation: 3.308954747146572]
	TIME [epoch: 9.74 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6969308813868365		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 3.6969308813868365 | validation: 3.180118205344519]
	TIME [epoch: 9.75 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6042553050505077		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 3.6042553050505077 | validation: 3.1108059605173937]
	TIME [epoch: 9.73 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.663964920168483		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 3.663964920168483 | validation: 3.11931511311101]
	TIME [epoch: 9.74 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6897769620175858		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.6897769620175858 | validation: 3.21396230279445]
	TIME [epoch: 9.75 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6394873799448098		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 3.6394873799448098 | validation: 3.4187245384139056]
	TIME [epoch: 9.73 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.729153511313627		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 3.729153511313627 | validation: 3.1447782776317412]
	TIME [epoch: 9.74 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6928875563997225		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 3.6928875563997225 | validation: 3.1640286492356218]
	TIME [epoch: 9.75 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.730222490390741		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.730222490390741 | validation: 3.1487637639368007]
	TIME [epoch: 9.73 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.64801331812684		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 3.64801331812684 | validation: 3.164389930088687]
	TIME [epoch: 9.73 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625208574801995		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.625208574801995 | validation: 3.1618905571297184]
	TIME [epoch: 9.73 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.650710423146889		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.650710423146889 | validation: 3.1851816192923224]
	TIME [epoch: 9.74 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640189899573258		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.640189899573258 | validation: 3.1929766797117685]
	TIME [epoch: 9.74 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618735132079022		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.618735132079022 | validation: 3.187223702989499]
	TIME [epoch: 9.73 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.704402641352273		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 3.704402641352273 | validation: 3.1411182667844173]
	TIME [epoch: 9.76 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6305104670129724		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.6305104670129724 | validation: 3.0915327089369633]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.689911273399448		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.689911273399448 | validation: 3.1084662414486663]
	TIME [epoch: 9.75 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8715404673661156		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.8715404673661156 | validation: 3.2617262025485334]
	TIME [epoch: 9.74 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.67936382574405		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 3.67936382574405 | validation: 3.2348520302586046]
	TIME [epoch: 9.73 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6661363190287326		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 3.6661363190287326 | validation: 3.115324740132461]
	TIME [epoch: 9.73 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.719944533662585		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.719944533662585 | validation: 3.202772883739568]
	TIME [epoch: 9.73 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643310952391397		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.643310952391397 | validation: 3.1422786618186143]
	TIME [epoch: 9.74 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.632910648956721		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.632910648956721 | validation: 3.1660976139581964]
	TIME [epoch: 9.72 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638607940331363		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 3.638607940331363 | validation: 3.118876992770597]
	TIME [epoch: 9.71 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6547025564581914		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.6547025564581914 | validation: 3.1582640114386082]
	TIME [epoch: 9.72 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608920311984318		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.608920311984318 | validation: 3.0757981724484194]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6542812736328214		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.6542812736328214 | validation: 3.123882358175336]
	TIME [epoch: 9.74 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.585992679643963		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.585992679643963 | validation: 3.074734495491299]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.571356943005265		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.571356943005265 | validation: 3.074049478850319]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6067988586282795		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 3.6067988586282795 | validation: 3.2544654497925958]
	TIME [epoch: 9.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6565647910503083		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.6565647910503083 | validation: 3.0655583997553904]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.621974395817054		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 3.621974395817054 | validation: 3.173178940838984]
	TIME [epoch: 9.76 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.676154131672334		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.676154131672334 | validation: 3.1107253958292813]
	TIME [epoch: 9.76 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5845675768041487		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 3.5845675768041487 | validation: 3.074122390354796]
	TIME [epoch: 9.75 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5911197358180047		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 3.5911197358180047 | validation: 3.0711587455536673]
	TIME [epoch: 9.74 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6823695079602827		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.6823695079602827 | validation: 3.220679803560383]
	TIME [epoch: 9.76 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6232814164591254		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 3.6232814164591254 | validation: 3.0723238648505076]
	TIME [epoch: 9.74 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.56432832352834		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.56432832352834 | validation: 3.076637532444621]
	TIME [epoch: 9.74 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.703394205599765		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 3.703394205599765 | validation: 3.1689048880226824]
	TIME [epoch: 9.73 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6170417042446084		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 3.6170417042446084 | validation: 3.276251962959353]
	TIME [epoch: 9.76 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.693603996283835		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.693603996283835 | validation: 3.0918515185723967]
	TIME [epoch: 9.74 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.695831220822799		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.695831220822799 | validation: 3.1507257837644693]
	TIME [epoch: 9.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.582040881572053		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 3.582040881572053 | validation: 3.455328923320659]
	TIME [epoch: 9.75 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.699662127320798		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 3.699662127320798 | validation: 3.1166037701276115]
	TIME [epoch: 9.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.697682035312922		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.697682035312922 | validation: 3.0853202918750786]
	TIME [epoch: 9.74 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.707684820063142		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.707684820063142 | validation: 3.0962140897811925]
	TIME [epoch: 9.74 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6482927966566434		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.6482927966566434 | validation: 3.131970875387415]
	TIME [epoch: 9.76 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.623818023356068		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.623818023356068 | validation: 3.1281038954153657]
	TIME [epoch: 9.74 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605280817430992		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.605280817430992 | validation: 3.0800723719912515]
	TIME [epoch: 9.74 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.652070346765447		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 3.652070346765447 | validation: 3.1564904988321816]
	TIME [epoch: 9.76 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5773211927536153		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 3.5773211927536153 | validation: 3.145582877036998]
	TIME [epoch: 9.74 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5817133676595487		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.5817133676595487 | validation: 3.1019675486213307]
	TIME [epoch: 9.73 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6009314020870904		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 3.6009314020870904 | validation: 3.1337701440457195]
	TIME [epoch: 9.75 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.63614959072693		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.63614959072693 | validation: 3.1291699868156586]
	TIME [epoch: 9.75 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643956177951501		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.643956177951501 | validation: 3.1183787603699233]
	TIME [epoch: 9.74 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5832471124636065		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.5832471124636065 | validation: 3.145112871458607]
	TIME [epoch: 9.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5876104279483614		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.5876104279483614 | validation: 3.100590278728619]
	TIME [epoch: 9.76 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6164345277802665		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 3.6164345277802665 | validation: 3.1292483718397035]
	TIME [epoch: 9.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6473504969219617		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.6473504969219617 | validation: 3.1364518344980126]
	TIME [epoch: 9.74 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.65259067015714		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.65259067015714 | validation: 3.1556460327736695]
	TIME [epoch: 9.75 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5820029897169583		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 3.5820029897169583 | validation: 3.265485386103902]
	TIME [epoch: 9.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6401126916213293		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.6401126916213293 | validation: 3.136756892518158]
	TIME [epoch: 9.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6766177293822166		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.6766177293822166 | validation: 3.376653206326422]
	TIME [epoch: 9.73 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.665803300737415		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.665803300737415 | validation: 3.112717787754781]
	TIME [epoch: 9.75 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.583666811795453		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 3.583666811795453 | validation: 3.0718792688547865]
	TIME [epoch: 9.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6019223977570554		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.6019223977570554 | validation: 3.074032523617756]
	TIME [epoch: 9.73 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.612619195808802		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.612619195808802 | validation: 3.1763654923661715]
	TIME [epoch: 9.75 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607870913020103		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.607870913020103 | validation: 3.1939670833817706]
	TIME [epoch: 9.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.658594683256837		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.658594683256837 | validation: 3.085963829986912]
	TIME [epoch: 9.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6592806979169774		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.6592806979169774 | validation: 3.3184358743240026]
	TIME [epoch: 9.74 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6988097123610593		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 3.6988097123610593 | validation: 3.3045897275108156]
	TIME [epoch: 9.75 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6505820759508354		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.6505820759508354 | validation: 3.146018430190707]
	TIME [epoch: 9.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5729173598487605		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.5729173598487605 | validation: 3.1175052106588197]
	TIME [epoch: 9.74 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5735352247896115		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.5735352247896115 | validation: 3.087639039099581]
	TIME [epoch: 9.77 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.574686089692115		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.574686089692115 | validation: 3.191930027188722]
	TIME [epoch: 9.73 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.672018775259881		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.672018775259881 | validation: 3.100415604436995]
	TIME [epoch: 9.75 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.638810683895533		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.638810683895533 | validation: 3.1041336403771234]
	TIME [epoch: 9.77 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6482827972617655		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 3.6482827972617655 | validation: 3.1314448406396913]
	TIME [epoch: 9.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6172541354467187		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 3.6172541354467187 | validation: 3.079228167389815]
	TIME [epoch: 9.75 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603642213175358		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.603642213175358 | validation: 3.1591101889040454]
	TIME [epoch: 9.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6397188631356556		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.6397188631356556 | validation: 3.103421403794355]
	TIME [epoch: 9.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.588628161967822		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.588628161967822 | validation: 3.0864574415988377]
	TIME [epoch: 9.74 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5820881721887248		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.5820881721887248 | validation: 3.1525784223570623]
	TIME [epoch: 9.75 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5947693428590144		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.5947693428590144 | validation: 3.136334048622753]
	TIME [epoch: 9.75 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6435021141136223		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.6435021141136223 | validation: 3.058937682685694]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60062063611091		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.60062063611091 | validation: 3.2967735242493608]
	TIME [epoch: 9.73 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6051803860282106		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.6051803860282106 | validation: 3.115279995557637]
	TIME [epoch: 9.74 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6120103528039893		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.6120103528039893 | validation: 3.324875123445788]
	TIME [epoch: 9.74 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625264137439474		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.625264137439474 | validation: 3.081669005658631]
	TIME [epoch: 9.73 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6323478062220773		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.6323478062220773 | validation: 3.088074162902834]
	TIME [epoch: 9.73 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6189757368505853		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 3.6189757368505853 | validation: 3.1186557893329643]
	TIME [epoch: 9.75 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6174388822142154		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.6174388822142154 | validation: 3.0889784232164357]
	TIME [epoch: 9.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.575253303502085		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.575253303502085 | validation: 3.129935440655369]
	TIME [epoch: 9.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6175940481764277		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.6175940481764277 | validation: 3.056300858303247]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.623121516052154		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 3.623121516052154 | validation: 3.137728554557023]
	TIME [epoch: 9.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593212926123389		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 3.593212926123389 | validation: 3.1011659722867355]
	TIME [epoch: 9.74 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5712015970217172		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.5712015970217172 | validation: 3.1241643346797163]
	TIME [epoch: 9.74 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566204205514058		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.566204205514058 | validation: 3.127500414613065]
	TIME [epoch: 9.73 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602711335495021		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.602711335495021 | validation: 3.1177496687559203]
	TIME [epoch: 9.76 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620726332172219		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.620726332172219 | validation: 3.176876815120962]
	TIME [epoch: 9.74 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.603321939127035		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.603321939127035 | validation: 3.1776555164321993]
	TIME [epoch: 9.74 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.589060820828567		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.589060820828567 | validation: 3.1578708093408325]
	TIME [epoch: 9.75 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618526349964801		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.618526349964801 | validation: 3.210855799219101]
	TIME [epoch: 9.75 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6073153358642522		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.6073153358642522 | validation: 3.0617357439860062]
	TIME [epoch: 9.74 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.589302763932261		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.589302763932261 | validation: 3.0985659981324374]
	TIME [epoch: 9.74 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598709325530155		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.598709325530155 | validation: 3.1488528061657792]
	TIME [epoch: 9.75 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5805915748176185		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.5805915748176185 | validation: 3.081386740879348]
	TIME [epoch: 9.74 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6049149116360355		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.6049149116360355 | validation: 3.1847761102516756]
	TIME [epoch: 9.73 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.660320425441141		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.660320425441141 | validation: 3.0893538066174084]
	TIME [epoch: 9.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5613278129393406		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.5613278129393406 | validation: 3.0845644181257543]
	TIME [epoch: 9.73 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.695533688910224		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.695533688910224 | validation: 3.138595733401802]
	TIME [epoch: 9.73 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5928358310957194		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.5928358310957194 | validation: 3.123720892843106]
	TIME [epoch: 9.74 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5994092011671484		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.5994092011671484 | validation: 3.092030352074314]
	TIME [epoch: 9.75 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566906290020854		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.566906290020854 | validation: 3.1025839619942963]
	TIME [epoch: 9.73 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6297619569745594		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.6297619569745594 | validation: 3.4983591355411954]
	TIME [epoch: 9.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.737367960267374		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.737367960267374 | validation: 3.0681423044919756]
	TIME [epoch: 9.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.553926692117875		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.553926692117875 | validation: 3.11296032371685]
	TIME [epoch: 9.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.565584065929169		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.565584065929169 | validation: 3.1261602117178904]
	TIME [epoch: 9.74 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591314629187033		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 3.591314629187033 | validation: 3.1506567965252708]
	TIME [epoch: 9.75 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6184790041090573		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.6184790041090573 | validation: 3.18288030188159]
	TIME [epoch: 9.73 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5933005286015876		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.5933005286015876 | validation: 3.130190882443194]
	TIME [epoch: 9.73 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5535610840407363		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.5535610840407363 | validation: 3.252305831856063]
	TIME [epoch: 9.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5950830115940464		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.5950830115940464 | validation: 3.0820455212886224]
	TIME [epoch: 9.75 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.607692297666401		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.607692297666401 | validation: 3.133507511197318]
	TIME [epoch: 9.73 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5571822395889883		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.5571822395889883 | validation: 3.097081275384239]
	TIME [epoch: 9.73 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5778524309051347		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.5778524309051347 | validation: 3.0867316060403143]
	TIME [epoch: 9.74 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5756945630919885		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.5756945630919885 | validation: 3.167832270846941]
	TIME [epoch: 9.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5792518604372274		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.5792518604372274 | validation: 3.144724254028678]
	TIME [epoch: 9.73 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.622523259112684		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.622523259112684 | validation: 3.0898792390963035]
	TIME [epoch: 9.73 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5899468338812355		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.5899468338812355 | validation: 3.1463593104756833]
	TIME [epoch: 9.74 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5806539808407707		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.5806539808407707 | validation: 3.0573048414665833]
	TIME [epoch: 9.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5954286409740086		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.5954286409740086 | validation: 3.1881250630721345]
	TIME [epoch: 9.73 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601799393511179		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.601799393511179 | validation: 3.1727099664047547]
	TIME [epoch: 9.76 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5961674047853647		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 3.5961674047853647 | validation: 3.1083256174223006]
	TIME [epoch: 9.73 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5796882496888864		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.5796882496888864 | validation: 3.179492906405352]
	TIME [epoch: 9.73 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.574375293059918		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 3.574375293059918 | validation: 3.097197046289792]
	TIME [epoch: 9.74 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6085924121308524		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.6085924121308524 | validation: 3.087155406313925]
	TIME [epoch: 9.73 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5813521034138183		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.5813521034138183 | validation: 3.061045001753015]
	TIME [epoch: 9.73 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.574309440288218		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.574309440288218 | validation: 3.1260522293825397]
	TIME [epoch: 9.73 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.575767074243907		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.575767074243907 | validation: 3.0916143440757673]
	TIME [epoch: 9.75 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5662958909395703		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.5662958909395703 | validation: 3.086169272339779]
	TIME [epoch: 9.73 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5700364528058857		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.5700364528058857 | validation: 3.2967399649512323]
	TIME [epoch: 9.73 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608467018500368		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 3.608467018500368 | validation: 3.1858441185463233]
	TIME [epoch: 9.74 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5967375147613154		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.5967375147613154 | validation: 3.081393981197008]
	TIME [epoch: 9.73 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566670933795187		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.566670933795187 | validation: 3.2167865731144447]
	TIME [epoch: 9.73 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598758734743529		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.598758734743529 | validation: 3.2216055305594353]
	TIME [epoch: 9.74 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6122459768599002		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.6122459768599002 | validation: 3.110605062131692]
	TIME [epoch: 9.74 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5972857189512126		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.5972857189512126 | validation: 3.1287155813057517]
	TIME [epoch: 9.72 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5857748628989348		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.5857748628989348 | validation: 3.145273853297173]
	TIME [epoch: 9.73 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5533102404328445		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.5533102404328445 | validation: 3.1835354594089633]
	TIME [epoch: 9.74 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59784197965594		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.59784197965594 | validation: 3.089077378283655]
	TIME [epoch: 9.72 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.587072911064496		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.587072911064496 | validation: 3.1112145344499855]
	TIME [epoch: 9.72 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.590379945536443		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.590379945536443 | validation: 3.0882475701984573]
	TIME [epoch: 9.73 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5888455876867313		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.5888455876867313 | validation: 3.094700979691372]
	TIME [epoch: 9.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559334565829687		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.559334565829687 | validation: 3.1958305983574644]
	TIME [epoch: 9.72 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6150019017007815		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 3.6150019017007815 | validation: 3.263220694266804]
	TIME [epoch: 9.73 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.586823261422615		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.586823261422615 | validation: 3.066802577662411]
	TIME [epoch: 9.75 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.585235479348893		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.585235479348893 | validation: 3.097118364925777]
	TIME [epoch: 9.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6327195635199496		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.6327195635199496 | validation: 3.0839014892850356]
	TIME [epoch: 9.73 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5644085959596374		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.5644085959596374 | validation: 3.1045968067120646]
	TIME [epoch: 9.73 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6117093424122175		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.6117093424122175 | validation: 3.3248117889838125]
	TIME [epoch: 9.74 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6068387763658207		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.6068387763658207 | validation: 3.0747851738680176]
	TIME [epoch: 9.72 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5718359617891147		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.5718359617891147 | validation: 3.089951733743497]
	TIME [epoch: 9.73 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5562830208514704		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.5562830208514704 | validation: 3.123678568886131]
	TIME [epoch: 9.74 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.584634156180132		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.584634156180132 | validation: 3.0799154044007846]
	TIME [epoch: 9.72 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5333410318483764		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.5333410318483764 | validation: 3.2167641135046883]
	TIME [epoch: 9.72 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.631335193473898		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.631335193473898 | validation: 3.090041756164855]
	TIME [epoch: 9.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595194945436014		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.595194945436014 | validation: 3.086994558038591]
	TIME [epoch: 9.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.55052802922865		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.55052802922865 | validation: 3.0781853791934184]
	TIME [epoch: 9.73 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5743426261220974		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.5743426261220974 | validation: 3.0728482191927498]
	TIME [epoch: 9.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559068580444586		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.559068580444586 | validation: 3.09153153638779]
	TIME [epoch: 9.75 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.546478142693202		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.546478142693202 | validation: 3.1321776079531527]
	TIME [epoch: 9.73 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.579244133500474		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.579244133500474 | validation: 3.1771810323494845]
	TIME [epoch: 9.73 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559721471363342		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.559721471363342 | validation: 3.110743357457068]
	TIME [epoch: 9.75 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559391264065906		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.559391264065906 | validation: 3.0839399326545776]
	TIME [epoch: 9.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5897097017059303		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.5897097017059303 | validation: 3.108066687393606]
	TIME [epoch: 9.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.557360696398109		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.557360696398109 | validation: 3.064854366816386]
	TIME [epoch: 9.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6507801598417786		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.6507801598417786 | validation: 3.0973385317475453]
	TIME [epoch: 9.74 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.561273658559858		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.561273658559858 | validation: 3.0890490844983116]
	TIME [epoch: 9.73 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6492838860830674		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.6492838860830674 | validation: 3.103970699505918]
	TIME [epoch: 9.73 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5504058616885006		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.5504058616885006 | validation: 3.1283112323892968]
	TIME [epoch: 9.76 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5665849065264497		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.5665849065264497 | validation: 3.075341508883913]
	TIME [epoch: 9.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5886798510431035		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.5886798510431035 | validation: 3.1346389516563207]
	TIME [epoch: 9.72 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6048265269370874		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.6048265269370874 | validation: 3.0899104956729686]
	TIME [epoch: 9.75 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5825330922329117		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.5825330922329117 | validation: 3.0993695435115676]
	TIME [epoch: 9.73 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.589925293699202		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.589925293699202 | validation: 3.211858771285786]
	TIME [epoch: 9.72 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602950964512926		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.602950964512926 | validation: 3.092354288486547]
	TIME [epoch: 9.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559072932760036		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.559072932760036 | validation: 3.067443525030633]
	TIME [epoch: 9.76 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.557277797619599		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.557277797619599 | validation: 3.0679219353960283]
	TIME [epoch: 9.73 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.569977966675679		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.569977966675679 | validation: 3.2315584935316544]
	TIME [epoch: 9.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.578997837787753		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.578997837787753 | validation: 3.081573505878273]
	TIME [epoch: 9.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.578496437117887		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.578496437117887 | validation: 3.162337956364244]
	TIME [epoch: 9.73 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5829839020239533		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.5829839020239533 | validation: 3.12598522494215]
	TIME [epoch: 9.73 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.597330428181647		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.597330428181647 | validation: 3.1576702075477674]
	TIME [epoch: 9.74 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59115953898296		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.59115953898296 | validation: 3.2846405855750436]
	TIME [epoch: 9.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.640450361645402		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.640450361645402 | validation: 3.124653959218033]
	TIME [epoch: 9.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5766293673897005		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.5766293673897005 | validation: 3.0763089006050053]
	TIME [epoch: 9.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.564712854221881		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.564712854221881 | validation: 3.1339126962833097]
	TIME [epoch: 9.75 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600563295556399		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.600563295556399 | validation: 3.160832479705547]
	TIME [epoch: 9.73 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5801743548463634		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.5801743548463634 | validation: 3.0957534669099642]
	TIME [epoch: 9.72 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5935117041175273		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.5935117041175273 | validation: 3.1958714435977336]
	TIME [epoch: 9.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6326129687901805		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.6326129687901805 | validation: 3.0944910555430774]
	TIME [epoch: 9.73 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.590714408578491		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.590714408578491 | validation: 3.187960807007631]
	TIME [epoch: 9.72 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.584126628389046		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.584126628389046 | validation: 3.127292681118256]
	TIME [epoch: 9.73 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.567200634013159		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.567200634013159 | validation: 3.1318954085699464]
	TIME [epoch: 9.74 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.552381006512161		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.552381006512161 | validation: 3.302523397918734]
	TIME [epoch: 9.73 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.620312804226291		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.620312804226291 | validation: 3.0856364491251886]
	TIME [epoch: 9.72 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.548774456864657		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.548774456864657 | validation: 3.094619769705577]
	TIME [epoch: 9.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5401149033543726		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.5401149033543726 | validation: 3.0604178010923513]
	TIME [epoch: 9.72 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5538493466748164		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.5538493466748164 | validation: 3.062992149445474]
	TIME [epoch: 9.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6371085656265643		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.6371085656265643 | validation: 3.2851578461710544]
	TIME [epoch: 9.73 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6208465844553515		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.6208465844553515 | validation: 3.1079179491318474]
	TIME [epoch: 9.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5572143201600497		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.5572143201600497 | validation: 3.1187161469907667]
	TIME [epoch: 9.72 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.552910731163869		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.552910731163869 | validation: 3.0771800190506498]
	TIME [epoch: 9.72 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5420900577151913		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.5420900577151913 | validation: 3.1333234639343006]
	TIME [epoch: 9.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566203807754905		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.566203807754905 | validation: 3.0864349811637863]
	TIME [epoch: 9.73 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.555007372722592		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.555007372722592 | validation: 3.0761217550760604]
	TIME [epoch: 9.72 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5503143531245724		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.5503143531245724 | validation: 3.1648863661683744]
	TIME [epoch: 9.75 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5538930899536254		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.5538930899536254 | validation: 3.0585701138568107]
	TIME [epoch: 9.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5376138263817523		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.5376138263817523 | validation: 3.094980841402702]
	TIME [epoch: 9.73 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.579900112666902		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.579900112666902 | validation: 3.0981122517346367]
	TIME [epoch: 9.73 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.586413290273189		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.586413290273189 | validation: 3.138384473148552]
	TIME [epoch: 9.75 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5626010891468374		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 3.5626010891468374 | validation: 3.1123618718838397]
	TIME [epoch: 9.73 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5802580011969263		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.5802580011969263 | validation: 3.14861649512693]
	TIME [epoch: 9.72 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550837583959852		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.550837583959852 | validation: 3.1305448422420166]
	TIME [epoch: 9.74 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5578431050230876		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.5578431050230876 | validation: 3.078805074426179]
	TIME [epoch: 9.73 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.558442755839013		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.558442755839013 | validation: 3.0619326962062896]
	TIME [epoch: 9.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5620896211895974		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.5620896211895974 | validation: 3.066271317957172]
	TIME [epoch: 9.73 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.554135575107871		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.554135575107871 | validation: 3.0751534360993906]
	TIME [epoch: 9.73 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5806751027576853		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.5806751027576853 | validation: 3.151812621197613]
	TIME [epoch: 9.73 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.574590426375137		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.574590426375137 | validation: 3.058852455455883]
	TIME [epoch: 9.72 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5493461326233637		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.5493461326233637 | validation: 3.268049706964275]
	TIME [epoch: 9.75 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5884838507501655		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.5884838507501655 | validation: 3.107962938175538]
	TIME [epoch: 9.73 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5596865134355484		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.5596865134355484 | validation: 3.0757648705614784]
	TIME [epoch: 9.73 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.555095676617962		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.555095676617962 | validation: 3.0726119739769877]
	TIME [epoch: 9.75 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5468548848565815		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.5468548848565815 | validation: 3.1611726079926887]
	TIME [epoch: 9.72 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5734669865176336		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.5734669865176336 | validation: 3.0532042234033625]
	TIME [epoch: 9.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547565593274406		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.547565593274406 | validation: 3.0489238581809013]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5409098625983475		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.5409098625983475 | validation: 3.1255138653870156]
	TIME [epoch: 9.75 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5714833904934915		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.5714833904934915 | validation: 3.05864255628163]
	TIME [epoch: 9.71 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.571428646049422		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.571428646049422 | validation: 3.138224059083338]
	TIME [epoch: 9.73 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5488265491903137		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.5488265491903137 | validation: 3.0472894642167296]
	TIME [epoch: 9.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.567660471610285		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.567660471610285 | validation: 3.140884581651247]
	TIME [epoch: 9.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541089113118506		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.541089113118506 | validation: 3.089022207130788]
	TIME [epoch: 9.73 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.551058659296712		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.551058659296712 | validation: 3.0704208550904215]
	TIME [epoch: 9.73 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5490844503661583		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.5490844503661583 | validation: 3.1250062074227585]
	TIME [epoch: 9.74 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5494729406848933		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.5494729406848933 | validation: 3.079124133446314]
	TIME [epoch: 9.73 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5860673447196034		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.5860673447196034 | validation: 3.07440923653008]
	TIME [epoch: 9.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.561969418418388		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.561969418418388 | validation: 3.0991965294955275]
	TIME [epoch: 9.73 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5771579283067103		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.5771579283067103 | validation: 3.0827958322829923]
	TIME [epoch: 9.76 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5669156560256496		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.5669156560256496 | validation: 3.1288791922953214]
	TIME [epoch: 9.74 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.563725335856342		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.563725335856342 | validation: 3.132167375709449]
	TIME [epoch: 9.73 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541550270539463		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.541550270539463 | validation: 3.0696092761592637]
	TIME [epoch: 9.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.556409571110817		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.556409571110817 | validation: 3.07986308602093]
	TIME [epoch: 9.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5469765272794467		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.5469765272794467 | validation: 3.1418061688695627]
	TIME [epoch: 9.73 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.574410257036862		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.574410257036862 | validation: 3.0737812850618047]
	TIME [epoch: 9.73 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5610508648752472		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.5610508648752472 | validation: 3.046110509484139]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550738594618319		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 3.550738594618319 | validation: 3.066796365538335]
	TIME [epoch: 9.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5525312687270025		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.5525312687270025 | validation: 3.06008874689509]
	TIME [epoch: 9.72 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.555112594938842		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.555112594938842 | validation: 3.0657900304295467]
	TIME [epoch: 9.74 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5990886136199145		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.5990886136199145 | validation: 3.077136486646325]
	TIME [epoch: 9.73 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5802942624389233		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.5802942624389233 | validation: 3.2042269580859006]
	TIME [epoch: 9.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5949349970950033		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.5949349970950033 | validation: 3.0905023187254703]
	TIME [epoch: 9.71 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5387555353596722		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 3.5387555353596722 | validation: 3.0591072305361786]
	TIME [epoch: 9.74 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5515009317340516		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.5515009317340516 | validation: 3.1669584492892784]
	TIME [epoch: 9.72 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547637880659459		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.547637880659459 | validation: 3.0673923110521337]
	TIME [epoch: 9.73 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547692265191766		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.547692265191766 | validation: 3.0887911074877605]
	TIME [epoch: 9.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5572013491275754		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.5572013491275754 | validation: 3.092127312309869]
	TIME [epoch: 9.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5749118427815554		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.5749118427815554 | validation: 3.1300594363392307]
	TIME [epoch: 9.72 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5490170226280804		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.5490170226280804 | validation: 3.1405044019210813]
	TIME [epoch: 9.73 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5459763720631594		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.5459763720631594 | validation: 3.1146894253489457]
	TIME [epoch: 9.73 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.548908230585373		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.548908230585373 | validation: 3.054392489102535]
	TIME [epoch: 9.72 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5946209580642594		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.5946209580642594 | validation: 3.0867522579898536]
	TIME [epoch: 9.73 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5422402974282017		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.5422402974282017 | validation: 3.0623311398877333]
	TIME [epoch: 9.74 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5605908475588697		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.5605908475588697 | validation: 3.064323579876915]
	TIME [epoch: 9.73 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550870730537622		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 3.550870730537622 | validation: 3.0673159754923995]
	TIME [epoch: 9.72 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.551328644059159		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.551328644059159 | validation: 3.0722537541631367]
	TIME [epoch: 9.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540605548873421		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.540605548873421 | validation: 3.0929947582325283]
	TIME [epoch: 9.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5477415099048684		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 3.5477415099048684 | validation: 3.059040031214678]
	TIME [epoch: 9.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.546122601219446		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.546122601219446 | validation: 3.1441465578078045]
	TIME [epoch: 9.72 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5913551465366367		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.5913551465366367 | validation: 3.06445960298656]
	TIME [epoch: 9.75 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5616139411854837		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.5616139411854837 | validation: 3.2739996064297556]
	TIME [epoch: 9.71 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.581281148638196		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.581281148638196 | validation: 3.082501957258856]
	TIME [epoch: 9.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.538571243420553		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.538571243420553 | validation: 3.08557391974571]
	TIME [epoch: 9.73 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5429718334478806		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 3.5429718334478806 | validation: 3.0742026530110786]
	TIME [epoch: 9.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541918937836539		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 3.541918937836539 | validation: 3.074638015878713]
	TIME [epoch: 9.72 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.54859792065535		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.54859792065535 | validation: 3.089162849488206]
	TIME [epoch: 9.74 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5452255158213384		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.5452255158213384 | validation: 3.0631552004297125]
	TIME [epoch: 9.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5461576232347474		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.5461576232347474 | validation: 3.0517202638261303]
	TIME [epoch: 9.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5413197769170326		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.5413197769170326 | validation: 3.1155309875463675]
	TIME [epoch: 9.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5629619249630915		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.5629619249630915 | validation: 3.10014387643321]
	TIME [epoch: 9.74 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550762177070884		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.550762177070884 | validation: 3.0641986826164715]
	TIME [epoch: 9.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540231127903339		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 3.540231127903339 | validation: 3.117776029354806]
	TIME [epoch: 9.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5396338713620414		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 3.5396338713620414 | validation: 3.108734351079387]
	TIME [epoch: 9.73 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.562479303188419		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.562479303188419 | validation: 3.0965857960721883]
	TIME [epoch: 9.73 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5362711862154597		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.5362711862154597 | validation: 3.06204910238931]
	TIME [epoch: 9.71 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.536535932423527		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.536535932423527 | validation: 3.0841662019996083]
	TIME [epoch: 9.73 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5384518988232463		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.5384518988232463 | validation: 3.0703935423932642]
	TIME [epoch: 9.74 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.537433738955273		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 3.537433738955273 | validation: 3.051857620273478]
	TIME [epoch: 9.73 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530826443852733		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.530826443852733 | validation: 3.07211084455002]
	TIME [epoch: 9.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.556997827777424		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.556997827777424 | validation: 3.06581936781696]
	TIME [epoch: 9.75 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5748939047074417		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.5748939047074417 | validation: 3.0613110768846994]
	TIME [epoch: 9.73 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5304258898606014		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 3.5304258898606014 | validation: 3.0553205774595398]
	TIME [epoch: 9.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5455664075614535		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 3.5455664075614535 | validation: 3.11307547434557]
	TIME [epoch: 9.74 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6065880151625853		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.6065880151625853 | validation: 3.0594767732138655]
	TIME [epoch: 9.74 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.542346522256861		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 3.542346522256861 | validation: 3.1131911789443474]
	TIME [epoch: 9.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5727083378175712		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.5727083378175712 | validation: 3.075931391811452]
	TIME [epoch: 9.73 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.549358732967393		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 3.549358732967393 | validation: 3.0590326553528167]
	TIME [epoch: 9.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5373508704484586		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 3.5373508704484586 | validation: 3.058262794152241]
	TIME [epoch: 9.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534599661417323		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 3.534599661417323 | validation: 3.0619227011468118]
	TIME [epoch: 9.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531922260643963		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 3.531922260643963 | validation: 3.070433410543949]
	TIME [epoch: 9.73 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605317153264115		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 3.605317153264115 | validation: 3.090692111455644]
	TIME [epoch: 9.74 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5822461242576296		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 3.5822461242576296 | validation: 3.0770958181996164]
	TIME [epoch: 9.72 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.533367266950036		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.533367266950036 | validation: 3.067613164966521]
	TIME [epoch: 9.74 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5465862736127476		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.5465862736127476 | validation: 3.0677926629519647]
	TIME [epoch: 9.74 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527807516793147		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 3.527807516793147 | validation: 3.082984332623579]
	TIME [epoch: 9.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5428727368961233		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 3.5428727368961233 | validation: 3.051528292537982]
	TIME [epoch: 9.71 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.539440172221797		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 3.539440172221797 | validation: 3.053951036967287]
	TIME [epoch: 9.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528024363905361		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.528024363905361 | validation: 3.0999069956289715]
	TIME [epoch: 9.73 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.553454950037562		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.553454950037562 | validation: 3.0707581567644486]
	TIME [epoch: 9.73 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.577829944261045		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 3.577829944261045 | validation: 3.0527438799492894]
	TIME [epoch: 9.72 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.546591472431062		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 3.546591472431062 | validation: 3.061177206180117]
	TIME [epoch: 9.73 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540697414898476		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 3.540697414898476 | validation: 3.0872573294439576]
	TIME [epoch: 9.72 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5603822041829885		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.5603822041829885 | validation: 3.1995918959768073]
	TIME [epoch: 9.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5805384235837785		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.5805384235837785 | validation: 3.06262582327427]
	TIME [epoch: 9.73 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5467020991694023		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.5467020991694023 | validation: 3.0578329366253367]
	TIME [epoch: 9.72 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534043432212639		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 3.534043432212639 | validation: 3.1057670335624086]
	TIME [epoch: 9.71 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5362835646722184		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 3.5362835646722184 | validation: 3.0830134706728543]
	TIME [epoch: 9.72 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5413657242534824		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.5413657242534824 | validation: 3.0800209430535244]
	TIME [epoch: 9.74 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.548282846758363		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.548282846758363 | validation: 3.0833786317972147]
	TIME [epoch: 9.72 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.54777117113771		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 3.54777117113771 | validation: 3.048811709958649]
	TIME [epoch: 9.73 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5473947917571267		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.5473947917571267 | validation: 3.0966360318307897]
	TIME [epoch: 9.74 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5726898914961103		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 3.5726898914961103 | validation: 3.0669190976801395]
	TIME [epoch: 9.72 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5449984962758037		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.5449984962758037 | validation: 3.1527901836310117]
	TIME [epoch: 9.71 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.582230053093139		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 3.582230053093139 | validation: 3.0662716812471933]
	TIME [epoch: 9.72 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531508277192651		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 3.531508277192651 | validation: 3.05749413003498]
	TIME [epoch: 9.73 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5409745715739853		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 3.5409745715739853 | validation: 3.0814710170549966]
	TIME [epoch: 9.72 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5331466041583353		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 3.5331466041583353 | validation: 3.1120192325494216]
	TIME [epoch: 9.71 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5535611184263423		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.5535611184263423 | validation: 3.098312241156567]
	TIME [epoch: 9.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5582812072363366		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 3.5582812072363366 | validation: 3.0912652479491265]
	TIME [epoch: 9.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550600752444781		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.550600752444781 | validation: 3.072350912338856]
	TIME [epoch: 9.73 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5482610314033556		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.5482610314033556 | validation: 3.057416591080364]
	TIME [epoch: 9.73 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527181477004585		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.527181477004585 | validation: 3.0707064383242937]
	TIME [epoch: 9.73 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.570234263624946		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 3.570234263624946 | validation: 3.110636044914904]
	TIME [epoch: 9.73 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5551917676996423		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 3.5551917676996423 | validation: 3.076012562890361]
	TIME [epoch: 9.72 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.578747771908913		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.578747771908913 | validation: 3.0673539667469494]
	TIME [epoch: 9.74 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5280171023430036		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 3.5280171023430036 | validation: 3.063996335288674]
	TIME [epoch: 9.72 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526767425998881		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.526767425998881 | validation: 3.070094558594294]
	TIME [epoch: 9.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5332683165251573		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 3.5332683165251573 | validation: 3.0657476948195823]
	TIME [epoch: 9.73 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.552560091153322		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.552560091153322 | validation: 3.0828884378450185]
	TIME [epoch: 9.72 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.570855995357756		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 3.570855995357756 | validation: 3.0515442965014956]
	TIME [epoch: 9.73 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5281002989642216		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 3.5281002989642216 | validation: 3.0631256612859112]
	TIME [epoch: 9.73 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5440246074509347		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 3.5440246074509347 | validation: 3.0849712064130657]
	TIME [epoch: 9.73 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5368059647115864		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 3.5368059647115864 | validation: 3.057925317447706]
	TIME [epoch: 9.73 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5297799805336667		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 3.5297799805336667 | validation: 3.058152136638605]
	TIME [epoch: 9.72 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5470621059703666		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 3.5470621059703666 | validation: 3.0931696688468118]
	TIME [epoch: 9.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5515475834030115		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 3.5515475834030115 | validation: 3.1120146353149565]
	TIME [epoch: 9.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5570357956633325		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 3.5570357956633325 | validation: 3.0522972882060584]
	TIME [epoch: 9.73 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.539799479633517		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 3.539799479633517 | validation: 3.062088479823525]
	TIME [epoch: 9.73 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.536109790088756		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 3.536109790088756 | validation: 3.0915925106945066]
	TIME [epoch: 9.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.537779595928341		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 3.537779595928341 | validation: 3.084218849893961]
	TIME [epoch: 9.72 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5406425279872096		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 3.5406425279872096 | validation: 3.129460948756565]
	TIME [epoch: 9.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.563210689710309		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 3.563210689710309 | validation: 3.068790454791888]
	TIME [epoch: 9.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535611750556792		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.535611750556792 | validation: 3.039096645985183]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_665.pth
	Model improved!!!
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5326998217896475		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 3.5326998217896475 | validation: 3.0840405557720665]
	TIME [epoch: 9.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.536457750270894		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 3.536457750270894 | validation: 3.205090262700021]
	TIME [epoch: 9.74 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5759352315675637		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.5759352315675637 | validation: 3.0557131905182904]
	TIME [epoch: 9.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5387683789058		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 3.5387683789058 | validation: 3.062804653980032]
	TIME [epoch: 9.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.554670454337453		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 3.554670454337453 | validation: 3.0944877900681305]
	TIME [epoch: 9.71 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535313467432984		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 3.535313467432984 | validation: 3.1338125967802033]
	TIME [epoch: 9.74 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5513358432855737		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 3.5513358432855737 | validation: 3.078579846883812]
	TIME [epoch: 9.73 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.545520457066985		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.545520457066985 | validation: 3.0792649189175774]
	TIME [epoch: 9.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5523050104264096		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 3.5523050104264096 | validation: 3.0816308070619485]
	TIME [epoch: 9.74 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531451921074165		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 3.531451921074165 | validation: 3.075416677077384]
	TIME [epoch: 9.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5464054436761456		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 3.5464054436761456 | validation: 3.0915528278523534]
	TIME [epoch: 9.71 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.542176465249979		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 3.542176465249979 | validation: 3.115131440100638]
	TIME [epoch: 9.73 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5613029333102104		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 3.5613029333102104 | validation: 3.0648251885734625]
	TIME [epoch: 9.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529534851819146		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 3.529534851819146 | validation: 3.073987539077764]
	TIME [epoch: 9.71 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5360484217217056		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.5360484217217056 | validation: 3.12347757774754]
	TIME [epoch: 9.71 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5417815594182036		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 3.5417815594182036 | validation: 3.0575471470610216]
	TIME [epoch: 9.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5494460153844165		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.5494460153844165 | validation: 3.0535442450403996]
	TIME [epoch: 9.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531229979564185		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 3.531229979564185 | validation: 3.057626623079972]
	TIME [epoch: 9.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5349928830917756		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 3.5349928830917756 | validation: 3.058302390771639]
	TIME [epoch: 9.74 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.533500228416192		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 3.533500228416192 | validation: 3.080726154747048]
	TIME [epoch: 9.73 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547696797304268		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 3.547696797304268 | validation: 3.108448076953166]
	TIME [epoch: 9.71 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.553888680138516		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 3.553888680138516 | validation: 3.0488525564902624]
	TIME [epoch: 9.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531273968398795		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 3.531273968398795 | validation: 3.082400549382112]
	TIME [epoch: 9.74 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5471876635868598		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 3.5471876635868598 | validation: 3.1126735955463873]
	TIME [epoch: 9.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5492558782604995		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 3.5492558782604995 | validation: 3.096970169621211]
	TIME [epoch: 9.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520921527280561		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 3.520921527280561 | validation: 3.058550986118547]
	TIME [epoch: 9.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534709607572773		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 3.534709607572773 | validation: 3.1029404356902646]
	TIME [epoch: 9.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5427832170933433		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 3.5427832170933433 | validation: 3.090209938512379]
	TIME [epoch: 9.71 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5418956485079987		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 3.5418956485079987 | validation: 3.092340555566557]
	TIME [epoch: 9.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5457948579215874		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 3.5457948579215874 | validation: 3.121475520561111]
	TIME [epoch: 9.74 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5488165848678577		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 3.5488165848678577 | validation: 3.1545197755829926]
	TIME [epoch: 9.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559178494509298		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 3.559178494509298 | validation: 3.055140308904514]
	TIME [epoch: 9.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5383754744048694		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 3.5383754744048694 | validation: 3.0640375330198584]
	TIME [epoch: 9.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5691223992915013		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 3.5691223992915013 | validation: 3.0702053333902803]
	TIME [epoch: 9.73 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530123926483751		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 3.530123926483751 | validation: 3.0939722609358276]
	TIME [epoch: 9.73 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535104004697021		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 3.535104004697021 | validation: 3.081328449772607]
	TIME [epoch: 9.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.545382268200515		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 3.545382268200515 | validation: 3.0669813543699127]
	TIME [epoch: 9.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5620774710514445		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 3.5620774710514445 | validation: 3.09384367154462]
	TIME [epoch: 9.73 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5340009542893887		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 3.5340009542893887 | validation: 3.0608052699993165]
	TIME [epoch: 9.71 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5260389661597804		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 3.5260389661597804 | validation: 3.0681032334976126]
	TIME [epoch: 9.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5317356878975055		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 3.5317356878975055 | validation: 3.053994736497824]
	TIME [epoch: 9.72 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5340060664256905		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 3.5340060664256905 | validation: 3.097731247079658]
	TIME [epoch: 9.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5390470976958093		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 3.5390470976958093 | validation: 3.0564832558108668]
	TIME [epoch: 9.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.571667708317728		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 3.571667708317728 | validation: 3.053845843498989]
	TIME [epoch: 9.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531071901531521		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 3.531071901531521 | validation: 3.0457551504011544]
	TIME [epoch: 9.72 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5471646428159502		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 3.5471646428159502 | validation: 3.1202490759128576]
	TIME [epoch: 9.73 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5392644286448687		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 3.5392644286448687 | validation: 3.0669968251273345]
	TIME [epoch: 9.75 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529980787418218		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 3.529980787418218 | validation: 3.059399550112522]
	TIME [epoch: 9.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5247387219480624		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 3.5247387219480624 | validation: 3.052949095744209]
	TIME [epoch: 9.73 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5277785694782486		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 3.5277785694782486 | validation: 3.07288334137883]
	TIME [epoch: 9.76 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5329432244217225		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 3.5329432244217225 | validation: 3.0753965482405476]
	TIME [epoch: 9.73 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5255383741784576		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 3.5255383741784576 | validation: 3.0617525349750077]
	TIME [epoch: 9.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52833292863951		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 3.52833292863951 | validation: 3.0741546295745508]
	TIME [epoch: 9.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5398401992270236		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 3.5398401992270236 | validation: 3.057675580972292]
	TIME [epoch: 9.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.537450517684486		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 3.537450517684486 | validation: 3.0469338541390263]
	TIME [epoch: 9.72 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530302407993465		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 3.530302407993465 | validation: 3.073033005094604]
	TIME [epoch: 9.73 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5259515784203623		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 3.5259515784203623 | validation: 3.089411696858491]
	TIME [epoch: 9.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5337119379445		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 3.5337119379445 | validation: 3.0649336469933615]
	TIME [epoch: 9.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5410645799180394		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 3.5410645799180394 | validation: 3.0558368828038973]
	TIME [epoch: 9.73 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5233481391236277		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 3.5233481391236277 | validation: 3.076044693913155]
	TIME [epoch: 9.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5304310555137968		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 3.5304310555137968 | validation: 3.0557700018162177]
	TIME [epoch: 9.73 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5387714027802666		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 3.5387714027802666 | validation: 3.0616309669587185]
	TIME [epoch: 9.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526296918320658		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 3.526296918320658 | validation: 3.0613274917611455]
	TIME [epoch: 9.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5379858654570207		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 3.5379858654570207 | validation: 3.06130162685795]
	TIME [epoch: 9.75 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5347829009195655		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 3.5347829009195655 | validation: 3.0587360058768094]
	TIME [epoch: 9.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534418732452192		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 3.534418732452192 | validation: 3.0504536539387312]
	TIME [epoch: 9.73 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530803532917742		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 3.530803532917742 | validation: 3.063784368214456]
	TIME [epoch: 9.76 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5310744014498736		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 3.5310744014498736 | validation: 3.0532852554934355]
	TIME [epoch: 9.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526422500819712		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 3.526422500819712 | validation: 3.0593060025007404]
	TIME [epoch: 9.71 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5505896968785864		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 3.5505896968785864 | validation: 3.0959934590310385]
	TIME [epoch: 9.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5408494060521667		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 3.5408494060521667 | validation: 3.0522923473404306]
	TIME [epoch: 9.76 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531478003119915		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 3.531478003119915 | validation: 3.0581107618810406]
	TIME [epoch: 9.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532398879539305		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 3.532398879539305 | validation: 3.0487116332712914]
	TIME [epoch: 9.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530293643820612		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 3.530293643820612 | validation: 3.157085790264802]
	TIME [epoch: 9.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.563133484660103		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 3.563133484660103 | validation: 3.035580846718093]
	TIME [epoch: 9.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531896223929786		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 3.531896223929786 | validation: 3.0794748127006746]
	TIME [epoch: 9.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531899035365106		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 3.531899035365106 | validation: 3.047054732682393]
	TIME [epoch: 9.75 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540644280601872		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 3.540644280601872 | validation: 3.054266641083842]
	TIME [epoch: 9.73 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5295779163244445		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 3.5295779163244445 | validation: 3.073051408761019]
	TIME [epoch: 9.71 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5282914868217867		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 3.5282914868217867 | validation: 3.0767218044473736]
	TIME [epoch: 9.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5311016211943262		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 3.5311016211943262 | validation: 3.0512616781996518]
	TIME [epoch: 9.73 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535113307756139		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 3.535113307756139 | validation: 3.0702090531988966]
	TIME [epoch: 9.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526040690056683		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 3.526040690056683 | validation: 3.0692524965197516]
	TIME [epoch: 9.71 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528532913166368		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 3.528532913166368 | validation: 3.0558253010388388]
	TIME [epoch: 9.72 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530488054248535		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 3.530488054248535 | validation: 3.073284842812309]
	TIME [epoch: 9.74 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5358132121714547		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 3.5358132121714547 | validation: 3.096994232532613]
	TIME [epoch: 9.72 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5272329308387427		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 3.5272329308387427 | validation: 3.058089693772629]
	TIME [epoch: 9.72 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5363081479708276		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 3.5363081479708276 | validation: 3.0838833107640466]
	TIME [epoch: 9.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.537461630163736		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 3.537461630163736 | validation: 3.0724800628670694]
	TIME [epoch: 9.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529951594105436		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 3.529951594105436 | validation: 3.067255369131276]
	TIME [epoch: 9.72 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5179666695424503		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 3.5179666695424503 | validation: 3.057890089643646]
	TIME [epoch: 9.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540263118616449		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 3.540263118616449 | validation: 3.044236245035469]
	TIME [epoch: 9.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.536941523556494		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 3.536941523556494 | validation: 3.054771765946288]
	TIME [epoch: 9.72 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532992410575737		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 3.532992410575737 | validation: 3.0822772863016463]
	TIME [epoch: 9.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.551739815013815		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 3.551739815013815 | validation: 3.0611392883737643]
	TIME [epoch: 9.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534130134531226		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 3.534130134531226 | validation: 3.059260027072391]
	TIME [epoch: 9.74 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5298932714794504		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 3.5298932714794504 | validation: 3.098861786363486]
	TIME [epoch: 9.72 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5469439570244576		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 3.5469439570244576 | validation: 3.070771016193353]
	TIME [epoch: 9.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5300008590244403		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 3.5300008590244403 | validation: 3.050711528273778]
	TIME [epoch: 9.71 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5419963453585352		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 3.5419963453585352 | validation: 3.0554265850033255]
	TIME [epoch: 9.73 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5325754814713		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 3.5325754814713 | validation: 3.075382154699739]
	TIME [epoch: 9.71 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.538278327107352		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 3.538278327107352 | validation: 3.0685101606123353]
	TIME [epoch: 9.71 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5345777259001325		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 3.5345777259001325 | validation: 3.0586155733083915]
	TIME [epoch: 9.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5412852085809474		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 3.5412852085809474 | validation: 3.0567963865244447]
	TIME [epoch: 9.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5315504231070456		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 3.5315504231070456 | validation: 3.0914478428769154]
	TIME [epoch: 9.71 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532302279643299		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 3.532302279643299 | validation: 3.080400411947104]
	TIME [epoch: 9.71 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541974720736538		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 3.541974720736538 | validation: 3.077260253936966]
	TIME [epoch: 9.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5467864561637916		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 3.5467864561637916 | validation: 3.0547267104412414]
	TIME [epoch: 9.71 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.542373121800901		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 3.542373121800901 | validation: 3.080017672659196]
	TIME [epoch: 9.71 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528590034613624		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 3.528590034613624 | validation: 3.0525189333044898]
	TIME [epoch: 9.71 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5385212603292153		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 3.5385212603292153 | validation: 3.0609068506197765]
	TIME [epoch: 9.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524012702135072		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 3.524012702135072 | validation: 3.089931632715833]
	TIME [epoch: 9.71 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5317496043055123		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 3.5317496043055123 | validation: 3.0616135396277198]
	TIME [epoch: 9.7 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531150277519221		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 3.531150277519221 | validation: 3.094772157112432]
	TIME [epoch: 9.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5308194573705087		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 3.5308194573705087 | validation: 3.061756993501773]
	TIME [epoch: 9.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521258341986859		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 3.521258341986859 | validation: 3.06999439210854]
	TIME [epoch: 9.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.563804625996248		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 3.563804625996248 | validation: 3.040926069454888]
	TIME [epoch: 9.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5523901426966034		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 3.5523901426966034 | validation: 3.1440741334957374]
	TIME [epoch: 9.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547274625712846		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 3.547274625712846 | validation: 3.0673409711946262]
	TIME [epoch: 9.7 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5286422915500077		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 3.5286422915500077 | validation: 3.0744722373685733]
	TIME [epoch: 9.71 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.533625234847605		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 3.533625234847605 | validation: 3.0699139126993504]
	TIME [epoch: 9.7 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531572955834824		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 3.531572955834824 | validation: 3.076899451512362]
	TIME [epoch: 9.71 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5516159501821782		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 3.5516159501821782 | validation: 3.113928609408018]
	TIME [epoch: 9.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5452868220777574		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 3.5452868220777574 | validation: 3.05407253616841]
	TIME [epoch: 9.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.539736515259714		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 3.539736515259714 | validation: 3.0492479552920306]
	TIME [epoch: 9.71 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5420273716455823		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 3.5420273716455823 | validation: 3.0579731582091463]
	TIME [epoch: 9.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528204722868753		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 3.528204722868753 | validation: 3.0802716294552868]
	TIME [epoch: 9.71 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5424738142449153		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 3.5424738142449153 | validation: 3.08429880258927]
	TIME [epoch: 9.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.533719171431932		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 3.533719171431932 | validation: 3.0560284300453238]
	TIME [epoch: 9.71 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5273073187208643		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 3.5273073187208643 | validation: 3.0608904147654377]
	TIME [epoch: 9.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5218421553177293		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 3.5218421553177293 | validation: 3.055952645231738]
	TIME [epoch: 9.71 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5302610015001186		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 3.5302610015001186 | validation: 3.041036459675283]
	TIME [epoch: 9.71 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5418919897476733		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 3.5418919897476733 | validation: 3.045044900983173]
	TIME [epoch: 9.73 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5215012108309245		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 3.5215012108309245 | validation: 3.073333472234814]
	TIME [epoch: 9.71 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5515073334140084		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 3.5515073334140084 | validation: 3.047135650573711]
	TIME [epoch: 9.71 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519622512293914		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 3.519622512293914 | validation: 3.048956901922446]
	TIME [epoch: 9.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523327876183747		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 3.523327876183747 | validation: 3.0531989615315593]
	TIME [epoch: 9.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540276520769224		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 3.540276520769224 | validation: 3.058252908116603]
	TIME [epoch: 9.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5211880832268365		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 3.5211880832268365 | validation: 3.055958424793679]
	TIME [epoch: 9.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223349568875784		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 3.5223349568875784 | validation: 3.053488703441589]
	TIME [epoch: 9.71 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5299688935916906		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 3.5299688935916906 | validation: 3.0778239343714278]
	TIME [epoch: 9.72 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5411009372561084		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 3.5411009372561084 | validation: 3.0661215026583757]
	TIME [epoch: 9.71 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5400947741905417		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 3.5400947741905417 | validation: 3.063835019652682]
	TIME [epoch: 9.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540045745695167		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 3.540045745695167 | validation: 3.0466734230371855]
	TIME [epoch: 9.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522353479224371		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 3.522353479224371 | validation: 3.082275002293351]
	TIME [epoch: 9.71 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5306458471513387		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 3.5306458471513387 | validation: 3.062301029661585]
	TIME [epoch: 9.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541045074174786		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 3.541045074174786 | validation: 3.0826010117741682]
	TIME [epoch: 9.71 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5373586832873327		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 3.5373586832873327 | validation: 3.055894224025368]
	TIME [epoch: 9.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5274387625046293		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 3.5274387625046293 | validation: 3.056026286287485]
	TIME [epoch: 9.71 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540098739008408		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 3.540098739008408 | validation: 3.1040511938453768]
	TIME [epoch: 9.71 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5306075853525334		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 3.5306075853525334 | validation: 3.04558685981277]
	TIME [epoch: 9.71 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526966279633932		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 3.526966279633932 | validation: 3.060657349057665]
	TIME [epoch: 9.72 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5205528474848435		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 3.5205528474848435 | validation: 3.0558276610680366]
	TIME [epoch: 9.71 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521211313964593		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 3.521211313964593 | validation: 3.0995171854248795]
	TIME [epoch: 9.71 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5404665122534276		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 3.5404665122534276 | validation: 3.0641410623665934]
	TIME [epoch: 9.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523897541678918		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 3.523897541678918 | validation: 3.05202941400168]
	TIME [epoch: 9.71 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5185765532897166		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 3.5185765532897166 | validation: 3.0567398669464003]
	TIME [epoch: 9.71 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535048945788321		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 3.535048945788321 | validation: 3.075886824463883]
	TIME [epoch: 9.71 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.543233248669515		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 3.543233248669515 | validation: 3.0821219653290575]
	TIME [epoch: 9.73 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5396551268275		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 3.5396551268275 | validation: 3.0572113244570414]
	TIME [epoch: 9.71 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524122423631634		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 3.524122423631634 | validation: 3.0694666487245788]
	TIME [epoch: 9.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5314994380097		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 3.5314994380097 | validation: 3.0828479697477413]
	TIME [epoch: 9.71 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5375394455077243		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 3.5375394455077243 | validation: 3.0569166367752136]
	TIME [epoch: 9.72 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5233569472992103		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 3.5233569472992103 | validation: 3.0600965574452155]
	TIME [epoch: 9.71 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5239593183841746		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 3.5239593183841746 | validation: 3.0560540384992194]
	TIME [epoch: 9.71 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5231673999884245		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 3.5231673999884245 | validation: 3.0742059442148633]
	TIME [epoch: 9.71 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5357471185333473		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 3.5357471185333473 | validation: 3.0526857282539197]
	TIME [epoch: 9.72 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5231339521486844		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 3.5231339521486844 | validation: 3.0948280009051574]
	TIME [epoch: 9.71 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519816522210971		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 3.519816522210971 | validation: 3.0614916072770417]
	TIME [epoch: 9.71 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5208859038513993		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 3.5208859038513993 | validation: 3.0548590521676395]
	TIME [epoch: 9.73 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529099374645594		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 3.529099374645594 | validation: 3.0558075158458617]
	TIME [epoch: 9.71 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5205940996945175		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 3.5205940996945175 | validation: 3.0702343545224053]
	TIME [epoch: 9.71 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5339251172406208		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 3.5339251172406208 | validation: 3.076729824663728]
	TIME [epoch: 9.71 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529758398856066		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 3.529758398856066 | validation: 3.0633617143415903]
	TIME [epoch: 9.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5185902226919175		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 3.5185902226919175 | validation: 3.0805443666778127]
	TIME [epoch: 9.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524507461816995		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 3.524507461816995 | validation: 3.0764140299332667]
	TIME [epoch: 9.71 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550418971156404		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 3.550418971156404 | validation: 3.048769432365916]
	TIME [epoch: 9.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202905299253024		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 3.5202905299253024 | validation: 3.059418444013563]
	TIME [epoch: 9.73 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5224282423286923		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 3.5224282423286923 | validation: 3.051724453034572]
	TIME [epoch: 9.71 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5270317243603833		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 3.5270317243603833 | validation: 3.09354108527428]
	TIME [epoch: 9.71 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5276894315474103		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 3.5276894315474103 | validation: 3.0716006365866555]
	TIME [epoch: 9.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5241782923144305		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 3.5241782923144305 | validation: 3.0636934981661206]
	TIME [epoch: 9.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522486843325448		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 3.522486843325448 | validation: 3.0475465362771774]
	TIME [epoch: 9.71 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519170299451136		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 3.519170299451136 | validation: 3.0488438121639287]
	TIME [epoch: 9.71 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5311853885158073		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 3.5311853885158073 | validation: 3.060613063838782]
	TIME [epoch: 9.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525988437238033		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 3.525988437238033 | validation: 3.076480704853852]
	TIME [epoch: 9.71 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5385716954175073		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 3.5385716954175073 | validation: 3.0889064149525995]
	TIME [epoch: 9.71 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527222261974947		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 3.527222261974947 | validation: 3.051028939820158]
	TIME [epoch: 9.71 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5248948881327182		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 3.5248948881327182 | validation: 3.055914615456444]
	TIME [epoch: 9.73 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528871539065121		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 3.528871539065121 | validation: 3.0752604327859956]
	TIME [epoch: 9.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528728559503134		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 3.528728559503134 | validation: 3.053711140940982]
	TIME [epoch: 9.71 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52284084901317		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 3.52284084901317 | validation: 3.058060174749654]
	TIME [epoch: 9.71 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528259731414168		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 3.528259731414168 | validation: 3.0609443533321268]
	TIME [epoch: 9.73 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166446279796104		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 3.5166446279796104 | validation: 3.045876228631169]
	TIME [epoch: 9.71 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197828560598707		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 3.5197828560598707 | validation: 3.049608573628944]
	TIME [epoch: 9.71 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5313911074110194		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 3.5313911074110194 | validation: 3.105656862811329]
	TIME [epoch: 9.71 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5396289615633263		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 3.5396289615633263 | validation: 3.0481791456397276]
	TIME [epoch: 9.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5226833260138557		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 3.5226833260138557 | validation: 3.0602351452001204]
	TIME [epoch: 9.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5285024383444563		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 3.5285024383444563 | validation: 3.060570682805393]
	TIME [epoch: 9.71 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521524214335462		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 3.521524214335462 | validation: 3.050099455971081]
	TIME [epoch: 9.72 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5241569614873782		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 3.5241569614873782 | validation: 3.054384960607638]
	TIME [epoch: 9.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5296635183056706		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 3.5296635183056706 | validation: 3.06594026567834]
	TIME [epoch: 9.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5264350907705357		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 3.5264350907705357 | validation: 3.0746011697624347]
	TIME [epoch: 9.71 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5360803123709004		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 3.5360803123709004 | validation: 3.0760562981767525]
	TIME [epoch: 9.72 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523659032330211		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 3.523659032330211 | validation: 3.0927468287580697]
	TIME [epoch: 9.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5581880518117863		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 3.5581880518117863 | validation: 3.0596108036743033]
	TIME [epoch: 9.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5204737607673535		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 3.5204737607673535 | validation: 3.0556334952739976]
	TIME [epoch: 9.71 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5357729638047717		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 3.5357729638047717 | validation: 3.0616348131304516]
	TIME [epoch: 9.71 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521568154985215		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 3.521568154985215 | validation: 3.0589332983054063]
	TIME [epoch: 9.71 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5297875026524776		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 3.5297875026524776 | validation: 3.0424155530332837]
	TIME [epoch: 9.71 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5229632333276406		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 3.5229632333276406 | validation: 3.0637814663286713]
	TIME [epoch: 9.71 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531906710088846		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 3.531906710088846 | validation: 3.0576984534862026]
	TIME [epoch: 9.71 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5243809239923003		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 3.5243809239923003 | validation: 3.05621234220497]
	TIME [epoch: 9.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171405922499988		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 3.5171405922499988 | validation: 3.052776349342173]
	TIME [epoch: 9.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5226905147463965		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 3.5226905147463965 | validation: 3.085130838243619]
	TIME [epoch: 9.71 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5395465395885326		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 3.5395465395885326 | validation: 3.052235632487433]
	TIME [epoch: 9.7 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193043480881565		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 3.5193043480881565 | validation: 3.0452183071179455]
	TIME [epoch: 9.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5351834628364704		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 3.5351834628364704 | validation: 3.0709556980587336]
	TIME [epoch: 9.7 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5256889494692003		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 3.5256889494692003 | validation: 3.048148811540009]
	TIME [epoch: 9.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5205776474258847		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 3.5205776474258847 | validation: 3.0772882170845186]
	TIME [epoch: 9.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526557803049659		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 3.526557803049659 | validation: 3.0595713468309773]
	TIME [epoch: 9.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529648240591682		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 3.529648240591682 | validation: 3.073507481598056]
	TIME [epoch: 9.71 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5258590827717717		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 3.5258590827717717 | validation: 3.073949706081909]
	TIME [epoch: 9.71 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540047372309721		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 3.540047372309721 | validation: 3.0940715154010765]
	TIME [epoch: 9.7 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5244288604358616		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 3.5244288604358616 | validation: 3.0585487110909755]
	TIME [epoch: 9.7 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5403944442454587		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 3.5403944442454587 | validation: 3.055636024065308]
	TIME [epoch: 9.71 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165739951201522		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 3.5165739951201522 | validation: 3.0548039895922625]
	TIME [epoch: 9.7 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5209307457585224		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 3.5209307457585224 | validation: 3.081856707147646]
	TIME [epoch: 9.7 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5264041524873972		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 3.5264041524873972 | validation: 3.04913983369042]
	TIME [epoch: 9.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5269031912716096		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 3.5269031912716096 | validation: 3.0694271968977365]
	TIME [epoch: 9.72 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519342480806003		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 3.519342480806003 | validation: 3.067540627788801]
	TIME [epoch: 9.7 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5455917838627817		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 3.5455917838627817 | validation: 3.0675124652053785]
	TIME [epoch: 9.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5244712675423058		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 3.5244712675423058 | validation: 3.0586334097972054]
	TIME [epoch: 9.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520559815465063		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 3.520559815465063 | validation: 3.051105070099876]
	TIME [epoch: 9.71 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5198472143567314		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 3.5198472143567314 | validation: 3.0576251004831883]
	TIME [epoch: 9.7 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517529191122288		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 3.517529191122288 | validation: 3.062734391143722]
	TIME [epoch: 9.7 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526958362146199		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 3.526958362146199 | validation: 3.0578556137663084]
	TIME [epoch: 9.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5242571398656346		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 3.5242571398656346 | validation: 3.0515542688713784]
	TIME [epoch: 9.7 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5241146614390053		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 3.5241146614390053 | validation: 3.063766103872356]
	TIME [epoch: 9.7 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514893799542224		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 3.514893799542224 | validation: 3.055235935711264]
	TIME [epoch: 9.7 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5290704052201596		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 3.5290704052201596 | validation: 3.0562033439514598]
	TIME [epoch: 9.71 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5268699511735875		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 3.5268699511735875 | validation: 3.0543407430473226]
	TIME [epoch: 9.7 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530879507736202		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 3.530879507736202 | validation: 3.0643982448239173]
	TIME [epoch: 9.7 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521035640836117		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 3.521035640836117 | validation: 3.0481193215643363]
	TIME [epoch: 9.7 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519539251642829		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 3.519539251642829 | validation: 3.056720226764544]
	TIME [epoch: 9.72 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5203700816658063		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 3.5203700816658063 | validation: 3.0719244095287026]
	TIME [epoch: 9.7 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523288924949764		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 3.523288924949764 | validation: 3.0572723687706604]
	TIME [epoch: 9.7 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212804972593106		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 3.5212804972593106 | validation: 3.066257171169673]
	TIME [epoch: 9.7 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5321445145983184		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 3.5321445145983184 | validation: 3.086237911202736]
	TIME [epoch: 9.7 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527890601599404		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 3.527890601599404 | validation: 3.0529811014598796]
	TIME [epoch: 9.69 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535477755906971		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 3.535477755906971 | validation: 3.056452026486763]
	TIME [epoch: 9.7 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5264439813352673		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 3.5264439813352673 | validation: 3.05920657464616]
	TIME [epoch: 9.71 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522723050098616		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 3.522723050098616 | validation: 3.060929729349098]
	TIME [epoch: 9.7 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540968095362591		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 3.540968095362591 | validation: 3.1001658094948077]
	TIME [epoch: 9.7 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524452222775834		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 3.524452222775834 | validation: 3.0554335680828513]
	TIME [epoch: 9.7 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521495797841655		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 3.521495797841655 | validation: 3.0760947290204523]
	TIME [epoch: 9.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529575895136368		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 3.529575895136368 | validation: 3.064026820968528]
	TIME [epoch: 9.7 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527597776976311		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 3.527597776976311 | validation: 3.0492664358343884]
	TIME [epoch: 9.7 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512156179991704		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 3.512156179991704 | validation: 3.072503484282828]
	TIME [epoch: 9.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524841278303603		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 3.524841278303603 | validation: 3.04855474001318]
	TIME [epoch: 9.71 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5229784341008505		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 3.5229784341008505 | validation: 3.0488962719668007]
	TIME [epoch: 9.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5229459046697036		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 3.5229459046697036 | validation: 3.0667407786278957]
	TIME [epoch: 9.7 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5359376252482306		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 3.5359376252482306 | validation: 3.0787700800085473]
	TIME [epoch: 9.71 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5275184060561786		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 3.5275184060561786 | validation: 3.0639169362482064]
	TIME [epoch: 9.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5246262665965076		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 3.5246262665965076 | validation: 3.0703060745413833]
	TIME [epoch: 9.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534842051082601		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 3.534842051082601 | validation: 3.1139102508843006]
	TIME [epoch: 9.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526114831437583		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 3.526114831437583 | validation: 3.055583964621433]
	TIME [epoch: 9.71 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5211413609801		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 3.5211413609801 | validation: 3.0520259599153725]
	TIME [epoch: 9.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522846482365536		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 3.522846482365536 | validation: 3.078335585688261]
	TIME [epoch: 9.7 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5327464331772247		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 3.5327464331772247 | validation: 3.0529403764750986]
	TIME [epoch: 9.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517073985908464		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 3.517073985908464 | validation: 3.0541978449057376]
	TIME [epoch: 9.71 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527032679236304		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 3.527032679236304 | validation: 3.0517441554786195]
	TIME [epoch: 9.69 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525511878094153		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 3.525511878094153 | validation: 3.0564283415976012]
	TIME [epoch: 9.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195979979710685		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 3.5195979979710685 | validation: 3.048029341550422]
	TIME [epoch: 9.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202185200357916		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 3.5202185200357916 | validation: 3.0673425890509614]
	TIME [epoch: 9.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520889108006977		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 3.520889108006977 | validation: 3.049332682117097]
	TIME [epoch: 9.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5247350656789793		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 3.5247350656789793 | validation: 3.051819582760662]
	TIME [epoch: 9.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5233135186474103		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 3.5233135186474103 | validation: 3.056798422312246]
	TIME [epoch: 9.71 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5243549385344273		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 3.5243549385344273 | validation: 3.054578539821737]
	TIME [epoch: 9.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540626243980003		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 3.540626243980003 | validation: 3.0410118543742546]
	TIME [epoch: 9.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5222843234863555		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 3.5222843234863555 | validation: 3.0602759720209236]
	TIME [epoch: 9.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175723362371465		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 3.5175723362371465 | validation: 3.054172104683663]
	TIME [epoch: 9.72 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5306707229639676		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 3.5306707229639676 | validation: 3.0723413403344253]
	TIME [epoch: 9.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5217481399556823		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 3.5217481399556823 | validation: 3.04201207496559]
	TIME [epoch: 9.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531509883134074		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 3.531509883134074 | validation: 3.0706246305222122]
	TIME [epoch: 9.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5244271750448357		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 3.5244271750448357 | validation: 3.0571919876633666]
	TIME [epoch: 9.72 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520673109898488		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 3.520673109898488 | validation: 3.0503448181470225]
	TIME [epoch: 9.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522964576553116		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 3.522964576553116 | validation: 3.0797191265001436]
	TIME [epoch: 9.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520855250685739		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 3.520855250685739 | validation: 3.054647265924454]
	TIME [epoch: 9.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523481692632611		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 3.523481692632611 | validation: 3.058137085768057]
	TIME [epoch: 9.71 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520634220900271		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 3.520634220900271 | validation: 3.0557893789514243]
	TIME [epoch: 9.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169480291350745		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 3.5169480291350745 | validation: 3.082794133095731]
	TIME [epoch: 9.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5317083746012345		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 3.5317083746012345 | validation: 3.0782883437342026]
	TIME [epoch: 9.71 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5278973334490344		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 3.5278973334490344 | validation: 3.062819559847725]
	TIME [epoch: 9.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522043988288061		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 3.522043988288061 | validation: 3.074874032067852]
	TIME [epoch: 9.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5312577055542476		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 3.5312577055542476 | validation: 3.1035029758808714]
	TIME [epoch: 9.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5410396853770267		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 3.5410396853770267 | validation: 3.057660636685142]
	TIME [epoch: 9.71 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197331493683692		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 3.5197331493683692 | validation: 3.049541347378331]
	TIME [epoch: 9.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5164521305680587		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 3.5164521305680587 | validation: 3.0544239138333573]
	TIME [epoch: 9.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5123092384355146		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 3.5123092384355146 | validation: 3.034942040824668]
	TIME [epoch: 9.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_965.pth
	Model improved!!!
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5254415796133607		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 3.5254415796133607 | validation: 3.0550803731690745]
	TIME [epoch: 9.71 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5347900646850037		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 3.5347900646850037 | validation: 3.0534880431572384]
	TIME [epoch: 9.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522425955314079		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 3.522425955314079 | validation: 3.0985062933503187]
	TIME [epoch: 9.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5296924501575346		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 3.5296924501575346 | validation: 3.047921833009656]
	TIME [epoch: 9.71 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523409883981813		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 3.523409883981813 | validation: 3.057802158662533]
	TIME [epoch: 9.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5263958425628994		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 3.5263958425628994 | validation: 3.042567193417038]
	TIME [epoch: 9.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172535102932643		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 3.5172535102932643 | validation: 3.0487991901769385]
	TIME [epoch: 9.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524803838433376		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 3.524803838433376 | validation: 3.070290409128095]
	TIME [epoch: 9.71 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.536988744973938		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 3.536988744973938 | validation: 3.0636054754937243]
	TIME [epoch: 9.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528569935722603		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 3.528569935722603 | validation: 3.055153352222362]
	TIME [epoch: 9.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516880531073278		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 3.516880531073278 | validation: 3.0575912015988242]
	TIME [epoch: 9.71 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5210318740654274		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 3.5210318740654274 | validation: 3.0705758798302663]
	TIME [epoch: 9.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520013927611142		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 3.520013927611142 | validation: 3.054895116016224]
	TIME [epoch: 9.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223847598713673		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 3.5223847598713673 | validation: 3.07482043874018]
	TIME [epoch: 9.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5252207741896564		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 3.5252207741896564 | validation: 3.090827162481688]
	TIME [epoch: 9.71 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5318454095144367		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 3.5318454095144367 | validation: 3.0574999897736417]
	TIME [epoch: 9.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520433414758898		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 3.520433414758898 | validation: 3.058029788160259]
	TIME [epoch: 9.69 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5347290883895903		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 3.5347290883895903 | validation: 3.0439528211539373]
	TIME [epoch: 9.72 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52194140579425		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 3.52194140579425 | validation: 3.05749510378278]
	TIME [epoch: 9.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5203812926859106		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 3.5203812926859106 | validation: 3.047871048047978]
	TIME [epoch: 9.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5194078497055914		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 3.5194078497055914 | validation: 3.0532678478272706]
	TIME [epoch: 9.71 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5206809030178805		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 3.5206809030178805 | validation: 3.064901043307949]
	TIME [epoch: 9.72 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5278890172096227		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 3.5278890172096227 | validation: 3.04972149425338]
	TIME [epoch: 9.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524940326037965		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 3.524940326037965 | validation: 3.0840593459296772]
	TIME [epoch: 9.69 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525666620936211		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 3.525666620936211 | validation: 3.0556925270403155]
	TIME [epoch: 9.71 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528349984916301		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 3.528349984916301 | validation: 3.0653267541039195]
	TIME [epoch: 9.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5220319263754		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 3.5220319263754 | validation: 3.051552759151314]
	TIME [epoch: 9.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522423926327652		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 3.522423926327652 | validation: 3.05073857091896]
	TIME [epoch: 9.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517401344837347		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 3.517401344837347 | validation: 3.058311800559462]
	TIME [epoch: 9.71 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521933889867732		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 3.521933889867732 | validation: 3.062221003921455]
	TIME [epoch: 9.7 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525010229689054		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 3.525010229689054 | validation: 3.0712449485349613]
	TIME [epoch: 9.69 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527190016778737		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 3.527190016778737 | validation: 3.0644492038480893]
	TIME [epoch: 9.71 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52465845618496		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 3.52465845618496 | validation: 3.078645811506906]
	TIME [epoch: 9.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523248680464488		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 3.523248680464488 | validation: 3.067765136911029]
	TIME [epoch: 9.69 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5243903084365362		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 3.5243903084365362 | validation: 3.0511506377490036]
	TIME [epoch: 9.71 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161843114270313		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 3.5161843114270313 | validation: 3.080303079368807]
	TIME [epoch: 9.7 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5239934080923776		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 3.5239934080923776 | validation: 3.0878736286062924]
	TIME [epoch: 9.69 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52749185978533		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 3.52749185978533 | validation: 3.0667849176892674]
	TIME [epoch: 9.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524468804219064		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 3.524468804219064 | validation: 3.056030070235317]
	TIME [epoch: 9.71 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52030133968164		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 3.52030133968164 | validation: 3.059700034735623]
	TIME [epoch: 9.69 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193150604304058		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 3.5193150604304058 | validation: 3.0600033740544466]
	TIME [epoch: 9.69 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5192063331439627		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 3.5192063331439627 | validation: 3.0787629101285727]
	TIME [epoch: 9.71 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535194023307941		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 3.535194023307941 | validation: 3.068091909239506]
	TIME [epoch: 9.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163019027448192		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 3.5163019027448192 | validation: 3.057351098868303]
	TIME [epoch: 9.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516591364604298		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 3.516591364604298 | validation: 3.054187485368968]
	TIME [epoch: 9.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175316543003623		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 3.5175316543003623 | validation: 3.0532421996883876]
	TIME [epoch: 9.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5249935405382486		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 3.5249935405382486 | validation: 3.0470699869260445]
	TIME [epoch: 9.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5168258518153115		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 3.5168258518153115 | validation: 3.0600977201878514]
	TIME [epoch: 9.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177570374158584		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 3.5177570374158584 | validation: 3.0596293598218143]
	TIME [epoch: 9.72 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529928395946507		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 3.529928395946507 | validation: 3.070604147071528]
	TIME [epoch: 9.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5257267580397134		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 3.5257267580397134 | validation: 3.0659741955967945]
	TIME [epoch: 9.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515724990554		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 3.515724990554 | validation: 3.070184093858623]
	TIME [epoch: 9.71 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172961194128276		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 3.5172961194128276 | validation: 3.063649942065495]
	TIME [epoch: 9.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520180466852639		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 3.520180466852639 | validation: 3.049031340769714]
	TIME [epoch: 9.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518936930505288		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 3.518936930505288 | validation: 3.069937697623195]
	TIME [epoch: 9.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5119322040074126		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 3.5119322040074126 | validation: 3.0604696776150684]
	TIME [epoch: 9.71 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5290068096517673		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 3.5290068096517673 | validation: 3.055675282511105]
	TIME [epoch: 9.69 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190717275417667		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 3.5190717275417667 | validation: 3.07070287913555]
	TIME [epoch: 9.69 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5255302424231267		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 3.5255302424231267 | validation: 3.0604102826265187]
	TIME [epoch: 9.71 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525098351887748		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 3.525098351887748 | validation: 3.058219327882057]
	TIME [epoch: 9.69 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212992907888725		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 3.5212992907888725 | validation: 3.0453287336770285]
	TIME [epoch: 9.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5242399301736214		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 3.5242399301736214 | validation: 3.0536183056382322]
	TIME [epoch: 9.71 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5109812741083495		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 3.5109812741083495 | validation: 3.0580738134297363]
	TIME [epoch: 9.71 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5194552372458197		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 3.5194552372458197 | validation: 3.061582398813126]
	TIME [epoch: 9.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522584320793179		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 3.522584320793179 | validation: 3.0519182057078993]
	TIME [epoch: 9.69 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186916553435923		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 3.5186916553435923 | validation: 3.0810402157409893]
	TIME [epoch: 9.72 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530306558502054		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 3.530306558502054 | validation: 3.069009381778145]
	TIME [epoch: 9.69 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5256942621998575		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 3.5256942621998575 | validation: 3.054152405384208]
	TIME [epoch: 9.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5296419960072227		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 3.5296419960072227 | validation: 3.0570992320976083]
	TIME [epoch: 9.73 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5194053055989882		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 3.5194053055989882 | validation: 3.066394101538193]
	TIME [epoch: 9.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5250228209794443		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 3.5250228209794443 | validation: 3.060723228655283]
	TIME [epoch: 9.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5271325630202575		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 3.5271325630202575 | validation: 3.058169970274223]
	TIME [epoch: 9.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.528466759096948		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 3.528466759096948 | validation: 3.076115800294641]
	TIME [epoch: 9.72 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5228750235419257		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 3.5228750235419257 | validation: 3.086480939576189]
	TIME [epoch: 9.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5252615506885903		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 3.5252615506885903 | validation: 3.0605721410792923]
	TIME [epoch: 9.72 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5130605875365752		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 3.5130605875365752 | validation: 3.0539311195019208]
	TIME [epoch: 9.73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521737798924412		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 3.521737798924412 | validation: 3.0602806856209828]
	TIME [epoch: 9.72 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212337882441167		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 3.5212337882441167 | validation: 3.056353375195922]
	TIME [epoch: 9.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524299427860747		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 3.524299427860747 | validation: 3.06590257371211]
	TIME [epoch: 9.71 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5203940933657036		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 3.5203940933657036 | validation: 3.0587759979405122]
	TIME [epoch: 9.72 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518412313783746		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 3.518412313783746 | validation: 3.058142582294728]
	TIME [epoch: 9.71 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5146281281008243		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 3.5146281281008243 | validation: 3.061668891016308]
	TIME [epoch: 9.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5303986911562992		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 3.5303986911562992 | validation: 3.061422018996515]
	TIME [epoch: 9.73 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148772254345446		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 3.5148772254345446 | validation: 3.0745874187940045]
	TIME [epoch: 9.72 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525251746088108		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 3.525251746088108 | validation: 3.0695452692366167]
	TIME [epoch: 9.72 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524826744241569		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 3.524826744241569 | validation: 3.053916640706286]
	TIME [epoch: 9.71 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155871079214167		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 3.5155871079214167 | validation: 3.0680830502216008]
	TIME [epoch: 9.72 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520736340170887		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 3.520736340170887 | validation: 3.082417103823461]
	TIME [epoch: 9.72 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527710963094846		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 3.527710963094846 | validation: 3.067114269207853]
	TIME [epoch: 9.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152102709703867		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 3.5152102709703867 | validation: 3.0701806086925427]
	TIME [epoch: 9.74 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5230642756602464		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 3.5230642756602464 | validation: 3.0711482258086216]
	TIME [epoch: 9.71 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5214796732809632		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 3.5214796732809632 | validation: 3.0545761928163073]
	TIME [epoch: 9.73 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519577441184686		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 3.519577441184686 | validation: 3.070530225014309]
	TIME [epoch: 9.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5214367802215394		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 3.5214367802215394 | validation: 3.038743859641985]
	TIME [epoch: 9.71 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5168250589636116		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 3.5168250589636116 | validation: 3.052565521838364]
	TIME [epoch: 9.71 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520893974461836		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 3.520893974461836 | validation: 3.0531888310883204]
	TIME [epoch: 9.72 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5264946275234657		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 3.5264946275234657 | validation: 3.0844102271611393]
	TIME [epoch: 9.73 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52141700205877		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 3.52141700205877 | validation: 3.061890295199615]
	TIME [epoch: 9.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525099446470074		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 3.525099446470074 | validation: 3.0452704071239043]
	TIME [epoch: 9.71 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527563007472364		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 3.527563007472364 | validation: 3.0508061570154412]
	TIME [epoch: 9.72 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5271262385130955		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 3.5271262385130955 | validation: 3.0508076183429433]
	TIME [epoch: 9.72 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522172920140851		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 3.522172920140851 | validation: 3.0549769645554243]
	TIME [epoch: 9.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5289277095128484		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 3.5289277095128484 | validation: 3.083296037747633]
	TIME [epoch: 9.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183969370354653		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 3.5183969370354653 | validation: 3.0571410317228747]
	TIME [epoch: 9.72 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5270929175771295		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 3.5270929175771295 | validation: 3.0631223419789966]
	TIME [epoch: 9.71 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5286522094472765		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 3.5286522094472765 | validation: 3.067655441445088]
	TIME [epoch: 9.71 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5315800140584317		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 3.5315800140584317 | validation: 3.056598368006373]
	TIME [epoch: 9.73 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5181546241536816		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 3.5181546241536816 | validation: 3.048324213610268]
	TIME [epoch: 9.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5234916521202755		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 3.5234916521202755 | validation: 3.070654175615674]
	TIME [epoch: 9.71 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.53659798804168		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 3.53659798804168 | validation: 3.093772482425146]
	TIME [epoch: 9.72 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5196804416993492		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 3.5196804416993492 | validation: 3.0455260915465647]
	TIME [epoch: 9.72 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5213236663891285		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 3.5213236663891285 | validation: 3.0476090853581614]
	TIME [epoch: 9.71 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519106434024421		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 3.519106434024421 | validation: 3.054045438553046]
	TIME [epoch: 9.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520019560403896		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 3.520019560403896 | validation: 3.058919495506534]
	TIME [epoch: 9.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513812111809382		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 3.513812111809382 | validation: 3.0654616080083152]
	TIME [epoch: 9.71 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135009734958245		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 3.5135009734958245 | validation: 3.0422255471437336]
	TIME [epoch: 9.71 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516362423112395		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 3.516362423112395 | validation: 3.0521886759743793]
	TIME [epoch: 9.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138172885897867		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 3.5138172885897867 | validation: 3.0525310565554014]
	TIME [epoch: 9.72 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5181155969722866		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 3.5181155969722866 | validation: 3.0566187165839858]
	TIME [epoch: 9.71 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176709700244144		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 3.5176709700244144 | validation: 3.057268884269648]
	TIME [epoch: 9.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530695661670132		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 3.530695661670132 | validation: 3.0752099442636234]
	TIME [epoch: 9.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5249092724017776		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 3.5249092724017776 | validation: 3.074533173052803]
	TIME [epoch: 9.71 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5214392401659147		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 3.5214392401659147 | validation: 3.056672935241228]
	TIME [epoch: 9.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5331186400650645		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 3.5331186400650645 | validation: 3.0683072740639648]
	TIME [epoch: 9.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523375833937679		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 3.523375833937679 | validation: 3.047792363364179]
	TIME [epoch: 9.72 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517766803556235		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 3.517766803556235 | validation: 3.0632468251660483]
	TIME [epoch: 9.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5211617418785415		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 3.5211617418785415 | validation: 3.052091470260754]
	TIME [epoch: 9.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5162022868758362		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 3.5162022868758362 | validation: 3.0526450429151875]
	TIME [epoch: 9.72 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515742951774132		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 3.515742951774132 | validation: 3.060420799193953]
	TIME [epoch: 9.71 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518902304278342		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 3.518902304278342 | validation: 3.0461035547212156]
	TIME [epoch: 9.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197204779680518		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 3.5197204779680518 | validation: 3.0485568941839922]
	TIME [epoch: 9.73 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515359542893978		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 3.515359542893978 | validation: 3.0496278628390803]
	TIME [epoch: 9.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5137479530910847		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 3.5137479530910847 | validation: 3.048299871188603]
	TIME [epoch: 9.71 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5216416205622254		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 3.5216416205622254 | validation: 3.0407558324006985]
	TIME [epoch: 9.71 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51898602378778		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 3.51898602378778 | validation: 3.0498702544156884]
	TIME [epoch: 9.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5101576930786833		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 3.5101576930786833 | validation: 3.0479826302871413]
	TIME [epoch: 9.71 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520738550846302		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 3.520738550846302 | validation: 3.0430632419226384]
	TIME [epoch: 9.71 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193262782886663		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 3.5193262782886663 | validation: 3.0444745664182675]
	TIME [epoch: 9.73 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193965003147083		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 3.5193965003147083 | validation: 3.049011967404047]
	TIME [epoch: 9.71 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521293936421759		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 3.521293936421759 | validation: 3.057988798136686]
	TIME [epoch: 9.71 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5244165112128925		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 3.5244165112128925 | validation: 3.055301609737751]
	TIME [epoch: 9.73 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514258991927165		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 3.514258991927165 | validation: 3.0420641724161137]
	TIME [epoch: 9.72 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5215183904202916		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 3.5215183904202916 | validation: 3.0548506385050915]
	TIME [epoch: 9.71 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52343163807395		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 3.52343163807395 | validation: 3.058227080082655]
	TIME [epoch: 9.71 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5244000066325265		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 3.5244000066325265 | validation: 3.0511316604875978]
	TIME [epoch: 9.71 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518203226679888		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 3.518203226679888 | validation: 3.0485104782445083]
	TIME [epoch: 9.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515308665520209		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 3.515308665520209 | validation: 3.0528676849363454]
	TIME [epoch: 9.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516292735412776		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 3.516292735412776 | validation: 3.062804246110758]
	TIME [epoch: 9.72 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521364202310835		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 3.521364202310835 | validation: 3.0715259290791215]
	TIME [epoch: 9.71 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5218891312946554		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 3.5218891312946554 | validation: 3.0753013632962247]
	TIME [epoch: 9.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5255450634307843		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 3.5255450634307843 | validation: 3.0810944564958933]
	TIME [epoch: 9.71 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5230719215338033		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 3.5230719215338033 | validation: 3.0526799694448314]
	TIME [epoch: 9.71 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522991778314099		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 3.522991778314099 | validation: 3.071315870276469]
	TIME [epoch: 9.71 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166407102555226		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 3.5166407102555226 | validation: 3.0576513384790163]
	TIME [epoch: 9.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5199192367014143		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 3.5199192367014143 | validation: 3.0634863233369782]
	TIME [epoch: 9.73 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522950288057841		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 3.522950288057841 | validation: 3.060617402944881]
	TIME [epoch: 9.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511744256751682		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 3.511744256751682 | validation: 3.047832398584313]
	TIME [epoch: 9.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520233259979579		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 3.520233259979579 | validation: 3.056987472592867]
	TIME [epoch: 9.72 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152767823812896		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 3.5152767823812896 | validation: 3.0579165051466033]
	TIME [epoch: 9.71 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516063277831239		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 3.516063277831239 | validation: 3.052710146822932]
	TIME [epoch: 9.71 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155811745384833		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 3.5155811745384833 | validation: 3.063001878934184]
	TIME [epoch: 9.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513612368829534		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 3.513612368829534 | validation: 3.056545628878615]
	TIME [epoch: 9.73 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516458359643378		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 3.516458359643378 | validation: 3.0589310701404204]
	TIME [epoch: 9.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519034442986367		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 3.519034442986367 | validation: 3.0616665062307122]
	TIME [epoch: 9.69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529689737485172		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 3.529689737485172 | validation: 3.0626348477605565]
	TIME [epoch: 9.71 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172766275038683		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 3.5172766275038683 | validation: 3.0598467353870116]
	TIME [epoch: 9.71 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154064823469633		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 3.5154064823469633 | validation: 3.0586987130170904]
	TIME [epoch: 9.71 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5192349824980873		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 3.5192349824980873 | validation: 3.0635975549651606]
	TIME [epoch: 9.72 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519319955216524		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 3.519319955216524 | validation: 3.0711164836443827]
	TIME [epoch: 9.72 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188588567018897		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 3.5188588567018897 | validation: 3.0539331787342405]
	TIME [epoch: 9.71 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5218159281332966		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 3.5218159281332966 | validation: 3.0515368536941536]
	TIME [epoch: 9.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5288480952793355		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 3.5288480952793355 | validation: 3.043270567023807]
	TIME [epoch: 9.73 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516479280724089		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 3.516479280724089 | validation: 3.05411240357963]
	TIME [epoch: 9.71 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5206623505238084		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 3.5206623505238084 | validation: 3.0472438827516886]
	TIME [epoch: 9.71 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188116105759817		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 3.5188116105759817 | validation: 3.0530348685041204]
	TIME [epoch: 9.72 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5222021897149047		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 3.5222021897149047 | validation: 3.056186051352703]
	TIME [epoch: 9.71 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5192413359570773		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 3.5192413359570773 | validation: 3.050028696630794]
	TIME [epoch: 9.71 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512762269200595		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 3.512762269200595 | validation: 3.068903663294357]
	TIME [epoch: 9.72 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5227273814300863		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 3.5227273814300863 | validation: 3.046060490969208]
	TIME [epoch: 9.72 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172113429699907		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 3.5172113429699907 | validation: 3.0529787396123194]
	TIME [epoch: 9.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173763588182267		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 3.5173763588182267 | validation: 3.0738492985927706]
	TIME [epoch: 9.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182002907173504		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 3.5182002907173504 | validation: 3.059447628209298]
	TIME [epoch: 9.72 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516053566415077		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 3.516053566415077 | validation: 3.0545372491168474]
	TIME [epoch: 9.72 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167740165105017		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 3.5167740165105017 | validation: 3.054714832331772]
	TIME [epoch: 9.72 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519228898441388		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 3.519228898441388 | validation: 3.0556215903136876]
	TIME [epoch: 9.72 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519738252855877		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 3.519738252855877 | validation: 3.0529326438619835]
	TIME [epoch: 9.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150920369057843		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 3.5150920369057843 | validation: 3.0650399873593144]
	TIME [epoch: 9.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195213857735856		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 3.5195213857735856 | validation: 3.061571064565858]
	TIME [epoch: 9.71 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51617256980571		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 3.51617256980571 | validation: 3.0825954563950586]
	TIME [epoch: 9.72 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523713101009528		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 3.523713101009528 | validation: 3.0635919728448258]
	TIME [epoch: 9.71 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5206435543009142		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 3.5206435543009142 | validation: 3.046206235756302]
	TIME [epoch: 9.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516761550620745		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 3.516761550620745 | validation: 3.0601884143865146]
	TIME [epoch: 9.74 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521646228970192		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 3.521646228970192 | validation: 3.081742314761783]
	TIME [epoch: 9.71 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5238232257084703		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 3.5238232257084703 | validation: 3.054993141526251]
	TIME [epoch: 9.71 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520847287001511		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 3.520847287001511 | validation: 3.060854172588798]
	TIME [epoch: 9.7 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161213828010247		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 3.5161213828010247 | validation: 3.0532631803723014]
	TIME [epoch: 9.73 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511906454567584		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 3.511906454567584 | validation: 3.052303536911574]
	TIME [epoch: 9.71 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171757553203036		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 3.5171757553203036 | validation: 3.0499072661609086]
	TIME [epoch: 9.72 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163326076093826		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 3.5163326076093826 | validation: 3.0545855253497813]
	TIME [epoch: 9.72 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514391166218087		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 3.514391166218087 | validation: 3.0614405481012756]
	TIME [epoch: 9.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5209261327303225		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 3.5209261327303225 | validation: 3.048603457542695]
	TIME [epoch: 9.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51799925823835		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 3.51799925823835 | validation: 3.057016917633444]
	TIME [epoch: 9.71 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5191678731763063		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 3.5191678731763063 | validation: 3.0478962416354967]
	TIME [epoch: 9.71 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515631688121207		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 3.515631688121207 | validation: 3.0529835876839218]
	TIME [epoch: 9.72 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5268341910119054		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 3.5268341910119054 | validation: 3.0551095864141713]
	TIME [epoch: 9.73 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520708897494744		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 3.520708897494744 | validation: 3.0680880306131564]
	TIME [epoch: 9.72 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520665018921897		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 3.520665018921897 | validation: 3.0578327975661965]
	TIME [epoch: 9.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186519889819188		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 3.5186519889819188 | validation: 3.080798128198272]
	TIME [epoch: 9.72 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171091135398136		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 3.5171091135398136 | validation: 3.062135762336315]
	TIME [epoch: 9.73 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5214065156962304		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 3.5214065156962304 | validation: 3.062110498467323]
	TIME [epoch: 9.71 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212117569073107		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 3.5212117569073107 | validation: 3.0681136614946185]
	TIME [epoch: 9.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152223038087294		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 3.5152223038087294 | validation: 3.046407084990245]
	TIME [epoch: 9.71 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139701014566205		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 3.5139701014566205 | validation: 3.0460074396177546]
	TIME [epoch: 9.72 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173850217873266		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 3.5173850217873266 | validation: 3.0656024369711874]
	TIME [epoch: 9.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190361947782542		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 3.5190361947782542 | validation: 3.059660676436219]
	TIME [epoch: 9.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193176831922686		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 3.5193176831922686 | validation: 3.0580607324175633]
	TIME [epoch: 9.73 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149088815819907		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 3.5149088815819907 | validation: 3.0567707595963625]
	TIME [epoch: 9.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166363546018067		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 3.5166363546018067 | validation: 3.04130962432557]
	TIME [epoch: 9.71 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5082555179035753		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 3.5082555179035753 | validation: 3.063058869679519]
	TIME [epoch: 9.71 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223039002669942		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 3.5223039002669942 | validation: 3.0525944502532374]
	TIME [epoch: 9.73 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177396432874266		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 3.5177396432874266 | validation: 3.042997811476049]
	TIME [epoch: 9.72 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5174966052393897		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 3.5174966052393897 | validation: 3.0554511350675693]
	TIME [epoch: 9.72 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171457348864443		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 3.5171457348864443 | validation: 3.041591042760542]
	TIME [epoch: 9.73 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5133637068469015		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 3.5133637068469015 | validation: 3.0585893115927334]
	TIME [epoch: 9.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5164538701394754		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 3.5164538701394754 | validation: 3.0556679928350934]
	TIME [epoch: 9.7 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167927176080993		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 3.5167927176080993 | validation: 3.066981942108084]
	TIME [epoch: 9.73 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51819367204767		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 3.51819367204767 | validation: 3.0467558699695223]
	TIME [epoch: 9.71 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225632621086973		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 3.5225632621086973 | validation: 3.0514797767268376]
	TIME [epoch: 9.71 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5181154513860604		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 3.5181154513860604 | validation: 3.0590272238151677]
	TIME [epoch: 9.72 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520202022147905		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 3.520202022147905 | validation: 3.044107272790766]
	TIME [epoch: 9.72 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519535687307021		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 3.519535687307021 | validation: 3.047675305290629]
	TIME [epoch: 9.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197769771713183		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 3.5197769771713183 | validation: 3.0552259768102625]
	TIME [epoch: 9.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530269401271209		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 3.530269401271209 | validation: 3.059386874637603]
	TIME [epoch: 9.72 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147873464656585		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 3.5147873464656585 | validation: 3.051920367700104]
	TIME [epoch: 9.71 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5205857695159812		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 3.5205857695159812 | validation: 3.0652126503302233]
	TIME [epoch: 9.71 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519187649726649		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 3.519187649726649 | validation: 3.0568062737645607]
	TIME [epoch: 9.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5203400179041253		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 3.5203400179041253 | validation: 3.051000872661146]
	TIME [epoch: 9.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142774211793117		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 3.5142774211793117 | validation: 3.0600985532082023]
	TIME [epoch: 9.71 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5256076499696176		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 3.5256076499696176 | validation: 3.063228833759323]
	TIME [epoch: 9.71 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5224210385362413		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 3.5224210385362413 | validation: 3.057155767708552]
	TIME [epoch: 9.71 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151680701046337		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 3.5151680701046337 | validation: 3.052209064235624]
	TIME [epoch: 9.71 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172211155082254		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 3.5172211155082254 | validation: 3.0628083306723863]
	TIME [epoch: 9.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5213147575860986		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 3.5213147575860986 | validation: 3.055276468032263]
	TIME [epoch: 9.73 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518745385140516		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 3.518745385140516 | validation: 3.057881148435208]
	TIME [epoch: 9.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521248399008992		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 3.521248399008992 | validation: 3.060819944729602]
	TIME [epoch: 9.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5184186971755067		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 3.5184186971755067 | validation: 3.053848974534166]
	TIME [epoch: 9.72 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5136169732547344		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 3.5136169732547344 | validation: 3.059709995363225]
	TIME [epoch: 9.73 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5098367333305758		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 3.5098367333305758 | validation: 3.0444373833163376]
	TIME [epoch: 9.71 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5178213343969924		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 3.5178213343969924 | validation: 3.0546418070564845]
	TIME [epoch: 9.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5209237549489325		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 3.5209237549489325 | validation: 3.0350577113903037]
	TIME [epoch: 9.73 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158918331487796		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 3.5158918331487796 | validation: 3.055548706170814]
	TIME [epoch: 9.71 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180172186013783		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 3.5180172186013783 | validation: 3.0507437756541993]
	TIME [epoch: 9.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5103198665667		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 3.5103198665667 | validation: 3.0580327392907614]
	TIME [epoch: 9.71 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155371817482064		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 3.5155371817482064 | validation: 3.0574125676689494]
	TIME [epoch: 9.72 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5156112845742613		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 3.5156112845742613 | validation: 3.047386192542217]
	TIME [epoch: 9.71 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516745588356649		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 3.516745588356649 | validation: 3.047698335497047]
	TIME [epoch: 9.71 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144370995517895		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 3.5144370995517895 | validation: 3.050027368033624]
	TIME [epoch: 9.74 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520736996918939		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 3.520736996918939 | validation: 3.0519124981739036]
	TIME [epoch: 9.74 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154276675339746		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 3.5154276675339746 | validation: 3.049628894117735]
	TIME [epoch: 9.72 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5275951437277158		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 3.5275951437277158 | validation: 3.0542025855705766]
	TIME [epoch: 9.73 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512268505092851		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 3.512268505092851 | validation: 3.047989096541204]
	TIME [epoch: 9.73 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183538282654245		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 3.5183538282654245 | validation: 3.053169540122592]
	TIME [epoch: 9.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152195088524976		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 3.5152195088524976 | validation: 3.057921087032629]
	TIME [epoch: 9.71 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519613997268512		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 3.519613997268512 | validation: 3.040804872134362]
	TIME [epoch: 9.75 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172773813983524		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 3.5172773813983524 | validation: 3.049807364131747]
	TIME [epoch: 9.71 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151787363996703		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 3.5151787363996703 | validation: 3.0572338894058717]
	TIME [epoch: 9.71 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5208124607268614		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 3.5208124607268614 | validation: 3.0482343539932657]
	TIME [epoch: 9.73 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52034486482266		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 3.52034486482266 | validation: 3.0447284040801255]
	TIME [epoch: 9.71 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161493757904596		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 3.5161493757904596 | validation: 3.0540396596373753]
	TIME [epoch: 9.72 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521198079653973		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 3.521198079653973 | validation: 3.054687356878161]
	TIME [epoch: 9.71 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5232541292960766		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 3.5232541292960766 | validation: 3.0594800133959827]
	TIME [epoch: 9.73 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520912484983674		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 3.520912484983674 | validation: 3.0528768639745927]
	TIME [epoch: 9.71 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139866609694854		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 3.5139866609694854 | validation: 3.051316201844454]
	TIME [epoch: 9.71 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5130190623201694		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 3.5130190623201694 | validation: 3.0496616897078432]
	TIME [epoch: 9.73 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5201429858982807		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 3.5201429858982807 | validation: 3.053189013930987]
	TIME [epoch: 9.73 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522225204885488		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 3.522225204885488 | validation: 3.0651660868616477]
	TIME [epoch: 9.71 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5219758842025315		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 3.5219758842025315 | validation: 3.0814449808487043]
	TIME [epoch: 9.73 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5205860519250414		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 3.5205860519250414 | validation: 3.0522356699825197]
	TIME [epoch: 9.74 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182675192664474		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 3.5182675192664474 | validation: 3.055641223162694]
	TIME [epoch: 9.72 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158909482679315		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 3.5158909482679315 | validation: 3.078878594980624]
	TIME [epoch: 9.71 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5292711948585387		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 3.5292711948585387 | validation: 3.065271280407876]
	TIME [epoch: 9.74 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51919880740043		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 3.51919880740043 | validation: 3.064803299874835]
	TIME [epoch: 9.73 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197867082908494		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 3.5197867082908494 | validation: 3.050745861267986]
	TIME [epoch: 9.71 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519432426542351		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 3.519432426542351 | validation: 3.0573272999039927]
	TIME [epoch: 9.74 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517581258760758		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 3.517581258760758 | validation: 3.046313552642495]
	TIME [epoch: 9.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518272907132535		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 3.518272907132535 | validation: 3.049103113652677]
	TIME [epoch: 9.72 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519064516003543		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 3.519064516003543 | validation: 3.057801305833061]
	TIME [epoch: 9.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212615250893835		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 3.5212615250893835 | validation: 3.050632170779506]
	TIME [epoch: 9.73 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519781138111502		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 3.519781138111502 | validation: 3.0519885103562]
	TIME [epoch: 9.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515793828655551		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 3.515793828655551 | validation: 3.05459635821717]
	TIME [epoch: 9.73 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518039510048548		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 3.518039510048548 | validation: 3.054655923705733]
	TIME [epoch: 9.73 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520021882408352		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 3.520021882408352 | validation: 3.043716377790226]
	TIME [epoch: 9.72 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509943575704095		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 3.509943575704095 | validation: 3.051934877271125]
	TIME [epoch: 9.72 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197434830792864		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 3.5197434830792864 | validation: 3.056646646621224]
	TIME [epoch: 9.73 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5187317171963364		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 3.5187317171963364 | validation: 3.0528916597293163]
	TIME [epoch: 9.74 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176134136693777		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 3.5176134136693777 | validation: 3.0474232371811887]
	TIME [epoch: 9.71 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5207184230383666		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 3.5207184230383666 | validation: 3.058366512266394]
	TIME [epoch: 9.71 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5198039739810283		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 3.5198039739810283 | validation: 3.047253151662632]
	TIME [epoch: 9.73 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182978971738015		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 3.5182978971738015 | validation: 3.057316681655143]
	TIME [epoch: 9.71 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186023600963496		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 3.5186023600963496 | validation: 3.0489500096892357]
	TIME [epoch: 9.71 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5132053498829867		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 3.5132053498829867 | validation: 3.0403797996302693]
	TIME [epoch: 9.72 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180697559066294		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 3.5180697559066294 | validation: 3.0592091132960713]
	TIME [epoch: 9.73 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138466378988666		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 3.5138466378988666 | validation: 3.0459261099278256]
	TIME [epoch: 9.71 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5098555545828916		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 3.5098555545828916 | validation: 3.0470096066921393]
	TIME [epoch: 9.71 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145420257406186		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 3.5145420257406186 | validation: 3.043311030562406]
	TIME [epoch: 9.73 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5146819415985098		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 3.5146819415985098 | validation: 3.0523590456340286]
	TIME [epoch: 9.73 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519770985629529		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 3.519770985629529 | validation: 3.056928962267586]
	TIME [epoch: 9.72 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161782925579317		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 3.5161782925579317 | validation: 3.0612852388764127]
	TIME [epoch: 9.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513403086703055		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 3.513403086703055 | validation: 3.066211067049039]
	TIME [epoch: 9.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522547010984391		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 3.522547010984391 | validation: 3.0611515058580974]
	TIME [epoch: 9.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5192417748710505		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 3.5192417748710505 | validation: 3.0553623860114842]
	TIME [epoch: 9.73 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5174178282319692		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 3.5174178282319692 | validation: 3.0499513899359996]
	TIME [epoch: 9.73 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135653027918776		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 3.5135653027918776 | validation: 3.052162473009314]
	TIME [epoch: 9.72 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5159079951208496		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 3.5159079951208496 | validation: 3.0429914546743375]
	TIME [epoch: 9.71 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517301778013946		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 3.517301778013946 | validation: 3.0548914674036842]
	TIME [epoch: 9.72 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518057362696267		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 3.518057362696267 | validation: 3.0436824971372483]
	TIME [epoch: 9.72 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148947892658207		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 3.5148947892658207 | validation: 3.047666945182359]
	TIME [epoch: 9.71 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5168283635088153		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 3.5168283635088153 | validation: 3.066428633721338]
	TIME [epoch: 9.73 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155548451814504		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 3.5155548451814504 | validation: 3.0499043394888408]
	TIME [epoch: 9.71 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5105737590492154		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 3.5105737590492154 | validation: 3.053489073297769]
	TIME [epoch: 9.71 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514208558974032		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 3.514208558974032 | validation: 3.0664331676759775]
	TIME [epoch: 9.71 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5206707017042462		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 3.5206707017042462 | validation: 3.0627064012394043]
	TIME [epoch: 9.73 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5189580460898697		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 3.5189580460898697 | validation: 3.05945638473215]
	TIME [epoch: 9.7 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515978382339506		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 3.515978382339506 | validation: 3.0550322571208013]
	TIME [epoch: 9.7 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516593215389366		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 3.516593215389366 | validation: 3.0536750334896077]
	TIME [epoch: 9.72 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176277981586166		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 3.5176277981586166 | validation: 3.054883537145678]
	TIME [epoch: 9.7 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5179725891729503		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 3.5179725891729503 | validation: 3.035708442786455]
	TIME [epoch: 9.71 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5156358993527377		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 3.5156358993527377 | validation: 3.0469855739651677]
	TIME [epoch: 9.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514746018262605		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 3.514746018262605 | validation: 3.05787063109811]
	TIME [epoch: 9.73 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183889879888346		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 3.5183889879888346 | validation: 3.0472494934744807]
	TIME [epoch: 9.72 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5204488067086706		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 3.5204488067086706 | validation: 3.0539117011915153]
	TIME [epoch: 9.71 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155991677752767		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 3.5155991677752767 | validation: 3.050685955423446]
	TIME [epoch: 9.72 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161555467796513		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 3.5161555467796513 | validation: 3.0643673156040467]
	TIME [epoch: 9.71 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153447503440787		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 3.5153447503440787 | validation: 3.0558201471492907]
	TIME [epoch: 9.7 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202259970685517		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 3.5202259970685517 | validation: 3.0480543526767305]
	TIME [epoch: 9.72 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518232797158723		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 3.518232797158723 | validation: 3.054698677361272]
	TIME [epoch: 9.72 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5205669194584326		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 3.5205669194584326 | validation: 3.044752146492143]
	TIME [epoch: 9.7 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5174755549289403		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 3.5174755549289403 | validation: 3.058849151662055]
	TIME [epoch: 9.7 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183702851149286		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 3.5183702851149286 | validation: 3.061450464974937]
	TIME [epoch: 9.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142511768432683		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 3.5142511768432683 | validation: 3.049510985499745]
	TIME [epoch: 9.72 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149853849622055		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 3.5149853849622055 | validation: 3.051649680361686]
	TIME [epoch: 9.69 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516102120161007		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 3.516102120161007 | validation: 3.0532228241154145]
	TIME [epoch: 9.7 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517776316177703		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 3.517776316177703 | validation: 3.056649225638196]
	TIME [epoch: 9.69 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157423550284035		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 3.5157423550284035 | validation: 3.0511346485459705]
	TIME [epoch: 9.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5181320216909784		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 3.5181320216909784 | validation: 3.05605050352886]
	TIME [epoch: 9.72 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139043997850514		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 3.5139043997850514 | validation: 3.0414711638432714]
	TIME [epoch: 9.72 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138959661855247		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 3.5138959661855247 | validation: 3.049340939418249]
	TIME [epoch: 9.7 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518655382382358		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 3.518655382382358 | validation: 3.049915735408077]
	TIME [epoch: 9.72 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515634075735832		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 3.515634075735832 | validation: 3.051997262733982]
	TIME [epoch: 9.73 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513373290022051		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 3.513373290022051 | validation: 3.0408649331593165]
	TIME [epoch: 9.73 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212763742236213		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 3.5212763742236213 | validation: 3.0429323017519003]
	TIME [epoch: 9.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5162212849638492		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 3.5162212849638492 | validation: 3.0513348761484465]
	TIME [epoch: 9.72 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158599400979087		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 3.5158599400979087 | validation: 3.050363997730382]
	TIME [epoch: 9.7 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513225447526569		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 3.513225447526569 | validation: 3.0479324409186748]
	TIME [epoch: 9.71 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152924803675583		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 3.5152924803675583 | validation: 3.0662367239237995]
	TIME [epoch: 9.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173327765507856		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 3.5173327765507856 | validation: 3.0446524589796566]
	TIME [epoch: 9.71 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186778294485292		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 3.5186778294485292 | validation: 3.0605200618323045]
	TIME [epoch: 9.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518523796923026		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 3.518523796923026 | validation: 3.059395414265012]
	TIME [epoch: 9.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513406400259568		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 3.513406400259568 | validation: 3.060714624673855]
	TIME [epoch: 9.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52025713217857		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 3.52025713217857 | validation: 3.0461910352169865]
	TIME [epoch: 9.71 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144464718053796		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 3.5144464718053796 | validation: 3.0449257854880294]
	TIME [epoch: 9.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519657473326165		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 3.519657473326165 | validation: 3.054727428736675]
	TIME [epoch: 9.72 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5136063872634566		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 3.5136063872634566 | validation: 3.0684954432098754]
	TIME [epoch: 9.71 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5141717496691762		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 3.5141717496691762 | validation: 3.0550714497974845]
	TIME [epoch: 9.72 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5174707916056676		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 3.5174707916056676 | validation: 3.058430171053169]
	TIME [epoch: 9.71 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135992576640285		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 3.5135992576640285 | validation: 3.049472829054864]
	TIME [epoch: 9.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5123810833125475		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 3.5123810833125475 | validation: 3.0462524223115306]
	TIME [epoch: 9.69 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514131308834709		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 3.514131308834709 | validation: 3.0575070362607466]
	TIME [epoch: 9.7 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518428131535275		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 3.518428131535275 | validation: 3.0515662239321917]
	TIME [epoch: 9.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157538454237054		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 3.5157538454237054 | validation: 3.041114740381902]
	TIME [epoch: 9.7 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152091248990978		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 3.5152091248990978 | validation: 3.0536883860600175]
	TIME [epoch: 9.7 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167627194920748		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 3.5167627194920748 | validation: 3.062527808726681]
	TIME [epoch: 9.7 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522611138797825		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 3.522611138797825 | validation: 3.0681016097885663]
	TIME [epoch: 9.72 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151460321486616		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 3.5151460321486616 | validation: 3.0472661396472067]
	TIME [epoch: 9.7 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523134262580231		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 3.523134262580231 | validation: 3.0635038669446786]
	TIME [epoch: 9.7 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516016643500534		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 3.516016643500534 | validation: 3.05740726041628]
	TIME [epoch: 9.72 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151889550964817		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 3.5151889550964817 | validation: 3.052350155803605]
	TIME [epoch: 9.7 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511428004966511		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 3.511428004966511 | validation: 3.062153829367567]
	TIME [epoch: 9.7 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513071671385385		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 3.513071671385385 | validation: 3.055618566218943]
	TIME [epoch: 9.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165349915196344		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 3.5165349915196344 | validation: 3.0527020160955924]
	TIME [epoch: 9.73 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197284074796578		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 3.5197284074796578 | validation: 3.0515996544234567]
	TIME [epoch: 9.71 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5128818246934044		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 3.5128818246934044 | validation: 3.0586910068085666]
	TIME [epoch: 9.71 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521190982075022		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 3.521190982075022 | validation: 3.060491325223637]
	TIME [epoch: 9.71 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516361696350404		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 3.516361696350404 | validation: 3.0631641106312344]
	TIME [epoch: 9.71 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172012874331413		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 3.5172012874331413 | validation: 3.056343299694874]
	TIME [epoch: 9.7 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527095854839339		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 3.527095854839339 | validation: 3.0652203527993147]
	TIME [epoch: 9.72 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517183061615963		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 3.517183061615963 | validation: 3.067274263413458]
	TIME [epoch: 9.71 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180740396297936		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 3.5180740396297936 | validation: 3.0446988692994945]
	TIME [epoch: 9.71 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150890225451876		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 3.5150890225451876 | validation: 3.067693841571879]
	TIME [epoch: 9.7 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517775284624142		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 3.517775284624142 | validation: 3.0633498532043366]
	TIME [epoch: 9.71 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519088435713523		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 3.519088435713523 | validation: 3.062092797934708]
	TIME [epoch: 9.69 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519170541775556		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 3.519170541775556 | validation: 3.0594459353885752]
	TIME [epoch: 9.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517817088253956		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 3.517817088253956 | validation: 3.0474541391247274]
	TIME [epoch: 9.72 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5185073324318012		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 3.5185073324318012 | validation: 3.0550395488169757]
	TIME [epoch: 9.72 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157369146815896		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 3.5157369146815896 | validation: 3.0472820873662956]
	TIME [epoch: 9.7 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515688836253685		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 3.515688836253685 | validation: 3.0463179625809467]
	TIME [epoch: 9.71 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5103844372759334		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 3.5103844372759334 | validation: 3.055257765145006]
	TIME [epoch: 9.73 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193924393181093		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 3.5193924393181093 | validation: 3.053385990498455]
	TIME [epoch: 9.71 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515544117807532		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 3.515544117807532 | validation: 3.052360736162455]
	TIME [epoch: 9.71 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154160416836375		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 3.5154160416836375 | validation: 3.0494512024609026]
	TIME [epoch: 9.73 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513432763948967		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 3.513432763948967 | validation: 3.0538272784131473]
	TIME [epoch: 9.71 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190393348546443		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 3.5190393348546443 | validation: 3.055549605204145]
	TIME [epoch: 9.7 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183181025105		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 3.5183181025105 | validation: 3.055086480362129]
	TIME [epoch: 9.72 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5108863672697432		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 3.5108863672697432 | validation: 3.0553887687782195]
	TIME [epoch: 9.73 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519228446030533		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 3.519228446030533 | validation: 3.0645231515164415]
	TIME [epoch: 9.72 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5206495803317326		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 3.5206495803317326 | validation: 3.071665208154315]
	TIME [epoch: 9.71 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51814562652275		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 3.51814562652275 | validation: 3.0514068037640873]
	TIME [epoch: 9.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5201234885948787		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 3.5201234885948787 | validation: 3.0601462947887996]
	TIME [epoch: 9.71 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5224488562209837		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 3.5224488562209837 | validation: 3.047907583964037]
	TIME [epoch: 9.71 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195301767179217		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 3.5195301767179217 | validation: 3.049419398386825]
	TIME [epoch: 9.73 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515140645847828		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 3.515140645847828 | validation: 3.05020374863105]
	TIME [epoch: 9.71 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517827854547659		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 3.517827854547659 | validation: 3.0538474217310125]
	TIME [epoch: 9.71 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517063246465588		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 3.517063246465588 | validation: 3.050554771196447]
	TIME [epoch: 9.7 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5184330892795863		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 3.5184330892795863 | validation: 3.050612154633609]
	TIME [epoch: 9.72 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169532864156947		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 3.5169532864156947 | validation: 3.0600645071543227]
	TIME [epoch: 9.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5097589417102872		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 3.5097589417102872 | validation: 3.062229150449665]
	TIME [epoch: 9.72 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519180265367588		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 3.519180265367588 | validation: 3.068227089374371]
	TIME [epoch: 9.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5208875977890957		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 3.5208875977890957 | validation: 3.049271832158081]
	TIME [epoch: 9.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190839203088133		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 3.5190839203088133 | validation: 3.0477737723875338]
	TIME [epoch: 9.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519852305633415		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 3.519852305633415 | validation: 3.057576304943901]
	TIME [epoch: 9.71 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188428290329625		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 3.5188428290329625 | validation: 3.0495029139527037]
	TIME [epoch: 9.73 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144287517170327		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 3.5144287517170327 | validation: 3.055006155780617]
	TIME [epoch: 9.7 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520430398951286		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 3.520430398951286 | validation: 3.047504022037166]
	TIME [epoch: 9.7 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517468888773572		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 3.517468888773572 | validation: 3.056033526877377]
	TIME [epoch: 9.71 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144513839908207		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 3.5144513839908207 | validation: 3.0452801384707993]
	TIME [epoch: 9.7 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182604679545455		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 3.5182604679545455 | validation: 3.0662979859957282]
	TIME [epoch: 9.71 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513923135233403		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 3.513923135233403 | validation: 3.0638444629105694]
	TIME [epoch: 9.72 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512150174068927		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 3.512150174068927 | validation: 3.0583623596794713]
	TIME [epoch: 9.71 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161925514122894		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 3.5161925514122894 | validation: 3.0506470036318594]
	TIME [epoch: 9.7 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513386446382158		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 3.513386446382158 | validation: 3.0724483504556765]
	TIME [epoch: 9.7 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51592157211005		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 3.51592157211005 | validation: 3.0471059089545385]
	TIME [epoch: 9.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5187575525976227		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 3.5187575525976227 | validation: 3.0488333538944654]
	TIME [epoch: 9.69 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51220208466808		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 3.51220208466808 | validation: 3.0448546779422907]
	TIME [epoch: 9.7 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513131668319976		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 3.513131668319976 | validation: 3.051187241182056]
	TIME [epoch: 9.71 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5237770806807673		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 3.5237770806807673 | validation: 3.0521793125676617]
	TIME [epoch: 9.71 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5136519012397303		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 3.5136519012397303 | validation: 3.054313011576776]
	TIME [epoch: 9.7 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145116295493595		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 3.5145116295493595 | validation: 3.050818377814475]
	TIME [epoch: 9.72 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51564690666669		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 3.51564690666669 | validation: 3.051419150816183]
	TIME [epoch: 9.71 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516798153040397		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 3.516798153040397 | validation: 3.0535663404291524]
	TIME [epoch: 9.7 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170878322339556		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 3.5170878322339556 | validation: 3.0534496641597846]
	TIME [epoch: 9.71 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5178253570436118		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 3.5178253570436118 | validation: 3.0608018288089385]
	TIME [epoch: 9.73 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512757843518093		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 3.512757843518093 | validation: 3.0576204737269608]
	TIME [epoch: 9.7 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225098157521977		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 3.5225098157521977 | validation: 3.056925468878542]
	TIME [epoch: 9.7 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513830493770957		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 3.513830493770957 | validation: 3.0517408017059116]
	TIME [epoch: 9.72 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5201804549077296		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 3.5201804549077296 | validation: 3.045470095009407]
	TIME [epoch: 9.71 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517693380926449		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 3.517693380926449 | validation: 3.059132124124434]
	TIME [epoch: 9.69 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165912175742307		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 3.5165912175742307 | validation: 3.059113206269176]
	TIME [epoch: 9.7 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166937621544854		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 3.5166937621544854 | validation: 3.0580100151332044]
	TIME [epoch: 9.73 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144242052256223		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 3.5144242052256223 | validation: 3.062324696565329]
	TIME [epoch: 9.72 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165219335663958		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 3.5165219335663958 | validation: 3.0520113949686]
	TIME [epoch: 9.71 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126755243752243		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 3.5126755243752243 | validation: 3.051746476606521]
	TIME [epoch: 9.73 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138553599725015		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 3.5138553599725015 | validation: 3.045094463009207]
	TIME [epoch: 9.71 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157353446646176		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 3.5157353446646176 | validation: 3.058125557665148]
	TIME [epoch: 9.7 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5129574058722297		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 3.5129574058722297 | validation: 3.050863461896529]
	TIME [epoch: 9.7 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197950373936804		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 3.5197950373936804 | validation: 3.06387730041467]
	TIME [epoch: 9.72 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515510519163866		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 3.515510519163866 | validation: 3.0524657984511614]
	TIME [epoch: 9.71 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518569851772188		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 3.518569851772188 | validation: 3.057278597306955]
	TIME [epoch: 9.7 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183676849396903		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 3.5183676849396903 | validation: 3.0502378765219764]
	TIME [epoch: 9.72 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5127113994571593		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 3.5127113994571593 | validation: 3.0533533064504765]
	TIME [epoch: 9.71 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169630466913695		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 3.5169630466913695 | validation: 3.0653593366460656]
	TIME [epoch: 9.71 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5214577052578626		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 3.5214577052578626 | validation: 3.0602658225057]
	TIME [epoch: 9.72 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515846564560074		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 3.515846564560074 | validation: 3.047445028095324]
	TIME [epoch: 9.72 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512391611710311		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 3.512391611710311 | validation: 3.0505754725123264]
	TIME [epoch: 9.7 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5132760137304437		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 3.5132760137304437 | validation: 3.0632414394888867]
	TIME [epoch: 9.7 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170544838953957		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 3.5170544838953957 | validation: 3.055155434435079]
	TIME [epoch: 9.72 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177822741059344		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 3.5177822741059344 | validation: 3.0588563728187355]
	TIME [epoch: 9.7 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517947131661695		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 3.517947131661695 | validation: 3.047228593941154]
	TIME [epoch: 9.7 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516935552910725		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 3.516935552910725 | validation: 3.0714792244070996]
	TIME [epoch: 9.72 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180575132291843		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 3.5180575132291843 | validation: 3.0452431610048145]
	TIME [epoch: 9.72 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5162605898806056		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 3.5162605898806056 | validation: 3.0496458890194003]
	TIME [epoch: 9.71 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51859478960134		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 3.51859478960134 | validation: 3.056908661648036]
	TIME [epoch: 9.7 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175546360105345		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 3.5175546360105345 | validation: 3.052354990730072]
	TIME [epoch: 9.71 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5127565385805672		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 3.5127565385805672 | validation: 3.0591575992911975]
	TIME [epoch: 9.7 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173306993293907		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 3.5173306993293907 | validation: 3.0552780141053097]
	TIME [epoch: 9.71 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171553947812164		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 3.5171553947812164 | validation: 3.06146657042791]
	TIME [epoch: 9.73 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161655852374323		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 3.5161655852374323 | validation: 3.0503175507553077]
	TIME [epoch: 9.71 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51978236915998		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 3.51978236915998 | validation: 3.0414247448048672]
	TIME [epoch: 9.71 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5143243529036594		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 3.5143243529036594 | validation: 3.0577762894477374]
	TIME [epoch: 9.71 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167351452825244		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 3.5167351452825244 | validation: 3.06171594146134]
	TIME [epoch: 9.72 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167416016645916		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 3.5167416016645916 | validation: 3.0604415266687646]
	TIME [epoch: 9.71 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160414638705917		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 3.5160414638705917 | validation: 3.043709498363924]
	TIME [epoch: 9.7 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5113017463785403		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 3.5113017463785403 | validation: 3.0485958940750018]
	TIME [epoch: 9.73 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513428478160962		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 3.513428478160962 | validation: 3.0594897448970397]
	TIME [epoch: 9.7 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153263221551456		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 3.5153263221551456 | validation: 3.044597529485875]
	TIME [epoch: 9.71 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167430186248843		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 3.5167430186248843 | validation: 3.047659648144817]
	TIME [epoch: 9.72 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167604662350755		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 3.5167604662350755 | validation: 3.053532723261392]
	TIME [epoch: 9.71 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153128342363784		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 3.5153128342363784 | validation: 3.0581586850852944]
	TIME [epoch: 9.71 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514780550201183		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 3.514780550201183 | validation: 3.048128839838489]
	TIME [epoch: 9.73 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190671035488625		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 3.5190671035488625 | validation: 3.056853794002591]
	TIME [epoch: 9.73 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5199987537083146		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 3.5199987537083146 | validation: 3.055745985099727]
	TIME [epoch: 9.72 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517903208003567		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 3.517903208003567 | validation: 3.050354520079252]
	TIME [epoch: 9.72 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175539883227827		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 3.5175539883227827 | validation: 3.0538880117866776]
	TIME [epoch: 9.73 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202386820910156		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 3.5202386820910156 | validation: 3.059567743867711]
	TIME [epoch: 9.72 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518389291725679		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 3.518389291725679 | validation: 3.0455020545705844]
	TIME [epoch: 9.7 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5112229099992716		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 3.5112229099992716 | validation: 3.0449701975917036]
	TIME [epoch: 9.72 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517795822716278		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 3.517795822716278 | validation: 3.051004996510633]
	TIME [epoch: 9.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5159554191716618		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 3.5159554191716618 | validation: 3.04602287205096]
	TIME [epoch: 9.71 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5130895469999914		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 3.5130895469999914 | validation: 3.047205738454143]
	TIME [epoch: 9.72 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5115562582953133		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 3.5115562582953133 | validation: 3.052673403055694]
	TIME [epoch: 9.74 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5178016373990877		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 3.5178016373990877 | validation: 3.0471472699287445]
	TIME [epoch: 9.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5109899969298866		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 3.5109899969298866 | validation: 3.0521784983445945]
	TIME [epoch: 9.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180302964914887		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 3.5180302964914887 | validation: 3.0493401631696133]
	TIME [epoch: 9.73 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150427442323746		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 3.5150427442323746 | validation: 3.050125322596204]
	TIME [epoch: 9.72 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153914836057334		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 3.5153914836057334 | validation: 3.0602752780239726]
	TIME [epoch: 9.71 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516012836469735		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 3.516012836469735 | validation: 3.053271960469045]
	TIME [epoch: 9.72 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170289064780946		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 3.5170289064780946 | validation: 3.042048812227989]
	TIME [epoch: 9.72 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183529701215845		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 3.5183529701215845 | validation: 3.045864870205164]
	TIME [epoch: 9.71 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5133289299515793		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 3.5133289299515793 | validation: 3.0463081187577403]
	TIME [epoch: 9.71 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510951142180983		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 3.510951142180983 | validation: 3.050057992633846]
	TIME [epoch: 9.73 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513713417776087		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 3.513713417776087 | validation: 3.0546913246091636]
	TIME [epoch: 9.71 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150397628874805		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 3.5150397628874805 | validation: 3.0555243661626172]
	TIME [epoch: 9.71 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516370307391906		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 3.516370307391906 | validation: 3.045908535907771]
	TIME [epoch: 9.71 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5116557661698495		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 3.5116557661698495 | validation: 3.052697987868106]
	TIME [epoch: 9.72 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5110621506321875		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 3.5110621506321875 | validation: 3.048677714660035]
	TIME [epoch: 9.71 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514234698768438		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 3.514234698768438 | validation: 3.0477183057862556]
	TIME [epoch: 9.71 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169503176301946		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 3.5169503176301946 | validation: 3.060011770774654]
	TIME [epoch: 9.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5178253374744366		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 3.5178253374744366 | validation: 3.050363691749626]
	TIME [epoch: 9.72 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161124654859996		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 3.5161124654859996 | validation: 3.042773263364736]
	TIME [epoch: 9.71 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173240613752172		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 3.5173240613752172 | validation: 3.049799217747022]
	TIME [epoch: 9.73 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163204064028646		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 3.5163204064028646 | validation: 3.052976309116853]
	TIME [epoch: 9.72 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518340400394642		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 3.518340400394642 | validation: 3.056085690645382]
	TIME [epoch: 9.72 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160571789836785		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 3.5160571789836785 | validation: 3.061468662566865]
	TIME [epoch: 9.71 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5216599000048774		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 3.5216599000048774 | validation: 3.055138985674651]
	TIME [epoch: 9.72 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514313596757984		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 3.514313596757984 | validation: 3.040845067279097]
	TIME [epoch: 9.71 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5191872425595156		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 3.5191872425595156 | validation: 3.059496733889434]
	TIME [epoch: 9.7 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516929203974044		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 3.516929203974044 | validation: 3.0610162718546405]
	TIME [epoch: 9.71 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517108937348206		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 3.517108937348206 | validation: 3.0440925431994077]
	TIME [epoch: 9.7 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152300703043435		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 3.5152300703043435 | validation: 3.0485092376335206]
	TIME [epoch: 9.72 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5116154367399446		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 3.5116154367399446 | validation: 3.0416190691438945]
	TIME [epoch: 9.71 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5185588039466786		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 3.5185588039466786 | validation: 3.0498244407110806]
	TIME [epoch: 9.72 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193408277054488		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 3.5193408277054488 | validation: 3.0433641758462286]
	TIME [epoch: 9.7 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513181233826824		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 3.513181233826824 | validation: 3.0496572168850458]
	TIME [epoch: 9.71 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514408179107351		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 3.514408179107351 | validation: 3.053612506013477]
	TIME [epoch: 9.73 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512393572267121		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 3.512393572267121 | validation: 3.0594107529960612]
	TIME [epoch: 9.73 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517765890287251		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 3.517765890287251 | validation: 3.046609171232447]
	TIME [epoch: 9.73 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5136750111671162		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 3.5136750111671162 | validation: 3.058446100800011]
	TIME [epoch: 9.74 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5159430494168036		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 3.5159430494168036 | validation: 3.056015323187854]
	TIME [epoch: 9.71 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5181712139261		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 3.5181712139261 | validation: 3.049246441482311]
	TIME [epoch: 9.73 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157137972867085		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 3.5157137972867085 | validation: 3.0583799198353483]
	TIME [epoch: 9.71 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225009539358276		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 3.5225009539358276 | validation: 3.0500084266374996]
	TIME [epoch: 9.73 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515818172939055		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 3.515818172939055 | validation: 3.057829670501182]
	TIME [epoch: 9.72 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145122272259313		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 3.5145122272259313 | validation: 3.033797143610076]
	TIME [epoch: 9.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_1507.pth
	Model improved!!!
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155653473364965		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 3.5155653473364965 | validation: 3.0405469735720483]
	TIME [epoch: 9.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186690561791023		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 3.5186690561791023 | validation: 3.033875603286221]
	TIME [epoch: 9.71 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5143141094962616		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 3.5143141094962616 | validation: 3.0513962106465304]
	TIME [epoch: 9.7 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161940860074865		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 3.5161940860074865 | validation: 3.059705204533102]
	TIME [epoch: 9.72 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513894744326431		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 3.513894744326431 | validation: 3.051233764953071]
	TIME [epoch: 9.73 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5131469048728987		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 3.5131469048728987 | validation: 3.057130017794256]
	TIME [epoch: 9.71 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51899820715341		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 3.51899820715341 | validation: 3.0442868911941736]
	TIME [epoch: 9.72 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516124543655723		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 3.516124543655723 | validation: 3.049181634698515]
	TIME [epoch: 9.71 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182420640994314		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 3.5182420640994314 | validation: 3.058245790268336]
	TIME [epoch: 9.72 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515887226079356		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 3.515887226079356 | validation: 3.0619539488510323]
	TIME [epoch: 9.71 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513470138323413		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 3.513470138323413 | validation: 3.052539534580865]
	TIME [epoch: 9.71 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154878479417286		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 3.5154878479417286 | validation: 3.057094334526718]
	TIME [epoch: 9.73 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517258364362849		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 3.517258364362849 | validation: 3.0531028180432602]
	TIME [epoch: 9.7 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154177337653727		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 3.5154177337653727 | validation: 3.052224575153602]
	TIME [epoch: 9.7 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177324827946825		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 3.5177324827946825 | validation: 3.0559806953088584]
	TIME [epoch: 9.72 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152520905574582		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 3.5152520905574582 | validation: 3.0492183992200723]
	TIME [epoch: 9.71 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5132681208859156		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 3.5132681208859156 | validation: 3.0611698555227727]
	TIME [epoch: 9.71 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511895955719288		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 3.511895955719288 | validation: 3.0555420051330104]
	TIME [epoch: 9.72 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512962815787097		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 3.512962815787097 | validation: 3.0548063533954464]
	TIME [epoch: 9.72 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511117598689733		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 3.511117598689733 | validation: 3.0427191562316076]
	TIME [epoch: 9.71 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145648627430894		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 3.5145648627430894 | validation: 3.049047948725345]
	TIME [epoch: 9.71 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5208667354359946		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 3.5208667354359946 | validation: 3.0542468818706885]
	TIME [epoch: 9.72 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511722124486289		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 3.511722124486289 | validation: 3.06237069163811]
	TIME [epoch: 9.71 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520504559391695		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 3.520504559391695 | validation: 3.0544439083878707]
	TIME [epoch: 9.71 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135456694702554		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 3.5135456694702554 | validation: 3.0533088380520326]
	TIME [epoch: 9.71 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5097116455403627		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 3.5097116455403627 | validation: 3.067033284290268]
	TIME [epoch: 9.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512974983505238		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 3.512974983505238 | validation: 3.0534812824287845]
	TIME [epoch: 9.7 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5143928813288725		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 3.5143928813288725 | validation: 3.04285847152475]
	TIME [epoch: 9.72 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126738793072945		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 3.5126738793072945 | validation: 3.0594685516330875]
	TIME [epoch: 9.72 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5096446970489525		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 3.5096446970489525 | validation: 3.053195330890034]
	TIME [epoch: 9.71 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51863176685451		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 3.51863176685451 | validation: 3.0506518985350386]
	TIME [epoch: 9.72 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5162833458075426		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 3.5162833458075426 | validation: 3.046663737577509]
	TIME [epoch: 9.72 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515980243110504		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 3.515980243110504 | validation: 3.0559399339716533]
	TIME [epoch: 9.71 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167372296151753		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 3.5167372296151753 | validation: 3.046187520531953]
	TIME [epoch: 9.71 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513222124340281		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 3.513222124340281 | validation: 3.0489781742226705]
	TIME [epoch: 9.71 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513408103914474		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 3.513408103914474 | validation: 3.0501028498966685]
	TIME [epoch: 9.72 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190081936017146		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 3.5190081936017146 | validation: 3.0479562304543357]
	TIME [epoch: 9.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513522697544926		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 3.513522697544926 | validation: 3.057673489890939]
	TIME [epoch: 9.71 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165369415656587		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 3.5165369415656587 | validation: 3.0603200088707]
	TIME [epoch: 9.72 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518223019561158		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 3.518223019561158 | validation: 3.040200363615959]
	TIME [epoch: 9.71 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5143194175798023		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 3.5143194175798023 | validation: 3.0516559809454167]
	TIME [epoch: 9.71 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5100958055537843		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 3.5100958055537843 | validation: 3.055530357110474]
	TIME [epoch: 9.71 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166605269946225		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 3.5166605269946225 | validation: 3.0636301171462366]
	TIME [epoch: 9.72 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512576652022196		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 3.512576652022196 | validation: 3.041531397622946]
	TIME [epoch: 9.71 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5122740448739633		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 3.5122740448739633 | validation: 3.0417285305561803]
	TIME [epoch: 9.72 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515103086583315		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 3.515103086583315 | validation: 3.062106446196941]
	TIME [epoch: 9.73 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51977210337932		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 3.51977210337932 | validation: 3.047265784952273]
	TIME [epoch: 9.71 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516374040127778		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 3.516374040127778 | validation: 3.047293510866637]
	TIME [epoch: 9.72 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515014271874884		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 3.515014271874884 | validation: 3.0557911520414485]
	TIME [epoch: 9.72 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5120310518394255		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 3.5120310518394255 | validation: 3.0449474500104827]
	TIME [epoch: 9.71 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165822545470746		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 3.5165822545470746 | validation: 3.0489648571924]
	TIME [epoch: 9.7 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139190272313954		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 3.5139190272313954 | validation: 3.0451829441789022]
	TIME [epoch: 9.71 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145863821950902		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 3.5145863821950902 | validation: 3.0508973483117474]
	TIME [epoch: 9.73 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183129609980375		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 3.5183129609980375 | validation: 3.051884189421958]
	TIME [epoch: 9.73 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515066676883464		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 3.515066676883464 | validation: 3.050784599688861]
	TIME [epoch: 9.71 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515230733079862		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 3.515230733079862 | validation: 3.0528209361709986]
	TIME [epoch: 9.74 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5118715773682467		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 3.5118715773682467 | validation: 3.0480628808306993]
	TIME [epoch: 9.72 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5117343153217284		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 3.5117343153217284 | validation: 3.0408101598830712]
	TIME [epoch: 9.72 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152211508851665		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 3.5152211508851665 | validation: 3.0600523992084563]
	TIME [epoch: 9.72 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517740780594746		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 3.517740780594746 | validation: 3.0560220114830816]
	TIME [epoch: 9.74 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183180589686467		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 3.5183180589686467 | validation: 3.035592191655768]
	TIME [epoch: 9.72 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5168330620360635		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 3.5168330620360635 | validation: 3.0489960182627125]
	TIME [epoch: 9.72 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5114332589837405		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 3.5114332589837405 | validation: 3.0506820965268897]
	TIME [epoch: 9.73 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142128906274452		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 3.5142128906274452 | validation: 3.0583173065899736]
	TIME [epoch: 9.72 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511357904962948		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 3.511357904962948 | validation: 3.050287450387221]
	TIME [epoch: 9.72 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511287640054031		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 3.511287640054031 | validation: 3.0597033186676703]
	TIME [epoch: 9.73 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182826278223893		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 3.5182826278223893 | validation: 3.047502797729925]
	TIME [epoch: 9.72 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512853132550505		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 3.512853132550505 | validation: 3.0647002803022416]
	TIME [epoch: 9.72 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516979082817019		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 3.516979082817019 | validation: 3.0466797413145983]
	TIME [epoch: 9.72 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5137843405331695		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 3.5137843405331695 | validation: 3.0487780120670753]
	TIME [epoch: 9.73 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511443783564208		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 3.511443783564208 | validation: 3.0488201763783316]
	TIME [epoch: 9.71 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147578506685226		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 3.5147578506685226 | validation: 3.059019591042718]
	TIME [epoch: 9.71 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518454367955332		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 3.518454367955332 | validation: 3.059200494401324]
	TIME [epoch: 9.73 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165965938594077		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 3.5165965938594077 | validation: 3.052690783540701]
	TIME [epoch: 9.71 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520394344277014		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 3.520394344277014 | validation: 3.054123914484609]
	TIME [epoch: 9.71 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515712014128796		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 3.515712014128796 | validation: 3.041571495587823]
	TIME [epoch: 9.72 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151071591209897		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 3.5151071591209897 | validation: 3.053983117602074]
	TIME [epoch: 9.73 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523766170988685		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 3.523766170988685 | validation: 3.0494277065799067]
	TIME [epoch: 9.72 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154435283616943		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 3.5154435283616943 | validation: 3.0458498787351505]
	TIME [epoch: 9.72 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51811292660564		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 3.51811292660564 | validation: 3.047740245318916]
	TIME [epoch: 9.73 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518116463597445		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 3.518116463597445 | validation: 3.052432888470571]
	TIME [epoch: 9.72 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158644135341626		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 3.5158644135341626 | validation: 3.060335857248731]
	TIME [epoch: 9.71 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515907791157461		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 3.515907791157461 | validation: 3.0564348835111184]
	TIME [epoch: 9.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5196809168343988		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 3.5196809168343988 | validation: 3.045409012010612]
	TIME [epoch: 9.73 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148362716107613		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 3.5148362716107613 | validation: 3.0451686978707917]
	TIME [epoch: 9.72 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514471878327683		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 3.514471878327683 | validation: 3.0594974716982803]
	TIME [epoch: 9.72 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515220420126082		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 3.515220420126082 | validation: 3.0517996643399603]
	TIME [epoch: 9.73 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511678509130937		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 3.511678509130937 | validation: 3.053751838055831]
	TIME [epoch: 9.72 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514582813445408		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 3.514582813445408 | validation: 3.046278023068955]
	TIME [epoch: 9.71 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515424185035637		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 3.515424185035637 | validation: 3.0592694972418717]
	TIME [epoch: 9.72 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144531619960753		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 3.5144531619960753 | validation: 3.060556528688826]
	TIME [epoch: 9.72 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150173665017497		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 3.5150173665017497 | validation: 3.054457879006577]
	TIME [epoch: 9.71 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173833534112107		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 3.5173833534112107 | validation: 3.056654915204685]
	TIME [epoch: 9.71 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516130388383298		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 3.516130388383298 | validation: 3.0495900629496413]
	TIME [epoch: 9.72 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515935452516703		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 3.515935452516703 | validation: 3.0632338904205203]
	TIME [epoch: 9.71 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177914533827965		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 3.5177914533827965 | validation: 3.044052354264466]
	TIME [epoch: 9.71 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149675414034647		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 3.5149675414034647 | validation: 3.049798888418061]
	TIME [epoch: 9.73 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149155154426106		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 3.5149155154426106 | validation: 3.0503729805768995]
	TIME [epoch: 9.7 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160174396472827		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 3.5160174396472827 | validation: 3.045428567889145]
	TIME [epoch: 9.71 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520147600553848		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 3.520147600553848 | validation: 3.0453773379256037]
	TIME [epoch: 9.73 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5218254211021547		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 3.5218254211021547 | validation: 3.0478744980913195]
	TIME [epoch: 9.73 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519098458249111		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 3.519098458249111 | validation: 3.049987333930563]
	TIME [epoch: 9.71 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160714767880092		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 3.5160714767880092 | validation: 3.0534083967395054]
	TIME [epoch: 9.72 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514400098316147		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 3.514400098316147 | validation: 3.051294960168042]
	TIME [epoch: 9.74 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173382036103584		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 3.5173382036103584 | validation: 3.0568299777420203]
	TIME [epoch: 9.71 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517158749597006		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 3.517158749597006 | validation: 3.0578572495661236]
	TIME [epoch: 9.72 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511200737907153		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 3.511200737907153 | validation: 3.0446266049313024]
	TIME [epoch: 9.72 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516046789974743		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 3.516046789974743 | validation: 3.0544218361804996]
	TIME [epoch: 9.72 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155836260541777		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 3.5155836260541777 | validation: 3.0534988981040967]
	TIME [epoch: 9.72 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5207849194735026		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 3.5207849194735026 | validation: 3.0584074280455833]
	TIME [epoch: 9.72 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5178582016331283		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 3.5178582016331283 | validation: 3.0536209396611276]
	TIME [epoch: 9.71 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5174341125973165		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 3.5174341125973165 | validation: 3.0498339633421176]
	TIME [epoch: 9.7 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511108300409332		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 3.511108300409332 | validation: 3.051762519668125]
	TIME [epoch: 9.7 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5191077636238446		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 3.5191077636238446 | validation: 3.057814132151675]
	TIME [epoch: 9.72 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511404507546735		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 3.511404507546735 | validation: 3.04307289226634]
	TIME [epoch: 9.72 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165485633402347		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 3.5165485633402347 | validation: 3.0443513721992828]
	TIME [epoch: 9.71 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5117534590961497		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 3.5117534590961497 | validation: 3.048262289850907]
	TIME [epoch: 9.72 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5181352609913326		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 3.5181352609913326 | validation: 3.0517202805961596]
	TIME [epoch: 9.73 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142362956127378		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 3.5142362956127378 | validation: 3.050894978779658]
	TIME [epoch: 9.7 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126590555425743		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 3.5126590555425743 | validation: 3.049825670863669]
	TIME [epoch: 9.71 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512254784686757		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 3.512254784686757 | validation: 3.0434501352516077]
	TIME [epoch: 9.74 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5103688096077237		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 3.5103688096077237 | validation: 3.053250078140186]
	TIME [epoch: 9.72 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51611183343068		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 3.51611183343068 | validation: 3.0565671664297627]
	TIME [epoch: 9.72 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5134692924075113		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 3.5134692924075113 | validation: 3.05566223794515]
	TIME [epoch: 9.72 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5094118991070418		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 3.5094118991070418 | validation: 3.0588552341429716]
	TIME [epoch: 9.71 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518035975448254		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 3.518035975448254 | validation: 3.0445388943824407]
	TIME [epoch: 9.72 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509946960092049		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 3.509946960092049 | validation: 3.052386697381769]
	TIME [epoch: 9.7 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170192828476936		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 3.5170192828476936 | validation: 3.053398654267241]
	TIME [epoch: 9.74 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180878578953987		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 3.5180878578953987 | validation: 3.054479386223189]
	TIME [epoch: 9.71 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511916537892681		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 3.511916537892681 | validation: 3.056023058129491]
	TIME [epoch: 9.72 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51330362688397		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 3.51330362688397 | validation: 3.0610508055086223]
	TIME [epoch: 9.73 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126282078043487		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 3.5126282078043487 | validation: 3.045539054284625]
	TIME [epoch: 9.72 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148834771961504		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 3.5148834771961504 | validation: 3.051546934908113]
	TIME [epoch: 9.72 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513840230290936		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 3.513840230290936 | validation: 3.0405623802417683]
	TIME [epoch: 9.71 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167341241640164		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 3.5167341241640164 | validation: 3.0541528909237785]
	TIME [epoch: 9.72 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147696273581324		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 3.5147696273581324 | validation: 3.047059805987183]
	TIME [epoch: 9.71 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515072899636793		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 3.515072899636793 | validation: 3.050163609489364]
	TIME [epoch: 9.7 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514672336287056		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 3.514672336287056 | validation: 3.050311676132502]
	TIME [epoch: 9.72 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195445721632077		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 3.5195445721632077 | validation: 3.046766666562752]
	TIME [epoch: 9.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516607035495808		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 3.516607035495808 | validation: 3.0527493870608384]
	TIME [epoch: 9.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520160071306408		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 3.520160071306408 | validation: 3.0548323307400427]
	TIME [epoch: 9.72 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514005800738051		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 3.514005800738051 | validation: 3.0706630654995664]
	TIME [epoch: 9.72 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151983265230946		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 3.5151983265230946 | validation: 3.040512669278854]
	TIME [epoch: 9.71 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512653223311902		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 3.512653223311902 | validation: 3.0593997153818497]
	TIME [epoch: 9.71 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176854725370434		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 3.5176854725370434 | validation: 3.045246966367993]
	TIME [epoch: 9.72 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165017057857297		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 3.5165017057857297 | validation: 3.0565377448393525]
	TIME [epoch: 9.7 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507859043336553		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 3.507859043336553 | validation: 3.0491256126560695]
	TIME [epoch: 9.71 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5108983381449574		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 3.5108983381449574 | validation: 3.04904166304973]
	TIME [epoch: 9.73 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516201947992157		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 3.516201947992157 | validation: 3.044086460207048]
	TIME [epoch: 9.7 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5189143094879283		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 3.5189143094879283 | validation: 3.065517827610894]
	TIME [epoch: 9.72 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5156492810829194		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 3.5156492810829194 | validation: 3.0637517339471243]
	TIME [epoch: 9.72 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515088177440732		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 3.515088177440732 | validation: 3.0462021944955997]
	TIME [epoch: 9.74 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513734111678356		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 3.513734111678356 | validation: 3.043362565995924]
	TIME [epoch: 9.7 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5208098500759517		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 3.5208098500759517 | validation: 3.0504271140969275]
	TIME [epoch: 9.7 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510402373241109		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 3.510402373241109 | validation: 3.0479829989933007]
	TIME [epoch: 9.73 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514249330903641		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 3.514249330903641 | validation: 3.0515262570617927]
	TIME [epoch: 9.71 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511769362459293		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 3.511769362459293 | validation: 3.053620428209837]
	TIME [epoch: 9.7 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516133837198786		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 3.516133837198786 | validation: 3.044149092714263]
	TIME [epoch: 9.71 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170361684482345		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 3.5170361684482345 | validation: 3.0571309743875283]
	TIME [epoch: 9.72 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516557641089533		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 3.516557641089533 | validation: 3.052468880732665]
	TIME [epoch: 9.71 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514260855933693		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 3.514260855933693 | validation: 3.0579815032160393]
	TIME [epoch: 9.71 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513986571173126		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 3.513986571173126 | validation: 3.0533813519692985]
	TIME [epoch: 9.72 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5136152827079457		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 3.5136152827079457 | validation: 3.0608412094884563]
	TIME [epoch: 9.71 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5120291694897454		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 3.5120291694897454 | validation: 3.0571654730612345]
	TIME [epoch: 9.7 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51446350086907		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 3.51446350086907 | validation: 3.0606163376485855]
	TIME [epoch: 9.72 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5179680614267945		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 3.5179680614267945 | validation: 3.052072137059356]
	TIME [epoch: 9.7 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516945745603787		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 3.516945745603787 | validation: 3.04897874700487]
	TIME [epoch: 9.71 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5189116840925343		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 3.5189116840925343 | validation: 3.052166971417133]
	TIME [epoch: 9.72 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510194196076447		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 3.510194196076447 | validation: 3.0499214022187573]
	TIME [epoch: 9.74 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161002600617883		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 3.5161002600617883 | validation: 3.0644131794897578]
	TIME [epoch: 9.72 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153732719810975		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 3.5153732719810975 | validation: 3.0561643753148173]
	TIME [epoch: 9.71 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51097797196351		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 3.51097797196351 | validation: 3.0593472271234807]
	TIME [epoch: 9.73 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5124657768450165		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 3.5124657768450165 | validation: 3.054244045470308]
	TIME [epoch: 9.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5131178015503766		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 3.5131178015503766 | validation: 3.0638797516703473]
	TIME [epoch: 9.72 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516386173741702		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 3.516386173741702 | validation: 3.0534632637545793]
	TIME [epoch: 9.71 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176772774250127		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 3.5176772774250127 | validation: 3.0483431399603376]
	TIME [epoch: 9.72 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154477243953943		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 3.5154477243953943 | validation: 3.056857301987177]
	TIME [epoch: 9.71 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511315242769739		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 3.511315242769739 | validation: 3.045702465508798]
	TIME [epoch: 9.71 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5122275950754513		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 3.5122275950754513 | validation: 3.0497774726563773]
	TIME [epoch: 9.73 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508161990480333		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 3.508161990480333 | validation: 3.0424033631813234]
	TIME [epoch: 9.71 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509428539976915		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 3.509428539976915 | validation: 3.0630164949800003]
	TIME [epoch: 9.71 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163924294713107		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 3.5163924294713107 | validation: 3.0467877588805283]
	TIME [epoch: 9.72 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515540943222985		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 3.515540943222985 | validation: 3.0439435056371913]
	TIME [epoch: 9.71 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5129731586886335		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 3.5129731586886335 | validation: 3.04401563934016]
	TIME [epoch: 9.71 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51338784808094		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 3.51338784808094 | validation: 3.044863204127905]
	TIME [epoch: 9.71 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520809361500233		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 3.520809361500233 | validation: 3.0564836226578347]
	TIME [epoch: 9.72 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512615252313825		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 3.512615252313825 | validation: 3.0384218630318536]
	TIME [epoch: 9.71 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5112601174359		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 3.5112601174359 | validation: 3.0539926680343403]
	TIME [epoch: 9.72 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512296716628864		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 3.512296716628864 | validation: 3.0647029110019286]
	TIME [epoch: 9.73 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521401849009655		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 3.521401849009655 | validation: 3.0498912285884128]
	TIME [epoch: 9.71 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5196386913894813		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 3.5196386913894813 | validation: 3.051162671552053]
	TIME [epoch: 9.71 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5100848864079404		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 3.5100848864079404 | validation: 3.0505645408136868]
	TIME [epoch: 9.71 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172013473743275		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 3.5172013473743275 | validation: 3.037634947827821]
	TIME [epoch: 9.73 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163463091332057		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 3.5163463091332057 | validation: 3.0566669745308355]
	TIME [epoch: 9.7 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515506893354736		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 3.515506893354736 | validation: 3.045644163582368]
	TIME [epoch: 9.7 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150898734592686		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 3.5150898734592686 | validation: 3.0542167450076194]
	TIME [epoch: 9.73 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5146891176776087		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 3.5146891176776087 | validation: 3.0499900911543434]
	TIME [epoch: 9.71 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516669470847206		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 3.516669470847206 | validation: 3.051259733883187]
	TIME [epoch: 9.71 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144823526841362		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 3.5144823526841362 | validation: 3.0612817933225402]
	TIME [epoch: 9.72 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515527217554292		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 3.515527217554292 | validation: 3.0399534824742886]
	TIME [epoch: 9.71 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519162698156356		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 3.519162698156356 | validation: 3.0489594964177273]
	TIME [epoch: 9.71 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183114238056183		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 3.5183114238056183 | validation: 3.049790782021347]
	TIME [epoch: 9.71 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517673077081595		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 3.517673077081595 | validation: 3.0514991419131823]
	TIME [epoch: 9.73 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513588965940097		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 3.513588965940097 | validation: 3.0553024954623185]
	TIME [epoch: 9.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515226603476175		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 3.515226603476175 | validation: 3.047189970772529]
	TIME [epoch: 9.71 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195886367376317		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 3.5195886367376317 | validation: 3.0587970100009354]
	TIME [epoch: 9.73 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186534121687627		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 3.5186534121687627 | validation: 3.0512182239665515]
	TIME [epoch: 9.71 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155627549254787		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 3.5155627549254787 | validation: 3.0518030034355754]
	TIME [epoch: 9.71 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5189374154272315		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 3.5189374154272315 | validation: 3.040601505018925]
	TIME [epoch: 9.72 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148032643725715		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 3.5148032643725715 | validation: 3.0401989786697885]
	TIME [epoch: 9.73 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514215506370389		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 3.514215506370389 | validation: 3.0472043433916625]
	TIME [epoch: 9.71 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511515747185966		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 3.511515747185966 | validation: 3.0466699083291657]
	TIME [epoch: 9.7 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5212802086619197		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 3.5212802086619197 | validation: 3.044319919806747]
	TIME [epoch: 9.71 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154565929438326		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 3.5154565929438326 | validation: 3.0363555522122923]
	TIME [epoch: 9.72 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513871849756241		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 3.513871849756241 | validation: 3.056330875122185]
	TIME [epoch: 9.7 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518673107454508		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 3.518673107454508 | validation: 3.043789510509707]
	TIME [epoch: 9.73 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175704271757775		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 3.5175704271757775 | validation: 3.0479535646310194]
	TIME [epoch: 9.72 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513752722163365		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 3.513752722163365 | validation: 3.0555728265399944]
	TIME [epoch: 9.72 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5159783995057685		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 3.5159783995057685 | validation: 3.0503621168681114]
	TIME [epoch: 9.72 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5090061999979563		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 3.5090061999979563 | validation: 3.027743236132551]
	TIME [epoch: 9.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r0_20240219_233648/states/model_tr_study206_1727.pth
	Model improved!!!
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5116014823325243		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 3.5116014823325243 | validation: 3.047937751133397]
	TIME [epoch: 9.72 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515769630510828		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 3.515769630510828 | validation: 3.0455426969610233]
	TIME [epoch: 9.72 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170805034874264		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 3.5170805034874264 | validation: 3.0697894323105563]
	TIME [epoch: 9.74 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517798295065424		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 3.517798295065424 | validation: 3.056732449440904]
	TIME [epoch: 9.73 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515054836772692		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 3.515054836772692 | validation: 3.042937033069744]
	TIME [epoch: 9.7 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186032450123106		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 3.5186032450123106 | validation: 3.044117233600106]
	TIME [epoch: 9.71 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514596732529737		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 3.514596732529737 | validation: 3.0435288985733453]
	TIME [epoch: 9.72 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519722934838992		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 3.519722934838992 | validation: 3.0610778316211564]
	TIME [epoch: 9.72 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508428002825295		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 3.508428002825295 | validation: 3.0515102258763807]
	TIME [epoch: 9.72 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512550501413446		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 3.512550501413446 | validation: 3.0560711907965947]
	TIME [epoch: 9.73 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515657444943467		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 3.515657444943467 | validation: 3.050777401974985]
	TIME [epoch: 9.72 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521144078410292		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 3.521144078410292 | validation: 3.0520412212305805]
	TIME [epoch: 9.71 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511780350240516		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 3.511780350240516 | validation: 3.0539073420743037]
	TIME [epoch: 9.7 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510076363501728		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 3.510076363501728 | validation: 3.0445222851260314]
	TIME [epoch: 9.74 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513579665181979		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 3.513579665181979 | validation: 3.05358868160388]
	TIME [epoch: 9.71 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5141665502449912		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 3.5141665502449912 | validation: 3.0621035529055876]
	TIME [epoch: 9.71 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515532256887571		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 3.515532256887571 | validation: 3.0544842212152865]
	TIME [epoch: 9.73 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165889062912425		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 3.5165889062912425 | validation: 3.0524771769380052]
	TIME [epoch: 9.73 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154632164825346		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 3.5154632164825346 | validation: 3.049022198684834]
	TIME [epoch: 9.71 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5073125431014853		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 3.5073125431014853 | validation: 3.0504446750297842]
	TIME [epoch: 9.71 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142295204887426		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 3.5142295204887426 | validation: 3.0488738561967152]
	TIME [epoch: 9.72 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202618999900492		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 3.5202618999900492 | validation: 3.0465500518847985]
	TIME [epoch: 9.72 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514397489629789		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 3.514397489629789 | validation: 3.0568238040524696]
	TIME [epoch: 9.7 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135158749382414		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 3.5135158749382414 | validation: 3.0569437371360526]
	TIME [epoch: 9.72 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151684194512223		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 3.5151684194512223 | validation: 3.038402929342408]
	TIME [epoch: 9.72 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509588555888082		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 3.509588555888082 | validation: 3.050741013567491]
	TIME [epoch: 9.72 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5185367594071075		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 3.5185367594071075 | validation: 3.0525183956261777]
	TIME [epoch: 9.74 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154703810829586		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 3.5154703810829586 | validation: 3.0590545153331803]
	TIME [epoch: 9.72 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516937707056239		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 3.516937707056239 | validation: 3.062171831970171]
	TIME [epoch: 9.71 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135647245665487		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 3.5135647245665487 | validation: 3.0361999246241824]
	TIME [epoch: 9.7 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5128419593791462		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 3.5128419593791462 | validation: 3.056052636460462]
	TIME [epoch: 9.72 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510496854698979		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 3.510496854698979 | validation: 3.048253242778144]
	TIME [epoch: 9.71 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5079091516367753		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 3.5079091516367753 | validation: 3.044562243592149]
	TIME [epoch: 9.7 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160495344813834		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 3.5160495344813834 | validation: 3.0528734648457436]
	TIME [epoch: 9.73 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5173073075716474		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 3.5173073075716474 | validation: 3.05886375574755]
	TIME [epoch: 9.71 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139113573764726		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 3.5139113573764726 | validation: 3.0535157441993177]
	TIME [epoch: 9.71 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193545469209253		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 3.5193545469209253 | validation: 3.0592539658174545]
	TIME [epoch: 9.72 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5083345531208563		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 3.5083345531208563 | validation: 3.0461091827789892]
	TIME [epoch: 9.73 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5209222264586493		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 3.5209222264586493 | validation: 3.059065322887448]
	TIME [epoch: 9.71 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5192038125542178		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 3.5192038125542178 | validation: 3.0432389058998788]
	TIME [epoch: 9.72 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5119625688922413		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 3.5119625688922413 | validation: 3.058643670932208]
	TIME [epoch: 9.73 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158324356272823		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 3.5158324356272823 | validation: 3.0453088662658514]
	TIME [epoch: 9.72 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513666462407		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 3.513666462407 | validation: 3.0554389446469505]
	TIME [epoch: 9.72 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5133681819086133		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 3.5133681819086133 | validation: 3.0486462711770526]
	TIME [epoch: 9.72 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126999411818938		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 3.5126999411818938 | validation: 3.055283058952814]
	TIME [epoch: 9.72 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5184447187840107		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 3.5184447187840107 | validation: 3.0492433351018065]
	TIME [epoch: 9.71 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5162758952555		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 3.5162758952555 | validation: 3.055668593288368]
	TIME [epoch: 9.71 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517962620212445		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 3.517962620212445 | validation: 3.0532441471880065]
	TIME [epoch: 9.73 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188204941474472		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 3.5188204941474472 | validation: 3.060951003566547]
	TIME [epoch: 9.7 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5124781986632945		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 3.5124781986632945 | validation: 3.050738795928073]
	TIME [epoch: 9.71 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5110927160639966		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 3.5110927160639966 | validation: 3.0411759071013806]
	TIME [epoch: 9.73 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5132429884040723		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 3.5132429884040723 | validation: 3.0447588187178347]
	TIME [epoch: 9.7 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5131815558454718		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 3.5131815558454718 | validation: 3.0379509841614523]
	TIME [epoch: 9.71 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511948020047993		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 3.511948020047993 | validation: 3.053080895711005]
	TIME [epoch: 9.71 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5113906835543873		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 3.5113906835543873 | validation: 3.0458008482654466]
	TIME [epoch: 9.73 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517147470057938		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 3.517147470057938 | validation: 3.0488983241704104]
	TIME [epoch: 9.71 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5178371767042362		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 3.5178371767042362 | validation: 3.050884796337648]
	TIME [epoch: 9.72 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195054753500346		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 3.5195054753500346 | validation: 3.0393749356185245]
	TIME [epoch: 9.73 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5134756928883633		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 3.5134756928883633 | validation: 3.053629925897799]
	TIME [epoch: 9.73 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155691748845705		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 3.5155691748845705 | validation: 3.046164610347298]
	TIME [epoch: 9.71 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5164696027322777		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 3.5164696027322777 | validation: 3.0519915851825377]
	TIME [epoch: 9.72 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135907455981985		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 3.5135907455981985 | validation: 3.049324810032697]
	TIME [epoch: 9.74 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5114930181225503		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 3.5114930181225503 | validation: 3.047864700209745]
	TIME [epoch: 9.72 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5132004315247265		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 3.5132004315247265 | validation: 3.0488863162054605]
	TIME [epoch: 9.71 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514837681135333		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 3.514837681135333 | validation: 3.0427108221032007]
	TIME [epoch: 9.74 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520726132389547		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 3.520726132389547 | validation: 3.0532607879166385]
	TIME [epoch: 9.72 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5143419048169955		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 3.5143419048169955 | validation: 3.0513224946401114]
	TIME [epoch: 9.72 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5044979933436293		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 3.5044979933436293 | validation: 3.0602237134149717]
	TIME [epoch: 9.72 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5123054845194694		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 3.5123054845194694 | validation: 3.055619047532068]
	TIME [epoch: 9.72 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167548732713714		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 3.5167548732713714 | validation: 3.059926165441682]
	TIME [epoch: 9.71 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518607726051259		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 3.518607726051259 | validation: 3.0558415834533608]
	TIME [epoch: 9.73 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5131951189017903		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 3.5131951189017903 | validation: 3.0433219977331114]
	TIME [epoch: 9.73 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5192900989281988		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 3.5192900989281988 | validation: 3.0491709122903417]
	TIME [epoch: 9.71 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510731562431301		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 3.510731562431301 | validation: 3.051089134165753]
	TIME [epoch: 9.71 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513206757636348		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 3.513206757636348 | validation: 3.0456116984048287]
	TIME [epoch: 9.73 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511829672126896		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 3.511829672126896 | validation: 3.046860593176163]
	TIME [epoch: 9.72 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155625604782466		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 3.5155625604782466 | validation: 3.0575468929152465]
	TIME [epoch: 9.71 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161276958772034		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 3.5161276958772034 | validation: 3.051108428829631]
	TIME [epoch: 9.72 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513775447659134		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 3.513775447659134 | validation: 3.0557420539487925]
	TIME [epoch: 9.73 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155099465923576		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 3.5155099465923576 | validation: 3.051695261389654]
	TIME [epoch: 9.71 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511250254755897		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 3.511250254755897 | validation: 3.0518377492634317]
	TIME [epoch: 9.71 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5129501530581573		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 3.5129501530581573 | validation: 3.0517006621300475]
	TIME [epoch: 9.72 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148089944024385		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 3.5148089944024385 | validation: 3.051713795008891]
	TIME [epoch: 9.71 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51655856073672		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 3.51655856073672 | validation: 3.0575246680295427]
	TIME [epoch: 9.7 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516076272861052		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 3.516076272861052 | validation: 3.0542408715016234]
	TIME [epoch: 9.73 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51317454419263		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 3.51317454419263 | validation: 3.041193467725699]
	TIME [epoch: 9.71 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144286052884963		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 3.5144286052884963 | validation: 3.056138123583671]
	TIME [epoch: 9.72 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512983138186789		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 3.512983138186789 | validation: 3.0506508834676183]
	TIME [epoch: 9.71 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512095052845629		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 3.512095052845629 | validation: 3.0545592354240223]
	TIME [epoch: 9.73 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515054630978766		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 3.515054630978766 | validation: 3.063121619549245]
	TIME [epoch: 9.71 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515443982058868		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 3.515443982058868 | validation: 3.0359371807686535]
	TIME [epoch: 9.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512833576232071		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 3.512833576232071 | validation: 3.04962127316729]
	TIME [epoch: 9.72 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515858852161819		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 3.515858852161819 | validation: 3.057312501383151]
	TIME [epoch: 9.71 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160250187609954		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 3.5160250187609954 | validation: 3.042430111451905]
	TIME [epoch: 9.7 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516403708158631		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 3.516403708158631 | validation: 3.0474677560490306]
	TIME [epoch: 9.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513119121481904		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 3.513119121481904 | validation: 3.041078385745014]
	TIME [epoch: 9.72 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513982749584104		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 3.513982749584104 | validation: 3.0520896622825853]
	TIME [epoch: 9.71 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171002246098815		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 3.5171002246098815 | validation: 3.0507499709201564]
	TIME [epoch: 9.7 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513360111511576		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 3.513360111511576 | validation: 3.0571595238765434]
	TIME [epoch: 9.73 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5128876628904777		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 3.5128876628904777 | validation: 3.0543613901656204]
	TIME [epoch: 9.72 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5137082103166044		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 3.5137082103166044 | validation: 3.054065045159216]
	TIME [epoch: 9.71 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5119938282360237		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 3.5119938282360237 | validation: 3.0470688925583183]
	TIME [epoch: 9.72 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5140786659276473		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 3.5140786659276473 | validation: 3.0489993020171324]
	TIME [epoch: 9.71 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509598952188811		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 3.509598952188811 | validation: 3.0492970696743056]
	TIME [epoch: 9.71 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519415509529393		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 3.519415509529393 | validation: 3.0460829052529164]
	TIME [epoch: 9.71 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516280754782847		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 3.516280754782847 | validation: 3.058835626587406]
	TIME [epoch: 9.73 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5164559129799136		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 3.5164559129799136 | validation: 3.058382659456329]
	TIME [epoch: 9.71 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5127067395600635		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 3.5127067395600635 | validation: 3.0490459871954068]
	TIME [epoch: 9.72 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158170952774084		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 3.5158170952774084 | validation: 3.0440420084461266]
	TIME [epoch: 9.74 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5122482725767497		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 3.5122482725767497 | validation: 3.0471977868564015]
	TIME [epoch: 9.71 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5093154774968403		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 3.5093154774968403 | validation: 3.0529346514759284]
	TIME [epoch: 9.72 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139851563251225		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 3.5139851563251225 | validation: 3.049399426424327]
	TIME [epoch: 9.72 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5193997225489975		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 3.5193997225489975 | validation: 3.05202667675794]
	TIME [epoch: 9.73 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5219570586986797		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 3.5219570586986797 | validation: 3.0469288939762045]
	TIME [epoch: 9.71 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518817954541015		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 3.518817954541015 | validation: 3.0494880312216153]
	TIME [epoch: 9.73 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51828189988224		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 3.51828189988224 | validation: 3.0487924748066884]
	TIME [epoch: 9.74 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515844367951614		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 3.515844367951614 | validation: 3.058083364248471]
	TIME [epoch: 9.72 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138502548899013		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 3.5138502548899013 | validation: 3.0463992722826747]
	TIME [epoch: 9.71 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5143445076061774		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 3.5143445076061774 | validation: 3.047622559182421]
	TIME [epoch: 9.74 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513265244111763		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 3.513265244111763 | validation: 3.054721213729454]
	TIME [epoch: 9.72 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5110256042826817		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 3.5110256042826817 | validation: 3.0435271511529334]
	TIME [epoch: 9.72 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510440063680548		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 3.510440063680548 | validation: 3.057884506203925]
	TIME [epoch: 9.71 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150236554901055		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 3.5150236554901055 | validation: 3.057318930559958]
	TIME [epoch: 9.74 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515922744555229		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 3.515922744555229 | validation: 3.063666055876498]
	TIME [epoch: 9.71 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518487126853606		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 3.518487126853606 | validation: 3.0508374692911775]
	TIME [epoch: 9.73 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5125050892144927		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 3.5125050892144927 | validation: 3.05038085804526]
	TIME [epoch: 9.73 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515961450298465		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 3.515961450298465 | validation: 3.0583357928140504]
	TIME [epoch: 9.72 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145125085128432		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 3.5145125085128432 | validation: 3.031557656951126]
	TIME [epoch: 9.72 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517124434783851		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 3.517124434783851 | validation: 3.050195627314853]
	TIME [epoch: 9.73 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514536149730008		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 3.514536149730008 | validation: 3.060537905521134]
	TIME [epoch: 9.73 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5117723456502246		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 3.5117723456502246 | validation: 3.050540399241104]
	TIME [epoch: 9.72 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5097284527040897		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 3.5097284527040897 | validation: 3.050568040440586]
	TIME [epoch: 9.71 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512759379987812		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 3.512759379987812 | validation: 3.0465811704961765]
	TIME [epoch: 9.73 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5118012287822467		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 3.5118012287822467 | validation: 3.039545120427597]
	TIME [epoch: 9.72 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518765468327684		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 3.518765468327684 | validation: 3.047906607798728]
	TIME [epoch: 9.72 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519449422263957		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 3.519449422263957 | validation: 3.0535302607929453]
	TIME [epoch: 9.72 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5134845501313365		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 3.5134845501313365 | validation: 3.056236019119098]
	TIME [epoch: 9.71 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157432396705475		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 3.5157432396705475 | validation: 3.0481211838586537]
	TIME [epoch: 9.71 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515504325739485		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 3.515504325739485 | validation: 3.052270375562605]
	TIME [epoch: 9.72 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148705238156523		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 3.5148705238156523 | validation: 3.0551043045497726]
	TIME [epoch: 9.73 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5068948300846166		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 3.5068948300846166 | validation: 3.0535745434738693]
	TIME [epoch: 9.72 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153044415809154		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 3.5153044415809154 | validation: 3.045884744306254]
	TIME [epoch: 9.71 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5127765082181837		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 3.5127765082181837 | validation: 3.0552441976188147]
	TIME [epoch: 9.73 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512219099492402		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 3.512219099492402 | validation: 3.0458951451299403]
	TIME [epoch: 9.72 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51140158343878		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 3.51140158343878 | validation: 3.056850109053612]
	TIME [epoch: 9.71 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165651355295657		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 3.5165651355295657 | validation: 3.0489723639248267]
	TIME [epoch: 9.71 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176416824736947		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 3.5176416824736947 | validation: 3.0508958629819984]
	TIME [epoch: 9.73 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515000586224801		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 3.515000586224801 | validation: 3.0454108635971546]
	TIME [epoch: 9.72 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508046850247704		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 3.508046850247704 | validation: 3.045554628028765]
	TIME [epoch: 9.72 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514555459977731		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 3.514555459977731 | validation: 3.0519372058464524]
	TIME [epoch: 9.74 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510043932682642		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 3.510043932682642 | validation: 3.045016461378757]
	TIME [epoch: 9.72 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154487676683166		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 3.5154487676683166 | validation: 3.056675430265375]
	TIME [epoch: 9.72 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5180836828551256		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 3.5180836828551256 | validation: 3.0575457031029747]
	TIME [epoch: 9.74 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147471703425923		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 3.5147471703425923 | validation: 3.053819121825165]
	TIME [epoch: 9.72 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163596865152313		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 3.5163596865152313 | validation: 3.0585596205525234]
	TIME [epoch: 9.72 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157625901824074		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 3.5157625901824074 | validation: 3.0457616811863604]
	TIME [epoch: 9.72 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514162026860073		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 3.514162026860073 | validation: 3.0544784803289677]
	TIME [epoch: 9.74 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5162634736037353		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 3.5162634736037353 | validation: 3.0453948782960665]
	TIME [epoch: 9.71 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150522364035686		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 3.5150522364035686 | validation: 3.0560401000618427]
	TIME [epoch: 9.73 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175140179097255		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 3.5175140179097255 | validation: 3.055423910879203]
	TIME [epoch: 9.73 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516282380366414		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 3.516282380366414 | validation: 3.0600858359150784]
	TIME [epoch: 9.71 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5146915672384678		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 3.5146915672384678 | validation: 3.0402421249418343]
	TIME [epoch: 9.71 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152094305292065		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 3.5152094305292065 | validation: 3.0610634007465998]
	TIME [epoch: 9.73 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135839403551166		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 3.5135839403551166 | validation: 3.043525320636486]
	TIME [epoch: 9.71 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182875193100656		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 3.5182875193100656 | validation: 3.041414141895464]
	TIME [epoch: 9.73 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5176224941552716		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 3.5176224941552716 | validation: 3.0593683619603245]
	TIME [epoch: 9.7 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513966198741042		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 3.513966198741042 | validation: 3.05455412310646]
	TIME [epoch: 9.71 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5219838931286302		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 3.5219838931286302 | validation: 3.0454607882738243]
	TIME [epoch: 9.71 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157538256566063		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 3.5157538256566063 | validation: 3.0554570974645543]
	TIME [epoch: 9.71 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5136460525038884		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 3.5136460525038884 | validation: 3.058645468954727]
	TIME [epoch: 9.72 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5158487922024038		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 3.5158487922024038 | validation: 3.0565462541748345]
	TIME [epoch: 9.7 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148290350631557		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 3.5148290350631557 | validation: 3.0489418721673394]
	TIME [epoch: 9.72 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514608657387459		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 3.514608657387459 | validation: 3.051200507360359]
	TIME [epoch: 9.72 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5134246409824845		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 3.5134246409824845 | validation: 3.0547245743449407]
	TIME [epoch: 9.73 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516866737003501		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 3.516866737003501 | validation: 3.0474475598553625]
	TIME [epoch: 9.73 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5148490925763753		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 3.5148490925763753 | validation: 3.0481088733022688]
	TIME [epoch: 9.71 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519653720681016		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 3.519653720681016 | validation: 3.057159118825042]
	TIME [epoch: 9.74 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5113237521420495		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 3.5113237521420495 | validation: 3.0556038646510695]
	TIME [epoch: 9.72 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515591909618584		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 3.515591909618584 | validation: 3.052812566960065]
	TIME [epoch: 9.71 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147783570261426		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 3.5147783570261426 | validation: 3.0466470904888796]
	TIME [epoch: 9.71 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177418311051802		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 3.5177418311051802 | validation: 3.0479063737996284]
	TIME [epoch: 9.73 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518139389445191		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 3.518139389445191 | validation: 3.0471953551757545]
	TIME [epoch: 9.72 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5183686614954235		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 3.5183686614954235 | validation: 3.04733814542854]
	TIME [epoch: 9.72 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511619353733032		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 3.511619353733032 | validation: 3.0423589988621313]
	TIME [epoch: 9.74 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516122810467177		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 3.516122810467177 | validation: 3.0442508775229964]
	TIME [epoch: 9.72 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5132190751554733		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 3.5132190751554733 | validation: 3.04517650738464]
	TIME [epoch: 9.72 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5159326447880472		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 3.5159326447880472 | validation: 3.036720346136716]
	TIME [epoch: 9.74 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519791012529064		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 3.519791012529064 | validation: 3.050917123901336]
	TIME [epoch: 9.72 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509138356265281		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 3.509138356265281 | validation: 3.050423632188905]
	TIME [epoch: 9.72 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5125374606833275		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 3.5125374606833275 | validation: 3.0465566904053465]
	TIME [epoch: 9.72 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516251624677402		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 3.516251624677402 | validation: 3.0499026723885745]
	TIME [epoch: 9.73 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514536589946111		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 3.514536589946111 | validation: 3.040640872141846]
	TIME [epoch: 9.73 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516987341416595		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 3.516987341416595 | validation: 3.0485275477146208]
	TIME [epoch: 9.73 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5200895771219125		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 3.5200895771219125 | validation: 3.053772674860071]
	TIME [epoch: 9.74 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515510799719537		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 3.515510799719537 | validation: 3.0511187941973135]
	TIME [epoch: 9.72 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511390299655063		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 3.511390299655063 | validation: 3.047680383365193]
	TIME [epoch: 9.72 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5157565288738346		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 3.5157565288738346 | validation: 3.046272189348358]
	TIME [epoch: 9.74 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5159027338958695		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 3.5159027338958695 | validation: 3.0598240414705287]
	TIME [epoch: 9.73 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5146161787713184		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 3.5146161787713184 | validation: 3.045152209163599]
	TIME [epoch: 9.72 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5113888761167416		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 3.5113888761167416 | validation: 3.059953233669087]
	TIME [epoch: 9.73 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511286178908287		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 3.511286178908287 | validation: 3.04727266894967]
	TIME [epoch: 9.73 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507284810704943		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 3.507284810704943 | validation: 3.046414858412429]
	TIME [epoch: 9.71 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5140108893873903		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 3.5140108893873903 | validation: 3.039026604306611]
	TIME [epoch: 9.7 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518568193798095		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 3.518568193798095 | validation: 3.049257088126334]
	TIME [epoch: 9.73 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518206690116062		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 3.518206690116062 | validation: 3.0539799292093073]
	TIME [epoch: 9.72 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5160000001910587		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 3.5160000001910587 | validation: 3.0557230064323857]
	TIME [epoch: 9.72 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169813799404017		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 3.5169813799404017 | validation: 3.050280290039633]
	TIME [epoch: 9.7 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5133529010990188		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 3.5133529010990188 | validation: 3.0488474214494987]
	TIME [epoch: 9.74 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511430850185455		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 3.511430850185455 | validation: 3.0474659518227085]
	TIME [epoch: 9.71 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223536136233675		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 3.5223536136233675 | validation: 3.0594388178395446]
	TIME [epoch: 9.72 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151731957555326		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 3.5151731957555326 | validation: 3.0475151646681016]
	TIME [epoch: 9.73 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511611137987868		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 3.511611137987868 | validation: 3.0603762252579565]
	TIME [epoch: 9.71 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517886829144308		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 3.517886829144308 | validation: 3.0338696284402933]
	TIME [epoch: 9.72 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170179438110503		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 3.5170179438110503 | validation: 3.0509461206914934]
	TIME [epoch: 9.73 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517149007761398		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 3.517149007761398 | validation: 3.045147235868622]
	TIME [epoch: 9.72 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147258853388736		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 3.5147258853388736 | validation: 3.048062820123699]
	TIME [epoch: 9.71 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166094736936997		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 3.5166094736936997 | validation: 3.0609993397579616]
	TIME [epoch: 9.72 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5141533289663074		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 3.5141533289663074 | validation: 3.0619324150416403]
	TIME [epoch: 9.73 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5199244663471996		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 3.5199244663471996 | validation: 3.0603421475040054]
	TIME [epoch: 9.71 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509492292012242		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 3.509492292012242 | validation: 3.054852554716391]
	TIME [epoch: 9.72 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142127903696747		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 3.5142127903696747 | validation: 3.04623814339448]
	TIME [epoch: 9.72 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514035785633239		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 3.514035785633239 | validation: 3.054137171531849]
	TIME [epoch: 9.71 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163129291829742		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 3.5163129291829742 | validation: 3.040390976660285]
	TIME [epoch: 9.71 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5086629177425714		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 3.5086629177425714 | validation: 3.0523841150844397]
	TIME [epoch: 9.73 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512079506221719		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 3.512079506221719 | validation: 3.0415002659805452]
	TIME [epoch: 9.71 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5113130764150284		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 3.5113130764150284 | validation: 3.04780157916171]
	TIME [epoch: 9.72 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517452201615632		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 3.517452201615632 | validation: 3.0401736532965713]
	TIME [epoch: 9.71 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5156302999681563		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 3.5156302999681563 | validation: 3.051070963153669]
	TIME [epoch: 9.73 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512907350415938		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 3.512907350415938 | validation: 3.045188466635159]
	TIME [epoch: 9.72 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515951809516129		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 3.515951809516129 | validation: 3.0482603811312727]
	TIME [epoch: 9.72 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512742755307849		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 3.512742755307849 | validation: 3.0630760873923437]
	TIME [epoch: 9.73 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513834127360096		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 3.513834127360096 | validation: 3.0571755388168786]
	TIME [epoch: 9.71 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169527786404444		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 3.5169527786404444 | validation: 3.0481288591665745]
	TIME [epoch: 9.72 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5131508283482296		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 3.5131508283482296 | validation: 3.0453938735547235]
	TIME [epoch: 9.72 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145077847914754		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 3.5145077847914754 | validation: 3.0493989609072596]
	TIME [epoch: 9.74 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516985648665178		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 3.516985648665178 | validation: 3.0550952586019267]
	TIME [epoch: 9.71 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135546968398943		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 3.5135546968398943 | validation: 3.0504983887028834]
	TIME [epoch: 9.72 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151642198973585		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 3.5151642198973585 | validation: 3.0401215385985814]
	TIME [epoch: 9.73 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151690694080835		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 3.5151690694080835 | validation: 3.039161612691278]
	TIME [epoch: 9.72 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170735633805292		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 3.5170735633805292 | validation: 3.034672874204802]
	TIME [epoch: 9.71 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519695726829274		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 3.519695726829274 | validation: 3.055561954411121]
	TIME [epoch: 9.72 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5123457892869867		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 3.5123457892869867 | validation: 3.0484718617601256]
	TIME [epoch: 9.72 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5186395251659803		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 3.5186395251659803 | validation: 3.052856371365362]
	TIME [epoch: 9.71 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154767566637437		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 3.5154767566637437 | validation: 3.041440272350203]
	TIME [epoch: 9.71 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153922661602808		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 3.5153922661602808 | validation: 3.0640928093122795]
	TIME [epoch: 9.73 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5123992470475174		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 3.5123992470475174 | validation: 3.0459043406933315]
	TIME [epoch: 9.71 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5116960222266336		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 3.5116960222266336 | validation: 3.053846488761818]
	TIME [epoch: 9.71 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5120803302147423		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 3.5120803302147423 | validation: 3.0575792935623793]
	TIME [epoch: 9.73 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516670879922058		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 3.516670879922058 | validation: 3.05913621562117]
	TIME [epoch: 9.72 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5122863155013233		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 3.5122863155013233 | validation: 3.060136041704768]
	TIME [epoch: 9.71 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145815514447163		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 3.5145815514447163 | validation: 3.0445042840773353]
	TIME [epoch: 9.72 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152583730274194		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 3.5152583730274194 | validation: 3.0384312543141196]
	TIME [epoch: 9.73 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516753299814687		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 3.516753299814687 | validation: 3.051316101109818]
	TIME [epoch: 9.72 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513180689544014		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 3.513180689544014 | validation: 3.0519053491348256]
	TIME [epoch: 9.71 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511710520190211		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 3.511710520190211 | validation: 3.0612354414487233]
	TIME [epoch: 9.74 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5133273616618483		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 3.5133273616618483 | validation: 3.050255196535207]
	TIME [epoch: 9.72 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513790175927606		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 3.513790175927606 | validation: 3.0498154489210227]
	TIME [epoch: 9.72 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513120159247891		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 3.513120159247891 | validation: 3.0339633657496403]
	TIME [epoch: 9.73 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172766486463365		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 3.5172766486463365 | validation: 3.0485915817074627]
	TIME [epoch: 9.72 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515783191314825		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 3.515783191314825 | validation: 3.0573488621919664]
	TIME [epoch: 9.72 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516022867142148		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 3.516022867142148 | validation: 3.0513861926298382]
	TIME [epoch: 9.71 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139294821587406		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 3.5139294821587406 | validation: 3.052497711180103]
	TIME [epoch: 9.75 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5139265906853794		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 3.5139265906853794 | validation: 3.0583349170326044]
	TIME [epoch: 9.72 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151373834841793		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 3.5151373834841793 | validation: 3.0460268873990164]
	TIME [epoch: 9.72 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514152798670262		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 3.514152798670262 | validation: 3.0433055221607765]
	TIME [epoch: 9.73 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5150611177377713		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 3.5150611177377713 | validation: 3.0615055921864873]
	TIME [epoch: 9.72 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5104263419395507		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 3.5104263419395507 | validation: 3.038306806729007]
	TIME [epoch: 9.71 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5161250123357406		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 3.5161250123357406 | validation: 3.048505783753433]
	TIME [epoch: 9.72 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517035255529117		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 3.517035255529117 | validation: 3.0499153760345314]
	TIME [epoch: 9.72 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5152194734365394		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 3.5152194734365394 | validation: 3.052078255862861]
	TIME [epoch: 9.7 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513565846725297		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 3.513565846725297 | validation: 3.0533829183515597]
	TIME [epoch: 9.71 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519925484472511		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 3.519925484472511 | validation: 3.0638137159308036]
	TIME [epoch: 9.73 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5211379602378963		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 3.5211379602378963 | validation: 3.052811099883842]
	TIME [epoch: 9.72 sec]
Finished training in 19580.206 seconds.
