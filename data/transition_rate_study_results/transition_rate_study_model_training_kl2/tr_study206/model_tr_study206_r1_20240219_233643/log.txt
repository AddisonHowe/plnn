Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r1', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3491480682

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.108689528579449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.108689528579449 | validation: 10.539788790309222]
	TIME [epoch: 49.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.452831717177279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.452831717177279 | validation: 11.023992531394727]
	TIME [epoch: 10.4 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.387423155087834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.387423155087834 | validation: 9.004999222384615]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.901645476573483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.901645476573483 | validation: 8.14230833289786]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.065590986429454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.065590986429454 | validation: 7.297407183414381]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.064871029978083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.064871029978083 | validation: 6.706418825607772]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.6970298642754695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.6970298642754695 | validation: 6.309621111059339]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.253719613028453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.253719613028453 | validation: 5.746528422971055]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.563851249813008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.563851249813008 | validation: 5.700555168962144]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.498863953057464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.498863953057464 | validation: 5.020350372722183]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.947810066222248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.947810066222248 | validation: 5.806220838802501]
	TIME [epoch: 10.4 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.399857951167395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.399857951167395 | validation: 4.866456844342465]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.980705095042855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.980705095042855 | validation: 4.818427644169341]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.936842802284681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.936842802284681 | validation: 4.750470034920515]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.560991982084323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.560991982084323 | validation: 4.794416228687298]
	TIME [epoch: 10.4 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.997845482349182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.997845482349182 | validation: 6.0098453834257635]
	TIME [epoch: 10.4 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.919947923969187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.919947923969187 | validation: 5.274332017879202]
	TIME [epoch: 10.4 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7985618992965025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7985618992965025 | validation: 4.6104130317461]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.804683452269453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.804683452269453 | validation: 4.645780531265719]
	TIME [epoch: 10.4 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.425909481670745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.425909481670745 | validation: 4.444096619368638]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.624739867912363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.624739867912363 | validation: 4.8820120513169325]
	TIME [epoch: 10.4 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.376741827907754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.376741827907754 | validation: 4.393534782433469]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.592095291946231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.592095291946231 | validation: 4.865479963355839]
	TIME [epoch: 10.4 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.686019791941874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.686019791941874 | validation: 4.436355340194294]
	TIME [epoch: 10.4 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.608708873691873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.608708873691873 | validation: 4.30970788499163]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.353950080651477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.353950080651477 | validation: 4.668093801530751]
	TIME [epoch: 10.4 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.343151993182255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.343151993182255 | validation: 4.551092725507937]
	TIME [epoch: 10.4 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.369624208362682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.369624208362682 | validation: 4.409663673986477]
	TIME [epoch: 10.4 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.439943040082673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.439943040082673 | validation: 4.179225878339759]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.56702436737991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.56702436737991 | validation: 4.218679206884185]
	TIME [epoch: 10.4 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.230021722510648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.230021722510648 | validation: 4.370398950186675]
	TIME [epoch: 10.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.306076136980991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.306076136980991 | validation: 4.104699543701193]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.12316992194644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.12316992194644 | validation: 4.94530723025511]
	TIME [epoch: 10.4 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.242365843445368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.242365843445368 | validation: 4.549090934794346]
	TIME [epoch: 10.4 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.192477806790842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.192477806790842 | validation: 4.273271715897569]
	TIME [epoch: 10.4 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.160480223829106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.160480223829106 | validation: 4.509194045256195]
	TIME [epoch: 10.4 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.068169582238221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.068169582238221 | validation: 4.080033200147965]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8452078623836186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8452078623836186 | validation: 4.054713997539119]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.214025862023403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.214025862023403 | validation: 3.9093813300322893]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.985763202632139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.985763202632139 | validation: 4.497077492653997]
	TIME [epoch: 10.4 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.05702723838534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.05702723838534 | validation: 4.041917164226477]
	TIME [epoch: 10.4 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8636346173814324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8636346173814324 | validation: 4.446586098601569]
	TIME [epoch: 10.4 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.917860656162688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.917860656162688 | validation: 4.068676004217696]
	TIME [epoch: 10.4 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9663877246787145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9663877246787145 | validation: 4.030605682039633]
	TIME [epoch: 10.4 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.935300624875093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.935300624875093 | validation: 4.249068309132963]
	TIME [epoch: 10.4 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8539457224661575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8539457224661575 | validation: 4.345072986124904]
	TIME [epoch: 10.4 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.755475621440142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.755475621440142 | validation: 5.3126640078819705]
	TIME [epoch: 10.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.136400093944029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.136400093944029 | validation: 3.9101551906819316]
	TIME [epoch: 10.4 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7565166298443473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7565166298443473 | validation: 3.7211346803704566]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.819336859681242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819336859681242 | validation: 4.1718814877384]
	TIME [epoch: 10.4 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.961609400306324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.961609400306324 | validation: 3.7091614732775255]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7428852686975347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7428852686975347 | validation: 3.6868915116726173]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.88205392556723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.88205392556723 | validation: 3.9400640259405053]
	TIME [epoch: 10.4 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.819969434481016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819969434481016 | validation: 4.132203391503791]
	TIME [epoch: 10.4 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.783819694058723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.783819694058723 | validation: 4.211080854736923]
	TIME [epoch: 10.4 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7921722805143148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7921722805143148 | validation: 3.9537169571139446]
	TIME [epoch: 10.4 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.67792770901559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.67792770901559 | validation: 3.8406922494439595]
	TIME [epoch: 10.4 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.659334175906943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.659334175906943 | validation: 3.906856397237792]
	TIME [epoch: 10.4 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.793296249400034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.793296249400034 | validation: 3.9776685690841203]
	TIME [epoch: 10.4 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7061845572535574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7061845572535574 | validation: 3.625217120826321]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6666308602519306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6666308602519306 | validation: 3.7260278867606984]
	TIME [epoch: 10.4 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.628830714022141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.628830714022141 | validation: 3.875597335043736]
	TIME [epoch: 10.4 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.710141282260898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.710141282260898 | validation: 3.7548329236150915]
	TIME [epoch: 10.4 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6578494606155694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6578494606155694 | validation: 3.8387922545807096]
	TIME [epoch: 10.4 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.608313651778765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.608313651778765 | validation: 3.662377813093034]
	TIME [epoch: 10.4 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.584371021364247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.584371021364247 | validation: 3.8384263046251825]
	TIME [epoch: 10.4 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8975627787064355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8975627787064355 | validation: 3.9821814945587612]
	TIME [epoch: 10.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662063225388251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.662063225388251 | validation: 3.6506107977294553]
	TIME [epoch: 10.4 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5144236661986796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5144236661986796 | validation: 4.316179565846023]
	TIME [epoch: 10.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7323972892982313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7323972892982313 | validation: 3.6407090015486485]
	TIME [epoch: 10.4 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6405093157710153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6405093157710153 | validation: 3.778973797475308]
	TIME [epoch: 10.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6318032224619516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6318032224619516 | validation: 3.741053857194782]
	TIME [epoch: 10.4 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.412761975150429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.412761975150429 | validation: 3.8785617206290137]
	TIME [epoch: 10.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.687851649092971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.687851649092971 | validation: 3.6987282710312015]
	TIME [epoch: 10.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5197132826138016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5197132826138016 | validation: 4.244141824969882]
	TIME [epoch: 10.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7397599474318817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7397599474318817 | validation: 3.6829899156380344]
	TIME [epoch: 10.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.485347079833006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.485347079833006 | validation: 3.7053838870436504]
	TIME [epoch: 10.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5802582154512104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5802582154512104 | validation: 3.653192605866766]
	TIME [epoch: 10.4 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.62649367829289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.62649367829289 | validation: 3.61432096900767]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527694644179411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.527694644179411 | validation: 3.533576058354803]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4392444518011374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4392444518011374 | validation: 3.876114305941521]
	TIME [epoch: 10.4 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534796716331806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.534796716331806 | validation: 4.587698943849362]
	TIME [epoch: 10.4 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.718175962587164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.718175962587164 | validation: 3.7097276291587447]
	TIME [epoch: 10.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6629270025893086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6629270025893086 | validation: 3.9906296952319695]
	TIME [epoch: 10.4 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615677067782587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.615677067782587 | validation: 3.7841484549633377]
	TIME [epoch: 10.4 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4598866667243335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4598866667243335 | validation: 3.5899549754206705]
	TIME [epoch: 10.4 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5464168689649966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5464168689649966 | validation: 3.7183411152958694]
	TIME [epoch: 10.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952266283900064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4952266283900064 | validation: 3.7041296399049246]
	TIME [epoch: 10.4 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.506151848116896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.506151848116896 | validation: 3.5883912942311484]
	TIME [epoch: 10.4 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.439993199515155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.439993199515155 | validation: 3.695367889865638]
	TIME [epoch: 10.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5301386257458764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5301386257458764 | validation: 3.7209983488587124]
	TIME [epoch: 10.4 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.470311273000868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.470311273000868 | validation: 3.662704411328655]
	TIME [epoch: 10.4 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4708902214429145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4708902214429145 | validation: 3.6417343060064433]
	TIME [epoch: 10.4 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.409692407820984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.409692407820984 | validation: 4.204845692022782]
	TIME [epoch: 10.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504579668627153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.504579668627153 | validation: 3.9071491949924066]
	TIME [epoch: 10.4 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.584958767998681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.584958767998681 | validation: 3.5725904552327195]
	TIME [epoch: 10.4 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126625482387768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5126625482387768 | validation: 3.667805705815444]
	TIME [epoch: 10.4 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.464623257379974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.464623257379974 | validation: 3.5275842472513337]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.359925265414982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359925265414982 | validation: 4.161715128830466]
	TIME [epoch: 10.4 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6156761219614557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6156761219614557 | validation: 3.527733822545693]
	TIME [epoch: 10.4 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.404024373657017		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 3.404024373657017 | validation: 3.667446939171865]
	TIME [epoch: 10.4 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.445826332561005		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 3.445826332561005 | validation: 3.6442618475422695]
	TIME [epoch: 10.4 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.41192160861078		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 3.41192160861078 | validation: 3.580356690579299]
	TIME [epoch: 10.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4416978747439777		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.4416978747439777 | validation: 3.701654887965324]
	TIME [epoch: 10.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4608976088164694		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 3.4608976088164694 | validation: 3.656392560187539]
	TIME [epoch: 10.4 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.448858532979987		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 3.448858532979987 | validation: 3.5769171023696527]
	TIME [epoch: 10.4 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.300673706170577		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.300673706170577 | validation: 3.674001199098799]
	TIME [epoch: 10.4 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.466946181534027		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.466946181534027 | validation: 3.4813189325253786]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4051016114322956		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.4051016114322956 | validation: 3.753598359843313]
	TIME [epoch: 10.4 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.414623636546198		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.414623636546198 | validation: 3.746447352743347]
	TIME [epoch: 10.4 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4175290748558664		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.4175290748558664 | validation: 3.54714348353834]
	TIME [epoch: 10.4 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3505610575999825		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.3505610575999825 | validation: 3.7961567073950038]
	TIME [epoch: 10.4 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.397453445214475		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.397453445214475 | validation: 3.5966061946174945]
	TIME [epoch: 10.4 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.399424441709427		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.399424441709427 | validation: 3.6384757303844246]
	TIME [epoch: 10.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3887370423904732		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.3887370423904732 | validation: 3.572772770357399]
	TIME [epoch: 10.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3261165152469667		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.3261165152469667 | validation: 3.4808905676278994]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3939941171917467		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.3939941171917467 | validation: 3.491744140055772]
	TIME [epoch: 10.4 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.331031976549358		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.331031976549358 | validation: 3.5313011113975525]
	TIME [epoch: 10.4 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.360214396903072		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.360214396903072 | validation: 3.929752644412222]
	TIME [epoch: 10.4 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.426157845703142		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.426157845703142 | validation: 3.6121713481104796]
	TIME [epoch: 10.4 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2985472170860355		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 3.2985472170860355 | validation: 3.5562143285752548]
	TIME [epoch: 10.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3561817164555867		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 3.3561817164555867 | validation: 3.4777955594673076]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2716117603585615		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 3.2716117603585615 | validation: 3.5872430985125563]
	TIME [epoch: 10.4 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.367277603055407		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 3.367277603055407 | validation: 3.472404205945398]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3893215669777668		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 3.3893215669777668 | validation: 3.4660884951418827]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4084321315574684		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 3.4084321315574684 | validation: 3.540845950008768]
	TIME [epoch: 10.4 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3275433107815644		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.3275433107815644 | validation: 3.7440813387882]
	TIME [epoch: 10.4 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.32828992875962		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.32828992875962 | validation: 3.496867890339603]
	TIME [epoch: 10.4 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.29179804395715		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 3.29179804395715 | validation: 3.4736337370314767]
	TIME [epoch: 10.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.302240090309149		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.302240090309149 | validation: 3.505446265126925]
	TIME [epoch: 10.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.309392202309355		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 3.309392202309355 | validation: 3.554423671164426]
	TIME [epoch: 10.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3383603737598655		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 3.3383603737598655 | validation: 3.487283026915587]
	TIME [epoch: 10.4 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.241003840678836		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.241003840678836 | validation: 3.502995975221415]
	TIME [epoch: 10.4 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3204960784547763		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.3204960784547763 | validation: 3.4407847924542865]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2773134092314016		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 3.2773134092314016 | validation: 3.4383480489471845]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.343832519574921		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 3.343832519574921 | validation: 3.405387121902927]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.303638387744287		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.303638387744287 | validation: 3.6535682344367943]
	TIME [epoch: 10.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.312723833320402		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.312723833320402 | validation: 3.4663768644770814]
	TIME [epoch: 10.4 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.343606047311984		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.343606047311984 | validation: 3.6079448529770928]
	TIME [epoch: 10.4 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.312094556649103		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.312094556649103 | validation: 3.593840344649729]
	TIME [epoch: 10.4 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2302232600947844		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.2302232600947844 | validation: 3.478277151855736]
	TIME [epoch: 10.4 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.296434724713239		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.296434724713239 | validation: 3.408738416587294]
	TIME [epoch: 10.4 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.443618751315229		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.443618751315229 | validation: 3.5027069602161998]
	TIME [epoch: 10.4 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.285781620273967		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 3.285781620273967 | validation: 3.4205738449018934]
	TIME [epoch: 10.4 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.174579709174437		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.174579709174437 | validation: 3.428328223170022]
	TIME [epoch: 10.4 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.311537154607996		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.311537154607996 | validation: 3.441917672755953]
	TIME [epoch: 10.4 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4861620179715636		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.4861620179715636 | validation: 3.412858620730331]
	TIME [epoch: 10.4 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.29760321833657		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.29760321833657 | validation: 3.4476243221982634]
	TIME [epoch: 10.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3331668894318818		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.3331668894318818 | validation: 3.4564236026335595]
	TIME [epoch: 10.4 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.202481387513488		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.202481387513488 | validation: 3.562308829734659]
	TIME [epoch: 10.4 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3629958028899933		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.3629958028899933 | validation: 3.6401154287042448]
	TIME [epoch: 10.4 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.323999969950486		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.323999969950486 | validation: 3.5749664617922083]
	TIME [epoch: 10.4 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2422495855702755		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.2422495855702755 | validation: 3.4296976123940013]
	TIME [epoch: 10.4 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2631918443670047		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.2631918443670047 | validation: 3.4045844279092656]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1764671719569835		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.1764671719569835 | validation: 3.6023466495620817]
	TIME [epoch: 10.4 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3374120286514453		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 3.3374120286514453 | validation: 3.632797825271065]
	TIME [epoch: 10.4 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2849792287980613		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.2849792287980613 | validation: 3.475910299944201]
	TIME [epoch: 10.4 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.184187621786279		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 3.184187621786279 | validation: 3.577031157479629]
	TIME [epoch: 10.4 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4739452187444755		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 3.4739452187444755 | validation: 3.7865190482736844]
	TIME [epoch: 10.4 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4437836952764274		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 3.4437836952764274 | validation: 3.449566267063433]
	TIME [epoch: 10.4 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.388783512887669		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.388783512887669 | validation: 3.409364144571149]
	TIME [epoch: 10.4 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2342025812459143		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.2342025812459143 | validation: 3.57395086590446]
	TIME [epoch: 10.4 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.212881038534305		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.212881038534305 | validation: 3.5123732526644087]
	TIME [epoch: 10.4 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2562494606191854		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.2562494606191854 | validation: 3.5798724615679185]
	TIME [epoch: 10.4 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2395708236896525		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 3.2395708236896525 | validation: 3.4890005565673734]
	TIME [epoch: 10.4 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.259918815154795		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 3.259918815154795 | validation: 3.409013084984099]
	TIME [epoch: 10.4 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.466957520330044		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.466957520330044 | validation: 3.631348201819551]
	TIME [epoch: 10.4 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3158898095109364		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 3.3158898095109364 | validation: 3.4649289239604992]
	TIME [epoch: 10.4 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2044554737199094		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 3.2044554737199094 | validation: 3.6382879817065685]
	TIME [epoch: 10.4 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.314016168931932		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 3.314016168931932 | validation: 3.6096424321639002]
	TIME [epoch: 10.4 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.285367744221888		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.285367744221888 | validation: 3.5844598301396218]
	TIME [epoch: 10.4 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.223808789241187		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 3.223808789241187 | validation: 3.4091161155343013]
	TIME [epoch: 10.4 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1714992208391344		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 3.1714992208391344 | validation: 3.6684754783160294]
	TIME [epoch: 10.4 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.260179303434267		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 3.260179303434267 | validation: 3.4043316632765617]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2215724616463817		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 3.2215724616463817 | validation: 3.59869772597718]
	TIME [epoch: 10.4 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2454406102568725		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.2454406102568725 | validation: 3.7599317154279026]
	TIME [epoch: 10.4 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.306821385111346		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 3.306821385111346 | validation: 3.457412993877859]
	TIME [epoch: 10.4 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1798189800235277		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 3.1798189800235277 | validation: 3.4832429751755547]
	TIME [epoch: 10.4 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.18230460454774		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 3.18230460454774 | validation: 3.6462563260839658]
	TIME [epoch: 10.4 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.258180986953721		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 3.258180986953721 | validation: 3.7004212181229916]
	TIME [epoch: 10.4 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2577231756630605		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 3.2577231756630605 | validation: 3.511314986374845]
	TIME [epoch: 10.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1809490882247156		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 3.1809490882247156 | validation: 3.398705085585766]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1798540862794225		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 3.1798540862794225 | validation: 3.344727516558316]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2067054591308546		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 3.2067054591308546 | validation: 3.4054813918888995]
	TIME [epoch: 10.4 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.224534527617812		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 3.224534527617812 | validation: 3.604453828097279]
	TIME [epoch: 10.4 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2527009508138343		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 3.2527009508138343 | validation: 3.578695547762748]
	TIME [epoch: 10.4 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2452112211675237		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.2452112211675237 | validation: 3.4297534990679504]
	TIME [epoch: 10.4 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.218248515198708		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 3.218248515198708 | validation: 3.4405831200909676]
	TIME [epoch: 10.4 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1970328629285447		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 3.1970328629285447 | validation: 3.4346758738430494]
	TIME [epoch: 10.4 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2135779573402017		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 3.2135779573402017 | validation: 3.3538050413028904]
	TIME [epoch: 10.4 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.140809948101628		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 3.140809948101628 | validation: 3.4001968305553363]
	TIME [epoch: 10.4 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.168342029540116		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.168342029540116 | validation: 3.4776392818336994]
	TIME [epoch: 10.4 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2666706374810603		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 3.2666706374810603 | validation: 3.4031241640506336]
	TIME [epoch: 10.4 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1383666754288795		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.1383666754288795 | validation: 3.4263058062069]
	TIME [epoch: 10.4 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1864200094462634		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 3.1864200094462634 | validation: 3.6000084382958972]
	TIME [epoch: 10.4 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2459678780670416		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 3.2459678780670416 | validation: 3.3803473335864895]
	TIME [epoch: 10.4 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2586476850194948		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.2586476850194948 | validation: 3.393172144284473]
	TIME [epoch: 10.4 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1628850195182645		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 3.1628850195182645 | validation: 3.412641315828762]
	TIME [epoch: 10.4 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2483435726293175		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 3.2483435726293175 | validation: 3.441663101066092]
	TIME [epoch: 10.4 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.136305567914769		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 3.136305567914769 | validation: 3.6017244131667576]
	TIME [epoch: 10.4 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2246605553825183		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 3.2246605553825183 | validation: 3.3782891131963373]
	TIME [epoch: 10.4 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.149382798250355		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 3.149382798250355 | validation: 3.3520493769510837]
	TIME [epoch: 10.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.174518280999127		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 3.174518280999127 | validation: 3.3851347487012924]
	TIME [epoch: 10.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2743370327291705		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 3.2743370327291705 | validation: 3.561023657229256]
	TIME [epoch: 10.4 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3874504883818304		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 3.3874504883818304 | validation: 3.4935138500537413]
	TIME [epoch: 10.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.189942086140105		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 3.189942086140105 | validation: 3.372731313312207]
	TIME [epoch: 10.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1473637739207496		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.1473637739207496 | validation: 3.45747989395917]
	TIME [epoch: 10.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3005915279467857		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.3005915279467857 | validation: 3.629789321162544]
	TIME [epoch: 10.4 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1969640017376486		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 3.1969640017376486 | validation: 3.331638000604172]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1256735798830464		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.1256735798830464 | validation: 3.4260069330830762]
	TIME [epoch: 10.4 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.117659406486619		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.117659406486619 | validation: 3.33953070485463]
	TIME [epoch: 10.4 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2001460161052386		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.2001460161052386 | validation: 3.347331529079893]
	TIME [epoch: 10.4 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2769795987621193		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 3.2769795987621193 | validation: 3.333617319842077]
	TIME [epoch: 10.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2243551038557903		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 3.2243551038557903 | validation: 3.3387604837277816]
	TIME [epoch: 10.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1746899046158275		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.1746899046158275 | validation: 3.348607314373753]
	TIME [epoch: 10.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.171352926087505		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.171352926087505 | validation: 3.3461369537229904]
	TIME [epoch: 10.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.197878010436882		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 3.197878010436882 | validation: 3.4407963933404733]
	TIME [epoch: 10.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1922707606223097		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 3.1922707606223097 | validation: 3.3954230107194054]
	TIME [epoch: 10.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1571254840903613		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 3.1571254840903613 | validation: 3.3620614699711373]
	TIME [epoch: 10.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.086479380421134		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.086479380421134 | validation: 3.5379363531599384]
	TIME [epoch: 10.4 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.165750694389432		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 3.165750694389432 | validation: 3.3732720908907976]
	TIME [epoch: 10.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1483412842598275		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.1483412842598275 | validation: 3.388934756894871]
	TIME [epoch: 10.4 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2088738447595824		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 3.2088738447595824 | validation: 3.3887639834802052]
	TIME [epoch: 10.4 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1728365133682983		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 3.1728365133682983 | validation: 3.5392062683202417]
	TIME [epoch: 10.4 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.187368129339374		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 3.187368129339374 | validation: 3.4159765613139372]
	TIME [epoch: 10.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5583760564604305		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.5583760564604305 | validation: 3.4185641728023213]
	TIME [epoch: 10.4 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.174327505728821		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 3.174327505728821 | validation: 3.4036216475053194]
	TIME [epoch: 10.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1466567975881956		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.1466567975881956 | validation: 3.341974403180807]
	TIME [epoch: 10.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.186204859593054		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.186204859593054 | validation: 3.367552883583818]
	TIME [epoch: 10.4 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1827997817434515		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.1827997817434515 | validation: 3.5620376870736767]
	TIME [epoch: 10.4 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.251414395466584		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 3.251414395466584 | validation: 3.3496631453680243]
	TIME [epoch: 10.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1483837998394844		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 3.1483837998394844 | validation: 3.5251813376765244]
	TIME [epoch: 10.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1752692688799273		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 3.1752692688799273 | validation: 3.375829144546117]
	TIME [epoch: 10.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.130540185079899		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 3.130540185079899 | validation: 3.5362225814585475]
	TIME [epoch: 10.4 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.133622499365843		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 3.133622499365843 | validation: 3.973908196952523]
	TIME [epoch: 10.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.336306660788015		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.336306660788015 | validation: 3.4765194075632384]
	TIME [epoch: 10.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1388389212130337		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.1388389212130337 | validation: 3.386674703221015]
	TIME [epoch: 10.4 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.173701193089277		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 3.173701193089277 | validation: 3.45350348160532]
	TIME [epoch: 10.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.159219626606486		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 3.159219626606486 | validation: 3.5788120221403825]
	TIME [epoch: 10.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2089515293413697		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 3.2089515293413697 | validation: 3.4513166639452666]
	TIME [epoch: 10.4 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.114239002851697		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.114239002851697 | validation: 3.3517128516063837]
	TIME [epoch: 10.4 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1642686061086693		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 3.1642686061086693 | validation: 3.4713689008815276]
	TIME [epoch: 10.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1860347660342505		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.1860347660342505 | validation: 3.3434337195973134]
	TIME [epoch: 10.4 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.128026221921009		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 3.128026221921009 | validation: 3.3410803617927223]
	TIME [epoch: 10.4 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1107577629113945		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 3.1107577629113945 | validation: 3.405539005122751]
	TIME [epoch: 10.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1382328509576167		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 3.1382328509576167 | validation: 3.354431208390931]
	TIME [epoch: 10.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1467009605577303		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 3.1467009605577303 | validation: 3.3439824641434166]
	TIME [epoch: 10.4 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1142051003329434		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 3.1142051003329434 | validation: 3.372331598467287]
	TIME [epoch: 10.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1553971512258814		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 3.1553971512258814 | validation: 3.3628218314663525]
	TIME [epoch: 10.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1292966153008415		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 3.1292966153008415 | validation: 3.536901695219898]
	TIME [epoch: 10.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.169205876864288		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 3.169205876864288 | validation: 3.355038092166464]
	TIME [epoch: 10.4 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2177780850627626		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 3.2177780850627626 | validation: 3.361534544795861]
	TIME [epoch: 10.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.146635540376839		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.146635540376839 | validation: 3.713396497588774]
	TIME [epoch: 10.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.203546668153976		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 3.203546668153976 | validation: 3.4878890070338002]
	TIME [epoch: 10.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1270168414190973		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 3.1270168414190973 | validation: 3.3168422943444877]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1846905469215443		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 3.1846905469215443 | validation: 3.3419048104116844]
	TIME [epoch: 10.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0936248366983117		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.0936248366983117 | validation: 3.3902289013113562]
	TIME [epoch: 10.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.146035773204511		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 3.146035773204511 | validation: 3.3303761080016745]
	TIME [epoch: 10.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.16869637833262		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.16869637833262 | validation: 3.4600075402156527]
	TIME [epoch: 10.4 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2053384279698314		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 3.2053384279698314 | validation: 3.3137486709569277]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.126406628678378		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 3.126406628678378 | validation: 3.38201528559444]
	TIME [epoch: 10.4 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.105028465665085		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 3.105028465665085 | validation: 3.3970738682732033]
	TIME [epoch: 10.4 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.135717114199165		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 3.135717114199165 | validation: 3.3656884204430515]
	TIME [epoch: 10.4 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0984910818885574		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.0984910818885574 | validation: 3.3239423957397003]
	TIME [epoch: 10.4 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2939624480827248		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.2939624480827248 | validation: 3.3135641372459403]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1168505501042327		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.1168505501042327 | validation: 3.306292689659415]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.114291338854131		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.114291338854131 | validation: 3.420608374126971]
	TIME [epoch: 10.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.138963259715953		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 3.138963259715953 | validation: 3.4238662607746098]
	TIME [epoch: 10.4 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1199241046249346		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.1199241046249346 | validation: 3.33686117585061]
	TIME [epoch: 10.4 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.078903302851019		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.078903302851019 | validation: 3.4220521952092042]
	TIME [epoch: 10.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.174669235714247		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 3.174669235714247 | validation: 3.3101820340938732]
	TIME [epoch: 10.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1518799662383947		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 3.1518799662383947 | validation: 3.3244108384597615]
	TIME [epoch: 10.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1530781572756226		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.1530781572756226 | validation: 3.483361012173937]
	TIME [epoch: 10.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1606248603991145		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 3.1606248603991145 | validation: 3.4426525447015583]
	TIME [epoch: 10.4 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.174257614256883		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.174257614256883 | validation: 3.383731504437421]
	TIME [epoch: 10.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1293807743281645		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 3.1293807743281645 | validation: 3.3261973476419935]
	TIME [epoch: 10.4 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1660182434166115		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 3.1660182434166115 | validation: 3.33648460345206]
	TIME [epoch: 10.4 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.100208711366615		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 3.100208711366615 | validation: 3.3939663261531905]
	TIME [epoch: 10.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1099248143378175		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 3.1099248143378175 | validation: 3.5510000305036717]
	TIME [epoch: 10.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.164638781765811		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.164638781765811 | validation: 3.3405117529184363]
	TIME [epoch: 10.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1186298226358096		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.1186298226358096 | validation: 3.3309599975048294]
	TIME [epoch: 10.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0854095787555798		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.0854095787555798 | validation: 3.4212326307433507]
	TIME [epoch: 10.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.123315883306181		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.123315883306181 | validation: 3.3678514606034278]
	TIME [epoch: 10.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.199516842251616		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 3.199516842251616 | validation: 3.3846381847627107]
	TIME [epoch: 10.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1057613545496463		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 3.1057613545496463 | validation: 3.375275835004804]
	TIME [epoch: 10.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1255249058804546		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.1255249058804546 | validation: 3.3097951733604383]
	TIME [epoch: 10.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0944022547044256		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 3.0944022547044256 | validation: 3.3272329446497384]
	TIME [epoch: 10.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1350411783354		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.1350411783354 | validation: 3.3254018529963156]
	TIME [epoch: 10.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0818672772615243		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 3.0818672772615243 | validation: 3.3335443071224127]
	TIME [epoch: 10.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1448872763251963		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.1448872763251963 | validation: 3.4189335294496828]
	TIME [epoch: 10.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2934902625234903		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.2934902625234903 | validation: 3.3816768287827497]
	TIME [epoch: 10.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1068554323427002		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 3.1068554323427002 | validation: 3.301950353898968]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0991312852832005		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.0991312852832005 | validation: 3.345315662907806]
	TIME [epoch: 10.4 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.116458114277009		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.116458114277009 | validation: 3.309496862906004]
	TIME [epoch: 10.4 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.09062186513085		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.09062186513085 | validation: 3.315371494705021]
	TIME [epoch: 10.4 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1218863530265017		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.1218863530265017 | validation: 3.2987163124922656]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.077499571986789		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 3.077499571986789 | validation: 3.3053822901616456]
	TIME [epoch: 10.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1187981826347495		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 3.1187981826347495 | validation: 3.4657917587998424]
	TIME [epoch: 10.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0973001993595863		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.0973001993595863 | validation: 3.3518503537583983]
	TIME [epoch: 10.4 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2885361843069654		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.2885361843069654 | validation: 3.740795630501534]
	TIME [epoch: 10.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1797196173622457		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.1797196173622457 | validation: 3.3836024599207937]
	TIME [epoch: 10.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0907238706047453		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.0907238706047453 | validation: 3.288692476186684]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.069369168250906		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 3.069369168250906 | validation: 3.3557129926225606]
	TIME [epoch: 10.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.163686705634464		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 3.163686705634464 | validation: 3.312090812296075]
	TIME [epoch: 10.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.110347338722355		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 3.110347338722355 | validation: 3.342086438725946]
	TIME [epoch: 10.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1275237262530324		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.1275237262530324 | validation: 3.4260586963551116]
	TIME [epoch: 10.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.139696580257548		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 3.139696580257548 | validation: 3.376432881246775]
	TIME [epoch: 10.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0930478321004484		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 3.0930478321004484 | validation: 3.3405328595220203]
	TIME [epoch: 10.4 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.081721168400139		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 3.081721168400139 | validation: 3.3005074524017006]
	TIME [epoch: 10.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.152290044007381		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.152290044007381 | validation: 3.4492913358180473]
	TIME [epoch: 10.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1437477804792233		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 3.1437477804792233 | validation: 3.287632270969307]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.14951726493998		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.14951726493998 | validation: 3.2773607995041023]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.101432221560683		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.101432221560683 | validation: 3.3300291687302104]
	TIME [epoch: 10.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1619426950675034		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.1619426950675034 | validation: 3.402072308471186]
	TIME [epoch: 10.4 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0871874426849644		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.0871874426849644 | validation: 3.4161489834551766]
	TIME [epoch: 10.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1143382275857583		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 3.1143382275857583 | validation: 3.3333484955284267]
	TIME [epoch: 10.4 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1460102789069353		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.1460102789069353 | validation: 3.541410756258938]
	TIME [epoch: 10.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.142320573129305		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.142320573129305 | validation: 3.3083975641438252]
	TIME [epoch: 10.4 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.056511039674791		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.056511039674791 | validation: 3.4831994243596647]
	TIME [epoch: 10.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1169607608736207		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 3.1169607608736207 | validation: 3.4037527507857033]
	TIME [epoch: 10.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1014889932736684		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 3.1014889932736684 | validation: 3.372169511997056]
	TIME [epoch: 10.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0876363927803205		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.0876363927803205 | validation: 3.2846646774519095]
	TIME [epoch: 10.4 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0704925550961204		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.0704925550961204 | validation: 3.404521301733007]
	TIME [epoch: 10.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0910693673561473		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.0910693673561473 | validation: 3.4062966816746747]
	TIME [epoch: 10.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0970538537743364		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 3.0970538537743364 | validation: 3.5066481845009196]
	TIME [epoch: 10.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0952139836543324		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.0952139836543324 | validation: 3.597770326152597]
	TIME [epoch: 10.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1327764237349407		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.1327764237349407 | validation: 3.2918306185793234]
	TIME [epoch: 10.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0647384597366463		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.0647384597366463 | validation: 3.322036600531073]
	TIME [epoch: 10.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.134443520540019		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.134443520540019 | validation: 3.2863997522439625]
	TIME [epoch: 10.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1229855676857197		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.1229855676857197 | validation: 3.2781117794779795]
	TIME [epoch: 10.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0603572791291724		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 3.0603572791291724 | validation: 3.3274070780689273]
	TIME [epoch: 10.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0645234006608537		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.0645234006608537 | validation: 3.3216392572327993]
	TIME [epoch: 10.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.051082437353365		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 3.051082437353365 | validation: 3.285083917635615]
	TIME [epoch: 10.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.179107115002549		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.179107115002549 | validation: 3.4473988095198487]
	TIME [epoch: 10.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1001644209358092		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 3.1001644209358092 | validation: 3.3686050327411987]
	TIME [epoch: 10.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0720554076428557		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 3.0720554076428557 | validation: 3.2837295590741946]
	TIME [epoch: 10.4 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0848793055441495		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.0848793055441495 | validation: 3.361838937053044]
	TIME [epoch: 10.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.055336332300967		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 3.055336332300967 | validation: 3.3047453277602608]
	TIME [epoch: 10.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.081712509132002		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.081712509132002 | validation: 3.320111790183101]
	TIME [epoch: 10.4 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0816456678227544		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 3.0816456678227544 | validation: 3.31705209341148]
	TIME [epoch: 10.4 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0784614631158718		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 3.0784614631158718 | validation: 3.3120549270753443]
	TIME [epoch: 10.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.055549916931459		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.055549916931459 | validation: 3.3377302743603243]
	TIME [epoch: 10.4 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0823297023850325		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.0823297023850325 | validation: 3.376480814596942]
	TIME [epoch: 10.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0869216142232805		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 3.0869216142232805 | validation: 3.713211360979226]
	TIME [epoch: 10.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.253246558661137		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 3.253246558661137 | validation: 3.309267136103323]
	TIME [epoch: 10.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0718213801910155		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.0718213801910155 | validation: 3.29454484371546]
	TIME [epoch: 10.4 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.054381686394291		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.054381686394291 | validation: 3.329092733027899]
	TIME [epoch: 10.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0852846497171065		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.0852846497171065 | validation: 3.3053342691859253]
	TIME [epoch: 10.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.049729753385207		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.049729753385207 | validation: 3.31547298839421]
	TIME [epoch: 10.4 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.071348188192124		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.071348188192124 | validation: 3.3366120383094517]
	TIME [epoch: 10.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.089047513495552		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 3.089047513495552 | validation: 3.2784431794583737]
	TIME [epoch: 10.4 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.093902048926028		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 3.093902048926028 | validation: 3.3382919650801046]
	TIME [epoch: 10.4 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1156974975797356		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.1156974975797356 | validation: 3.2800069536437664]
	TIME [epoch: 10.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.083152199074349		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 3.083152199074349 | validation: 3.2823344650591]
	TIME [epoch: 10.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0448222186680356		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.0448222186680356 | validation: 3.3853808255779474]
	TIME [epoch: 10.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0788391387954515		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.0788391387954515 | validation: 3.3150403653256877]
	TIME [epoch: 10.4 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.05760497918387		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.05760497918387 | validation: 3.31098635669936]
	TIME [epoch: 10.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0613922691343083		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.0613922691343083 | validation: 3.3337754706049316]
	TIME [epoch: 10.4 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.103859284940171		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 3.103859284940171 | validation: 3.377611136015621]
	TIME [epoch: 10.4 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0781924093785045		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.0781924093785045 | validation: 3.282060754030414]
	TIME [epoch: 10.4 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.121905389427329		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.121905389427329 | validation: 3.4357620026346822]
	TIME [epoch: 10.4 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.094925460402444		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 3.094925460402444 | validation: 3.4400762312850386]
	TIME [epoch: 10.4 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1380463844075797		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.1380463844075797 | validation: 3.2995934935164923]
	TIME [epoch: 10.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0769671369055813		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.0769671369055813 | validation: 3.2664635551684236]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0833696365160557		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.0833696365160557 | validation: 3.4633021562415864]
	TIME [epoch: 10.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.093847899114189		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 3.093847899114189 | validation: 3.2809829533460606]
	TIME [epoch: 10.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0540905315656923		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.0540905315656923 | validation: 3.273888135703853]
	TIME [epoch: 10.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.04273172132662		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.04273172132662 | validation: 3.3710810608545647]
	TIME [epoch: 10.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.087112950174789		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.087112950174789 | validation: 3.3174902911624544]
	TIME [epoch: 10.4 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.10761331410921		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.10761331410921 | validation: 3.3178303531485853]
	TIME [epoch: 10.4 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1061822808425292		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.1061822808425292 | validation: 3.492831184282453]
	TIME [epoch: 10.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.117878786024203		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 3.117878786024203 | validation: 3.312084819839185]
	TIME [epoch: 10.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0445423740859408		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.0445423740859408 | validation: 3.333348407896665]
	TIME [epoch: 10.4 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.059090432779506		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.059090432779506 | validation: 3.3757905169265663]
	TIME [epoch: 10.4 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.050745579266878		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.050745579266878 | validation: 3.3016229535543733]
	TIME [epoch: 10.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.091297750740533		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.091297750740533 | validation: 3.3076106224888484]
	TIME [epoch: 10.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0903002748916295		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.0903002748916295 | validation: 3.3029319227802016]
	TIME [epoch: 10.4 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.043998141481647		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.043998141481647 | validation: 3.283967381213122]
	TIME [epoch: 10.4 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.072541662228902		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 3.072541662228902 | validation: 3.274909096357025]
	TIME [epoch: 10.4 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0337943465048482		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 3.0337943465048482 | validation: 3.281413191461191]
	TIME [epoch: 10.4 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058043122257516		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.058043122257516 | validation: 3.2803009931184057]
	TIME [epoch: 10.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058650724752115		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.058650724752115 | validation: 3.5014190247733508]
	TIME [epoch: 10.4 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1297263839495324		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.1297263839495324 | validation: 3.4954058390488503]
	TIME [epoch: 10.4 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.096128038551675		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.096128038551675 | validation: 3.3049258007486344]
	TIME [epoch: 10.4 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0690078196227866		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.0690078196227866 | validation: 3.292163122893153]
	TIME [epoch: 10.4 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0516456758475963		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.0516456758475963 | validation: 3.2951451837826586]
	TIME [epoch: 10.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.06310724030081		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.06310724030081 | validation: 3.4133811439404487]
	TIME [epoch: 10.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0974649388549924		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.0974649388549924 | validation: 3.282768123062614]
	TIME [epoch: 10.4 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.048986819748191		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.048986819748191 | validation: 3.4283850382563834]
	TIME [epoch: 10.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0715454197636287		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.0715454197636287 | validation: 3.315041373617631]
	TIME [epoch: 10.4 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0671927794196323		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.0671927794196323 | validation: 3.2945172810611063]
	TIME [epoch: 10.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.064348139947218		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 3.064348139947218 | validation: 3.310127671186116]
	TIME [epoch: 10.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0927674122166438		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.0927674122166438 | validation: 3.3287507710441155]
	TIME [epoch: 10.4 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1509658429588248		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.1509658429588248 | validation: 3.2858116038001994]
	TIME [epoch: 10.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.099597967921809		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.099597967921809 | validation: 3.3043018208824764]
	TIME [epoch: 10.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0499395656310964		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 3.0499395656310964 | validation: 3.2581470773297077]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0239447001483954		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 3.0239447001483954 | validation: 3.259990255233503]
	TIME [epoch: 10.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0454658441625475		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.0454658441625475 | validation: 3.293429315239596]
	TIME [epoch: 10.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0684170547297946		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.0684170547297946 | validation: 3.274808670189275]
	TIME [epoch: 10.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0632943397901986		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.0632943397901986 | validation: 3.273237968319264]
	TIME [epoch: 10.4 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.08174512152019		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.08174512152019 | validation: 3.298953440441727]
	TIME [epoch: 10.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0597996352951773		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.0597996352951773 | validation: 3.295588860089389]
	TIME [epoch: 10.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.3682572347679143		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.3682572347679143 | validation: 3.4289745452228035]
	TIME [epoch: 10.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.093729675286083		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.093729675286083 | validation: 3.351212104245875]
	TIME [epoch: 10.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0517029624295184		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.0517029624295184 | validation: 3.2932376011353535]
	TIME [epoch: 10.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.047334852909824		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.047334852909824 | validation: 3.439051188004488]
	TIME [epoch: 10.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.11574175198787		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.11574175198787 | validation: 3.288063767170827]
	TIME [epoch: 10.4 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0400804463617277		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.0400804463617277 | validation: 3.3850918612106002]
	TIME [epoch: 10.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0890884681549484		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.0890884681549484 | validation: 3.2842534884033365]
	TIME [epoch: 10.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0293059936185514		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.0293059936185514 | validation: 3.270985062526225]
	TIME [epoch: 10.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.094655260316114		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.094655260316114 | validation: 3.353448976260751]
	TIME [epoch: 10.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0903399018753026		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.0903399018753026 | validation: 3.264788759519303]
	TIME [epoch: 10.4 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0355852017807976		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.0355852017807976 | validation: 3.30908063132695]
	TIME [epoch: 10.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.059828061532129		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.059828061532129 | validation: 3.2826215157548266]
	TIME [epoch: 10.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0578145443904807		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.0578145443904807 | validation: 3.4382684532585905]
	TIME [epoch: 10.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0816443402550795		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.0816443402550795 | validation: 3.2508820331463575]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.050479285245726		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.050479285245726 | validation: 3.462450173005284]
	TIME [epoch: 10.4 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0987406032218523		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.0987406032218523 | validation: 3.287795108180861]
	TIME [epoch: 10.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.061516982083494		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.061516982083494 | validation: 3.2804058547759065]
	TIME [epoch: 10.4 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0580563768291227		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 3.0580563768291227 | validation: 3.270534306025653]
	TIME [epoch: 10.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0177009434791477		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.0177009434791477 | validation: 3.2937954054851786]
	TIME [epoch: 10.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0835845975097675		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.0835845975097675 | validation: 3.3038544720467713]
	TIME [epoch: 10.4 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.076454063918935		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.076454063918935 | validation: 3.333100908418318]
	TIME [epoch: 10.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0794527007025834		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.0794527007025834 | validation: 3.470495408043951]
	TIME [epoch: 10.4 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0941608302759733		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.0941608302759733 | validation: 3.310361278942803]
	TIME [epoch: 10.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1380457453762065		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.1380457453762065 | validation: 3.534407544857468]
	TIME [epoch: 10.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.109258255872116		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.109258255872116 | validation: 3.35758080430639]
	TIME [epoch: 10.4 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0781249487511055		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.0781249487511055 | validation: 3.312307516011838]
	TIME [epoch: 10.4 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.032636582663683		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.032636582663683 | validation: 3.264436323373895]
	TIME [epoch: 10.4 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0861020422493346		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.0861020422493346 | validation: 3.296817767435467]
	TIME [epoch: 10.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.085373765460579		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.085373765460579 | validation: 3.2738611563164146]
	TIME [epoch: 10.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0636227630913155		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.0636227630913155 | validation: 3.484385411107946]
	TIME [epoch: 10.4 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.102065135652194		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.102065135652194 | validation: 3.3480441346475245]
	TIME [epoch: 10.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.04029029960439		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.04029029960439 | validation: 3.394294344525073]
	TIME [epoch: 10.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.124269802355186		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 3.124269802355186 | validation: 3.3750531216926563]
	TIME [epoch: 10.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.047445574028555		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.047445574028555 | validation: 3.3371767552496987]
	TIME [epoch: 10.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.058113822826092		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 3.058113822826092 | validation: 3.347494221759547]
	TIME [epoch: 10.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0358267704736965		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.0358267704736965 | validation: 3.3387329934852166]
	TIME [epoch: 10.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.076528395225318		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.076528395225318 | validation: 3.369618945455834]
	TIME [epoch: 10.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.068140815275021		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.068140815275021 | validation: 3.2906045608058356]
	TIME [epoch: 10.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0260281973925984		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.0260281973925984 | validation: 3.2917341007441507]
	TIME [epoch: 10.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.039698932754849		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.039698932754849 | validation: 3.2649249408294287]
	TIME [epoch: 10.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0305898439948438		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.0305898439948438 | validation: 3.2777737181451574]
	TIME [epoch: 10.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.060423198058161		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 3.060423198058161 | validation: 3.2989695412175832]
	TIME [epoch: 10.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.03827912735571		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.03827912735571 | validation: 3.2781298149861495]
	TIME [epoch: 10.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0237138802912935		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.0237138802912935 | validation: 3.2829478289932372]
	TIME [epoch: 10.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0195339204998133		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.0195339204998133 | validation: 3.2817191788851]
	TIME [epoch: 10.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0292171627617224		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.0292171627617224 | validation: 3.294519210725748]
	TIME [epoch: 10.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0271972743636972		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.0271972743636972 | validation: 3.2897746041857805]
	TIME [epoch: 10.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.039211858179928		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.039211858179928 | validation: 3.366714242111006]
	TIME [epoch: 10.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0620693531153793		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.0620693531153793 | validation: 3.3137976253954626]
	TIME [epoch: 10.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0546244122793045		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.0546244122793045 | validation: 3.28942710565151]
	TIME [epoch: 10.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0345350513018223		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.0345350513018223 | validation: 3.2793486658356255]
	TIME [epoch: 10.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.061729021386117		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.061729021386117 | validation: 3.3452344176823403]
	TIME [epoch: 10.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0334699056099694		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.0334699056099694 | validation: 3.26723485838869]
	TIME [epoch: 10.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.041729599385905		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.041729599385905 | validation: 3.2823664946303333]
	TIME [epoch: 10.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.070877070570947		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 3.070877070570947 | validation: 3.2752729382855166]
	TIME [epoch: 10.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.04124371817919		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.04124371817919 | validation: 3.2668172013810204]
	TIME [epoch: 10.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0292559525712583		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.0292559525712583 | validation: 3.4189588244326665]
	TIME [epoch: 10.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0705079202142658		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.0705079202142658 | validation: 3.2820367707895683]
	TIME [epoch: 10.4 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0110436698791085		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.0110436698791085 | validation: 3.396811414336287]
	TIME [epoch: 10.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.077337047829452		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.077337047829452 | validation: 3.3060854012162646]
	TIME [epoch: 10.3 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0553180886083227		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.0553180886083227 | validation: 3.274867870479557]
	TIME [epoch: 10.4 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1103490517060144		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.1103490517060144 | validation: 3.2734965988433933]
	TIME [epoch: 10.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.039888253129278		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.039888253129278 | validation: 3.3359154967268014]
	TIME [epoch: 10.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0388870666864807		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.0388870666864807 | validation: 3.3726983008758022]
	TIME [epoch: 10.4 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0822595419484897		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.0822595419484897 | validation: 3.2848681133361852]
	TIME [epoch: 10.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0379904373666227		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.0379904373666227 | validation: 3.366358293354939]
	TIME [epoch: 10.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.049048923345536		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.049048923345536 | validation: 3.289816270901462]
	TIME [epoch: 10.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0601264752482455		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.0601264752482455 | validation: 3.2575291029282964]
	TIME [epoch: 10.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.030827716353843		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.030827716353843 | validation: 3.2989175358788008]
	TIME [epoch: 10.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.017869705196845		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.017869705196845 | validation: 3.269416766331217]
	TIME [epoch: 10.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.03634051292652		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.03634051292652 | validation: 3.2634742622283945]
	TIME [epoch: 10.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0705082479977874		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.0705082479977874 | validation: 3.2526137509404154]
	TIME [epoch: 10.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0520340310078162		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.0520340310078162 | validation: 3.2891848636647354]
	TIME [epoch: 10.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.045757766997378		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.045757766997378 | validation: 3.2675555194616788]
	TIME [epoch: 10.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0263079463658045		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.0263079463658045 | validation: 3.2694951201087905]
	TIME [epoch: 10.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.042598608758819		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.042598608758819 | validation: 3.3250837022358777]
	TIME [epoch: 10.4 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0620280298368874		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.0620280298368874 | validation: 3.2831181741045565]
	TIME [epoch: 10.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0213929991925914		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.0213929991925914 | validation: 3.2641647346202927]
	TIME [epoch: 10.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034328512271547		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.034328512271547 | validation: 3.287555949943211]
	TIME [epoch: 10.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0191970040123683		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.0191970040123683 | validation: 3.2715285462957575]
	TIME [epoch: 10.4 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0197993595258135		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.0197993595258135 | validation: 3.2705718324222643]
	TIME [epoch: 10.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0131671923198122		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.0131671923198122 | validation: 3.290147734866309]
	TIME [epoch: 10.4 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.042395168801545		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.042395168801545 | validation: 3.348210181834027]
	TIME [epoch: 10.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.048110167352275		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.048110167352275 | validation: 3.3066548055791976]
	TIME [epoch: 10.4 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.031985939757498		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.031985939757498 | validation: 3.258320598792661]
	TIME [epoch: 10.4 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.022923431360195		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.022923431360195 | validation: 3.2678663650312276]
	TIME [epoch: 10.4 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.027225669429325		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.027225669429325 | validation: 3.3036526694664174]
	TIME [epoch: 10.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0416606788921134		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.0416606788921134 | validation: 3.2645623288995047]
	TIME [epoch: 10.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0217197388551127		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.0217197388551127 | validation: 3.2578661734440186]
	TIME [epoch: 10.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0236599346878217		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.0236599346878217 | validation: 3.330336811190323]
	TIME [epoch: 10.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.066799418698587		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.066799418698587 | validation: 3.2734786308348554]
	TIME [epoch: 10.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0280482651549754		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.0280482651549754 | validation: 3.2803403826209103]
	TIME [epoch: 10.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0142719202394317		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.0142719202394317 | validation: 3.265089175069079]
	TIME [epoch: 10.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0267162990885352		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.0267162990885352 | validation: 3.269385804778342]
	TIME [epoch: 10.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0282948559871143		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.0282948559871143 | validation: 3.274068713466495]
	TIME [epoch: 10.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0350229249656553		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.0350229249656553 | validation: 3.267939268071509]
	TIME [epoch: 10.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0607663865493056		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.0607663865493056 | validation: 3.280143505058983]
	TIME [epoch: 10.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.044707216573986		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.044707216573986 | validation: 3.2922754532259457]
	TIME [epoch: 10.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.026147021152356		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.026147021152356 | validation: 3.320378274321644]
	TIME [epoch: 10.4 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0191118074670187		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.0191118074670187 | validation: 3.3010105949855437]
	TIME [epoch: 10.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0542174441717607		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.0542174441717607 | validation: 3.2740070831481494]
	TIME [epoch: 10.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029852216489767		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.029852216489767 | validation: 3.2910985568688655]
	TIME [epoch: 10.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0144852954866477		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.0144852954866477 | validation: 3.290663824654642]
	TIME [epoch: 10.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0546343280012205		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.0546343280012205 | validation: 3.2937715619018952]
	TIME [epoch: 10.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.026274725421941		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.026274725421941 | validation: 3.2595562282235813]
	TIME [epoch: 10.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0286534044515028		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.0286534044515028 | validation: 3.2691116099706643]
	TIME [epoch: 10.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0307311604507974		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.0307311604507974 | validation: 3.27985808280833]
	TIME [epoch: 10.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.032978715356665		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.032978715356665 | validation: 3.325358959966982]
	TIME [epoch: 10.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0324174017798726		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.0324174017798726 | validation: 3.330936485713252]
	TIME [epoch: 10.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0235414791573354		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.0235414791573354 | validation: 3.267029534427062]
	TIME [epoch: 10.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0117400131268046		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.0117400131268046 | validation: 3.261032993526097]
	TIME [epoch: 10.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0337810704830988		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.0337810704830988 | validation: 3.268296003268728]
	TIME [epoch: 10.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024609518644948		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.024609518644948 | validation: 3.253452438455796]
	TIME [epoch: 10.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0388688010744493		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.0388688010744493 | validation: 3.2743617693922995]
	TIME [epoch: 10.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.008106508836442		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.008106508836442 | validation: 3.248602999381331]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.015584996106553		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.015584996106553 | validation: 3.2782068217472293]
	TIME [epoch: 10.4 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0165960253500606		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.0165960253500606 | validation: 3.2581020573503965]
	TIME [epoch: 10.4 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0329509752543244		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.0329509752543244 | validation: 3.2946518252984074]
	TIME [epoch: 10.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.025380317111032		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.025380317111032 | validation: 3.2844367112477983]
	TIME [epoch: 10.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0693752924085516		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.0693752924085516 | validation: 3.2670166779994494]
	TIME [epoch: 10.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0171398522999473		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.0171398522999473 | validation: 3.256877979128577]
	TIME [epoch: 10.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0322623620532743		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 3.0322623620532743 | validation: 3.260574633044945]
	TIME [epoch: 10.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0553327059357187		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.0553327059357187 | validation: 3.2644220335326817]
	TIME [epoch: 10.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0157676196966126		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.0157676196966126 | validation: 3.3126849854746343]
	TIME [epoch: 10.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0206465898798642		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.0206465898798642 | validation: 3.24933787858512]
	TIME [epoch: 10.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0202993028677056		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.0202993028677056 | validation: 3.255407252199887]
	TIME [epoch: 10.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.042603887767855		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.042603887767855 | validation: 3.2603602260147317]
	TIME [epoch: 10.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029804635137649		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.029804635137649 | validation: 3.332544649457343]
	TIME [epoch: 10.4 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034622388130562		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.034622388130562 | validation: 3.2641669235947193]
	TIME [epoch: 10.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1053239937900146		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.1053239937900146 | validation: 3.334907461376614]
	TIME [epoch: 10.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1270034286402697		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.1270034286402697 | validation: 3.2644339391866923]
	TIME [epoch: 10.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0050034312498504		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.0050034312498504 | validation: 3.3154686000301665]
	TIME [epoch: 10.4 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0235344248548524		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.0235344248548524 | validation: 3.270460635233226]
	TIME [epoch: 10.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0178233199973308		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.0178233199973308 | validation: 3.244821061018707]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.099037228691992		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.099037228691992 | validation: 3.2765775992485406]
	TIME [epoch: 10.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.026173976099847		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.026173976099847 | validation: 3.25936582333797]
	TIME [epoch: 10.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.03206321669854		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.03206321669854 | validation: 3.3346053227537995]
	TIME [epoch: 10.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034597143855287		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.034597143855287 | validation: 3.2821101135489403]
	TIME [epoch: 10.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0439536690966857		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.0439536690966857 | validation: 3.267214923532432]
	TIME [epoch: 10.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.06368028269208		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.06368028269208 | validation: 3.2643864438983488]
	TIME [epoch: 10.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.017917971843949		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.017917971843949 | validation: 3.2820530634912566]
	TIME [epoch: 10.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0141691644293855		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.0141691644293855 | validation: 3.2501901521367054]
	TIME [epoch: 10.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0107039433062077		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.0107039433062077 | validation: 3.2909487685181746]
	TIME [epoch: 10.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.029565822709337		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.029565822709337 | validation: 3.2765875676786265]
	TIME [epoch: 10.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.052905955871851		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.052905955871851 | validation: 3.273739305688895]
	TIME [epoch: 10.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0431156498331293		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.0431156498331293 | validation: 3.279498827217011]
	TIME [epoch: 10.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0268922110139465		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.0268922110139465 | validation: 3.2635450037823444]
	TIME [epoch: 10.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0264948530156253		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.0264948530156253 | validation: 3.2634350929297886]
	TIME [epoch: 10.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0138080084124583		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.0138080084124583 | validation: 3.266901782496534]
	TIME [epoch: 10.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.01068233908815		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.01068233908815 | validation: 3.259283319606167]
	TIME [epoch: 10.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0132413857581533		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.0132413857581533 | validation: 3.301600865648377]
	TIME [epoch: 10.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020809677587771		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.020809677587771 | validation: 3.2596689178902114]
	TIME [epoch: 10.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0138860611220446		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.0138860611220446 | validation: 3.381299286489266]
	TIME [epoch: 10.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0302880327524746		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.0302880327524746 | validation: 3.2493997594949096]
	TIME [epoch: 10.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0467389895494863		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.0467389895494863 | validation: 3.4051661664744244]
	TIME [epoch: 10.4 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0421516776485986		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.0421516776485986 | validation: 3.263792445348164]
	TIME [epoch: 10.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.998693672592327		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 2.998693672592327 | validation: 3.381117543125946]
	TIME [epoch: 10.4 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0473856908528956		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.0473856908528956 | validation: 3.2744945792960425]
	TIME [epoch: 10.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.016093519048112		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.016093519048112 | validation: 3.261079534203933]
	TIME [epoch: 10.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.004999748409132		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.004999748409132 | validation: 3.2901661349463343]
	TIME [epoch: 10.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.019112832751261		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.019112832751261 | validation: 3.273530966108881]
	TIME [epoch: 10.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0215404809559376		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.0215404809559376 | validation: 3.2571000309715292]
	TIME [epoch: 10.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.021920422579764		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 3.021920422579764 | validation: 3.2482020282786146]
	TIME [epoch: 10.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0281354681978185		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.0281354681978185 | validation: 3.2572299687281623]
	TIME [epoch: 10.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0226067068067137		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.0226067068067137 | validation: 3.3333158364826336]
	TIME [epoch: 10.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0204011467512215		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.0204011467512215 | validation: 3.256555129120765]
	TIME [epoch: 10.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000706372382188		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.000706372382188 | validation: 3.262288838687067]
	TIME [epoch: 10.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.008646121672559		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.008646121672559 | validation: 3.2380501974811002]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020494248079495		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.020494248079495 | validation: 3.3272311215872095]
	TIME [epoch: 10.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0420698945880424		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.0420698945880424 | validation: 3.238661718846308]
	TIME [epoch: 10.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.016921938591906		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.016921938591906 | validation: 3.347356197190477]
	TIME [epoch: 10.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024170899501625		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.024170899501625 | validation: 3.274081624463033]
	TIME [epoch: 10.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.011198617058512		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.011198617058512 | validation: 3.264510760093923]
	TIME [epoch: 10.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0087403154102628		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.0087403154102628 | validation: 3.239777463353858]
	TIME [epoch: 10.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991523460129656		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 2.991523460129656 | validation: 3.286514713923626]
	TIME [epoch: 10.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0041178667248856		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.0041178667248856 | validation: 3.250175997214707]
	TIME [epoch: 10.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0150215501579942		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.0150215501579942 | validation: 3.2599427080463044]
	TIME [epoch: 10.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.998748551371454		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 2.998748551371454 | validation: 3.2378896312262375]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0109184908211186		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.0109184908211186 | validation: 3.254843424146354]
	TIME [epoch: 10.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.151648933129678		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.151648933129678 | validation: 3.2929077006905096]
	TIME [epoch: 10.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.021938091237616		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.021938091237616 | validation: 3.344253272759407]
	TIME [epoch: 10.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0144346555481882		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.0144346555481882 | validation: 3.260288729807573]
	TIME [epoch: 10.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.001413182250051		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.001413182250051 | validation: 3.258628656059646]
	TIME [epoch: 10.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9988191292992656		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 2.9988191292992656 | validation: 3.256821852087984]
	TIME [epoch: 10.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991167504687103		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 2.991167504687103 | validation: 3.278557934563257]
	TIME [epoch: 10.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.04564519904437		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.04564519904437 | validation: 3.2509665097071694]
	TIME [epoch: 10.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0034279651771763		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.0034279651771763 | validation: 3.3125151172371092]
	TIME [epoch: 10.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.015489955524861		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.015489955524861 | validation: 3.251531321734116]
	TIME [epoch: 10.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.022275633468655		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.022275633468655 | validation: 3.2568565554540863]
	TIME [epoch: 10.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.021755145378835		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.021755145378835 | validation: 3.284434993779287]
	TIME [epoch: 10.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.056488622011929		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.056488622011929 | validation: 3.25661865239774]
	TIME [epoch: 10.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9928756843849884		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 2.9928756843849884 | validation: 3.249329676594956]
	TIME [epoch: 10.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9929335830840658		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 2.9929335830840658 | validation: 3.240583211417606]
	TIME [epoch: 10.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.023627272523923		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.023627272523923 | validation: 3.2445007929849305]
	TIME [epoch: 10.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0109739319576376		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.0109739319576376 | validation: 3.2536730372716307]
	TIME [epoch: 10.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0215475094082334		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.0215475094082334 | validation: 3.267601073590349]
	TIME [epoch: 10.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0148078434893693		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.0148078434893693 | validation: 3.258558355992939]
	TIME [epoch: 10.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995869730110761		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 2.995869730110761 | validation: 3.253620261267453]
	TIME [epoch: 10.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.017365811749654		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.017365811749654 | validation: 3.298785987240947]
	TIME [epoch: 10.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0949827334014723		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.0949827334014723 | validation: 3.291918773253028]
	TIME [epoch: 10.4 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.003095800546548		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.003095800546548 | validation: 3.2528237816123347]
	TIME [epoch: 10.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.999174892841078		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 2.999174892841078 | validation: 3.244903566229286]
	TIME [epoch: 10.4 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987364703845213		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 2.987364703845213 | validation: 3.332666360668066]
	TIME [epoch: 10.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0331728227880776		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.0331728227880776 | validation: 3.2737223107392572]
	TIME [epoch: 10.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.998763087187391		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 2.998763087187391 | validation: 3.261580143690874]
	TIME [epoch: 10.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.001018334516037		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.001018334516037 | validation: 3.2559445349228566]
	TIME [epoch: 10.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.986689439081153		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 2.986689439081153 | validation: 3.257190524984527]
	TIME [epoch: 10.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9982143432331316		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 2.9982143432331316 | validation: 3.267575422600537]
	TIME [epoch: 10.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991883755161849		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 2.991883755161849 | validation: 3.2461259808999188]
	TIME [epoch: 10.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9996507890686877		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 2.9996507890686877 | validation: 3.2425590065738072]
	TIME [epoch: 10.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.99721256861941		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 2.99721256861941 | validation: 3.2560517501231305]
	TIME [epoch: 10.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9891998606026586		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 2.9891998606026586 | validation: 3.2845772341839092]
	TIME [epoch: 10.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.014809761585621		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.014809761585621 | validation: 3.2662192955064744]
	TIME [epoch: 10.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00625838380359		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.00625838380359 | validation: 3.2365011206291308]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9894796966057804		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 2.9894796966057804 | validation: 3.2645097471874873]
	TIME [epoch: 10.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.99896961232509		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 2.99896961232509 | validation: 3.2692634572668133]
	TIME [epoch: 10.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.992580687623421		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 2.992580687623421 | validation: 3.3656323796173617]
	TIME [epoch: 10.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.032772388825518		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.032772388825518 | validation: 3.234545214574217]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_620.pth
	Model improved!!!
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0076894939770087		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.0076894939770087 | validation: 3.2928324273073986]
	TIME [epoch: 10.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.993903232567378		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 2.993903232567378 | validation: 3.2715623624023467]
	TIME [epoch: 10.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.993705445864859		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 2.993705445864859 | validation: 3.272555333230656]
	TIME [epoch: 10.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9902905575272642		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 2.9902905575272642 | validation: 3.2427885877456495]
	TIME [epoch: 10.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0383332562328618		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.0383332562328618 | validation: 3.3066299236242993]
	TIME [epoch: 10.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.003521555602511		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.003521555602511 | validation: 3.233332978935988]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.033278321312872		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.033278321312872 | validation: 3.2356417636099777]
	TIME [epoch: 10.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9887968926376978		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 2.9887968926376978 | validation: 3.2334205044555753]
	TIME [epoch: 10.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9889889518011925		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 2.9889889518011925 | validation: 3.258261801482655]
	TIME [epoch: 10.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0158720051290535		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.0158720051290535 | validation: 3.2474947267410244]
	TIME [epoch: 10.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0020024637063902		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.0020024637063902 | validation: 3.2644217508269544]
	TIME [epoch: 10.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.998183207710769		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 2.998183207710769 | validation: 3.229092124202405]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0189799287433035		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.0189799287433035 | validation: 3.24800812681854]
	TIME [epoch: 10.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.996959158926301		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 2.996959158926301 | validation: 3.2489614292689164]
	TIME [epoch: 10.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.004457866984846		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.004457866984846 | validation: 3.262912117851231]
	TIME [epoch: 10.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.98942789585434		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 2.98942789585434 | validation: 3.2697988884695053]
	TIME [epoch: 10.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.011436135759459		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 3.011436135759459 | validation: 3.2259187477201805]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_637.pth
	Model improved!!!
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.996899281261091		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 2.996899281261091 | validation: 3.2434122503396576]
	TIME [epoch: 10.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9908155360221427		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 2.9908155360221427 | validation: 3.369458273803866]
	TIME [epoch: 10.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.034793653493742		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.034793653493742 | validation: 3.2603199347395297]
	TIME [epoch: 10.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9858894421872693		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 2.9858894421872693 | validation: 3.2822731336251048]
	TIME [epoch: 10.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.002226619972776		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.002226619972776 | validation: 3.2498251978930113]
	TIME [epoch: 10.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.012094355384586		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.012094355384586 | validation: 3.2623814028947153]
	TIME [epoch: 10.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024086402312359		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.024086402312359 | validation: 3.2513635819036018]
	TIME [epoch: 10.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9971506314133474		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 2.9971506314133474 | validation: 3.2233476141982442]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9975185617776625		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 2.9975185617776625 | validation: 3.2943126318010583]
	TIME [epoch: 10.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0025862269774692		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.0025862269774692 | validation: 3.2479723356620576]
	TIME [epoch: 10.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.99461081054221		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 2.99461081054221 | validation: 3.3468453177456037]
	TIME [epoch: 10.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0198026648599594		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.0198026648599594 | validation: 3.2337663168798634]
	TIME [epoch: 10.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.010895103662474		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 3.010895103662474 | validation: 3.2476145253946163]
	TIME [epoch: 10.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.10786884774628		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.10786884774628 | validation: 3.2332939023872065]
	TIME [epoch: 10.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9866209552662144		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 2.9866209552662144 | validation: 3.2402701850025837]
	TIME [epoch: 10.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9942777685782755		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 2.9942777685782755 | validation: 3.2521076241968068]
	TIME [epoch: 10.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9986126943765705		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 2.9986126943765705 | validation: 3.2386676333981996]
	TIME [epoch: 10.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9809208409766046		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 2.9809208409766046 | validation: 3.23756153242563]
	TIME [epoch: 10.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985445525031481		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 2.985445525031481 | validation: 3.240943485294839]
	TIME [epoch: 10.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9817554751515933		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 2.9817554751515933 | validation: 3.245797794996184]
	TIME [epoch: 10.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9959465544005957		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 2.9959465544005957 | validation: 3.229633780556991]
	TIME [epoch: 10.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9964449152352612		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 2.9964449152352612 | validation: 3.2434094703862675]
	TIME [epoch: 10.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9825513941357435		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 2.9825513941357435 | validation: 3.251930466891993]
	TIME [epoch: 10.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9877929313781877		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 2.9877929313781877 | validation: 3.2566385331941348]
	TIME [epoch: 10.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0254624088362627		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 3.0254624088362627 | validation: 3.2421755184926275]
	TIME [epoch: 10.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9990699456122414		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 2.9990699456122414 | validation: 3.236733518095718]
	TIME [epoch: 10.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.997403452730226		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 2.997403452730226 | validation: 3.2492773510252917]
	TIME [epoch: 10.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.011251415069072		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.011251415069072 | validation: 3.24061297982849]
	TIME [epoch: 10.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.993478213707616		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 2.993478213707616 | validation: 3.2609437781952706]
	TIME [epoch: 10.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9910593116290594		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 2.9910593116290594 | validation: 3.2387203173596153]
	TIME [epoch: 10.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0157701095490954		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.0157701095490954 | validation: 3.245365059547264]
	TIME [epoch: 10.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.993611817242524		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 2.993611817242524 | validation: 3.248334367465552]
	TIME [epoch: 10.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9940341988389223		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 2.9940341988389223 | validation: 3.2336623688824875]
	TIME [epoch: 10.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9961041986368233		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 2.9961041986368233 | validation: 3.2390646339057585]
	TIME [epoch: 10.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9888369883691466		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 2.9888369883691466 | validation: 3.239113891937308]
	TIME [epoch: 10.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0052506908747043		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.0052506908747043 | validation: 3.2760817012112864]
	TIME [epoch: 10.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9920852993150153		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 2.9920852993150153 | validation: 3.232088454659392]
	TIME [epoch: 10.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9786564695274333		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 2.9786564695274333 | validation: 3.284641758653428]
	TIME [epoch: 10.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.014371601801099		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 3.014371601801099 | validation: 3.2471193904691766]
	TIME [epoch: 10.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9983135328474684		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 2.9983135328474684 | validation: 3.2334784079894083]
	TIME [epoch: 10.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9820416227585524		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 2.9820416227585524 | validation: 3.235435689291153]
	TIME [epoch: 10.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985454231785533		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 2.985454231785533 | validation: 3.269054979666847]
	TIME [epoch: 10.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.024542031495044		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.024542031495044 | validation: 3.2439680085726614]
	TIME [epoch: 10.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991170933190425		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 2.991170933190425 | validation: 3.250388115331327]
	TIME [epoch: 10.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0055347009069195		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.0055347009069195 | validation: 3.2332408895213627]
	TIME [epoch: 10.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0001191497399033		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 3.0001191497399033 | validation: 3.3336405752188125]
	TIME [epoch: 10.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020342059907774		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 3.020342059907774 | validation: 3.257879142097686]
	TIME [epoch: 10.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9863659238788314		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 2.9863659238788314 | validation: 3.236914844170552]
	TIME [epoch: 10.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9834369539180585		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 2.9834369539180585 | validation: 3.2339733354065925]
	TIME [epoch: 10.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994189463344		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 2.994189463344 | validation: 3.2300172860364214]
	TIME [epoch: 10.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0055419904157885		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 3.0055419904157885 | validation: 3.226374131832953]
	TIME [epoch: 10.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9894708728104247		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 2.9894708728104247 | validation: 3.225567971806579]
	TIME [epoch: 10.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9838142924884674		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 2.9838142924884674 | validation: 3.238461664728747]
	TIME [epoch: 10.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0038606177961076		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 3.0038606177961076 | validation: 3.2606387143925595]
	TIME [epoch: 10.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0007505963178014		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 3.0007505963178014 | validation: 3.2334220417152473]
	TIME [epoch: 10.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.983620564418041		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 2.983620564418041 | validation: 3.269664428809193]
	TIME [epoch: 10.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0058855638982394		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 3.0058855638982394 | validation: 3.240947248633246]
	TIME [epoch: 10.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974956859992115		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 2.974956859992115 | validation: 3.2331740390088575]
	TIME [epoch: 10.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9840064493676612		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 2.9840064493676612 | validation: 3.228488980872108]
	TIME [epoch: 10.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9857467887107156		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 2.9857467887107156 | validation: 3.25341796847385]
	TIME [epoch: 10.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0165137627380845		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 3.0165137627380845 | validation: 3.2355401028531676]
	TIME [epoch: 10.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.981583426399892		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 2.981583426399892 | validation: 3.231483115223808]
	TIME [epoch: 10.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9900505095617023		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 2.9900505095617023 | validation: 3.2536612778955067]
	TIME [epoch: 10.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9960218924340203		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 2.9960218924340203 | validation: 3.2410939683349373]
	TIME [epoch: 10.4 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9799366906378646		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 2.9799366906378646 | validation: 3.232580562933762]
	TIME [epoch: 10.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990070556374848		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 2.990070556374848 | validation: 3.2445035935140982]
	TIME [epoch: 10.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994570676176459		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 2.994570676176459 | validation: 3.2386228699135495]
	TIME [epoch: 10.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9739147173151155		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 2.9739147173151155 | validation: 3.26257878092607]
	TIME [epoch: 10.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9943512082254453		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 2.9943512082254453 | validation: 3.2435916042723365]
	TIME [epoch: 10.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9849590173667453		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 2.9849590173667453 | validation: 3.2425439781418888]
	TIME [epoch: 10.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.98864711436923		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 2.98864711436923 | validation: 3.2294990404316035]
	TIME [epoch: 10.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.986808011958049		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 2.986808011958049 | validation: 3.237162901921487]
	TIME [epoch: 10.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991426386027766		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 2.991426386027766 | validation: 3.258510519508563]
	TIME [epoch: 10.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9788545908267223		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 2.9788545908267223 | validation: 3.2423147722972065]
	TIME [epoch: 10.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9895577612773367		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 2.9895577612773367 | validation: 3.2292060381401804]
	TIME [epoch: 10.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9920001389528057		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 2.9920001389528057 | validation: 3.2557063096999075]
	TIME [epoch: 10.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.006787920976344		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 3.006787920976344 | validation: 3.2347821303833126]
	TIME [epoch: 10.4 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9802385769122073		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 2.9802385769122073 | validation: 3.270247884486391]
	TIME [epoch: 10.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9983970282597694		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 2.9983970282597694 | validation: 3.234487727050522]
	TIME [epoch: 10.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9915499479660657		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 2.9915499479660657 | validation: 3.2626206171995946]
	TIME [epoch: 10.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9917896482327557		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 2.9917896482327557 | validation: 3.2399187844232133]
	TIME [epoch: 10.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.998450955404942		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 2.998450955404942 | validation: 3.248874250916183]
	TIME [epoch: 10.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9851672007015115		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 2.9851672007015115 | validation: 3.2278379639836454]
	TIME [epoch: 10.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980814083945172		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 2.980814083945172 | validation: 3.235232068661824]
	TIME [epoch: 10.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9796747197242213		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 2.9796747197242213 | validation: 3.2234229989100904]
	TIME [epoch: 10.3 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9745580899490616		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 2.9745580899490616 | validation: 3.223913869124075]
	TIME [epoch: 10.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9855594665391068		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 2.9855594665391068 | validation: 3.224207277454625]
	TIME [epoch: 10.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9736340088751327		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 2.9736340088751327 | validation: 3.262617402420419]
	TIME [epoch: 10.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9881149724167537		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 2.9881149724167537 | validation: 3.224962792136243]
	TIME [epoch: 10.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9854391416813053		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 2.9854391416813053 | validation: 3.2452366750741444]
	TIME [epoch: 10.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987644943909642		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 2.987644943909642 | validation: 3.2581742412014147]
	TIME [epoch: 10.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9904843046689598		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 2.9904843046689598 | validation: 3.2617273915624914]
	TIME [epoch: 10.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990271240400234		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 2.990271240400234 | validation: 3.2381475827150292]
	TIME [epoch: 10.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9893007805633482		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 2.9893007805633482 | validation: 3.244951756091126]
	TIME [epoch: 10.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9852119172275597		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 2.9852119172275597 | validation: 3.257885905749446]
	TIME [epoch: 10.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.997530170072271		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 2.997530170072271 | validation: 3.2859687627695178]
	TIME [epoch: 10.3 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000573050885579		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 3.000573050885579 | validation: 3.222855141320425]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9901398550496516		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 2.9901398550496516 | validation: 3.251466732275084]
	TIME [epoch: 10.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990214249030204		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 2.990214249030204 | validation: 3.3331614019369717]
	TIME [epoch: 10.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.020060122996509		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 3.020060122996509 | validation: 3.2374609553496683]
	TIME [epoch: 10.3 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.992421657348701		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 2.992421657348701 | validation: 3.244742072045355]
	TIME [epoch: 10.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9841883239576044		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 2.9841883239576044 | validation: 3.22764448658206]
	TIME [epoch: 10.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9800577733319464		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 2.9800577733319464 | validation: 3.276164242202898]
	TIME [epoch: 10.3 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995420971717505		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 2.995420971717505 | validation: 3.2490066828898025]
	TIME [epoch: 10.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987046050225595		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 2.987046050225595 | validation: 3.2571857065282046]
	TIME [epoch: 10.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9927779317190097		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 2.9927779317190097 | validation: 3.2404691240185843]
	TIME [epoch: 10.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9769806841535087		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 2.9769806841535087 | validation: 3.252228186856681]
	TIME [epoch: 10.4 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9795090761593572		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 2.9795090761593572 | validation: 3.2254710276663343]
	TIME [epoch: 10.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9888576345658073		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 2.9888576345658073 | validation: 3.260622277400869]
	TIME [epoch: 10.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9811214257171437		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 2.9811214257171437 | validation: 3.2516731593889747]
	TIME [epoch: 10.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9803040362362863		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 2.9803040362362863 | validation: 3.2300123962328313]
	TIME [epoch: 10.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.992317159649718		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 2.992317159649718 | validation: 3.230640623057731]
	TIME [epoch: 10.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978339233489657		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 2.978339233489657 | validation: 3.24684789587855]
	TIME [epoch: 10.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9753238261573203		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 2.9753238261573203 | validation: 3.238564096208863]
	TIME [epoch: 10.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.994269977471725		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 2.994269977471725 | validation: 3.308595296653591]
	TIME [epoch: 10.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9922042336051		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 2.9922042336051 | validation: 3.236158300928143]
	TIME [epoch: 10.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9740843339838485		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 2.9740843339838485 | validation: 3.2315044410582723]
	TIME [epoch: 10.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9692398317399236		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 2.9692398317399236 | validation: 3.219507366456359]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9773817500064754		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 2.9773817500064754 | validation: 3.2986117753983506]
	TIME [epoch: 10.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990018382985475		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 2.990018382985475 | validation: 3.2284048031940085]
	TIME [epoch: 10.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9751186524654867		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 2.9751186524654867 | validation: 3.234266273115659]
	TIME [epoch: 10.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9751543099170688		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 2.9751543099170688 | validation: 3.2285387888909804]
	TIME [epoch: 10.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0154959378325343		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 3.0154959378325343 | validation: 3.2692248899406287]
	TIME [epoch: 10.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987412456341473		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 2.987412456341473 | validation: 3.234021939347513]
	TIME [epoch: 10.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9824463227678004		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 2.9824463227678004 | validation: 3.264652194941925]
	TIME [epoch: 10.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.002029894795824		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 3.002029894795824 | validation: 3.266731893726342]
	TIME [epoch: 10.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9959472981687982		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 2.9959472981687982 | validation: 3.249971489066677]
	TIME [epoch: 10.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9830046604079588		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 2.9830046604079588 | validation: 3.266040980374429]
	TIME [epoch: 10.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.995421400823149		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 2.995421400823149 | validation: 3.22391813815264]
	TIME [epoch: 10.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97620435482047		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 2.97620435482047 | validation: 3.2218163152234194]
	TIME [epoch: 10.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9874873631296		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 2.9874873631296 | validation: 3.2197840962568245]
	TIME [epoch: 10.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9793934028432787		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 2.9793934028432787 | validation: 3.272936892599633]
	TIME [epoch: 10.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.981496735239193		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 2.981496735239193 | validation: 3.2279933250942174]
	TIME [epoch: 10.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968254514266561		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 2.968254514266561 | validation: 3.2354685589801595]
	TIME [epoch: 10.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9993838461791027		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 2.9993838461791027 | validation: 3.3571482790259193]
	TIME [epoch: 10.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.00253018050888		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 3.00253018050888 | validation: 3.227611812194582]
	TIME [epoch: 10.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97637062758424		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 2.97637062758424 | validation: 3.2290087092762856]
	TIME [epoch: 10.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9751741324994976		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 2.9751741324994976 | validation: 3.2242899519155515]
	TIME [epoch: 10.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9780716509878276		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 2.9780716509878276 | validation: 3.318699728009733]
	TIME [epoch: 10.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.000237622501567		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 3.000237622501567 | validation: 3.226196041125626]
	TIME [epoch: 10.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.989901729968108		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 2.989901729968108 | validation: 3.2418299802038324]
	TIME [epoch: 10.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.981808754343894		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 2.981808754343894 | validation: 3.2230092183440298]
	TIME [epoch: 10.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9922320399366114		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 2.9922320399366114 | validation: 3.2351220806553163]
	TIME [epoch: 10.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9901372219351368		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 2.9901372219351368 | validation: 3.2269907546405]
	TIME [epoch: 10.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9747383595814028		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 2.9747383595814028 | validation: 3.2256346483529943]
	TIME [epoch: 10.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9705187776536293		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 2.9705187776536293 | validation: 3.252961281166929]
	TIME [epoch: 10.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9878034595714547		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 2.9878034595714547 | validation: 3.2389833124721146]
	TIME [epoch: 10.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.982728395791396		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 2.982728395791396 | validation: 3.2401253391584808]
	TIME [epoch: 10.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9674236106531837		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 2.9674236106531837 | validation: 3.2269084461681365]
	TIME [epoch: 10.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9927577511556254		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 2.9927577511556254 | validation: 3.229319099902322]
	TIME [epoch: 10.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977206305155703		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 2.977206305155703 | validation: 3.230876956197671]
	TIME [epoch: 10.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9776350889412977		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 2.9776350889412977 | validation: 3.2452145984290706]
	TIME [epoch: 10.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.009351719124167		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 3.009351719124167 | validation: 3.240552041343211]
	TIME [epoch: 10.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9872410199977457		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 2.9872410199977457 | validation: 3.218539517859208]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0061367159199492		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 3.0061367159199492 | validation: 3.3387466778369084]
	TIME [epoch: 10.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.998510922651664		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 2.998510922651664 | validation: 3.230744850801075]
	TIME [epoch: 10.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977603255489339		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 2.977603255489339 | validation: 3.2203197764733105]
	TIME [epoch: 10.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973178697607943		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 2.973178697607943 | validation: 3.2239524747275956]
	TIME [epoch: 10.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972680884826187		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 2.972680884826187 | validation: 3.227671358845079]
	TIME [epoch: 10.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9761015904019157		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 2.9761015904019157 | validation: 3.2170305317728727]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9783349867745974		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 2.9783349867745974 | validation: 3.2229345191482923]
	TIME [epoch: 10.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971491231026939		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 2.971491231026939 | validation: 3.2540419635354363]
	TIME [epoch: 10.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.001926238647969		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 3.001926238647969 | validation: 3.233405517149713]
	TIME [epoch: 10.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9726964424464706		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 2.9726964424464706 | validation: 3.2302549660311457]
	TIME [epoch: 10.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969040077274541		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 2.969040077274541 | validation: 3.2100794138083657]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.974300126101484		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 2.974300126101484 | validation: 3.2373723098713434]
	TIME [epoch: 10.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976332490281178		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 2.976332490281178 | validation: 3.245084540777481]
	TIME [epoch: 10.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.980460836279758		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 2.980460836279758 | validation: 3.27187716542942]
	TIME [epoch: 10.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.002568730417055		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 3.002568730417055 | validation: 3.247164789276237]
	TIME [epoch: 10.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971241000525588		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 2.971241000525588 | validation: 3.2224957208863554]
	TIME [epoch: 10.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9743607544648802		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 2.9743607544648802 | validation: 3.220100459402821]
	TIME [epoch: 10.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9672004454332197		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 2.9672004454332197 | validation: 3.2243528023067087]
	TIME [epoch: 10.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9791948764722873		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 2.9791948764722873 | validation: 3.261867118335332]
	TIME [epoch: 10.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990723017506429		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 2.990723017506429 | validation: 3.2278550891245072]
	TIME [epoch: 10.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97578192971576		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 2.97578192971576 | validation: 3.2311633613534867]
	TIME [epoch: 10.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9750182840295665		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 2.9750182840295665 | validation: 3.2169883404813]
	TIME [epoch: 10.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971210600915245		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 2.971210600915245 | validation: 3.22804207447478]
	TIME [epoch: 10.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975297711272044		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 2.975297711272044 | validation: 3.2416383608343167]
	TIME [epoch: 10.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9778418497104533		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 2.9778418497104533 | validation: 3.228154197647689]
	TIME [epoch: 10.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972305548948132		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 2.972305548948132 | validation: 3.2198089846910545]
	TIME [epoch: 10.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9621815580202413		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 2.9621815580202413 | validation: 3.2458038012502692]
	TIME [epoch: 10.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9831897813359403		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 2.9831897813359403 | validation: 3.2269565967420393]
	TIME [epoch: 10.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9757039003212817		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 2.9757039003212817 | validation: 3.225077500847256]
	TIME [epoch: 10.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97324557772114		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 2.97324557772114 | validation: 3.219277825453787]
	TIME [epoch: 10.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97027815618333		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 2.97027815618333 | validation: 3.231186789937492]
	TIME [epoch: 10.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978228383621879		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 2.978228383621879 | validation: 3.237986692251003]
	TIME [epoch: 10.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9872728718575483		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 2.9872728718575483 | validation: 3.2202401509818173]
	TIME [epoch: 10.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9758350790748027		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 2.9758350790748027 | validation: 3.25171797355971]
	TIME [epoch: 10.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973616215509849		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 2.973616215509849 | validation: 3.245208779135408]
	TIME [epoch: 10.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0194151840714865		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 3.0194151840714865 | validation: 3.2441985414214365]
	TIME [epoch: 10.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9794277512637244		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 2.9794277512637244 | validation: 3.237109365585409]
	TIME [epoch: 10.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969745652728669		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 2.969745652728669 | validation: 3.2292204263619433]
	TIME [epoch: 10.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9703634365155613		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 2.9703634365155613 | validation: 3.2322987632287226]
	TIME [epoch: 10.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9712887837572444		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 2.9712887837572444 | validation: 3.2264248392626698]
	TIME [epoch: 10.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978406074906584		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 2.978406074906584 | validation: 3.227946426817969]
	TIME [epoch: 10.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9789660618325717		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 2.9789660618325717 | validation: 3.221299679236631]
	TIME [epoch: 10.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9659425859460846		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 2.9659425859460846 | validation: 3.2245999746676346]
	TIME [epoch: 10.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9669730482414485		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 2.9669730482414485 | validation: 3.2302693539992613]
	TIME [epoch: 10.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9812899558400625		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 2.9812899558400625 | validation: 3.248615823168645]
	TIME [epoch: 10.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9730447012728547		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 2.9730447012728547 | validation: 3.232022968675668]
	TIME [epoch: 10.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0096951769025946		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 3.0096951769025946 | validation: 3.258824646864332]
	TIME [epoch: 10.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9817599061033158		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 2.9817599061033158 | validation: 3.2569032843443586]
	TIME [epoch: 10.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9865494218293334		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 2.9865494218293334 | validation: 3.2182119927356196]
	TIME [epoch: 10.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970742355307036		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 2.970742355307036 | validation: 3.221980420128123]
	TIME [epoch: 10.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9723568070547923		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 2.9723568070547923 | validation: 3.226445059460866]
	TIME [epoch: 10.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971835826547756		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 2.971835826547756 | validation: 3.2303947561097615]
	TIME [epoch: 10.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9737911877187813		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 2.9737911877187813 | validation: 3.2193344033315747]
	TIME [epoch: 10.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9706560664532566		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 2.9706560664532566 | validation: 3.2409094178695192]
	TIME [epoch: 10.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.979857407405956		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 2.979857407405956 | validation: 3.2300085883987504]
	TIME [epoch: 10.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9714988231107875		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 2.9714988231107875 | validation: 3.2448696084037785]
	TIME [epoch: 10.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.991930942474333		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 2.991930942474333 | validation: 3.2260293689234607]
	TIME [epoch: 10.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9660769412443484		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 2.9660769412443484 | validation: 3.233957650586084]
	TIME [epoch: 10.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976572973135655		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 2.976572973135655 | validation: 3.2577379425009516]
	TIME [epoch: 10.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9702670635217863		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 2.9702670635217863 | validation: 3.2146879583427266]
	TIME [epoch: 10.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965776065796013		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 2.965776065796013 | validation: 3.2390790832326593]
	TIME [epoch: 10.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973014167299074		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 2.973014167299074 | validation: 3.2272388543147663]
	TIME [epoch: 10.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9691391159499982		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 2.9691391159499982 | validation: 3.2270050621156736]
	TIME [epoch: 10.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9814694253508023		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 2.9814694253508023 | validation: 3.2404958868209732]
	TIME [epoch: 10.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9814033420747106		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 2.9814033420747106 | validation: 3.2364824392905907]
	TIME [epoch: 10.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.989586064809946		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 2.989586064809946 | validation: 3.232211544725241]
	TIME [epoch: 10.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9865544216958506		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 2.9865544216958506 | validation: 3.22805238857073]
	TIME [epoch: 10.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9708573198732666		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 2.9708573198732666 | validation: 3.2461977123078216]
	TIME [epoch: 10.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973972076106951		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 2.973972076106951 | validation: 3.229042977637322]
	TIME [epoch: 10.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9770089957903827		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 2.9770089957903827 | validation: 3.223035673485899]
	TIME [epoch: 10.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964535436072803		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 2.964535436072803 | validation: 3.219276163145381]
	TIME [epoch: 10.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.985231148434638		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 2.985231148434638 | validation: 3.2105858434237415]
	TIME [epoch: 10.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961379029596337		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 2.961379029596337 | validation: 3.2234095430214964]
	TIME [epoch: 10.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9689815120685306		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 2.9689815120685306 | validation: 3.231171234694782]
	TIME [epoch: 10.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9740551347871316		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 2.9740551347871316 | validation: 3.2440812783430477]
	TIME [epoch: 10.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0092615161754397		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 3.0092615161754397 | validation: 3.2143751413064603]
	TIME [epoch: 10.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97378455576486		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 2.97378455576486 | validation: 3.2310083228855375]
	TIME [epoch: 10.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9771450660673238		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 2.9771450660673238 | validation: 3.2513397623011886]
	TIME [epoch: 10.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9736352945800566		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 2.9736352945800566 | validation: 3.2321436097361413]
	TIME [epoch: 10.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9835831111314297		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 2.9835831111314297 | validation: 3.221249236836241]
	TIME [epoch: 10.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97545239154835		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 2.97545239154835 | validation: 3.2241742269744322]
	TIME [epoch: 10.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9842649097628984		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 2.9842649097628984 | validation: 3.210386473995494]
	TIME [epoch: 10.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987674519399353		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 2.987674519399353 | validation: 3.2519482016295265]
	TIME [epoch: 10.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.979158118430137		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 2.979158118430137 | validation: 3.2282869283515025]
	TIME [epoch: 10.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966308496297188		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 2.966308496297188 | validation: 3.220100622493657]
	TIME [epoch: 10.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966466659458755		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 2.966466659458755 | validation: 3.232670191270862]
	TIME [epoch: 10.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.986281036298831		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 2.986281036298831 | validation: 3.238339290402089]
	TIME [epoch: 10.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977934182272089		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 2.977934182272089 | validation: 3.2272519934072115]
	TIME [epoch: 10.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968626194568216		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 2.968626194568216 | validation: 3.2238396554495297]
	TIME [epoch: 10.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.977115565692963		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 2.977115565692963 | validation: 3.228791599076012]
	TIME [epoch: 10.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9836048147163883		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 2.9836048147163883 | validation: 3.2201264297737953]
	TIME [epoch: 10.4 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960555088502639		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 2.960555088502639 | validation: 3.2473939368973435]
	TIME [epoch: 10.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9716878823728594		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 2.9716878823728594 | validation: 3.2164552451474906]
	TIME [epoch: 10.4 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9680893711033094		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 2.9680893711033094 | validation: 3.224969863975323]
	TIME [epoch: 10.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.987734056036665		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 2.987734056036665 | validation: 3.2417789740807144]
	TIME [epoch: 10.4 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.99212476813973		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 2.99212476813973 | validation: 3.2532226370764112]
	TIME [epoch: 10.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.984740034716679		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 2.984740034716679 | validation: 3.218455412367766]
	TIME [epoch: 10.4 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970712042786473		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 2.970712042786473 | validation: 3.21469357885712]
	TIME [epoch: 10.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967794000348834		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 2.967794000348834 | validation: 3.222017055988268]
	TIME [epoch: 10.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9737710920495717		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 2.9737710920495717 | validation: 3.215966841174709]
	TIME [epoch: 10.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969427126039744		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 2.969427126039744 | validation: 3.241065280384743]
	TIME [epoch: 10.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9800859190902864		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 2.9800859190902864 | validation: 3.2110839767387547]
	TIME [epoch: 10.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9597674992692458		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 2.9597674992692458 | validation: 3.222019016284125]
	TIME [epoch: 10.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9681616470491092		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 2.9681616470491092 | validation: 3.237336912188737]
	TIME [epoch: 10.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9816330044249577		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 2.9816330044249577 | validation: 3.245560593807728]
	TIME [epoch: 10.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9781991823121055		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 2.9781991823121055 | validation: 3.2200814662912736]
	TIME [epoch: 10.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96791049901083		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 2.96791049901083 | validation: 3.2149627569222985]
	TIME [epoch: 10.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9782276934950227		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 2.9782276934950227 | validation: 3.2277056839114655]
	TIME [epoch: 10.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965472736480228		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 2.965472736480228 | validation: 3.219309675468931]
	TIME [epoch: 10.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9603652085935144		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 2.9603652085935144 | validation: 3.2273733885045877]
	TIME [epoch: 10.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9767803517483022		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 2.9767803517483022 | validation: 3.2239621388928006]
	TIME [epoch: 10.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9595251299543817		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 2.9595251299543817 | validation: 3.2133442090387905]
	TIME [epoch: 10.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9663178248818136		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 2.9663178248818136 | validation: 3.2432172473884475]
	TIME [epoch: 10.4 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976992220797682		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 2.976992220797682 | validation: 3.242020319421697]
	TIME [epoch: 10.4 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9668887061784686		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 2.9668887061784686 | validation: 3.2239988726884623]
	TIME [epoch: 10.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971116603749953		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 2.971116603749953 | validation: 3.2772684112034574]
	TIME [epoch: 10.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9852602560865327		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 2.9852602560865327 | validation: 3.2242681218287217]
	TIME [epoch: 10.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9771021093521357		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 2.9771021093521357 | validation: 3.215172350781466]
	TIME [epoch: 10.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9802585369899752		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 2.9802585369899752 | validation: 3.2306184220338583]
	TIME [epoch: 10.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9687657476558025		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 2.9687657476558025 | validation: 3.2142034709884117]
	TIME [epoch: 10.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965990129412656		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 2.965990129412656 | validation: 3.2218400412419888]
	TIME [epoch: 10.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9639867015730426		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 2.9639867015730426 | validation: 3.2348912517463133]
	TIME [epoch: 10.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972179954487727		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 2.972179954487727 | validation: 3.24217221901745]
	TIME [epoch: 10.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.986154932372262		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 2.986154932372262 | validation: 3.238527464069906]
	TIME [epoch: 10.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.971218728242012		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 2.971218728242012 | validation: 3.241486772376518]
	TIME [epoch: 10.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975434131259773		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 2.975434131259773 | validation: 3.242854320719165]
	TIME [epoch: 10.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9673472976211017		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 2.9673472976211017 | validation: 3.226867016237762]
	TIME [epoch: 10.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964843082848996		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 2.964843082848996 | validation: 3.228645103542973]
	TIME [epoch: 10.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9635178436784537		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 2.9635178436784537 | validation: 3.2266297413863185]
	TIME [epoch: 10.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962992786099113		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 2.962992786099113 | validation: 3.2067754073998045]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9666989118203175		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 2.9666989118203175 | validation: 3.2370245922975607]
	TIME [epoch: 10.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968975650563446		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 2.968975650563446 | validation: 3.2310930803569864]
	TIME [epoch: 10.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9697867842173853		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 2.9697867842173853 | validation: 3.24573769577057]
	TIME [epoch: 10.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9663502284390644		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 2.9663502284390644 | validation: 3.2219891275668693]
	TIME [epoch: 10.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9649349327608747		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 2.9649349327608747 | validation: 3.228321781989414]
	TIME [epoch: 10.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9659156146972885		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 2.9659156146972885 | validation: 3.2287257846316773]
	TIME [epoch: 10.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9742886826083224		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 2.9742886826083224 | validation: 3.224539755601214]
	TIME [epoch: 10.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.990033375057501		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 2.990033375057501 | validation: 3.282395625687392]
	TIME [epoch: 10.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.981846908211933		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 2.981846908211933 | validation: 3.2307207289736466]
	TIME [epoch: 10.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9685512164736556		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 2.9685512164736556 | validation: 3.2169544431114776]
	TIME [epoch: 10.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643006324656893		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 2.9643006324656893 | validation: 3.232622670964531]
	TIME [epoch: 10.3 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9683001365619295		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 2.9683001365619295 | validation: 3.2132197980442196]
	TIME [epoch: 10.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9723293970037714		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 2.9723293970037714 | validation: 3.219536800975527]
	TIME [epoch: 10.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9637763024647477		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 2.9637763024647477 | validation: 3.208369984927349]
	TIME [epoch: 10.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9650417091691508		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 2.9650417091691508 | validation: 3.2234028081531423]
	TIME [epoch: 10.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9675293454356955		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 2.9675293454356955 | validation: 3.2261978039778625]
	TIME [epoch: 10.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.973301054929375		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 2.973301054929375 | validation: 3.20370827106822]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_938.pth
	Model improved!!!
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967695566629618		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 2.967695566629618 | validation: 3.226410725627522]
	TIME [epoch: 10.4 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9678634176849212		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 2.9678634176849212 | validation: 3.220948631203912]
	TIME [epoch: 10.3 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9672159299546186		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 2.9672159299546186 | validation: 3.217510905040415]
	TIME [epoch: 10.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9655656890167057		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 2.9655656890167057 | validation: 3.237356588100603]
	TIME [epoch: 10.3 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972877077964457		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 2.972877077964457 | validation: 3.2447802676841766]
	TIME [epoch: 10.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964256802468686		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 2.964256802468686 | validation: 3.2173964054752497]
	TIME [epoch: 10.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.978292432323329		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 2.978292432323329 | validation: 3.275386900721718]
	TIME [epoch: 10.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9932280715644444		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 2.9932280715644444 | validation: 3.216952555716862]
	TIME [epoch: 10.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9675965488639933		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 2.9675965488639933 | validation: 3.2213350819327675]
	TIME [epoch: 10.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9667388686160345		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 2.9667388686160345 | validation: 3.21419168728632]
	TIME [epoch: 10.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961379067663559		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 2.961379067663559 | validation: 3.2140109361759177]
	TIME [epoch: 10.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9657366585615974		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 2.9657366585615974 | validation: 3.2260544943251124]
	TIME [epoch: 10.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966055564693191		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 2.966055564693191 | validation: 3.2261957385659565]
	TIME [epoch: 10.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.979158607756265		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 2.979158607756265 | validation: 3.2292765367920637]
	TIME [epoch: 10.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961880983990373		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 2.961880983990373 | validation: 3.2071468551111333]
	TIME [epoch: 10.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964230773405511		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 2.964230773405511 | validation: 3.2114329895709623]
	TIME [epoch: 10.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962330015860693		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 2.962330015860693 | validation: 3.223070495715191]
	TIME [epoch: 10.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9652497697784908		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 2.9652497697784908 | validation: 3.225516742198317]
	TIME [epoch: 10.3 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9567059444718575		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 2.9567059444718575 | validation: 3.244930082224317]
	TIME [epoch: 10.3 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9679399512061213		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 2.9679399512061213 | validation: 3.208162452838118]
	TIME [epoch: 10.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605252995983493		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 2.9605252995983493 | validation: 3.2308882851276555]
	TIME [epoch: 10.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9706928485198434		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 2.9706928485198434 | validation: 3.215946256277315]
	TIME [epoch: 10.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9717177216243043		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 2.9717177216243043 | validation: 3.2146763698732923]
	TIME [epoch: 10.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964287843415793		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 2.964287843415793 | validation: 3.212008060435547]
	TIME [epoch: 10.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9681002285999947		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 2.9681002285999947 | validation: 3.2317356097746273]
	TIME [epoch: 10.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96502973761025		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 2.96502973761025 | validation: 3.220781273996465]
	TIME [epoch: 10.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9704565809926122		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 2.9704565809926122 | validation: 3.2216726803288287]
	TIME [epoch: 10.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613566047359443		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 2.9613566047359443 | validation: 3.221034392959404]
	TIME [epoch: 10.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9642481605121427		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 2.9642481605121427 | validation: 3.2147447512447025]
	TIME [epoch: 10.4 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9598008291768956		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 2.9598008291768956 | validation: 3.2060650547471075]
	TIME [epoch: 10.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9625787938646635		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 2.9625787938646635 | validation: 3.206046008765199]
	TIME [epoch: 10.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960754723791414		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 2.960754723791414 | validation: 3.2431040952609176]
	TIME [epoch: 10.3 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9731172669371135		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 2.9731172669371135 | validation: 3.226377762113513]
	TIME [epoch: 10.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9632638751868376		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 2.9632638751868376 | validation: 3.216270056182257]
	TIME [epoch: 10.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9682535091692297		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 2.9682535091692297 | validation: 3.240115096149009]
	TIME [epoch: 10.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9720371528027507		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 2.9720371528027507 | validation: 3.218153004883951]
	TIME [epoch: 10.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9662566843907534		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 2.9662566843907534 | validation: 3.232709124563503]
	TIME [epoch: 10.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9771613186169135		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 2.9771613186169135 | validation: 3.2229118978859495]
	TIME [epoch: 10.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96206564718772		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 2.96206564718772 | validation: 3.2196198738111206]
	TIME [epoch: 10.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9637082692799286		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 2.9637082692799286 | validation: 3.21885227026053]
	TIME [epoch: 10.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963775994544389		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 2.963775994544389 | validation: 3.2328421276524715]
	TIME [epoch: 10.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9704777448358493		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 2.9704777448358493 | validation: 3.214771389983406]
	TIME [epoch: 10.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972068960687703		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 2.972068960687703 | validation: 3.232174466055816]
	TIME [epoch: 10.3 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9739429249987466		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 2.9739429249987466 | validation: 3.2218207597822692]
	TIME [epoch: 10.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963874608404074		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 2.963874608404074 | validation: 3.2160373410278638]
	TIME [epoch: 10.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.970034750240351		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 2.970034750240351 | validation: 3.233711352607912]
	TIME [epoch: 10.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957111544349803		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 2.957111544349803 | validation: 3.2171562560004183]
	TIME [epoch: 10.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9779341069063987		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 2.9779341069063987 | validation: 3.2150245689119794]
	TIME [epoch: 10.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605662608379197		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 2.9605662608379197 | validation: 3.213692438338817]
	TIME [epoch: 10.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963253765150285		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 2.963253765150285 | validation: 3.2439702216266553]
	TIME [epoch: 10.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975720101140755		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 2.975720101140755 | validation: 3.2219980783488222]
	TIME [epoch: 10.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.972373207018328		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 2.972373207018328 | validation: 3.2197944234383886]
	TIME [epoch: 10.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9687686618320823		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 2.9687686618320823 | validation: 3.2234863858204257]
	TIME [epoch: 10.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613213817441317		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 2.9613213817441317 | validation: 3.2082946856471084]
	TIME [epoch: 10.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960813578602786		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 2.960813578602786 | validation: 3.231776676550349]
	TIME [epoch: 10.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966820944258118		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 2.966820944258118 | validation: 3.235373187617289]
	TIME [epoch: 10.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9790668947549044		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 2.9790668947549044 | validation: 3.2221739708362875]
	TIME [epoch: 10.4 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.975035957887639		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 2.975035957887639 | validation: 3.216699588536708]
	TIME [epoch: 10.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966717478041367		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 2.966717478041367 | validation: 3.2154511342767673]
	TIME [epoch: 10.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9667205123884135		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 2.9667205123884135 | validation: 3.220430029321166]
	TIME [epoch: 10.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957707045673915		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 2.957707045673915 | validation: 3.202206572716964]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_999.pth
	Model improved!!!
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562380366893257		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 2.9562380366893257 | validation: 3.209521737571963]
	TIME [epoch: 10.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960547999269549		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 2.960547999269549 | validation: 3.2225831972020536]
	TIME [epoch: 10.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.968353449627476		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 2.968353449627476 | validation: 3.2568468877330647]
	TIME [epoch: 10.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9704269503450225		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 2.9704269503450225 | validation: 3.229831029904577]
	TIME [epoch: 10.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9644801906918348		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 2.9644801906918348 | validation: 3.23099883498801]
	TIME [epoch: 10.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962198274495584		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 2.962198274495584 | validation: 3.225244814600577]
	TIME [epoch: 10.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9621143869784428		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 2.9621143869784428 | validation: 3.2239464567253386]
	TIME [epoch: 10.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9664908076017102		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 2.9664908076017102 | validation: 3.2175144065195727]
	TIME [epoch: 10.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9668199077146835		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 2.9668199077146835 | validation: 3.2247877383816688]
	TIME [epoch: 10.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9581226385300696		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 2.9581226385300696 | validation: 3.2179048596323403]
	TIME [epoch: 10.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576530642393832		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 2.9576530642393832 | validation: 3.2046772007337005]
	TIME [epoch: 10.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95945818571571		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 2.95945818571571 | validation: 3.2237699057403484]
	TIME [epoch: 10.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9634862876743506		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 2.9634862876743506 | validation: 3.2159786256172835]
	TIME [epoch: 10.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964537272797131		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 2.964537272797131 | validation: 3.236175829412273]
	TIME [epoch: 10.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9673957271713673		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 2.9673957271713673 | validation: 3.2214484247933015]
	TIME [epoch: 10.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9700555434605307		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 2.9700555434605307 | validation: 3.2250416330474514]
	TIME [epoch: 10.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9734434768217852		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 2.9734434768217852 | validation: 3.2306985479218033]
	TIME [epoch: 10.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9632225072503386		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 2.9632225072503386 | validation: 3.2194124185349438]
	TIME [epoch: 10.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959542153823202		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 2.959542153823202 | validation: 3.2136920151468304]
	TIME [epoch: 10.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9589335767193266		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 2.9589335767193266 | validation: 3.215522429156739]
	TIME [epoch: 10.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965855818732542		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 2.965855818732542 | validation: 3.215219129024207]
	TIME [epoch: 10.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9646593789471116		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 2.9646593789471116 | validation: 3.2301412484375125]
	TIME [epoch: 10.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9665775474907066		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 2.9665775474907066 | validation: 3.2179664374729295]
	TIME [epoch: 10.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964329952291471		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 2.964329952291471 | validation: 3.2221837988248403]
	TIME [epoch: 10.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9702558915182036		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 2.9702558915182036 | validation: 3.2402434915042586]
	TIME [epoch: 10.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965242427393846		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 2.965242427393846 | validation: 3.207118664700831]
	TIME [epoch: 10.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958153159374637		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 2.958153159374637 | validation: 3.2115413812362488]
	TIME [epoch: 10.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9603102084744215		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 2.9603102084744215 | validation: 3.220691809737923]
	TIME [epoch: 10.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9801608458820277		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 2.9801608458820277 | validation: 3.263105460751002]
	TIME [epoch: 10.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9725585497888964		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 2.9725585497888964 | validation: 3.2209419233038363]
	TIME [epoch: 10.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9697181365271073		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 2.9697181365271073 | validation: 3.2245045239385104]
	TIME [epoch: 10.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9621745758341143		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 2.9621745758341143 | validation: 3.216391324068674]
	TIME [epoch: 10.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9643183911420623		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 2.9643183911420623 | validation: 3.2262728226877426]
	TIME [epoch: 10.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613510332893505		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 2.9613510332893505 | validation: 3.2274392621205323]
	TIME [epoch: 10.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967917247865599		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 2.967917247865599 | validation: 3.2258027460300447]
	TIME [epoch: 10.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9706378847475814		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 2.9706378847475814 | validation: 3.2176858850139274]
	TIME [epoch: 10.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613436350697344		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 2.9613436350697344 | validation: 3.218836167979707]
	TIME [epoch: 10.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95939599674726		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 2.95939599674726 | validation: 3.2132945831844597]
	TIME [epoch: 10.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960014519000041		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 2.960014519000041 | validation: 3.2199179090362735]
	TIME [epoch: 10.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9664351551428934		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 2.9664351551428934 | validation: 3.228024362453873]
	TIME [epoch: 10.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9589259113780857		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 2.9589259113780857 | validation: 3.2126196165343153]
	TIME [epoch: 10.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956876010839183		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 2.956876010839183 | validation: 3.2107581994017402]
	TIME [epoch: 10.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956587184870035		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 2.956587184870035 | validation: 3.2244652837252805]
	TIME [epoch: 10.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9701258907963712		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 2.9701258907963712 | validation: 3.2174599439090685]
	TIME [epoch: 10.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9617513352308618		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 2.9617513352308618 | validation: 3.2175198205454314]
	TIME [epoch: 10.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960173197781012		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 2.960173197781012 | validation: 3.219665434187898]
	TIME [epoch: 10.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566131568170846		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 2.9566131568170846 | validation: 3.213346219184856]
	TIME [epoch: 10.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96332846313807		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 2.96332846313807 | validation: 3.2114187608905933]
	TIME [epoch: 10.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958438971738541		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 2.958438971738541 | validation: 3.209359299401708]
	TIME [epoch: 10.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9593003090196923		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 2.9593003090196923 | validation: 3.225162001012718]
	TIME [epoch: 10.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963815852545279		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 2.963815852545279 | validation: 3.22073345038313]
	TIME [epoch: 10.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9612158641256023		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 2.9612158641256023 | validation: 3.219086252710915]
	TIME [epoch: 10.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545912350394046		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 2.9545912350394046 | validation: 3.2226054072249086]
	TIME [epoch: 10.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9653684345516163		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 2.9653684345516163 | validation: 3.205636238320766]
	TIME [epoch: 10.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9599298741778375		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 2.9599298741778375 | validation: 3.21783316684861]
	TIME [epoch: 10.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9631496395668817		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 2.9631496395668817 | validation: 3.21891291460954]
	TIME [epoch: 10.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9693768054401954		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 2.9693768054401954 | validation: 3.207108430604023]
	TIME [epoch: 10.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963865345892802		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 2.963865345892802 | validation: 3.2051673477825338]
	TIME [epoch: 10.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9638242264465458		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 2.9638242264465458 | validation: 3.2221429450371013]
	TIME [epoch: 10.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9712759444906536		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 2.9712759444906536 | validation: 3.2210484998180164]
	TIME [epoch: 10.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95781311729721		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 2.95781311729721 | validation: 3.2226794920934654]
	TIME [epoch: 10.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9713547157970117		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 2.9713547157970117 | validation: 3.219187789567011]
	TIME [epoch: 10.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957702990948328		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 2.957702990948328 | validation: 3.2154670010341575]
	TIME [epoch: 10.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9662040425400082		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 2.9662040425400082 | validation: 3.2372717196787533]
	TIME [epoch: 10.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.976270220780421		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 2.976270220780421 | validation: 3.218924443251467]
	TIME [epoch: 10.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9644095879643566		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 2.9644095879643566 | validation: 3.217776796558376]
	TIME [epoch: 10.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9661298114300862		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 2.9661298114300862 | validation: 3.228041989008576]
	TIME [epoch: 10.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962879640471148		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 2.962879640471148 | validation: 3.2142838760435075]
	TIME [epoch: 10.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.979299258490582		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 2.979299258490582 | validation: 3.2173139934824055]
	TIME [epoch: 10.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957015787150678		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 2.957015787150678 | validation: 3.215715894182574]
	TIME [epoch: 10.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961625723586038		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 2.961625723586038 | validation: 3.2095872054291]
	TIME [epoch: 10.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9614519794707297		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 2.9614519794707297 | validation: 3.2263350069751424]
	TIME [epoch: 10.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9621377266347153		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 2.9621377266347153 | validation: 3.2090141928145046]
	TIME [epoch: 10.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579770511018286		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 2.9579770511018286 | validation: 3.200760920016751]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_1073.pth
	Model improved!!!
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9721019802494366		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 2.9721019802494366 | validation: 3.217254579529917]
	TIME [epoch: 10.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9618992148413277		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 2.9618992148413277 | validation: 3.2168746482457187]
	TIME [epoch: 10.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9645158630701838		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 2.9645158630701838 | validation: 3.2239180240961263]
	TIME [epoch: 10.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9659534629718207		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 2.9659534629718207 | validation: 3.2267552059863847]
	TIME [epoch: 10.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960252750093286		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 2.960252750093286 | validation: 3.206338615658976]
	TIME [epoch: 10.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9670166972595156		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 2.9670166972595156 | validation: 3.221623798134828]
	TIME [epoch: 10.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9644428601867054		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 2.9644428601867054 | validation: 3.2271427106896757]
	TIME [epoch: 10.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962890612118315		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 2.962890612118315 | validation: 3.2228996347227996]
	TIME [epoch: 10.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9661508070466835		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 2.9661508070466835 | validation: 3.217763302987618]
	TIME [epoch: 10.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964296029663266		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 2.964296029663266 | validation: 3.210086981702494]
	TIME [epoch: 10.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9682377288541084		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 2.9682377288541084 | validation: 3.2335821382400596]
	TIME [epoch: 10.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9672580605095376		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 2.9672580605095376 | validation: 3.2336869443111143]
	TIME [epoch: 10.3 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9623799230362358		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 2.9623799230362358 | validation: 3.2226192937159728]
	TIME [epoch: 10.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9603348683155386		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 2.9603348683155386 | validation: 3.214265029156104]
	TIME [epoch: 10.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967090752672411		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 2.967090752672411 | validation: 3.2221097079815806]
	TIME [epoch: 10.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9599911145314026		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 2.9599911145314026 | validation: 3.206909670239255]
	TIME [epoch: 10.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560576076709144		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 2.9560576076709144 | validation: 3.2126804546945062]
	TIME [epoch: 10.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958001406451254		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 2.958001406451254 | validation: 3.21698838948543]
	TIME [epoch: 10.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596933610226834		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 2.9596933610226834 | validation: 3.2177027492878847]
	TIME [epoch: 10.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9628876433243607		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 2.9628876433243607 | validation: 3.2265181853690494]
	TIME [epoch: 10.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9569304127318317		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 2.9569304127318317 | validation: 3.21559216171944]
	TIME [epoch: 10.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585431359358685		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 2.9585431359358685 | validation: 3.2096234806433865]
	TIME [epoch: 10.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956974179052902		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 2.956974179052902 | validation: 3.2239131115432555]
	TIME [epoch: 10.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9534695188426396		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 2.9534695188426396 | validation: 3.231231322080556]
	TIME [epoch: 10.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959297066969373		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 2.959297066969373 | validation: 3.224626982807722]
	TIME [epoch: 10.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9629072196677733		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 2.9629072196677733 | validation: 3.2139663293205705]
	TIME [epoch: 10.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963509424325194		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 2.963509424325194 | validation: 3.2128039217486934]
	TIME [epoch: 10.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958833024926689		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 2.958833024926689 | validation: 3.212595313090398]
	TIME [epoch: 10.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955987577554604		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 2.955987577554604 | validation: 3.2132831545617444]
	TIME [epoch: 10.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9688585975253465		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 2.9688585975253465 | validation: 3.231124023699239]
	TIME [epoch: 10.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963638674416156		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 2.963638674416156 | validation: 3.2158393792899607]
	TIME [epoch: 10.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9825884108186544		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 2.9825884108186544 | validation: 3.2333797197930005]
	TIME [epoch: 10.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963809579176668		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 2.963809579176668 | validation: 3.215516662146695]
	TIME [epoch: 10.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582926834871395		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 2.9582926834871395 | validation: 3.214685558222446]
	TIME [epoch: 10.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9658290354758363		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 2.9658290354758363 | validation: 3.2221815286043016]
	TIME [epoch: 10.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966803894251334		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 2.966803894251334 | validation: 3.2235388379190915]
	TIME [epoch: 10.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9653091016642295		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 2.9653091016642295 | validation: 3.2286709208239257]
	TIME [epoch: 10.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9699043305402135		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 2.9699043305402135 | validation: 3.2094589802267928]
	TIME [epoch: 10.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9636385674323953		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 2.9636385674323953 | validation: 3.2132400650760533]
	TIME [epoch: 10.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9601512266428904		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 2.9601512266428904 | validation: 3.2246661748813326]
	TIME [epoch: 10.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9672823657470833		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 2.9672823657470833 | validation: 3.210175271318027]
	TIME [epoch: 10.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566319755723454		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 2.9566319755723454 | validation: 3.2154926737109624]
	TIME [epoch: 10.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9602337488527786		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 2.9602337488527786 | validation: 3.2201681345291115]
	TIME [epoch: 10.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9737605285256272		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 2.9737605285256272 | validation: 3.2415362082520165]
	TIME [epoch: 10.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963902443814098		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 2.963902443814098 | validation: 3.233548015756818]
	TIME [epoch: 10.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9668403474581693		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 2.9668403474581693 | validation: 3.243502025031008]
	TIME [epoch: 10.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962887539526878		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 2.962887539526878 | validation: 3.2151284062153627]
	TIME [epoch: 10.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9625176451842266		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 2.9625176451842266 | validation: 3.211872989855806]
	TIME [epoch: 10.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9847557524860755		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 2.9847557524860755 | validation: 3.242005251801195]
	TIME [epoch: 10.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9630235351531136		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 2.9630235351531136 | validation: 3.217044886732572]
	TIME [epoch: 10.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96238950878559		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 2.96238950878559 | validation: 3.2129100677631217]
	TIME [epoch: 10.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961007872796477		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 2.961007872796477 | validation: 3.217106705892436]
	TIME [epoch: 10.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960835800488341		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 2.960835800488341 | validation: 3.221183763929576]
	TIME [epoch: 10.3 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9606018435989965		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 2.9606018435989965 | validation: 3.213088854861781]
	TIME [epoch: 10.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953464261278066		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 2.953464261278066 | validation: 3.202664616170655]
	TIME [epoch: 10.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961533849244541		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 2.961533849244541 | validation: 3.216154775111528]
	TIME [epoch: 10.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613678976624582		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 2.9613678976624582 | validation: 3.243921785571761]
	TIME [epoch: 10.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9681924191498394		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 2.9681924191498394 | validation: 3.220650722796078]
	TIME [epoch: 10.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959365425819673		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 2.959365425819673 | validation: 3.2293239280288004]
	TIME [epoch: 10.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582630618458037		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 2.9582630618458037 | validation: 3.2238116926349756]
	TIME [epoch: 10.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9580374667361586		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 2.9580374667361586 | validation: 3.208931512224357]
	TIME [epoch: 10.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960666876679968		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 2.960666876679968 | validation: 3.2142812427309644]
	TIME [epoch: 10.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527183001946553		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 2.9527183001946553 | validation: 3.2169273413005373]
	TIME [epoch: 10.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9669781694841895		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 2.9669781694841895 | validation: 3.2176712406103314]
	TIME [epoch: 10.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9629679439973637		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 2.9629679439973637 | validation: 3.206054104351722]
	TIME [epoch: 10.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9645600398915954		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 2.9645600398915954 | validation: 3.212967031730287]
	TIME [epoch: 10.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.97170325279116		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 2.97170325279116 | validation: 3.2252177369895247]
	TIME [epoch: 10.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9703045468567972		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 2.9703045468567972 | validation: 3.229158784358692]
	TIME [epoch: 10.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96487510272391		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 2.96487510272391 | validation: 3.22275169725324]
	TIME [epoch: 10.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95713553734079		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 2.95713553734079 | validation: 3.2170632263479093]
	TIME [epoch: 10.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9649551909015184		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 2.9649551909015184 | validation: 3.227430859459928]
	TIME [epoch: 10.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9651930960908675		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 2.9651930960908675 | validation: 3.233043322326736]
	TIME [epoch: 10.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9641207383004025		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 2.9641207383004025 | validation: 3.2122516369411818]
	TIME [epoch: 10.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958268733769396		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 2.958268733769396 | validation: 3.2116514551773827]
	TIME [epoch: 10.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605438791696654		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 2.9605438791696654 | validation: 3.2024622296177463]
	TIME [epoch: 10.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955182726992674		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 2.955182726992674 | validation: 3.211368490370036]
	TIME [epoch: 10.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570203471469307		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 2.9570203471469307 | validation: 3.2073523562260617]
	TIME [epoch: 10.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962434985689553		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 2.962434985689553 | validation: 3.21573627611437]
	TIME [epoch: 10.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540741693618497		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 2.9540741693618497 | validation: 3.23034361667575]
	TIME [epoch: 10.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963770992663246		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 2.963770992663246 | validation: 3.222237239878835]
	TIME [epoch: 10.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963838794176781		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 2.963838794176781 | validation: 3.2167108440708]
	TIME [epoch: 10.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96371886980848		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 2.96371886980848 | validation: 3.2215899960810974]
	TIME [epoch: 10.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962654157740521		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 2.962654157740521 | validation: 3.22554526935879]
	TIME [epoch: 10.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545016545668448		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 2.9545016545668448 | validation: 3.2097809888571858]
	TIME [epoch: 10.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9599527620824926		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 2.9599527620824926 | validation: 3.2134673054589262]
	TIME [epoch: 10.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.965389438923682		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 2.965389438923682 | validation: 3.224592475511898]
	TIME [epoch: 10.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961644268871279		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 2.961644268871279 | validation: 3.226587374171272]
	TIME [epoch: 10.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958038418155335		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 2.958038418155335 | validation: 3.238361727956829]
	TIME [epoch: 10.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9673160084096972		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 2.9673160084096972 | validation: 3.2229358171662703]
	TIME [epoch: 10.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955054297748468		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 2.955054297748468 | validation: 3.2123122302438305]
	TIME [epoch: 10.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953870400259521		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 2.953870400259521 | validation: 3.2067522529556194]
	TIME [epoch: 10.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963702176551183		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 2.963702176551183 | validation: 3.230578675854973]
	TIME [epoch: 10.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9628087961914895		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 2.9628087961914895 | validation: 3.2096982679350226]
	TIME [epoch: 10.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959300485433615		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 2.959300485433615 | validation: 3.208535765608693]
	TIME [epoch: 10.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953803574717522		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 2.953803574717522 | validation: 3.2125678218727014]
	TIME [epoch: 10.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.964688925592722		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 2.964688925592722 | validation: 3.2239070844471076]
	TIME [epoch: 10.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.96419858968694		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 2.96419858968694 | validation: 3.2072608358632544]
	TIME [epoch: 10.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9598970594045433		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 2.9598970594045433 | validation: 3.226075542706315]
	TIME [epoch: 10.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9624868045121127		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 2.9624868045121127 | validation: 3.230689485536998]
	TIME [epoch: 10.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605989253047635		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 2.9605989253047635 | validation: 3.204865596977399]
	TIME [epoch: 10.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958766875037248		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 2.958766875037248 | validation: 3.2274575536462793]
	TIME [epoch: 10.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9610866189232583		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 2.9610866189232583 | validation: 3.215634156381034]
	TIME [epoch: 10.4 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596133533967732		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 2.9596133533967732 | validation: 3.2122494934086276]
	TIME [epoch: 10.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562216058521615		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 2.9562216058521615 | validation: 3.2204814838018234]
	TIME [epoch: 10.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9641300356654208		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 2.9641300356654208 | validation: 3.2116489689121335]
	TIME [epoch: 10.4 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9586230543769583		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 2.9586230543769583 | validation: 3.2257734981783335]
	TIME [epoch: 10.4 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.966606461746212		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 2.966606461746212 | validation: 3.2259168079488543]
	TIME [epoch: 10.4 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9724858428470617		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 2.9724858428470617 | validation: 3.2090617767706204]
	TIME [epoch: 10.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9642081920586394		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 2.9642081920586394 | validation: 3.2207032603904384]
	TIME [epoch: 10.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961858597029554		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 2.961858597029554 | validation: 3.2199493940393484]
	TIME [epoch: 10.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562827569219934		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 2.9562827569219934 | validation: 3.215355624605529]
	TIME [epoch: 10.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955749960089384		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 2.955749960089384 | validation: 3.220987382052787]
	TIME [epoch: 10.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959524371761704		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 2.959524371761704 | validation: 3.2243757203372385]
	TIME [epoch: 10.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953249196697674		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 2.953249196697674 | validation: 3.207635746953507]
	TIME [epoch: 10.4 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9554061328118353		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 2.9554061328118353 | validation: 3.216918645654638]
	TIME [epoch: 10.4 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962191866591398		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 2.962191866591398 | validation: 3.2118421377958986]
	TIME [epoch: 10.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545471026909076		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 2.9545471026909076 | validation: 3.2103871202060983]
	TIME [epoch: 10.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576275611521363		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 2.9576275611521363 | validation: 3.2156571229440765]
	TIME [epoch: 10.4 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9592617848717473		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 2.9592617848717473 | validation: 3.2125984567220245]
	TIME [epoch: 10.4 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540565125574147		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 2.9540565125574147 | validation: 3.2075119885373273]
	TIME [epoch: 10.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613615635610846		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 2.9613615635610846 | validation: 3.2214018184177395]
	TIME [epoch: 10.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9563316046843946		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 2.9563316046843946 | validation: 3.20168835497352]
	TIME [epoch: 10.4 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543682900435178		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 2.9543682900435178 | validation: 3.231743230237625]
	TIME [epoch: 10.4 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9677496544190434		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 2.9677496544190434 | validation: 3.2299567385042547]
	TIME [epoch: 10.4 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957387903389482		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 2.957387903389482 | validation: 3.2117583201346385]
	TIME [epoch: 10.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9584811039034618		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 2.9584811039034618 | validation: 3.2149066627210665]
	TIME [epoch: 10.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953334979782872		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 2.953334979782872 | validation: 3.213748507459501]
	TIME [epoch: 10.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582428892524044		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 2.9582428892524044 | validation: 3.235270281127472]
	TIME [epoch: 10.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9780633248478416		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 2.9780633248478416 | validation: 3.2296222300427555]
	TIME [epoch: 10.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553568040321383		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 2.9553568040321383 | validation: 3.2076201208721047]
	TIME [epoch: 10.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953878470911868		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 2.953878470911868 | validation: 3.2229875500383676]
	TIME [epoch: 10.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9583341813121566		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 2.9583341813121566 | validation: 3.2157209480954942]
	TIME [epoch: 10.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523188274629595		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 2.9523188274629595 | validation: 3.2110849777042394]
	TIME [epoch: 10.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542359400690996		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 2.9542359400690996 | validation: 3.210198148110744]
	TIME [epoch: 10.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559676006256628		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 2.9559676006256628 | validation: 3.220760805553133]
	TIME [epoch: 10.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952692947460691		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 2.952692947460691 | validation: 3.2107007337260325]
	TIME [epoch: 10.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9557360398874097		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 2.9557360398874097 | validation: 3.212906766102021]
	TIME [epoch: 10.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571540925247444		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 2.9571540925247444 | validation: 3.203849789194909]
	TIME [epoch: 10.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954078323734985		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 2.954078323734985 | validation: 3.2085496851386894]
	TIME [epoch: 10.4 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9557229200439936		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 2.9557229200439936 | validation: 3.2209490459482657]
	TIME [epoch: 10.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605529872476217		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 2.9605529872476217 | validation: 3.2135822756081525]
	TIME [epoch: 10.4 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9567824336446127		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 2.9567824336446127 | validation: 3.2132011084923198]
	TIME [epoch: 10.4 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952212976221607		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 2.952212976221607 | validation: 3.2194431360541573]
	TIME [epoch: 10.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951930859766229		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 2.951930859766229 | validation: 3.207381629951306]
	TIME [epoch: 10.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955030262511234		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 2.955030262511234 | validation: 3.2254515355584195]
	TIME [epoch: 10.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967091105239609		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 2.967091105239609 | validation: 3.2180809355066318]
	TIME [epoch: 10.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953153001832347		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 2.953153001832347 | validation: 3.2037164673953114]
	TIME [epoch: 10.4 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9544060415661995		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 2.9544060415661995 | validation: 3.210096708826579]
	TIME [epoch: 10.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952607575662035		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 2.952607575662035 | validation: 3.216306539154118]
	TIME [epoch: 10.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.969886824131056		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 2.969886824131056 | validation: 3.23541696407567]
	TIME [epoch: 10.4 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9690679740416654		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 2.9690679740416654 | validation: 3.232219712489384]
	TIME [epoch: 10.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9654877206194934		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 2.9654877206194934 | validation: 3.2160849230513913]
	TIME [epoch: 10.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546268574626238		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 2.9546268574626238 | validation: 3.2070549157876287]
	TIME [epoch: 10.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570269347308433		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 2.9570269347308433 | validation: 3.2156668844047966]
	TIME [epoch: 10.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95970179114051		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 2.95970179114051 | validation: 3.219784878266963]
	TIME [epoch: 10.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9586472176109395		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 2.9586472176109395 | validation: 3.2076750146652278]
	TIME [epoch: 10.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9580857539113343		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 2.9580857539113343 | validation: 3.2139573026959]
	TIME [epoch: 10.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955737096384312		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 2.955737096384312 | validation: 3.212262991905243]
	TIME [epoch: 10.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9637519080020542		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 2.9637519080020542 | validation: 3.214521622654023]
	TIME [epoch: 10.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9573397291632713		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 2.9573397291632713 | validation: 3.2139778035520488]
	TIME [epoch: 10.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9594842851260057		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 2.9594842851260057 | validation: 3.2119386987627387]
	TIME [epoch: 10.4 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9639331951358807		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 2.9639331951358807 | validation: 3.2140571335498467]
	TIME [epoch: 10.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9573290204002527		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 2.9573290204002527 | validation: 3.2094452267183886]
	TIME [epoch: 10.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963074394146215		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 2.963074394146215 | validation: 3.2188001738373093]
	TIME [epoch: 10.4 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9632995278014973		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 2.9632995278014973 | validation: 3.2149256504519963]
	TIME [epoch: 10.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957188499940471		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 2.957188499940471 | validation: 3.2137923904229155]
	TIME [epoch: 10.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553972390039585		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 2.9553972390039585 | validation: 3.2158419341941893]
	TIME [epoch: 10.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950055058502465		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 2.950055058502465 | validation: 3.2135150879807464]
	TIME [epoch: 10.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960882301687114		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 2.960882301687114 | validation: 3.2171928877681024]
	TIME [epoch: 10.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555650732914174		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 2.9555650732914174 | validation: 3.2088466124215773]
	TIME [epoch: 10.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954736083144021		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 2.954736083144021 | validation: 3.2189308388060045]
	TIME [epoch: 10.4 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505517611453795		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 2.9505517611453795 | validation: 3.2157313978493303]
	TIME [epoch: 10.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531727906326646		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 2.9531727906326646 | validation: 3.2069624886296135]
	TIME [epoch: 10.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9630743342767487		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 2.9630743342767487 | validation: 3.2139831207411724]
	TIME [epoch: 10.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9668054012318086		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 2.9668054012318086 | validation: 3.231205562026947]
	TIME [epoch: 10.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9578655510052534		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 2.9578655510052534 | validation: 3.2157983943559727]
	TIME [epoch: 10.4 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9603895805239526		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 2.9603895805239526 | validation: 3.2184293957173633]
	TIME [epoch: 10.4 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95662359078753		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 2.95662359078753 | validation: 3.2287230919204415]
	TIME [epoch: 10.4 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9613145105181933		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 2.9613145105181933 | validation: 3.2095172254913047]
	TIME [epoch: 10.4 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9611543772484614		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 2.9611543772484614 | validation: 3.203504420603807]
	TIME [epoch: 10.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9617004710756976		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 2.9617004710756976 | validation: 3.2180531508265346]
	TIME [epoch: 10.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9624954271316772		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 2.9624954271316772 | validation: 3.213086246776362]
	TIME [epoch: 10.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527432880377424		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 2.9527432880377424 | validation: 3.2137306488508774]
	TIME [epoch: 10.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950649558034539		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 2.950649558034539 | validation: 3.2062982855171684]
	TIME [epoch: 10.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559450445584927		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 2.9559450445584927 | validation: 3.2032097672036164]
	TIME [epoch: 10.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955905630760898		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 2.955905630760898 | validation: 3.206746188255836]
	TIME [epoch: 10.4 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955977698679596		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 2.955977698679596 | validation: 3.200810693359724]
	TIME [epoch: 10.4 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539734376931146		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 2.9539734376931146 | validation: 3.2067393914008346]
	TIME [epoch: 10.4 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9633192441169642		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 2.9633192441169642 | validation: 3.211941447365096]
	TIME [epoch: 10.4 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541750051185653		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 2.9541750051185653 | validation: 3.212386395006126]
	TIME [epoch: 10.4 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959016591551719		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 2.959016591551719 | validation: 3.220523016301879]
	TIME [epoch: 10.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962750394594157		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 2.962750394594157 | validation: 3.212886712777888]
	TIME [epoch: 10.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955572862862391		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 2.955572862862391 | validation: 3.204688490898173]
	TIME [epoch: 10.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9574599421460155		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 2.9574599421460155 | validation: 3.2191811498671985]
	TIME [epoch: 10.4 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952051656428903		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 2.952051656428903 | validation: 3.2167975431343514]
	TIME [epoch: 10.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9548277276424235		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 2.9548277276424235 | validation: 3.2258880095079823]
	TIME [epoch: 10.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955476750084845		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 2.955476750084845 | validation: 3.197823591388348]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_1270.pth
	Model improved!!!
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956959846586623		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 2.956959846586623 | validation: 3.207334038695817]
	TIME [epoch: 10.4 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.963254988120862		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 2.963254988120862 | validation: 3.2151993275232327]
	TIME [epoch: 10.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957688244926068		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 2.957688244926068 | validation: 3.2152987411006855]
	TIME [epoch: 10.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956065751302111		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 2.956065751302111 | validation: 3.221665472287325]
	TIME [epoch: 10.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958615050685548		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 2.958615050685548 | validation: 3.2168983765633388]
	TIME [epoch: 10.4 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9583736978504227		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 2.9583736978504227 | validation: 3.2234760619598632]
	TIME [epoch: 10.4 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9681065678725767		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 2.9681065678725767 | validation: 3.2380657253037373]
	TIME [epoch: 10.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9697213770877857		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 2.9697213770877857 | validation: 3.209302061123628]
	TIME [epoch: 10.4 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961787936667828		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 2.961787936667828 | validation: 3.2023294101047317]
	TIME [epoch: 10.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9583351096782935		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 2.9583351096782935 | validation: 3.223046294344233]
	TIME [epoch: 10.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568084082941564		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 2.9568084082941564 | validation: 3.217113047407255]
	TIME [epoch: 10.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957316565968067		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 2.957316565968067 | validation: 3.2063461029640896]
	TIME [epoch: 10.3 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9611120424546606		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 2.9611120424546606 | validation: 3.2134886370730693]
	TIME [epoch: 10.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955559279654155		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 2.955559279654155 | validation: 3.2179756113928586]
	TIME [epoch: 10.4 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585281725918646		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 2.9585281725918646 | validation: 3.218734637629733]
	TIME [epoch: 10.4 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9653563201076447		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 2.9653563201076447 | validation: 3.1917113003990063]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_1286.pth
	Model improved!!!
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956425621573525		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 2.956425621573525 | validation: 3.2169029476648725]
	TIME [epoch: 10.4 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955217411773817		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 2.955217411773817 | validation: 3.2303164259037316]
	TIME [epoch: 10.4 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555571403458756		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 2.9555571403458756 | validation: 3.223757596357942]
	TIME [epoch: 10.4 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9583404506367637		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 2.9583404506367637 | validation: 3.2106684590185406]
	TIME [epoch: 10.4 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959163272037554		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 2.959163272037554 | validation: 3.226092589786134]
	TIME [epoch: 10.4 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9499634059360447		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 2.9499634059360447 | validation: 3.209044385248494]
	TIME [epoch: 10.4 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9583986843592314		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 2.9583986843592314 | validation: 3.2103053578637812]
	TIME [epoch: 10.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585221938410373		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 2.9585221938410373 | validation: 3.2088055464065635]
	TIME [epoch: 10.4 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9584449987641177		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 2.9584449987641177 | validation: 3.208924249690681]
	TIME [epoch: 10.4 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960122408590576		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 2.960122408590576 | validation: 3.223523769592906]
	TIME [epoch: 10.4 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9604346528113177		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 2.9604346528113177 | validation: 3.2134600334154944]
	TIME [epoch: 10.4 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9594746364808855		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 2.9594746364808855 | validation: 3.1954274994724314]
	TIME [epoch: 10.4 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525904997734997		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 2.9525904997734997 | validation: 3.2105009202925774]
	TIME [epoch: 10.4 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525290363980234		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 2.9525290363980234 | validation: 3.214815909618784]
	TIME [epoch: 10.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953832098881895		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 2.953832098881895 | validation: 3.2109244096674874]
	TIME [epoch: 10.4 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524281069836404		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 2.9524281069836404 | validation: 3.2004076703678037]
	TIME [epoch: 10.4 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954294801785997		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 2.954294801785997 | validation: 3.2081815738866397]
	TIME [epoch: 10.4 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957404682917939		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 2.957404682917939 | validation: 3.2089737395943696]
	TIME [epoch: 10.4 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531815588175667		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 2.9531815588175667 | validation: 3.2055996804111726]
	TIME [epoch: 10.4 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504431346581015		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 2.9504431346581015 | validation: 3.2212220297047667]
	TIME [epoch: 10.4 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952125912735124		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 2.952125912735124 | validation: 3.2014671730905344]
	TIME [epoch: 10.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9522393467455106		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 2.9522393467455106 | validation: 3.2006888132751032]
	TIME [epoch: 10.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9601712315696207		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 2.9601712315696207 | validation: 3.217291830643312]
	TIME [epoch: 10.4 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536472199520225		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 2.9536472199520225 | validation: 3.216975947090359]
	TIME [epoch: 10.3 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568883281167033		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 2.9568883281167033 | validation: 3.217976675775286]
	TIME [epoch: 10.4 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953746194033134		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 2.953746194033134 | validation: 3.202599888557687]
	TIME [epoch: 10.4 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500726680743776		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 2.9500726680743776 | validation: 3.2108840391295765]
	TIME [epoch: 10.4 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956977848371219		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 2.956977848371219 | validation: 3.2114020174187603]
	TIME [epoch: 10.3 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511633839045026		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 2.9511633839045026 | validation: 3.21577176839532]
	TIME [epoch: 10.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959724887638643		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 2.959724887638643 | validation: 3.221346291225572]
	TIME [epoch: 10.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955358400199299		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 2.955358400199299 | validation: 3.2047392187012806]
	TIME [epoch: 10.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952403911275688		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 2.952403911275688 | validation: 3.208133830626396]
	TIME [epoch: 10.4 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954529620770567		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 2.954529620770567 | validation: 3.212770438407082]
	TIME [epoch: 10.4 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9592095043665427		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 2.9592095043665427 | validation: 3.2180130129075075]
	TIME [epoch: 10.3 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961875516859699		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 2.961875516859699 | validation: 3.2107067607675157]
	TIME [epoch: 10.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579741999969675		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 2.9579741999969675 | validation: 3.218052148765387]
	TIME [epoch: 10.4 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542121293470984		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 2.9542121293470984 | validation: 3.2147327137181914]
	TIME [epoch: 10.4 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954008897371659		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 2.954008897371659 | validation: 3.219769787708902]
	TIME [epoch: 10.4 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550800495260967		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 2.9550800495260967 | validation: 3.207585048183131]
	TIME [epoch: 10.4 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556899112346713		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 2.9556899112346713 | validation: 3.2296064819530415]
	TIME [epoch: 10.3 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95299394945472		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 2.95299394945472 | validation: 3.2190665551521227]
	TIME [epoch: 10.4 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9554174964817235		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 2.9554174964817235 | validation: 3.2230610021902395]
	TIME [epoch: 10.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954891811446074		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 2.954891811446074 | validation: 3.212495293731123]
	TIME [epoch: 10.4 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956107508762172		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 2.956107508762172 | validation: 3.216487659660621]
	TIME [epoch: 10.4 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954963501138926		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 2.954963501138926 | validation: 3.209100994041064]
	TIME [epoch: 10.4 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9598714280961134		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 2.9598714280961134 | validation: 3.21929930194879]
	TIME [epoch: 10.4 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960950957342959		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 2.960950957342959 | validation: 3.2100774958506935]
	TIME [epoch: 10.3 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954542818996342		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 2.954542818996342 | validation: 3.202664235061322]
	TIME [epoch: 10.4 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545358162313202		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 2.9545358162313202 | validation: 3.2119032053606125]
	TIME [epoch: 10.4 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956652531590079		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 2.956652531590079 | validation: 3.20194907211477]
	TIME [epoch: 10.4 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957431857342546		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 2.957431857342546 | validation: 3.2035860306587227]
	TIME [epoch: 10.4 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958069904409658		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 2.958069904409658 | validation: 3.208820275980912]
	TIME [epoch: 10.4 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9638619202757286		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 2.9638619202757286 | validation: 3.210523453394728]
	TIME [epoch: 10.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955844899372935		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 2.955844899372935 | validation: 3.2224594993524263]
	TIME [epoch: 10.4 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558255768291954		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 2.9558255768291954 | validation: 3.21255314168655]
	TIME [epoch: 10.4 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950426894108335		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 2.950426894108335 | validation: 3.205206466515661]
	TIME [epoch: 10.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952961727828657		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 2.952961727828657 | validation: 3.207822409129578]
	TIME [epoch: 10.4 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9548237527789096		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 2.9548237527789096 | validation: 3.2217230766082934]
	TIME [epoch: 10.4 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545914203277213		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 2.9545914203277213 | validation: 3.221411001217975]
	TIME [epoch: 10.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953940765925		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 2.953940765925 | validation: 3.2177010476758627]
	TIME [epoch: 10.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953618200689802		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 2.953618200689802 | validation: 3.206513402215702]
	TIME [epoch: 10.4 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535557985229732		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 2.9535557985229732 | validation: 3.229100806820785]
	TIME [epoch: 10.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953153075305034		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 2.953153075305034 | validation: 3.22605592738146]
	TIME [epoch: 10.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542013258737763		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 2.9542013258737763 | validation: 3.2087198016178884]
	TIME [epoch: 10.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546888636455866		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 2.9546888636455866 | validation: 3.2131453969874624]
	TIME [epoch: 10.4 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9470076523999835		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 2.9470076523999835 | validation: 3.217514430309028]
	TIME [epoch: 10.4 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95674174971952		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 2.95674174971952 | validation: 3.219788001003019]
	TIME [epoch: 10.3 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538323876548622		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 2.9538323876548622 | validation: 3.2299579330017503]
	TIME [epoch: 10.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.967284221160869		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 2.967284221160869 | validation: 3.215915533535422]
	TIME [epoch: 10.4 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955116081667936		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 2.955116081667936 | validation: 3.2156931359124536]
	TIME [epoch: 10.4 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9644719760718026		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 2.9644719760718026 | validation: 3.206908920926427]
	TIME [epoch: 10.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952323051001575		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 2.952323051001575 | validation: 3.1976744256093004]
	TIME [epoch: 10.3 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511338109905205		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 2.9511338109905205 | validation: 3.2098103191523695]
	TIME [epoch: 10.4 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955071558300072		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 2.955071558300072 | validation: 3.2205329994414225]
	TIME [epoch: 10.4 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528896548084607		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 2.9528896548084607 | validation: 3.210099863265293]
	TIME [epoch: 10.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957099934824443		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 2.957099934824443 | validation: 3.206131763734405]
	TIME [epoch: 10.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9580087734920113		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 2.9580087734920113 | validation: 3.2026002072518556]
	TIME [epoch: 10.4 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9608410730054144		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 2.9608410730054144 | validation: 3.2226994463629257]
	TIME [epoch: 10.6 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954247129128186		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 2.954247129128186 | validation: 3.206801698128258]
	TIME [epoch: 10.4 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9533635112850343		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 2.9533635112850343 | validation: 3.2154117084378426]
	TIME [epoch: 10.4 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954318252835992		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 2.954318252835992 | validation: 3.2141918682703468]
	TIME [epoch: 10.4 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950541285116383		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 2.950541285116383 | validation: 3.2174003972740173]
	TIME [epoch: 10.4 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951750442217535		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 2.951750442217535 | validation: 3.2079909909826587]
	TIME [epoch: 10.4 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520987368206475		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 2.9520987368206475 | validation: 3.2098491815227237]
	TIME [epoch: 10.4 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961746051065821		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 2.961746051065821 | validation: 3.20215037340328]
	TIME [epoch: 10.4 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.962320144113444		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 2.962320144113444 | validation: 3.221047942554893]
	TIME [epoch: 10.4 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9644734035021054		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 2.9644734035021054 | validation: 3.2152253233620502]
	TIME [epoch: 10.4 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9598202942156457		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 2.9598202942156457 | validation: 3.226795202870716]
	TIME [epoch: 10.4 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957337132285491		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 2.957337132285491 | validation: 3.1974132639389627]
	TIME [epoch: 10.4 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956489466414488		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 2.956489466414488 | validation: 3.2026107183804844]
	TIME [epoch: 10.4 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958919420633438		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 2.958919420633438 | validation: 3.210509928344006]
	TIME [epoch: 10.4 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957238879255015		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 2.957238879255015 | validation: 3.2121112724442806]
	TIME [epoch: 10.4 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951616588037747		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 2.951616588037747 | validation: 3.2136155251297165]
	TIME [epoch: 10.4 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9576948925689144		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 2.9576948925689144 | validation: 3.223637043315576]
	TIME [epoch: 10.4 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9617914099096665		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 2.9617914099096665 | validation: 3.2116171845067014]
	TIME [epoch: 10.4 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9572257626893603		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 2.9572257626893603 | validation: 3.206911536873233]
	TIME [epoch: 10.4 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9530543413748056		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 2.9530543413748056 | validation: 3.192182812840655]
	TIME [epoch: 10.4 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9605812707748997		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 2.9605812707748997 | validation: 3.1992330153982693]
	TIME [epoch: 10.4 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955923208472574		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 2.955923208472574 | validation: 3.204798690279107]
	TIME [epoch: 10.4 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536699963299333		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 2.9536699963299333 | validation: 3.224349621149942]
	TIME [epoch: 10.4 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95424205173409		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 2.95424205173409 | validation: 3.2151941223311984]
	TIME [epoch: 10.4 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527271148338334		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 2.9527271148338334 | validation: 3.206131929575472]
	TIME [epoch: 10.4 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558926052635366		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 2.9558926052635366 | validation: 3.206525192532106]
	TIME [epoch: 10.4 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956938968819115		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 2.956938968819115 | validation: 3.210505744880908]
	TIME [epoch: 10.4 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9549636358133116		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 2.9549636358133116 | validation: 3.2211627517308545]
	TIME [epoch: 10.4 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.960444578615165		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 2.960444578615165 | validation: 3.212982042848486]
	TIME [epoch: 10.4 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961853210979166		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 2.961853210979166 | validation: 3.222788891474105]
	TIME [epoch: 10.4 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526793587298505		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 2.9526793587298505 | validation: 3.2047970643666295]
	TIME [epoch: 10.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958304743638858		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 2.958304743638858 | validation: 3.219020981204884]
	TIME [epoch: 10.4 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955064483963436		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 2.955064483963436 | validation: 3.2126503128456334]
	TIME [epoch: 10.4 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952912202837809		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 2.952912202837809 | validation: 3.2084676490981643]
	TIME [epoch: 10.4 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95192591525017		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 2.95192591525017 | validation: 3.216343900921468]
	TIME [epoch: 10.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541607597285156		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 2.9541607597285156 | validation: 3.2147902641098334]
	TIME [epoch: 10.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956178871552497		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 2.956178871552497 | validation: 3.2098413820756924]
	TIME [epoch: 10.4 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500653180005547		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 2.9500653180005547 | validation: 3.224137836273251]
	TIME [epoch: 10.4 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951428953887317		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 2.951428953887317 | validation: 3.2023976114385992]
	TIME [epoch: 10.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520818950826833		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 2.9520818950826833 | validation: 3.217671334573586]
	TIME [epoch: 10.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952272258445985		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 2.952272258445985 | validation: 3.2238227529938777]
	TIME [epoch: 10.4 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538846138739983		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 2.9538846138739983 | validation: 3.211060920261417]
	TIME [epoch: 10.4 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552105264624258		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 2.9552105264624258 | validation: 3.2136588394414116]
	TIME [epoch: 10.4 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536134009978214		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 2.9536134009978214 | validation: 3.217158377446895]
	TIME [epoch: 10.4 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551813931625297		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 2.9551813931625297 | validation: 3.2088775027534617]
	TIME [epoch: 10.4 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954149069056419		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 2.954149069056419 | validation: 3.213594298595077]
	TIME [epoch: 10.4 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951665086115687		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 2.951665086115687 | validation: 3.2195099002312544]
	TIME [epoch: 10.4 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955793878214734		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 2.955793878214734 | validation: 3.216646597194458]
	TIME [epoch: 10.4 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952959544134486		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 2.952959544134486 | validation: 3.2088544616959633]
	TIME [epoch: 10.4 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543585697568227		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 2.9543585697568227 | validation: 3.203202700404535]
	TIME [epoch: 10.4 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579357948259775		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 2.9579357948259775 | validation: 3.2026022588875156]
	TIME [epoch: 10.4 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532330729525325		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 2.9532330729525325 | validation: 3.212070611191823]
	TIME [epoch: 10.4 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524985003921156		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 2.9524985003921156 | validation: 3.1982371915585386]
	TIME [epoch: 10.4 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954065211428953		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 2.954065211428953 | validation: 3.212066271028565]
	TIME [epoch: 10.4 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568612523507816		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 2.9568612523507816 | validation: 3.1970604590639646]
	TIME [epoch: 10.4 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955508733421733		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 2.955508733421733 | validation: 3.202183712303248]
	TIME [epoch: 10.4 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555180258029146		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 2.9555180258029146 | validation: 3.217221196409448]
	TIME [epoch: 10.4 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508826004901474		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 2.9508826004901474 | validation: 3.211585593319826]
	TIME [epoch: 10.4 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535025936797545		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 2.9535025936797545 | validation: 3.215080193754366]
	TIME [epoch: 10.4 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954118783978913		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 2.954118783978913 | validation: 3.21947956107136]
	TIME [epoch: 10.4 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951232423106991		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 2.951232423106991 | validation: 3.202415338854624]
	TIME [epoch: 10.4 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9529697548682208		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 2.9529697548682208 | validation: 3.220656717989159]
	TIME [epoch: 10.4 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958917453042745		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 2.958917453042745 | validation: 3.2125270097799445]
	TIME [epoch: 10.4 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95180223978615		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 2.95180223978615 | validation: 3.206150445773977]
	TIME [epoch: 10.4 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9566675515157828		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 2.9566675515157828 | validation: 3.2130438042386817]
	TIME [epoch: 10.4 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9564433455252153		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 2.9564433455252153 | validation: 3.2199677634114083]
	TIME [epoch: 10.4 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9479923414048033		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 2.9479923414048033 | validation: 3.2288988709085364]
	TIME [epoch: 10.4 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9592874028392746		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 2.9592874028392746 | validation: 3.21205264394053]
	TIME [epoch: 10.4 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958527389366822		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 2.958527389366822 | validation: 3.215793166487589]
	TIME [epoch: 10.4 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95660383630327		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 2.95660383630327 | validation: 3.2094976302287943]
	TIME [epoch: 10.4 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517194893745544		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 2.9517194893745544 | validation: 3.2218608945186062]
	TIME [epoch: 10.4 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550967070954277		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 2.9550967070954277 | validation: 3.206056448456367]
	TIME [epoch: 10.4 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523121628550806		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 2.9523121628550806 | validation: 3.2143209266036115]
	TIME [epoch: 10.4 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953050150482554		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 2.953050150482554 | validation: 3.2232018568393936]
	TIME [epoch: 10.4 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552589245295833		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 2.9552589245295833 | validation: 3.2155223828599073]
	TIME [epoch: 10.4 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515416733986957		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 2.9515416733986957 | validation: 3.206228671934]
	TIME [epoch: 10.4 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9442695295327477		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 2.9442695295327477 | validation: 3.21499722817781]
	TIME [epoch: 10.4 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546922832094835		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 2.9546922832094835 | validation: 3.213347916894345]
	TIME [epoch: 10.4 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551476706637163		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 2.9551476706637163 | validation: 3.2110827480811714]
	TIME [epoch: 10.4 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9530141420421385		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 2.9530141420421385 | validation: 3.195070182793735]
	TIME [epoch: 10.4 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950655812960561		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 2.950655812960561 | validation: 3.2129949586986637]
	TIME [epoch: 10.4 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958152674106352		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 2.958152674106352 | validation: 3.1992222084883397]
	TIME [epoch: 10.4 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951862655523012		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 2.951862655523012 | validation: 3.2120239043123924]
	TIME [epoch: 10.4 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955828508966209		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 2.955828508966209 | validation: 3.2138135160851897]
	TIME [epoch: 10.4 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955719822314679		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 2.955719822314679 | validation: 3.1987489356886547]
	TIME [epoch: 10.4 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953695921357728		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 2.953695921357728 | validation: 3.2095264928420546]
	TIME [epoch: 10.4 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957340558124416		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 2.957340558124416 | validation: 3.214025358731191]
	TIME [epoch: 10.4 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953558043670269		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 2.953558043670269 | validation: 3.213266897646114]
	TIME [epoch: 10.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536473139596806		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 2.9536473139596806 | validation: 3.214018245175258]
	TIME [epoch: 10.4 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556250337901924		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 2.9556250337901924 | validation: 3.2065061180835803]
	TIME [epoch: 10.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95469915159231		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 2.95469915159231 | validation: 3.2091539763612005]
	TIME [epoch: 10.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952151932606		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 2.952151932606 | validation: 3.205343463988108]
	TIME [epoch: 10.4 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956081791270649		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 2.956081791270649 | validation: 3.207995540256619]
	TIME [epoch: 10.4 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9569949856043345		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 2.9569949856043345 | validation: 3.2038332119473374]
	TIME [epoch: 10.4 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561012290409034		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 2.9561012290409034 | validation: 3.2083473911262486]
	TIME [epoch: 10.4 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553696169498975		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 2.9553696169498975 | validation: 3.205801591325485]
	TIME [epoch: 10.4 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538863095010273		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 2.9538863095010273 | validation: 3.2037391560302684]
	TIME [epoch: 10.4 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95754640562523		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 2.95754640562523 | validation: 3.208438521149859]
	TIME [epoch: 10.4 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953711630196625		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 2.953711630196625 | validation: 3.2105360354878294]
	TIME [epoch: 10.4 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9462223824048466		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 2.9462223824048466 | validation: 3.2189424565958213]
	TIME [epoch: 10.4 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957647182031516		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 2.957647182031516 | validation: 3.208732380476352]
	TIME [epoch: 10.4 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949245596992344		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 2.949245596992344 | validation: 3.2045705410014262]
	TIME [epoch: 10.4 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949606080138944		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 2.949606080138944 | validation: 3.206218172171602]
	TIME [epoch: 10.4 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527027187684993		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 2.9527027187684993 | validation: 3.211193768950931]
	TIME [epoch: 10.4 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9490010125275825		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 2.9490010125275825 | validation: 3.214502396517565]
	TIME [epoch: 10.4 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9563934311935967		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 2.9563934311935967 | validation: 3.211917795703619]
	TIME [epoch: 10.4 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956025604161831		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 2.956025604161831 | validation: 3.2062363765571464]
	TIME [epoch: 10.4 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949787367667215		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 2.949787367667215 | validation: 3.2137784426766087]
	TIME [epoch: 10.4 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956826655869027		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 2.956826655869027 | validation: 3.218441745088158]
	TIME [epoch: 10.4 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955092909817753		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 2.955092909817753 | validation: 3.221947677064297]
	TIME [epoch: 10.4 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500512798367295		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 2.9500512798367295 | validation: 3.2041647206583286]
	TIME [epoch: 10.4 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541355304527577		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 2.9541355304527577 | validation: 3.2021263129026316]
	TIME [epoch: 10.4 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95687200410223		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 2.95687200410223 | validation: 3.214067163211073]
	TIME [epoch: 10.4 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956019977205382		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 2.956019977205382 | validation: 3.2182506393811967]
	TIME [epoch: 10.4 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9472194765158393		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 2.9472194765158393 | validation: 3.2137466860280983]
	TIME [epoch: 10.4 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9564937565671787		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 2.9564937565671787 | validation: 3.220582049869407]
	TIME [epoch: 10.4 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956943370174005		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 2.956943370174005 | validation: 3.2154550128445125]
	TIME [epoch: 10.4 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9572687445990615		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 2.9572687445990615 | validation: 3.2172715386579247]
	TIME [epoch: 10.4 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555333315137404		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 2.9555333315137404 | validation: 3.2184790392587774]
	TIME [epoch: 10.4 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954997979514329		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 2.954997979514329 | validation: 3.231369578444827]
	TIME [epoch: 10.4 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952517376280497		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 2.952517376280497 | validation: 3.2117553983110234]
	TIME [epoch: 10.4 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9618494806290423		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 2.9618494806290423 | validation: 3.2026648805194258]
	TIME [epoch: 10.4 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953441133310432		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 2.953441133310432 | validation: 3.2128469748738677]
	TIME [epoch: 10.4 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9582409128650395		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 2.9582409128650395 | validation: 3.212062758362839]
	TIME [epoch: 10.4 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950112434569615		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 2.950112434569615 | validation: 3.214153913736272]
	TIME [epoch: 10.4 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954149227231011		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 2.954149227231011 | validation: 3.212416705562514]
	TIME [epoch: 10.4 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509729534795475		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 2.9509729534795475 | validation: 3.203923661031206]
	TIME [epoch: 10.4 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550893982719475		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 2.9550893982719475 | validation: 3.2051583161959583]
	TIME [epoch: 10.4 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955677599707782		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 2.955677599707782 | validation: 3.206460539737291]
	TIME [epoch: 10.4 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9575715546540122		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 2.9575715546540122 | validation: 3.2121707807148896]
	TIME [epoch: 10.4 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956841794945282		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 2.956841794945282 | validation: 3.2029278657172098]
	TIME [epoch: 10.4 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560962311191155		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 2.9560962311191155 | validation: 3.2084252081545763]
	TIME [epoch: 10.4 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558716866455588		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 2.9558716866455588 | validation: 3.2136944967672316]
	TIME [epoch: 10.4 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539387495979		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 2.9539387495979 | validation: 3.222500913028655]
	TIME [epoch: 10.4 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561722379994455		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 2.9561722379994455 | validation: 3.2088883860213993]
	TIME [epoch: 10.4 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953276973787478		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 2.953276973787478 | validation: 3.2097848010848296]
	TIME [epoch: 10.4 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9586746050165393		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 2.9586746050165393 | validation: 3.2102923745016745]
	TIME [epoch: 10.4 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952535193793398		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 2.952535193793398 | validation: 3.205253130142379]
	TIME [epoch: 10.4 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552948540190522		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 2.9552948540190522 | validation: 3.209954268195945]
	TIME [epoch: 10.4 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951201645284644		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 2.951201645284644 | validation: 3.2141365634769192]
	TIME [epoch: 10.4 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538775437615827		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 2.9538775437615827 | validation: 3.223337716479341]
	TIME [epoch: 10.4 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9610768846387057		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 2.9610768846387057 | validation: 3.20283151124916]
	TIME [epoch: 10.4 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9557659589665533		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 2.9557659589665533 | validation: 3.203537494066402]
	TIME [epoch: 10.4 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493865654242666		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 2.9493865654242666 | validation: 3.2163855896535667]
	TIME [epoch: 10.4 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517586273520005		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 2.9517586273520005 | validation: 3.2046844005010895]
	TIME [epoch: 10.4 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953018361733495		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 2.953018361733495 | validation: 3.210601746207321]
	TIME [epoch: 10.4 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553776100403737		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 2.9553776100403737 | validation: 3.2015139854367045]
	TIME [epoch: 10.4 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952053197489452		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 2.952053197489452 | validation: 3.2015094129974186]
	TIME [epoch: 10.4 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953904095498717		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 2.953904095498717 | validation: 3.2142150800231786]
	TIME [epoch: 10.4 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9573082190013626		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 2.9573082190013626 | validation: 3.2137882860443874]
	TIME [epoch: 10.4 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955042710701553		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 2.955042710701553 | validation: 3.221149388327033]
	TIME [epoch: 10.4 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561495007712453		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 2.9561495007712453 | validation: 3.2016238925250504]
	TIME [epoch: 10.4 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9521758475982045		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 2.9521758475982045 | validation: 3.2056955703633663]
	TIME [epoch: 10.4 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948956410160158		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 2.948956410160158 | validation: 3.206478019650481]
	TIME [epoch: 10.4 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520182349136217		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 2.9520182349136217 | validation: 3.207277867600912]
	TIME [epoch: 10.4 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956385045448048		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 2.956385045448048 | validation: 3.215936154658475]
	TIME [epoch: 10.4 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9572483254186204		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 2.9572483254186204 | validation: 3.210516409684109]
	TIME [epoch: 10.4 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949045558831709		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 2.949045558831709 | validation: 3.2174868182588443]
	TIME [epoch: 10.4 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538469133525207		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 2.9538469133525207 | validation: 3.2181231632450324]
	TIME [epoch: 10.4 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538650163841815		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 2.9538650163841815 | validation: 3.2049287974486846]
	TIME [epoch: 10.4 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550524112714247		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 2.9550524112714247 | validation: 3.2107761421650687]
	TIME [epoch: 10.4 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527754782973545		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 2.9527754782973545 | validation: 3.2142401761829102]
	TIME [epoch: 10.4 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9533607911371758		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 2.9533607911371758 | validation: 3.209850866013247]
	TIME [epoch: 10.4 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496521547087733		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 2.9496521547087733 | validation: 3.216931895530804]
	TIME [epoch: 10.4 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558063357927535		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 2.9558063357927535 | validation: 3.2085558452074943]
	TIME [epoch: 10.4 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505693862042057		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 2.9505693862042057 | validation: 3.2115929786812525]
	TIME [epoch: 10.4 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954627654324418		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 2.954627654324418 | validation: 3.2137062732334303]
	TIME [epoch: 10.4 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951209185037166		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 2.951209185037166 | validation: 3.215151632193183]
	TIME [epoch: 10.4 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9512927296913345		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 2.9512927296913345 | validation: 3.210993563686626]
	TIME [epoch: 10.4 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954857694357332		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 2.954857694357332 | validation: 3.20388924369001]
	TIME [epoch: 10.4 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951286684602411		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 2.951286684602411 | validation: 3.220185727532055]
	TIME [epoch: 10.4 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956287550743205		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 2.956287550743205 | validation: 3.2128683986482987]
	TIME [epoch: 10.4 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.961518008379965		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 2.961518008379965 | validation: 3.207839331236211]
	TIME [epoch: 10.4 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555042420024846		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 2.9555042420024846 | validation: 3.205048734227792]
	TIME [epoch: 10.4 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550560152214165		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 2.9550560152214165 | validation: 3.2129557576361845]
	TIME [epoch: 10.4 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542054000634526		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 2.9542054000634526 | validation: 3.207973401128438]
	TIME [epoch: 10.4 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536739375910015		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 2.9536739375910015 | validation: 3.211691453602417]
	TIME [epoch: 10.4 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9580411158919113		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 2.9580411158919113 | validation: 3.2087359542812135]
	TIME [epoch: 10.4 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9488324446919068		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 2.9488324446919068 | validation: 3.213270143787359]
	TIME [epoch: 10.4 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531651148887565		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 2.9531651148887565 | validation: 3.2141246016443024]
	TIME [epoch: 10.4 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954581118327561		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 2.954581118327561 | validation: 3.203196391893079]
	TIME [epoch: 10.4 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95447134303676		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 2.95447134303676 | validation: 3.2109279175010643]
	TIME [epoch: 10.4 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547340006463743		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 2.9547340006463743 | validation: 3.196173392250596]
	TIME [epoch: 10.4 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957182545663456		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 2.957182545663456 | validation: 3.2048010633069226]
	TIME [epoch: 10.4 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519817563070236		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 2.9519817563070236 | validation: 3.203777072481949]
	TIME [epoch: 10.4 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954658600864774		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 2.954658600864774 | validation: 3.2216469574762527]
	TIME [epoch: 10.4 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955554309176437		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 2.955554309176437 | validation: 3.2008563115798614]
	TIME [epoch: 10.4 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953126956437317		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 2.953126956437317 | validation: 3.2061439008491597]
	TIME [epoch: 10.4 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570732774739503		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 2.9570732774739503 | validation: 3.202684746629783]
	TIME [epoch: 10.4 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511527705357894		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 2.9511527705357894 | validation: 3.207060615776596]
	TIME [epoch: 10.4 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954038477726562		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 2.954038477726562 | validation: 3.207554267787971]
	TIME [epoch: 10.4 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955885412257922		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 2.955885412257922 | validation: 3.2133509214351403]
	TIME [epoch: 10.4 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95454769293581		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 2.95454769293581 | validation: 3.213214822224253]
	TIME [epoch: 10.4 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562884591697345		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 2.9562884591697345 | validation: 3.2142387651982984]
	TIME [epoch: 10.4 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9593119168924584		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 2.9593119168924584 | validation: 3.2115885197972784]
	TIME [epoch: 10.4 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547996563506196		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 2.9547996563506196 | validation: 3.207284440972249]
	TIME [epoch: 10.4 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959389012121833		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 2.959389012121833 | validation: 3.2066875905346013]
	TIME [epoch: 10.4 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954466278499246		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 2.954466278499246 | validation: 3.213458008238691]
	TIME [epoch: 10.4 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9529549700747655		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 2.9529549700747655 | validation: 3.202576466021034]
	TIME [epoch: 10.4 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954226407771244		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 2.954226407771244 | validation: 3.2157411588271714]
	TIME [epoch: 10.4 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953671699849778		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 2.953671699849778 | validation: 3.2011137855873755]
	TIME [epoch: 10.4 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537388547734826		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 2.9537388547734826 | validation: 3.211296453825269]
	TIME [epoch: 10.3 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515222349698		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 2.9515222349698 | validation: 3.2094558885581304]
	TIME [epoch: 10.4 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513110458804226		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 2.9513110458804226 | validation: 3.2047658371983636]
	TIME [epoch: 10.4 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955107682751764		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 2.955107682751764 | validation: 3.2106049491519024]
	TIME [epoch: 10.4 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95101558612343		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 2.95101558612343 | validation: 3.2156346293919547]
	TIME [epoch: 10.4 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953279239131323		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 2.953279239131323 | validation: 3.2067734926626645]
	TIME [epoch: 10.4 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543707495369347		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 2.9543707495369347 | validation: 3.2121270930441224]
	TIME [epoch: 10.4 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560763867104463		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 2.9560763867104463 | validation: 3.2151892445985757]
	TIME [epoch: 10.4 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9554749226792882		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 2.9554749226792882 | validation: 3.2054834623389468]
	TIME [epoch: 10.4 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9492627010862624		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 2.9492627010862624 | validation: 3.210300090915938]
	TIME [epoch: 10.4 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955735519167277		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 2.955735519167277 | validation: 3.2087020166585]
	TIME [epoch: 10.4 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957497029711249		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 2.957497029711249 | validation: 3.2205193307636253]
	TIME [epoch: 10.4 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9565139149255435		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 2.9565139149255435 | validation: 3.224050265956586]
	TIME [epoch: 10.4 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958418815905147		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 2.958418815905147 | validation: 3.2142809326313717]
	TIME [epoch: 10.4 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955121848432433		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 2.955121848432433 | validation: 3.2175064685815293]
	TIME [epoch: 10.4 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956962675510704		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 2.956962675510704 | validation: 3.208340182180656]
	TIME [epoch: 10.4 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540833595223277		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 2.9540833595223277 | validation: 3.203079054652397]
	TIME [epoch: 10.4 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526390948549777		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 2.9526390948549777 | validation: 3.213421656228746]
	TIME [epoch: 10.4 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957434395463119		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 2.957434395463119 | validation: 3.2090594777581645]
	TIME [epoch: 10.4 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9594467001656155		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 2.9594467001656155 | validation: 3.217178885844435]
	TIME [epoch: 10.4 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955448185313755		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 2.955448185313755 | validation: 3.2152196482813054]
	TIME [epoch: 10.4 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950020631285625		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 2.950020631285625 | validation: 3.2158659134221033]
	TIME [epoch: 10.4 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506480733464318		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 2.9506480733464318 | validation: 3.2109101898267887]
	TIME [epoch: 10.4 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950798110072881		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 2.950798110072881 | validation: 3.2083741050893155]
	TIME [epoch: 10.4 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9483081852203274		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 2.9483081852203274 | validation: 3.212363290738448]
	TIME [epoch: 10.4 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950072019486584		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 2.950072019486584 | validation: 3.212480279172648]
	TIME [epoch: 10.4 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528685266833046		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 2.9528685266833046 | validation: 3.2234189282012813]
	TIME [epoch: 10.4 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538356363509646		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 2.9538356363509646 | validation: 3.1965554863528887]
	TIME [epoch: 10.4 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523671244994456		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 2.9523671244994456 | validation: 3.214008628936733]
	TIME [epoch: 10.4 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495086359872014		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 2.9495086359872014 | validation: 3.2132721376935196]
	TIME [epoch: 10.4 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532451425915314		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 2.9532451425915314 | validation: 3.207738845687981]
	TIME [epoch: 10.4 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9512996185472664		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 2.9512996185472664 | validation: 3.208413269017309]
	TIME [epoch: 10.4 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954336335465939		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 2.954336335465939 | validation: 3.2078452515360714]
	TIME [epoch: 10.4 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953859680916771		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 2.953859680916771 | validation: 3.2152975504130725]
	TIME [epoch: 10.4 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949614714656328		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 2.949614714656328 | validation: 3.2090053892439703]
	TIME [epoch: 10.4 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545152457286603		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 2.9545152457286603 | validation: 3.2235527420439087]
	TIME [epoch: 10.4 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955078460798648		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 2.955078460798648 | validation: 3.2154183162202004]
	TIME [epoch: 10.4 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524693009675995		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 2.9524693009675995 | validation: 3.2119806166388725]
	TIME [epoch: 10.4 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952615682339826		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 2.952615682339826 | validation: 3.205310411174642]
	TIME [epoch: 10.4 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951930539192248		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 2.951930539192248 | validation: 3.215065894103893]
	TIME [epoch: 10.4 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955373591654093		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 2.955373591654093 | validation: 3.213034983523581]
	TIME [epoch: 10.4 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502848631512473		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 2.9502848631512473 | validation: 3.1980409209215788]
	TIME [epoch: 10.4 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516906503916616		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 2.9516906503916616 | validation: 3.2078343697209553]
	TIME [epoch: 10.4 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507544945640243		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 2.9507544945640243 | validation: 3.206899301755793]
	TIME [epoch: 10.4 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951081457361845		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 2.951081457361845 | validation: 3.2117657847334535]
	TIME [epoch: 10.4 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957579320392293		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 2.957579320392293 | validation: 3.2128962233684892]
	TIME [epoch: 10.4 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952285881770519		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 2.952285881770519 | validation: 3.207039185389368]
	TIME [epoch: 10.4 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9533455996756253		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 2.9533455996756253 | validation: 3.21383934175063]
	TIME [epoch: 10.4 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957912264925294		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 2.957912264925294 | validation: 3.2116824070156373]
	TIME [epoch: 10.4 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953079302255708		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 2.953079302255708 | validation: 3.1973490728287794]
	TIME [epoch: 10.4 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537724999361528		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 2.9537724999361528 | validation: 3.2084972563132976]
	TIME [epoch: 10.4 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504546228932833		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 2.9504546228932833 | validation: 3.2051499637966643]
	TIME [epoch: 10.4 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524965218874932		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 2.9524965218874932 | validation: 3.2076717633788223]
	TIME [epoch: 10.4 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948091672641911		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 2.948091672641911 | validation: 3.2179647872716703]
	TIME [epoch: 10.4 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951167640346445		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 2.951167640346445 | validation: 3.212812957979841]
	TIME [epoch: 10.4 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9498694995780195		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 2.9498694995780195 | validation: 3.208864172912042]
	TIME [epoch: 10.4 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950933923434844		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 2.950933923434844 | validation: 3.210173435136986]
	TIME [epoch: 10.4 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953411506726776		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 2.953411506726776 | validation: 3.2169385154446095]
	TIME [epoch: 10.4 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955617372752697		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 2.955617372752697 | validation: 3.209716554815985]
	TIME [epoch: 10.4 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951038082025125		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 2.951038082025125 | validation: 3.2178046686635917]
	TIME [epoch: 10.4 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956936388727717		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 2.956936388727717 | validation: 3.2185280161479852]
	TIME [epoch: 10.4 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95191451133868		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 2.95191451133868 | validation: 3.2145255226922593]
	TIME [epoch: 10.4 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956989370022826		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 2.956989370022826 | validation: 3.2098988892154363]
	TIME [epoch: 10.4 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9564748209061027		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 2.9564748209061027 | validation: 3.221213036912321]
	TIME [epoch: 10.4 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516971137067083		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 2.9516971137067083 | validation: 3.2082019694311783]
	TIME [epoch: 10.4 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9523667561354383		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 2.9523667561354383 | validation: 3.2041427652883065]
	TIME [epoch: 10.4 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550439233723735		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 2.9550439233723735 | validation: 3.2021846775159846]
	TIME [epoch: 10.4 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9596435023954033		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 2.9596435023954033 | validation: 3.208101385800145]
	TIME [epoch: 10.4 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538665020778594		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 2.9538665020778594 | validation: 3.206774438441605]
	TIME [epoch: 10.4 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515217429508063		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 2.9515217429508063 | validation: 3.208875887607529]
	TIME [epoch: 10.4 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552521173683957		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 2.9552521173683957 | validation: 3.210835004500926]
	TIME [epoch: 10.4 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552241549649203		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 2.9552241549649203 | validation: 3.202795682411196]
	TIME [epoch: 10.4 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525756699767998		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 2.9525756699767998 | validation: 3.2023600357264197]
	TIME [epoch: 10.4 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9591049707187898		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 2.9591049707187898 | validation: 3.212507904430514]
	TIME [epoch: 10.4 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949359877672152		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 2.949359877672152 | validation: 3.1983285692039365]
	TIME [epoch: 10.4 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524293699671427		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 2.9524293699671427 | validation: 3.215646084425052]
	TIME [epoch: 10.4 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954689554393716		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 2.954689554393716 | validation: 3.2199747941478156]
	TIME [epoch: 10.4 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949933919608001		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 2.949933919608001 | validation: 3.2163583762355974]
	TIME [epoch: 10.4 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957434106681704		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 2.957434106681704 | validation: 3.2179941363815483]
	TIME [epoch: 10.4 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9573049064255934		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 2.9573049064255934 | validation: 3.2082596994209087]
	TIME [epoch: 10.4 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508839670511		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 2.9508839670511 | validation: 3.2122129942579294]
	TIME [epoch: 10.4 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556256432647636		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 2.9556256432647636 | validation: 3.2146539836566728]
	TIME [epoch: 10.4 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956432893553209		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 2.956432893553209 | validation: 3.2103975069659687]
	TIME [epoch: 10.4 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571806926346142		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 2.9571806926346142 | validation: 3.2136697428984475]
	TIME [epoch: 10.4 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950523597318705		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 2.950523597318705 | validation: 3.211712652251555]
	TIME [epoch: 10.4 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9514238224514604		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 2.9514238224514604 | validation: 3.204852690934361]
	TIME [epoch: 10.4 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552188663664323		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 2.9552188663664323 | validation: 3.215424803447057]
	TIME [epoch: 10.4 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513820725518283		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 2.9513820725518283 | validation: 3.2183730302466182]
	TIME [epoch: 10.4 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511409997432496		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 2.9511409997432496 | validation: 3.2006919831994014]
	TIME [epoch: 10.4 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9572591722391914		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 2.9572591722391914 | validation: 3.2025905346137895]
	TIME [epoch: 10.4 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9464046503586383		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 2.9464046503586383 | validation: 3.206710123897435]
	TIME [epoch: 10.4 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543848074425063		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 2.9543848074425063 | validation: 3.2019221724358484]
	TIME [epoch: 10.4 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954860583300794		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 2.954860583300794 | validation: 3.2116657620817985]
	TIME [epoch: 10.4 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502341505076344		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 2.9502341505076344 | validation: 3.205578259262868]
	TIME [epoch: 10.4 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95480783319485		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 2.95480783319485 | validation: 3.2160865760098853]
	TIME [epoch: 10.4 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9516269756594924		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 2.9516269756594924 | validation: 3.208518948201641]
	TIME [epoch: 10.4 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9544956205044683		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 2.9544956205044683 | validation: 3.2220231899938985]
	TIME [epoch: 10.4 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579293797621182		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 2.9579293797621182 | validation: 3.214244688482562]
	TIME [epoch: 10.4 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531092205488965		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 2.9531092205488965 | validation: 3.212918927305654]
	TIME [epoch: 10.4 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509728009231297		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 2.9509728009231297 | validation: 3.2005727539121676]
	TIME [epoch: 10.4 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505897984446277		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 2.9505897984446277 | validation: 3.207575306655406]
	TIME [epoch: 10.4 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952742020468849		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 2.952742020468849 | validation: 3.2172266909398535]
	TIME [epoch: 10.4 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538226276171273		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 2.9538226276171273 | validation: 3.207645177999389]
	TIME [epoch: 10.4 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561528867962683		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 2.9561528867962683 | validation: 3.211057150487117]
	TIME [epoch: 10.4 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9549253197748757		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 2.9549253197748757 | validation: 3.213444410175272]
	TIME [epoch: 10.4 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9567545831342676		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 2.9567545831342676 | validation: 3.219905068572584]
	TIME [epoch: 10.4 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9503387813229516		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 2.9503387813229516 | validation: 3.2088831980550716]
	TIME [epoch: 10.4 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555406885045423		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 2.9555406885045423 | validation: 3.20904592565294]
	TIME [epoch: 10.4 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558404824896995		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 2.9558404824896995 | validation: 3.213575916399087]
	TIME [epoch: 10.4 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547725828419784		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 2.9547725828419784 | validation: 3.2090639075181877]
	TIME [epoch: 10.4 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954531148041295		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 2.954531148041295 | validation: 3.2133105019874404]
	TIME [epoch: 10.4 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954990246607224		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 2.954990246607224 | validation: 3.2121578051942627]
	TIME [epoch: 10.4 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543595126655564		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 2.9543595126655564 | validation: 3.218891498297417]
	TIME [epoch: 10.4 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955959971993397		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 2.955959971993397 | validation: 3.2046731582318886]
	TIME [epoch: 10.4 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949815032737684		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 2.949815032737684 | validation: 3.2192539147076356]
	TIME [epoch: 10.4 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513533986835427		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 2.9513533986835427 | validation: 3.2022390606824844]
	TIME [epoch: 10.4 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547927797123483		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 2.9547927797123483 | validation: 3.205423276192837]
	TIME [epoch: 10.4 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9597474809996287		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 2.9597474809996287 | validation: 3.2119252755204935]
	TIME [epoch: 10.4 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955408432324385		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 2.955408432324385 | validation: 3.2031915327946394]
	TIME [epoch: 10.4 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539605332305827		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 2.9539605332305827 | validation: 3.205823271708989]
	TIME [epoch: 10.4 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955853880651764		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 2.955853880651764 | validation: 3.210071048260252]
	TIME [epoch: 10.4 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9508397008310387		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 2.9508397008310387 | validation: 3.2178120149145006]
	TIME [epoch: 10.4 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541344997433154		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 2.9541344997433154 | validation: 3.2225577474543785]
	TIME [epoch: 10.4 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525765219559963		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 2.9525765219559963 | validation: 3.2095704909905023]
	TIME [epoch: 10.4 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953772177465073		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 2.953772177465073 | validation: 3.220710969833888]
	TIME [epoch: 10.4 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570317327337383		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 2.9570317327337383 | validation: 3.2010052952992063]
	TIME [epoch: 10.4 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528131719620108		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 2.9528131719620108 | validation: 3.2073145712948627]
	TIME [epoch: 10.4 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9563482917571604		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 2.9563482917571604 | validation: 3.215406141080866]
	TIME [epoch: 10.4 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585627989164776		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 2.9585627989164776 | validation: 3.211398495858512]
	TIME [epoch: 10.4 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9602495166885965		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 2.9602495166885965 | validation: 3.214451652739125]
	TIME [epoch: 10.4 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.959032405890156		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 2.959032405890156 | validation: 3.2108912728705343]
	TIME [epoch: 10.4 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95293386437583		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 2.95293386437583 | validation: 3.2128774812649876]
	TIME [epoch: 10.4 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9554870818076737		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 2.9554870818076737 | validation: 3.210473313893849]
	TIME [epoch: 10.4 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519212680469598		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 2.9519212680469598 | validation: 3.208088128787576]
	TIME [epoch: 10.4 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9501891715340856		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 2.9501891715340856 | validation: 3.2100287613041747]
	TIME [epoch: 10.4 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559931354030202		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 2.9559931354030202 | validation: 3.207113852562478]
	TIME [epoch: 10.4 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954001187329361		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 2.954001187329361 | validation: 3.20918306445003]
	TIME [epoch: 10.4 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95289447464341		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 2.95289447464341 | validation: 3.2017272900397398]
	TIME [epoch: 10.4 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9533948766436082		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 2.9533948766436082 | validation: 3.2029842551646333]
	TIME [epoch: 10.4 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955042316669321		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 2.955042316669321 | validation: 3.206121247050669]
	TIME [epoch: 10.4 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953615083915871		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 2.953615083915871 | validation: 3.2192021453490565]
	TIME [epoch: 10.4 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9500086482728536		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 2.9500086482728536 | validation: 3.211236950670906]
	TIME [epoch: 10.4 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955311154385594		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 2.955311154385594 | validation: 3.211766876122241]
	TIME [epoch: 10.4 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946669235085564		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 2.946669235085564 | validation: 3.206164898841598]
	TIME [epoch: 10.4 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525966375322015		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 2.9525966375322015 | validation: 3.2051149812561337]
	TIME [epoch: 10.4 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953011861758359		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 2.953011861758359 | validation: 3.211177679786979]
	TIME [epoch: 10.4 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958044531003269		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 2.958044531003269 | validation: 3.2180583237779947]
	TIME [epoch: 10.4 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9564207173266137		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 2.9564207173266137 | validation: 3.2116056028540028]
	TIME [epoch: 10.4 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9474792369685803		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 2.9474792369685803 | validation: 3.2147375415236974]
	TIME [epoch: 10.4 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527685188649757		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 2.9527685188649757 | validation: 3.2175422144739203]
	TIME [epoch: 10.4 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9554372731745904		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 2.9554372731745904 | validation: 3.2184874911302153]
	TIME [epoch: 10.4 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9484472143688185		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 2.9484472143688185 | validation: 3.217315710708225]
	TIME [epoch: 10.4 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953448177095876		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 2.953448177095876 | validation: 3.204766059472123]
	TIME [epoch: 10.4 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550602571796043		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 2.9550602571796043 | validation: 3.2087627722319985]
	TIME [epoch: 10.4 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532412570566575		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 2.9532412570566575 | validation: 3.213342168866037]
	TIME [epoch: 10.4 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949303284691246		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 2.949303284691246 | validation: 3.212664602120059]
	TIME [epoch: 10.4 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95143244067914		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 2.95143244067914 | validation: 3.1982560724841433]
	TIME [epoch: 10.4 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952818184605744		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 2.952818184605744 | validation: 3.195803608488061]
	TIME [epoch: 10.4 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505303038646944		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 2.9505303038646944 | validation: 3.2043257626707944]
	TIME [epoch: 10.4 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950227423153821		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 2.950227423153821 | validation: 3.205947518776539]
	TIME [epoch: 10.4 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536707552224613		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 2.9536707552224613 | validation: 3.207672808243616]
	TIME [epoch: 10.4 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957507256491717		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 2.957507256491717 | validation: 3.1980386248141066]
	TIME [epoch: 10.4 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956439505739798		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 2.956439505739798 | validation: 3.2093531435393166]
	TIME [epoch: 10.4 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95474246559161		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 2.95474246559161 | validation: 3.219721276327631]
	TIME [epoch: 10.4 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954087078454128		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 2.954087078454128 | validation: 3.208863085316143]
	TIME [epoch: 10.4 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956490814579348		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 2.956490814579348 | validation: 3.1955699394319517]
	TIME [epoch: 10.4 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949610193110357		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 2.949610193110357 | validation: 3.2115179122570647]
	TIME [epoch: 10.4 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527837615781887		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 2.9527837615781887 | validation: 3.218073968622342]
	TIME [epoch: 10.4 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502998050230493		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 2.9502998050230493 | validation: 3.212573308293379]
	TIME [epoch: 10.4 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515356529318266		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 2.9515356529318266 | validation: 3.2031622316101336]
	TIME [epoch: 10.4 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520744294793033		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 2.9520744294793033 | validation: 3.199790146378943]
	TIME [epoch: 10.4 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9579178577596776		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 2.9579178577596776 | validation: 3.207774953707606]
	TIME [epoch: 10.4 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561895017505506		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 2.9561895017505506 | validation: 3.203223077602587]
	TIME [epoch: 10.4 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954621483003511		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 2.954621483003511 | validation: 3.2180937235080194]
	TIME [epoch: 10.4 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955093777823282		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 2.955093777823282 | validation: 3.209812282390259]
	TIME [epoch: 10.4 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9485201092816267		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 2.9485201092816267 | validation: 3.211210322979821]
	TIME [epoch: 10.4 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9533396829902565		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 2.9533396829902565 | validation: 3.2016716413089314]
	TIME [epoch: 10.4 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524279567146903		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 2.9524279567146903 | validation: 3.2063331911434467]
	TIME [epoch: 10.4 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551319239298515		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 2.9551319239298515 | validation: 3.211994832900872]
	TIME [epoch: 10.4 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948745042738815		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 2.948745042738815 | validation: 3.20627198311358]
	TIME [epoch: 10.4 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950975145594989		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 2.950975145594989 | validation: 3.215198303411128]
	TIME [epoch: 10.4 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952458790561215		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 2.952458790561215 | validation: 3.203840399401273]
	TIME [epoch: 10.4 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94867555539406		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 2.94867555539406 | validation: 3.2067532529005707]
	TIME [epoch: 10.4 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950034532749275		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 2.950034532749275 | validation: 3.2189873375461735]
	TIME [epoch: 10.4 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526692472272695		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 2.9526692472272695 | validation: 3.2077040044455716]
	TIME [epoch: 10.4 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955930049277984		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 2.955930049277984 | validation: 3.216795969262894]
	TIME [epoch: 10.4 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9511951954595532		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 2.9511951954595532 | validation: 3.2278282397886597]
	TIME [epoch: 10.4 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950117726721639		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 2.950117726721639 | validation: 3.2119786078362753]
	TIME [epoch: 10.4 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542860212524173		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 2.9542860212524173 | validation: 3.209537182288516]
	TIME [epoch: 10.4 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550276503709134		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 2.9550276503709134 | validation: 3.201080113135937]
	TIME [epoch: 10.4 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513615864641696		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 2.9513615864641696 | validation: 3.207921135406514]
	TIME [epoch: 10.4 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955718643213939		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 2.955718643213939 | validation: 3.201873666279752]
	TIME [epoch: 10.4 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953110108956269		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 2.953110108956269 | validation: 3.2181045107267288]
	TIME [epoch: 10.4 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955550813187979		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 2.955550813187979 | validation: 3.2063435790377275]
	TIME [epoch: 10.4 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9522636424262174		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 2.9522636424262174 | validation: 3.206584001776271]
	TIME [epoch: 10.4 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9586077520047853		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 2.9586077520047853 | validation: 3.2023831805800387]
	TIME [epoch: 10.4 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506052465874966		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 2.9506052465874966 | validation: 3.206263015431387]
	TIME [epoch: 10.4 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517063283160025		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 2.9517063283160025 | validation: 3.2121417073513245]
	TIME [epoch: 10.4 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94984836331361		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 2.94984836331361 | validation: 3.2026678651477343]
	TIME [epoch: 10.4 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954015051839934		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 2.954015051839934 | validation: 3.213249471834331]
	TIME [epoch: 10.4 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9470890090284394		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 2.9470890090284394 | validation: 3.2051122583810576]
	TIME [epoch: 10.4 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542855727314175		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 2.9542855727314175 | validation: 3.215437854874265]
	TIME [epoch: 10.4 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954648674367558		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 2.954648674367558 | validation: 3.211639460292722]
	TIME [epoch: 10.4 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9493315287567197		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 2.9493315287567197 | validation: 3.218824147537157]
	TIME [epoch: 10.4 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9522915037854878		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 2.9522915037854878 | validation: 3.2080076447996353]
	TIME [epoch: 10.4 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9536478131858357		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 2.9536478131858357 | validation: 3.208854643246698]
	TIME [epoch: 10.4 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952367043652123		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 2.952367043652123 | validation: 3.204644709884704]
	TIME [epoch: 10.4 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950682068589198		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 2.950682068589198 | validation: 3.2116032279284163]
	TIME [epoch: 10.4 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547170360637836		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 2.9547170360637836 | validation: 3.217196668963496]
	TIME [epoch: 10.4 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524810891113495		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 2.9524810891113495 | validation: 3.2046898075366173]
	TIME [epoch: 10.4 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9560799352143623		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 2.9560799352143623 | validation: 3.206843191575156]
	TIME [epoch: 10.4 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538839274774054		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 2.9538839274774054 | validation: 3.213967699513006]
	TIME [epoch: 10.4 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955059590293465		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 2.955059590293465 | validation: 3.2107381225857483]
	TIME [epoch: 10.4 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9547769945286513		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 2.9547769945286513 | validation: 3.2214251481617815]
	TIME [epoch: 10.4 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9465757138484245		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 2.9465757138484245 | validation: 3.2148054458588002]
	TIME [epoch: 10.4 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9543484357827636		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 2.9543484357827636 | validation: 3.2013212843617156]
	TIME [epoch: 10.4 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9499042676284555		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 2.9499042676284555 | validation: 3.2138766406075447]
	TIME [epoch: 10.4 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9521758722061753		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 2.9521758722061753 | validation: 3.20387774097429]
	TIME [epoch: 10.4 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952268810605472		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 2.952268810605472 | validation: 3.1994641483336896]
	TIME [epoch: 10.4 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506872295292874		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 2.9506872295292874 | validation: 3.207979975354832]
	TIME [epoch: 10.4 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950874370970909		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 2.950874370970909 | validation: 3.2052009608138565]
	TIME [epoch: 10.4 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539416781440613		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 2.9539416781440613 | validation: 3.208035475280049]
	TIME [epoch: 10.4 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540225506050937		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 2.9540225506050937 | validation: 3.2160848385965326]
	TIME [epoch: 10.4 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955280172395606		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 2.955280172395606 | validation: 3.215361673935253]
	TIME [epoch: 10.4 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519890821585077		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 2.9519890821585077 | validation: 3.215300837469016]
	TIME [epoch: 10.4 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953100619327812		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 2.953100619327812 | validation: 3.218471139580686]
	TIME [epoch: 10.4 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552199379486477		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 2.9552199379486477 | validation: 3.2101903047355633]
	TIME [epoch: 10.4 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571898044913456		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 2.9571898044913456 | validation: 3.211004919067942]
	TIME [epoch: 10.4 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9510584673503595		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 2.9510584673503595 | validation: 3.2076984962120467]
	TIME [epoch: 10.4 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9583286093372503		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 2.9583286093372503 | validation: 3.215028312766825]
	TIME [epoch: 10.4 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95528290563858		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 2.95528290563858 | validation: 3.2103331403565525]
	TIME [epoch: 10.4 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9568790945541688		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 2.9568790945541688 | validation: 3.2047390667106215]
	TIME [epoch: 10.4 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955596445732076		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 2.955596445732076 | validation: 3.2095037108846087]
	TIME [epoch: 10.4 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9558640602938313		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 2.9558640602938313 | validation: 3.2195277240898723]
	TIME [epoch: 10.4 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526195274859517		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 2.9526195274859517 | validation: 3.2090256466160363]
	TIME [epoch: 10.4 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957572583031605		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 2.957572583031605 | validation: 3.1961585380565785]
	TIME [epoch: 10.3 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955176896087811		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 2.955176896087811 | validation: 3.2128108931345]
	TIME [epoch: 10.3 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531182013766286		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 2.9531182013766286 | validation: 3.2172023467549025]
	TIME [epoch: 10.4 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949914623089568		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 2.949914623089568 | validation: 3.2037413997464466]
	TIME [epoch: 10.4 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9562635102280646		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 2.9562635102280646 | validation: 3.2019288521076783]
	TIME [epoch: 10.4 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509263299630684		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 2.9509263299630684 | validation: 3.2164872387251537]
	TIME [epoch: 10.4 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9548497049462648		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 2.9548497049462648 | validation: 3.212113644434439]
	TIME [epoch: 10.4 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952894964368599		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 2.952894964368599 | validation: 3.2078031233494277]
	TIME [epoch: 10.4 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9506421499677584		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 2.9506421499677584 | validation: 3.2138503426298035]
	TIME [epoch: 10.4 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9466781441112078		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 2.9466781441112078 | validation: 3.2054788181729608]
	TIME [epoch: 10.4 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953585825316822		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 2.953585825316822 | validation: 3.210712699359643]
	TIME [epoch: 10.4 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.958174999008223		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 2.958174999008223 | validation: 3.2109769630156655]
	TIME [epoch: 10.4 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948326736267986		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 2.948326736267986 | validation: 3.212503107011609]
	TIME [epoch: 10.4 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496623202905226		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 2.9496623202905226 | validation: 3.2113932021367533]
	TIME [epoch: 10.4 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956143169820078		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 2.956143169820078 | validation: 3.210750671980886]
	TIME [epoch: 10.4 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9555507318851797		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 2.9555507318851797 | validation: 3.2074411616434855]
	TIME [epoch: 10.4 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954591087159043		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 2.954591087159043 | validation: 3.208308717186209]
	TIME [epoch: 10.4 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9517680796613663		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 2.9517680796613663 | validation: 3.2077898600018573]
	TIME [epoch: 10.4 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541993683954235		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 2.9541993683954235 | validation: 3.204162674278748]
	TIME [epoch: 10.4 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95432485336993		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 2.95432485336993 | validation: 3.2113697305347273]
	TIME [epoch: 10.4 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539929717414757		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 2.9539929717414757 | validation: 3.2094587363407543]
	TIME [epoch: 10.4 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948009047285011		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 2.948009047285011 | validation: 3.2067747434467258]
	TIME [epoch: 10.4 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948772973510978		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 2.948772973510978 | validation: 3.2082975197630503]
	TIME [epoch: 10.4 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527019161021433		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 2.9527019161021433 | validation: 3.2116078902137373]
	TIME [epoch: 10.4 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532703972851655		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 2.9532703972851655 | validation: 3.2065580382536205]
	TIME [epoch: 10.4 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952956636084355		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 2.952956636084355 | validation: 3.203820172288666]
	TIME [epoch: 10.4 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550521269029444		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 2.9550521269029444 | validation: 3.203628432251286]
	TIME [epoch: 10.4 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527635621165205		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 2.9527635621165205 | validation: 3.216183042157814]
	TIME [epoch: 10.4 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9571844456114627		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 2.9571844456114627 | validation: 3.210190827699581]
	TIME [epoch: 10.4 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525929218485105		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 2.9525929218485105 | validation: 3.2171413027186704]
	TIME [epoch: 10.4 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9513502506138947		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 2.9513502506138947 | validation: 3.2098121417198753]
	TIME [epoch: 10.4 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95654425875593		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 2.95654425875593 | validation: 3.196084549652822]
	TIME [epoch: 10.4 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9585036401149187		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 2.9585036401149187 | validation: 3.2081429562903714]
	TIME [epoch: 10.4 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953212845911291		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 2.953212845911291 | validation: 3.2057575250488144]
	TIME [epoch: 10.4 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956259087857286		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 2.956259087857286 | validation: 3.2133148584528577]
	TIME [epoch: 10.4 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519029263154275		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 2.9519029263154275 | validation: 3.2014627895674685]
	TIME [epoch: 10.4 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559276223875575		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 2.9559276223875575 | validation: 3.2079181570130912]
	TIME [epoch: 10.4 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9602941926415793		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 2.9602941926415793 | validation: 3.203207607970358]
	TIME [epoch: 10.4 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948750956572352		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 2.948750956572352 | validation: 3.204066152044394]
	TIME [epoch: 10.4 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9546609210491805		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 2.9546609210491805 | validation: 3.2184421171622137]
	TIME [epoch: 10.3 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9556729777542867		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 2.9556729777542867 | validation: 3.214937031847069]
	TIME [epoch: 10.3 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953237891711689		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 2.953237891711689 | validation: 3.2026575209576986]
	TIME [epoch: 10.3 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955138863633784		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 2.955138863633784 | validation: 3.207716619616491]
	TIME [epoch: 10.4 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950609888333087		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 2.950609888333087 | validation: 3.208961894031158]
	TIME [epoch: 10.4 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.945174972371629		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 2.945174972371629 | validation: 3.2080782941158863]
	TIME [epoch: 10.4 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9495956405056973		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 2.9495956405056973 | validation: 3.2097960706204987]
	TIME [epoch: 10.4 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953622349281204		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 2.953622349281204 | validation: 3.21354975245883]
	TIME [epoch: 10.4 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528288852463294		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 2.9528288852463294 | validation: 3.20616591865184]
	TIME [epoch: 10.4 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507333783249763		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 2.9507333783249763 | validation: 3.2074848326588383]
	TIME [epoch: 10.4 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541622699164947		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 2.9541622699164947 | validation: 3.2042665295749084]
	TIME [epoch: 10.4 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951290397564071		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 2.951290397564071 | validation: 3.207696657940553]
	TIME [epoch: 10.4 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952296784849634		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 2.952296784849634 | validation: 3.206344709501467]
	TIME [epoch: 10.4 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9467171522117552		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 2.9467171522117552 | validation: 3.2106883772145682]
	TIME [epoch: 10.4 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953330819585557		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 2.953330819585557 | validation: 3.208204549718453]
	TIME [epoch: 10.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952394478613697		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 2.952394478613697 | validation: 3.20198354246869]
	TIME [epoch: 10.4 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956667003770952		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 2.956667003770952 | validation: 3.2044472955303673]
	TIME [epoch: 10.4 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552595044061176		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 2.9552595044061176 | validation: 3.2172979083451247]
	TIME [epoch: 10.4 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95096015028811		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 2.95096015028811 | validation: 3.2021017228323276]
	TIME [epoch: 10.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949140285527735		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 2.949140285527735 | validation: 3.2179014647395285]
	TIME [epoch: 10.4 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954531753921225		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 2.954531753921225 | validation: 3.200524338953805]
	TIME [epoch: 10.4 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951071364281635		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 2.951071364281635 | validation: 3.2165438941411004]
	TIME [epoch: 10.4 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526290198703307		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 2.9526290198703307 | validation: 3.209251958754805]
	TIME [epoch: 10.4 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953026048709962		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 2.953026048709962 | validation: 3.208138835044024]
	TIME [epoch: 10.4 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952261002398285		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 2.952261002398285 | validation: 3.218140703869533]
	TIME [epoch: 10.4 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9532524226248604		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 2.9532524226248604 | validation: 3.2116231920122007]
	TIME [epoch: 10.4 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540697569539		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 2.9540697569539 | validation: 3.2150067505706175]
	TIME [epoch: 10.3 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952864150011504		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 2.952864150011504 | validation: 3.212749828101423]
	TIME [epoch: 10.4 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950897571269847		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 2.950897571269847 | validation: 3.2095108890620954]
	TIME [epoch: 10.4 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538210130176723		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 2.9538210130176723 | validation: 3.213552831308267]
	TIME [epoch: 10.3 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950918899852823		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 2.950918899852823 | validation: 3.207566707841891]
	TIME [epoch: 10.4 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953858382851579		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 2.953858382851579 | validation: 3.215934524780296]
	TIME [epoch: 10.4 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95530667806893		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 2.95530667806893 | validation: 3.216524352525098]
	TIME [epoch: 10.4 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95255278452832		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 2.95255278452832 | validation: 3.2211143824796364]
	TIME [epoch: 10.4 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956872571123079		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 2.956872571123079 | validation: 3.2179674765637656]
	TIME [epoch: 10.4 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950013227361687		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 2.950013227361687 | validation: 3.2149956457603173]
	TIME [epoch: 10.4 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540566765646674		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 2.9540566765646674 | validation: 3.2116654606255612]
	TIME [epoch: 10.3 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949138079716273		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 2.949138079716273 | validation: 3.2118044990228247]
	TIME [epoch: 10.4 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541686936854266		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 2.9541686936854266 | validation: 3.2201997371083975]
	TIME [epoch: 10.4 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952474152639481		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 2.952474152639481 | validation: 3.207233198519914]
	TIME [epoch: 10.4 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9565617027548816		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 2.9565617027548816 | validation: 3.2164509837015745]
	TIME [epoch: 10.4 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9496753334891386		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 2.9496753334891386 | validation: 3.1988494281411297]
	TIME [epoch: 10.4 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951193306085876		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 2.951193306085876 | validation: 3.205934567630517]
	TIME [epoch: 10.3 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.946830972842947		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 2.946830972842947 | validation: 3.209054579761998]
	TIME [epoch: 10.4 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9524693077774056		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 2.9524693077774056 | validation: 3.2071064918401007]
	TIME [epoch: 10.4 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948926218951661		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 2.948926218951661 | validation: 3.209649320738602]
	TIME [epoch: 10.4 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95534212338538		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 2.95534212338538 | validation: 3.212609274568872]
	TIME [epoch: 10.4 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559710591673545		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 2.9559710591673545 | validation: 3.2113042237735114]
	TIME [epoch: 10.4 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950109230295609		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 2.950109230295609 | validation: 3.1980478380929345]
	TIME [epoch: 10.4 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9540446918775607		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 2.9540446918775607 | validation: 3.2078632791346293]
	TIME [epoch: 10.3 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520358421627386		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 2.9520358421627386 | validation: 3.204545190250251]
	TIME [epoch: 10.4 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955442025747741		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 2.955442025747741 | validation: 3.2101218327043344]
	TIME [epoch: 10.4 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9494447915628106		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 2.9494447915628106 | validation: 3.208006258725081]
	TIME [epoch: 10.4 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953439981666712		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 2.953439981666712 | validation: 3.209101800370513]
	TIME [epoch: 10.3 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9484982355092395		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 2.9484982355092395 | validation: 3.215165891951809]
	TIME [epoch: 10.3 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505698638119595		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 2.9505698638119595 | validation: 3.212816321436319]
	TIME [epoch: 10.4 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553822437263824		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 2.9553822437263824 | validation: 3.2105051788247194]
	TIME [epoch: 10.4 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.945785311136384		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 2.945785311136384 | validation: 3.2054627066022654]
	TIME [epoch: 10.4 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950145810998086		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 2.950145810998086 | validation: 3.210728119853394]
	TIME [epoch: 10.4 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956651822809925		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 2.956651822809925 | validation: 3.216893090475395]
	TIME [epoch: 10.4 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948478464456189		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 2.948478464456189 | validation: 3.2116721643021426]
	TIME [epoch: 10.4 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545972445957838		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 2.9545972445957838 | validation: 3.1999233989242293]
	TIME [epoch: 10.4 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9527119092678715		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 2.9527119092678715 | validation: 3.2231875483494936]
	TIME [epoch: 10.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955060143196783		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 2.955060143196783 | validation: 3.2156550857708925]
	TIME [epoch: 10.4 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.947459884693761		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 2.947459884693761 | validation: 3.2125282472981667]
	TIME [epoch: 10.4 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542351416425525		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 2.9542351416425525 | validation: 3.211411520854947]
	TIME [epoch: 10.3 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9509274447983715		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 2.9509274447983715 | validation: 3.217374354423656]
	TIME [epoch: 10.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952615957003822		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 2.952615957003822 | validation: 3.216819295676741]
	TIME [epoch: 10.4 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95193765775999		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 2.95193765775999 | validation: 3.2113204013789294]
	TIME [epoch: 10.4 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502813772814744		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 2.9502813772814744 | validation: 3.206914208040922]
	TIME [epoch: 10.4 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957435589487475		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 2.957435589487475 | validation: 3.216683074119791]
	TIME [epoch: 10.4 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9504096328229945		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 2.9504096328229945 | validation: 3.210789640655519]
	TIME [epoch: 10.4 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953224580895729		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 2.953224580895729 | validation: 3.222308869404019]
	TIME [epoch: 10.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954691988101129		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 2.954691988101129 | validation: 3.212857503417837]
	TIME [epoch: 10.4 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9552291811114118		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 2.9552291811114118 | validation: 3.2108744699305807]
	TIME [epoch: 10.4 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9574172053416294		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 2.9574172053416294 | validation: 3.2025556142880327]
	TIME [epoch: 10.4 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9537351898419018		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 2.9537351898419018 | validation: 3.20932171138757]
	TIME [epoch: 10.4 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953848010057244		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 2.953848010057244 | validation: 3.218459724000938]
	TIME [epoch: 10.4 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520888392143796		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 2.9520888392143796 | validation: 3.2110392881897605]
	TIME [epoch: 10.4 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951694552720771		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 2.951694552720771 | validation: 3.2115203148144658]
	TIME [epoch: 10.4 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949474125310713		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 2.949474125310713 | validation: 3.20801519506827]
	TIME [epoch: 10.4 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519552797262043		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 2.9519552797262043 | validation: 3.2073584788129312]
	TIME [epoch: 10.4 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545001023636788		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 2.9545001023636788 | validation: 3.2140081577608646]
	TIME [epoch: 10.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955942392249268		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 2.955942392249268 | validation: 3.213609279429038]
	TIME [epoch: 10.4 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9539533591078735		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 2.9539533591078735 | validation: 3.220925257599788]
	TIME [epoch: 10.4 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948832716583057		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 2.948832716583057 | validation: 3.214445492464075]
	TIME [epoch: 10.4 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955616541688641		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 2.955616541688641 | validation: 3.21402871846187]
	TIME [epoch: 10.4 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950398902404033		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 2.950398902404033 | validation: 3.205228853988324]
	TIME [epoch: 10.4 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.948987248068504		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 2.948987248068504 | validation: 3.217553016366112]
	TIME [epoch: 10.4 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955456111337567		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 2.955456111337567 | validation: 3.206165270940787]
	TIME [epoch: 10.4 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953974427251525		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 2.953974427251525 | validation: 3.2077705912011494]
	TIME [epoch: 10.4 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95351833294209		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 2.95351833294209 | validation: 3.2104114778327926]
	TIME [epoch: 10.4 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956785220399227		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 2.956785220399227 | validation: 3.2083130068730528]
	TIME [epoch: 10.4 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561142038322563		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 2.9561142038322563 | validation: 3.2064308170186155]
	TIME [epoch: 10.4 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95247396417387		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 2.95247396417387 | validation: 3.20773266235443]
	TIME [epoch: 10.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950046199802354		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 2.950046199802354 | validation: 3.2083458424993347]
	TIME [epoch: 10.4 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951724035757679		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 2.951724035757679 | validation: 3.1950223396892943]
	TIME [epoch: 10.4 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94728357715653		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 2.94728357715653 | validation: 3.206042235012717]
	TIME [epoch: 10.4 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950915040295783		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 2.950915040295783 | validation: 3.213135539657722]
	TIME [epoch: 10.4 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9515002566934982		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 2.9515002566934982 | validation: 3.211727026361465]
	TIME [epoch: 10.4 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9529386042918198		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 2.9529386042918198 | validation: 3.217081381922157]
	TIME [epoch: 10.4 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9541675066991973		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 2.9541675066991973 | validation: 3.2151942079664173]
	TIME [epoch: 10.4 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9507412760720175		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 2.9507412760720175 | validation: 3.20823718214568]
	TIME [epoch: 10.4 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951121646526535		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 2.951121646526535 | validation: 3.196471903705737]
	TIME [epoch: 10.4 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9531545744907297		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 2.9531545744907297 | validation: 3.2162378219460717]
	TIME [epoch: 10.4 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535555863997756		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 2.9535555863997756 | validation: 3.207101052926892]
	TIME [epoch: 10.3 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955307619873266		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 2.955307619873266 | validation: 3.2085256958477504]
	TIME [epoch: 10.4 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9520140242444044		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 2.9520140242444044 | validation: 3.2007989535297954]
	TIME [epoch: 10.4 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550459398681195		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 2.9550459398681195 | validation: 3.2058873299816266]
	TIME [epoch: 10.4 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9518202371610416		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 2.9518202371610416 | validation: 3.205308524291114]
	TIME [epoch: 10.4 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952901087413852		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 2.952901087413852 | validation: 3.216306597992788]
	TIME [epoch: 10.4 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95460965748814		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 2.95460965748814 | validation: 3.195460288169441]
	TIME [epoch: 10.4 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9528157849047387		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 2.9528157849047387 | validation: 3.209303842752421]
	TIME [epoch: 10.3 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9491669677407		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 2.9491669677407 | validation: 3.2138017713802665]
	TIME [epoch: 10.4 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.94923592831155		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 2.94923592831155 | validation: 3.21097918974516]
	TIME [epoch: 10.4 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956928636568626		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 2.956928636568626 | validation: 3.205231258237034]
	TIME [epoch: 10.4 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951143178281515		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 2.951143178281515 | validation: 3.2164771531082876]
	TIME [epoch: 10.4 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519267669442812		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 2.9519267669442812 | validation: 3.2034756782482225]
	TIME [epoch: 10.4 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9551740033581044		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 2.9551740033581044 | validation: 3.2059683053344905]
	TIME [epoch: 10.4 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502589149599245		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 2.9502589149599245 | validation: 3.208811710184032]
	TIME [epoch: 10.4 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957612298298998		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 2.957612298298998 | validation: 3.207923622756117]
	TIME [epoch: 10.3 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9526940257697127		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 2.9526940257697127 | validation: 3.207238360388981]
	TIME [epoch: 10.4 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954788907572966		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 2.954788907572966 | validation: 3.1915284642668276]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r1_20240219_233643/states/model_tr_study206_1961.pth
	Model improved!!!
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949899790096412		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 2.949899790096412 | validation: 3.213991652835093]
	TIME [epoch: 10.4 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.954858025469739		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 2.954858025469739 | validation: 3.2117926080321837]
	TIME [epoch: 10.3 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9535995621767994		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 2.9535995621767994 | validation: 3.2127993974900058]
	TIME [epoch: 10.4 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956814865623304		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 2.956814865623304 | validation: 3.212370080331955]
	TIME [epoch: 10.4 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955121289159597		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 2.955121289159597 | validation: 3.198271181197895]
	TIME [epoch: 10.4 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950750261283693		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 2.950750261283693 | validation: 3.2254931626860204]
	TIME [epoch: 10.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9553044884528976		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 2.9553044884528976 | validation: 3.217793179871693]
	TIME [epoch: 10.3 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9534267120113364		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 2.9534267120113364 | validation: 3.2103284828615792]
	TIME [epoch: 10.3 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953691384834913		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 2.953691384834913 | validation: 3.2136215852625605]
	TIME [epoch: 10.4 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.950445758209927		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 2.950445758209927 | validation: 3.2038383322902235]
	TIME [epoch: 10.4 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952554079202817		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 2.952554079202817 | validation: 3.202540242202949]
	TIME [epoch: 10.4 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9594464612165563		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 2.9594464612165563 | validation: 3.204355847803912]
	TIME [epoch: 10.3 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.95318113793332		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 2.95318113793332 | validation: 3.2027894052061523]
	TIME [epoch: 10.4 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951022344341829		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 2.951022344341829 | validation: 3.212208510771932]
	TIME [epoch: 10.4 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953681556748214		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 2.953681556748214 | validation: 3.1993555216952316]
	TIME [epoch: 10.3 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953033167957442		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 2.953033167957442 | validation: 3.210489082817218]
	TIME [epoch: 10.4 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952458578884826		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 2.952458578884826 | validation: 3.203898474446894]
	TIME [epoch: 10.4 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.951784665815543		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 2.951784665815543 | validation: 3.1987758417878216]
	TIME [epoch: 10.3 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953182423935226		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 2.953182423935226 | validation: 3.2005179320829233]
	TIME [epoch: 10.3 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.942548565626897		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 2.942548565626897 | validation: 3.2009159171989268]
	TIME [epoch: 10.3 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9498527549263955		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 2.9498527549263955 | validation: 3.2138455094105196]
	TIME [epoch: 10.4 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9486646062036725		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 2.9486646062036725 | validation: 3.2035418736768646]
	TIME [epoch: 10.3 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952941742601203		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 2.952941742601203 | validation: 3.208284653551315]
	TIME [epoch: 10.3 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9542129554954735		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 2.9542129554954735 | validation: 3.2084093905889097]
	TIME [epoch: 10.3 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9538892534527736		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 2.9538892534527736 | validation: 3.2048358987805843]
	TIME [epoch: 10.4 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9570641942496034		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 2.9570641942496034 | validation: 3.1971878477289586]
	TIME [epoch: 10.3 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.956942264088464		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 2.956942264088464 | validation: 3.2091412576892204]
	TIME [epoch: 10.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.957849526702979		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 2.957849526702979 | validation: 3.205958392220018]
	TIME [epoch: 10.3 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9559437405331352		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 2.9559437405331352 | validation: 3.2166745987374306]
	TIME [epoch: 10.3 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9550296680233585		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 2.9550296680233585 | validation: 3.208995061701602]
	TIME [epoch: 10.3 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.955043256702259		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 2.955043256702259 | validation: 3.205765835137327]
	TIME [epoch: 10.3 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.949913114233606		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 2.949913114233606 | validation: 3.2036581940899356]
	TIME [epoch: 10.3 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9545330154749694		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 2.9545330154749694 | validation: 3.1984470428702787]
	TIME [epoch: 10.3 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9561387888496204		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 2.9561387888496204 | validation: 3.2099770571866784]
	TIME [epoch: 10.3 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.953020824891289		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 2.953020824891289 | validation: 3.2125786870965776]
	TIME [epoch: 10.3 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.952925593897763		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 2.952925593897763 | validation: 3.1942993433834346]
	TIME [epoch: 10.3 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9519402910447208		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 2.9519402910447208 | validation: 3.210857552579014]
	TIME [epoch: 10.3 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9505495274041946		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 2.9505495274041946 | validation: 3.214551635073906]
	TIME [epoch: 10.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9525405356522		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 2.9525405356522 | validation: 3.2094056901079124]
	TIME [epoch: 10.3 sec]
Finished training in 20857.943 seconds.
