Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r2', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3245685359

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 11.576546002012213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.576546002012213 | validation: 11.39512600562331]
	TIME [epoch: 49.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.909052053875671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.909052053875671 | validation: 6.5676337163445115]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.49101545142754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.49101545142754 | validation: 9.305051532896428]
	TIME [epoch: 10.3 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.668642413104475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.668642413104475 | validation: 6.264739929776876]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.020686106349319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.020686106349319 | validation: 6.617635014579015]
	TIME [epoch: 10.3 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.735171819230416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.735171819230416 | validation: 6.20903710352847]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3978256378168465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3978256378168465 | validation: 6.138396034251319]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.392630512974057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.392630512974057 | validation: 6.1347067060414995]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.390403449923322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.390403449923322 | validation: 6.890146655964481]
	TIME [epoch: 10.3 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.500282499874651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.500282499874651 | validation: 6.521337557144636]
	TIME [epoch: 10.3 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.415424184884826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.415424184884826 | validation: 6.115527848900188]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.44778458618646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.44778458618646 | validation: 6.035984766629776]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.405828829368476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.405828829368476 | validation: 6.065565949236418]
	TIME [epoch: 10.3 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.3549144770853765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3549144770853765 | validation: 6.839843134293392]
	TIME [epoch: 10.3 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.617103976604363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.617103976604363 | validation: 6.6114018517871935]
	TIME [epoch: 10.3 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.344600223869636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.344600223869636 | validation: 5.879306673708616]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.321625623662297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.321625623662297 | validation: 6.090481437145605]
	TIME [epoch: 10.3 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.428102831939966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.428102831939966 | validation: 5.862130889145228]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.28350516261853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.28350516261853 | validation: 5.815213800146528]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.25555098600322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.25555098600322 | validation: 5.888125718216771]
	TIME [epoch: 10.3 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.182049034761304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.182049034761304 | validation: 6.42197062072412]
	TIME [epoch: 10.3 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.397691122824505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.397691122824505 | validation: 5.852522913755579]
	TIME [epoch: 10.3 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.967555547408843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.967555547408843 | validation: 5.9481028743494075]
	TIME [epoch: 10.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.325708579701991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.325708579701991 | validation: 5.823801677771469]
	TIME [epoch: 10.3 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.188539838243939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.188539838243939 | validation: 6.03359991804552]
	TIME [epoch: 10.3 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.581047372541301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.581047372541301 | validation: 5.648779443697278]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.070662256960582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.070662256960582 | validation: 5.831861811225517]
	TIME [epoch: 10.3 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.043359814540789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.043359814540789 | validation: 5.686294029887895]
	TIME [epoch: 10.3 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.082834487404705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.082834487404705 | validation: 6.489609281585399]
	TIME [epoch: 10.3 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.224932631714973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.224932631714973 | validation: 6.014314378276435]
	TIME [epoch: 10.3 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.275638828150654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.275638828150654 | validation: 5.639424372467217]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.911894957674962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.911894957674962 | validation: 5.718106091898368]
	TIME [epoch: 10.3 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.181537772778347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.181537772778347 | validation: 5.718012358125605]
	TIME [epoch: 10.3 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.077881186506621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.077881186506621 | validation: 6.048960222261075]
	TIME [epoch: 10.3 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.238497437929015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.238497437929015 | validation: 5.946971346391699]
	TIME [epoch: 10.3 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.968135126891937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.968135126891937 | validation: 5.598508105415085]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.843583335365288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.843583335365288 | validation: 5.847447088572842]
	TIME [epoch: 10.3 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.905956424765383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.905956424765383 | validation: 5.628928155129972]
	TIME [epoch: 10.3 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.997890179606016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.997890179606016 | validation: 5.945823021658733]
	TIME [epoch: 10.3 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.883153258635379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.883153258635379 | validation: 5.4448910764973695]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9993659515751165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9993659515751165 | validation: 5.44608300726708]
	TIME [epoch: 10.3 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9425087423966625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9425087423966625 | validation: 5.42790272135602]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.806206382591202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.806206382591202 | validation: 5.4421117220224255]
	TIME [epoch: 10.3 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.807699322059797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.807699322059797 | validation: 5.930431090081474]
	TIME [epoch: 10.3 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.786569190276646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.786569190276646 | validation: 5.610503741532658]
	TIME [epoch: 10.3 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.815104317557484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.815104317557484 | validation: 5.993721051645803]
	TIME [epoch: 10.3 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.897599273620197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.897599273620197 | validation: 5.5972275110449345]
	TIME [epoch: 10.3 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.78299466657572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.78299466657572 | validation: 5.370883252808978]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.854325619933951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.854325619933951 | validation: 5.3551183688453365]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.611974042154983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.611974042154983 | validation: 5.738595734883335]
	TIME [epoch: 10.3 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.694183730044446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.694183730044446 | validation: 5.364303726800147]
	TIME [epoch: 10.3 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.601918553089563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.601918553089563 | validation: 5.2404587648002074]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.621410900050909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.621410900050909 | validation: 5.890053568318267]
	TIME [epoch: 10.3 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8012081008792835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8012081008792835 | validation: 5.460127949982336]
	TIME [epoch: 10.3 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.923979308759093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.923979308759093 | validation: 5.577540281534367]
	TIME [epoch: 10.3 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.400162515880275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.400162515880275 | validation: 5.3666308014404365]
	TIME [epoch: 10.3 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.86119116476806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.86119116476806 | validation: 5.899645228384086]
	TIME [epoch: 10.3 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.9175072133001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9175072133001 | validation: 5.529313153705791]
	TIME [epoch: 10.3 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.522835889697096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.522835889697096 | validation: 5.208558384201667]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.471734048163663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.471734048163663 | validation: 5.15463447776195]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5270083613034275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5270083613034275 | validation: 5.740675641919063]
	TIME [epoch: 10.3 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.582332379701829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.582332379701829 | validation: 5.262991917090512]
	TIME [epoch: 10.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.554252209922724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.554252209922724 | validation: 5.6690634898921095]
	TIME [epoch: 10.3 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.442738484072322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.442738484072322 | validation: 5.1672059338824985]
	TIME [epoch: 10.3 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.874569073048329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.874569073048329 | validation: 5.178608122975515]
	TIME [epoch: 10.3 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.38618502280683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.38618502280683 | validation: 5.585989469421779]
	TIME [epoch: 10.3 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.464383518783955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.464383518783955 | validation: 5.316433262548401]
	TIME [epoch: 10.3 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.501351196000105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.501351196000105 | validation: 5.581094537213208]
	TIME [epoch: 10.3 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.617455777154387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.617455777154387 | validation: 5.170283695001301]
	TIME [epoch: 10.3 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.559170863925901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.559170863925901 | validation: 5.1855156601471775]
	TIME [epoch: 10.3 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.586533977773215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.586533977773215 | validation: 5.687580659101009]
	TIME [epoch: 10.3 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.522953162413015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.522953162413015 | validation: 5.135329654586122]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.497958536457205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.497958536457205 | validation: 5.3037214723402215]
	TIME [epoch: 10.3 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.450270860637891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.450270860637891 | validation: 5.484074580204669]
	TIME [epoch: 10.3 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.498226495069082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.498226495069082 | validation: 5.252848478669635]
	TIME [epoch: 10.3 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.286572430601091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.286572430601091 | validation: 6.458551814425131]
	TIME [epoch: 10.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.7238374451309735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7238374451309735 | validation: 5.133475195647382]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.341413789796322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.341413789796322 | validation: 5.204313422690746]
	TIME [epoch: 10.3 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.199262544025494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.199262544025494 | validation: 5.52458286228406]
	TIME [epoch: 10.3 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.5782619230123744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5782619230123744 | validation: 5.590567865741994]
	TIME [epoch: 10.3 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.476663343844918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.476663343844918 | validation: 5.101745282803687]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.413388956715936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.413388956715936 | validation: 5.097395674763935]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.730653961251422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.730653961251422 | validation: 5.337184326488709]
	TIME [epoch: 10.3 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.4405176220660865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4405176220660865 | validation: 5.221453308805799]
	TIME [epoch: 10.3 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.330733472354703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.330733472354703 | validation: 5.207707747737304]
	TIME [epoch: 10.3 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.310481593880729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.310481593880729 | validation: 5.743704719957305]
	TIME [epoch: 10.3 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.555199866330534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.555199866330534 | validation: 5.112814250401999]
	TIME [epoch: 10.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.29565955894369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.29565955894369 | validation: 5.105012778576947]
	TIME [epoch: 10.3 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.370445591959031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.370445591959031 | validation: 5.304044202212704]
	TIME [epoch: 10.3 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.234416008280862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.234416008280862 | validation: 5.814696808342328]
	TIME [epoch: 10.3 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.813355097767849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.813355097767849 | validation: 5.595170950578095]
	TIME [epoch: 10.3 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.338123332669515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.338123332669515 | validation: 5.433448008629198]
	TIME [epoch: 10.3 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.24371376962158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.24371376962158 | validation: 5.247889593385184]
	TIME [epoch: 10.3 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.565745980751951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.565745980751951 | validation: 5.022182279902435]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.50260349481325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.50260349481325 | validation: 5.288068908427415]
	TIME [epoch: 10.3 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3094551953244835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3094551953244835 | validation: 5.986208250168595]
	TIME [epoch: 10.3 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.369811010109506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.369811010109506 | validation: 5.940488693123764]
	TIME [epoch: 10.3 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.328026719986626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.328026719986626 | validation: 5.018096818235894]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.391365310893757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.391365310893757 | validation: 5.005850469324448]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.272802046942088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.272802046942088 | validation: 5.5902455915960845]
	TIME [epoch: 10.3 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.435959458205302		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 4.435959458205302 | validation: 5.118501827100919]
	TIME [epoch: 10.3 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.348875447794452		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 4.348875447794452 | validation: 5.032012962285746]
	TIME [epoch: 10.3 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2170909719106096		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 4.2170909719106096 | validation: 5.063127159631126]
	TIME [epoch: 10.3 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.220948272210139		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 4.220948272210139 | validation: 5.02727398467092]
	TIME [epoch: 10.3 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.23542304561259		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 4.23542304561259 | validation: 5.592094302515282]
	TIME [epoch: 10.3 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.381645211111346		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 4.381645211111346 | validation: 6.2365884541303105]
	TIME [epoch: 10.3 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.472603906495578		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 4.472603906495578 | validation: 4.990060816355095]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.290776309213544		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 4.290776309213544 | validation: 5.08433966237274]
	TIME [epoch: 10.3 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.241310186124198		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 4.241310186124198 | validation: 5.187840838133627]
	TIME [epoch: 10.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0761429895875745		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 4.0761429895875745 | validation: 5.33212146971359]
	TIME [epoch: 10.3 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.228939453759773		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 4.228939453759773 | validation: 5.353448452613218]
	TIME [epoch: 10.3 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.313036802120484		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 4.313036802120484 | validation: 5.315002611731054]
	TIME [epoch: 10.3 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.261669453488502		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 4.261669453488502 | validation: 5.263484973467467]
	TIME [epoch: 10.3 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.16233905351327		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 4.16233905351327 | validation: 5.2153773635121015]
	TIME [epoch: 10.3 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.201436282633438		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 4.201436282633438 | validation: 5.001613697314692]
	TIME [epoch: 10.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.268825280556909		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 4.268825280556909 | validation: 5.045891358470562]
	TIME [epoch: 10.3 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.17609804339275		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 4.17609804339275 | validation: 5.024048227298433]
	TIME [epoch: 10.3 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.233652615480013		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 4.233652615480013 | validation: 4.961907021071933]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.268147932799264		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 4.268147932799264 | validation: 5.699229725644998]
	TIME [epoch: 10.3 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.429469015983813		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 4.429469015983813 | validation: 5.115931113467725]
	TIME [epoch: 10.3 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.122486231653452		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 4.122486231653452 | validation: 5.884149664393101]
	TIME [epoch: 10.3 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.477812218546539		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 4.477812218546539 | validation: 5.281603578873783]
	TIME [epoch: 10.3 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.221665808988177		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 4.221665808988177 | validation: 5.163055116290888]
	TIME [epoch: 10.3 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.167495547535968		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 4.167495547535968 | validation: 5.375639807410339]
	TIME [epoch: 10.3 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.237960862474344		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 4.237960862474344 | validation: 5.1447097337179954]
	TIME [epoch: 10.3 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.316765812300706		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 4.316765812300706 | validation: 5.013129937678299]
	TIME [epoch: 10.3 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.483311229099887		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 4.483311229099887 | validation: 5.559180919891187]
	TIME [epoch: 10.3 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.32116367077597		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 4.32116367077597 | validation: 5.036387057107892]
	TIME [epoch: 10.3 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.152590145670226		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 4.152590145670226 | validation: 5.075427912293962]
	TIME [epoch: 10.3 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.376617903049693		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 4.376617903049693 | validation: 5.203433487247265]
	TIME [epoch: 10.3 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.094950900421728		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 4.094950900421728 | validation: 5.249010232491278]
	TIME [epoch: 10.3 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.241378791627569		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 4.241378791627569 | validation: 5.042513519840729]
	TIME [epoch: 10.3 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.069185647523719		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 4.069185647523719 | validation: 5.079658849206689]
	TIME [epoch: 10.3 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.198662603071467		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 4.198662603071467 | validation: 4.944478513670778]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.107815078095653		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 4.107815078095653 | validation: 5.156704246318245]
	TIME [epoch: 10.3 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.186474926950551		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 4.186474926950551 | validation: 5.189646401000892]
	TIME [epoch: 10.3 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.173403627844412		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 4.173403627844412 | validation: 5.434217622309485]
	TIME [epoch: 10.3 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.190904018411841		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 4.190904018411841 | validation: 5.049847803198528]
	TIME [epoch: 10.3 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.190343107093549		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 4.190343107093549 | validation: 4.898411211799126]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.210445043715952		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 4.210445043715952 | validation: 5.223832087842854]
	TIME [epoch: 10.3 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.13917205833733		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 4.13917205833733 | validation: 4.940383137616391]
	TIME [epoch: 10.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.063759886956599		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 4.063759886956599 | validation: 4.984117204174143]
	TIME [epoch: 10.3 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.10643245921324		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 4.10643245921324 | validation: 6.107895905341593]
	TIME [epoch: 10.3 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.663654249700498		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 4.663654249700498 | validation: 5.002694254044001]
	TIME [epoch: 10.3 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9862336361783326		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.9862336361783326 | validation: 5.583168146196974]
	TIME [epoch: 10.3 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3486818331727815		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 4.3486818331727815 | validation: 5.129285855926203]
	TIME [epoch: 10.3 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.036359170122784		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 4.036359170122784 | validation: 5.193075204954992]
	TIME [epoch: 10.3 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1382499608419465		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 4.1382499608419465 | validation: 4.976327254644641]
	TIME [epoch: 10.3 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.052822022429533		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 4.052822022429533 | validation: 4.879266706588754]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1005782017558925		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 4.1005782017558925 | validation: 5.274624902674845]
	TIME [epoch: 10.3 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.150235895019548		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 4.150235895019548 | validation: 5.487782246467577]
	TIME [epoch: 10.3 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.231604328197501		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 4.231604328197501 | validation: 4.907831172639538]
	TIME [epoch: 10.3 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.031652357200139		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 4.031652357200139 | validation: 4.978883281853215]
	TIME [epoch: 10.3 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.027903549092615		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 4.027903549092615 | validation: 5.078804287414816]
	TIME [epoch: 10.3 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.155005460859022		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 4.155005460859022 | validation: 4.9490551827417715]
	TIME [epoch: 10.3 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.067521360388161		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 4.067521360388161 | validation: 5.296051340791025]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.146007418453438		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 4.146007418453438 | validation: 4.953520099816553]
	TIME [epoch: 10.3 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2437711007691234		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 4.2437711007691234 | validation: 5.016782812005244]
	TIME [epoch: 10.3 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.09136540376369		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 4.09136540376369 | validation: 5.8259456552222595]
	TIME [epoch: 10.3 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.354718723854353		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 4.354718723854353 | validation: 5.017087794536523]
	TIME [epoch: 10.3 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9931854209704305		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.9931854209704305 | validation: 5.108599543789516]
	TIME [epoch: 10.3 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.162134606684882		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 4.162134606684882 | validation: 4.975182516210325]
	TIME [epoch: 10.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9748673744099703		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.9748673744099703 | validation: 5.071057176861407]
	TIME [epoch: 10.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0582670024212995		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 4.0582670024212995 | validation: 5.037177251434768]
	TIME [epoch: 10.3 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.069255516874079		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 4.069255516874079 | validation: 4.983139196338233]
	TIME [epoch: 10.3 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013180976306652		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 4.013180976306652 | validation: 5.365954016015567]
	TIME [epoch: 10.3 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.166243831553426		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 4.166243831553426 | validation: 4.910044177839896]
	TIME [epoch: 10.3 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.00612255712836		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 4.00612255712836 | validation: 4.978651874722827]
	TIME [epoch: 10.3 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.068848640957983		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 4.068848640957983 | validation: 4.993150777465737]
	TIME [epoch: 10.3 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1534471055479685		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 4.1534471055479685 | validation: 4.970902198546666]
	TIME [epoch: 10.3 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.941504670864905		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.941504670864905 | validation: 4.875846476417319]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.067522203687381		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 4.067522203687381 | validation: 5.405810601489084]
	TIME [epoch: 10.3 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.244463841805301		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 4.244463841805301 | validation: 5.453204713461791]
	TIME [epoch: 10.3 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.073282409085503		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 4.073282409085503 | validation: 5.052270481569778]
	TIME [epoch: 10.3 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.016271303248791		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 4.016271303248791 | validation: 5.155953130775115]
	TIME [epoch: 10.3 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.143618544996658		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 4.143618544996658 | validation: 4.892898195475915]
	TIME [epoch: 10.3 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9602945706129495		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 3.9602945706129495 | validation: 4.988208102257613]
	TIME [epoch: 10.3 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.969174977338086		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 3.969174977338086 | validation: 5.018609303405139]
	TIME [epoch: 10.3 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.144022218290716		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 4.144022218290716 | validation: 5.120725112740264]
	TIME [epoch: 10.3 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.038014058877149		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 4.038014058877149 | validation: 5.035057867990282]
	TIME [epoch: 10.3 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.157859278664194		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 4.157859278664194 | validation: 4.903665636162016]
	TIME [epoch: 10.3 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.233440735568879		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 4.233440735568879 | validation: 5.17989926409843]
	TIME [epoch: 10.3 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.996065788919585		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 3.996065788919585 | validation: 5.097820464746269]
	TIME [epoch: 10.3 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.056685838122805		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 4.056685838122805 | validation: 4.913393482469409]
	TIME [epoch: 10.3 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.034461968389811		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 4.034461968389811 | validation: 4.86961701518634]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.181708744503279		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 4.181708744503279 | validation: 4.9554192969494215]
	TIME [epoch: 10.3 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.963174870917866		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.963174870917866 | validation: 5.111350284741965]
	TIME [epoch: 10.3 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9695249923005065		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 3.9695249923005065 | validation: 5.04214006153757]
	TIME [epoch: 10.3 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.994919307620107		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 3.994919307620107 | validation: 4.899692175625006]
	TIME [epoch: 10.3 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9327191491392015		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 3.9327191491392015 | validation: 5.694196179206789]
	TIME [epoch: 10.3 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.352570681813476		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 4.352570681813476 | validation: 4.9227322219239555]
	TIME [epoch: 10.3 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9982330936113852		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.9982330936113852 | validation: 5.055649107043968]
	TIME [epoch: 10.3 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.018782517242191		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 4.018782517242191 | validation: 4.860921088118955]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.990742259762411		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.990742259762411 | validation: 4.858681190203455]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9776303481565494		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 3.9776303481565494 | validation: 4.854258896364563]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9765718918553703		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 3.9765718918553703 | validation: 4.99606835879487]
	TIME [epoch: 10.3 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.942590949758754		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.942590949758754 | validation: 4.983452128679494]
	TIME [epoch: 10.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9992772826735545		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 3.9992772826735545 | validation: 4.902541272458533]
	TIME [epoch: 10.3 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9943440938377277		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 3.9943440938377277 | validation: 4.907137230977005]
	TIME [epoch: 10.3 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.047230676354231		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 4.047230676354231 | validation: 5.091164561764298]
	TIME [epoch: 10.3 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.220870566788319		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 4.220870566788319 | validation: 5.112835301510387]
	TIME [epoch: 10.3 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.140138186449413		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 4.140138186449413 | validation: 4.967389741521994]
	TIME [epoch: 10.3 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.09207151507405		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 4.09207151507405 | validation: 4.87090060122587]
	TIME [epoch: 10.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.939852993552388		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 3.939852993552388 | validation: 5.153582000469853]
	TIME [epoch: 10.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013063227899622		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 4.013063227899622 | validation: 4.885007494096491]
	TIME [epoch: 10.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9903917269520393		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 3.9903917269520393 | validation: 4.957940075929703]
	TIME [epoch: 10.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9771557845985583		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.9771557845985583 | validation: 4.886295543680683]
	TIME [epoch: 10.3 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.020211736252751		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 4.020211736252751 | validation: 5.192295991698891]
	TIME [epoch: 10.3 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1023537345555585		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 4.1023537345555585 | validation: 4.8596547056335035]
	TIME [epoch: 10.3 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.052035399702663		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 4.052035399702663 | validation: 4.908017772699362]
	TIME [epoch: 10.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.030940847806572		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 4.030940847806572 | validation: 4.866367389138564]
	TIME [epoch: 10.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9356212575905105		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.9356212575905105 | validation: 6.19502198506948]
	TIME [epoch: 10.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.340626213348014		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 4.340626213348014 | validation: 4.8810483868698755]
	TIME [epoch: 10.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8991939352588396		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 3.8991939352588396 | validation: 4.968634076452735]
	TIME [epoch: 10.3 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.932096682976715		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.932096682976715 | validation: 5.184750558348459]
	TIME [epoch: 10.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.032836450317359		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 4.032836450317359 | validation: 5.484665336176309]
	TIME [epoch: 10.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1652299918552504		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 4.1652299918552504 | validation: 4.948344765618258]
	TIME [epoch: 10.3 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.059592485283855		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 4.059592485283855 | validation: 4.895149522043815]
	TIME [epoch: 10.3 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.069944671854247		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 4.069944671854247 | validation: 4.93180590745503]
	TIME [epoch: 10.3 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9523184626847994		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.9523184626847994 | validation: 5.090854626079899]
	TIME [epoch: 10.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.036574654673653		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 4.036574654673653 | validation: 4.861984182752528]
	TIME [epoch: 10.3 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9614171943200573		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.9614171943200573 | validation: 4.950495876872704]
	TIME [epoch: 10.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.004791966223863		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 4.004791966223863 | validation: 4.963629893095195]
	TIME [epoch: 10.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.977118536098696		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 3.977118536098696 | validation: 5.080499741815912]
	TIME [epoch: 10.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9915381579802824		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 3.9915381579802824 | validation: 4.9504448355809805]
	TIME [epoch: 10.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0585369854511		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 4.0585369854511 | validation: 4.959784838810286]
	TIME [epoch: 10.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.010865520562108		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 4.010865520562108 | validation: 4.922138923363299]
	TIME [epoch: 10.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.987418677505712		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.987418677505712 | validation: 4.898489947585379]
	TIME [epoch: 10.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9686729940042214		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.9686729940042214 | validation: 4.9581653836855075]
	TIME [epoch: 10.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.97039022914239		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.97039022914239 | validation: 4.914124120100103]
	TIME [epoch: 10.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.080521367310139		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 4.080521367310139 | validation: 5.499192499778062]
	TIME [epoch: 10.3 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.242657632341911		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 4.242657632341911 | validation: 4.92318806206594]
	TIME [epoch: 10.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.064793763857698		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 4.064793763857698 | validation: 5.336689388925777]
	TIME [epoch: 10.3 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.012417621811586		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 4.012417621811586 | validation: 6.323183173774992]
	TIME [epoch: 10.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3944624698554335		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 4.3944624698554335 | validation: 4.875761136510909]
	TIME [epoch: 10.3 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9602969475359564		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.9602969475359564 | validation: 4.867859438620792]
	TIME [epoch: 10.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9520872860016083		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.9520872860016083 | validation: 4.8566178835249305]
	TIME [epoch: 10.3 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9124260452675466		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 3.9124260452675466 | validation: 4.8696385794915376]
	TIME [epoch: 10.3 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.935755181883196		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 3.935755181883196 | validation: 5.99994343835693]
	TIME [epoch: 10.3 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.389797789545069		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 4.389797789545069 | validation: 4.990035538802133]
	TIME [epoch: 10.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9592016082664463		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.9592016082664463 | validation: 4.873877521688273]
	TIME [epoch: 10.3 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.003901990360794		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 4.003901990360794 | validation: 4.850308181504367]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.909899284091291		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.909899284091291 | validation: 4.985321582302143]
	TIME [epoch: 10.3 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.011733831176898		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 4.011733831176898 | validation: 4.875219133662334]
	TIME [epoch: 10.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9412357120469466		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 3.9412357120469466 | validation: 4.974176356033607]
	TIME [epoch: 10.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9639656181473946		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 3.9639656181473946 | validation: 4.907439189703228]
	TIME [epoch: 10.3 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.950282244940476		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 3.950282244940476 | validation: 5.050242965240108]
	TIME [epoch: 10.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9790967054879856		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 3.9790967054879856 | validation: 4.849700637433651]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.085040300734428		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 4.085040300734428 | validation: 4.888418216548347]
	TIME [epoch: 10.3 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.216290662976124		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 4.216290662976124 | validation: 5.013640705338688]
	TIME [epoch: 10.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.089924948374831		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 4.089924948374831 | validation: 4.988350407732169]
	TIME [epoch: 10.3 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9518637263092176		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 3.9518637263092176 | validation: 4.944522331762454]
	TIME [epoch: 10.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9064979917432945		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.9064979917432945 | validation: 5.086037936291071]
	TIME [epoch: 10.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.028582699430123		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 4.028582699430123 | validation: 4.941771718121017]
	TIME [epoch: 10.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.893609878059981		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 3.893609878059981 | validation: 5.046406473918708]
	TIME [epoch: 10.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.93087362101726		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 3.93087362101726 | validation: 5.020936656399142]
	TIME [epoch: 10.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.963510894566003		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.963510894566003 | validation: 5.061905503329731]
	TIME [epoch: 10.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.945666530463246		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 3.945666530463246 | validation: 5.0880383158845754]
	TIME [epoch: 10.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.978984765602484		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.978984765602484 | validation: 4.859332005277947]
	TIME [epoch: 10.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.906148479066966		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 3.906148479066966 | validation: 4.888121747921524]
	TIME [epoch: 10.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9568626026156046		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 3.9568626026156046 | validation: 4.8641505603032105]
	TIME [epoch: 10.3 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.967712464642151		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 3.967712464642151 | validation: 4.908161609799686]
	TIME [epoch: 10.3 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9822556935322404		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 3.9822556935322404 | validation: 4.880374140943022]
	TIME [epoch: 10.3 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.913743491603401		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.913743491603401 | validation: 4.870090039146979]
	TIME [epoch: 10.3 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.990771843830597		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.990771843830597 | validation: 5.0416869102402515]
	TIME [epoch: 10.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9849093128555424		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.9849093128555424 | validation: 4.888209945721355]
	TIME [epoch: 10.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.879352653104476		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.879352653104476 | validation: 5.5355615205636495]
	TIME [epoch: 10.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.197545365634834		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 4.197545365634834 | validation: 4.921332738449943]
	TIME [epoch: 10.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9546532362292885		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.9546532362292885 | validation: 4.928576514334828]
	TIME [epoch: 10.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8898291358089794		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.8898291358089794 | validation: 4.84748834717124]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.969410732638709		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 3.969410732638709 | validation: 4.891422102086529]
	TIME [epoch: 10.3 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.921057439402793		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 3.921057439402793 | validation: 5.37779927718587]
	TIME [epoch: 10.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.220808024493121		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 4.220808024493121 | validation: 4.9708173238143845]
	TIME [epoch: 10.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.941323035428612		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 3.941323035428612 | validation: 4.921381221775342]
	TIME [epoch: 10.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.937287489905856		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.937287489905856 | validation: 4.84461296794615]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2623162977926325		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 4.2623162977926325 | validation: 5.979783718811587]
	TIME [epoch: 10.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.199302743391118		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 4.199302743391118 | validation: 4.8906440217452785]
	TIME [epoch: 10.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.89213832018988		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 3.89213832018988 | validation: 4.876791925028074]
	TIME [epoch: 10.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8974598201947224		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 3.8974598201947224 | validation: 4.906256049176153]
	TIME [epoch: 10.3 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9725128585801945		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.9725128585801945 | validation: 4.8765105864658045]
	TIME [epoch: 10.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.929944970092849		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.929944970092849 | validation: 5.082575469507488]
	TIME [epoch: 10.3 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9365152979723583		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.9365152979723583 | validation: 4.882212025615456]
	TIME [epoch: 10.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.911392362321122		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.911392362321122 | validation: 4.932490016535972]
	TIME [epoch: 10.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9475224358874774		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 3.9475224358874774 | validation: 4.947745160878046]
	TIME [epoch: 10.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9765063494194677		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 3.9765063494194677 | validation: 4.944315037969525]
	TIME [epoch: 10.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.898017883740578		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.898017883740578 | validation: 4.960201500035621]
	TIME [epoch: 10.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9055928722334827		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 3.9055928722334827 | validation: 4.848781233870815]
	TIME [epoch: 10.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.877171197291768		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.877171197291768 | validation: 5.059312896779769]
	TIME [epoch: 10.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.921573088262574		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 3.921573088262574 | validation: 4.8843960642731785]
	TIME [epoch: 10.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9628861749162483		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.9628861749162483 | validation: 4.86822233877228]
	TIME [epoch: 10.3 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8915382420151756		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.8915382420151756 | validation: 4.9337887845862864]
	TIME [epoch: 10.3 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.968342969783189		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 3.968342969783189 | validation: 5.047603624697225]
	TIME [epoch: 10.3 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9638634724288124		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.9638634724288124 | validation: 4.901049851159599]
	TIME [epoch: 10.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.886364233350875		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.886364233350875 | validation: 4.9489497736356745]
	TIME [epoch: 10.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.920831925578475		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.920831925578475 | validation: 4.877166075605012]
	TIME [epoch: 10.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9663585590884063		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.9663585590884063 | validation: 5.015713065771524]
	TIME [epoch: 10.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.068330903831094		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 4.068330903831094 | validation: 4.94118036616099]
	TIME [epoch: 10.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9496641547680413		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 3.9496641547680413 | validation: 4.9010610069698775]
	TIME [epoch: 10.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.868718307433197		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.868718307433197 | validation: 4.84025358469552]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8492499386100656		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.8492499386100656 | validation: 4.896742925487498]
	TIME [epoch: 10.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8753806391420467		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.8753806391420467 | validation: 5.0119565363655125]
	TIME [epoch: 10.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.898103973142001		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.898103973142001 | validation: 5.038316702200444]
	TIME [epoch: 10.3 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.03299414384473		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 4.03299414384473 | validation: 4.973274725293909]
	TIME [epoch: 10.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.945156353580132		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 3.945156353580132 | validation: 4.858990866846364]
	TIME [epoch: 10.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.065674544777772		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 4.065674544777772 | validation: 4.9810418597842805]
	TIME [epoch: 10.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.931273538321352		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.931273538321352 | validation: 4.8716150151674436]
	TIME [epoch: 10.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.000617756545622		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 4.000617756545622 | validation: 5.021610391266361]
	TIME [epoch: 10.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9695639536479304		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 3.9695639536479304 | validation: 5.259651755007317]
	TIME [epoch: 10.3 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.013323545360825		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 4.013323545360825 | validation: 4.8580290971384406]
	TIME [epoch: 10.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9505419804807667		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.9505419804807667 | validation: 4.884747454500016]
	TIME [epoch: 10.3 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8700382847836634		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 3.8700382847836634 | validation: 4.868133236472775]
	TIME [epoch: 10.3 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8630294175710604		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.8630294175710604 | validation: 4.9113687359079785]
	TIME [epoch: 10.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9409848040536444		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.9409848040536444 | validation: 5.121535614253162]
	TIME [epoch: 10.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9944989831469173		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.9944989831469173 | validation: 4.883219646604007]
	TIME [epoch: 10.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8806834394414684		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.8806834394414684 | validation: 4.886939964841091]
	TIME [epoch: 10.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8641712136984494		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 3.8641712136984494 | validation: 5.011620522015926]
	TIME [epoch: 10.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9130611816091565		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.9130611816091565 | validation: 4.909408123746138]
	TIME [epoch: 10.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9077996598249065		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.9077996598249065 | validation: 4.880418594940784]
	TIME [epoch: 10.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8570502207491026		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.8570502207491026 | validation: 4.975472227128937]
	TIME [epoch: 10.3 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9483742186799815		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 3.9483742186799815 | validation: 5.050706170771931]
	TIME [epoch: 10.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.041996641163093		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 4.041996641163093 | validation: 4.865126513492977]
	TIME [epoch: 10.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.865229122750294		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.865229122750294 | validation: 4.884923504761508]
	TIME [epoch: 10.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9069696600714954		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.9069696600714954 | validation: 4.868141563518409]
	TIME [epoch: 10.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.86295838074576		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.86295838074576 | validation: 4.960890820818508]
	TIME [epoch: 10.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.04051679638094		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 4.04051679638094 | validation: 4.9513331934927285]
	TIME [epoch: 10.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9487043216755042		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.9487043216755042 | validation: 4.885481077677894]
	TIME [epoch: 10.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.951497253065804		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.951497253065804 | validation: 4.8402514539369035]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8643189232424944		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.8643189232424944 | validation: 4.869885493429098]
	TIME [epoch: 10.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8662779837750976		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.8662779837750976 | validation: 5.008791932293569]
	TIME [epoch: 10.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.956780930506871		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.956780930506871 | validation: 4.897035231340521]
	TIME [epoch: 10.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.083693015685953		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 4.083693015685953 | validation: 4.871220804622722]
	TIME [epoch: 10.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8804158559504147		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.8804158559504147 | validation: 5.171945145965583]
	TIME [epoch: 10.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.002991106113888		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 4.002991106113888 | validation: 4.988143440422386]
	TIME [epoch: 10.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.90537946325239		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.90537946325239 | validation: 5.052320837147888]
	TIME [epoch: 10.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8865416897764526		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 3.8865416897764526 | validation: 5.099300655316934]
	TIME [epoch: 10.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9839438417117194		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 3.9839438417117194 | validation: 4.9218586218032625]
	TIME [epoch: 10.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.883527901045305		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.883527901045305 | validation: 5.029909083321611]
	TIME [epoch: 10.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9958206848477196		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 3.9958206848477196 | validation: 4.848202631257433]
	TIME [epoch: 10.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.879907195448516		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.879907195448516 | validation: 4.920442858780981]
	TIME [epoch: 10.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.000256458300648		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 4.000256458300648 | validation: 4.885864624658064]
	TIME [epoch: 10.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9026892775332938		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 3.9026892775332938 | validation: 4.901860049417171]
	TIME [epoch: 10.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.878438963889135		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.878438963889135 | validation: 5.059780411023884]
	TIME [epoch: 10.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9285503769098056		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.9285503769098056 | validation: 4.861529885783628]
	TIME [epoch: 10.3 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.026981199157642		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 4.026981199157642 | validation: 4.941696859607983]
	TIME [epoch: 10.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.864566762722176		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 3.864566762722176 | validation: 4.858065378983922]
	TIME [epoch: 10.3 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.897114016189343		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.897114016189343 | validation: 4.86208394602251]
	TIME [epoch: 10.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9421624830152604		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.9421624830152604 | validation: 4.869339329685372]
	TIME [epoch: 10.3 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.952292541522742		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.952292541522742 | validation: 4.881993125726288]
	TIME [epoch: 10.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8325819978066287		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.8325819978066287 | validation: 5.07966770749854]
	TIME [epoch: 10.3 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9705901830679737		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.9705901830679737 | validation: 4.864391577498763]
	TIME [epoch: 10.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8531149592685323		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 3.8531149592685323 | validation: 4.953310752693585]
	TIME [epoch: 10.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.159787525347161		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 4.159787525347161 | validation: 5.072648496811316]
	TIME [epoch: 10.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.91451636084503		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.91451636084503 | validation: 5.103892094140706]
	TIME [epoch: 10.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.021755120320649		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 4.021755120320649 | validation: 4.87095153822849]
	TIME [epoch: 10.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9233235581305337		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.9233235581305337 | validation: 4.946117335310488]
	TIME [epoch: 10.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.926479989902351		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.926479989902351 | validation: 5.1009550993561]
	TIME [epoch: 10.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9610092921437454		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.9610092921437454 | validation: 4.92061740806802]
	TIME [epoch: 10.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8808009716242724		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.8808009716242724 | validation: 4.868857678055392]
	TIME [epoch: 10.3 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8817955624512583		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 3.8817955624512583 | validation: 4.913950119992903]
	TIME [epoch: 10.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9675458023543855		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.9675458023543855 | validation: 4.857696078934631]
	TIME [epoch: 10.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9222527099577165		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.9222527099577165 | validation: 4.857111633782198]
	TIME [epoch: 10.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8772830278712775		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 3.8772830278712775 | validation: 4.999048916912213]
	TIME [epoch: 10.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8802538383866123		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.8802538383866123 | validation: 4.8970615046453805]
	TIME [epoch: 10.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9109768058749546		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.9109768058749546 | validation: 4.873607148048304]
	TIME [epoch: 10.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9042144314229787		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.9042144314229787 | validation: 4.881749344849702]
	TIME [epoch: 10.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9445141476357803		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 3.9445141476357803 | validation: 4.968507853801612]
	TIME [epoch: 10.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9242620086958615		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.9242620086958615 | validation: 4.850333484416677]
	TIME [epoch: 10.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9105636608896175		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.9105636608896175 | validation: 4.851760448990665]
	TIME [epoch: 10.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.895415234037549		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.895415234037549 | validation: 4.872978485109358]
	TIME [epoch: 10.3 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.945323188297908		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.945323188297908 | validation: 4.901740108528019]
	TIME [epoch: 10.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8674763269034145		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.8674763269034145 | validation: 4.921725561694344]
	TIME [epoch: 10.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.004842027493752		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 4.004842027493752 | validation: 4.93512941670819]
	TIME [epoch: 10.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9091292201016556		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.9091292201016556 | validation: 4.8486919111605085]
	TIME [epoch: 10.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8581329501255355		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.8581329501255355 | validation: 4.888461556083403]
	TIME [epoch: 10.3 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8810673342587636		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.8810673342587636 | validation: 4.867775218213356]
	TIME [epoch: 10.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.857646538010877		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.857646538010877 | validation: 4.973683292083014]
	TIME [epoch: 10.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.933245610066776		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.933245610066776 | validation: 5.044068378522959]
	TIME [epoch: 10.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.88079778693234		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.88079778693234 | validation: 4.858942481397588]
	TIME [epoch: 10.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8980439643266713		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 3.8980439643266713 | validation: 5.126399191862297]
	TIME [epoch: 10.3 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0093750295929365		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 4.0093750295929365 | validation: 4.860755384597514]
	TIME [epoch: 10.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.918280900585342		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.918280900585342 | validation: 4.873735141937352]
	TIME [epoch: 10.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.96154995984975		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.96154995984975 | validation: 4.8713582957093156]
	TIME [epoch: 10.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.994934806552614		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.994934806552614 | validation: 4.906454627056267]
	TIME [epoch: 10.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.924641433485328		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.924641433485328 | validation: 4.886932151998495]
	TIME [epoch: 10.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.898347944310963		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.898347944310963 | validation: 4.902807608377193]
	TIME [epoch: 10.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8914447748788157		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.8914447748788157 | validation: 4.898911994727288]
	TIME [epoch: 10.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8948160356653005		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.8948160356653005 | validation: 4.907251963161995]
	TIME [epoch: 10.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9950562642948184		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.9950562642948184 | validation: 5.058953848243008]
	TIME [epoch: 10.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9232517198073067		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.9232517198073067 | validation: 4.853649222880408]
	TIME [epoch: 10.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8963755882375706		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.8963755882375706 | validation: 4.988474731259941]
	TIME [epoch: 10.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.91240639705127		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.91240639705127 | validation: 4.84354883450353]
	TIME [epoch: 10.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9124454093737526		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 3.9124454093737526 | validation: 4.869420264389112]
	TIME [epoch: 10.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.861149224553121		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.861149224553121 | validation: 5.092763750460174]
	TIME [epoch: 10.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.919613038718631		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.919613038718631 | validation: 4.871884889135941]
	TIME [epoch: 10.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.834354364121915		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.834354364121915 | validation: 4.945008171701943]
	TIME [epoch: 10.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8854791008892136		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 3.8854791008892136 | validation: 5.297480343075011]
	TIME [epoch: 10.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.01128509716093		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 4.01128509716093 | validation: 4.886774319133249]
	TIME [epoch: 10.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8547069918780466		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.8547069918780466 | validation: 4.9947173027738545]
	TIME [epoch: 10.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8963017460924356		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.8963017460924356 | validation: 4.87440205933496]
	TIME [epoch: 10.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.868657178365715		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.868657178365715 | validation: 4.877892204650455]
	TIME [epoch: 10.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9521490856008206		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.9521490856008206 | validation: 5.082019946063666]
	TIME [epoch: 10.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9795261154752692		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.9795261154752692 | validation: 4.924934642755763]
	TIME [epoch: 10.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.869619381730357		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.869619381730357 | validation: 4.9059890886138575]
	TIME [epoch: 10.3 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8594920737385605		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.8594920737385605 | validation: 5.014790497762498]
	TIME [epoch: 10.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9352512743575994		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.9352512743575994 | validation: 4.877534233823793]
	TIME [epoch: 10.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8774985693659616		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.8774985693659616 | validation: 4.921872378891558]
	TIME [epoch: 10.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.866250905045095		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.866250905045095 | validation: 4.963902250547396]
	TIME [epoch: 10.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8583008722765513		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.8583008722765513 | validation: 5.086129023695342]
	TIME [epoch: 10.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.888886788065963		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.888886788065963 | validation: 4.859880548954046]
	TIME [epoch: 10.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848601854234137		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.848601854234137 | validation: 4.954064071566025]
	TIME [epoch: 10.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8621655929872967		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.8621655929872967 | validation: 4.8550129751476945]
	TIME [epoch: 10.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849125783246328		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.849125783246328 | validation: 4.904698475044594]
	TIME [epoch: 10.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862365488889163		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.862365488889163 | validation: 4.852194597261956]
	TIME [epoch: 10.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.846146020308263		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.846146020308263 | validation: 4.89435623227894]
	TIME [epoch: 10.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.95332240882772		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.95332240882772 | validation: 4.874824993783917]
	TIME [epoch: 10.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8378990944220917		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.8378990944220917 | validation: 4.849710658106866]
	TIME [epoch: 10.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8747610779335098		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.8747610779335098 | validation: 4.878533842545371]
	TIME [epoch: 10.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.951975482932877		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.951975482932877 | validation: 4.856231977644202]
	TIME [epoch: 10.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.837894532918245		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.837894532918245 | validation: 4.855946321354795]
	TIME [epoch: 10.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.20164844992429		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 4.20164844992429 | validation: 4.859243524453939]
	TIME [epoch: 10.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.901674226363025		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.901674226363025 | validation: 4.894143070985271]
	TIME [epoch: 10.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8606515776292283		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.8606515776292283 | validation: 4.935996258132956]
	TIME [epoch: 10.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.877627099131588		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.877627099131588 | validation: 4.84519074233419]
	TIME [epoch: 10.3 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.87518429394603		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.87518429394603 | validation: 4.861909767541201]
	TIME [epoch: 10.3 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8782783793158715		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.8782783793158715 | validation: 4.991357742212685]
	TIME [epoch: 10.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.904353933985906		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.904353933985906 | validation: 4.885004657126183]
	TIME [epoch: 10.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8866892416017373		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.8866892416017373 | validation: 4.842860379391778]
	TIME [epoch: 10.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8921107318195354		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.8921107318195354 | validation: 4.939064024450359]
	TIME [epoch: 10.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9269642998098795		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.9269642998098795 | validation: 4.875424367840342]
	TIME [epoch: 10.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8426866807442956		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.8426866807442956 | validation: 4.8436197160682735]
	TIME [epoch: 10.3 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.894862774540569		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.894862774540569 | validation: 4.8511843336989475]
	TIME [epoch: 10.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832935009303514		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.832935009303514 | validation: 4.878020054094395]
	TIME [epoch: 10.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8456592148889954		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.8456592148889954 | validation: 4.960930054956026]
	TIME [epoch: 10.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9508497823170408		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.9508497823170408 | validation: 5.271355890656414]
	TIME [epoch: 10.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.112332301699995		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 4.112332301699995 | validation: 4.8654252124993205]
	TIME [epoch: 10.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.91082227983241		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.91082227983241 | validation: 4.923254270610281]
	TIME [epoch: 10.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.165192222002619		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 4.165192222002619 | validation: 4.939548607071116]
	TIME [epoch: 10.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9041926313711697		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.9041926313711697 | validation: 4.862022387296162]
	TIME [epoch: 10.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.836735955307616		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.836735955307616 | validation: 4.847425061083094]
	TIME [epoch: 10.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8414011960004104		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.8414011960004104 | validation: 4.8641001942821624]
	TIME [epoch: 10.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8942883667746577		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.8942883667746577 | validation: 4.854107654148163]
	TIME [epoch: 10.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8535666611922785		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.8535666611922785 | validation: 4.937318656340996]
	TIME [epoch: 10.3 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8713044859037224		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.8713044859037224 | validation: 4.906311204754257]
	TIME [epoch: 10.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8704731271024997		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 3.8704731271024997 | validation: 4.870578466695146]
	TIME [epoch: 10.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.839983043606966		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.839983043606966 | validation: 4.841700068399984]
	TIME [epoch: 10.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9131859020853383		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.9131859020853383 | validation: 4.872979317235336]
	TIME [epoch: 10.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8804867799254756		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.8804867799254756 | validation: 4.878452936619644]
	TIME [epoch: 10.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.843667582879712		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.843667582879712 | validation: 4.849404719001827]
	TIME [epoch: 10.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.859535650820552		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.859535650820552 | validation: 4.90971488846317]
	TIME [epoch: 10.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9038426011159566		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.9038426011159566 | validation: 5.061682646644792]
	TIME [epoch: 10.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9023005945248577		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.9023005945248577 | validation: 4.8668673682543835]
	TIME [epoch: 10.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.843982228791851		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.843982228791851 | validation: 4.8390931589434]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.885821361010174		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.885821361010174 | validation: 4.957266601578698]
	TIME [epoch: 10.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.916237718060959		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.916237718060959 | validation: 4.838061422372053]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848608875905994		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.848608875905994 | validation: 4.894230115608882]
	TIME [epoch: 10.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8521700381507102		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.8521700381507102 | validation: 4.8369561494388655]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8675344634438127		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 3.8675344634438127 | validation: 4.899665443616985]
	TIME [epoch: 10.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8511268495180344		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.8511268495180344 | validation: 4.8455816296734255]
	TIME [epoch: 10.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.855970874396295		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.855970874396295 | validation: 4.9247940984887775]
	TIME [epoch: 10.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.855438865989904		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.855438865989904 | validation: 5.068814055646871]
	TIME [epoch: 10.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.943105047884667		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.943105047884667 | validation: 4.903042291981653]
	TIME [epoch: 10.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9144603243446943		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.9144603243446943 | validation: 4.835051080064743]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8369359017326117		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.8369359017326117 | validation: 4.857259579178923]
	TIME [epoch: 10.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.895980095102827		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.895980095102827 | validation: 4.883710411272057]
	TIME [epoch: 10.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8905375790312817		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.8905375790312817 | validation: 4.887379681447945]
	TIME [epoch: 10.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8308595619116375		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.8308595619116375 | validation: 4.837887388010442]
	TIME [epoch: 10.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.867857369199098		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.867857369199098 | validation: 4.876489196934818]
	TIME [epoch: 10.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9047174869305414		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.9047174869305414 | validation: 4.916893831134074]
	TIME [epoch: 10.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9465860824879244		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.9465860824879244 | validation: 4.882729423785011]
	TIME [epoch: 10.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.851771756678866		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.851771756678866 | validation: 4.864271387294058]
	TIME [epoch: 10.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.858062502750239		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.858062502750239 | validation: 5.077258970227957]
	TIME [epoch: 10.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8865309154894154		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.8865309154894154 | validation: 4.868115490173417]
	TIME [epoch: 10.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.838694586912009		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.838694586912009 | validation: 4.845642073213079]
	TIME [epoch: 10.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.874621617706609		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.874621617706609 | validation: 4.863209557401823]
	TIME [epoch: 10.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9061209940490413		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.9061209940490413 | validation: 4.839607299472994]
	TIME [epoch: 10.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8245058993550627		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.8245058993550627 | validation: 4.8936917228408365]
	TIME [epoch: 10.3 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.852633904427082		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.852633904427082 | validation: 4.8444586023122564]
	TIME [epoch: 10.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826858673113777		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.826858673113777 | validation: 4.630824137479427]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.803865837861038		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.803865837861038 | validation: 4.868753769137057]
	TIME [epoch: 10.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8596232995577195		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.8596232995577195 | validation: 5.0233586622961965]
	TIME [epoch: 10.3 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9063962141059116		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.9063962141059116 | validation: 4.834919521430942]
	TIME [epoch: 10.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8378776414612106		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.8378776414612106 | validation: 4.876451874477412]
	TIME [epoch: 10.3 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.863507652842606		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.863507652842606 | validation: 4.967828591170729]
	TIME [epoch: 10.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8688735757958783		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.8688735757958783 | validation: 4.873028557680806]
	TIME [epoch: 10.3 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.896392325603121		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.896392325603121 | validation: 4.837040968011393]
	TIME [epoch: 10.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.886433255057733		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.886433255057733 | validation: 4.83029927997858]
	TIME [epoch: 10.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8360291755663702		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.8360291755663702 | validation: 4.8988282691651515]
	TIME [epoch: 10.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.836029109526657		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.836029109526657 | validation: 4.8459895165313505]
	TIME [epoch: 10.3 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832160252257874		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.832160252257874 | validation: 4.868321895430617]
	TIME [epoch: 10.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.890905044897927		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.890905044897927 | validation: 4.873973008297276]
	TIME [epoch: 10.3 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.831522674898737		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.831522674898737 | validation: 4.867703115870358]
	TIME [epoch: 10.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8545441447911273		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.8545441447911273 | validation: 5.049284474437565]
	TIME [epoch: 10.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8830200370126264		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.8830200370126264 | validation: 4.837482528678741]
	TIME [epoch: 10.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8367870317990596		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.8367870317990596 | validation: 4.872616366546561]
	TIME [epoch: 10.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8720249346851956		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.8720249346851956 | validation: 4.843601630269359]
	TIME [epoch: 10.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.860208130092846		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.860208130092846 | validation: 4.8619835643835145]
	TIME [epoch: 10.3 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8312512382189454		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.8312512382189454 | validation: 4.871342948092214]
	TIME [epoch: 10.3 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.843248842710504		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.843248842710504 | validation: 4.932734593300161]
	TIME [epoch: 10.3 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849286599944852		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.849286599944852 | validation: 4.8335325460854754]
	TIME [epoch: 10.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8507648419849163		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.8507648419849163 | validation: 4.8464446256689095]
	TIME [epoch: 10.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.878714889722837		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.878714889722837 | validation: 4.843511967773272]
	TIME [epoch: 10.3 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.846609906408949		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.846609906408949 | validation: 4.846804462295828]
	TIME [epoch: 10.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832794281838608		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.832794281838608 | validation: 4.835509900094897]
	TIME [epoch: 10.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8371744578322073		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.8371744578322073 | validation: 4.91375765996996]
	TIME [epoch: 10.3 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862866512547986		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.862866512547986 | validation: 4.873117640770441]
	TIME [epoch: 10.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8329038277027707		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.8329038277027707 | validation: 5.039529854301429]
	TIME [epoch: 10.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9649285069796605		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.9649285069796605 | validation: 4.856356074616331]
	TIME [epoch: 10.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.83345401435792		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.83345401435792 | validation: 4.892211862628684]
	TIME [epoch: 10.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.775774452816144		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.775774452816144 | validation: 4.482906627336836]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524082040498681		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.524082040498681 | validation: 4.523861651490307]
	TIME [epoch: 10.3 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51404784005746		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.51404784005746 | validation: 4.600921868298931]
	TIME [epoch: 10.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7253226492546196		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.7253226492546196 | validation: 4.845354641231902]
	TIME [epoch: 10.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8472432262274068		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.8472432262274068 | validation: 4.884822812925471]
	TIME [epoch: 10.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849594583680381		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.849594583680381 | validation: 4.83976028871425]
	TIME [epoch: 10.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8397916358418884		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.8397916358418884 | validation: 4.834857115255068]
	TIME [epoch: 10.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820960720554941		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.820960720554941 | validation: 4.846311644586574]
	TIME [epoch: 10.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826036820428257		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.826036820428257 | validation: 4.822792342370403]
	TIME [epoch: 10.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8565316015710223		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.8565316015710223 | validation: 4.852746644209896]
	TIME [epoch: 10.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.851025011745702		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.851025011745702 | validation: 4.887450851610315]
	TIME [epoch: 10.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8275962123543295		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.8275962123543295 | validation: 4.838781693970053]
	TIME [epoch: 10.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8315592973217973		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.8315592973217973 | validation: 4.868053882160225]
	TIME [epoch: 10.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8409709343927565		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.8409709343927565 | validation: 4.8315903433537795]
	TIME [epoch: 10.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827770392652655		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.827770392652655 | validation: 4.836614932783403]
	TIME [epoch: 10.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8278653538754583		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 3.8278653538754583 | validation: 4.841124264566935]
	TIME [epoch: 10.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8623903926934497		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.8623903926934497 | validation: 4.902253978954262]
	TIME [epoch: 10.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8653448819038942		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.8653448819038942 | validation: 4.9081879101620345]
	TIME [epoch: 10.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8387690530133947		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.8387690530133947 | validation: 4.883609954993812]
	TIME [epoch: 10.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.857715235221979		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.857715235221979 | validation: 4.860471515189757]
	TIME [epoch: 10.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848363282586798		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.848363282586798 | validation: 4.834122598802702]
	TIME [epoch: 10.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.870072056315267		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.870072056315267 | validation: 4.8401897922187604]
	TIME [epoch: 10.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.840673644055511		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.840673644055511 | validation: 4.830673619608758]
	TIME [epoch: 10.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.842735427280645		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.842735427280645 | validation: 4.829043436555453]
	TIME [epoch: 10.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.839078113479148		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.839078113479148 | validation: 4.835622273235043]
	TIME [epoch: 10.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827904098360528		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.827904098360528 | validation: 4.8718828412196356]
	TIME [epoch: 10.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8882487747692958		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.8882487747692958 | validation: 4.851350461466264]
	TIME [epoch: 10.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8413033316075826		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.8413033316075826 | validation: 4.890851284953001]
	TIME [epoch: 10.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8547795326400824		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.8547795326400824 | validation: 4.871241411142304]
	TIME [epoch: 10.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8769187653228685		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.8769187653228685 | validation: 4.873743190538972]
	TIME [epoch: 10.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8357867903662255		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.8357867903662255 | validation: 4.8299346176891245]
	TIME [epoch: 10.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8247728087052346		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.8247728087052346 | validation: 4.924883278613727]
	TIME [epoch: 10.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8564468041591295		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.8564468041591295 | validation: 4.845366800198608]
	TIME [epoch: 10.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8240935808704117		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.8240935808704117 | validation: 4.8938492185802716]
	TIME [epoch: 10.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.83592394084042		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.83592394084042 | validation: 4.856779508602807]
	TIME [epoch: 10.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8619137009575546		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.8619137009575546 | validation: 4.834304773069521]
	TIME [epoch: 10.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8276643217572515		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.8276643217572515 | validation: 4.84016476420489]
	TIME [epoch: 10.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.846418797313711		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.846418797313711 | validation: 4.877732964763176]
	TIME [epoch: 10.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.909468698602942		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.909468698602942 | validation: 4.831291562305138]
	TIME [epoch: 10.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849355137077569		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.849355137077569 | validation: 4.855902935191711]
	TIME [epoch: 10.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862899856937621		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.862899856937621 | validation: 4.87799233098934]
	TIME [epoch: 10.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8440152715331393		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.8440152715331393 | validation: 4.849507657673]
	TIME [epoch: 10.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8185808803436303		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.8185808803436303 | validation: 4.915885239619778]
	TIME [epoch: 10.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8500678440411513		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.8500678440411513 | validation: 4.845683797060684]
	TIME [epoch: 10.3 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8341291621181526		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.8341291621181526 | validation: 4.832607746161909]
	TIME [epoch: 10.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8282889869373973		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.8282889869373973 | validation: 5.029492801054382]
	TIME [epoch: 10.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8694366524525727		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.8694366524525727 | validation: 4.844497903876738]
	TIME [epoch: 10.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8333205320554056		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.8333205320554056 | validation: 4.863310001931511]
	TIME [epoch: 10.3 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8439077408921714		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.8439077408921714 | validation: 4.866301862134381]
	TIME [epoch: 10.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8599425832741603		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.8599425832741603 | validation: 4.880938126148284]
	TIME [epoch: 10.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8413176480340723		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 3.8413176480340723 | validation: 4.832283416041393]
	TIME [epoch: 10.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.830457616821633		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.830457616821633 | validation: 4.888230405969889]
	TIME [epoch: 10.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.842639016029195		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.842639016029195 | validation: 4.848122304167462]
	TIME [epoch: 10.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8246480407080496		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.8246480407080496 | validation: 4.923709634403359]
	TIME [epoch: 10.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849221712740163		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.849221712740163 | validation: 4.879180623452505]
	TIME [epoch: 10.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9076110634481376		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.9076110634481376 | validation: 4.851332401233671]
	TIME [epoch: 10.3 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.044113225581833		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 4.044113225581833 | validation: 4.88251081704649]
	TIME [epoch: 10.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8389967425104516		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.8389967425104516 | validation: 4.861105069629812]
	TIME [epoch: 10.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8435040720822764		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.8435040720822764 | validation: 4.947217937092389]
	TIME [epoch: 10.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.82864717411998		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.82864717411998 | validation: 4.838293995021537]
	TIME [epoch: 10.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823486251808916		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.823486251808916 | validation: 4.853715743543516]
	TIME [epoch: 10.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.82995637028898		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.82995637028898 | validation: 4.842159029498897]
	TIME [epoch: 10.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9014312637486612		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.9014312637486612 | validation: 4.915269729010838]
	TIME [epoch: 10.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8193846572934165		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.8193846572934165 | validation: 4.922524044504491]
	TIME [epoch: 10.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829257264680346		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.829257264680346 | validation: 4.895398069827849]
	TIME [epoch: 10.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8520444400772034		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.8520444400772034 | validation: 4.867588011088163]
	TIME [epoch: 10.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.888991103423377		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.888991103423377 | validation: 4.839298348953792]
	TIME [epoch: 10.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.842140008666199		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.842140008666199 | validation: 4.84868859928486]
	TIME [epoch: 10.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8793818066737074		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 3.8793818066737074 | validation: 4.844065894797364]
	TIME [epoch: 10.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.862362102709497		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.862362102709497 | validation: 4.851775706504643]
	TIME [epoch: 10.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.841165346292617		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.841165346292617 | validation: 4.864786009296198]
	TIME [epoch: 10.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.819794884611646		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 3.819794884611646 | validation: 4.853623174773741]
	TIME [epoch: 10.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822885426249158		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.822885426249158 | validation: 4.8379106022605916]
	TIME [epoch: 10.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8329541843967747		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.8329541843967747 | validation: 4.892258207459571]
	TIME [epoch: 10.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8520200101700746		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.8520200101700746 | validation: 4.885227069153808]
	TIME [epoch: 10.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8453368281343656		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.8453368281343656 | validation: 4.833478682692803]
	TIME [epoch: 10.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8287587882979013		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.8287587882979013 | validation: 4.868515068300225]
	TIME [epoch: 10.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8239920668634424		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 3.8239920668634424 | validation: 5.054476804624625]
	TIME [epoch: 10.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9145649620457403		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 3.9145649620457403 | validation: 4.84819078879223]
	TIME [epoch: 10.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.816582239516729		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.816582239516729 | validation: 4.905233305681321]
	TIME [epoch: 10.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8327794781952074		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.8327794781952074 | validation: 4.879250125863898]
	TIME [epoch: 10.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8472487507922444		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.8472487507922444 | validation: 4.851935580513991]
	TIME [epoch: 10.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8624127043502816		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.8624127043502816 | validation: 4.828955897169715]
	TIME [epoch: 10.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8273198031037636		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.8273198031037636 | validation: 4.828808303740924]
	TIME [epoch: 10.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.85042264093491		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.85042264093491 | validation: 5.0050681146951055]
	TIME [epoch: 10.3 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.80548278479952		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 3.80548278479952 | validation: 4.650765323950201]
	TIME [epoch: 10.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.79136111636666		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 3.79136111636666 | validation: 4.780079884023036]
	TIME [epoch: 10.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.825435174669542		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.825435174669542 | validation: 4.833969886357299]
	TIME [epoch: 10.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.82509336539713		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.82509336539713 | validation: 4.830812387968689]
	TIME [epoch: 10.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820533566246911		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.820533566246911 | validation: 4.8111271443298484]
	TIME [epoch: 10.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833043270277775		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.833043270277775 | validation: 4.848323880613633]
	TIME [epoch: 10.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820232207881749		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 3.820232207881749 | validation: 4.8469968794131]
	TIME [epoch: 10.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8629714792825154		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.8629714792825154 | validation: 4.8838454162268]
	TIME [epoch: 10.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.832423600305622		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.832423600305622 | validation: 4.830714382106895]
	TIME [epoch: 10.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.830328224257019		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.830328224257019 | validation: 4.8584880439069345]
	TIME [epoch: 10.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8288211432123695		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 3.8288211432123695 | validation: 4.905837393281775]
	TIME [epoch: 10.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.824258034610972		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 3.824258034610972 | validation: 4.881243373028191]
	TIME [epoch: 10.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829997066413619		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.829997066413619 | validation: 4.837401923534619]
	TIME [epoch: 10.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.857271781465088		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 3.857271781465088 | validation: 4.832943163495425]
	TIME [epoch: 10.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.854086167196151		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.854086167196151 | validation: 4.828051085820629]
	TIME [epoch: 10.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8384171357906007		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 3.8384171357906007 | validation: 4.894677635427245]
	TIME [epoch: 10.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8150496702899233		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 3.8150496702899233 | validation: 4.891294961442489]
	TIME [epoch: 10.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8440286651832727		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 3.8440286651832727 | validation: 4.832763102292754]
	TIME [epoch: 10.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823866030541134		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 3.823866030541134 | validation: 4.856227630706032]
	TIME [epoch: 10.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826477718144413		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 3.826477718144413 | validation: 4.823933406610274]
	TIME [epoch: 10.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8402955136824355		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 3.8402955136824355 | validation: 4.856409170450271]
	TIME [epoch: 10.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8333173421307434		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.8333173421307434 | validation: 4.83847510265433]
	TIME [epoch: 10.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81976453737731		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.81976453737731 | validation: 4.845426540236122]
	TIME [epoch: 10.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8410628576584527		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 3.8410628576584527 | validation: 4.850772240672136]
	TIME [epoch: 10.3 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8501954743223825		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 3.8501954743223825 | validation: 4.909569293531553]
	TIME [epoch: 10.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.852052271902019		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 3.852052271902019 | validation: 4.872418661487668]
	TIME [epoch: 10.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8208290393999667		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.8208290393999667 | validation: 4.854983178221657]
	TIME [epoch: 10.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.825880483968162		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.825880483968162 | validation: 4.870262853675801]
	TIME [epoch: 10.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8493242236393597		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 3.8493242236393597 | validation: 4.841461494080005]
	TIME [epoch: 10.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8281399181519604		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 3.8281399181519604 | validation: 4.863041828967254]
	TIME [epoch: 10.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8300558581234916		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 3.8300558581234916 | validation: 4.839814639372584]
	TIME [epoch: 10.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9070888437960294		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.9070888437960294 | validation: 4.913309509269664]
	TIME [epoch: 10.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.833568328529586		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.833568328529586 | validation: 4.906017027165261]
	TIME [epoch: 10.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8821232418036766		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.8821232418036766 | validation: 4.845964784090169]
	TIME [epoch: 10.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8627231703666554		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 3.8627231703666554 | validation: 4.883314763463864]
	TIME [epoch: 10.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.831073582874235		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 3.831073582874235 | validation: 4.8358104903629116]
	TIME [epoch: 10.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.810745536886725		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.810745536886725 | validation: 4.92763480562356]
	TIME [epoch: 10.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8321225244531645		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.8321225244531645 | validation: 4.852725410593186]
	TIME [epoch: 10.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8299566004734826		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 3.8299566004734826 | validation: 4.799980649326386]
	TIME [epoch: 10.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8438972902537216		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.8438972902537216 | validation: 4.853669486645484]
	TIME [epoch: 10.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8273010005098618		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 3.8273010005098618 | validation: 4.829203656346736]
	TIME [epoch: 10.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8139527691658124		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.8139527691658124 | validation: 4.845772696742119]
	TIME [epoch: 10.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.827842660366246		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 3.827842660366246 | validation: 4.860035364319179]
	TIME [epoch: 10.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823451369582594		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 3.823451369582594 | validation: 4.915133075820919]
	TIME [epoch: 10.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8374275082108027		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 3.8374275082108027 | validation: 4.840636938837161]
	TIME [epoch: 10.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8234302053307916		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 3.8234302053307916 | validation: 4.869813234743121]
	TIME [epoch: 10.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8273247671725783		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.8273247671725783 | validation: 4.876775121622118]
	TIME [epoch: 10.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822307431490838		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 3.822307431490838 | validation: 4.8394887142092715]
	TIME [epoch: 10.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8190141812757448		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.8190141812757448 | validation: 4.792260805657803]
	TIME [epoch: 10.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8226597666000437		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.8226597666000437 | validation: 4.838367961714308]
	TIME [epoch: 10.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8157642662922853		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.8157642662922853 | validation: 4.893951975836333]
	TIME [epoch: 10.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8364626948790628		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 3.8364626948790628 | validation: 4.85912222968836]
	TIME [epoch: 10.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8379200435624923		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 3.8379200435624923 | validation: 4.867189683971386]
	TIME [epoch: 10.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.835264025433047		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.835264025433047 | validation: 4.878420845041311]
	TIME [epoch: 10.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.850897085655576		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 3.850897085655576 | validation: 4.852195523922968]
	TIME [epoch: 10.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8299214029046604		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.8299214029046604 | validation: 4.951297963742063]
	TIME [epoch: 10.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8459355410789327		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 3.8459355410789327 | validation: 4.833723252574513]
	TIME [epoch: 10.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.829253332225752		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.829253332225752 | validation: 4.877338761356682]
	TIME [epoch: 10.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.888287767973787		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 3.888287767973787 | validation: 4.870736460990401]
	TIME [epoch: 10.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.824630252644687		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 3.824630252644687 | validation: 4.833173335616121]
	TIME [epoch: 10.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8185035003249697		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 3.8185035003249697 | validation: 4.846855561224289]
	TIME [epoch: 10.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.822427688303761		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 3.822427688303761 | validation: 4.835069101117719]
	TIME [epoch: 10.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8168006245105444		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 3.8168006245105444 | validation: 4.886860255682448]
	TIME [epoch: 10.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8438117299765096		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 3.8438117299765096 | validation: 4.828073289685311]
	TIME [epoch: 10.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8239793544000547		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 3.8239793544000547 | validation: 4.867160711762576]
	TIME [epoch: 10.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8359600218380754		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 3.8359600218380754 | validation: 4.84423580442901]
	TIME [epoch: 10.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.812188425113272		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 3.812188425113272 | validation: 4.8356151498730275]
	TIME [epoch: 10.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8147915437020785		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 3.8147915437020785 | validation: 4.826389497966204]
	TIME [epoch: 10.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8336416664525346		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 3.8336416664525346 | validation: 4.831341916506136]
	TIME [epoch: 10.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8306255305398027		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 3.8306255305398027 | validation: 4.85429889252454]
	TIME [epoch: 10.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8128192837310237		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 3.8128192837310237 | validation: 4.829529274001892]
	TIME [epoch: 10.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.806001180343847		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.806001180343847 | validation: 4.80365099207702]
	TIME [epoch: 10.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8220527770596604		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 3.8220527770596604 | validation: 4.817580757649853]
	TIME [epoch: 10.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8113247203727383		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 3.8113247203727383 | validation: 4.840828039979975]
	TIME [epoch: 10.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.823531028411079		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.823531028411079 | validation: 4.8565864565343535]
	TIME [epoch: 10.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.828659861665554		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 3.828659861665554 | validation: 4.855359283468123]
	TIME [epoch: 10.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8277100981181156		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 3.8277100981181156 | validation: 4.876031959330942]
	TIME [epoch: 10.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.848216066082169		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 3.848216066082169 | validation: 4.795223223237313]
	TIME [epoch: 10.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.826705599013374		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 3.826705599013374 | validation: 4.789163719892458]
	TIME [epoch: 10.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.813071828179029		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.813071828179029 | validation: 4.597104060572396]
	TIME [epoch: 10.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4402589426662886		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 3.4402589426662886 | validation: 4.077308674731257]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2495026225145587		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 3.2495026225145587 | validation: 4.0954234523459965]
	TIME [epoch: 10.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2442065170026924		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 3.2442065170026924 | validation: 4.077945540498578]
	TIME [epoch: 10.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.1875360312604455		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 3.1875360312604455 | validation: 4.049625469882153]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.23006826308571		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 3.23006826308571 | validation: 4.100570232580857]
	TIME [epoch: 10.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5515544892703255		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 3.5515544892703255 | validation: 4.562391254489322]
	TIME [epoch: 10.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.652338028292791		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.652338028292791 | validation: 4.2987804310884945]
	TIME [epoch: 10.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5290687096228814		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 3.5290687096228814 | validation: 4.004561368484286]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0525701527769606		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.0525701527769606 | validation: 3.887964945164731]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_682.pth
	Model improved!!!
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9004222675615843		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 2.9004222675615843 | validation: 3.9244553276433143]
	TIME [epoch: 10.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.873807935457943		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 2.873807935457943 | validation: 3.835857999358608]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8496843759686556		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 2.8496843759686556 | validation: 3.780721611725846]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.829683575295941		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 2.829683575295941 | validation: 3.754321245285154]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7943719070178727		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 2.7943719070178727 | validation: 3.55930060584287]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8331181371413274		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 2.8331181371413274 | validation: 3.4758871681580894]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.770444743576389		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 2.770444743576389 | validation: 3.3933789103227423]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.6892365513963825		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 2.6892365513963825 | validation: 3.323101598762234]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8295478741901414		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 2.8295478741901414 | validation: 3.3854984208819374]
	TIME [epoch: 10.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7586516231950027		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 2.7586516231950027 | validation: 3.5068523644906957]
	TIME [epoch: 10.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.59569473018554		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 2.59569473018554 | validation: 3.494886288381814]
	TIME [epoch: 10.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.577127497899067		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 2.577127497899067 | validation: 3.4872383865982455]
	TIME [epoch: 10.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.53747652460385		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 2.53747652460385 | validation: 3.3537804790625154]
	TIME [epoch: 10.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4765124363475666		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 2.4765124363475666 | validation: 3.243712649031928]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4487735189722875		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 2.4487735189722875 | validation: 3.1684926226754175]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.438709921934514		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 2.438709921934514 | validation: 3.2297959366971236]
	TIME [epoch: 10.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.413167088190261		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 2.413167088190261 | validation: 3.2132636719314323]
	TIME [epoch: 10.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3962693340910706		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 2.3962693340910706 | validation: 3.2346698132992002]
	TIME [epoch: 10.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.321887649009737		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 2.321887649009737 | validation: 3.167852157270534]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3359741062234947		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 2.3359741062234947 | validation: 3.0179261994051467]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3561887962589756		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 2.3561887962589756 | validation: 3.1371253790256466]
	TIME [epoch: 10.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2506361042796614		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 2.2506361042796614 | validation: 3.115084394540338]
	TIME [epoch: 10.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3427353326795197		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 2.3427353326795197 | validation: 3.10329016639064]
	TIME [epoch: 10.3 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.278610064036407		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 2.278610064036407 | validation: 3.0122857563331626]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2379107058977317		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 2.2379107058977317 | validation: 2.9822068192263154]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.210518164841433		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 2.210518164841433 | validation: 2.9894173760119505]
	TIME [epoch: 10.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2081241897596082		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 2.2081241897596082 | validation: 3.0204529692311715]
	TIME [epoch: 10.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.165133429425552		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 2.165133429425552 | validation: 2.9655226429943546]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.148037143131443		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 2.148037143131443 | validation: 2.905475275482294]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_711.pth
	Model improved!!!
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1270583704093546		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 2.1270583704093546 | validation: 3.04437717312691]
	TIME [epoch: 10.3 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.092311049927261		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 2.092311049927261 | validation: 2.9672355220827926]
	TIME [epoch: 10.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.078554650318046		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 2.078554650318046 | validation: 2.9700963655123576]
	TIME [epoch: 10.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0627071158636485		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 2.0627071158636485 | validation: 2.940426038479245]
	TIME [epoch: 10.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.093586179158316		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 2.093586179158316 | validation: 2.8650455331583964]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1091391599843012		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 2.1091391599843012 | validation: 3.0008400106149407]
	TIME [epoch: 10.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0668960727537815		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 2.0668960727537815 | validation: 2.935342556300379]
	TIME [epoch: 10.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9860282133312048		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 1.9860282133312048 | validation: 2.885231817976088]
	TIME [epoch: 10.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9929468127507295		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 1.9929468127507295 | validation: 2.817970640282159]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9890987624375316		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 1.9890987624375316 | validation: 2.813758420938504]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9413788091472166		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 1.9413788091472166 | validation: 2.789915791279174]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9537447662768213		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 1.9537447662768213 | validation: 2.733130438889922]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9433656386609848		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 1.9433656386609848 | validation: 2.7825716899605504]
	TIME [epoch: 10.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9027547495365305		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 1.9027547495365305 | validation: 2.8443820485265396]
	TIME [epoch: 10.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.907869834325015		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 1.907869834325015 | validation: 2.7426308643092487]
	TIME [epoch: 10.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9160541736226466		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 1.9160541736226466 | validation: 2.6950482370096074]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_727.pth
	Model improved!!!
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9121464817188958		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 1.9121464817188958 | validation: 2.720811606180839]
	TIME [epoch: 10.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8714786393473255		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 1.8714786393473255 | validation: 2.713588798535476]
	TIME [epoch: 10.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8529736935364334		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 1.8529736935364334 | validation: 2.7952178719882865]
	TIME [epoch: 10.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8555445308774154		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 1.8555445308774154 | validation: 2.770315856948779]
	TIME [epoch: 10.3 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8277763236671967		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 1.8277763236671967 | validation: 2.7524097333333537]
	TIME [epoch: 10.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8279461732677587		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 1.8279461732677587 | validation: 2.63250409549394]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.875891404640344		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 1.875891404640344 | validation: 2.717663232995529]
	TIME [epoch: 10.3 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8252039960114295		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 1.8252039960114295 | validation: 2.8061370230055047]
	TIME [epoch: 10.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8027143483039096		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 1.8027143483039096 | validation: 2.685711641368505]
	TIME [epoch: 10.3 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.776775764172477		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 1.776775764172477 | validation: 2.5800175668454814]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.792431905942016		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 1.792431905942016 | validation: 2.6570249890108766]
	TIME [epoch: 10.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8624854109059679		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 1.8624854109059679 | validation: 2.7071403727034107]
	TIME [epoch: 10.3 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8058213424334113		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 1.8058213424334113 | validation: 2.5798231947909653]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8195933780570641		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 1.8195933780570641 | validation: 2.67054269421428]
	TIME [epoch: 10.3 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.750548530815894		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 1.750548530815894 | validation: 2.6268529724256626]
	TIME [epoch: 10.3 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.75610794484804		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 1.75610794484804 | validation: 2.641052616899937]
	TIME [epoch: 10.3 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7598381160218222		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 1.7598381160218222 | validation: 2.6372029827858605]
	TIME [epoch: 10.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7852328343952792		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 1.7852328343952792 | validation: 2.6560457186474795]
	TIME [epoch: 10.3 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7714461109381883		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 1.7714461109381883 | validation: 2.6762141353250075]
	TIME [epoch: 10.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.756493692185731		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 1.756493692185731 | validation: 2.673897238388861]
	TIME [epoch: 10.3 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.76035530976032		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 1.76035530976032 | validation: 2.667338164657241]
	TIME [epoch: 10.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7628689202289027		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 1.7628689202289027 | validation: 2.6109489101888195]
	TIME [epoch: 10.3 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7420585478550286		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 1.7420585478550286 | validation: 2.6492214053761836]
	TIME [epoch: 10.3 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.77354602540168		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 1.77354602540168 | validation: 2.60356789400402]
	TIME [epoch: 10.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.740276935777515		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 1.740276935777515 | validation: 2.5062404160421443]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7447460472796785		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 1.7447460472796785 | validation: 2.6908390647425744]
	TIME [epoch: 10.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7421968997266322		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 1.7421968997266322 | validation: 2.584823946299209]
	TIME [epoch: 10.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7476040183429469		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 1.7476040183429469 | validation: 2.5805073730329315]
	TIME [epoch: 10.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7092361255813338		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 1.7092361255813338 | validation: 2.620290573214144]
	TIME [epoch: 10.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7531685024007082		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 1.7531685024007082 | validation: 2.5611211363670647]
	TIME [epoch: 10.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.743575384830739		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 1.743575384830739 | validation: 2.6000507975289304]
	TIME [epoch: 10.3 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.731032965711305		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 1.731032965711305 | validation: 2.62539030192796]
	TIME [epoch: 10.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7227062695670579		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 1.7227062695670579 | validation: 2.4831805666913]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7059142364243833		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 1.7059142364243833 | validation: 2.4511678083996378]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.712945491007987		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 1.712945491007987 | validation: 2.447986825887634]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_762.pth
	Model improved!!!
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7239067666155485		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 1.7239067666155485 | validation: 2.4449126039792675]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_763.pth
	Model improved!!!
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.699223568889833		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 1.699223568889833 | validation: 2.4764076719693935]
	TIME [epoch: 10.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6938221647106793		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 1.6938221647106793 | validation: 2.525637986321276]
	TIME [epoch: 10.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.675600289521873		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 1.675600289521873 | validation: 2.453393167302235]
	TIME [epoch: 10.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6993401331655889		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 1.6993401331655889 | validation: 2.4605153661237043]
	TIME [epoch: 10.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7113186428266638		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 1.7113186428266638 | validation: 2.5099861829367947]
	TIME [epoch: 10.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6960434650905505		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 1.6960434650905505 | validation: 2.485524925618566]
	TIME [epoch: 10.3 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6440822678258247		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 1.6440822678258247 | validation: 2.4477601042874295]
	TIME [epoch: 10.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.657171599504348		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 1.657171599504348 | validation: 2.424329946404964]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.661164922181234		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 1.661164922181234 | validation: 2.41098612550913]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6544555152882978		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 1.6544555152882978 | validation: 2.4895334284504314]
	TIME [epoch: 10.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.668181706942979		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 1.668181706942979 | validation: 2.484481686807263]
	TIME [epoch: 10.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6515253709257212		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 1.6515253709257212 | validation: 2.417361827666275]
	TIME [epoch: 10.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6681428703813357		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 1.6681428703813357 | validation: 2.4930986216612583]
	TIME [epoch: 10.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.677441653794506		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 1.677441653794506 | validation: 2.4238369708661356]
	TIME [epoch: 10.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6770264995714554		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 1.6770264995714554 | validation: 2.463684968592737]
	TIME [epoch: 10.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.647906243160375		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 1.647906243160375 | validation: 2.472635097082116]
	TIME [epoch: 10.3 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6333582473678063		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 1.6333582473678063 | validation: 2.3995547400714416]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6381192773029027		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 1.6381192773029027 | validation: 2.39170988493053]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_781.pth
	Model improved!!!
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6233845442959816		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 1.6233845442959816 | validation: 2.392901415268023]
	TIME [epoch: 10.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6331595891789519		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 1.6331595891789519 | validation: 2.375783348440254]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6358282493919911		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 1.6358282493919911 | validation: 2.4208118103431064]
	TIME [epoch: 10.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6008512033401263		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 1.6008512033401263 | validation: 2.4409861284310326]
	TIME [epoch: 10.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6156250233467868		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 1.6156250233467868 | validation: 2.4137898318542264]
	TIME [epoch: 10.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6258437973506195		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 1.6258437973506195 | validation: 2.395267801501301]
	TIME [epoch: 10.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6076180510573976		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 1.6076180510573976 | validation: 2.3711715714467987]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6257691658173106		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 1.6257691658173106 | validation: 2.488115408035627]
	TIME [epoch: 10.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6340945930087643		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 1.6340945930087643 | validation: 2.4039563610724297]
	TIME [epoch: 10.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6067030116048897		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 1.6067030116048897 | validation: 2.3664853278932703]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6140153530602803		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 1.6140153530602803 | validation: 2.490738341069694]
	TIME [epoch: 10.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6048078329444604		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 1.6048078329444604 | validation: 2.387648116323946]
	TIME [epoch: 10.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6048921055698808		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 1.6048921055698808 | validation: 2.385693813160856]
	TIME [epoch: 10.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5562171077377833		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 1.5562171077377833 | validation: 2.424805945305453]
	TIME [epoch: 10.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5905758155354186		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 1.5905758155354186 | validation: 2.3846786534531224]
	TIME [epoch: 10.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.59633137897492		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 1.59633137897492 | validation: 2.4526115585860815]
	TIME [epoch: 10.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5900417331805716		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 1.5900417331805716 | validation: 2.3163006813053313]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_798.pth
	Model improved!!!
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5706507123509934		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 1.5706507123509934 | validation: 2.4036321230411057]
	TIME [epoch: 10.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5691868484101377		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 1.5691868484101377 | validation: 2.39026558938468]
	TIME [epoch: 10.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5808280150260061		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 1.5808280150260061 | validation: 2.3380910849403156]
	TIME [epoch: 10.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.639225462398254		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 1.639225462398254 | validation: 2.326194934859902]
	TIME [epoch: 10.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5659087389457769		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 1.5659087389457769 | validation: 2.40047716672267]
	TIME [epoch: 10.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5660147797945865		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 1.5660147797945865 | validation: 2.323027483537067]
	TIME [epoch: 10.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.565101039978134		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 1.565101039978134 | validation: 2.248320185653621]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5520234716344636		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 1.5520234716344636 | validation: 2.337741150448542]
	TIME [epoch: 10.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.567527998526169		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 1.567527998526169 | validation: 2.307828776238929]
	TIME [epoch: 10.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.527725007657113		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 1.527725007657113 | validation: 2.3350900700305823]
	TIME [epoch: 10.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5376966822375		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 1.5376966822375 | validation: 2.3632403762585255]
	TIME [epoch: 10.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5848726495605097		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 1.5848726495605097 | validation: 2.400195225316871]
	TIME [epoch: 10.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.553678386319603		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 1.553678386319603 | validation: 2.335169322238861]
	TIME [epoch: 10.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5251484676786817		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 1.5251484676786817 | validation: 2.325988077606582]
	TIME [epoch: 10.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5442193944344784		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 1.5442193944344784 | validation: 2.2756546828994555]
	TIME [epoch: 10.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.530221765187329		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 1.530221765187329 | validation: 2.3210659121677804]
	TIME [epoch: 10.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.530517192751167		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 1.530517192751167 | validation: 2.3846568358072875]
	TIME [epoch: 10.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5360486336229293		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 1.5360486336229293 | validation: 2.3315610989120428]
	TIME [epoch: 10.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5326814126271127		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 1.5326814126271127 | validation: 2.4196550841776743]
	TIME [epoch: 10.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.509291951417761		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 1.509291951417761 | validation: 2.342016442565664]
	TIME [epoch: 10.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5062288596263134		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 1.5062288596263134 | validation: 2.3339634670676213]
	TIME [epoch: 10.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4904424207641926		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 1.4904424207641926 | validation: 2.2831261764472925]
	TIME [epoch: 10.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5153263665527286		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 1.5153263665527286 | validation: 2.3687382140035056]
	TIME [epoch: 10.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5140430743812192		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 1.5140430743812192 | validation: 2.388950458895911]
	TIME [epoch: 10.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5230800114094598		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 1.5230800114094598 | validation: 2.305448450172496]
	TIME [epoch: 10.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4910459006663057		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 1.4910459006663057 | validation: 2.3804274584995704]
	TIME [epoch: 10.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.518743237053356		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 1.518743237053356 | validation: 2.378113225536761]
	TIME [epoch: 10.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5069080921922133		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 1.5069080921922133 | validation: 2.309477338100225]
	TIME [epoch: 10.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.520242188626273		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 1.520242188626273 | validation: 2.4308798304322274]
	TIME [epoch: 10.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4959702448932837		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 1.4959702448932837 | validation: 2.311009820054253]
	TIME [epoch: 10.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.504197248759351		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 1.504197248759351 | validation: 2.3204668914763618]
	TIME [epoch: 10.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4740393568057144		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 1.4740393568057144 | validation: 2.3265053536687996]
	TIME [epoch: 10.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4672398738317498		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 1.4672398738317498 | validation: 2.301498423147577]
	TIME [epoch: 10.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4839946556062127		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 1.4839946556062127 | validation: 2.2903048080630097]
	TIME [epoch: 10.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.491969233933566		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 1.491969233933566 | validation: 2.190102309212241]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_833.pth
	Model improved!!!
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4518069759774517		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 1.4518069759774517 | validation: 2.2713960838145595]
	TIME [epoch: 10.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4653881376648636		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 1.4653881376648636 | validation: 2.288884469698297]
	TIME [epoch: 10.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4491610266883939		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 1.4491610266883939 | validation: 2.281416971850632]
	TIME [epoch: 10.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4414300976976182		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 1.4414300976976182 | validation: 2.1998959771436293]
	TIME [epoch: 10.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4211472445871807		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 1.4211472445871807 | validation: 2.227502922709486]
	TIME [epoch: 10.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.441118525186484		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 1.441118525186484 | validation: 2.357176984637821]
	TIME [epoch: 10.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5244880575294242		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 1.5244880575294242 | validation: 2.282762360927525]
	TIME [epoch: 10.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.411964916059271		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 1.411964916059271 | validation: 2.1592900947904297]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4461516727358403		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 1.4461516727358403 | validation: 2.2286735891751204]
	TIME [epoch: 10.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4137789316318594		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 1.4137789316318594 | validation: 2.2197512421970362]
	TIME [epoch: 10.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4282424276191816		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 1.4282424276191816 | validation: 2.2008046874368943]
	TIME [epoch: 10.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4417731065799462		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 1.4417731065799462 | validation: 2.2508365622327893]
	TIME [epoch: 10.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.406906264162302		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 1.406906264162302 | validation: 2.29274872112847]
	TIME [epoch: 10.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4027609683190954		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 1.4027609683190954 | validation: 2.2209916374277516]
	TIME [epoch: 10.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4249408246900697		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 1.4249408246900697 | validation: 2.2800652927479197]
	TIME [epoch: 10.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4172470326234161		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 1.4172470326234161 | validation: 2.2158641557598795]
	TIME [epoch: 10.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3893098545825695		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 1.3893098545825695 | validation: 2.18880232540998]
	TIME [epoch: 10.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3751152596336813		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 1.3751152596336813 | validation: 2.224793576278212]
	TIME [epoch: 10.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4053570529595456		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 1.4053570529595456 | validation: 2.3109845743558384]
	TIME [epoch: 10.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3986358635230012		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 1.3986358635230012 | validation: 2.127438270574962]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_853.pth
	Model improved!!!
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3779612563185342		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 1.3779612563185342 | validation: 2.1197159694679617]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_854.pth
	Model improved!!!
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.381321523576678		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 1.381321523576678 | validation: 2.1975833120880317]
	TIME [epoch: 10.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3539302421211936		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 1.3539302421211936 | validation: 2.143128109508594]
	TIME [epoch: 10.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.395605295547212		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 1.395605295547212 | validation: 2.143315249922121]
	TIME [epoch: 10.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3547576327774462		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 1.3547576327774462 | validation: 2.204128211539037]
	TIME [epoch: 10.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.369549275293026		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 1.369549275293026 | validation: 2.211534833143133]
	TIME [epoch: 10.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3593411821792862		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 1.3593411821792862 | validation: 2.1927304795249176]
	TIME [epoch: 10.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3710447708718418		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 1.3710447708718418 | validation: 2.2257239337016266]
	TIME [epoch: 10.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3645031124525797		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 1.3645031124525797 | validation: 2.11099632291423]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3469302863360413		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 1.3469302863360413 | validation: 2.15301185799898]
	TIME [epoch: 10.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3643648149948355		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 1.3643648149948355 | validation: 2.197572327458949]
	TIME [epoch: 10.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3673208638090002		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 1.3673208638090002 | validation: 2.1879610277810038]
	TIME [epoch: 10.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3534947786113292		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 1.3534947786113292 | validation: 2.162606145286896]
	TIME [epoch: 10.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3449516846000025		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 1.3449516846000025 | validation: 2.2008727648614914]
	TIME [epoch: 10.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3448538314381304		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 1.3448538314381304 | validation: 2.234119246132031]
	TIME [epoch: 10.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3317307402473133		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 1.3317307402473133 | validation: 2.1441700977888862]
	TIME [epoch: 10.3 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3095500119581391		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 1.3095500119581391 | validation: 2.172039269933352]
	TIME [epoch: 10.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3098862450590771		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 1.3098862450590771 | validation: 2.1593513136748363]
	TIME [epoch: 10.3 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988654741332404		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 1.2988654741332404 | validation: 2.14696767777173]
	TIME [epoch: 10.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3158653311068416		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 1.3158653311068416 | validation: 2.089048543935427]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2963034153976598		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 1.2963034153976598 | validation: 2.1294962679524145]
	TIME [epoch: 10.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2813608512755383		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 1.2813608512755383 | validation: 2.15581926079853]
	TIME [epoch: 10.3 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3351484876013622		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 1.3351484876013622 | validation: 2.199038909433539]
	TIME [epoch: 10.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296286798423274		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 1.296286798423274 | validation: 2.1097954934541825]
	TIME [epoch: 10.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2786511644244987		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 1.2786511644244987 | validation: 2.1433574910914515]
	TIME [epoch: 10.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973802028924293		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 1.2973802028924293 | validation: 2.130053017779833]
	TIME [epoch: 10.3 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977221197409146		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 1.2977221197409146 | validation: 2.054082360531844]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.266026373137545		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 1.266026373137545 | validation: 2.0679847703687297]
	TIME [epoch: 10.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2751394611590068		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 1.2751394611590068 | validation: 2.063811487232234]
	TIME [epoch: 10.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3008698950636912		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 1.3008698950636912 | validation: 2.0520802623423675]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_883.pth
	Model improved!!!
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2605319290720836		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 1.2605319290720836 | validation: 2.127624686666734]
	TIME [epoch: 10.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2758296658729489		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 1.2758296658729489 | validation: 2.1001987476491575]
	TIME [epoch: 10.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.287016752647161		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 1.287016752647161 | validation: 2.0995833884755406]
	TIME [epoch: 10.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2509042593885806		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 1.2509042593885806 | validation: 2.0273539631263446]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2608862906404095		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 1.2608862906404095 | validation: 2.110008019363519]
	TIME [epoch: 10.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2486760224564848		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 1.2486760224564848 | validation: 2.0481223256264953]
	TIME [epoch: 10.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2132805947304892		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 1.2132805947304892 | validation: 2.1757057157715463]
	TIME [epoch: 10.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2415118828502274		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 1.2415118828502274 | validation: 2.012300459210789]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_891.pth
	Model improved!!!
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2405955966737428		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 1.2405955966737428 | validation: 2.081140682459706]
	TIME [epoch: 10.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2351846598324139		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 1.2351846598324139 | validation: 2.046128157065995]
	TIME [epoch: 10.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.228430297398845		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 1.228430297398845 | validation: 2.1623471172544133]
	TIME [epoch: 10.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2547491769289343		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 1.2547491769289343 | validation: 2.052124367782474]
	TIME [epoch: 10.3 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2101632032712		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 1.2101632032712 | validation: 1.9876603612266481]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.240667359314838		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 1.240667359314838 | validation: 2.026343370321432]
	TIME [epoch: 10.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2161896664792724		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 1.2161896664792724 | validation: 2.1100342761085993]
	TIME [epoch: 10.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2370838044126597		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 1.2370838044126597 | validation: 2.0298070945915567]
	TIME [epoch: 10.3 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.225833844738659		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 1.225833844738659 | validation: 2.1264836661936015]
	TIME [epoch: 10.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2264915037160313		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 1.2264915037160313 | validation: 2.0703828736079557]
	TIME [epoch: 10.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1871084004014116		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 1.1871084004014116 | validation: 1.959744011317599]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1826498656452658		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 1.1826498656452658 | validation: 2.145009174993814]
	TIME [epoch: 10.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1828128904411226		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 1.1828128904411226 | validation: 2.0477308622813046]
	TIME [epoch: 10.3 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1874476086375323		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 1.1874476086375323 | validation: 2.0276857753743047]
	TIME [epoch: 10.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1743478832552485		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 1.1743478832552485 | validation: 2.0327769100845825]
	TIME [epoch: 10.3 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1873616285434607		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 1.1873616285434607 | validation: 2.0441826205911147]
	TIME [epoch: 10.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1832123024838006		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 1.1832123024838006 | validation: 2.1859629664911178]
	TIME [epoch: 10.3 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2484352073710907		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 1.2484352073710907 | validation: 2.033863382463535]
	TIME [epoch: 10.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.151634486208158		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 1.151634486208158 | validation: 2.072995738511933]
	TIME [epoch: 10.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.203955406842789		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 1.203955406842789 | validation: 2.046614344747707]
	TIME [epoch: 10.3 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1866134679762488		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 1.1866134679762488 | validation: 2.038144277442799]
	TIME [epoch: 10.3 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1777588888689223		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 1.1777588888689223 | validation: 1.9567346597526438]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_913.pth
	Model improved!!!
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1626108003641105		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 1.1626108003641105 | validation: 2.0971795496120174]
	TIME [epoch: 10.3 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1508140718353252		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 1.1508140718353252 | validation: 1.9910084801112038]
	TIME [epoch: 10.3 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1582268033786776		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 1.1582268033786776 | validation: 2.105434804370546]
	TIME [epoch: 10.3 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1729937210453985		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 1.1729937210453985 | validation: 2.0870536386039804]
	TIME [epoch: 10.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1766204765271446		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 1.1766204765271446 | validation: 2.0330843755686328]
	TIME [epoch: 10.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1388084073977935		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 1.1388084073977935 | validation: 1.9795126939698957]
	TIME [epoch: 10.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1577514689490962		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 1.1577514689490962 | validation: 2.007987524043344]
	TIME [epoch: 10.3 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1428807949392854		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 1.1428807949392854 | validation: 1.9280035663530901]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_921.pth
	Model improved!!!
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1633930284431058		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 1.1633930284431058 | validation: 2.0067868842668384]
	TIME [epoch: 10.3 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.12776831573579		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 1.12776831573579 | validation: 1.9666982181245964]
	TIME [epoch: 10.3 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1726421166463254		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 1.1726421166463254 | validation: 1.9070755472378311]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1549459197441951		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 1.1549459197441951 | validation: 1.9321601543465283]
	TIME [epoch: 10.3 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1284949990171438		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 1.1284949990171438 | validation: 1.8873978686692419]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.117290931230324		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 1.117290931230324 | validation: 1.913254859732853]
	TIME [epoch: 10.3 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.120842216967218		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 1.120842216967218 | validation: 1.9398722419889913]
	TIME [epoch: 10.3 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.143098649298356		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 1.143098649298356 | validation: 1.9850709834814135]
	TIME [epoch: 10.3 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.124417171905582		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 1.124417171905582 | validation: 2.0384348282644735]
	TIME [epoch: 10.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1112574003079518		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 1.1112574003079518 | validation: 1.9044894556861869]
	TIME [epoch: 10.3 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1164111791749558		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 1.1164111791749558 | validation: 1.842330348467661]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_932.pth
	Model improved!!!
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0849824220367894		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 1.0849824220367894 | validation: 1.9229510225341058]
	TIME [epoch: 10.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1159095035534619		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 1.1159095035534619 | validation: 1.9156058981904542]
	TIME [epoch: 10.3 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.094839021729608		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 1.094839021729608 | validation: 1.8224579984305516]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_935.pth
	Model improved!!!
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.089066114787217		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 1.089066114787217 | validation: 1.803351292061289]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_936.pth
	Model improved!!!
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1269323278913108		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 1.1269323278913108 | validation: 1.841008380188256]
	TIME [epoch: 10.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.093519154472907		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 1.093519154472907 | validation: 1.7792915194024423]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_938.pth
	Model improved!!!
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0834478006912402		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 1.0834478006912402 | validation: 1.7568092261682051]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.088472051902869		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 1.088472051902869 | validation: 1.8053044014038446]
	TIME [epoch: 10.3 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0986639125130524		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 1.0986639125130524 | validation: 1.7167155135090337]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_941.pth
	Model improved!!!
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0783213779502296		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 1.0783213779502296 | validation: 1.8255316946541924]
	TIME [epoch: 10.3 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.1369658245801484		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 1.1369658245801484 | validation: 1.7407660016422648]
	TIME [epoch: 10.3 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0785870494159142		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 1.0785870494159142 | validation: 1.6998064713014998]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_944.pth
	Model improved!!!
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0783420925653522		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 1.0783420925653522 | validation: 1.6908264187663977]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_945.pth
	Model improved!!!
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0506035630421358		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 1.0506035630421358 | validation: 1.666606593982941]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0507785239780203		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 1.0507785239780203 | validation: 1.6799708264519382]
	TIME [epoch: 10.3 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0316006245791605		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 1.0316006245791605 | validation: 1.70803416477565]
	TIME [epoch: 10.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0424636058448875		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 1.0424636058448875 | validation: 1.6134517754189066]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0358297709878552		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 1.0358297709878552 | validation: 1.6636129204941024]
	TIME [epoch: 10.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0164001319904297		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 1.0164001319904297 | validation: 1.743514483309795]
	TIME [epoch: 10.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0611538135941563		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 1.0611538135941563 | validation: 1.636051505589956]
	TIME [epoch: 10.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0151381725709596		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 1.0151381725709596 | validation: 1.6515714064337312]
	TIME [epoch: 10.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0166337004785593		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 1.0166337004785593 | validation: 1.631458349716824]
	TIME [epoch: 10.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0098172153202		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 1.0098172153202 | validation: 1.638851249657871]
	TIME [epoch: 10.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0277281281435957		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 1.0277281281435957 | validation: 1.7073526504857546]
	TIME [epoch: 10.3 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0240090667260786		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 1.0240090667260786 | validation: 1.6606511775270127]
	TIME [epoch: 10.3 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0272798264195309		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 1.0272798264195309 | validation: 1.6245275918383921]
	TIME [epoch: 10.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.975123656774261		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 0.975123656774261 | validation: 1.6122496054072457]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_959.pth
	Model improved!!!
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0071034755521882		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 1.0071034755521882 | validation: 1.7049340399280808]
	TIME [epoch: 10.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0085006695195005		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 1.0085006695195005 | validation: 1.6108062303575208]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_961.pth
	Model improved!!!
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.005848983936588		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 1.005848983936588 | validation: 1.5347149609634567]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_962.pth
	Model improved!!!
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.026282464366483		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 1.026282464366483 | validation: 1.6656334116575686]
	TIME [epoch: 10.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.004507597337911		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 1.004507597337911 | validation: 1.57757140146376]
	TIME [epoch: 10.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0164494613355177		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 1.0164494613355177 | validation: 1.5758357745913125]
	TIME [epoch: 10.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9921897540985034		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 0.9921897540985034 | validation: 1.5063867589148987]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_966.pth
	Model improved!!!
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9450527918539884		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 0.9450527918539884 | validation: 1.5267842639251845]
	TIME [epoch: 10.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.0108399956483536		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 1.0108399956483536 | validation: 1.535236133346736]
	TIME [epoch: 10.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9723420220669953		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 0.9723420220669953 | validation: 1.6439464189877842]
	TIME [epoch: 10.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9873251561104327		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 0.9873251561104327 | validation: 1.503035957295013]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_970.pth
	Model improved!!!
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9966328409916091		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 0.9966328409916091 | validation: 1.549909742270159]
	TIME [epoch: 10.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9738321474691609		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 0.9738321474691609 | validation: 1.5499553787259799]
	TIME [epoch: 10.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.966297954437475		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 0.966297954437475 | validation: 1.578580184926754]
	TIME [epoch: 10.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9551144535879722		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 0.9551144535879722 | validation: 1.512498666438887]
	TIME [epoch: 10.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9356285170057401		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 0.9356285170057401 | validation: 1.5951960580468467]
	TIME [epoch: 10.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9542728125086294		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 0.9542728125086294 | validation: 1.550681551828952]
	TIME [epoch: 10.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9459053945003977		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 0.9459053945003977 | validation: 1.5233261954647277]
	TIME [epoch: 10.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.960803654236767		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 0.960803654236767 | validation: 1.6062507053348691]
	TIME [epoch: 10.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9573304922415599		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 0.9573304922415599 | validation: 1.5388660771331686]
	TIME [epoch: 10.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.939342638849762		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 0.939342638849762 | validation: 1.5565546451446488]
	TIME [epoch: 10.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9942534230031521		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 0.9942534230031521 | validation: 1.4674380731276855]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_981.pth
	Model improved!!!
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9616999090542043		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 0.9616999090542043 | validation: 1.5587213230039652]
	TIME [epoch: 10.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9567678679161504		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 0.9567678679161504 | validation: 1.522308917823328]
	TIME [epoch: 10.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.929362061409624		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 0.929362061409624 | validation: 1.516390694697168]
	TIME [epoch: 10.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9285791759397363		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 0.9285791759397363 | validation: 1.4672285262012679]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9523018671221297		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 0.9523018671221297 | validation: 1.475061236488754]
	TIME [epoch: 10.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9563100054916699		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 0.9563100054916699 | validation: 1.5650525710926209]
	TIME [epoch: 10.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9430143049334576		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 0.9430143049334576 | validation: 1.4649127572959812]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_988.pth
	Model improved!!!
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9361999873913722		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 0.9361999873913722 | validation: 1.5115575631787657]
	TIME [epoch: 10.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9266921462412036		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 0.9266921462412036 | validation: 1.461218431513722]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9208528360527877		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 0.9208528360527877 | validation: 1.503667926909533]
	TIME [epoch: 10.3 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9303520888401124		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 0.9303520888401124 | validation: 1.5048493160092877]
	TIME [epoch: 10.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9352642161751892		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 0.9352642161751892 | validation: 1.459202355813963]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_993.pth
	Model improved!!!
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9240350642722289		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 0.9240350642722289 | validation: 1.4589540501851181]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9353505894216104		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 0.9353505894216104 | validation: 1.5145955037116607]
	TIME [epoch: 10.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9036591061955852		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 0.9036591061955852 | validation: 1.4522281656753553]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_996.pth
	Model improved!!!
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9176011088077903		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 0.9176011088077903 | validation: 1.4341772648288367]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_997.pth
	Model improved!!!
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8967110922395187		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 0.8967110922395187 | validation: 1.4510689722221335]
	TIME [epoch: 10.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9038220778533447		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 0.9038220778533447 | validation: 1.4834980966978162]
	TIME [epoch: 10.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9005827877550706		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 0.9005827877550706 | validation: 1.4735525294540868]
	TIME [epoch: 10.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9210175118360056		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 0.9210175118360056 | validation: 1.4402792371561035]
	TIME [epoch: 10.3 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8928532454264992		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 0.8928532454264992 | validation: 1.4503957341861327]
	TIME [epoch: 10.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9192263200451256		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 0.9192263200451256 | validation: 1.4978475842612358]
	TIME [epoch: 10.3 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9112544905458704		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 0.9112544905458704 | validation: 1.537984320226072]
	TIME [epoch: 10.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8864764707749636		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 0.8864764707749636 | validation: 1.5179508613417034]
	TIME [epoch: 10.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9008570493149437		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 0.9008570493149437 | validation: 1.474541752987351]
	TIME [epoch: 10.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.875504085833609		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 0.875504085833609 | validation: 1.4586569004529013]
	TIME [epoch: 10.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8854493958733031		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 0.8854493958733031 | validation: 1.3821589873964644]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.9134287962735987		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 0.9134287962735987 | validation: 1.505297092820128]
	TIME [epoch: 10.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8803097545933689		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 0.8803097545933689 | validation: 1.4058049067832354]
	TIME [epoch: 10.3 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8884444924776064		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 0.8884444924776064 | validation: 1.5080963980873925]
	TIME [epoch: 10.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8808672406067585		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 0.8808672406067585 | validation: 1.4400699175474097]
	TIME [epoch: 10.3 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.880103263158667		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 0.880103263158667 | validation: 1.4166480299759063]
	TIME [epoch: 10.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8732628379781746		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 0.8732628379781746 | validation: 1.4903243408555171]
	TIME [epoch: 10.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8660521746644969		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 0.8660521746644969 | validation: 1.44231502597928]
	TIME [epoch: 10.3 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8626914998594131		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 0.8626914998594131 | validation: 1.4150124847561205]
	TIME [epoch: 10.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8783464259868369		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 0.8783464259868369 | validation: 1.5147451619162806]
	TIME [epoch: 10.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8579434275627265		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 0.8579434275627265 | validation: 1.5356248097866598]
	TIME [epoch: 10.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8723172013468771		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 0.8723172013468771 | validation: 1.4892602307637395]
	TIME [epoch: 10.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8453715044928583		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 0.8453715044928583 | validation: 1.4350902738271645]
	TIME [epoch: 10.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8727647678983768		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 0.8727647678983768 | validation: 1.4468583174856509]
	TIME [epoch: 10.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8678117150621327		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 0.8678117150621327 | validation: 1.3777142470102697]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.859544588086178		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 0.859544588086178 | validation: 1.441531870447615]
	TIME [epoch: 10.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8489111731134932		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 0.8489111731134932 | validation: 1.3869817180467856]
	TIME [epoch: 10.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8618256069127108		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 0.8618256069127108 | validation: 1.4152434073806461]
	TIME [epoch: 10.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8602512017365991		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 0.8602512017365991 | validation: 1.4340754146816488]
	TIME [epoch: 10.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8380984139257418		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 0.8380984139257418 | validation: 1.4460529479662663]
	TIME [epoch: 10.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8486283045430536		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 0.8486283045430536 | validation: 1.4045346822263696]
	TIME [epoch: 10.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8700213671627133		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 0.8700213671627133 | validation: 1.4776869925618434]
	TIME [epoch: 10.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8577201254885093		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 0.8577201254885093 | validation: 1.4624140730666801]
	TIME [epoch: 10.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8581790634904698		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 0.8581790634904698 | validation: 1.386993197989164]
	TIME [epoch: 10.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8625354426222265		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 0.8625354426222265 | validation: 1.4746087437055513]
	TIME [epoch: 10.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8443983621894751		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 0.8443983621894751 | validation: 1.422633208215741]
	TIME [epoch: 10.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8548717959308318		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 0.8548717959308318 | validation: 1.4147181527003825]
	TIME [epoch: 10.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8513650140420989		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 0.8513650140420989 | validation: 1.4213381353658252]
	TIME [epoch: 10.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8618497178869333		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 0.8618497178869333 | validation: 1.366132463946091]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1036.pth
	Model improved!!!
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8505191605227811		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 0.8505191605227811 | validation: 1.3113373454168085]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1037.pth
	Model improved!!!
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8304397772328109		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 0.8304397772328109 | validation: 1.3650267980880206]
	TIME [epoch: 10.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8297927801223602		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 0.8297927801223602 | validation: 1.3045836916758373]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1039.pth
	Model improved!!!
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8363263717126603		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 0.8363263717126603 | validation: 1.264622594067475]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1040.pth
	Model improved!!!
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8521906396324571		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 0.8521906396324571 | validation: 1.2959036717185952]
	TIME [epoch: 10.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8319334968568477		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 0.8319334968568477 | validation: 1.3040473331104965]
	TIME [epoch: 10.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8562464589019477		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 0.8562464589019477 | validation: 1.2695161623342859]
	TIME [epoch: 10.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8473665799296477		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 0.8473665799296477 | validation: 1.2910693145552796]
	TIME [epoch: 10.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8348485220285216		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 0.8348485220285216 | validation: 1.2995412818196497]
	TIME [epoch: 10.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8434710067279045		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 0.8434710067279045 | validation: 1.3399565716202286]
	TIME [epoch: 10.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8235565911473639		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 0.8235565911473639 | validation: 1.3422939615348957]
	TIME [epoch: 10.3 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8145109373116466		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 0.8145109373116466 | validation: 1.3711017389530802]
	TIME [epoch: 10.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8357471208494595		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 0.8357471208494595 | validation: 1.3210101140977673]
	TIME [epoch: 10.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8296671621180636		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 0.8296671621180636 | validation: 1.3177298020763448]
	TIME [epoch: 10.3 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8030531019806115		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 0.8030531019806115 | validation: 1.3718173373444829]
	TIME [epoch: 10.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8188612061576006		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 0.8188612061576006 | validation: 1.3489334527308205]
	TIME [epoch: 10.3 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.807357186254843		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 0.807357186254843 | validation: 1.4149930063663414]
	TIME [epoch: 10.3 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8226174808780107		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 0.8226174808780107 | validation: 1.3234534705051153]
	TIME [epoch: 10.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8356248184232491		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 0.8356248184232491 | validation: 1.3738997930420005]
	TIME [epoch: 10.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.808924462623889		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 0.808924462623889 | validation: 1.4228368431462313]
	TIME [epoch: 10.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8414729194901407		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 0.8414729194901407 | validation: 1.4404575244660018]
	TIME [epoch: 10.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8235102010875321		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 0.8235102010875321 | validation: 1.3673681617546332]
	TIME [epoch: 10.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8037109411554825		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 0.8037109411554825 | validation: 1.3704436795707386]
	TIME [epoch: 10.3 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8267760588403267		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 0.8267760588403267 | validation: 1.337106844386709]
	TIME [epoch: 10.3 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7960702064922626		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 0.7960702064922626 | validation: 1.417772712071125]
	TIME [epoch: 10.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8130316827146492		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 0.8130316827146492 | validation: 1.3467369275138463]
	TIME [epoch: 10.3 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7996446108190781		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 0.7996446108190781 | validation: 1.340864739666985]
	TIME [epoch: 10.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8137164199628104		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 0.8137164199628104 | validation: 1.351652105156563]
	TIME [epoch: 10.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8093864651677771		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 0.8093864651677771 | validation: 1.3614115808723568]
	TIME [epoch: 10.3 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8219000602564354		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 0.8219000602564354 | validation: 1.3931253431473054]
	TIME [epoch: 10.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8135821270160161		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 0.8135821270160161 | validation: 1.3820323801939634]
	TIME [epoch: 10.3 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.806606992194775		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 0.806606992194775 | validation: 1.3558133531759904]
	TIME [epoch: 10.3 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8041761729128029		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 0.8041761729128029 | validation: 1.3794440195647564]
	TIME [epoch: 10.3 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8102742071889419		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 0.8102742071889419 | validation: 1.3164544696223923]
	TIME [epoch: 10.3 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.787484376688099		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 0.787484376688099 | validation: 1.3308110348617663]
	TIME [epoch: 10.3 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8191223635445797		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 0.8191223635445797 | validation: 1.2964222561931005]
	TIME [epoch: 10.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7908601901149148		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 0.7908601901149148 | validation: 1.3122962944565952]
	TIME [epoch: 10.3 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8041719348875684		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 0.8041719348875684 | validation: 1.3817172986242494]
	TIME [epoch: 10.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8098404087604683		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 0.8098404087604683 | validation: 1.3586506631304718]
	TIME [epoch: 10.3 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8008529318633114		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 0.8008529318633114 | validation: 1.349049464978786]
	TIME [epoch: 10.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7980644129754261		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 0.7980644129754261 | validation: 1.2356523631134226]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1077.pth
	Model improved!!!
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7980916825445581		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 0.7980916825445581 | validation: 1.2561755170704072]
	TIME [epoch: 10.3 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7914767064184889		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 0.7914767064184889 | validation: 1.4019612637770678]
	TIME [epoch: 10.3 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.814151476603886		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 0.814151476603886 | validation: 1.3629847173105365]
	TIME [epoch: 10.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7906776741310892		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 0.7906776741310892 | validation: 1.3323488280317304]
	TIME [epoch: 10.3 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7940507433941754		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 0.7940507433941754 | validation: 1.4105780590203474]
	TIME [epoch: 10.3 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7951111192558613		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 0.7951111192558613 | validation: 1.3113826673056341]
	TIME [epoch: 10.3 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.791173200664218		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 0.791173200664218 | validation: 1.3169222203323931]
	TIME [epoch: 10.3 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7973555777727672		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 0.7973555777727672 | validation: 1.227318453888214]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1085.pth
	Model improved!!!
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7851589905985974		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 0.7851589905985974 | validation: 1.3291645869014057]
	TIME [epoch: 10.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8094034071863728		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 0.8094034071863728 | validation: 1.382527644318611]
	TIME [epoch: 10.3 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7896770871583115		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 0.7896770871583115 | validation: 1.3287986832474774]
	TIME [epoch: 10.3 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7707470670709059		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 0.7707470670709059 | validation: 1.373116423247262]
	TIME [epoch: 10.3 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7889362915920625		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 0.7889362915920625 | validation: 1.36789457645668]
	TIME [epoch: 10.3 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8178195678537297		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 0.8178195678537297 | validation: 1.3467597700131995]
	TIME [epoch: 10.3 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8091663182547884		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 0.8091663182547884 | validation: 1.305822222360642]
	TIME [epoch: 10.3 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7836583087203518		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 0.7836583087203518 | validation: 1.3062358798791467]
	TIME [epoch: 10.3 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.776158411412137		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 0.776158411412137 | validation: 1.3155133078296815]
	TIME [epoch: 10.3 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7871169430727265		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 0.7871169430727265 | validation: 1.3163046532655376]
	TIME [epoch: 10.3 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7801619623170918		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 0.7801619623170918 | validation: 1.354478630775382]
	TIME [epoch: 10.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7847628795280908		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 0.7847628795280908 | validation: 1.5066052342864853]
	TIME [epoch: 10.3 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8225540176518649		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 0.8225540176518649 | validation: 1.3514197419224971]
	TIME [epoch: 10.3 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.798537386674205		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 0.798537386674205 | validation: 1.3666711976203119]
	TIME [epoch: 10.3 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7633901922488059		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 0.7633901922488059 | validation: 1.3621343461801159]
	TIME [epoch: 10.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7908277861123376		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 0.7908277861123376 | validation: 1.388054065358026]
	TIME [epoch: 10.3 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8007636537547123		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 0.8007636537547123 | validation: 1.428793894202843]
	TIME [epoch: 10.3 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7791751478210378		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 0.7791751478210378 | validation: 1.3778277801871799]
	TIME [epoch: 10.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7925489840263574		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 0.7925489840263574 | validation: 1.3822795044985996]
	TIME [epoch: 10.3 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7737821253535976		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 0.7737821253535976 | validation: 1.3568270064176138]
	TIME [epoch: 10.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7841283764618197		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 0.7841283764618197 | validation: 1.3259841941059274]
	TIME [epoch: 10.3 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7834581896018704		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 0.7834581896018704 | validation: 1.3381633008269722]
	TIME [epoch: 10.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.791440557485124		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 0.791440557485124 | validation: 1.32938210950501]
	TIME [epoch: 10.3 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7841222981015437		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 0.7841222981015437 | validation: 1.3650961574171276]
	TIME [epoch: 10.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7580286736977511		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 0.7580286736977511 | validation: 1.3335924761221043]
	TIME [epoch: 10.3 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7952711397727568		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 0.7952711397727568 | validation: 1.3816094406690973]
	TIME [epoch: 10.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8055749953238986		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 0.8055749953238986 | validation: 1.2704880553411444]
	TIME [epoch: 10.3 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.805474851940836		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 0.805474851940836 | validation: 1.3317775790703708]
	TIME [epoch: 10.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7868076224098113		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 0.7868076224098113 | validation: 1.3620099901423215]
	TIME [epoch: 10.3 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7782072545100885		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 0.7782072545100885 | validation: 1.3034848636738492]
	TIME [epoch: 10.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7788527982980893		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 0.7788527982980893 | validation: 1.3630855919895815]
	TIME [epoch: 10.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7828497771617451		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 0.7828497771617451 | validation: 1.3237963780983182]
	TIME [epoch: 10.3 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7651128555282509		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 0.7651128555282509 | validation: 1.305085005168604]
	TIME [epoch: 10.3 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7477568238594474		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 0.7477568238594474 | validation: 1.3446512914959323]
	TIME [epoch: 10.3 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.781619334324829		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 0.781619334324829 | validation: 1.3015600588698095]
	TIME [epoch: 10.3 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.8016551661747717		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 0.8016551661747717 | validation: 1.206774467895821]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1121.pth
	Model improved!!!
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7712982412672881		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 0.7712982412672881 | validation: 1.2288400466354394]
	TIME [epoch: 10.3 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7685240389715358		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 0.7685240389715358 | validation: 1.394920583394747]
	TIME [epoch: 10.3 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7565947712816867		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 0.7565947712816867 | validation: 1.254900353495108]
	TIME [epoch: 10.3 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7734793374904122		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 0.7734793374904122 | validation: 1.2234760597802912]
	TIME [epoch: 10.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7729535424499518		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 0.7729535424499518 | validation: 1.2008241326246496]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1126.pth
	Model improved!!!
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7772516771703204		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 0.7772516771703204 | validation: 1.188046110387571]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7919852499776319		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 0.7919852499776319 | validation: 1.197357548597293]
	TIME [epoch: 10.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7607814641039938		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 0.7607814641039938 | validation: 1.2207315326907258]
	TIME [epoch: 10.3 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7687360662090167		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 0.7687360662090167 | validation: 1.2226498849089407]
	TIME [epoch: 10.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.776896966433928		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 0.776896966433928 | validation: 1.210980013034918]
	TIME [epoch: 10.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7688241445888728		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 0.7688241445888728 | validation: 1.1994021895181834]
	TIME [epoch: 10.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.752706134408127		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 0.752706134408127 | validation: 1.294867179451073]
	TIME [epoch: 10.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7650001491448315		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 0.7650001491448315 | validation: 1.2359463619439044]
	TIME [epoch: 10.3 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7742907293529657		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 0.7742907293529657 | validation: 1.1833707401703504]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1135.pth
	Model improved!!!
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7720794061464021		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 0.7720794061464021 | validation: 1.3623304953364568]
	TIME [epoch: 10.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7677764190915638		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 0.7677764190915638 | validation: 1.302107877277923]
	TIME [epoch: 10.3 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7512407372422694		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 0.7512407372422694 | validation: 1.2832803149440946]
	TIME [epoch: 10.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7521333153272762		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 0.7521333153272762 | validation: 1.4059973863384065]
	TIME [epoch: 10.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7507318474984169		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 0.7507318474984169 | validation: 1.250191283920253]
	TIME [epoch: 10.3 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7687263729717803		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 0.7687263729717803 | validation: 1.2142462512281214]
	TIME [epoch: 10.3 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7449028188698328		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 0.7449028188698328 | validation: 1.2215048869289917]
	TIME [epoch: 10.3 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7391970444176494		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 0.7391970444176494 | validation: 1.3437874938400587]
	TIME [epoch: 10.3 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7478525813098724		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 0.7478525813098724 | validation: 1.3141063839642535]
	TIME [epoch: 10.3 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7395586409977581		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 0.7395586409977581 | validation: 1.322400088679219]
	TIME [epoch: 10.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7717871610010769		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 0.7717871610010769 | validation: 1.2170268737785426]
	TIME [epoch: 10.3 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7485963060980659		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 0.7485963060980659 | validation: 1.237859559926697]
	TIME [epoch: 10.3 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7417842073615996		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 0.7417842073615996 | validation: 1.1862199545322307]
	TIME [epoch: 10.3 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7427928206631836		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 0.7427928206631836 | validation: 1.1730163809289564]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1149.pth
	Model improved!!!
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7397453413535388		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 0.7397453413535388 | validation: 1.1639966758330738]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1150.pth
	Model improved!!!
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7492144485198401		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 0.7492144485198401 | validation: 1.2097186758370069]
	TIME [epoch: 10.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7582320282831481		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 0.7582320282831481 | validation: 1.2790183436713074]
	TIME [epoch: 10.3 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7584361766256111		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 0.7584361766256111 | validation: 1.2255130932008333]
	TIME [epoch: 10.3 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7361405401236241		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 0.7361405401236241 | validation: 1.361959744414521]
	TIME [epoch: 10.3 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7430202093969672		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 0.7430202093969672 | validation: 1.172046978359582]
	TIME [epoch: 10.3 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7299833603114496		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 0.7299833603114496 | validation: 1.123050700773775]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1156.pth
	Model improved!!!
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7485742850241234		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 0.7485742850241234 | validation: 1.1776365464740801]
	TIME [epoch: 10.3 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7380875698551723		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 0.7380875698551723 | validation: 1.1603781823417996]
	TIME [epoch: 10.3 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7355795084935177		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 0.7355795084935177 | validation: 1.2644809054606083]
	TIME [epoch: 10.3 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7556584866893471		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 0.7556584866893471 | validation: 1.2605008683356886]
	TIME [epoch: 10.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7450058314537221		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 0.7450058314537221 | validation: 1.1951642197599746]
	TIME [epoch: 10.3 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7406207871421644		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 0.7406207871421644 | validation: 1.2381586313344988]
	TIME [epoch: 10.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7484673655499406		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 0.7484673655499406 | validation: 1.2569558281954198]
	TIME [epoch: 10.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7388507962379993		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 0.7388507962379993 | validation: 1.3119101123059367]
	TIME [epoch: 10.3 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7465874864117616		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 0.7465874864117616 | validation: 1.2734225230118017]
	TIME [epoch: 10.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.749476046658304		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 0.749476046658304 | validation: 1.2744833830558184]
	TIME [epoch: 10.3 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7797216574837702		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 0.7797216574837702 | validation: 1.1839244401986349]
	TIME [epoch: 10.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7640098861573513		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 0.7640098861573513 | validation: 1.2012445627256025]
	TIME [epoch: 10.3 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7624400775735525		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 0.7624400775735525 | validation: 1.1911163610053848]
	TIME [epoch: 10.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7328706747172763		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 0.7328706747172763 | validation: 1.2762847144044784]
	TIME [epoch: 10.3 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7353638690124773		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 0.7353638690124773 | validation: 1.3284710750981759]
	TIME [epoch: 10.3 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.746131024316547		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 0.746131024316547 | validation: 1.2425351120923045]
	TIME [epoch: 10.3 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7351233674649961		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 0.7351233674649961 | validation: 1.2335625584145997]
	TIME [epoch: 10.3 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7261359461382422		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 0.7261359461382422 | validation: 1.2639875942231773]
	TIME [epoch: 10.3 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7297201365292096		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 0.7297201365292096 | validation: 1.2115416868040338]
	TIME [epoch: 10.3 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7556180107022132		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 0.7556180107022132 | validation: 1.2691223223203583]
	TIME [epoch: 10.3 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7308966949413533		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 0.7308966949413533 | validation: 1.3008147901239127]
	TIME [epoch: 10.3 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7321633338269864		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 0.7321633338269864 | validation: 1.2370003296154386]
	TIME [epoch: 10.3 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7295999454595468		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 0.7295999454595468 | validation: 1.2758679658856193]
	TIME [epoch: 10.3 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7356556942114713		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 0.7356556942114713 | validation: 1.311158471205754]
	TIME [epoch: 10.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7375508384118188		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 0.7375508384118188 | validation: 1.347646719594723]
	TIME [epoch: 10.3 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7093182680739601		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 0.7093182680739601 | validation: 1.268768973172266]
	TIME [epoch: 10.3 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7461947323689547		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 0.7461947323689547 | validation: 1.2327728448840198]
	TIME [epoch: 10.3 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7264885609267833		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 0.7264885609267833 | validation: 1.2581517206848105]
	TIME [epoch: 10.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7389480630187274		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 0.7389480630187274 | validation: 1.1589891110963804]
	TIME [epoch: 10.3 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.737365453953296		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 0.737365453953296 | validation: 1.230925554107396]
	TIME [epoch: 10.3 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7169098613114931		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 0.7169098613114931 | validation: 1.1836491946692764]
	TIME [epoch: 10.3 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7290465964257742		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 0.7290465964257742 | validation: 1.1259809192446495]
	TIME [epoch: 10.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7254704748666743		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 0.7254704748666743 | validation: 1.1431512135148867]
	TIME [epoch: 10.3 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7347255915986194		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 0.7347255915986194 | validation: 1.2124794980454663]
	TIME [epoch: 10.3 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7326311385183997		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 0.7326311385183997 | validation: 1.2604737153645693]
	TIME [epoch: 10.3 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7395064045641722		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 0.7395064045641722 | validation: 1.256096693315693]
	TIME [epoch: 10.3 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7197046919818083		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 0.7197046919818083 | validation: 1.1457690514896022]
	TIME [epoch: 10.3 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7366394029388053		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 0.7366394029388053 | validation: 1.1723794296913137]
	TIME [epoch: 10.3 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7535282835768238		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 0.7535282835768238 | validation: 1.1405252236737664]
	TIME [epoch: 10.3 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7402727822687926		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 0.7402727822687926 | validation: 1.1426004479745973]
	TIME [epoch: 10.3 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.740004966380001		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 0.740004966380001 | validation: 1.1834116216329116]
	TIME [epoch: 10.3 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7199236155604479		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 0.7199236155604479 | validation: 1.2496783099744873]
	TIME [epoch: 10.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7125325661857624		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 0.7125325661857624 | validation: 1.179075089486344]
	TIME [epoch: 10.3 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.709955997134063		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 0.709955997134063 | validation: 1.2428300188370325]
	TIME [epoch: 10.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.708561358999642		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 0.708561358999642 | validation: 1.1715073910195706]
	TIME [epoch: 10.3 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7171090955354533		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 0.7171090955354533 | validation: 1.1936161998340538]
	TIME [epoch: 10.3 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7312193252033323		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 0.7312193252033323 | validation: 1.1954922005153659]
	TIME [epoch: 10.3 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7364612413652329		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 0.7364612413652329 | validation: 1.1583126085365443]
	TIME [epoch: 10.3 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7180247144230419		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 0.7180247144230419 | validation: 1.2083815332797119]
	TIME [epoch: 10.3 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.715114925070902		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 0.715114925070902 | validation: 1.1382370798215766]
	TIME [epoch: 10.3 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7227604762079032		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 0.7227604762079032 | validation: 1.210645728519127]
	TIME [epoch: 10.3 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7140582146705254		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 0.7140582146705254 | validation: 1.2368032847778303]
	TIME [epoch: 10.3 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.715094350074488		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 0.715094350074488 | validation: 1.1826025032464418]
	TIME [epoch: 10.3 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6990068082060314		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 0.6990068082060314 | validation: 1.2125499516978646]
	TIME [epoch: 10.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7384307753853289		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 0.7384307753853289 | validation: 1.2430363820528683]
	TIME [epoch: 10.3 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7259761073305652		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 0.7259761073305652 | validation: 1.1927482842395558]
	TIME [epoch: 10.3 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.710648518017336		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 0.710648518017336 | validation: 1.1684260820068437]
	TIME [epoch: 10.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7234966242464421		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 0.7234966242464421 | validation: 1.157347603237193]
	TIME [epoch: 10.3 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7223714258034573		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 0.7223714258034573 | validation: 1.2196218380225474]
	TIME [epoch: 10.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.719633701141481		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 0.719633701141481 | validation: 1.2002479320256598]
	TIME [epoch: 10.3 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7079805839446236		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 0.7079805839446236 | validation: 1.2343018779543018]
	TIME [epoch: 10.3 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7233351065156517		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 0.7233351065156517 | validation: 1.1989026331236041]
	TIME [epoch: 10.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.723634134997041		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 0.723634134997041 | validation: 1.2300736275645898]
	TIME [epoch: 10.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7107692697407793		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 0.7107692697407793 | validation: 1.2386343589410858]
	TIME [epoch: 10.3 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7276735993069237		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 0.7276735993069237 | validation: 1.253412095183221]
	TIME [epoch: 10.3 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7165111660900236		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 0.7165111660900236 | validation: 1.2224332534724152]
	TIME [epoch: 10.3 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7112721310730192		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 0.7112721310730192 | validation: 1.2283004635792067]
	TIME [epoch: 10.3 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7067995597646339		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 0.7067995597646339 | validation: 1.1571479588875369]
	TIME [epoch: 10.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7252177610923021		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 0.7252177610923021 | validation: 1.1277222847080237]
	TIME [epoch: 10.3 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7254960231415827		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 0.7254960231415827 | validation: 1.1559915587430996]
	TIME [epoch: 10.3 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7220899114759889		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 0.7220899114759889 | validation: 1.1762916726144876]
	TIME [epoch: 10.3 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6991225477532179		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 0.6991225477532179 | validation: 1.1533631478377937]
	TIME [epoch: 10.3 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7051041004484018		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 0.7051041004484018 | validation: 1.3066874106579527]
	TIME [epoch: 10.3 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7038789087936538		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 0.7038789087936538 | validation: 1.1983746022251307]
	TIME [epoch: 10.3 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7422794621873471		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 0.7422794621873471 | validation: 1.2269033250391084]
	TIME [epoch: 10.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7169513346817149		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 0.7169513346817149 | validation: 1.1842036282998063]
	TIME [epoch: 10.3 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7070387454754223		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 0.7070387454754223 | validation: 1.3173986311841832]
	TIME [epoch: 10.3 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6987761724157308		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 0.6987761724157308 | validation: 1.1627586815670399]
	TIME [epoch: 10.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6994022009049627		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 0.6994022009049627 | validation: 1.2125904105002316]
	TIME [epoch: 10.3 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7114850532000851		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 0.7114850532000851 | validation: 1.2527678946659646]
	TIME [epoch: 10.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6941587766955614		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 0.6941587766955614 | validation: 1.1540301177671408]
	TIME [epoch: 10.3 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6970911091662293		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 0.6970911091662293 | validation: 1.1560130530704709]
	TIME [epoch: 10.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7095202207598396		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 0.7095202207598396 | validation: 1.0789242776611532]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1239.pth
	Model improved!!!
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7052203470888776		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 0.7052203470888776 | validation: 1.1735026528843773]
	TIME [epoch: 10.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010293119902709		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 0.7010293119902709 | validation: 1.1744816373320628]
	TIME [epoch: 10.3 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6934107201023436		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 0.6934107201023436 | validation: 1.1401057184473244]
	TIME [epoch: 10.3 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7056181861381967		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 0.7056181861381967 | validation: 1.143741576447341]
	TIME [epoch: 10.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7053306487806671		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 0.7053306487806671 | validation: 1.1845079574538833]
	TIME [epoch: 10.3 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7121706901468986		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 0.7121706901468986 | validation: 1.1713128883140005]
	TIME [epoch: 10.3 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6983623577063856		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 0.6983623577063856 | validation: 1.2040393606715754]
	TIME [epoch: 10.3 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.689796643760468		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 0.689796643760468 | validation: 1.2031940888729216]
	TIME [epoch: 10.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7010838217458308		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 0.7010838217458308 | validation: 1.2066767969797239]
	TIME [epoch: 10.3 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6980935325894297		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 0.6980935325894297 | validation: 1.32724756613629]
	TIME [epoch: 10.3 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7253714263049494		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 0.7253714263049494 | validation: 1.2289084925289795]
	TIME [epoch: 10.3 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7314549589763323		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 0.7314549589763323 | validation: 1.3025253086640964]
	TIME [epoch: 10.3 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7029901781273632		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 0.7029901781273632 | validation: 1.1265471850990019]
	TIME [epoch: 10.3 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6953264014321163		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 0.6953264014321163 | validation: 1.1430069827649354]
	TIME [epoch: 10.3 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7246157640687829		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 0.7246157640687829 | validation: 1.0994735661184183]
	TIME [epoch: 10.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7188886417411098		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 0.7188886417411098 | validation: 1.118104014852732]
	TIME [epoch: 10.3 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6940520869326157		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 0.6940520869326157 | validation: 1.1573590385454462]
	TIME [epoch: 10.3 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6952089898137184		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 0.6952089898137184 | validation: 1.1582280503366678]
	TIME [epoch: 10.3 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7128539982496174		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 0.7128539982496174 | validation: 1.2388813207229756]
	TIME [epoch: 10.3 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7074023068612127		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 0.7074023068612127 | validation: 1.1118970235866206]
	TIME [epoch: 10.3 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7118266509764191		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 0.7118266509764191 | validation: 1.1043245093581167]
	TIME [epoch: 10.3 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7103987288403438		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 0.7103987288403438 | validation: 1.1631340792437808]
	TIME [epoch: 10.3 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7048510527877332		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 0.7048510527877332 | validation: 1.0843420382959257]
	TIME [epoch: 10.3 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6973983575840492		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 0.6973983575840492 | validation: 1.1393731114312877]
	TIME [epoch: 10.3 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7147263396988504		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 0.7147263396988504 | validation: 1.1264948792174863]
	TIME [epoch: 10.3 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6882049905240457		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 0.6882049905240457 | validation: 1.1394955163940916]
	TIME [epoch: 10.3 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7000633727946939		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 0.7000633727946939 | validation: 1.1325064531696052]
	TIME [epoch: 10.3 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7071962116646219		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 0.7071962116646219 | validation: 1.1877413863221145]
	TIME [epoch: 10.3 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7063490439743692		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 0.7063490439743692 | validation: 1.1518951676954967]
	TIME [epoch: 10.3 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6953356386967766		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 0.6953356386967766 | validation: 1.2193521345752236]
	TIME [epoch: 10.3 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6923220062065008		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 0.6923220062065008 | validation: 1.1154656625966741]
	TIME [epoch: 10.3 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6957303385868274		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 0.6957303385868274 | validation: 1.2423670049810214]
	TIME [epoch: 10.3 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6944532272260244		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 0.6944532272260244 | validation: 1.1387914791467453]
	TIME [epoch: 10.3 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6860068146992608		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 0.6860068146992608 | validation: 1.1613214783661874]
	TIME [epoch: 10.3 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7092237426602281		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 0.7092237426602281 | validation: 1.1216060636368865]
	TIME [epoch: 10.3 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6906065467557638		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 0.6906065467557638 | validation: 1.2357716200637234]
	TIME [epoch: 10.3 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6993793596297833		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 0.6993793596297833 | validation: 1.1628651134755972]
	TIME [epoch: 10.3 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6948801132912292		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 0.6948801132912292 | validation: 1.1685026463889714]
	TIME [epoch: 10.3 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.678777499025826		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 0.678777499025826 | validation: 1.1195365263109005]
	TIME [epoch: 10.3 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7013367851579003		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 0.7013367851579003 | validation: 1.1831114406738439]
	TIME [epoch: 10.3 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6981122328602398		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 0.6981122328602398 | validation: 1.1362363228607673]
	TIME [epoch: 10.3 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7051698960176416		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 0.7051698960176416 | validation: 1.1701861009768662]
	TIME [epoch: 10.3 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6825033278548347		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 0.6825033278548347 | validation: 1.108355374180087]
	TIME [epoch: 10.3 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6906463753311082		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 0.6906463753311082 | validation: 1.128004646909146]
	TIME [epoch: 10.3 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6877518639929845		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 0.6877518639929845 | validation: 1.1987937143154233]
	TIME [epoch: 10.3 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6960767404297371		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 0.6960767404297371 | validation: 1.2558799092059485]
	TIME [epoch: 10.3 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947650199793854		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 0.6947650199793854 | validation: 1.2179869557417258]
	TIME [epoch: 10.3 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6995174472845731		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 0.6995174472845731 | validation: 1.2164214265627316]
	TIME [epoch: 10.3 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6880477567527506		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 0.6880477567527506 | validation: 1.1139693373002169]
	TIME [epoch: 10.3 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6874199447574906		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 0.6874199447574906 | validation: 1.184870699972117]
	TIME [epoch: 10.3 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6873127109405235		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 0.6873127109405235 | validation: 1.1210369156127435]
	TIME [epoch: 10.3 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.702629204787055		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 0.702629204787055 | validation: 1.20274638222895]
	TIME [epoch: 10.3 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7211818210401448		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 0.7211818210401448 | validation: 1.159591430805412]
	TIME [epoch: 10.3 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6817415295606066		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 0.6817415295606066 | validation: 1.185383561935967]
	TIME [epoch: 10.3 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7059492480632322		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 0.7059492480632322 | validation: 1.2161993357864993]
	TIME [epoch: 10.3 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6801826690015315		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 0.6801826690015315 | validation: 1.1655849894658377]
	TIME [epoch: 10.3 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6880456091233095		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 0.6880456091233095 | validation: 1.1596921544100172]
	TIME [epoch: 10.3 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6879862179449929		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 0.6879862179449929 | validation: 1.2146683786705743]
	TIME [epoch: 10.3 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6905979812261857		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 0.6905979812261857 | validation: 1.1946985007279687]
	TIME [epoch: 10.3 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6652173754657673		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 0.6652173754657673 | validation: 1.1112530206808064]
	TIME [epoch: 10.3 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6998768801110736		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 0.6998768801110736 | validation: 1.1082571218255048]
	TIME [epoch: 10.3 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6988271161194561		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 0.6988271161194561 | validation: 1.1271964253541795]
	TIME [epoch: 10.3 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6876079974933018		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 0.6876079974933018 | validation: 1.1098487166924778]
	TIME [epoch: 10.3 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6986727417879591		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 0.6986727417879591 | validation: 1.1540872045733392]
	TIME [epoch: 10.3 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6866844889433903		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 0.6866844889433903 | validation: 1.093637315741379]
	TIME [epoch: 10.3 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7018675088788237		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 0.7018675088788237 | validation: 1.138652943945491]
	TIME [epoch: 10.3 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6914697663910665		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 0.6914697663910665 | validation: 1.0914894335294278]
	TIME [epoch: 10.3 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.69788676463445		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 0.69788676463445 | validation: 1.1106599797249306]
	TIME [epoch: 10.3 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7138152017776953		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 0.7138152017776953 | validation: 1.1043483877636668]
	TIME [epoch: 10.3 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6983921392374549		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 0.6983921392374549 | validation: 1.1233127041711375]
	TIME [epoch: 10.3 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6824003119163699		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 0.6824003119163699 | validation: 1.0756231660670077]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1310.pth
	Model improved!!!
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6760957711427243		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 0.6760957711427243 | validation: 1.0972173197639061]
	TIME [epoch: 10.3 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7068170279498815		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 0.7068170279498815 | validation: 1.143581812291524]
	TIME [epoch: 10.3 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6727442789852341		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 0.6727442789852341 | validation: 1.138914020132357]
	TIME [epoch: 10.3 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6866260940048303		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 0.6866260940048303 | validation: 1.1191203919142145]
	TIME [epoch: 10.3 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6876828250868092		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 0.6876828250868092 | validation: 1.1361486404711987]
	TIME [epoch: 10.3 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6851003872262927		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 0.6851003872262927 | validation: 1.1004756754050395]
	TIME [epoch: 10.3 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6790965944487847		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 0.6790965944487847 | validation: 1.1589119752434314]
	TIME [epoch: 10.3 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6775836083450821		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 0.6775836083450821 | validation: 1.1217067836260304]
	TIME [epoch: 10.3 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.68601841883066		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 0.68601841883066 | validation: 1.0875482882013958]
	TIME [epoch: 10.3 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6904932519951678		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 0.6904932519951678 | validation: 1.109520761610557]
	TIME [epoch: 10.3 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6757515732205696		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 0.6757515732205696 | validation: 1.114662432397058]
	TIME [epoch: 10.3 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.682914811115896		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 0.682914811115896 | validation: 1.0738900779298548]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1322.pth
	Model improved!!!
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6814689727899854		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 0.6814689727899854 | validation: 1.0954780290121797]
	TIME [epoch: 10.3 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6911409214923167		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 0.6911409214923167 | validation: 1.1213718092618263]
	TIME [epoch: 10.3 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6792948155709124		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 0.6792948155709124 | validation: 1.110816813038762]
	TIME [epoch: 10.6 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6987633278325264		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 0.6987633278325264 | validation: 1.1175044196505908]
	TIME [epoch: 10.3 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6783639610989001		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 0.6783639610989001 | validation: 1.1134947726243047]
	TIME [epoch: 10.3 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6903549461717885		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 0.6903549461717885 | validation: 1.0950421671867483]
	TIME [epoch: 10.3 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6797137821128896		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 0.6797137821128896 | validation: 1.1690785197668945]
	TIME [epoch: 10.3 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6858555063005085		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 0.6858555063005085 | validation: 1.108959561964711]
	TIME [epoch: 10.3 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.684518112483401		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 0.684518112483401 | validation: 1.102129356011494]
	TIME [epoch: 10.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6749076442577953		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 0.6749076442577953 | validation: 1.0889266825050414]
	TIME [epoch: 10.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.680295795897738		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 0.680295795897738 | validation: 1.1347936100379183]
	TIME [epoch: 10.3 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6919443857146187		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 0.6919443857146187 | validation: 1.1132304640774668]
	TIME [epoch: 10.3 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6750758751741015		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 0.6750758751741015 | validation: 1.1327793269259365]
	TIME [epoch: 10.3 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.689500506584367		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 0.689500506584367 | validation: 1.1401718269053311]
	TIME [epoch: 10.3 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6962151042207709		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 0.6962151042207709 | validation: 1.1497925957008628]
	TIME [epoch: 10.3 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6713652187315675		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 0.6713652187315675 | validation: 1.132850357248657]
	TIME [epoch: 10.3 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6615568411756365		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 0.6615568411756365 | validation: 1.092932825371194]
	TIME [epoch: 10.3 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6691168144623882		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 0.6691168144623882 | validation: 1.1202800182321764]
	TIME [epoch: 10.3 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6498316686009373		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 0.6498316686009373 | validation: 1.1115979635583708]
	TIME [epoch: 10.3 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6676163594417737		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 0.6676163594417737 | validation: 1.118745420147174]
	TIME [epoch: 10.3 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.680865513078698		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 0.680865513078698 | validation: 1.1102725674745584]
	TIME [epoch: 10.3 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6659757353208313		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 0.6659757353208313 | validation: 1.119647932915363]
	TIME [epoch: 10.3 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6837900026614838		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 0.6837900026614838 | validation: 1.1473062491002393]
	TIME [epoch: 10.3 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6886567872672217		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 0.6886567872672217 | validation: 1.1318181538739647]
	TIME [epoch: 10.3 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.691716599853433		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 0.691716599853433 | validation: 1.1055396058693312]
	TIME [epoch: 10.3 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6761690190042622		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 0.6761690190042622 | validation: 1.1614358717359858]
	TIME [epoch: 10.3 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6904951564468339		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 0.6904951564468339 | validation: 1.146478595975217]
	TIME [epoch: 10.3 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6710351657703295		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 0.6710351657703295 | validation: 1.1503685713980074]
	TIME [epoch: 10.3 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6869516968999875		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 0.6869516968999875 | validation: 1.1221659901034289]
	TIME [epoch: 10.3 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6712592387777123		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 0.6712592387777123 | validation: 1.1001799066500841]
	TIME [epoch: 10.3 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6759419937636821		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 0.6759419937636821 | validation: 1.1496832006407294]
	TIME [epoch: 10.3 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.7014882182394317		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 0.7014882182394317 | validation: 1.145272534079725]
	TIME [epoch: 10.3 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6889665870213206		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 0.6889665870213206 | validation: 1.1648175161115693]
	TIME [epoch: 10.3 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6892679343851312		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 0.6892679343851312 | validation: 1.2382911422876277]
	TIME [epoch: 10.3 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6788369998516354		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 0.6788369998516354 | validation: 1.1588600337383612]
	TIME [epoch: 10.3 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6834487414461654		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 0.6834487414461654 | validation: 1.1404178238401859]
	TIME [epoch: 10.3 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6823810895360273		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 0.6823810895360273 | validation: 1.2274208375853022]
	TIME [epoch: 10.3 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6770072012981163		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 0.6770072012981163 | validation: 1.1504043743227779]
	TIME [epoch: 10.3 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6833182737379974		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 0.6833182737379974 | validation: 1.112877820833702]
	TIME [epoch: 10.3 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6891763303948243		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 0.6891763303948243 | validation: 1.1818353629613616]
	TIME [epoch: 10.3 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6745721438372403		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 0.6745721438372403 | validation: 1.0875862996662704]
	TIME [epoch: 10.3 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6728261068219565		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 0.6728261068219565 | validation: 1.1514492565562389]
	TIME [epoch: 10.3 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6912315501707563		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 0.6912315501707563 | validation: 1.1309379157281507]
	TIME [epoch: 10.3 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6523485578756422		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 0.6523485578756422 | validation: 1.1238860789979876]
	TIME [epoch: 10.3 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6655262123223177		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 0.6655262123223177 | validation: 1.2045151741935987]
	TIME [epoch: 10.3 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.67134417195571		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 0.67134417195571 | validation: 1.0902078299171445]
	TIME [epoch: 10.3 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6681221594096199		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 0.6681221594096199 | validation: 1.1410847128881236]
	TIME [epoch: 10.3 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6947324722463193		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 0.6947324722463193 | validation: 1.1123945311146828]
	TIME [epoch: 10.3 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6669219126621594		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 0.6669219126621594 | validation: 1.1336097698083965]
	TIME [epoch: 10.3 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6630266543727908		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 0.6630266543727908 | validation: 1.1161197319690483]
	TIME [epoch: 10.3 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6887936117921918		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 0.6887936117921918 | validation: 1.1812194051414795]
	TIME [epoch: 10.3 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.676097431469182		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 0.676097431469182 | validation: 1.1317193905691132]
	TIME [epoch: 10.3 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6793469133476726		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 0.6793469133476726 | validation: 1.094682006099296]
	TIME [epoch: 10.3 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6739516923474662		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 0.6739516923474662 | validation: 1.168960620714463]
	TIME [epoch: 10.3 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6798672189741267		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 0.6798672189741267 | validation: 1.1654250576351368]
	TIME [epoch: 10.3 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6843851980404617		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 0.6843851980404617 | validation: 1.1268457942465342]
	TIME [epoch: 10.3 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6614353379489116		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 0.6614353379489116 | validation: 1.1183259543501536]
	TIME [epoch: 10.3 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6545141010082818		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 0.6545141010082818 | validation: 1.1120756400698582]
	TIME [epoch: 10.3 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.667356014467319		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 0.667356014467319 | validation: 1.0920924775181318]
	TIME [epoch: 10.3 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6835754947998596		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 0.6835754947998596 | validation: 1.084954574783643]
	TIME [epoch: 10.3 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6618933588797742		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 0.6618933588797742 | validation: 1.0867907486932276]
	TIME [epoch: 10.3 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6722338445104877		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 0.6722338445104877 | validation: 1.115724261805796]
	TIME [epoch: 10.3 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.667813896933137		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 0.667813896933137 | validation: 1.1584671734422824]
	TIME [epoch: 10.3 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6784733315138792		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 0.6784733315138792 | validation: 1.1269185659570276]
	TIME [epoch: 10.3 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6530870404637439		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 0.6530870404637439 | validation: 1.0593125709456168]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1387.pth
	Model improved!!!
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6645149781935328		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 0.6645149781935328 | validation: 1.1284470971289597]
	TIME [epoch: 10.3 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6598843416624327		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 0.6598843416624327 | validation: 1.1031532040502605]
	TIME [epoch: 10.3 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6594770981106309		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 0.6594770981106309 | validation: 1.1610147942099145]
	TIME [epoch: 10.3 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.658659928693286		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 0.658659928693286 | validation: 1.089550340048733]
	TIME [epoch: 10.3 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6850768240278123		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 0.6850768240278123 | validation: 1.1203611187977223]
	TIME [epoch: 10.3 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6690519162727008		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 0.6690519162727008 | validation: 1.1237315443151152]
	TIME [epoch: 10.3 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6632741508863077		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 0.6632741508863077 | validation: 1.1291761051708051]
	TIME [epoch: 10.3 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.671693659045109		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 0.671693659045109 | validation: 1.089777585597456]
	TIME [epoch: 10.3 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6733757617183336		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 0.6733757617183336 | validation: 1.1252761665276063]
	TIME [epoch: 10.3 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6790731462677309		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 0.6790731462677309 | validation: 1.1020193317408873]
	TIME [epoch: 10.3 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6675968303033821		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 0.6675968303033821 | validation: 1.1067283108886932]
	TIME [epoch: 10.3 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6488681903723692		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 0.6488681903723692 | validation: 1.155595412332929]
	TIME [epoch: 10.3 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.657315396474248		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 0.657315396474248 | validation: 1.1643064023380583]
	TIME [epoch: 10.3 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6725722685388518		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 0.6725722685388518 | validation: 1.1444394390444317]
	TIME [epoch: 10.3 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6584719522650111		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 0.6584719522650111 | validation: 1.184666381417765]
	TIME [epoch: 10.3 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6734126755693468		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 0.6734126755693468 | validation: 1.1749237093972655]
	TIME [epoch: 10.3 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6718638104315194		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 0.6718638104315194 | validation: 1.097497281049793]
	TIME [epoch: 10.3 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6472941032711395		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 0.6472941032711395 | validation: 1.1309438215177772]
	TIME [epoch: 10.3 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6579511151522068		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 0.6579511151522068 | validation: 1.1335555883621546]
	TIME [epoch: 10.3 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6558237186558393		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 0.6558237186558393 | validation: 1.1116668317995306]
	TIME [epoch: 10.3 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6752137064516384		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 0.6752137064516384 | validation: 1.1040172540389157]
	TIME [epoch: 10.3 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6494027126086392		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 0.6494027126086392 | validation: 1.1227682322346764]
	TIME [epoch: 10.3 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.664824947077421		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 0.664824947077421 | validation: 1.134344616471715]
	TIME [epoch: 10.3 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6702588891157137		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 0.6702588891157137 | validation: 1.123469838974371]
	TIME [epoch: 10.3 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6718675274714823		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 0.6718675274714823 | validation: 1.1527339283415416]
	TIME [epoch: 10.3 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6648715888627341		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 0.6648715888627341 | validation: 1.113160475702537]
	TIME [epoch: 10.3 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6615105889544284		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 0.6615105889544284 | validation: 1.0962786023110522]
	TIME [epoch: 10.3 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6624039276028334		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 0.6624039276028334 | validation: 1.08323248730279]
	TIME [epoch: 10.3 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6688172009191233		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 0.6688172009191233 | validation: 1.1606887101956644]
	TIME [epoch: 10.3 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6814499415189712		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 0.6814499415189712 | validation: 1.2069832585795635]
	TIME [epoch: 10.3 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6500118409488989		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 0.6500118409488989 | validation: 1.1093903023962752]
	TIME [epoch: 10.3 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6589761112390148		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 0.6589761112390148 | validation: 1.1003331910227987]
	TIME [epoch: 10.3 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6576275452064484		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 0.6576275452064484 | validation: 1.0997004254084015]
	TIME [epoch: 10.3 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.678845813111698		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 0.678845813111698 | validation: 1.1493707443184442]
	TIME [epoch: 10.3 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6554560324433013		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 0.6554560324433013 | validation: 1.120573766329932]
	TIME [epoch: 10.3 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6629950623775224		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 0.6629950623775224 | validation: 1.0635071246507533]
	TIME [epoch: 10.3 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6665507027434933		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 0.6665507027434933 | validation: 1.1074503180070483]
	TIME [epoch: 10.3 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6488246567604012		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 0.6488246567604012 | validation: 1.09871105405145]
	TIME [epoch: 10.3 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6515633430768528		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 0.6515633430768528 | validation: 1.1261073252822964]
	TIME [epoch: 10.3 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6628106162045819		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 0.6628106162045819 | validation: 1.1329746066259858]
	TIME [epoch: 10.3 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6789052373682807		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 0.6789052373682807 | validation: 1.0944242733031193]
	TIME [epoch: 10.3 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6496375975142785		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 0.6496375975142785 | validation: 1.0514574777539034]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1429.pth
	Model improved!!!
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.662823447130574		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 0.662823447130574 | validation: 1.1349785095658276]
	TIME [epoch: 10.3 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6514506697281911		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 0.6514506697281911 | validation: 1.075017839035707]
	TIME [epoch: 10.3 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6408495027023959		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 0.6408495027023959 | validation: 1.0705624879978222]
	TIME [epoch: 10.3 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6804156665512427		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 0.6804156665512427 | validation: 1.0522634191241398]
	TIME [epoch: 10.3 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6738605382742946		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 0.6738605382742946 | validation: 1.088870802519058]
	TIME [epoch: 10.3 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6511996028832492		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 0.6511996028832492 | validation: 1.0904222476733134]
	TIME [epoch: 10.3 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6541381210979902		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 0.6541381210979902 | validation: 1.1030012725529066]
	TIME [epoch: 10.3 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6599211632295114		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 0.6599211632295114 | validation: 1.1022323005286567]
	TIME [epoch: 10.3 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6653175094748452		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 0.6653175094748452 | validation: 1.2252040711792056]
	TIME [epoch: 10.3 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6617189613067243		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 0.6617189613067243 | validation: 1.1592367827746857]
	TIME [epoch: 10.3 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6729128916431382		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 0.6729128916431382 | validation: 1.118354140445103]
	TIME [epoch: 10.3 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6656133559147013		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 0.6656133559147013 | validation: 1.0952643857150637]
	TIME [epoch: 10.3 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6685018880465488		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 0.6685018880465488 | validation: 1.0648145633361188]
	TIME [epoch: 10.3 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6658806496761331		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 0.6658806496761331 | validation: 1.1224715564944814]
	TIME [epoch: 10.3 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6703931050903924		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 0.6703931050903924 | validation: 1.0893381438273633]
	TIME [epoch: 10.3 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6504888743757328		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 0.6504888743757328 | validation: 1.1183642956600022]
	TIME [epoch: 10.3 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6664976518648846		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 0.6664976518648846 | validation: 1.0570979478687317]
	TIME [epoch: 10.3 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.666067822940788		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 0.666067822940788 | validation: 1.0673695452868213]
	TIME [epoch: 10.3 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6627090124454625		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 0.6627090124454625 | validation: 1.0667760465174003]
	TIME [epoch: 10.3 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6574848768060082		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 0.6574848768060082 | validation: 1.083685921691132]
	TIME [epoch: 10.3 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6504837799753727		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 0.6504837799753727 | validation: 1.0620298078910653]
	TIME [epoch: 10.3 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6401834721542662		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 0.6401834721542662 | validation: 1.0746827235820515]
	TIME [epoch: 10.3 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6703027787716702		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 0.6703027787716702 | validation: 1.048956293445048]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1452.pth
	Model improved!!!
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6581977514247783		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 0.6581977514247783 | validation: 1.0469187585301294]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1453.pth
	Model improved!!!
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6598362943546076		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 0.6598362943546076 | validation: 1.0447675197944544]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1454.pth
	Model improved!!!
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6548681106450663		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 0.6548681106450663 | validation: 1.0565937001001342]
	TIME [epoch: 10.3 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6645478429463358		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 0.6645478429463358 | validation: 1.0895074896870551]
	TIME [epoch: 10.3 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6674891995869727		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 0.6674891995869727 | validation: 1.060750152943184]
	TIME [epoch: 10.3 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6652545808921551		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 0.6652545808921551 | validation: 1.1042946354453982]
	TIME [epoch: 10.3 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6653966589314054		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 0.6653966589314054 | validation: 1.0960543406844765]
	TIME [epoch: 10.3 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6443215892427092		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 0.6443215892427092 | validation: 1.1583728254365042]
	TIME [epoch: 10.3 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6540987712970754		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 0.6540987712970754 | validation: 1.0745778095503626]
	TIME [epoch: 10.3 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6569752122889927		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 0.6569752122889927 | validation: 1.1518304205906205]
	TIME [epoch: 10.3 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6595561312164436		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 0.6595561312164436 | validation: 1.0351497512202155]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1463.pth
	Model improved!!!
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.656889379907409		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 0.656889379907409 | validation: 1.103538815745707]
	TIME [epoch: 10.3 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6741901106356184		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 0.6741901106356184 | validation: 1.0633760784565292]
	TIME [epoch: 10.3 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6502724022321423		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 0.6502724022321423 | validation: 1.0897438280449534]
	TIME [epoch: 10.3 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6444365089342725		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 0.6444365089342725 | validation: 1.058409533398471]
	TIME [epoch: 10.3 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6552927285711985		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 0.6552927285711985 | validation: 1.0893069623239093]
	TIME [epoch: 10.3 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6739991242134207		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 0.6739991242134207 | validation: 1.1136538753514214]
	TIME [epoch: 10.3 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.662153798887568		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 0.662153798887568 | validation: 1.1037621319816837]
	TIME [epoch: 10.3 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6512407571681575		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 0.6512407571681575 | validation: 1.0780673669054]
	TIME [epoch: 10.3 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6741905709014531		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 0.6741905709014531 | validation: 1.0720433700013845]
	TIME [epoch: 10.3 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6587530479019519		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 0.6587530479019519 | validation: 1.066057020081025]
	TIME [epoch: 10.3 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6602929166709313		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 0.6602929166709313 | validation: 1.0489924831696587]
	TIME [epoch: 10.3 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6352906150982662		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 0.6352906150982662 | validation: 1.1215197561791437]
	TIME [epoch: 10.3 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6551196780085091		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 0.6551196780085091 | validation: 1.0454268107502065]
	TIME [epoch: 10.3 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6625287554186405		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 0.6625287554186405 | validation: 1.0525252548648911]
	TIME [epoch: 10.3 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6499804090491672		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 0.6499804090491672 | validation: 1.0029159119347384]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1478.pth
	Model improved!!!
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6715167531921863		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 0.6715167531921863 | validation: 1.0526892109812591]
	TIME [epoch: 10.3 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6666222561769654		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 0.6666222561769654 | validation: 1.044187716617955]
	TIME [epoch: 10.3 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6537752065722824		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 0.6537752065722824 | validation: 1.0290850587284768]
	TIME [epoch: 10.3 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6481862468971612		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 0.6481862468971612 | validation: 1.0359579516462214]
	TIME [epoch: 10.3 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6595841838204441		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 0.6595841838204441 | validation: 1.0372936003234292]
	TIME [epoch: 10.3 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6436005710924215		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 0.6436005710924215 | validation: 1.0635445415736675]
	TIME [epoch: 10.3 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6483929333986168		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 0.6483929333986168 | validation: 1.0511248071450225]
	TIME [epoch: 10.3 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6457845513365464		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 0.6457845513365464 | validation: 1.1253372835851048]
	TIME [epoch: 10.3 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.645232602122347		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 0.645232602122347 | validation: 1.0581961814064034]
	TIME [epoch: 10.3 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6587732698311172		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 0.6587732698311172 | validation: 1.0703021026267836]
	TIME [epoch: 10.3 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6523780298011166		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 0.6523780298011166 | validation: 1.0971738293075233]
	TIME [epoch: 10.3 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.652312291540233		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 0.652312291540233 | validation: 1.0530159853260495]
	TIME [epoch: 10.3 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6453615358912433		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 0.6453615358912433 | validation: 1.0561609725443037]
	TIME [epoch: 10.3 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6767662363727174		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 0.6767662363727174 | validation: 1.0628768216789033]
	TIME [epoch: 10.3 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6621325591257116		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 0.6621325591257116 | validation: 1.0572822880625352]
	TIME [epoch: 10.3 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442427301147654		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 0.6442427301147654 | validation: 1.03357299949947]
	TIME [epoch: 10.3 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6597519200023552		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 0.6597519200023552 | validation: 1.066677124754047]
	TIME [epoch: 10.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6447078491205078		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 0.6447078491205078 | validation: 1.0749013262223042]
	TIME [epoch: 10.3 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6524770380677518		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 0.6524770380677518 | validation: 1.0473801740784854]
	TIME [epoch: 10.3 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6736888437815228		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 0.6736888437815228 | validation: 1.0502872942272248]
	TIME [epoch: 10.3 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6505522120223938		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 0.6505522120223938 | validation: 1.0503457371631977]
	TIME [epoch: 10.3 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6418010910858593		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 0.6418010910858593 | validation: 1.078484408356022]
	TIME [epoch: 10.3 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6608408231107594		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 0.6608408231107594 | validation: 1.053194717935057]
	TIME [epoch: 10.3 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6449496755499597		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 0.6449496755499597 | validation: 1.0568123194506638]
	TIME [epoch: 10.3 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6530621567396468		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 0.6530621567396468 | validation: 1.0570348483058996]
	TIME [epoch: 10.3 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6469675386334609		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 0.6469675386334609 | validation: 1.0389096595872263]
	TIME [epoch: 10.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6612507608505979		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 0.6612507608505979 | validation: 1.0398982036280666]
	TIME [epoch: 10.3 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6492863437879169		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 0.6492863437879169 | validation: 1.1134189465961357]
	TIME [epoch: 10.3 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6627135358517846		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 0.6627135358517846 | validation: 1.043444300515005]
	TIME [epoch: 10.3 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6432132100332723		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 0.6432132100332723 | validation: 1.0599329638600135]
	TIME [epoch: 10.3 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6343026794698832		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 0.6343026794698832 | validation: 1.040575610012404]
	TIME [epoch: 10.3 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6411349035932147		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 0.6411349035932147 | validation: 1.0670658308463887]
	TIME [epoch: 10.3 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6708654597363894		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 0.6708654597363894 | validation: 1.0514231474324844]
	TIME [epoch: 10.3 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6456940485165382		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 0.6456940485165382 | validation: 1.0461770299959152]
	TIME [epoch: 10.3 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.639171107271199		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 0.639171107271199 | validation: 1.0649698346153083]
	TIME [epoch: 10.3 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6508463353129577		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 0.6508463353129577 | validation: 1.0847817368485764]
	TIME [epoch: 10.3 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6496002384499012		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 0.6496002384499012 | validation: 1.114417902548766]
	TIME [epoch: 10.3 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6429127170192457		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 0.6429127170192457 | validation: 1.1520326830714058]
	TIME [epoch: 10.3 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6677334546757894		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 0.6677334546757894 | validation: 1.0392093998630159]
	TIME [epoch: 10.3 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6560545788255276		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 0.6560545788255276 | validation: 1.059604501680279]
	TIME [epoch: 10.3 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.638824126845377		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 0.638824126845377 | validation: 1.0380819222947018]
	TIME [epoch: 10.3 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6488672175913459		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 0.6488672175913459 | validation: 1.0915765786236116]
	TIME [epoch: 10.3 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6488240799177186		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 0.6488240799177186 | validation: 1.0669645542035207]
	TIME [epoch: 10.3 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6515208075722562		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 0.6515208075722562 | validation: 1.0920606709490965]
	TIME [epoch: 10.3 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6605866897868247		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 0.6605866897868247 | validation: 1.0280198248809371]
	TIME [epoch: 10.3 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6431922438891493		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 0.6431922438891493 | validation: 1.020385787480112]
	TIME [epoch: 10.3 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6525520635703235		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 0.6525520635703235 | validation: 1.076346031438538]
	TIME [epoch: 10.3 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6436474714158763		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 0.6436474714158763 | validation: 1.0314743756860625]
	TIME [epoch: 10.3 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6398705305475774		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 0.6398705305475774 | validation: 1.0437413438249414]
	TIME [epoch: 10.3 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6405179927474647		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 0.6405179927474647 | validation: 1.0650755173353108]
	TIME [epoch: 10.3 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6508910199042226		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 0.6508910199042226 | validation: 1.0614280532480858]
	TIME [epoch: 10.3 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439366032509992		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 0.6439366032509992 | validation: 1.1346628253254436]
	TIME [epoch: 10.3 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6444259570082955		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 0.6444259570082955 | validation: 1.0081919152599557]
	TIME [epoch: 10.3 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.644087241676209		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 0.644087241676209 | validation: 1.0350117661252418]
	TIME [epoch: 10.3 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6587864394213843		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 0.6587864394213843 | validation: 1.0624009217647419]
	TIME [epoch: 10.3 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6445564626975636		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 0.6445564626975636 | validation: 1.1238924563428214]
	TIME [epoch: 10.3 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.661595600901592		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 0.661595600901592 | validation: 1.1028577931923353]
	TIME [epoch: 10.3 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.639639338271289		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 0.639639338271289 | validation: 1.0419579124205796]
	TIME [epoch: 10.3 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6364408788200958		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 0.6364408788200958 | validation: 1.002744286393512]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1537.pth
	Model improved!!!
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.666094500368518		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 0.666094500368518 | validation: 1.0450456873010883]
	TIME [epoch: 10.3 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6390504970333982		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 0.6390504970333982 | validation: 1.037875137400638]
	TIME [epoch: 10.3 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6469921352146495		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 0.6469921352146495 | validation: 1.0330795517482192]
	TIME [epoch: 10.3 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.659764987116672		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 0.659764987116672 | validation: 1.1530145693106444]
	TIME [epoch: 10.3 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442676029988619		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 0.6442676029988619 | validation: 1.0600433292639866]
	TIME [epoch: 10.3 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6533119985595122		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 0.6533119985595122 | validation: 1.0495230863828942]
	TIME [epoch: 10.3 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6432488080629446		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 0.6432488080629446 | validation: 1.0512634950091049]
	TIME [epoch: 10.3 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6356426270454675		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 0.6356426270454675 | validation: 1.0438197659131487]
	TIME [epoch: 10.3 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6640122489526455		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 0.6640122489526455 | validation: 1.047307293542201]
	TIME [epoch: 10.3 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6445685643792756		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 0.6445685643792756 | validation: 1.0266705412925716]
	TIME [epoch: 10.3 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6370109764935095		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 0.6370109764935095 | validation: 1.069706880195684]
	TIME [epoch: 10.3 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6424473375112766		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 0.6424473375112766 | validation: 1.0379779139378127]
	TIME [epoch: 10.3 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6653049314463397		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 0.6653049314463397 | validation: 1.033511606588799]
	TIME [epoch: 10.3 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6556295533603459		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 0.6556295533603459 | validation: 1.0095451262362967]
	TIME [epoch: 10.3 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6357586407729757		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 0.6357586407729757 | validation: 1.072285337325899]
	TIME [epoch: 10.3 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6608741750122682		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 0.6608741750122682 | validation: 1.0782935317353526]
	TIME [epoch: 10.3 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6674762267384391		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 0.6674762267384391 | validation: 1.076029982834929]
	TIME [epoch: 10.3 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6389846425540362		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 0.6389846425540362 | validation: 1.0320782646547417]
	TIME [epoch: 10.3 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6512237420090595		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 0.6512237420090595 | validation: 1.0268533022377582]
	TIME [epoch: 10.3 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.626255834214793		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 0.626255834214793 | validation: 1.0248264548301402]
	TIME [epoch: 10.3 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6451048419195707		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 0.6451048419195707 | validation: 1.0081945853393166]
	TIME [epoch: 10.3 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6386051778571039		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 0.6386051778571039 | validation: 1.005483036987404]
	TIME [epoch: 10.3 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6384770424002196		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 0.6384770424002196 | validation: 1.0435586295572998]
	TIME [epoch: 10.3 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6621285066376487		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 0.6621285066376487 | validation: 1.091388751575237]
	TIME [epoch: 10.3 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6419448701622672		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 0.6419448701622672 | validation: 1.0452153840639469]
	TIME [epoch: 10.3 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6391055249232422		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 0.6391055249232422 | validation: 1.0459853925710032]
	TIME [epoch: 10.3 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6541138751195452		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 0.6541138751195452 | validation: 1.036388402839058]
	TIME [epoch: 10.3 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6405843137702735		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 0.6405843137702735 | validation: 1.0600238186157163]
	TIME [epoch: 10.3 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6372750121057894		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 0.6372750121057894 | validation: 1.0365168176161672]
	TIME [epoch: 10.3 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6496089675213892		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 0.6496089675213892 | validation: 1.0243718866757479]
	TIME [epoch: 10.3 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6476000415256389		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 0.6476000415256389 | validation: 1.0779011699935808]
	TIME [epoch: 10.3 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6557176516282597		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 0.6557176516282597 | validation: 1.0243610346982803]
	TIME [epoch: 10.3 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6637950699683296		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 0.6637950699683296 | validation: 1.030045545018664]
	TIME [epoch: 10.3 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6434310500902105		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 0.6434310500902105 | validation: 1.032881605043416]
	TIME [epoch: 10.3 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6342449404836668		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 0.6342449404836668 | validation: 1.042133350157574]
	TIME [epoch: 10.3 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6459889769006263		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 0.6459889769006263 | validation: 1.1162161792679544]
	TIME [epoch: 10.3 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6434506260106574		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 0.6434506260106574 | validation: 1.027085606760925]
	TIME [epoch: 10.3 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442574404556934		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 0.6442574404556934 | validation: 1.0262316218148564]
	TIME [epoch: 10.3 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6407054712412021		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 0.6407054712412021 | validation: 1.1268821608186608]
	TIME [epoch: 10.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6479804412697834		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 0.6479804412697834 | validation: 1.014756201658146]
	TIME [epoch: 10.3 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.64580714034331		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 0.64580714034331 | validation: 1.0387825928045964]
	TIME [epoch: 10.3 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6565371913287115		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 0.6565371913287115 | validation: 1.0338416953250056]
	TIME [epoch: 10.3 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6370184149300806		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 0.6370184149300806 | validation: 1.0419869549435488]
	TIME [epoch: 10.3 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6482214146323976		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 0.6482214146323976 | validation: 1.0690879168012377]
	TIME [epoch: 10.3 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6633929383354897		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 0.6633929383354897 | validation: 1.03587986669103]
	TIME [epoch: 10.3 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442825261628856		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 0.6442825261628856 | validation: 1.0186576347951737]
	TIME [epoch: 10.3 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.632325499664007		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 0.632325499664007 | validation: 1.007236634446883]
	TIME [epoch: 10.3 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6464317446393675		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 0.6464317446393675 | validation: 1.0362912752125848]
	TIME [epoch: 10.3 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6481922482297189		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 0.6481922482297189 | validation: 1.0373412300012075]
	TIME [epoch: 10.3 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6377177908183698		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 0.6377177908183698 | validation: 1.06317431506896]
	TIME [epoch: 10.3 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6392341593362275		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 0.6392341593362275 | validation: 1.0335840689389288]
	TIME [epoch: 10.3 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6318099184957198		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 0.6318099184957198 | validation: 1.036612555145999]
	TIME [epoch: 10.3 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6475814939189786		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 0.6475814939189786 | validation: 1.0921943546932884]
	TIME [epoch: 10.3 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6530470030168909		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 0.6530470030168909 | validation: 1.048608576362033]
	TIME [epoch: 10.3 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6554673280483585		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 0.6554673280483585 | validation: 1.149511402206532]
	TIME [epoch: 10.3 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.656147799042456		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 0.656147799042456 | validation: 1.1173530904446407]
	TIME [epoch: 10.3 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6475210094145561		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 0.6475210094145561 | validation: 1.0970609156943503]
	TIME [epoch: 10.3 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6397678678814153		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 0.6397678678814153 | validation: 1.0664616557471545]
	TIME [epoch: 10.3 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6480487396328295		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 0.6480487396328295 | validation: 1.052984360015886]
	TIME [epoch: 10.3 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6357630637925183		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 0.6357630637925183 | validation: 1.0615861313188628]
	TIME [epoch: 10.3 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.637285556199393		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 0.637285556199393 | validation: 1.144045614163972]
	TIME [epoch: 10.3 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6520309255799281		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 0.6520309255799281 | validation: 1.0848480187664669]
	TIME [epoch: 10.3 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6301152856655835		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 0.6301152856655835 | validation: 1.1504402010858084]
	TIME [epoch: 10.3 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6493832415259522		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 0.6493832415259522 | validation: 1.0665930415801932]
	TIME [epoch: 10.3 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6524715092386695		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 0.6524715092386695 | validation: 1.1212116609287217]
	TIME [epoch: 10.3 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6502251492146414		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 0.6502251492146414 | validation: 1.0403284555913492]
	TIME [epoch: 10.3 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6392289055159027		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 0.6392289055159027 | validation: 1.0398510644941112]
	TIME [epoch: 10.3 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6285508255539145		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 0.6285508255539145 | validation: 1.041558204271389]
	TIME [epoch: 10.3 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6402248532001015		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 0.6402248532001015 | validation: 1.025663964910262]
	TIME [epoch: 10.3 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6405338491139035		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 0.6405338491139035 | validation: 1.0041596930996515]
	TIME [epoch: 10.3 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6418079681380453		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 0.6418079681380453 | validation: 1.0353673268289807]
	TIME [epoch: 10.3 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6269968633880026		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 0.6269968633880026 | validation: 1.0397256878521497]
	TIME [epoch: 10.3 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6408936539915496		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 0.6408936539915496 | validation: 1.0308761576976417]
	TIME [epoch: 10.3 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6329532378076685		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 0.6329532378076685 | validation: 1.0591653238872538]
	TIME [epoch: 10.3 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6388622589267071		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 0.6388622589267071 | validation: 1.1367338398882076]
	TIME [epoch: 10.3 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6381709433688425		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 0.6381709433688425 | validation: 1.069179075327987]
	TIME [epoch: 10.3 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6381124597423103		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 0.6381124597423103 | validation: 1.0544245500364458]
	TIME [epoch: 10.3 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6360724821763167		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 0.6360724821763167 | validation: 1.0727728528851583]
	TIME [epoch: 10.3 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6366687234340098		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 0.6366687234340098 | validation: 1.0471789431042178]
	TIME [epoch: 10.3 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6405975887345534		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 0.6405975887345534 | validation: 1.0843286462009778]
	TIME [epoch: 10.3 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6292625259348231		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 0.6292625259348231 | validation: 1.046574962239574]
	TIME [epoch: 10.3 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6388300892422765		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 0.6388300892422765 | validation: 1.0261383505847617]
	TIME [epoch: 10.3 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6411220829846485		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 0.6411220829846485 | validation: 1.0340637514163287]
	TIME [epoch: 10.3 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6404012026056674		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 0.6404012026056674 | validation: 1.0242702537248738]
	TIME [epoch: 10.3 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6348801140070526		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 0.6348801140070526 | validation: 1.0133846500706225]
	TIME [epoch: 10.3 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6388462508411172		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 0.6388462508411172 | validation: 1.0338503404526815]
	TIME [epoch: 10.3 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6635239143733684		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 0.6635239143733684 | validation: 1.0282578382657528]
	TIME [epoch: 10.3 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6402000295817454		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 0.6402000295817454 | validation: 1.0896156686625587]
	TIME [epoch: 10.3 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.663936207978306		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 0.663936207978306 | validation: 1.0356101370924762]
	TIME [epoch: 10.3 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6605786406250165		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 0.6605786406250165 | validation: 1.1280470401167741]
	TIME [epoch: 10.3 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6423488781076865		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 0.6423488781076865 | validation: 1.1106096501990304]
	TIME [epoch: 10.3 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6361026746611246		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 0.6361026746611246 | validation: 1.0560251200721904]
	TIME [epoch: 10.3 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6263888112172304		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 0.6263888112172304 | validation: 1.0618593346492264]
	TIME [epoch: 10.3 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.64987178211405		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 0.64987178211405 | validation: 1.0397736053556668]
	TIME [epoch: 10.3 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6505614770033133		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 0.6505614770033133 | validation: 1.0404770648834305]
	TIME [epoch: 10.3 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6401934035742667		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 0.6401934035742667 | validation: 1.0052930312211155]
	TIME [epoch: 10.3 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6344441952725306		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 0.6344441952725306 | validation: 1.0483279851021337]
	TIME [epoch: 10.3 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6431782560500093		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 0.6431782560500093 | validation: 1.036259461485101]
	TIME [epoch: 10.3 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6470246665955738		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 0.6470246665955738 | validation: 1.0295808105242037]
	TIME [epoch: 10.3 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6189541115715209		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 0.6189541115715209 | validation: 1.0583695453334119]
	TIME [epoch: 10.3 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6304589373481579		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 0.6304589373481579 | validation: 1.007990887652163]
	TIME [epoch: 10.3 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439849852051008		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 0.6439849852051008 | validation: 1.008969549735771]
	TIME [epoch: 10.3 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6449033726073322		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 0.6449033726073322 | validation: 1.038783153932153]
	TIME [epoch: 10.3 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6385466857643235		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 0.6385466857643235 | validation: 1.034750085832643]
	TIME [epoch: 10.3 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6635810192640147		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 0.6635810192640147 | validation: 1.0521897374783111]
	TIME [epoch: 10.3 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6323598282510987		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 0.6323598282510987 | validation: 1.0677857849126728]
	TIME [epoch: 10.3 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6328492776114174		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 0.6328492776114174 | validation: 1.0456795614584302]
	TIME [epoch: 10.3 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6449050269493106		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 0.6449050269493106 | validation: 1.0441976494456926]
	TIME [epoch: 10.3 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6313318937125232		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 0.6313318937125232 | validation: 1.0724907270373298]
	TIME [epoch: 10.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6372631643474269		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 0.6372631643474269 | validation: 1.0584201848283874]
	TIME [epoch: 10.3 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6400696424895924		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 0.6400696424895924 | validation: 1.0299056061186804]
	TIME [epoch: 10.3 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6402112668978532		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 0.6402112668978532 | validation: 1.0212824365114395]
	TIME [epoch: 10.3 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6323745857856387		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 0.6323745857856387 | validation: 1.034876466161685]
	TIME [epoch: 10.3 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6434025280738253		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 0.6434025280738253 | validation: 1.0473725322357186]
	TIME [epoch: 10.3 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6446981707694167		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 0.6446981707694167 | validation: 1.0456774940232811]
	TIME [epoch: 10.3 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6249301195558472		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 0.6249301195558472 | validation: 1.0351084982107421]
	TIME [epoch: 10.3 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6420743011625542		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 0.6420743011625542 | validation: 1.0362351096508826]
	TIME [epoch: 10.3 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6346184126741499		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 0.6346184126741499 | validation: 1.0692749345483412]
	TIME [epoch: 10.3 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6596522608497495		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 0.6596522608497495 | validation: 1.065077940502126]
	TIME [epoch: 10.3 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6225151843592664		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 0.6225151843592664 | validation: 1.0305072567708697]
	TIME [epoch: 10.3 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6207855103834552		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 0.6207855103834552 | validation: 1.029200275619567]
	TIME [epoch: 10.3 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6279404928108653		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 0.6279404928108653 | validation: 1.0287344749811012]
	TIME [epoch: 10.3 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6395526733414968		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 0.6395526733414968 | validation: 1.049063229167607]
	TIME [epoch: 10.3 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6406490747851035		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 0.6406490747851035 | validation: 0.9979467780918198]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1661.pth
	Model improved!!!
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6461508185262966		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 0.6461508185262966 | validation: 1.0166894430959281]
	TIME [epoch: 10.3 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6319474227605417		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 0.6319474227605417 | validation: 1.0389641622832766]
	TIME [epoch: 10.3 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6277021476744119		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 0.6277021476744119 | validation: 1.0880758097204]
	TIME [epoch: 10.3 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6334262187943273		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 0.6334262187943273 | validation: 1.0140882701757743]
	TIME [epoch: 10.3 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6462192374434285		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 0.6462192374434285 | validation: 0.996051536119438]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1666.pth
	Model improved!!!
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.639432922324161		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 0.639432922324161 | validation: 1.008116281909473]
	TIME [epoch: 10.3 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6415266145467713		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 0.6415266145467713 | validation: 1.0646241625088948]
	TIME [epoch: 10.3 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.631335242680833		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 0.631335242680833 | validation: 1.1131149776193718]
	TIME [epoch: 10.3 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6282833121195094		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 0.6282833121195094 | validation: 1.0179394980170715]
	TIME [epoch: 10.3 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6439007263824172		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 0.6439007263824172 | validation: 1.108695928947293]
	TIME [epoch: 10.3 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6437707193559256		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 0.6437707193559256 | validation: 1.063604561923764]
	TIME [epoch: 10.3 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6503143116581891		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 0.6503143116581891 | validation: 0.997607327587335]
	TIME [epoch: 10.3 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6290427481754925		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 0.6290427481754925 | validation: 1.0841312386601127]
	TIME [epoch: 10.3 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6523933846584169		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 0.6523933846584169 | validation: 1.0336927764603332]
	TIME [epoch: 10.3 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6435121055961315		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 0.6435121055961315 | validation: 1.061929448757566]
	TIME [epoch: 10.3 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6560709420540686		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 0.6560709420540686 | validation: 1.043139408049741]
	TIME [epoch: 10.3 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6399319809608371		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 0.6399319809608371 | validation: 1.0244410041833985]
	TIME [epoch: 10.3 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6442916363477972		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 0.6442916363477972 | validation: 1.0234397210293842]
	TIME [epoch: 10.3 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6306423893700144		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 0.6306423893700144 | validation: 1.030137489515124]
	TIME [epoch: 10.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.63289373259607		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 0.63289373259607 | validation: 1.0339871586218745]
	TIME [epoch: 10.3 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6511894667666935		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 0.6511894667666935 | validation: 1.0238628895790256]
	TIME [epoch: 10.3 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6476407820424868		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 0.6476407820424868 | validation: 1.0473218540613956]
	TIME [epoch: 10.3 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6358083170783468		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 0.6358083170783468 | validation: 1.0245437388488157]
	TIME [epoch: 10.3 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6333428892533983		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 0.6333428892533983 | validation: 1.0279308291255058]
	TIME [epoch: 10.3 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287348578363338		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 0.6287348578363338 | validation: 1.0397494905287896]
	TIME [epoch: 10.3 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6200527892907113		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 0.6200527892907113 | validation: 1.0405081518395523]
	TIME [epoch: 10.3 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6485087705930503		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 0.6485087705930503 | validation: 0.9826060248211892]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1688.pth
	Model improved!!!
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6342440083512796		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 0.6342440083512796 | validation: 1.0200307446813686]
	TIME [epoch: 10.3 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6392656066097412		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 0.6392656066097412 | validation: 1.0214980488609662]
	TIME [epoch: 10.3 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6240824465843969		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 0.6240824465843969 | validation: 1.031256555984083]
	TIME [epoch: 10.3 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6310041719350223		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 0.6310041719350223 | validation: 1.0349117347818482]
	TIME [epoch: 10.3 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6354733008922963		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 0.6354733008922963 | validation: 1.0142887557184639]
	TIME [epoch: 10.3 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6232242569658484		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 0.6232242569658484 | validation: 1.0578794324780338]
	TIME [epoch: 10.3 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6301479710141535		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 0.6301479710141535 | validation: 1.0314960303205933]
	TIME [epoch: 10.3 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6261144690510992		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 0.6261144690510992 | validation: 1.0353574082493373]
	TIME [epoch: 10.3 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.631843362232656		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 0.631843362232656 | validation: 1.0326302469295865]
	TIME [epoch: 10.3 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6322791997577544		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 0.6322791997577544 | validation: 1.1046261136951434]
	TIME [epoch: 10.3 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6291820044782119		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 0.6291820044782119 | validation: 1.0514945551683836]
	TIME [epoch: 10.3 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.637143619438244		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 0.637143619438244 | validation: 1.0409306629655781]
	TIME [epoch: 10.3 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6257712398424344		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 0.6257712398424344 | validation: 1.0431328622229268]
	TIME [epoch: 10.3 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6300811576231525		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 0.6300811576231525 | validation: 1.0269774217232308]
	TIME [epoch: 10.3 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6516981530856043		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 0.6516981530856043 | validation: 1.026865620626258]
	TIME [epoch: 10.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6280884759958585		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 0.6280884759958585 | validation: 1.0223813221934386]
	TIME [epoch: 10.3 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6155757733531653		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 0.6155757733531653 | validation: 1.027842896628869]
	TIME [epoch: 10.3 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6304966909768		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 0.6304966909768 | validation: 1.0487125677864966]
	TIME [epoch: 10.3 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6336669760231316		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 0.6336669760231316 | validation: 1.1054002945374182]
	TIME [epoch: 10.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6285550773509578		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 0.6285550773509578 | validation: 1.019202035026667]
	TIME [epoch: 10.3 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6416046948076504		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 0.6416046948076504 | validation: 1.0432074013087766]
	TIME [epoch: 10.3 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6523134386322522		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 0.6523134386322522 | validation: 1.0446370839031667]
	TIME [epoch: 10.3 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6239571527101536		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 0.6239571527101536 | validation: 1.0216322236626618]
	TIME [epoch: 10.3 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332218301680022		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 0.6332218301680022 | validation: 1.0420458097037493]
	TIME [epoch: 10.3 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332679670873395		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 0.6332679670873395 | validation: 1.0238020056512234]
	TIME [epoch: 10.3 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.646966352943328		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 0.646966352943328 | validation: 1.1070299566245752]
	TIME [epoch: 10.3 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6348808260155036		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 0.6348808260155036 | validation: 1.0120425975273295]
	TIME [epoch: 10.3 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6285169791243975		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 0.6285169791243975 | validation: 1.0518893296891012]
	TIME [epoch: 10.3 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6431797824092016		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 0.6431797824092016 | validation: 1.061992706449173]
	TIME [epoch: 10.3 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6232905685077751		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 0.6232905685077751 | validation: 1.028621941342096]
	TIME [epoch: 10.3 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6426262543398232		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 0.6426262543398232 | validation: 1.027233575603204]
	TIME [epoch: 10.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6191996635068209		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 0.6191996635068209 | validation: 0.9586593321281643]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1720.pth
	Model improved!!!
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287573815031815		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 0.6287573815031815 | validation: 1.0065038133842412]
	TIME [epoch: 10.3 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6299345622023247		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 0.6299345622023247 | validation: 1.0400873643471946]
	TIME [epoch: 10.3 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6413154225145488		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 0.6413154225145488 | validation: 1.0442953551286258]
	TIME [epoch: 10.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6447299733372654		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 0.6447299733372654 | validation: 1.0213547993125611]
	TIME [epoch: 10.3 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6392999958954715		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 0.6392999958954715 | validation: 1.0440949746239674]
	TIME [epoch: 10.3 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6421006364928813		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 0.6421006364928813 | validation: 1.054921111045394]
	TIME [epoch: 10.3 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6266008151476353		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 0.6266008151476353 | validation: 1.0211137865049613]
	TIME [epoch: 10.3 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.631662928195419		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 0.631662928195419 | validation: 1.0418135198343648]
	TIME [epoch: 10.3 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6221548254743834		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 0.6221548254743834 | validation: 1.0410400085790676]
	TIME [epoch: 10.3 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6207980082757001		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 0.6207980082757001 | validation: 1.0401251245792578]
	TIME [epoch: 10.3 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6278988237057337		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 0.6278988237057337 | validation: 1.0689943624457259]
	TIME [epoch: 10.3 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.644031884934076		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 0.644031884934076 | validation: 1.0460320061433093]
	TIME [epoch: 10.3 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6472496716166656		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 0.6472496716166656 | validation: 1.0495393785457672]
	TIME [epoch: 10.3 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6217999337810924		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 0.6217999337810924 | validation: 1.0368290561589766]
	TIME [epoch: 10.3 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6116008774387279		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 0.6116008774387279 | validation: 1.0245506383505767]
	TIME [epoch: 10.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6276254805038703		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 0.6276254805038703 | validation: 1.055248114255372]
	TIME [epoch: 10.3 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6164652924473126		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 0.6164652924473126 | validation: 1.0356544153431826]
	TIME [epoch: 10.3 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6392704026902614		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 0.6392704026902614 | validation: 1.0069300702457846]
	TIME [epoch: 10.3 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6374043290157017		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 0.6374043290157017 | validation: 1.029291285302474]
	TIME [epoch: 10.3 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6391348843821313		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 0.6391348843821313 | validation: 1.0366739161264094]
	TIME [epoch: 10.3 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6426129210095549		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 0.6426129210095549 | validation: 1.078964465856363]
	TIME [epoch: 10.3 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6306920768582216		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 0.6306920768582216 | validation: 1.0436071042188917]
	TIME [epoch: 10.3 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6329387382951783		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 0.6329387382951783 | validation: 1.0264533355060557]
	TIME [epoch: 10.3 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.629805803952868		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 0.629805803952868 | validation: 1.0593349713166345]
	TIME [epoch: 10.3 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6323453662396972		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 0.6323453662396972 | validation: 1.0202578682718073]
	TIME [epoch: 10.3 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6359015323098776		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 0.6359015323098776 | validation: 1.013576296979009]
	TIME [epoch: 10.3 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6157155377172439		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 0.6157155377172439 | validation: 1.002786758477601]
	TIME [epoch: 10.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6342611152680092		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 0.6342611152680092 | validation: 1.040638100281891]
	TIME [epoch: 10.3 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6357949155557172		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 0.6357949155557172 | validation: 1.0320288886844464]
	TIME [epoch: 10.3 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6213368749597861		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 0.6213368749597861 | validation: 1.0567755910386618]
	TIME [epoch: 10.3 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6226200497520846		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 0.6226200497520846 | validation: 1.0123687865884226]
	TIME [epoch: 10.3 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6349560039825014		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 0.6349560039825014 | validation: 0.9921304772106985]
	TIME [epoch: 10.3 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6394072736817903		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 0.6394072736817903 | validation: 1.0316235702616867]
	TIME [epoch: 10.3 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6506444521904908		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 0.6506444521904908 | validation: 0.9915331066240544]
	TIME [epoch: 10.3 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6353427194360868		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 0.6353427194360868 | validation: 1.1265948680082065]
	TIME [epoch: 10.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6464424519340696		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 0.6464424519340696 | validation: 1.037151169108777]
	TIME [epoch: 10.3 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6399279546852813		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 0.6399279546852813 | validation: 1.0276934599717316]
	TIME [epoch: 10.3 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6205449377109113		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 0.6205449377109113 | validation: 0.9796199038439831]
	TIME [epoch: 10.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287993785762891		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 0.6287993785762891 | validation: 1.0330125864053539]
	TIME [epoch: 10.3 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6300526918113384		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 0.6300526918113384 | validation: 1.0006963683609238]
	TIME [epoch: 10.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.627886376998387		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 0.627886376998387 | validation: 1.0174979261483044]
	TIME [epoch: 10.3 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.639148805829514		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 0.639148805829514 | validation: 1.026142293637721]
	TIME [epoch: 10.3 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6509354311923431		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 0.6509354311923431 | validation: 1.0304388787661285]
	TIME [epoch: 10.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332783747199768		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 0.6332783747199768 | validation: 1.0234775114713255]
	TIME [epoch: 10.3 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6286657534017536		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 0.6286657534017536 | validation: 1.0167371715980165]
	TIME [epoch: 10.3 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6153059114191063		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 0.6153059114191063 | validation: 1.0158133675539833]
	TIME [epoch: 10.3 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.627348223361096		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 0.627348223361096 | validation: 1.012324072133961]
	TIME [epoch: 10.3 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6223106241065431		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 0.6223106241065431 | validation: 1.0142957803637358]
	TIME [epoch: 10.3 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6323161757033937		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 0.6323161757033937 | validation: 1.0223533551420818]
	TIME [epoch: 10.3 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6265563194375766		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 0.6265563194375766 | validation: 1.020687107958864]
	TIME [epoch: 10.3 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6317313838075203		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 0.6317313838075203 | validation: 1.041044433141214]
	TIME [epoch: 10.3 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6403906196861409		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 0.6403906196861409 | validation: 1.0942968081261801]
	TIME [epoch: 10.3 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6423575649631883		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 0.6423575649631883 | validation: 1.0563533523910094]
	TIME [epoch: 10.3 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6319984516323596		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 0.6319984516323596 | validation: 1.033418367214787]
	TIME [epoch: 10.3 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6292907422062859		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 0.6292907422062859 | validation: 1.0489090509388388]
	TIME [epoch: 10.3 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6263517581595208		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 0.6263517581595208 | validation: 1.1164220166830907]
	TIME [epoch: 10.3 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6330866103015936		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 0.6330866103015936 | validation: 1.0321095046428275]
	TIME [epoch: 10.3 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6310344314993677		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 0.6310344314993677 | validation: 1.0443324537068197]
	TIME [epoch: 10.3 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6444757841335541		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 0.6444757841335541 | validation: 1.0317203595285975]
	TIME [epoch: 10.3 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.639019046308297		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 0.639019046308297 | validation: 1.089769036071647]
	TIME [epoch: 10.3 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6277309857600621		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 0.6277309857600621 | validation: 1.0330945927086734]
	TIME [epoch: 10.3 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6297914903658552		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 0.6297914903658552 | validation: 1.0312361434248014]
	TIME [epoch: 10.3 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.64145851358739		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 0.64145851358739 | validation: 1.1177184910061089]
	TIME [epoch: 10.3 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6306102027110659		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 0.6306102027110659 | validation: 1.0476636752489126]
	TIME [epoch: 10.3 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.624872119409116		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 0.624872119409116 | validation: 1.0318702175407979]
	TIME [epoch: 10.3 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6240772891053739		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 0.6240772891053739 | validation: 1.035947548140197]
	TIME [epoch: 10.3 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6275640520379369		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 0.6275640520379369 | validation: 1.0381632472652522]
	TIME [epoch: 10.3 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6445641958366112		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 0.6445641958366112 | validation: 1.0408347756854863]
	TIME [epoch: 10.3 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6292886553521555		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 0.6292886553521555 | validation: 1.0326150819112154]
	TIME [epoch: 10.3 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.639424924439071		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 0.639424924439071 | validation: 1.1108443561325427]
	TIME [epoch: 10.3 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6277460378013895		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 0.6277460378013895 | validation: 1.0461664888474085]
	TIME [epoch: 10.3 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6303481201476365		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 0.6303481201476365 | validation: 1.02860670996512]
	TIME [epoch: 10.3 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6309056584050381		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 0.6309056584050381 | validation: 1.0464385973330412]
	TIME [epoch: 10.3 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6417783165354967		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 0.6417783165354967 | validation: 1.0259939314721198]
	TIME [epoch: 10.3 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.629166620732782		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 0.629166620732782 | validation: 1.053426900639835]
	TIME [epoch: 10.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6175819537399068		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 0.6175819537399068 | validation: 1.123682969321108]
	TIME [epoch: 10.3 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6269391511086633		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 0.6269391511086633 | validation: 1.1124497178440012]
	TIME [epoch: 10.3 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6429977272870956		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 0.6429977272870956 | validation: 1.1153800004234022]
	TIME [epoch: 10.3 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6201967826568159		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 0.6201967826568159 | validation: 1.0076274320917022]
	TIME [epoch: 10.3 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6187378114569922		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 0.6187378114569922 | validation: 1.0615797594032006]
	TIME [epoch: 10.3 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6263804774093764		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 0.6263804774093764 | validation: 1.0274866916767063]
	TIME [epoch: 10.3 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6332967779889433		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 0.6332967779889433 | validation: 1.0574503730546179]
	TIME [epoch: 10.3 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6247387696859797		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 0.6247387696859797 | validation: 1.0581300491462298]
	TIME [epoch: 10.3 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6235928447354875		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 0.6235928447354875 | validation: 1.0433567315901981]
	TIME [epoch: 10.3 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.625830053782254		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 0.625830053782254 | validation: 0.9574146349373431]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r2_20240219_233643/states/model_tr_study206_1805.pth
	Model improved!!!
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6198382666747764		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 0.6198382666747764 | validation: 1.0523856756634786]
	TIME [epoch: 10.3 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6218056691243433		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 0.6218056691243433 | validation: 1.0547666094357242]
	TIME [epoch: 10.3 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6235321634821133		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 0.6235321634821133 | validation: 1.0948602358959385]
	TIME [epoch: 10.3 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287882695379358		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 0.6287882695379358 | validation: 1.0409845695026203]
	TIME [epoch: 10.3 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6316065228813186		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 0.6316065228813186 | validation: 1.0478948698625303]
	TIME [epoch: 10.3 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6321305716023786		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 0.6321305716023786 | validation: 1.063910916927822]
	TIME [epoch: 10.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6191060818378937		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 0.6191060818378937 | validation: 1.012039416528989]
	TIME [epoch: 10.3 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6127051805400716		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 0.6127051805400716 | validation: 1.0461978330520376]
	TIME [epoch: 10.3 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287719575513986		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 0.6287719575513986 | validation: 1.0510679471111015]
	TIME [epoch: 10.3 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6301385659850354		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 0.6301385659850354 | validation: 1.0265811411681687]
	TIME [epoch: 10.3 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6264525971092217		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 0.6264525971092217 | validation: 1.0566261341282617]
	TIME [epoch: 10.3 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6309877108965248		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 0.6309877108965248 | validation: 1.02195603498093]
	TIME [epoch: 10.3 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6280018263541918		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 0.6280018263541918 | validation: 1.046472802292739]
	TIME [epoch: 10.3 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6253023850847614		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 0.6253023850847614 | validation: 1.0452855840422113]
	TIME [epoch: 10.3 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6292802587567927		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 0.6292802587567927 | validation: 1.0301820512318052]
	TIME [epoch: 10.3 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6184968219728141		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 0.6184968219728141 | validation: 1.0854226040024022]
	TIME [epoch: 10.3 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6268910147350979		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 0.6268910147350979 | validation: 1.0114657180747468]
	TIME [epoch: 10.3 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6336896212657367		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 0.6336896212657367 | validation: 1.0455148738872575]
	TIME [epoch: 10.3 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6257110983890508		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 0.6257110983890508 | validation: 1.0605716080649383]
	TIME [epoch: 10.3 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.622041048767827		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 0.622041048767827 | validation: 1.100303264840107]
	TIME [epoch: 10.3 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6317090165091332		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 0.6317090165091332 | validation: 1.0273373920079063]
	TIME [epoch: 10.3 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6377332287747189		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 0.6377332287747189 | validation: 1.0408956074291276]
	TIME [epoch: 10.3 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6460509611113535		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 0.6460509611113535 | validation: 1.0052479450570702]
	TIME [epoch: 10.3 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6178863321521182		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 0.6178863321521182 | validation: 1.0306502856076292]
	TIME [epoch: 10.3 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6244957699336583		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 0.6244957699336583 | validation: 1.0945105670238544]
	TIME [epoch: 10.3 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6309942674479178		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 0.6309942674479178 | validation: 1.0431216321355907]
	TIME [epoch: 10.3 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6364755978990286		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 0.6364755978990286 | validation: 0.9951680316100674]
	TIME [epoch: 10.3 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6172217251903142		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 0.6172217251903142 | validation: 1.0460121683212311]
	TIME [epoch: 10.3 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.648499375014136		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 0.648499375014136 | validation: 1.0364168905931046]
	TIME [epoch: 10.3 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6441286381346042		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 0.6441286381346042 | validation: 1.0379891125429874]
	TIME [epoch: 10.3 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6325662858078049		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 0.6325662858078049 | validation: 1.0009013548233545]
	TIME [epoch: 10.3 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6237047195590151		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 0.6237047195590151 | validation: 1.0679710354027763]
	TIME [epoch: 10.3 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6165274207779714		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 0.6165274207779714 | validation: 1.0226130417812918]
	TIME [epoch: 10.3 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6296149543626257		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 0.6296149543626257 | validation: 1.024744966348108]
	TIME [epoch: 10.3 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.624794853751388		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 0.624794853751388 | validation: 1.084611526980392]
	TIME [epoch: 10.3 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6293399002437312		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 0.6293399002437312 | validation: 1.1149735927730529]
	TIME [epoch: 10.3 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6369381538678377		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 0.6369381538678377 | validation: 1.00427720812161]
	TIME [epoch: 10.3 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6193046023864627		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 0.6193046023864627 | validation: 1.0211662817919094]
	TIME [epoch: 10.3 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6177145517621699		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 0.6177145517621699 | validation: 1.0370629413229635]
	TIME [epoch: 10.3 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6358913099072646		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 0.6358913099072646 | validation: 1.0631368647480601]
	TIME [epoch: 10.3 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6324796213902887		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 0.6324796213902887 | validation: 1.0130117647874903]
	TIME [epoch: 10.3 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6323787593384769		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 0.6323787593384769 | validation: 1.0138063765912642]
	TIME [epoch: 10.3 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6326988622324619		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 0.6326988622324619 | validation: 1.023280528454118]
	TIME [epoch: 10.3 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6293217071350556		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 0.6293217071350556 | validation: 1.0142362368616065]
	TIME [epoch: 10.3 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6252452938344517		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 0.6252452938344517 | validation: 0.9848506080886658]
	TIME [epoch: 10.3 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6231469877092047		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 0.6231469877092047 | validation: 1.0155023811529216]
	TIME [epoch: 10.3 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.617934972542681		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 0.617934972542681 | validation: 1.062777953239158]
	TIME [epoch: 10.3 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6311498885402722		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 0.6311498885402722 | validation: 1.0382184306343347]
	TIME [epoch: 10.3 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6311218047894922		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 0.6311218047894922 | validation: 1.0467588638280612]
	TIME [epoch: 10.3 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6248272002554487		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 0.6248272002554487 | validation: 1.0287503857543998]
	TIME [epoch: 10.3 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.638526164056948		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 0.638526164056948 | validation: 1.0126387043256981]
	TIME [epoch: 10.3 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6317896311233381		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 0.6317896311233381 | validation: 1.0086870479908883]
	TIME [epoch: 10.3 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6291050226529591		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 0.6291050226529591 | validation: 1.0391588661684357]
	TIME [epoch: 10.3 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6384755444783079		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 0.6384755444783079 | validation: 1.015383803055394]
	TIME [epoch: 10.3 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6276715603065476		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 0.6276715603065476 | validation: 0.9973407012047213]
	TIME [epoch: 10.3 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6285529431001644		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 0.6285529431001644 | validation: 1.0184919686051084]
	TIME [epoch: 10.3 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6274423051741526		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 0.6274423051741526 | validation: 1.0198659998350903]
	TIME [epoch: 10.3 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6303674477158903		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 0.6303674477158903 | validation: 1.0226710145240814]
	TIME [epoch: 10.3 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6223667074238187		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 0.6223667074238187 | validation: 1.0190022664365666]
	TIME [epoch: 10.3 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6490624776533485		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 0.6490624776533485 | validation: 1.060615070166936]
	TIME [epoch: 10.3 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6378379817744102		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 0.6378379817744102 | validation: 0.9903951777934991]
	TIME [epoch: 10.3 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6312971065037282		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 0.6312971065037282 | validation: 1.0404291231196123]
	TIME [epoch: 10.3 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6200501928398771		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 0.6200501928398771 | validation: 1.0159946555393549]
	TIME [epoch: 10.3 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6257810471378019		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 0.6257810471378019 | validation: 1.0703176193980457]
	TIME [epoch: 10.3 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6138191075431523		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 0.6138191075431523 | validation: 1.0249003325849106]
	TIME [epoch: 10.3 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6168889467008778		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 0.6168889467008778 | validation: 1.0119492158081824]
	TIME [epoch: 10.3 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6426420731971628		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 0.6426420731971628 | validation: 1.0080528834128537]
	TIME [epoch: 10.3 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6047751432780866		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 0.6047751432780866 | validation: 1.0595345576819424]
	TIME [epoch: 10.3 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6377189599440556		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 0.6377189599440556 | validation: 1.0244224573350422]
	TIME [epoch: 10.3 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6214663304748619		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 0.6214663304748619 | validation: 1.0429098740470597]
	TIME [epoch: 10.3 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6180444639783651		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 0.6180444639783651 | validation: 1.0346733691343684]
	TIME [epoch: 10.3 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6147617651237585		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 0.6147617651237585 | validation: 1.0266508259954583]
	TIME [epoch: 10.3 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6234848711830491		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 0.6234848711830491 | validation: 1.050424089817046]
	TIME [epoch: 10.3 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.642100669820962		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 0.642100669820962 | validation: 1.034231580036964]
	TIME [epoch: 10.3 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.615014455113663		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 0.615014455113663 | validation: 1.0226953927896962]
	TIME [epoch: 10.3 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6299862597092074		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 0.6299862597092074 | validation: 1.1143904622237948]
	TIME [epoch: 10.3 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6091164359249633		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 0.6091164359249633 | validation: 1.0377565067479373]
	TIME [epoch: 10.3 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6108866685771716		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 0.6108866685771716 | validation: 1.0351386005884748]
	TIME [epoch: 10.3 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6287332733755872		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 0.6287332733755872 | validation: 1.0352484011722616]
	TIME [epoch: 10.3 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6117846220887732		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 0.6117846220887732 | validation: 1.0146504759422093]
	TIME [epoch: 10.3 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6333230517274142		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 0.6333230517274142 | validation: 1.0053496343314503]
	TIME [epoch: 10.3 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6347130848131773		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 0.6347130848131773 | validation: 1.1051262001213136]
	TIME [epoch: 10.3 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.618513828292922		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 0.618513828292922 | validation: 1.0242055700246888]
	TIME [epoch: 10.3 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6251308052787602		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 0.6251308052787602 | validation: 0.9832686242599326]
	TIME [epoch: 10.3 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.624237236932275		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 0.624237236932275 | validation: 1.0447945050319472]
	TIME [epoch: 10.3 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.625003193387308		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 0.625003193387308 | validation: 1.0261197407078084]
	TIME [epoch: 10.3 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6177143791697738		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 0.6177143791697738 | validation: 1.1101485933540491]
	TIME [epoch: 10.3 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6209601780070897		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 0.6209601780070897 | validation: 1.032506418601185]
	TIME [epoch: 10.3 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.617876523375281		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 0.617876523375281 | validation: 1.0198215815828369]
	TIME [epoch: 10.3 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6197942900660414		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 0.6197942900660414 | validation: 1.0109470156259552]
	TIME [epoch: 10.3 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6387999954283808		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 0.6387999954283808 | validation: 1.0342501357931635]
	TIME [epoch: 10.3 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6239578571873661		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 0.6239578571873661 | validation: 1.0376108473576167]
	TIME [epoch: 10.3 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6224222909151289		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 0.6224222909151289 | validation: 1.0081717576711864]
	TIME [epoch: 10.3 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6211305147916106		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 0.6211305147916106 | validation: 1.03234533089567]
	TIME [epoch: 10.3 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6104574359399123		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 0.6104574359399123 | validation: 1.0261430933303215]
	TIME [epoch: 10.3 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6123909225551223		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 0.6123909225551223 | validation: 1.000744519221569]
	TIME [epoch: 10.3 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6207023194210116		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 0.6207023194210116 | validation: 1.0348328134237554]
	TIME [epoch: 10.3 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6364659066084603		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 0.6364659066084603 | validation: 1.003909718647293]
	TIME [epoch: 10.3 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.613396066956318		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 0.613396066956318 | validation: 1.0150572828511755]
	TIME [epoch: 10.3 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6408194986406067		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 0.6408194986406067 | validation: 1.0051551885934569]
	TIME [epoch: 10.3 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6161984048406843		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 0.6161984048406843 | validation: 1.0499790710224313]
	TIME [epoch: 10.3 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6197816947757604		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 0.6197816947757604 | validation: 0.988552179434666]
	TIME [epoch: 10.3 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6256815418742925		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 0.6256815418742925 | validation: 1.0146030301275786]
	TIME [epoch: 10.3 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6331623114113403		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 0.6331623114113403 | validation: 1.0020401558776295]
	TIME [epoch: 10.3 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6175459094935583		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 0.6175459094935583 | validation: 1.0426356384035889]
	TIME [epoch: 10.3 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6432563871945676		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 0.6432563871945676 | validation: 1.0345711743845996]
	TIME [epoch: 10.3 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6457560040528757		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 0.6457560040528757 | validation: 1.0084070025532734]
	TIME [epoch: 10.3 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.635315474234553		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 0.635315474234553 | validation: 1.0334520257828441]
	TIME [epoch: 10.3 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6104301473666196		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 0.6104301473666196 | validation: 1.0064369560181194]
	TIME [epoch: 10.3 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6129414763892066		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 0.6129414763892066 | validation: 1.0532340473012811]
	TIME [epoch: 10.3 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6208278847480058		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 0.6208278847480058 | validation: 1.013203940164473]
	TIME [epoch: 10.3 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6276591343360554		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 0.6276591343360554 | validation: 1.0273099843375062]
	TIME [epoch: 10.3 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6188117139074204		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 0.6188117139074204 | validation: 1.0471200775439915]
	TIME [epoch: 10.3 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6189930136720254		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 0.6189930136720254 | validation: 1.0338904412252818]
	TIME [epoch: 10.3 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6120315696141643		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 0.6120315696141643 | validation: 1.056438228270433]
	TIME [epoch: 10.3 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6237015093162707		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 0.6237015093162707 | validation: 1.008250776270894]
	TIME [epoch: 10.3 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.613763846336351		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 0.613763846336351 | validation: 1.0065292766744771]
	TIME [epoch: 10.3 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6218573317456256		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 0.6218573317456256 | validation: 1.03208380840471]
	TIME [epoch: 10.3 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6260641232371762		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 0.6260641232371762 | validation: 0.9968907130738359]
	TIME [epoch: 10.3 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6142541471131444		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 0.6142541471131444 | validation: 1.0031713403501372]
	TIME [epoch: 10.3 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.624645225632474		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 0.624645225632474 | validation: 1.0618782060638576]
	TIME [epoch: 10.3 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6232540043139793		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 0.6232540043139793 | validation: 0.9968088641087038]
	TIME [epoch: 10.3 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6096923175604658		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 0.6096923175604658 | validation: 1.007149310857626]
	TIME [epoch: 10.3 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6118883571773222		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 0.6118883571773222 | validation: 1.0288967568663505]
	TIME [epoch: 10.3 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6174501816032926		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 0.6174501816032926 | validation: 1.0880066962593855]
	TIME [epoch: 10.3 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6185699524695123		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 0.6185699524695123 | validation: 1.0440060152964779]
	TIME [epoch: 10.3 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6160047168360584		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 0.6160047168360584 | validation: 0.9994952728742726]
	TIME [epoch: 10.3 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6238850992156048		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 0.6238850992156048 | validation: 1.0379849645937425]
	TIME [epoch: 10.3 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6152431265357736		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 0.6152431265357736 | validation: 1.0223844213431899]
	TIME [epoch: 10.3 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6176182200918989		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 0.6176182200918989 | validation: 0.998500603137969]
	TIME [epoch: 10.3 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6252323353639984		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 0.6252323353639984 | validation: 1.0101270217319551]
	TIME [epoch: 10.3 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6148007263668082		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 0.6148007263668082 | validation: 1.0092192995602045]
	TIME [epoch: 10.3 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6210331826696146		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 0.6210331826696146 | validation: 1.077109697571881]
	TIME [epoch: 10.3 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6252234543742932		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 0.6252234543742932 | validation: 1.0234504236631659]
	TIME [epoch: 10.3 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.627463601535827		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 0.627463601535827 | validation: 1.0327821420277044]
	TIME [epoch: 10.3 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6236228266467557		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 0.6236228266467557 | validation: 1.0056010663698631]
	TIME [epoch: 10.3 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150076090117791		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 0.6150076090117791 | validation: 1.0616514339891037]
	TIME [epoch: 10.3 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6227383657557148		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 0.6227383657557148 | validation: 1.020619129163828]
	TIME [epoch: 10.3 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6115624631410479		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 0.6115624631410479 | validation: 1.0475106345797796]
	TIME [epoch: 10.3 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6324515293372093		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 0.6324515293372093 | validation: 1.0349779015477174]
	TIME [epoch: 10.3 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.622166111909267		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 0.622166111909267 | validation: 1.0486474886651103]
	TIME [epoch: 10.3 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6176259468148378		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 0.6176259468148378 | validation: 1.0533556653131269]
	TIME [epoch: 10.3 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6193957114416209		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 0.6193957114416209 | validation: 1.0060452616972018]
	TIME [epoch: 10.3 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6201947862139573		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 0.6201947862139573 | validation: 1.0119702690676018]
	TIME [epoch: 10.3 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6288167841049864		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 0.6288167841049864 | validation: 1.0580050747612253]
	TIME [epoch: 10.3 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6222700817946387		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 0.6222700817946387 | validation: 1.0512230385293828]
	TIME [epoch: 10.3 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6231515811098344		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 0.6231515811098344 | validation: 1.0280700375320573]
	TIME [epoch: 10.3 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6184453781467449		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 0.6184453781467449 | validation: 1.023922036771355]
	TIME [epoch: 10.3 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6255503068329331		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 0.6255503068329331 | validation: 1.0395204623057908]
	TIME [epoch: 10.3 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6295515598308195		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 0.6295515598308195 | validation: 1.017966587993159]
	TIME [epoch: 10.3 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6357828243362773		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 0.6357828243362773 | validation: 1.0263686275364754]
	TIME [epoch: 10.3 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6298370087856815		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 0.6298370087856815 | validation: 1.0698152901913256]
	TIME [epoch: 10.3 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6349076549095358		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 0.6349076549095358 | validation: 1.0395658238919927]
	TIME [epoch: 10.3 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6150158247955604		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 0.6150158247955604 | validation: 1.0232870837695982]
	TIME [epoch: 10.3 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6408746849907486		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 0.6408746849907486 | validation: 1.007188173136417]
	TIME [epoch: 10.3 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6219957434799983		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 0.6219957434799983 | validation: 1.0257525860634196]
	TIME [epoch: 10.3 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6206651106476746		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 0.6206651106476746 | validation: 0.9908028798916431]
	TIME [epoch: 10.3 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6309933865990988		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 0.6309933865990988 | validation: 1.0416595817391312]
	TIME [epoch: 10.3 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6290034854741788		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 0.6290034854741788 | validation: 1.051134651177264]
	TIME [epoch: 10.3 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6240292545326411		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 0.6240292545326411 | validation: 1.0131719668152466]
	TIME [epoch: 10.3 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6301416105404855		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 0.6301416105404855 | validation: 1.0760912066890047]
	TIME [epoch: 10.3 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6326689517594194		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 0.6326689517594194 | validation: 1.0379090051245876]
	TIME [epoch: 10.3 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6193524872034912		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 0.6193524872034912 | validation: 1.0175931177789734]
	TIME [epoch: 10.3 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6389302350494693		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 0.6389302350494693 | validation: 1.0373439240808537]
	TIME [epoch: 10.3 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6249008738622355		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 0.6249008738622355 | validation: 1.0336696492527877]
	TIME [epoch: 10.3 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6238774737040096		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 0.6238774737040096 | validation: 1.0468351780885379]
	TIME [epoch: 10.3 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6093620734232921		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 0.6093620734232921 | validation: 1.028432002453584]
	TIME [epoch: 10.3 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6355510959162438		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 0.6355510959162438 | validation: 1.001829552529423]
	TIME [epoch: 10.3 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6202595484889594		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 0.6202595484889594 | validation: 1.0234131433774036]
	TIME [epoch: 10.3 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6228063217405767		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 0.6228063217405767 | validation: 1.021083753146311]
	TIME [epoch: 10.3 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6136457728201565		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 0.6136457728201565 | validation: 1.0114463180249782]
	TIME [epoch: 10.3 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6292569498121793		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 0.6292569498121793 | validation: 1.0027487778918045]
	TIME [epoch: 10.3 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6494344407450441		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 0.6494344407450441 | validation: 1.0221973796845492]
	TIME [epoch: 10.3 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6060163920960344		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 0.6060163920960344 | validation: 1.0318506099444422]
	TIME [epoch: 10.3 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6209424856050731		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 0.6209424856050731 | validation: 1.0413178474372922]
	TIME [epoch: 10.3 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.628563287470689		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 0.628563287470689 | validation: 1.0448080580573733]
	TIME [epoch: 10.3 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6156239952721275		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 0.6156239952721275 | validation: 1.0391735739115593]
	TIME [epoch: 10.3 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6083597161187237		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 0.6083597161187237 | validation: 1.0687365576990866]
	TIME [epoch: 10.3 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6348147728997907		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 0.6348147728997907 | validation: 1.0022896711880664]
	TIME [epoch: 10.3 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6197304801722945		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 0.6197304801722945 | validation: 1.035270495551864]
	TIME [epoch: 10.3 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6224365838088985		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 0.6224365838088985 | validation: 1.0237387039991617]
	TIME [epoch: 10.3 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6297235932695262		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 0.6297235932695262 | validation: 1.0073545660170378]
	TIME [epoch: 10.3 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6145813671773283		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 0.6145813671773283 | validation: 1.0047548486978932]
	TIME [epoch: 10.3 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.624803331341547		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 0.624803331341547 | validation: 1.0292939906502894]
	TIME [epoch: 10.3 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6144639250204741		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 0.6144639250204741 | validation: 1.0003703591914124]
	TIME [epoch: 10.3 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6170526677867112		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 0.6170526677867112 | validation: 1.022051162931826]
	TIME [epoch: 10.3 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6271032500124101		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 0.6271032500124101 | validation: 1.0191139910491827]
	TIME [epoch: 10.3 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6243990365124323		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 0.6243990365124323 | validation: 1.0338908869995538]
	TIME [epoch: 10.3 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6220708377870563		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 0.6220708377870563 | validation: 1.0303141338774224]
	TIME [epoch: 10.3 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6157529529449601		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 0.6157529529449601 | validation: 1.0029300430548598]
	TIME [epoch: 10.3 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6320662698414272		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 0.6320662698414272 | validation: 1.1110307825068002]
	TIME [epoch: 10.3 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6027868214323545		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 0.6027868214323545 | validation: 1.0038298318234116]
	TIME [epoch: 10.3 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6216518460220706		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 0.6216518460220706 | validation: 1.014723322769567]
	TIME [epoch: 10.3 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6374651711779852		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 0.6374651711779852 | validation: 1.0083062988425773]
	TIME [epoch: 10.3 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 0.6217461235119428		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 0.6217461235119428 | validation: 0.9950902981326649]
	TIME [epoch: 10.3 sec]
Finished training in 20726.686 seconds.
