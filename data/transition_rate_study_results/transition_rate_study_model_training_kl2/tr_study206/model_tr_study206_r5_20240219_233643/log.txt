Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r5', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r5', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1632532355

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.427049117800843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.427049117800843 | validation: 11.111433944595037]
	TIME [epoch: 49.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.250609437227128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.250609437227128 | validation: 10.927023063415655]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.833891534175883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.833891534175883 | validation: 9.607953524813581]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.185703713072277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.185703713072277 | validation: 10.27283714337085]
	TIME [epoch: 10.4 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.779534132082937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.779534132082937 | validation: 9.420575401311432]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.36056551410713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.36056551410713 | validation: 7.657602699367114]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.077498320641489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.077498320641489 | validation: 7.063888561761146]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.408044232226365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.408044232226365 | validation: 5.5274371901846076]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.206387441400851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.206387441400851 | validation: 7.1934328367290306]
	TIME [epoch: 10.4 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.563208212962783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.563208212962783 | validation: 5.175662247774662]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.315289236923016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.315289236923016 | validation: 4.97530891210172]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.623797137020067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.623797137020067 | validation: 8.41138735435376]
	TIME [epoch: 10.4 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.7369349182560745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7369349182560745 | validation: 5.866405492561669]
	TIME [epoch: 10.4 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.810852346776747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.810852346776747 | validation: 5.014670457264106]
	TIME [epoch: 10.4 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1140394739712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1140394739712 | validation: 4.639053582849129]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.334255427128492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.334255427128492 | validation: 6.956668791040441]
	TIME [epoch: 10.4 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.341532727398404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.341532727398404 | validation: 4.727214760134439]
	TIME [epoch: 10.4 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.207799872468208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.207799872468208 | validation: 5.43821839710595]
	TIME [epoch: 10.4 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.388782088678487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.388782088678487 | validation: 9.184732165474696]
	TIME [epoch: 10.4 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.731277061497606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.731277061497606 | validation: 4.814340429946111]
	TIME [epoch: 10.4 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.631477736472012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.631477736472012 | validation: 4.715804307371824]
	TIME [epoch: 10.4 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.101176402924407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.101176402924407 | validation: 4.777463051533702]
	TIME [epoch: 10.4 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.098858634617747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.098858634617747 | validation: 4.56107799805068]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.129719189315294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.129719189315294 | validation: 4.725707643634309]
	TIME [epoch: 10.4 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.183985124571748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.183985124571748 | validation: 4.629912971141039]
	TIME [epoch: 10.4 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9342331793783627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9342331793783627 | validation: 4.3543351494797875]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.2330446030922815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.2330446030922815 | validation: 4.881281464872782]
	TIME [epoch: 10.4 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.939236821701786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.939236821701786 | validation: 6.61043121601738]
	TIME [epoch: 10.4 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.603789413818172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.603789413818172 | validation: 4.387577057126516]
	TIME [epoch: 10.4 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8967435570236786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8967435570236786 | validation: 4.344688869759877]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.627504598478983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.627504598478983 | validation: 4.533992188614968]
	TIME [epoch: 10.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7201450064301262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7201450064301262 | validation: 4.19694776223738]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492515655850491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.492515655850491 | validation: 6.327658250407351]
	TIME [epoch: 10.5 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.261190616645431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.261190616645431 | validation: 4.067262843677201]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.616765874209361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.616765874209361 | validation: 4.000945163973053]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7246284098207667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7246284098207667 | validation: 4.040706373187223]
	TIME [epoch: 10.4 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.2594084219496344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2594084219496344 | validation: 4.304720119885486]
	TIME [epoch: 10.4 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.122908438647225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.122908438647225 | validation: 4.012806554651377]
	TIME [epoch: 10.4 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.228764637106648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.228764637106648 | validation: 4.548312233666279]
	TIME [epoch: 10.7 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.0559157872250946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0559157872250946 | validation: 3.972388994654672]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.251837699918707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.251837699918707 | validation: 4.220706678125074]
	TIME [epoch: 10.4 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.013267284287445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.013267284287445 | validation: 3.5900808660591426]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.712325963466613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.712325963466613 | validation: 4.027678596867289]
	TIME [epoch: 10.4 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9641720320164913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9641720320164913 | validation: 3.368906886801666]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9778192513197745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9778192513197745 | validation: 3.420502117315192]
	TIME [epoch: 10.4 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7057569882448016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7057569882448016 | validation: 3.977636104272126]
	TIME [epoch: 10.4 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7243594196332572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7243594196332572 | validation: 3.4425042171853635]
	TIME [epoch: 10.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.7923517004702996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7923517004702996 | validation: 3.7581586282729313]
	TIME [epoch: 10.4 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5235601852407656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5235601852407656 | validation: 3.319356315289726]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.570322252157143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.570322252157143 | validation: 3.526623463999397]
	TIME [epoch: 10.4 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4237111269772678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4237111269772678 | validation: 3.399144222415747]
	TIME [epoch: 10.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.463057032100582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.463057032100582 | validation: 3.668474735307041]
	TIME [epoch: 10.4 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.8558269492002863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8558269492002863 | validation: 3.2468945505638103]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5059634719363753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5059634719363753 | validation: 3.398617953955538]
	TIME [epoch: 10.4 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.495368713293965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.495368713293965 | validation: 3.674533172790267]
	TIME [epoch: 10.4 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3578629112999883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3578629112999883 | validation: 3.3322291381532034]
	TIME [epoch: 10.4 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.9502367378380305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9502367378380305 | validation: 3.188016818914377]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.315082919563577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.315082919563577 | validation: 3.7242440870631324]
	TIME [epoch: 10.4 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23081912776478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.23081912776478 | validation: 4.040101438069831]
	TIME [epoch: 10.4 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.555246194585323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.555246194585323 | validation: 3.2336069782271086]
	TIME [epoch: 10.4 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4444993713363985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4444993713363985 | validation: 3.382068436526503]
	TIME [epoch: 10.4 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.443400994052796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.443400994052796 | validation: 4.58116494537859]
	TIME [epoch: 10.4 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.4948366491590455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4948366491590455 | validation: 3.045261000470092]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3709911590591295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3709911590591295 | validation: 3.349806281121112]
	TIME [epoch: 10.4 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1227517770355364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1227517770355364 | validation: 3.818622350259238]
	TIME [epoch: 10.4 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.230263036480356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.230263036480356 | validation: 3.571864461915993]
	TIME [epoch: 10.4 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2553568493818053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2553568493818053 | validation: 3.376700009073383]
	TIME [epoch: 10.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3076405115977714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3076405115977714 | validation: 3.0116257662149897]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6543586054895214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6543586054895214 | validation: 3.470304078493896]
	TIME [epoch: 10.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.213254359891638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.213254359891638 | validation: 3.3128051297008096]
	TIME [epoch: 10.4 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2309639451278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2309639451278 | validation: 3.0562079527535126]
	TIME [epoch: 10.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0506885114837536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0506885114837536 | validation: 3.4424800635368435]
	TIME [epoch: 10.4 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2477651510952588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2477651510952588 | validation: 3.1949215790909635]
	TIME [epoch: 10.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.461966914477402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.461966914477402 | validation: 3.7993333368551374]
	TIME [epoch: 10.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0943978586141876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0943978586141876 | validation: 3.1205579590313937]
	TIME [epoch: 10.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.115155517205726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.115155517205726 | validation: 4.246321953200609]
	TIME [epoch: 10.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5055903822281684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5055903822281684 | validation: 3.1250773182925786]
	TIME [epoch: 10.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1610357351595164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1610357351595164 | validation: 2.927759505725253]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.15447414778816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.15447414778816 | validation: 3.2373045842244466]
	TIME [epoch: 10.4 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0716374664664245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0716374664664245 | validation: 4.360401150563259]
	TIME [epoch: 10.4 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2767685664542427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2767685664542427 | validation: 2.9876820731335547]
	TIME [epoch: 10.4 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.5737701296853337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5737701296853337 | validation: 2.983977356512411]
	TIME [epoch: 10.4 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.341862010128078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.341862010128078 | validation: 3.0895980524081246]
	TIME [epoch: 10.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0788230219530774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0788230219530774 | validation: 3.158881377628236]
	TIME [epoch: 10.4 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1847944747147774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1847944747147774 | validation: 2.916636338933082]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2518962119162613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2518962119162613 | validation: 3.071357364306606]
	TIME [epoch: 10.4 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.049018804569503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.049018804569503 | validation: 2.921254637521684]
	TIME [epoch: 10.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9355834104424356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9355834104424356 | validation: 3.4827359805775986]
	TIME [epoch: 10.4 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.237416973370697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.237416973370697 | validation: 3.071929412100199]
	TIME [epoch: 10.4 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2272274192934356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2272274192934356 | validation: 2.9431649589612183]
	TIME [epoch: 10.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.099066167747274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.099066167747274 | validation: 3.626922737845946]
	TIME [epoch: 10.4 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1842269440620106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1842269440620106 | validation: 3.08173668381781]
	TIME [epoch: 10.4 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.25156684888641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.25156684888641 | validation: 2.928555360393467]
	TIME [epoch: 10.4 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0393360026341694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0393360026341694 | validation: 2.9588491985436343]
	TIME [epoch: 10.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.010671865903718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.010671865903718 | validation: 3.1744278350720765]
	TIME [epoch: 10.4 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1119268902443395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1119268902443395 | validation: 2.9309335436462907]
	TIME [epoch: 10.4 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.3615077968839984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3615077968839984 | validation: 3.3726678380055035]
	TIME [epoch: 10.4 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.23844459184106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.23844459184106 | validation: 2.878823469899344]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9753631692376203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9753631692376203 | validation: 3.19006156614499]
	TIME [epoch: 10.4 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0715689695706745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0715689695706745 | validation: 2.8712419255062005]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8606978360680384		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 1.8606978360680384 | validation: 3.000615486442125]
	TIME [epoch: 10.4 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8763650322106362		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 1.8763650322106362 | validation: 3.486735846500983]
	TIME [epoch: 10.4 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0876916817614375		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 2.0876916817614375 | validation: 3.434696732033894]
	TIME [epoch: 10.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.036941168757397		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 2.036941168757397 | validation: 2.9893313927872276]
	TIME [epoch: 10.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9338081319945821		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 1.9338081319945821 | validation: 4.927660871783112]
	TIME [epoch: 10.4 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.548134030125559		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 2.548134030125559 | validation: 3.2691340713113783]
	TIME [epoch: 10.4 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9640710995525787		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 1.9640710995525787 | validation: 3.2282873834094965]
	TIME [epoch: 10.4 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8203822300548018		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 1.8203822300548018 | validation: 3.091013490790373]
	TIME [epoch: 10.4 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8393769655759704		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 1.8393769655759704 | validation: 2.9326734504383296]
	TIME [epoch: 10.4 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8802347564833521		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 1.8802347564833521 | validation: 4.38977575263646]
	TIME [epoch: 10.4 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1843115649451343		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 2.1843115649451343 | validation: 3.082086223933853]
	TIME [epoch: 10.4 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1907371857078695		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 2.1907371857078695 | validation: 2.8312252350331493]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0300603694165065		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 2.0300603694165065 | validation: 3.55917087460122]
	TIME [epoch: 10.4 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.995021411884279		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 1.995021411884279 | validation: 2.9845638367761227]
	TIME [epoch: 10.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8394555063962759		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 1.8394555063962759 | validation: 2.9422741544619004]
	TIME [epoch: 10.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9065798173849617		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 1.9065798173849617 | validation: 2.958302694026418]
	TIME [epoch: 10.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9531517685265896		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 1.9531517685265896 | validation: 2.8144123226932174]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9700712920563188		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 1.9700712920563188 | validation: 3.1500829706242754]
	TIME [epoch: 10.4 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9805801950619553		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 1.9805801950619553 | validation: 3.0657105602248276]
	TIME [epoch: 10.4 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9913513421658284		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 1.9913513421658284 | validation: 3.0898534782005096]
	TIME [epoch: 10.4 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9847662817785057		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 1.9847662817785057 | validation: 2.832741294337071]
	TIME [epoch: 10.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8695759559714538		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 1.8695759559714538 | validation: 2.825604273388259]
	TIME [epoch: 10.4 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8336937410566247		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 1.8336937410566247 | validation: 2.848635839831817]
	TIME [epoch: 10.4 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0018501918846447		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 2.0018501918846447 | validation: 3.357291137781256]
	TIME [epoch: 10.4 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9859757117086363		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 1.9859757117086363 | validation: 2.855133436527971]
	TIME [epoch: 10.4 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7671033105167067		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 1.7671033105167067 | validation: 2.887282214115432]
	TIME [epoch: 10.4 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8807224231427089		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 1.8807224231427089 | validation: 3.079425974709543]
	TIME [epoch: 10.4 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7807802595866284		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 1.7807802595866284 | validation: 2.858650803153746]
	TIME [epoch: 10.4 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8435605692925265		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 1.8435605692925265 | validation: 3.1020242965523814]
	TIME [epoch: 10.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.894953826552014		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 1.894953826552014 | validation: 3.2167603278228594]
	TIME [epoch: 10.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.123683785883491		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 2.123683785883491 | validation: 3.0150308613862253]
	TIME [epoch: 10.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8854827075108358		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 1.8854827075108358 | validation: 2.9663825241096236]
	TIME [epoch: 10.4 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8294591004081053		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 1.8294591004081053 | validation: 2.829833433482644]
	TIME [epoch: 10.4 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8023731846038153		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 1.8023731846038153 | validation: 2.8424251536013156]
	TIME [epoch: 10.4 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.823211288177117		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 1.823211288177117 | validation: 2.9293678626216604]
	TIME [epoch: 10.4 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8416433478370906		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 1.8416433478370906 | validation: 3.841987762276499]
	TIME [epoch: 10.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9994921251853164		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 1.9994921251853164 | validation: 3.0961935812913977]
	TIME [epoch: 10.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.945330721525695		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 1.945330721525695 | validation: 3.0055515182778856]
	TIME [epoch: 10.4 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8140660093038015		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 1.8140660093038015 | validation: 2.943600512286015]
	TIME [epoch: 10.4 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8652095448392216		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 1.8652095448392216 | validation: 2.9522732498179844]
	TIME [epoch: 10.4 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.021302056885186		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 2.021302056885186 | validation: 3.1489036400176817]
	TIME [epoch: 10.4 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0019128036657783		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 2.0019128036657783 | validation: 2.8657695664205893]
	TIME [epoch: 10.4 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7682158875435938		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 1.7682158875435938 | validation: 2.8393203748792533]
	TIME [epoch: 10.4 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9369893724532496		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 1.9369893724532496 | validation: 2.909211597084576]
	TIME [epoch: 10.4 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8119474390481656		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 1.8119474390481656 | validation: 2.792838357712877]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.863548382873353		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 1.863548382873353 | validation: 2.7644742437087904]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8543601582136546		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 1.8543601582136546 | validation: 3.0853747690318984]
	TIME [epoch: 10.4 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8241913369170877		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 1.8241913369170877 | validation: 2.8351632247289644]
	TIME [epoch: 10.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7155320881404943		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 1.7155320881404943 | validation: 2.764549256644067]
	TIME [epoch: 10.4 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9368180132102204		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 1.9368180132102204 | validation: 3.2888446160328106]
	TIME [epoch: 10.4 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7997598895438922		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 1.7997598895438922 | validation: 2.8139259956817613]
	TIME [epoch: 10.4 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7940602739828293		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 1.7940602739828293 | validation: 3.000920146481598]
	TIME [epoch: 10.4 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7983278827124671		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 1.7983278827124671 | validation: 3.1416861591985183]
	TIME [epoch: 10.4 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.974345176890339		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 1.974345176890339 | validation: 3.2493469755915023]
	TIME [epoch: 10.4 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8557656694011702		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 1.8557656694011702 | validation: 2.916579672105547]
	TIME [epoch: 10.4 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7496521798426663		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 1.7496521798426663 | validation: 2.958827785230405]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7528875632391052		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 1.7528875632391052 | validation: 3.3145354299961234]
	TIME [epoch: 10.4 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8386081308470132		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 1.8386081308470132 | validation: 3.0471093647331617]
	TIME [epoch: 10.4 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8180315187027791		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 1.8180315187027791 | validation: 3.0711271725466345]
	TIME [epoch: 10.4 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7200201032789202		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 1.7200201032789202 | validation: 2.959506525705674]
	TIME [epoch: 10.4 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0539497066806223		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 2.0539497066806223 | validation: 3.5175816577304153]
	TIME [epoch: 10.4 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.1560505596227153		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 2.1560505596227153 | validation: 2.7915659190241335]
	TIME [epoch: 10.3 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7581109310878538		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 1.7581109310878538 | validation: 2.781181041865913]
	TIME [epoch: 10.4 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6851737843031054		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 1.6851737843031054 | validation: 2.8735384950144867]
	TIME [epoch: 10.4 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7473171614670637		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 1.7473171614670637 | validation: 3.117998741304918]
	TIME [epoch: 10.4 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.032075557996072		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 2.032075557996072 | validation: 3.0870730148976624]
	TIME [epoch: 10.4 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7882817797375963		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 1.7882817797375963 | validation: 2.808271883795192]
	TIME [epoch: 10.4 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.867721122195108		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 1.867721122195108 | validation: 3.0120552387434034]
	TIME [epoch: 10.4 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.885114265555768		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 1.885114265555768 | validation: 2.738298047137829]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7285050498392416		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 1.7285050498392416 | validation: 3.0456008605891554]
	TIME [epoch: 10.4 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7381533448088125		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 1.7381533448088125 | validation: 2.8616970499715695]
	TIME [epoch: 10.4 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7879255347713165		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 1.7879255347713165 | validation: 2.870594988275311]
	TIME [epoch: 10.4 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8160658386789017		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 1.8160658386789017 | validation: 2.9500931899913785]
	TIME [epoch: 10.4 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6933107493751536		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 1.6933107493751536 | validation: 3.125076180498926]
	TIME [epoch: 10.4 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9346616910604013		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 1.9346616910604013 | validation: 2.805035441866477]
	TIME [epoch: 10.4 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7531426534448975		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 1.7531426534448975 | validation: 3.026735728786046]
	TIME [epoch: 10.4 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.842647967219777		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 1.842647967219777 | validation: 2.750209490179275]
	TIME [epoch: 10.4 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6934931832210485		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 1.6934931832210485 | validation: 2.83300602989254]
	TIME [epoch: 10.4 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7492011797992522		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 1.7492011797992522 | validation: 2.9477854964358676]
	TIME [epoch: 10.4 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.955062533597712		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 1.955062533597712 | validation: 2.743653119539297]
	TIME [epoch: 10.4 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9447496607341812		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 1.9447496607341812 | validation: 2.8327924595259857]
	TIME [epoch: 10.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7649893912559924		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 1.7649893912559924 | validation: 2.7791905268717993]
	TIME [epoch: 10.4 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7027350585340522		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 1.7027350585340522 | validation: 2.946355963952211]
	TIME [epoch: 10.4 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7905691333696878		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 1.7905691333696878 | validation: 3.4430364839841774]
	TIME [epoch: 10.4 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.9570410745969753		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 1.9570410745969753 | validation: 2.9696408151154094]
	TIME [epoch: 10.4 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8789859701675584		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 1.8789859701675584 | validation: 2.805528826068435]
	TIME [epoch: 10.4 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7456992734446466		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 1.7456992734446466 | validation: 2.763427871082701]
	TIME [epoch: 10.4 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7335391864356589		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 1.7335391864356589 | validation: 2.7321580457996477]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7112859654575843		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 1.7112859654575843 | validation: 2.883672437220568]
	TIME [epoch: 10.4 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8690643512448406		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 1.8690643512448406 | validation: 2.709526378124011]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7372973718685092		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 1.7372973718685092 | validation: 3.0899147180215256]
	TIME [epoch: 10.4 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8298789707673329		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 1.8298789707673329 | validation: 2.707738206059095]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8611772079626558		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 1.8611772079626558 | validation: 3.092147913545009]
	TIME [epoch: 10.4 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7442868341834292		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 1.7442868341834292 | validation: 4.264583834442052]
	TIME [epoch: 10.4 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.2283219316601355		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 2.2283219316601355 | validation: 2.811475578224241]
	TIME [epoch: 10.4 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6655197859206876		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 1.6655197859206876 | validation: 2.8273196235029823]
	TIME [epoch: 10.4 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.676614838500106		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 1.676614838500106 | validation: 2.6743697252375758]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6662707228330962		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 1.6662707228330962 | validation: 3.478650240975014]
	TIME [epoch: 10.4 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8167898059837193		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 1.8167898059837193 | validation: 2.804027472223505]
	TIME [epoch: 10.4 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7197385220918915		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 1.7197385220918915 | validation: 2.703368562344213]
	TIME [epoch: 10.4 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.59837241507853		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 1.59837241507853 | validation: 2.732105660638938]
	TIME [epoch: 10.4 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8557390101858076		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 1.8557390101858076 | validation: 2.7777499287655605]
	TIME [epoch: 10.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6426924134590757		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 1.6426924134590757 | validation: 2.999594205444353]
	TIME [epoch: 10.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8060452446943884		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 1.8060452446943884 | validation: 2.8632151573003184]
	TIME [epoch: 10.4 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.774173278005359		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 1.774173278005359 | validation: 2.785845309080478]
	TIME [epoch: 10.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7081362626759087		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 1.7081362626759087 | validation: 2.6826756239603617]
	TIME [epoch: 10.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.67201719696545		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 1.67201719696545 | validation: 2.7464409296383936]
	TIME [epoch: 10.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6342349150800923		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 1.6342349150800923 | validation: 2.78360316179947]
	TIME [epoch: 10.4 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.689949677701231		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 1.689949677701231 | validation: 3.7674051129510837]
	TIME [epoch: 10.4 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8340339522366542		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 1.8340339522366542 | validation: 2.722117466095639]
	TIME [epoch: 10.4 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6581430077975436		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 1.6581430077975436 | validation: 2.7913280166933485]
	TIME [epoch: 10.4 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7573169685655934		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 1.7573169685655934 | validation: 2.7961666060025685]
	TIME [epoch: 10.4 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7294518261106724		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 1.7294518261106724 | validation: 2.7802545153363156]
	TIME [epoch: 10.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.705077335961505		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 1.705077335961505 | validation: 2.675848625066273]
	TIME [epoch: 10.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6074088469882053		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 1.6074088469882053 | validation: 3.2619879867818224]
	TIME [epoch: 10.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8049261057282027		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 1.8049261057282027 | validation: 2.9593029088012925]
	TIME [epoch: 10.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6666550484080473		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 1.6666550484080473 | validation: 2.857223161388048]
	TIME [epoch: 10.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.586768522214942		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 1.586768522214942 | validation: 2.7644584742444667]
	TIME [epoch: 10.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.722399513196526		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 1.722399513196526 | validation: 3.2884552984983033]
	TIME [epoch: 10.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7308161182570985		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 1.7308161182570985 | validation: 2.8243402792771986]
	TIME [epoch: 10.4 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6448585468083265		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 1.6448585468083265 | validation: 3.3025754159714302]
	TIME [epoch: 10.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.798883955873121		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 1.798883955873121 | validation: 2.738167425913031]
	TIME [epoch: 10.4 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.714054302284915		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 1.714054302284915 | validation: 2.821301293490078]
	TIME [epoch: 10.4 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.575399856977446		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 1.575399856977446 | validation: 2.673045474657611]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6791532946350494		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 1.6791532946350494 | validation: 2.749034690790113]
	TIME [epoch: 10.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5835775707274613		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 1.5835775707274613 | validation: 2.733685522324405]
	TIME [epoch: 10.4 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6473740540520319		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 1.6473740540520319 | validation: 2.754531399738905]
	TIME [epoch: 10.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6147315156463073		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 1.6147315156463073 | validation: 2.767742848522089]
	TIME [epoch: 10.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7328320274731177		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 1.7328320274731177 | validation: 2.8669622634603504]
	TIME [epoch: 10.4 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6466956132466877		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 1.6466956132466877 | validation: 2.9458087412306937]
	TIME [epoch: 10.4 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7356703791278274		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 1.7356703791278274 | validation: 2.945309857991887]
	TIME [epoch: 10.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 2.0697859874772537		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 2.0697859874772537 | validation: 3.9984373507893634]
	TIME [epoch: 10.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8921040110208165		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 1.8921040110208165 | validation: 2.8866317904036745]
	TIME [epoch: 10.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7120261014677098		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 1.7120261014677098 | validation: 2.8004168133004397]
	TIME [epoch: 10.4 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6897675290238148		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 1.6897675290238148 | validation: 2.822744332483311]
	TIME [epoch: 10.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6458576846297088		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 1.6458576846297088 | validation: 2.8052754011823957]
	TIME [epoch: 10.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6906507307374885		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 1.6906507307374885 | validation: 2.76230261122434]
	TIME [epoch: 10.4 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5899824748910214		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 1.5899824748910214 | validation: 2.788010559426167]
	TIME [epoch: 10.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5824958644888925		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 1.5824958644888925 | validation: 2.7023249427768907]
	TIME [epoch: 10.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7928194319834945		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 1.7928194319834945 | validation: 2.6852099398795337]
	TIME [epoch: 10.4 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7628877149468818		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 1.7628877149468818 | validation: 2.6884849124488888]
	TIME [epoch: 10.4 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.646284045582977		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 1.646284045582977 | validation: 2.8080947760734465]
	TIME [epoch: 10.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5706564068248245		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 1.5706564068248245 | validation: 2.7666906031340455]
	TIME [epoch: 10.4 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6474912200278813		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 1.6474912200278813 | validation: 2.7434271535576342]
	TIME [epoch: 10.4 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6128027524543076		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 1.6128027524543076 | validation: 2.7329052754320777]
	TIME [epoch: 10.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6058078313927577		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 1.6058078313927577 | validation: 2.7198339333909307]
	TIME [epoch: 10.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.591109399232673		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 1.591109399232673 | validation: 2.7450054431162325]
	TIME [epoch: 10.4 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.608053904827727		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 1.608053904827727 | validation: 2.7924526201562414]
	TIME [epoch: 10.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6424904698010991		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 1.6424904698010991 | validation: 2.703371393732142]
	TIME [epoch: 10.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6718739898075772		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 1.6718739898075772 | validation: 3.0684595552688037]
	TIME [epoch: 10.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8362747708930403		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 1.8362747708930403 | validation: 2.8683163448135067]
	TIME [epoch: 10.4 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6616822203786665		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 1.6616822203786665 | validation: 2.8425373968196492]
	TIME [epoch: 10.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.679414914572406		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 1.679414914572406 | validation: 2.7413211972838942]
	TIME [epoch: 10.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6647761380251058		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 1.6647761380251058 | validation: 2.753612002711831]
	TIME [epoch: 10.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6894908103129112		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 1.6894908103129112 | validation: 2.750217167990026]
	TIME [epoch: 10.4 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.581050296970104		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 1.581050296970104 | validation: 3.0026803739429178]
	TIME [epoch: 10.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6847264363269605		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 1.6847264363269605 | validation: 2.8381398689070005]
	TIME [epoch: 10.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6370625549055908		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 1.6370625549055908 | validation: 2.7215860272925174]
	TIME [epoch: 10.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.587819509509644		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 1.587819509509644 | validation: 2.7474384615422003]
	TIME [epoch: 10.4 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7491857572834988		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 1.7491857572834988 | validation: 2.7715493388180583]
	TIME [epoch: 10.4 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6165438080090766		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 1.6165438080090766 | validation: 2.8055899924839514]
	TIME [epoch: 10.4 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6701336312169466		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 1.6701336312169466 | validation: 2.921458895419438]
	TIME [epoch: 10.4 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5792937922312056		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 1.5792937922312056 | validation: 2.8621525961367134]
	TIME [epoch: 10.4 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5644958871494716		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 1.5644958871494716 | validation: 2.738848605616472]
	TIME [epoch: 10.4 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6242019770074403		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 1.6242019770074403 | validation: 3.4043937303997542]
	TIME [epoch: 10.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.8011405757080208		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 1.8011405757080208 | validation: 2.7423134329848677]
	TIME [epoch: 10.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.824903136892912		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 1.824903136892912 | validation: 2.6947082420615516]
	TIME [epoch: 10.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5582359384679907		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 1.5582359384679907 | validation: 2.768099665269619]
	TIME [epoch: 10.4 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5154602900932617		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 1.5154602900932617 | validation: 2.6674179818852086]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5254181170636545		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 1.5254181170636545 | validation: 2.7106780095650476]
	TIME [epoch: 10.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.605526993224883		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 1.605526993224883 | validation: 2.7539624730084635]
	TIME [epoch: 10.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5348515841287882		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 1.5348515841287882 | validation: 2.786535787174912]
	TIME [epoch: 10.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.589351440955293		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 1.589351440955293 | validation: 2.7404561051955056]
	TIME [epoch: 10.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6828884569344784		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 1.6828884569344784 | validation: 2.6528268363623266]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6220245196387986		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 1.6220245196387986 | validation: 2.806474297816858]
	TIME [epoch: 10.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5933072022732842		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 1.5933072022732842 | validation: 2.760601679249803]
	TIME [epoch: 10.4 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5784136891605882		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 1.5784136891605882 | validation: 2.6972281171893657]
	TIME [epoch: 10.4 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5175585355207069		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 1.5175585355207069 | validation: 2.765261071944968]
	TIME [epoch: 10.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.578639094698805		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 1.578639094698805 | validation: 2.7270939469121367]
	TIME [epoch: 10.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5127123493609689		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 1.5127123493609689 | validation: 2.776444172040584]
	TIME [epoch: 10.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6893197992124214		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 1.6893197992124214 | validation: 2.6657423141063057]
	TIME [epoch: 10.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6309573308016507		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 1.6309573308016507 | validation: 2.6581476580353893]
	TIME [epoch: 10.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6343766179735657		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 1.6343766179735657 | validation: 2.765187355240524]
	TIME [epoch: 10.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5470597656072411		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 1.5470597656072411 | validation: 2.846807465664566]
	TIME [epoch: 10.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5923672443014119		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 1.5923672443014119 | validation: 2.658196187571175]
	TIME [epoch: 10.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.623254392321278		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 1.623254392321278 | validation: 2.6806349451619984]
	TIME [epoch: 10.4 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5895470399143758		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 1.5895470399143758 | validation: 2.6947382375718405]
	TIME [epoch: 10.4 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5296811593303947		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 1.5296811593303947 | validation: 2.6836591819137556]
	TIME [epoch: 10.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.530956655831273		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 1.530956655831273 | validation: 3.1628633290561585]
	TIME [epoch: 10.4 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.722442999199965		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 1.722442999199965 | validation: 2.756622338854978]
	TIME [epoch: 10.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5257805484093985		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 1.5257805484093985 | validation: 2.6132093124317834]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5294454913634883		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 1.5294454913634883 | validation: 2.7161294229427724]
	TIME [epoch: 10.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4964285469989922		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 1.4964285469989922 | validation: 2.663175736059568]
	TIME [epoch: 10.4 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6519579142534284		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 1.6519579142534284 | validation: 2.649955085074399]
	TIME [epoch: 10.4 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.578358662037417		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 1.578358662037417 | validation: 2.7539082886440247]
	TIME [epoch: 10.4 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.491571922615353		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 1.491571922615353 | validation: 2.9132052784087574]
	TIME [epoch: 10.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6922452699526587		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 1.6922452699526587 | validation: 2.6852065642479226]
	TIME [epoch: 10.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5189192570781895		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 1.5189192570781895 | validation: 2.647799368957761]
	TIME [epoch: 10.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5952787631695446		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 1.5952787631695446 | validation: 2.9226241292757607]
	TIME [epoch: 10.4 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5431351762834669		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 1.5431351762834669 | validation: 2.621565069478723]
	TIME [epoch: 10.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5237409984400823		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 1.5237409984400823 | validation: 2.7034870836783513]
	TIME [epoch: 10.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5202568609152336		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 1.5202568609152336 | validation: 2.6021189229782307]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5488320381007432		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 1.5488320381007432 | validation: 2.597669142722988]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5114734205124254		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 1.5114734205124254 | validation: 2.6692375201897325]
	TIME [epoch: 10.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7334823211921673		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 1.7334823211921673 | validation: 2.7448630717139997]
	TIME [epoch: 10.4 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4758499917493393		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 1.4758499917493393 | validation: 2.63950273309193]
	TIME [epoch: 10.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5913667710987558		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 1.5913667710987558 | validation: 2.668847684692092]
	TIME [epoch: 10.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5093891219340128		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 1.5093891219340128 | validation: 2.8393049385977474]
	TIME [epoch: 10.4 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7731092603818006		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 1.7731092603818006 | validation: 2.7463928250235807]
	TIME [epoch: 10.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.504370078490504		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 1.504370078490504 | validation: 2.6610522814412496]
	TIME [epoch: 10.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4581036951128366		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 1.4581036951128366 | validation: 2.854811761455561]
	TIME [epoch: 10.4 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5549349294416412		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 1.5549349294416412 | validation: 2.63074421526357]
	TIME [epoch: 10.4 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.486223173023384		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 1.486223173023384 | validation: 2.6283646283157918]
	TIME [epoch: 10.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5130256332671415		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 1.5130256332671415 | validation: 2.684111339649776]
	TIME [epoch: 10.4 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.636614032511012		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 1.636614032511012 | validation: 2.6782934661731295]
	TIME [epoch: 10.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4968030624915702		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 1.4968030624915702 | validation: 2.7096040268569435]
	TIME [epoch: 10.4 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5259923760030394		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 1.5259923760030394 | validation: 2.8804288961516384]
	TIME [epoch: 10.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5812632472049706		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 1.5812632472049706 | validation: 2.6735215685022173]
	TIME [epoch: 10.4 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5650049057094908		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 1.5650049057094908 | validation: 2.7018260885424286]
	TIME [epoch: 10.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5118332606521372		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 1.5118332606521372 | validation: 2.7057620525890327]
	TIME [epoch: 10.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5007866806237309		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 1.5007866806237309 | validation: 2.653406337853543]
	TIME [epoch: 10.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.456619294132441		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 1.456619294132441 | validation: 3.232196058361134]
	TIME [epoch: 10.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6458811769169928		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 1.6458811769169928 | validation: 2.6428837014047737]
	TIME [epoch: 10.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5089074913000946		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 1.5089074913000946 | validation: 2.6893018000332236]
	TIME [epoch: 10.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5364749976731396		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 1.5364749976731396 | validation: 2.7224640549708816]
	TIME [epoch: 10.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5928454734890893		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 1.5928454734890893 | validation: 2.677490582298834]
	TIME [epoch: 10.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.661207901438571		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 1.661207901438571 | validation: 2.6475267695990827]
	TIME [epoch: 10.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5292776305936537		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 1.5292776305936537 | validation: 2.6757297204559274]
	TIME [epoch: 10.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5317688891327983		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 1.5317688891327983 | validation: 2.697437112714595]
	TIME [epoch: 10.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4582800017575193		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 1.4582800017575193 | validation: 2.611845709020512]
	TIME [epoch: 10.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5222029776400294		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 1.5222029776400294 | validation: 2.7311653201344814]
	TIME [epoch: 10.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4867654600013096		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 1.4867654600013096 | validation: 2.6913997649090864]
	TIME [epoch: 10.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5055630847491064		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 1.5055630847491064 | validation: 2.692351200550258]
	TIME [epoch: 10.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.496790602909391		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 1.496790602909391 | validation: 2.624263575613976]
	TIME [epoch: 10.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5300789925581733		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 1.5300789925581733 | validation: 2.708841550481226]
	TIME [epoch: 10.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5139044656598766		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 1.5139044656598766 | validation: 2.6589412226251508]
	TIME [epoch: 10.4 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5081824019568753		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 1.5081824019568753 | validation: 2.7922446204064153]
	TIME [epoch: 10.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5914346352911388		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 1.5914346352911388 | validation: 3.0690688226126177]
	TIME [epoch: 10.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5575578023159748		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 1.5575578023159748 | validation: 2.6462020961051547]
	TIME [epoch: 10.4 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4732270173425097		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 1.4732270173425097 | validation: 2.7525620246674953]
	TIME [epoch: 10.4 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5271243957178178		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 1.5271243957178178 | validation: 2.6575230657361937]
	TIME [epoch: 10.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4816438569992765		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 1.4816438569992765 | validation: 2.6597588400554795]
	TIME [epoch: 10.4 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4762165134066667		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 1.4762165134066667 | validation: 2.625357201198785]
	TIME [epoch: 10.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4654153256923546		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 1.4654153256923546 | validation: 2.59994936252522]
	TIME [epoch: 10.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5193127937420292		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 1.5193127937420292 | validation: 2.677656640047506]
	TIME [epoch: 10.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.537169439526838		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 1.537169439526838 | validation: 2.8755219777682033]
	TIME [epoch: 10.4 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5337449098931129		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 1.5337449098931129 | validation: 2.626184279595608]
	TIME [epoch: 10.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4629371083763232		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 1.4629371083763232 | validation: 2.696039238767042]
	TIME [epoch: 10.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4884063313013747		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 1.4884063313013747 | validation: 2.718696221176074]
	TIME [epoch: 10.4 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.548852242090305		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 1.548852242090305 | validation: 2.80240624815797]
	TIME [epoch: 10.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.557431585216283		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 1.557431585216283 | validation: 2.7139844204292856]
	TIME [epoch: 10.4 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.535083860092905		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 1.535083860092905 | validation: 2.6278886908801495]
	TIME [epoch: 10.4 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4647976140308239		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 1.4647976140308239 | validation: 2.6325290961492134]
	TIME [epoch: 10.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4565756332561048		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 1.4565756332561048 | validation: 2.6850931459732554]
	TIME [epoch: 10.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4391138271429411		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 1.4391138271429411 | validation: 2.7718582714942803]
	TIME [epoch: 10.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5455700981607818		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 1.5455700981607818 | validation: 2.7228578057076676]
	TIME [epoch: 10.4 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.517909301653713		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 1.517909301653713 | validation: 2.657148246300734]
	TIME [epoch: 10.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5162175514625167		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 1.5162175514625167 | validation: 2.671534063559335]
	TIME [epoch: 10.4 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4895095398786382		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 1.4895095398786382 | validation: 2.7124788889279046]
	TIME [epoch: 10.4 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4441877606385005		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 1.4441877606385005 | validation: 2.854765275641968]
	TIME [epoch: 10.4 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5186514169312892		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 1.5186514169312892 | validation: 2.6313083695389285]
	TIME [epoch: 10.4 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5069247635192815		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 1.5069247635192815 | validation: 2.689270184494453]
	TIME [epoch: 10.4 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4537573522943839		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 1.4537573522943839 | validation: 2.636274351071606]
	TIME [epoch: 10.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5758313483805335		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 1.5758313483805335 | validation: 2.814294938479426]
	TIME [epoch: 10.4 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5135312444502134		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 1.5135312444502134 | validation: 2.670618973777144]
	TIME [epoch: 10.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5856317207650388		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 1.5856317207650388 | validation: 2.621362787627199]
	TIME [epoch: 10.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4429602805182973		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 1.4429602805182973 | validation: 2.638618111943314]
	TIME [epoch: 10.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4727591157923108		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 1.4727591157923108 | validation: 2.701692410249957]
	TIME [epoch: 10.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4657156742072046		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 1.4657156742072046 | validation: 2.7202302216574634]
	TIME [epoch: 10.4 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.469409785627272		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 1.469409785627272 | validation: 2.749684705819054]
	TIME [epoch: 10.4 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4573568348281205		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 1.4573568348281205 | validation: 2.6947670609751584]
	TIME [epoch: 10.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4599078569387307		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 1.4599078569387307 | validation: 2.5972724949682062]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4358916159213735		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 1.4358916159213735 | validation: 2.5823776087953862]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3896881520116404		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 1.3896881520116404 | validation: 2.6344772561914573]
	TIME [epoch: 10.4 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5799400377198185		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 1.5799400377198185 | validation: 2.6847067913512888]
	TIME [epoch: 10.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4519646967442466		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 1.4519646967442466 | validation: 2.6360518288433195]
	TIME [epoch: 10.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4925402591682762		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 1.4925402591682762 | validation: 2.705475145690248]
	TIME [epoch: 10.4 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5557522927764122		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 1.5557522927764122 | validation: 2.748194719888833]
	TIME [epoch: 10.4 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.50569491214863		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 1.50569491214863 | validation: 2.6877737793415806]
	TIME [epoch: 10.4 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.7000439203275923		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 1.7000439203275923 | validation: 2.759138681259509]
	TIME [epoch: 10.4 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.469657040737279		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 1.469657040737279 | validation: 2.626219961258156]
	TIME [epoch: 10.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.485087868768622		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 1.485087868768622 | validation: 2.605258201755898]
	TIME [epoch: 10.4 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4399496114777999		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 1.4399496114777999 | validation: 2.8000642994600504]
	TIME [epoch: 10.4 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4965706177399323		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 1.4965706177399323 | validation: 2.6000742196459963]
	TIME [epoch: 10.4 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4985829514809839		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 1.4985829514809839 | validation: 2.569635506536516]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4727844856766068		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 1.4727844856766068 | validation: 2.607387567118829]
	TIME [epoch: 10.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.454030412284986		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 1.454030412284986 | validation: 2.6232421186310466]
	TIME [epoch: 10.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.483511304117464		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 1.483511304117464 | validation: 2.7072632655247872]
	TIME [epoch: 10.4 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4670656850839765		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 1.4670656850839765 | validation: 2.663293509599076]
	TIME [epoch: 10.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.434933221604843		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 1.434933221604843 | validation: 2.6541069338142145]
	TIME [epoch: 10.4 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5143389828509055		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 1.5143389828509055 | validation: 2.747917077536298]
	TIME [epoch: 10.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4758166936998163		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 1.4758166936998163 | validation: 2.671123317639964]
	TIME [epoch: 10.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4366373018539753		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 1.4366373018539753 | validation: 2.629995946511802]
	TIME [epoch: 10.4 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4692417803122595		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 1.4692417803122595 | validation: 2.5845472574010833]
	TIME [epoch: 10.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5045356468293551		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 1.5045356468293551 | validation: 2.616147157139543]
	TIME [epoch: 10.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4835080818792956		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 1.4835080818792956 | validation: 2.655793321019597]
	TIME [epoch: 10.4 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4568344139078853		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 1.4568344139078853 | validation: 2.6473778293705776]
	TIME [epoch: 10.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4881233269298562		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 1.4881233269298562 | validation: 2.727558484291275]
	TIME [epoch: 10.4 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.539449302472668		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 1.539449302472668 | validation: 2.705024075888084]
	TIME [epoch: 10.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4155204979423996		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 1.4155204979423996 | validation: 2.595847462653304]
	TIME [epoch: 10.4 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4134906753223175		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 1.4134906753223175 | validation: 2.679000411985043]
	TIME [epoch: 10.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4500323803153623		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 1.4500323803153623 | validation: 2.674711846817096]
	TIME [epoch: 10.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5367398518385433		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 1.5367398518385433 | validation: 2.608308339878009]
	TIME [epoch: 10.4 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5904832163865432		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 1.5904832163865432 | validation: 2.614703625443314]
	TIME [epoch: 10.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4735247526076853		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 1.4735247526076853 | validation: 2.627660555034772]
	TIME [epoch: 10.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4165721605275512		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 1.4165721605275512 | validation: 2.599381385860595]
	TIME [epoch: 10.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4676977288951982		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 1.4676977288951982 | validation: 2.743952095291129]
	TIME [epoch: 10.4 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5121526383248365		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 1.5121526383248365 | validation: 2.6159930060858407]
	TIME [epoch: 10.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4207421780611869		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 1.4207421780611869 | validation: 2.645924672755026]
	TIME [epoch: 10.4 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4293451942883237		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 1.4293451942883237 | validation: 2.6443531798300115]
	TIME [epoch: 10.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4499473311788038		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 1.4499473311788038 | validation: 2.925011630707122]
	TIME [epoch: 10.4 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5180474779636204		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 1.5180474779636204 | validation: 2.6016449012650797]
	TIME [epoch: 10.4 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4386789475714068		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 1.4386789475714068 | validation: 2.652291471064111]
	TIME [epoch: 10.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4167907878860784		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 1.4167907878860784 | validation: 2.590071952257681]
	TIME [epoch: 10.4 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4373091805613223		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 1.4373091805613223 | validation: 2.6298536810467876]
	TIME [epoch: 10.4 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4158262948260656		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 1.4158262948260656 | validation: 2.8343507820993716]
	TIME [epoch: 10.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4942717516131159		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 1.4942717516131159 | validation: 2.675528681834068]
	TIME [epoch: 10.4 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4424147506127039		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 1.4424147506127039 | validation: 2.628817477623221]
	TIME [epoch: 10.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3979014074810658		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 1.3979014074810658 | validation: 2.7028082202663324]
	TIME [epoch: 10.4 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5767373743501376		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 1.5767373743501376 | validation: 2.668871339547162]
	TIME [epoch: 10.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.49727913753329		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 1.49727913753329 | validation: 2.6402490890681305]
	TIME [epoch: 10.4 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4324915139863017		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 1.4324915139863017 | validation: 2.57200566184046]
	TIME [epoch: 10.4 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4221769049492972		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 1.4221769049492972 | validation: 2.61829248967914]
	TIME [epoch: 10.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4207750323503687		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 1.4207750323503687 | validation: 2.6209323374218103]
	TIME [epoch: 10.4 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.6634880585050205		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 1.6634880585050205 | validation: 2.5787841228038104]
	TIME [epoch: 10.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.444207152118506		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 1.444207152118506 | validation: 2.625253887331007]
	TIME [epoch: 10.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4190711477433573		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 1.4190711477433573 | validation: 2.633306903583654]
	TIME [epoch: 10.4 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4632533375725267		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 1.4632533375725267 | validation: 2.6007049074731583]
	TIME [epoch: 10.4 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4169733772232718		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 1.4169733772232718 | validation: 2.7183794383628412]
	TIME [epoch: 10.4 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4063982656294727		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 1.4063982656294727 | validation: 2.6409637860012243]
	TIME [epoch: 10.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.517498869381361		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 1.517498869381361 | validation: 2.5854387719302974]
	TIME [epoch: 10.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.423481443633228		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 1.423481443633228 | validation: 2.68842625459063]
	TIME [epoch: 10.4 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.421113553189499		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 1.421113553189499 | validation: 2.659026445397952]
	TIME [epoch: 10.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4446462684935235		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 1.4446462684935235 | validation: 2.6771768614302704]
	TIME [epoch: 10.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.439240290295746		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 1.439240290295746 | validation: 2.6215049849049716]
	TIME [epoch: 10.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4903105851824232		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 1.4903105851824232 | validation: 2.6589669743798168]
	TIME [epoch: 10.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3945243836052073		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 1.3945243836052073 | validation: 2.6175281458463564]
	TIME [epoch: 10.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3953775267648738		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 1.3953775267648738 | validation: 2.6194057922320964]
	TIME [epoch: 10.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4493613154224145		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 1.4493613154224145 | validation: 2.998833102035139]
	TIME [epoch: 10.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5152697089184248		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 1.5152697089184248 | validation: 2.5984169147895857]
	TIME [epoch: 10.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3970453267731668		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 1.3970453267731668 | validation: 2.7445600250550024]
	TIME [epoch: 10.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4447245361076466		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 1.4447245361076466 | validation: 2.6812150516009363]
	TIME [epoch: 10.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4751724690977697		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 1.4751724690977697 | validation: 2.576423213444536]
	TIME [epoch: 10.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4272869424638193		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 1.4272869424638193 | validation: 2.599482216904952]
	TIME [epoch: 10.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4302558242951366		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 1.4302558242951366 | validation: 2.6400642132457213]
	TIME [epoch: 10.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3884182813818289		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 1.3884182813818289 | validation: 2.5591711832504345]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4347878396971356		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 1.4347878396971356 | validation: 2.7225628823296337]
	TIME [epoch: 10.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.435817710044327		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 1.435817710044327 | validation: 2.7987925751673486]
	TIME [epoch: 10.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4538599333332978		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 1.4538599333332978 | validation: 2.659305892330731]
	TIME [epoch: 10.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4241290600740486		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 1.4241290600740486 | validation: 2.6503245235093136]
	TIME [epoch: 10.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4484516858652896		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 1.4484516858652896 | validation: 2.7236721947315523]
	TIME [epoch: 10.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.449931733551256		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 1.449931733551256 | validation: 2.6001912854794473]
	TIME [epoch: 10.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3963339889477866		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 1.3963339889477866 | validation: 2.5908580633647116]
	TIME [epoch: 10.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.460286353686866		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 1.460286353686866 | validation: 2.608519308607977]
	TIME [epoch: 10.4 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4342447807235597		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 1.4342447807235597 | validation: 2.708561537814655]
	TIME [epoch: 10.4 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4088754534157462		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 1.4088754534157462 | validation: 2.5687938123264606]
	TIME [epoch: 10.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.466090254031378		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 1.466090254031378 | validation: 2.6128502885155855]
	TIME [epoch: 10.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4162821796321243		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 1.4162821796321243 | validation: 2.600975609128475]
	TIME [epoch: 10.4 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3845567931825415		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 1.3845567931825415 | validation: 2.5580155396663713]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.407874270145156		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 1.407874270145156 | validation: 2.61393105633038]
	TIME [epoch: 10.4 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4171692443438482		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 1.4171692443438482 | validation: 2.5972796449724522]
	TIME [epoch: 10.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4045444801506317		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 1.4045444801506317 | validation: 2.5898660214946303]
	TIME [epoch: 10.4 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.39177569424032		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 1.39177569424032 | validation: 2.623575784460212]
	TIME [epoch: 10.4 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4283606985378658		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 1.4283606985378658 | validation: 2.6511832343013033]
	TIME [epoch: 10.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4149238568663105		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 1.4149238568663105 | validation: 2.5901028012965814]
	TIME [epoch: 10.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3976054428318683		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 1.3976054428318683 | validation: 2.6542173279180123]
	TIME [epoch: 10.4 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4247344284333128		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 1.4247344284333128 | validation: 2.5965653531450092]
	TIME [epoch: 10.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4182864439991065		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 1.4182864439991065 | validation: 2.7087448494944453]
	TIME [epoch: 10.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4151919695938997		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 1.4151919695938997 | validation: 2.7176218950277553]
	TIME [epoch: 10.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4103085712174768		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 1.4103085712174768 | validation: 2.63110450462425]
	TIME [epoch: 10.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4371161410450355		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 1.4371161410450355 | validation: 2.684470618276268]
	TIME [epoch: 10.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4031633723811967		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 1.4031633723811967 | validation: 2.635389430818251]
	TIME [epoch: 10.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4245675046510866		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 1.4245675046510866 | validation: 2.6069282361665347]
	TIME [epoch: 10.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.441719515240248		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 1.441719515240248 | validation: 2.5736309510931896]
	TIME [epoch: 10.4 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3932209250131666		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 1.3932209250131666 | validation: 2.610240705748455]
	TIME [epoch: 10.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4482690806276695		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 1.4482690806276695 | validation: 2.694660330781034]
	TIME [epoch: 10.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.385605147415445		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 1.385605147415445 | validation: 2.573883678463908]
	TIME [epoch: 10.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4015723439280987		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 1.4015723439280987 | validation: 2.599774450391357]
	TIME [epoch: 10.4 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5417900460372154		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 1.5417900460372154 | validation: 2.785163044776358]
	TIME [epoch: 10.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4403853753140834		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 1.4403853753140834 | validation: 2.705414245094471]
	TIME [epoch: 10.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4152250803149076		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 1.4152250803149076 | validation: 2.716747508887334]
	TIME [epoch: 10.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.443139558498001		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 1.443139558498001 | validation: 2.5731394733982382]
	TIME [epoch: 10.4 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.387905135192113		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 1.387905135192113 | validation: 2.6479879877081545]
	TIME [epoch: 10.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4252670134042476		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 1.4252670134042476 | validation: 2.5915090151791107]
	TIME [epoch: 10.4 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.461181492848828		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 1.461181492848828 | validation: 2.603485097950844]
	TIME [epoch: 10.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3986713997536506		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 1.3986713997536506 | validation: 2.5805685327848344]
	TIME [epoch: 10.4 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4054626139568718		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 1.4054626139568718 | validation: 2.6426411651900104]
	TIME [epoch: 10.4 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.425595757684506		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 1.425595757684506 | validation: 2.636153440071644]
	TIME [epoch: 10.4 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3785219057494493		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 1.3785219057494493 | validation: 2.687586074832483]
	TIME [epoch: 10.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4043048472384023		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 1.4043048472384023 | validation: 2.5720737491304213]
	TIME [epoch: 10.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4079813985805387		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 1.4079813985805387 | validation: 2.5964591562770263]
	TIME [epoch: 10.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3561375422028656		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 1.3561375422028656 | validation: 2.612590290650631]
	TIME [epoch: 10.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.523458466018999		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 1.523458466018999 | validation: 2.5664861613253396]
	TIME [epoch: 10.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.369036955279492		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 1.369036955279492 | validation: 2.7893147480574845]
	TIME [epoch: 10.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4794720517755386		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 1.4794720517755386 | validation: 2.6042351336669927]
	TIME [epoch: 10.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4397553510620464		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 1.4397553510620464 | validation: 2.633807769660462]
	TIME [epoch: 10.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4148265975522827		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 1.4148265975522827 | validation: 2.6929963983230976]
	TIME [epoch: 10.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.470775903265428		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 1.470775903265428 | validation: 2.6284700613887115]
	TIME [epoch: 10.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3887152859367415		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 1.3887152859367415 | validation: 2.5749413763123976]
	TIME [epoch: 10.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4123540169217232		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 1.4123540169217232 | validation: 2.6011251489833147]
	TIME [epoch: 10.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4626524136508874		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 1.4626524136508874 | validation: 2.5748835118878106]
	TIME [epoch: 10.4 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4155699084909494		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 1.4155699084909494 | validation: 2.6411753604910255]
	TIME [epoch: 10.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4163020713895527		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 1.4163020713895527 | validation: 2.8980631570627158]
	TIME [epoch: 10.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.5534112470058663		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 1.5534112470058663 | validation: 2.585268158695641]
	TIME [epoch: 10.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3975220723267792		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 1.3975220723267792 | validation: 2.5811534668805214]
	TIME [epoch: 10.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3935931015327205		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 1.3935931015327205 | validation: 2.596096954429983]
	TIME [epoch: 10.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4259525985803365		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 1.4259525985803365 | validation: 2.6479732449159035]
	TIME [epoch: 10.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4220574046259755		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 1.4220574046259755 | validation: 2.576297003800741]
	TIME [epoch: 10.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4093609459385348		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 1.4093609459385348 | validation: 2.6482785311799613]
	TIME [epoch: 10.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4180893629042095		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 1.4180893629042095 | validation: 2.6077573067863806]
	TIME [epoch: 10.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.375892723301857		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 1.375892723301857 | validation: 2.567147648648906]
	TIME [epoch: 10.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3660287690006514		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 1.3660287690006514 | validation: 2.5609603976054225]
	TIME [epoch: 10.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3879718153844398		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 1.3879718153844398 | validation: 2.6205575542460116]
	TIME [epoch: 10.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3900066456292148		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 1.3900066456292148 | validation: 2.5732764833512243]
	TIME [epoch: 10.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.404164675013114		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 1.404164675013114 | validation: 2.6383671878998856]
	TIME [epoch: 10.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4968098160530303		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 1.4968098160530303 | validation: 2.5736020561929434]
	TIME [epoch: 10.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.399322772387406		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 1.399322772387406 | validation: 2.5550069598106604]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_517.pth
	Model improved!!!
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3510895489357069		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 1.3510895489357069 | validation: 2.5764719154229363]
	TIME [epoch: 10.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3712285099196726		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 1.3712285099196726 | validation: 2.595160379688356]
	TIME [epoch: 10.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4396743011501572		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 1.4396743011501572 | validation: 2.5771220600434925]
	TIME [epoch: 10.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.389514340304779		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 1.389514340304779 | validation: 2.6622156226630818]
	TIME [epoch: 10.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4363520577239501		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 1.4363520577239501 | validation: 2.6114255373410806]
	TIME [epoch: 10.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.379365443309269		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 1.379365443309269 | validation: 2.6146981428924665]
	TIME [epoch: 10.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4086872365900138		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 1.4086872365900138 | validation: 2.609983887532796]
	TIME [epoch: 10.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3634970799143442		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 1.3634970799143442 | validation: 2.5809018712374394]
	TIME [epoch: 10.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.380897417730693		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 1.380897417730693 | validation: 2.6485499511659247]
	TIME [epoch: 10.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3901402866975574		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 1.3901402866975574 | validation: 2.7547734771384262]
	TIME [epoch: 10.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4174878435616058		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 1.4174878435616058 | validation: 2.5666871638436475]
	TIME [epoch: 10.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4360547885722696		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 1.4360547885722696 | validation: 2.7199816539187474]
	TIME [epoch: 10.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4417817608010632		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 1.4417817608010632 | validation: 2.562370133063305]
	TIME [epoch: 10.4 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3597499792420198		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 1.3597499792420198 | validation: 2.6853484522323186]
	TIME [epoch: 10.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4350381426658159		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 1.4350381426658159 | validation: 2.5269448387163607]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3821380454816956		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 1.3821380454816956 | validation: 2.724356925778013]
	TIME [epoch: 10.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4482359477047304		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 1.4482359477047304 | validation: 2.572567773122467]
	TIME [epoch: 10.4 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3928673523645518		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 1.3928673523645518 | validation: 2.5728026283154617]
	TIME [epoch: 10.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4338125030163114		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 1.4338125030163114 | validation: 2.7081470900462414]
	TIME [epoch: 10.4 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4231745851633952		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 1.4231745851633952 | validation: 2.7595631677288037]
	TIME [epoch: 10.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4707686198549255		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 1.4707686198549255 | validation: 2.6098856242306234]
	TIME [epoch: 10.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.395097875316948		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 1.395097875316948 | validation: 2.602873347016718]
	TIME [epoch: 10.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3778760393847673		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 1.3778760393847673 | validation: 2.574652662628359]
	TIME [epoch: 10.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.392642867325683		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 1.392642867325683 | validation: 2.6178360162316516]
	TIME [epoch: 10.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4078100675035978		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 1.4078100675035978 | validation: 2.5971333670662444]
	TIME [epoch: 10.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3859997054095836		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 1.3859997054095836 | validation: 2.5654167570292463]
	TIME [epoch: 10.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3718921076469024		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 1.3718921076469024 | validation: 2.6130795114300747]
	TIME [epoch: 10.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3727488506895604		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 1.3727488506895604 | validation: 2.606425926584392]
	TIME [epoch: 10.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3936918901613065		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 1.3936918901613065 | validation: 2.575777240290788]
	TIME [epoch: 10.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3989687453842126		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 1.3989687453842126 | validation: 2.60352344415168]
	TIME [epoch: 10.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.364167221656628		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 1.364167221656628 | validation: 2.6848428412968515]
	TIME [epoch: 10.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4233535566278217		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 1.4233535566278217 | validation: 2.589549265358246]
	TIME [epoch: 10.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3983725704547716		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 1.3983725704547716 | validation: 2.5518515520574483]
	TIME [epoch: 10.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.359259153849373		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 1.359259153849373 | validation: 2.7501704234708666]
	TIME [epoch: 10.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4151044898465586		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 1.4151044898465586 | validation: 2.6164082691182937]
	TIME [epoch: 10.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3929807471451325		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 1.3929807471451325 | validation: 2.5793494536508725]
	TIME [epoch: 10.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3785924189365075		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 1.3785924189365075 | validation: 2.8986460588869707]
	TIME [epoch: 10.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4708427841143241		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 1.4708427841143241 | validation: 2.55254674457647]
	TIME [epoch: 10.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3842383613242297		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 1.3842383613242297 | validation: 2.5925576880903454]
	TIME [epoch: 10.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3690203316306735		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 1.3690203316306735 | validation: 2.612187034117552]
	TIME [epoch: 10.4 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.376750112638317		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 1.376750112638317 | validation: 2.5794323323897226]
	TIME [epoch: 10.4 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3662868708202427		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 1.3662868708202427 | validation: 2.5767646565829456]
	TIME [epoch: 10.4 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3846708805689603		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 1.3846708805689603 | validation: 2.5870316094845385]
	TIME [epoch: 10.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.397059651687132		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 1.397059651687132 | validation: 2.588154592785003]
	TIME [epoch: 10.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3629534960431513		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 1.3629534960431513 | validation: 2.5771374678639143]
	TIME [epoch: 10.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3848812434019662		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 1.3848812434019662 | validation: 2.554912137791977]
	TIME [epoch: 10.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4232099267449496		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 1.4232099267449496 | validation: 2.557511771076704]
	TIME [epoch: 10.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3687358941285184		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 1.3687358941285184 | validation: 2.5644355470071383]
	TIME [epoch: 10.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3588085615567689		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 1.3588085615567689 | validation: 2.5701058576082723]
	TIME [epoch: 10.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.443340237275852		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 1.443340237275852 | validation: 2.556807874050976]
	TIME [epoch: 10.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.467827155181752		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 1.467827155181752 | validation: 2.6709178068447965]
	TIME [epoch: 10.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3964473236915915		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 1.3964473236915915 | validation: 2.687575108396278]
	TIME [epoch: 10.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4004747796275314		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 1.4004747796275314 | validation: 2.605529941635146]
	TIME [epoch: 10.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.38638833538436		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 1.38638833538436 | validation: 2.566099788766684]
	TIME [epoch: 10.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3926065580300553		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 1.3926065580300553 | validation: 2.549202843936062]
	TIME [epoch: 10.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.362564536756813		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 1.362564536756813 | validation: 2.5965928233227857]
	TIME [epoch: 10.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4045942626734207		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 1.4045942626734207 | validation: 2.5741114595208097]
	TIME [epoch: 10.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3684450225006468		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 1.3684450225006468 | validation: 2.558347150455345]
	TIME [epoch: 10.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3615915592678411		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 1.3615915592678411 | validation: 2.728316112807002]
	TIME [epoch: 10.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3922590383230053		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 1.3922590383230053 | validation: 2.5634059957627575]
	TIME [epoch: 10.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3820053347131847		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 1.3820053347131847 | validation: 2.5779120952142707]
	TIME [epoch: 10.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4031965182788309		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 1.4031965182788309 | validation: 2.5375269935140774]
	TIME [epoch: 10.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.355076709679284		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 1.355076709679284 | validation: 2.540037578684903]
	TIME [epoch: 10.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.354286929297769		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 1.354286929297769 | validation: 2.5742565986196473]
	TIME [epoch: 10.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.375066030579172		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 1.375066030579172 | validation: 2.624081535291047]
	TIME [epoch: 10.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.373761822860168		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 1.373761822860168 | validation: 2.650996885515723]
	TIME [epoch: 10.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4197783677439777		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 1.4197783677439777 | validation: 2.658770664112885]
	TIME [epoch: 10.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3888333688159513		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 1.3888333688159513 | validation: 2.5747135453979406]
	TIME [epoch: 10.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4120757482957598		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 1.4120757482957598 | validation: 2.5572474145241104]
	TIME [epoch: 10.4 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3551582137726461		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 1.3551582137726461 | validation: 2.5770637895393533]
	TIME [epoch: 10.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3984046114858502		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 1.3984046114858502 | validation: 2.65951859790697]
	TIME [epoch: 10.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4058808541981649		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 1.4058808541981649 | validation: 2.562736692675088]
	TIME [epoch: 10.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3698959349801492		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 1.3698959349801492 | validation: 2.5707098008534435]
	TIME [epoch: 10.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3717175516559914		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 1.3717175516559914 | validation: 2.578654023074342]
	TIME [epoch: 10.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.367066142180454		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 1.367066142180454 | validation: 2.617060830203058]
	TIME [epoch: 10.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3472899610311535		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 1.3472899610311535 | validation: 2.5704078650305457]
	TIME [epoch: 10.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.364143688484129		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 1.364143688484129 | validation: 2.683853111606045]
	TIME [epoch: 10.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3910751042844514		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 1.3910751042844514 | validation: 2.547702041155297]
	TIME [epoch: 10.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3771163601642598		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 1.3771163601642598 | validation: 2.62932616955581]
	TIME [epoch: 10.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4078279172612727		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 1.4078279172612727 | validation: 2.595623415107521]
	TIME [epoch: 10.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3852395795048622		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 1.3852395795048622 | validation: 2.537943969944485]
	TIME [epoch: 10.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3536666751257875		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 1.3536666751257875 | validation: 2.634542186764748]
	TIME [epoch: 10.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3912173315104437		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 1.3912173315104437 | validation: 2.5835775314505454]
	TIME [epoch: 10.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3569694338752976		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 1.3569694338752976 | validation: 2.5743876160140555]
	TIME [epoch: 10.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3645940841455015		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 1.3645940841455015 | validation: 2.6229719169169776]
	TIME [epoch: 10.4 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3563545301840105		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 1.3563545301840105 | validation: 2.5570264243647616]
	TIME [epoch: 10.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3525080456017722		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 1.3525080456017722 | validation: 2.58060389611166]
	TIME [epoch: 10.4 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.358272849810074		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 1.358272849810074 | validation: 2.7454087896784642]
	TIME [epoch: 10.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4376771483777033		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 1.4376771483777033 | validation: 2.5608582781371956]
	TIME [epoch: 10.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3813851350098112		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 1.3813851350098112 | validation: 2.6015173484776883]
	TIME [epoch: 10.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3568328624158812		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 1.3568328624158812 | validation: 2.6117208753273893]
	TIME [epoch: 10.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.401565642953921		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 1.401565642953921 | validation: 2.6041201586280898]
	TIME [epoch: 10.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3522109597348562		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 1.3522109597348562 | validation: 2.538998108148836]
	TIME [epoch: 10.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3705735634624172		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 1.3705735634624172 | validation: 2.575948629554499]
	TIME [epoch: 10.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3534571485033704		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 1.3534571485033704 | validation: 2.586811362528562]
	TIME [epoch: 10.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3511323671048117		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 1.3511323671048117 | validation: 2.5568945287175353]
	TIME [epoch: 10.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3821707791020839		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 1.3821707791020839 | validation: 2.566131127212905]
	TIME [epoch: 10.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4246259883413874		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 1.4246259883413874 | validation: 2.778504259627919]
	TIME [epoch: 10.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4077154517004664		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 1.4077154517004664 | validation: 2.5649378224154855]
	TIME [epoch: 10.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3438868579201928		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 1.3438868579201928 | validation: 2.604288574734135]
	TIME [epoch: 10.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3547250614139061		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 1.3547250614139061 | validation: 2.5476437333807582]
	TIME [epoch: 10.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3647662480515697		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 1.3647662480515697 | validation: 2.641026438060734]
	TIME [epoch: 10.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4178617756270913		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 1.4178617756270913 | validation: 2.651086800427221]
	TIME [epoch: 10.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3975040357506558		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 1.3975040357506558 | validation: 2.6604639711856]
	TIME [epoch: 10.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3711611968092758		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 1.3711611968092758 | validation: 2.529106791407153]
	TIME [epoch: 10.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3736088248987008		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 1.3736088248987008 | validation: 2.538017411014787]
	TIME [epoch: 10.4 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3593569671240184		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 1.3593569671240184 | validation: 2.6063122333260873]
	TIME [epoch: 10.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3935363345401974		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 1.3935363345401974 | validation: 2.6249206919134958]
	TIME [epoch: 10.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3534229956018329		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 1.3534229956018329 | validation: 2.55200139191855]
	TIME [epoch: 10.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3549981094108194		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 1.3549981094108194 | validation: 2.561769053171538]
	TIME [epoch: 10.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3520222441332452		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 1.3520222441332452 | validation: 2.533890878614757]
	TIME [epoch: 10.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.353992344843626		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 1.353992344843626 | validation: 2.5348415893644796]
	TIME [epoch: 10.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3663132855487108		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 1.3663132855487108 | validation: 2.5553216220134978]
	TIME [epoch: 10.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3718452524511555		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 1.3718452524511555 | validation: 2.648507623824023]
	TIME [epoch: 10.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3738489232852917		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 1.3738489232852917 | validation: 2.557653679547999]
	TIME [epoch: 10.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3430575950643333		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 1.3430575950643333 | validation: 2.5870262265821395]
	TIME [epoch: 10.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3826287811562312		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 1.3826287811562312 | validation: 2.5836722650135675]
	TIME [epoch: 10.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.461936493760004		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 1.461936493760004 | validation: 2.6257647575111185]
	TIME [epoch: 10.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3660316935752665		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 1.3660316935752665 | validation: 2.685392651834726]
	TIME [epoch: 10.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3712312894447376		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 1.3712312894447376 | validation: 2.5765541370544716]
	TIME [epoch: 10.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3347252801803706		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 1.3347252801803706 | validation: 2.5397559312300975]
	TIME [epoch: 10.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3328735531838274		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 1.3328735531838274 | validation: 2.556240445398341]
	TIME [epoch: 10.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3644779470403954		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 1.3644779470403954 | validation: 2.5593197466467745]
	TIME [epoch: 10.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.344097637392073		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 1.344097637392073 | validation: 2.617701199758212]
	TIME [epoch: 10.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3587422793570023		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 1.3587422793570023 | validation: 2.5778497416464807]
	TIME [epoch: 10.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3468942293011323		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 1.3468942293011323 | validation: 2.65823900616706]
	TIME [epoch: 10.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4062663294620474		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 1.4062663294620474 | validation: 2.5668876125149156]
	TIME [epoch: 10.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3447822501944173		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 1.3447822501944173 | validation: 2.5622489162110744]
	TIME [epoch: 10.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.413952114394748		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 1.413952114394748 | validation: 2.604697789057921]
	TIME [epoch: 10.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3917080558507582		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 1.3917080558507582 | validation: 2.5663179692995097]
	TIME [epoch: 10.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.360115463532398		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 1.360115463532398 | validation: 2.610937459944312]
	TIME [epoch: 10.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3585058424143566		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 1.3585058424143566 | validation: 2.552429425435107]
	TIME [epoch: 10.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3319692670562238		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 1.3319692670562238 | validation: 2.535944796919302]
	TIME [epoch: 10.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3373328674421114		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 1.3373328674421114 | validation: 2.5449911639803164]
	TIME [epoch: 10.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3512164793062778		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 1.3512164793062778 | validation: 2.5417710672418075]
	TIME [epoch: 10.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.374254369629699		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 1.374254369629699 | validation: 2.6031057013999703]
	TIME [epoch: 10.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.455631231243188		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 1.455631231243188 | validation: 2.5543922174217175]
	TIME [epoch: 10.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.379560223766892		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 1.379560223766892 | validation: 2.5745486121892642]
	TIME [epoch: 10.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4404462368636861		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 1.4404462368636861 | validation: 2.602467854455952]
	TIME [epoch: 10.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.374444487065108		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 1.374444487065108 | validation: 2.643275596172222]
	TIME [epoch: 10.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3563222122022924		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 1.3563222122022924 | validation: 2.636969978355572]
	TIME [epoch: 10.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.442793281628838		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 1.442793281628838 | validation: 2.53960548627791]
	TIME [epoch: 10.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3440868219024629		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 1.3440868219024629 | validation: 2.5523176013467164]
	TIME [epoch: 10.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3506939863496308		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 1.3506939863496308 | validation: 2.553710191575337]
	TIME [epoch: 10.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.35483133340062		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 1.35483133340062 | validation: 2.605829958590107]
	TIME [epoch: 10.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3499428873204207		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 1.3499428873204207 | validation: 2.5286906881291613]
	TIME [epoch: 10.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3952978571828039		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 1.3952978571828039 | validation: 2.5656064459512224]
	TIME [epoch: 10.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3697012994781945		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 1.3697012994781945 | validation: 2.569419478776505]
	TIME [epoch: 10.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3441754609706447		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 1.3441754609706447 | validation: 2.5302955081384586]
	TIME [epoch: 10.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3313013463130197		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 1.3313013463130197 | validation: 2.520454232429268]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.360590760351395		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 1.360590760351395 | validation: 2.528824087543649]
	TIME [epoch: 10.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3419152259804108		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 1.3419152259804108 | validation: 2.544886884036863]
	TIME [epoch: 10.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3594230418562163		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 1.3594230418562163 | validation: 2.590944306179165]
	TIME [epoch: 10.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3429713911083945		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 1.3429713911083945 | validation: 2.629730457961264]
	TIME [epoch: 10.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.374249805353275		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 1.374249805353275 | validation: 2.5502998425029997]
	TIME [epoch: 10.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3423800934217085		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 1.3423800934217085 | validation: 2.5795891672698077]
	TIME [epoch: 10.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3436513042215643		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 1.3436513042215643 | validation: 2.6006706339263452]
	TIME [epoch: 10.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.345226733474788		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 1.345226733474788 | validation: 2.563793624920418]
	TIME [epoch: 10.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3302888754681663		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 1.3302888754681663 | validation: 2.5553536952994325]
	TIME [epoch: 10.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3377181059138281		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 1.3377181059138281 | validation: 2.5740509286770563]
	TIME [epoch: 10.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3473414552301333		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 1.3473414552301333 | validation: 2.5592297603827645]
	TIME [epoch: 10.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3677657885114858		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 1.3677657885114858 | validation: 2.5552173341341553]
	TIME [epoch: 10.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3772310557634033		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 1.3772310557634033 | validation: 2.5440917924407045]
	TIME [epoch: 10.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.358421605875651		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 1.358421605875651 | validation: 2.5216070725232584]
	TIME [epoch: 10.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3473197418225766		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 1.3473197418225766 | validation: 2.5605269696724835]
	TIME [epoch: 10.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3879095958567553		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 1.3879095958567553 | validation: 2.650806629835661]
	TIME [epoch: 10.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3717304472173673		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 1.3717304472173673 | validation: 2.553514054315037]
	TIME [epoch: 10.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3613045865449482		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 1.3613045865449482 | validation: 2.555813288003906]
	TIME [epoch: 10.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.359460998084049		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 1.359460998084049 | validation: 2.6300079478404235]
	TIME [epoch: 10.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3837279948992847		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 1.3837279948992847 | validation: 2.5761763626301852]
	TIME [epoch: 10.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3461444388925687		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 1.3461444388925687 | validation: 2.5248247432084567]
	TIME [epoch: 10.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.368505105617603		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 1.368505105617603 | validation: 2.637008368962277]
	TIME [epoch: 10.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3827288159451245		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 1.3827288159451245 | validation: 2.6430391549363255]
	TIME [epoch: 10.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3809280697227009		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 1.3809280697227009 | validation: 2.538448560611587]
	TIME [epoch: 10.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3475248083050968		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 1.3475248083050968 | validation: 2.5903945314836525]
	TIME [epoch: 10.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3366013738220013		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 1.3366013738220013 | validation: 2.564572897962147]
	TIME [epoch: 10.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3364528522612118		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 1.3364528522612118 | validation: 2.5503212669787056]
	TIME [epoch: 10.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.376107434782592		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 1.376107434782592 | validation: 2.642163140127717]
	TIME [epoch: 10.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.396064446683798		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 1.396064446683798 | validation: 2.6027904065960037]
	TIME [epoch: 10.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3974171333100978		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 1.3974171333100978 | validation: 2.549964785716907]
	TIME [epoch: 10.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3542181237056012		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 1.3542181237056012 | validation: 2.589080193926535]
	TIME [epoch: 10.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3303724800500052		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 1.3303724800500052 | validation: 2.5492612681661178]
	TIME [epoch: 10.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3396964157397833		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 1.3396964157397833 | validation: 2.6059158623350407]
	TIME [epoch: 10.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.354362922858849		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 1.354362922858849 | validation: 2.6055093877156357]
	TIME [epoch: 10.4 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3828714156543935		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 1.3828714156543935 | validation: 2.6266769144293995]
	TIME [epoch: 10.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.35627786483872		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 1.35627786483872 | validation: 2.554997884054562]
	TIME [epoch: 10.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.351087252814787		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 1.351087252814787 | validation: 2.5488262058685267]
	TIME [epoch: 10.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3274996379094468		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 1.3274996379094468 | validation: 2.5780822741850167]
	TIME [epoch: 10.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3605132674698517		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 1.3605132674698517 | validation: 2.574132481968495]
	TIME [epoch: 10.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3498169762560324		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 1.3498169762560324 | validation: 2.562166733332428]
	TIME [epoch: 10.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3214448535065997		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 1.3214448535065997 | validation: 2.587958644166323]
	TIME [epoch: 10.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3311926849203988		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 1.3311926849203988 | validation: 2.604394081667025]
	TIME [epoch: 10.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3574525990829496		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 1.3574525990829496 | validation: 2.582260646285281]
	TIME [epoch: 10.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3462382639135413		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 1.3462382639135413 | validation: 2.589870285462388]
	TIME [epoch: 10.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.347646007491682		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 1.347646007491682 | validation: 2.5454102723544767]
	TIME [epoch: 10.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3391576420847104		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 1.3391576420847104 | validation: 2.623258512119721]
	TIME [epoch: 10.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3772330931156387		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 1.3772330931156387 | validation: 2.5870762352953722]
	TIME [epoch: 10.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3456015791516593		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 1.3456015791516593 | validation: 2.5662073169791744]
	TIME [epoch: 10.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.372930074870753		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 1.372930074870753 | validation: 2.5818483081365002]
	TIME [epoch: 10.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.401589752779771		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 1.401589752779771 | validation: 2.5855560857216027]
	TIME [epoch: 10.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3490810806935278		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 1.3490810806935278 | validation: 2.626458809885834]
	TIME [epoch: 10.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3687024142032622		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 1.3687024142032622 | validation: 2.559147598351383]
	TIME [epoch: 10.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.348966072705094		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 1.348966072705094 | validation: 2.5174212648411918]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3308446971865473		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 1.3308446971865473 | validation: 2.527883129947209]
	TIME [epoch: 10.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3332084690802382		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 1.3332084690802382 | validation: 2.532161565418334]
	TIME [epoch: 10.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3356335348129762		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 1.3356335348129762 | validation: 2.6281808610241875]
	TIME [epoch: 10.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.4119318519193738		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 1.4119318519193738 | validation: 2.5319412457397505]
	TIME [epoch: 10.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.340373454100132		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 1.340373454100132 | validation: 2.533431889909275]
	TIME [epoch: 10.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.330414500808141		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 1.330414500808141 | validation: 2.534720056654504]
	TIME [epoch: 10.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.328916872203172		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 1.328916872203172 | validation: 2.5488296356104327]
	TIME [epoch: 10.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.366337406596773		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 1.366337406596773 | validation: 2.68369464359463]
	TIME [epoch: 10.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3678525169980769		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 1.3678525169980769 | validation: 2.52265699978437]
	TIME [epoch: 10.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3126725480177586		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 1.3126725480177586 | validation: 2.5735520315442737]
	TIME [epoch: 10.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3423525657793869		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 1.3423525657793869 | validation: 2.5606458229925306]
	TIME [epoch: 10.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3601781751792967		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 1.3601781751792967 | validation: 2.5268398516866823]
	TIME [epoch: 10.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3566834725040402		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 1.3566834725040402 | validation: 2.5721025793489707]
	TIME [epoch: 10.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3663476555270662		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 1.3663476555270662 | validation: 2.5427674183330087]
	TIME [epoch: 10.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3357178420188602		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 1.3357178420188602 | validation: 2.583495171647153]
	TIME [epoch: 10.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.383526401319565		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 1.383526401319565 | validation: 2.5458431017035332]
	TIME [epoch: 10.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3378269062373866		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 1.3378269062373866 | validation: 2.6094459872779914]
	TIME [epoch: 10.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3713935583211214		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 1.3713935583211214 | validation: 2.528590384007214]
	TIME [epoch: 10.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3197012171269502		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 1.3197012171269502 | validation: 2.5794751667477476]
	TIME [epoch: 10.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.336195028411654		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 1.336195028411654 | validation: 2.525158599781986]
	TIME [epoch: 10.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3249122409463385		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 1.3249122409463385 | validation: 2.602050621346682]
	TIME [epoch: 10.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3438796712448249		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 1.3438796712448249 | validation: 2.5413290306434986]
	TIME [epoch: 10.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3359907757552922		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 1.3359907757552922 | validation: 2.5421372446687336]
	TIME [epoch: 10.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3320567485034551		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 1.3320567485034551 | validation: 2.611731454913929]
	TIME [epoch: 10.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3829412779432761		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 1.3829412779432761 | validation: 2.5535721084470504]
	TIME [epoch: 10.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.332594545887084		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 1.332594545887084 | validation: 2.550800082007048]
	TIME [epoch: 10.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3398550481023814		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 1.3398550481023814 | validation: 2.5833907691090485]
	TIME [epoch: 10.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.356236536712898		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 1.356236536712898 | validation: 2.5730614558851728]
	TIME [epoch: 10.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3436682907063946		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 1.3436682907063946 | validation: 2.577009159121177]
	TIME [epoch: 10.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3370594623811123		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 1.3370594623811123 | validation: 2.5622988283929202]
	TIME [epoch: 10.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3426747947854938		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 1.3426747947854938 | validation: 2.556457147190577]
	TIME [epoch: 10.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3411587673322942		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 1.3411587673322942 | validation: 2.5398230270565527]
	TIME [epoch: 10.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3417306108757527		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 1.3417306108757527 | validation: 2.630183140330288]
	TIME [epoch: 10.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3518686505149402		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 1.3518686505149402 | validation: 2.5506767608195737]
	TIME [epoch: 10.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3258467326170116		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 1.3258467326170116 | validation: 2.5370886304221982]
	TIME [epoch: 10.4 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3510669172425283		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 1.3510669172425283 | validation: 2.5799946345917495]
	TIME [epoch: 10.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.337977182029197		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 1.337977182029197 | validation: 2.5408766928381663]
	TIME [epoch: 10.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.33521601037837		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 1.33521601037837 | validation: 2.6091499143501244]
	TIME [epoch: 10.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.382874313447007		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 1.382874313447007 | validation: 2.5691279848400645]
	TIME [epoch: 10.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3435731198474257		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 1.3435731198474257 | validation: 2.6463201790053144]
	TIME [epoch: 10.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3340047349144335		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 1.3340047349144335 | validation: 2.532261119417081]
	TIME [epoch: 10.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3280144451910998		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 1.3280144451910998 | validation: 2.65015828814584]
	TIME [epoch: 10.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3690522509236593		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 1.3690522509236593 | validation: 2.602581881235516]
	TIME [epoch: 10.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3705009437306601		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 1.3705009437306601 | validation: 2.5174750545856677]
	TIME [epoch: 10.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3428281428003304		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 1.3428281428003304 | validation: 2.5499792658110683]
	TIME [epoch: 10.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3322071921806469		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 1.3322071921806469 | validation: 2.599061077612576]
	TIME [epoch: 10.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3568901057826797		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 1.3568901057826797 | validation: 2.527898223782719]
	TIME [epoch: 10.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.329846784579019		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 1.329846784579019 | validation: 2.5542204595475555]
	TIME [epoch: 10.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3231641702233243		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 1.3231641702233243 | validation: 2.5914160307078293]
	TIME [epoch: 10.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3433111390997938		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 1.3433111390997938 | validation: 2.550870338066178]
	TIME [epoch: 10.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3579804030812297		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 1.3579804030812297 | validation: 2.5222531218221134]
	TIME [epoch: 10.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3976516111104451		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 1.3976516111104451 | validation: 2.576144543990953]
	TIME [epoch: 10.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3450826283080388		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 1.3450826283080388 | validation: 2.5267083493729827]
	TIME [epoch: 10.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3236727390820635		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 1.3236727390820635 | validation: 2.5525099765706534]
	TIME [epoch: 10.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323564958523658		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 1.323564958523658 | validation: 2.5559131027770223]
	TIME [epoch: 10.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3566866001034055		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 1.3566866001034055 | validation: 2.528815910865482]
	TIME [epoch: 10.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3341598845583147		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 1.3341598845583147 | validation: 2.555376692582725]
	TIME [epoch: 10.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3318870165754757		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 1.3318870165754757 | validation: 2.5879048333653834]
	TIME [epoch: 10.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.326724734146074		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 1.326724734146074 | validation: 2.5512300453610677]
	TIME [epoch: 10.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3701654942693016		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 1.3701654942693016 | validation: 2.711468677058905]
	TIME [epoch: 10.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3926761850739218		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 1.3926761850739218 | validation: 2.530401029826929]
	TIME [epoch: 10.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3190857585862361		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 1.3190857585862361 | validation: 2.5979742917628816]
	TIME [epoch: 10.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3212743685142014		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 1.3212743685142014 | validation: 2.5637230507839797]
	TIME [epoch: 10.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.348765445101399		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 1.348765445101399 | validation: 2.7462232333788057]
	TIME [epoch: 10.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3840980954324347		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 1.3840980954324347 | validation: 2.5232727856445107]
	TIME [epoch: 10.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3200471785972048		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 1.3200471785972048 | validation: 2.5299638301051037]
	TIME [epoch: 10.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3107431138684602		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 1.3107431138684602 | validation: 2.650380268532997]
	TIME [epoch: 10.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3275052313697215		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 1.3275052313697215 | validation: 2.555331173405781]
	TIME [epoch: 10.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323745816064784		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 1.323745816064784 | validation: 2.5833776013610885]
	TIME [epoch: 10.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3526694459570927		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 1.3526694459570927 | validation: 2.5586447566958275]
	TIME [epoch: 10.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3290876897010893		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 1.3290876897010893 | validation: 2.5325895227922937]
	TIME [epoch: 10.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3259537829238142		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 1.3259537829238142 | validation: 2.5269433819257396]
	TIME [epoch: 10.3 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3283835861431144		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 1.3283835861431144 | validation: 2.58520470383103]
	TIME [epoch: 10.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3248005892971642		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 1.3248005892971642 | validation: 2.5486944361205666]
	TIME [epoch: 10.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3384432929120507		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 1.3384432929120507 | validation: 2.5635793963755122]
	TIME [epoch: 10.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3394477991446936		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 1.3394477991446936 | validation: 2.5365970344787274]
	TIME [epoch: 10.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3243665924810892		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 1.3243665924810892 | validation: 2.573754464555491]
	TIME [epoch: 10.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3501000809974204		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 1.3501000809974204 | validation: 2.524850013274315]
	TIME [epoch: 10.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3167588265386942		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 1.3167588265386942 | validation: 2.5719889282581083]
	TIME [epoch: 10.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3745005274391677		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 1.3745005274391677 | validation: 2.5242365217229104]
	TIME [epoch: 10.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3227541851764626		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 1.3227541851764626 | validation: 2.5917353474467166]
	TIME [epoch: 10.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3434497437610244		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 1.3434497437610244 | validation: 2.5276429016734]
	TIME [epoch: 10.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3324112307870646		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 1.3324112307870646 | validation: 2.6195006976198023]
	TIME [epoch: 10.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.329322326346076		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 1.329322326346076 | validation: 2.549444900434573]
	TIME [epoch: 10.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3293845939455653		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 1.3293845939455653 | validation: 2.5349749667512786]
	TIME [epoch: 10.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3298391154992504		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 1.3298391154992504 | validation: 2.523384074305124]
	TIME [epoch: 10.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3211546204832965		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 1.3211546204832965 | validation: 2.570868647728303]
	TIME [epoch: 10.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3434295517100754		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 1.3434295517100754 | validation: 2.6019739120748633]
	TIME [epoch: 10.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.344913479922577		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 1.344913479922577 | validation: 2.562084977685758]
	TIME [epoch: 10.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.335082287913891		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 1.335082287913891 | validation: 2.5959318767686557]
	TIME [epoch: 10.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3427847830012374		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 1.3427847830012374 | validation: 2.567955708364043]
	TIME [epoch: 10.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3369769281541513		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 1.3369769281541513 | validation: 2.5335127392668686]
	TIME [epoch: 10.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3228887035261365		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 1.3228887035261365 | validation: 2.5156965474944752]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3250221871509607		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 1.3250221871509607 | validation: 2.534018966481211]
	TIME [epoch: 10.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3238380387366182		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 1.3238380387366182 | validation: 2.564847008129277]
	TIME [epoch: 10.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3176835272926715		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 1.3176835272926715 | validation: 2.535500536862254]
	TIME [epoch: 10.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3195071609212492		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 1.3195071609212492 | validation: 2.580874112366193]
	TIME [epoch: 10.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3393487607735282		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 1.3393487607735282 | validation: 2.5266453931383586]
	TIME [epoch: 10.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3191014723465635		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 1.3191014723465635 | validation: 2.5618716363237857]
	TIME [epoch: 10.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3117632244361745		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 1.3117632244361745 | validation: 2.601691917661524]
	TIME [epoch: 10.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3187958734977372		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 1.3187958734977372 | validation: 2.518206972556685]
	TIME [epoch: 10.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3160339105553456		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 1.3160339105553456 | validation: 2.5306817643098563]
	TIME [epoch: 10.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.31702007867731		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 1.31702007867731 | validation: 2.5135275692269903]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_823.pth
	Model improved!!!
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3205031768446487		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 1.3205031768446487 | validation: 2.5161472896361636]
	TIME [epoch: 10.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3412134519699435		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 1.3412134519699435 | validation: 2.5395141716713066]
	TIME [epoch: 10.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.340223714850991		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 1.340223714850991 | validation: 2.592474410980054]
	TIME [epoch: 10.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3580487054416734		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 1.3580487054416734 | validation: 2.510779640152575]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3192476228046544		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 1.3192476228046544 | validation: 2.5276449024180776]
	TIME [epoch: 10.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3250403087163911		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 1.3250403087163911 | validation: 2.5526529973682357]
	TIME [epoch: 10.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3341078880252704		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 1.3341078880252704 | validation: 2.5451590099372265]
	TIME [epoch: 10.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3521888864771772		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 1.3521888864771772 | validation: 2.6053802116998237]
	TIME [epoch: 10.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3566658624481693		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 1.3566658624481693 | validation: 2.575368289996485]
	TIME [epoch: 10.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3116804722919835		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 1.3116804722919835 | validation: 2.513351801493761]
	TIME [epoch: 10.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3287377046496078		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 1.3287377046496078 | validation: 2.5446407843435903]
	TIME [epoch: 10.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3264147001575886		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 1.3264147001575886 | validation: 2.554541933581285]
	TIME [epoch: 10.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.321812507576978		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 1.321812507576978 | validation: 2.535499408535103]
	TIME [epoch: 10.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3316373618058175		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 1.3316373618058175 | validation: 2.568441095489876]
	TIME [epoch: 10.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3391161521722945		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 1.3391161521722945 | validation: 2.5396233405584407]
	TIME [epoch: 10.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.36957195567981		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 1.36957195567981 | validation: 2.647168425577031]
	TIME [epoch: 10.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3371463657930203		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 1.3371463657930203 | validation: 2.5364543314159538]
	TIME [epoch: 10.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3325284609819663		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 1.3325284609819663 | validation: 2.5172086549097608]
	TIME [epoch: 10.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3231952679162313		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 1.3231952679162313 | validation: 2.5241556108575574]
	TIME [epoch: 10.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3234897241074555		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 1.3234897241074555 | validation: 2.5482411172174695]
	TIME [epoch: 10.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3221783258794293		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 1.3221783258794293 | validation: 2.552703082066635]
	TIME [epoch: 10.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3179274926056448		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 1.3179274926056448 | validation: 2.518033927027456]
	TIME [epoch: 10.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3387547906063266		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 1.3387547906063266 | validation: 2.525382201084294]
	TIME [epoch: 10.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3237149147026268		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 1.3237149147026268 | validation: 2.5918903382062797]
	TIME [epoch: 10.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3299331393705258		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 1.3299331393705258 | validation: 2.5160457795923783]
	TIME [epoch: 10.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.311561155709572		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 1.311561155709572 | validation: 2.527768104491015]
	TIME [epoch: 10.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3365225204131308		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 1.3365225204131308 | validation: 2.5347896560917973]
	TIME [epoch: 10.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.324075785964602		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 1.324075785964602 | validation: 2.5173261802730473]
	TIME [epoch: 10.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3226463482545272		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 1.3226463482545272 | validation: 2.585158281118355]
	TIME [epoch: 10.4 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3110636442929031		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 1.3110636442929031 | validation: 2.5417584207562705]
	TIME [epoch: 10.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3170672955153828		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 1.3170672955153828 | validation: 2.5711244593406435]
	TIME [epoch: 10.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3545449226127644		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 1.3545449226127644 | validation: 2.5873411256743544]
	TIME [epoch: 10.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3621369927414748		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 1.3621369927414748 | validation: 2.519613964755555]
	TIME [epoch: 10.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3207634770323424		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 1.3207634770323424 | validation: 2.5320831676822064]
	TIME [epoch: 10.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3490499606166044		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 1.3490499606166044 | validation: 2.6119588816596053]
	TIME [epoch: 10.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3320990164632955		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 1.3320990164632955 | validation: 2.538128345246413]
	TIME [epoch: 10.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3353176624423277		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 1.3353176624423277 | validation: 2.506947735655497]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3273275416924832		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 1.3273275416924832 | validation: 2.5301947882658795]
	TIME [epoch: 10.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.308783070319988		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 1.308783070319988 | validation: 2.5256507498567986]
	TIME [epoch: 10.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.324418397688739		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 1.324418397688739 | validation: 2.554456281910406]
	TIME [epoch: 10.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3162059391386605		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 1.3162059391386605 | validation: 2.531648178663573]
	TIME [epoch: 10.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3338706600043242		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 1.3338706600043242 | validation: 2.551538651907358]
	TIME [epoch: 10.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3385679494431888		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 1.3385679494431888 | validation: 2.5402950654094245]
	TIME [epoch: 10.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3142656555345233		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 1.3142656555345233 | validation: 2.530743780516293]
	TIME [epoch: 10.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3247242932684875		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 1.3247242932684875 | validation: 2.6133097188224133]
	TIME [epoch: 10.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3276935133837955		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 1.3276935133837955 | validation: 2.5465796391558913]
	TIME [epoch: 10.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323155848701923		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 1.323155848701923 | validation: 2.515369464241551]
	TIME [epoch: 10.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3294009094757147		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 1.3294009094757147 | validation: 2.551700683851012]
	TIME [epoch: 10.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.320074796115114		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 1.320074796115114 | validation: 2.519842106779858]
	TIME [epoch: 10.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.341929259201689		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 1.341929259201689 | validation: 2.5243124648998463]
	TIME [epoch: 10.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3362141046302853		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 1.3362141046302853 | validation: 2.5183311223965803]
	TIME [epoch: 10.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3470012771609683		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 1.3470012771609683 | validation: 2.535852777346822]
	TIME [epoch: 10.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3337273506582106		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 1.3337273506582106 | validation: 2.5456568572248437]
	TIME [epoch: 10.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3712767709732072		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 1.3712767709732072 | validation: 2.5165309102829783]
	TIME [epoch: 10.4 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3131443963878582		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 1.3131443963878582 | validation: 2.5289545867514844]
	TIME [epoch: 10.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.332100929555629		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 1.332100929555629 | validation: 2.5197404927730362]
	TIME [epoch: 10.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.32222719673541		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 1.32222719673541 | validation: 2.5225696332311704]
	TIME [epoch: 10.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3313789872653523		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 1.3313789872653523 | validation: 2.5612976416189395]
	TIME [epoch: 10.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.330720798659221		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 1.330720798659221 | validation: 2.5396277185098874]
	TIME [epoch: 10.4 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3313630364560236		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 1.3313630364560236 | validation: 2.5397511740106054]
	TIME [epoch: 10.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3329087889309952		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 1.3329087889309952 | validation: 2.5549961674101955]
	TIME [epoch: 10.4 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3137536535603458		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 1.3137536535603458 | validation: 2.5427569705715696]
	TIME [epoch: 10.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3438783797357063		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 1.3438783797357063 | validation: 2.5507410559233272]
	TIME [epoch: 10.4 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3115060642336838		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 1.3115060642336838 | validation: 2.5679157456779165]
	TIME [epoch: 10.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3199155519627075		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 1.3199155519627075 | validation: 2.52587710344827]
	TIME [epoch: 10.4 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3145288056965447		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 1.3145288056965447 | validation: 2.5390282429861144]
	TIME [epoch: 10.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3179136020206181		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 1.3179136020206181 | validation: 2.5224161598771433]
	TIME [epoch: 10.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3149676755033302		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 1.3149676755033302 | validation: 2.5460607177568098]
	TIME [epoch: 10.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.336193530474885		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 1.336193530474885 | validation: 2.582956136494642]
	TIME [epoch: 10.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3209554825008265		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 1.3209554825008265 | validation: 2.5407418731378932]
	TIME [epoch: 10.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.330537445417299		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 1.330537445417299 | validation: 2.535373936174726]
	TIME [epoch: 10.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3348883331648822		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 1.3348883331648822 | validation: 2.575054180654845]
	TIME [epoch: 10.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.353518380823325		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 1.353518380823325 | validation: 2.57335459641632]
	TIME [epoch: 10.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.334417974765975		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 1.334417974765975 | validation: 2.581308372727877]
	TIME [epoch: 10.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.333993655120263		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 1.333993655120263 | validation: 2.6183894519383735]
	TIME [epoch: 10.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3494579531017277		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 1.3494579531017277 | validation: 2.569853809031081]
	TIME [epoch: 10.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3305067222418407		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 1.3305067222418407 | validation: 2.5620322021055335]
	TIME [epoch: 10.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.329462870509162		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 1.329462870509162 | validation: 2.547824858659326]
	TIME [epoch: 10.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3286322501460268		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 1.3286322501460268 | validation: 2.518245707093947]
	TIME [epoch: 10.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.327540986138556		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 1.327540986138556 | validation: 2.5168198167215157]
	TIME [epoch: 10.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3198294893127374		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 1.3198294893127374 | validation: 2.5248197340589544]
	TIME [epoch: 10.4 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3171395056902573		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 1.3171395056902573 | validation: 2.5331487273078266]
	TIME [epoch: 10.4 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3375442082470521		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 1.3375442082470521 | validation: 2.5219668846240952]
	TIME [epoch: 10.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3163571028332974		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 1.3163571028332974 | validation: 2.5175322659714205]
	TIME [epoch: 10.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3260950470818638		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 1.3260950470818638 | validation: 2.510529859718915]
	TIME [epoch: 10.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3131721590564436		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 1.3131721590564436 | validation: 2.541395372025779]
	TIME [epoch: 10.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3172617372690318		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 1.3172617372690318 | validation: 2.561283720527415]
	TIME [epoch: 10.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3207506549631485		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 1.3207506549631485 | validation: 2.5464409061631628]
	TIME [epoch: 10.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323906538058408		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 1.323906538058408 | validation: 2.5303838043441225]
	TIME [epoch: 10.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3137847740443553		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 1.3137847740443553 | validation: 2.5245081128970854]
	TIME [epoch: 10.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.336475075844333		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 1.336475075844333 | validation: 2.5338350523346036]
	TIME [epoch: 10.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3189824056436514		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 1.3189824056436514 | validation: 2.5649129046435815]
	TIME [epoch: 10.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3094374154545314		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 1.3094374154545314 | validation: 2.5433344120606542]
	TIME [epoch: 10.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.327633014330909		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 1.327633014330909 | validation: 2.5569548317431376]
	TIME [epoch: 10.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3188369177522752		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 1.3188369177522752 | validation: 2.5408709494812807]
	TIME [epoch: 10.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3158548933396597		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 1.3158548933396597 | validation: 2.582526237648101]
	TIME [epoch: 10.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3291740475291878		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 1.3291740475291878 | validation: 2.5295323338188656]
	TIME [epoch: 10.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3209870306011857		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 1.3209870306011857 | validation: 2.5248530350956866]
	TIME [epoch: 10.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3178863560039678		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 1.3178863560039678 | validation: 2.5678848163457024]
	TIME [epoch: 10.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3380531423394144		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 1.3380531423394144 | validation: 2.531629463657857]
	TIME [epoch: 10.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3144336928487763		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 1.3144336928487763 | validation: 2.5351312099959307]
	TIME [epoch: 10.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3187667909024299		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 1.3187667909024299 | validation: 2.514143873829172]
	TIME [epoch: 10.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3182438592352432		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 1.3182438592352432 | validation: 2.5222617012204482]
	TIME [epoch: 10.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3200818317950804		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 1.3200818317950804 | validation: 2.5233528176340245]
	TIME [epoch: 10.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3186364337803809		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 1.3186364337803809 | validation: 2.533666501934154]
	TIME [epoch: 10.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3392283561867258		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 1.3392283561867258 | validation: 2.543401076146658]
	TIME [epoch: 10.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3206753794439867		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 1.3206753794439867 | validation: 2.5290830976662204]
	TIME [epoch: 10.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3271331683222176		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 1.3271331683222176 | validation: 2.633846223134608]
	TIME [epoch: 10.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3137589972441663		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 1.3137589972441663 | validation: 2.530501554099041]
	TIME [epoch: 10.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.314687149416648		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 1.314687149416648 | validation: 2.521236228454025]
	TIME [epoch: 10.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3155260476526422		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 1.3155260476526422 | validation: 2.511824425579467]
	TIME [epoch: 10.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3114480598001859		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 1.3114480598001859 | validation: 2.5199664714432366]
	TIME [epoch: 10.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3182824890601412		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 1.3182824890601412 | validation: 2.516412412328171]
	TIME [epoch: 10.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3144723685080746		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 1.3144723685080746 | validation: 2.5811361195576]
	TIME [epoch: 10.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3198950997831846		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 1.3198950997831846 | validation: 2.5330543095205247]
	TIME [epoch: 10.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3082952820278924		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 1.3082952820278924 | validation: 2.5181187487884293]
	TIME [epoch: 10.4 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3185880347425647		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 1.3185880347425647 | validation: 2.5240237854741383]
	TIME [epoch: 10.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049346270430173		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 1.3049346270430173 | validation: 2.530248741877011]
	TIME [epoch: 10.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.322322516629138		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 1.322322516629138 | validation: 2.5468329838335264]
	TIME [epoch: 10.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.315644772835538		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 1.315644772835538 | validation: 2.5827517459726512]
	TIME [epoch: 10.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3089292162295745		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 1.3089292162295745 | validation: 2.5636306096694184]
	TIME [epoch: 10.4 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3195345293580056		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 1.3195345293580056 | validation: 2.565295767143916]
	TIME [epoch: 10.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3220382965110185		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 1.3220382965110185 | validation: 2.568739072940372]
	TIME [epoch: 10.4 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3214633859164482		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 1.3214633859164482 | validation: 2.555981519775394]
	TIME [epoch: 10.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3116729179840771		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 1.3116729179840771 | validation: 2.63111190577164]
	TIME [epoch: 10.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3480269679055858		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 1.3480269679055858 | validation: 2.5273436829423224]
	TIME [epoch: 10.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3089895481995266		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 1.3089895481995266 | validation: 2.527137102375295]
	TIME [epoch: 10.4 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.311774889868202		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 1.311774889868202 | validation: 2.521638055882936]
	TIME [epoch: 10.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3164917148679884		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 1.3164917148679884 | validation: 2.5222883737924646]
	TIME [epoch: 10.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3234608560778685		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 1.3234608560778685 | validation: 2.510567857385634]
	TIME [epoch: 10.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.318062944184808		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 1.318062944184808 | validation: 2.5181831659558966]
	TIME [epoch: 10.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.317421073743903		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 1.317421073743903 | validation: 2.5090166556069464]
	TIME [epoch: 10.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049750143509329		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 1.3049750143509329 | validation: 2.5642113625243588]
	TIME [epoch: 10.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3194924892179032		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 1.3194924892179032 | validation: 2.5654666411992526]
	TIME [epoch: 10.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3036372228988422		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 1.3036372228988422 | validation: 2.5383483392673885]
	TIME [epoch: 10.4 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3299453730743198		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 1.3299453730743198 | validation: 2.5325799739386983]
	TIME [epoch: 10.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3519804640265973		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 1.3519804640265973 | validation: 2.537734987672153]
	TIME [epoch: 10.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3057067125639146		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 1.3057067125639146 | validation: 2.5565565411133235]
	TIME [epoch: 10.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3115800784957996		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 1.3115800784957996 | validation: 2.5207645612576237]
	TIME [epoch: 10.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3265626495872853		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 1.3265626495872853 | validation: 2.5330534167906165]
	TIME [epoch: 10.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3283785303641233		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 1.3283785303641233 | validation: 2.5763699492110277]
	TIME [epoch: 10.4 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3242929916057897		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 1.3242929916057897 | validation: 2.52690524432049]
	TIME [epoch: 10.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30924119513001		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 1.30924119513001 | validation: 2.542398032224903]
	TIME [epoch: 10.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3152622368611329		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 1.3152622368611329 | validation: 2.532893235113408]
	TIME [epoch: 10.4 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3193981084975488		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 1.3193981084975488 | validation: 2.5221039934220104]
	TIME [epoch: 10.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3237932758447006		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 1.3237932758447006 | validation: 2.5218895261987035]
	TIME [epoch: 10.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.322764662428885		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 1.322764662428885 | validation: 2.5158682422272607]
	TIME [epoch: 10.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3201594130391887		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 1.3201594130391887 | validation: 2.5714759991439418]
	TIME [epoch: 10.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.33003974909199		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 1.33003974909199 | validation: 2.618545867669601]
	TIME [epoch: 10.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.325804512317044		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 1.325804512317044 | validation: 2.5355544119709905]
	TIME [epoch: 10.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307022484327551		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 1.307022484327551 | validation: 2.5191322201128377]
	TIME [epoch: 10.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3155108437240501		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 1.3155108437240501 | validation: 2.5226431326347014]
	TIME [epoch: 10.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3162368169104948		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 1.3162368169104948 | validation: 2.523334884642234]
	TIME [epoch: 10.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3147482263559855		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 1.3147482263559855 | validation: 2.5144795873296304]
	TIME [epoch: 10.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3227020000662624		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 1.3227020000662624 | validation: 2.5416841842576963]
	TIME [epoch: 10.4 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3151152272179583		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 1.3151152272179583 | validation: 2.5299085427539967]
	TIME [epoch: 10.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3139319591691123		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 1.3139319591691123 | validation: 2.5413241772168864]
	TIME [epoch: 10.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3159882758614558		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 1.3159882758614558 | validation: 2.52125543598599]
	TIME [epoch: 10.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3078118546742923		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 1.3078118546742923 | validation: 2.5143372390709238]
	TIME [epoch: 10.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3259966248125385		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 1.3259966248125385 | validation: 2.529412823488673]
	TIME [epoch: 10.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3410096485246745		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 1.3410096485246745 | validation: 2.5170772018942786]
	TIME [epoch: 10.4 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3183070454918273		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 1.3183070454918273 | validation: 2.521042237150779]
	TIME [epoch: 10.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3070966569437077		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 1.3070966569437077 | validation: 2.5231391686643314]
	TIME [epoch: 10.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3181713725530788		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 1.3181713725530788 | validation: 2.5710462406509325]
	TIME [epoch: 10.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.334531812358207		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 1.334531812358207 | validation: 2.5352854662223026]
	TIME [epoch: 10.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3314108880414004		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 1.3314108880414004 | validation: 2.5894282783621954]
	TIME [epoch: 10.4 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.334192801828606		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 1.334192801828606 | validation: 2.5470227798365634]
	TIME [epoch: 10.4 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3198142315462444		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 1.3198142315462444 | validation: 2.5753873194079597]
	TIME [epoch: 10.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.311828459395554		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 1.311828459395554 | validation: 2.5800587795891494]
	TIME [epoch: 10.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.315602857720402		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 1.315602857720402 | validation: 2.526782328651301]
	TIME [epoch: 10.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3234904474438534		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 1.3234904474438534 | validation: 2.532998912274241]
	TIME [epoch: 10.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323605703143413		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 1.323605703143413 | validation: 2.533218882833365]
	TIME [epoch: 10.4 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3115663892630915		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 1.3115663892630915 | validation: 2.520753729145217]
	TIME [epoch: 10.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3217826516604334		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 1.3217826516604334 | validation: 2.5351710422854308]
	TIME [epoch: 10.4 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.362029739516752		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 1.362029739516752 | validation: 2.5259979049150867]
	TIME [epoch: 10.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3153013463406489		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 1.3153013463406489 | validation: 2.5296601110822863]
	TIME [epoch: 10.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3136723880017265		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 1.3136723880017265 | validation: 2.5146835993892847]
	TIME [epoch: 10.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3112779544825597		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 1.3112779544825597 | validation: 2.5254279175483125]
	TIME [epoch: 10.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3126336056775951		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 1.3126336056775951 | validation: 2.5316282045194836]
	TIME [epoch: 10.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3068275396522786		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 1.3068275396522786 | validation: 2.520569739932007]
	TIME [epoch: 10.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3081481124803542		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 1.3081481124803542 | validation: 2.5105851555445318]
	TIME [epoch: 10.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3010614358441865		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 1.3010614358441865 | validation: 2.512347536645966]
	TIME [epoch: 10.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3073512791353097		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 1.3073512791353097 | validation: 2.5961645446458195]
	TIME [epoch: 10.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.326926475355099		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 1.326926475355099 | validation: 2.525178904073152]
	TIME [epoch: 10.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3164456394815705		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 1.3164456394815705 | validation: 2.5640650967901832]
	TIME [epoch: 10.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3069147495710116		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 1.3069147495710116 | validation: 2.520674121189327]
	TIME [epoch: 10.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049472574967218		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 1.3049472574967218 | validation: 2.533874649234656]
	TIME [epoch: 10.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3238846784312217		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 1.3238846784312217 | validation: 2.5429139668762017]
	TIME [epoch: 10.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3260898463498925		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 1.3260898463498925 | validation: 2.5173565337234254]
	TIME [epoch: 10.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.323085161301257		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 1.323085161301257 | validation: 2.567048309818962]
	TIME [epoch: 10.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30605409332973		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 1.30605409332973 | validation: 2.536896382196101]
	TIME [epoch: 10.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.312063871763343		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 1.312063871763343 | validation: 2.5273298196064706]
	TIME [epoch: 10.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3119303140728331		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 1.3119303140728331 | validation: 2.5410188283335238]
	TIME [epoch: 10.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3270251461754117		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 1.3270251461754117 | validation: 2.5274605917001263]
	TIME [epoch: 10.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3187338390559205		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 1.3187338390559205 | validation: 2.501216726919196]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.312284750221817		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 1.312284750221817 | validation: 2.5178222377816]
	TIME [epoch: 10.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3191092235739053		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 1.3191092235739053 | validation: 2.6039808532614566]
	TIME [epoch: 10.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3359231617837424		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 1.3359231617837424 | validation: 2.5130694497775887]
	TIME [epoch: 10.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.315877032678229		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 1.315877032678229 | validation: 2.504757770162567]
	TIME [epoch: 10.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304662751003768		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 1.304662751003768 | validation: 2.5254817437577284]
	TIME [epoch: 10.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.312974759233357		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 1.312974759233357 | validation: 2.5152406543849812]
	TIME [epoch: 10.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3139613561637702		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 1.3139613561637702 | validation: 2.5290407995394095]
	TIME [epoch: 10.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3090391847170415		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 1.3090391847170415 | validation: 2.5224114834418225]
	TIME [epoch: 10.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3067321886817465		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 1.3067321886817465 | validation: 2.5227776990876265]
	TIME [epoch: 10.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3081639927417885		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 1.3081639927417885 | validation: 2.508731221070238]
	TIME [epoch: 10.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3072673876691794		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 1.3072673876691794 | validation: 2.5354933531280044]
	TIME [epoch: 10.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3099895599643296		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 1.3099895599643296 | validation: 2.5304305247097463]
	TIME [epoch: 10.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30822793824193		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 1.30822793824193 | validation: 2.5186154420065776]
	TIME [epoch: 10.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3108993200575807		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 1.3108993200575807 | validation: 2.5195950914270835]
	TIME [epoch: 10.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3115617336926997		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 1.3115617336926997 | validation: 2.516212494320532]
	TIME [epoch: 10.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3069533217941207		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 1.3069533217941207 | validation: 2.5286710587470216]
	TIME [epoch: 10.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3228028917503094		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 1.3228028917503094 | validation: 2.5266403889690277]
	TIME [epoch: 10.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3140451521560506		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 1.3140451521560506 | validation: 2.5352974135897233]
	TIME [epoch: 10.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302555161955338		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 1.302555161955338 | validation: 2.5062857540206465]
	TIME [epoch: 10.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3141999530021458		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 1.3141999530021458 | validation: 2.5084694638047917]
	TIME [epoch: 10.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.309169977874534		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 1.309169977874534 | validation: 2.5675509568650514]
	TIME [epoch: 10.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.316106500924878		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 1.316106500924878 | validation: 2.5682363114425435]
	TIME [epoch: 10.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3122299997256917		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 1.3122299997256917 | validation: 2.5398465916394737]
	TIME [epoch: 10.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3153991123906574		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 1.3153991123906574 | validation: 2.5222681789178334]
	TIME [epoch: 10.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307298830890167		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 1.307298830890167 | validation: 2.518458161184026]
	TIME [epoch: 10.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3116563193890962		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 1.3116563193890962 | validation: 2.6042088483603254]
	TIME [epoch: 10.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3374502912356276		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 1.3374502912356276 | validation: 2.5277170351097062]
	TIME [epoch: 10.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3250711367467		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 1.3250711367467 | validation: 2.5277846904574854]
	TIME [epoch: 10.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3079072726226557		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 1.3079072726226557 | validation: 2.5318141194701806]
	TIME [epoch: 10.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3146706704724953		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 1.3146706704724953 | validation: 2.5076028146297187]
	TIME [epoch: 10.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3097679208523056		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 1.3097679208523056 | validation: 2.5332040556474964]
	TIME [epoch: 10.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3103322404060527		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 1.3103322404060527 | validation: 2.5504491941791585]
	TIME [epoch: 10.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3187339931377422		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 1.3187339931377422 | validation: 2.5195356953815473]
	TIME [epoch: 10.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.309017778963089		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 1.309017778963089 | validation: 2.5027708700379305]
	TIME [epoch: 10.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.320611039487154		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 1.320611039487154 | validation: 2.5472573778380085]
	TIME [epoch: 10.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3179109247403298		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 1.3179109247403298 | validation: 2.521657074117022]
	TIME [epoch: 10.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3195730888554555		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 1.3195730888554555 | validation: 2.5866898801444997]
	TIME [epoch: 10.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3189362586756102		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 1.3189362586756102 | validation: 2.5111648832545193]
	TIME [epoch: 10.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3033092728889462		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 1.3033092728889462 | validation: 2.5170138827431097]
	TIME [epoch: 10.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988085876507174		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 1.2988085876507174 | validation: 2.52068460246234]
	TIME [epoch: 10.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3066644099767073		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 1.3066644099767073 | validation: 2.5290806234296963]
	TIME [epoch: 10.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.309991512509312		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 1.309991512509312 | validation: 2.5039780147580704]
	TIME [epoch: 10.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3080946393889463		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 1.3080946393889463 | validation: 2.4969790653572344]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_1061.pth
	Model improved!!!
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3142954077056		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 1.3142954077056 | validation: 2.516560086882376]
	TIME [epoch: 10.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3166182090547138		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 1.3166182090547138 | validation: 2.52744412574881]
	TIME [epoch: 10.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3081403697748089		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 1.3081403697748089 | validation: 2.5095697229114715]
	TIME [epoch: 10.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3251236781930698		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 1.3251236781930698 | validation: 2.5140655198257438]
	TIME [epoch: 10.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3113894006629452		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 1.3113894006629452 | validation: 2.5620257364370453]
	TIME [epoch: 10.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3128141402489548		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 1.3128141402489548 | validation: 2.5219225573616333]
	TIME [epoch: 10.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305902464739855		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 1.305902464739855 | validation: 2.513926498401264]
	TIME [epoch: 10.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3184477124014875		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 1.3184477124014875 | validation: 2.5327219702563033]
	TIME [epoch: 10.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3172230671648355		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 1.3172230671648355 | validation: 2.516798116234171]
	TIME [epoch: 10.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.318119684201266		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 1.318119684201266 | validation: 2.5148115386184555]
	TIME [epoch: 10.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3048897751462143		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 1.3048897751462143 | validation: 2.5212314056981935]
	TIME [epoch: 10.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3208159680921834		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 1.3208159680921834 | validation: 2.5297466932913166]
	TIME [epoch: 10.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.313682673769246		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 1.313682673769246 | validation: 2.5127768922729756]
	TIME [epoch: 10.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3090603972652033		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 1.3090603972652033 | validation: 2.5214867045626765]
	TIME [epoch: 10.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3146305144926254		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 1.3146305144926254 | validation: 2.536010128775355]
	TIME [epoch: 10.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053332318017763		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 1.3053332318017763 | validation: 2.5240237951842355]
	TIME [epoch: 10.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304478618384644		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 1.304478618384644 | validation: 2.5355054952543328]
	TIME [epoch: 10.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053850637144038		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 1.3053850637144038 | validation: 2.5284248502740114]
	TIME [epoch: 10.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3249219183960235		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 1.3249219183960235 | validation: 2.515587967009068]
	TIME [epoch: 10.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3061732165763338		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 1.3061732165763338 | validation: 2.5407339184230824]
	TIME [epoch: 10.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3137267818564955		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 1.3137267818564955 | validation: 2.536456995280227]
	TIME [epoch: 10.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3132168895184857		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 1.3132168895184857 | validation: 2.5522918942049624]
	TIME [epoch: 10.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3110648091879447		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 1.3110648091879447 | validation: 2.5159059617252306]
	TIME [epoch: 10.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.314382407963846		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 1.314382407963846 | validation: 2.5193998167322444]
	TIME [epoch: 10.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3142831446055578		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 1.3142831446055578 | validation: 2.5082867985461887]
	TIME [epoch: 10.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305513567356126		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 1.305513567356126 | validation: 2.507863249177159]
	TIME [epoch: 10.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.308180231494989		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 1.308180231494989 | validation: 2.5152073155516828]
	TIME [epoch: 10.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3162389681463442		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 1.3162389681463442 | validation: 2.52326946332191]
	TIME [epoch: 10.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018825600283275		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 1.3018825600283275 | validation: 2.5146625169138885]
	TIME [epoch: 10.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3075978374570003		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 1.3075978374570003 | validation: 2.5096878300580783]
	TIME [epoch: 10.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2989540236654593		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 1.2989540236654593 | validation: 2.534298913663133]
	TIME [epoch: 10.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3109085062417425		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 1.3109085062417425 | validation: 2.561276835542044]
	TIME [epoch: 10.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3048626209301155		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 1.3048626209301155 | validation: 2.5041403612926136]
	TIME [epoch: 10.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049822621692801		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 1.3049822621692801 | validation: 2.545467549542742]
	TIME [epoch: 10.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3044993761332193		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 1.3044993761332193 | validation: 2.5159200326551923]
	TIME [epoch: 10.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3068944969408953		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 1.3068944969408953 | validation: 2.529606592515733]
	TIME [epoch: 10.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3071861050109217		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 1.3071861050109217 | validation: 2.5180153034529718]
	TIME [epoch: 10.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3029788700355422		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 1.3029788700355422 | validation: 2.506467112636317]
	TIME [epoch: 10.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3036327613041845		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 1.3036327613041845 | validation: 2.50184309801437]
	TIME [epoch: 10.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3157801930024269		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 1.3157801930024269 | validation: 2.556214786660216]
	TIME [epoch: 10.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3179821137714023		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 1.3179821137714023 | validation: 2.5114320871340197]
	TIME [epoch: 10.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3035273865546384		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 1.3035273865546384 | validation: 2.5005566446644356]
	TIME [epoch: 10.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3104638365694832		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 1.3104638365694832 | validation: 2.5119837489905694]
	TIME [epoch: 10.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3125564663742921		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 1.3125564663742921 | validation: 2.53274677389771]
	TIME [epoch: 10.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3180735658366476		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 1.3180735658366476 | validation: 2.548047901891025]
	TIME [epoch: 10.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.309577404860918		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 1.309577404860918 | validation: 2.5244009685111344]
	TIME [epoch: 10.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3215817194481403		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 1.3215817194481403 | validation: 2.5221943637848154]
	TIME [epoch: 10.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3059937537599533		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 1.3059937537599533 | validation: 2.5140661080775972]
	TIME [epoch: 10.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3116144063872157		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 1.3116144063872157 | validation: 2.5652780899827152]
	TIME [epoch: 10.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3074130229341407		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 1.3074130229341407 | validation: 2.517667367225316]
	TIME [epoch: 10.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3171373650816929		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 1.3171373650816929 | validation: 2.5026344319173757]
	TIME [epoch: 10.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3011508821095663		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 1.3011508821095663 | validation: 2.518047708452108]
	TIME [epoch: 10.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305549164312588		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 1.305549164312588 | validation: 2.5199922409425795]
	TIME [epoch: 10.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3058105000756304		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 1.3058105000756304 | validation: 2.4997695097042305]
	TIME [epoch: 10.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304043571768385		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 1.304043571768385 | validation: 2.510904874277662]
	TIME [epoch: 10.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3033611978669908		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 1.3033611978669908 | validation: 2.521572103173027]
	TIME [epoch: 10.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3019596311456487		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 1.3019596311456487 | validation: 2.5792971075832405]
	TIME [epoch: 10.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3061078215064827		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 1.3061078215064827 | validation: 2.5630975056307297]
	TIME [epoch: 10.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3011084502804553		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 1.3011084502804553 | validation: 2.527284969425027]
	TIME [epoch: 10.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305585945128762		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 1.305585945128762 | validation: 2.5573117735678883]
	TIME [epoch: 10.4 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3022782758916118		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 1.3022782758916118 | validation: 2.559578693807043]
	TIME [epoch: 10.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3041712354842574		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 1.3041712354842574 | validation: 2.51089830853123]
	TIME [epoch: 10.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3027168595085026		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 1.3027168595085026 | validation: 2.5175904510257956]
	TIME [epoch: 10.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049670225148189		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 1.3049670225148189 | validation: 2.514441162256706]
	TIME [epoch: 10.4 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.318234195282611		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 1.318234195282611 | validation: 2.5260765282114654]
	TIME [epoch: 10.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303891848489384		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 1.303891848489384 | validation: 2.5135880065823537]
	TIME [epoch: 10.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3093628590452533		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 1.3093628590452533 | validation: 2.5793630279426214]
	TIME [epoch: 10.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3233013712910213		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 1.3233013712910213 | validation: 2.535365296118197]
	TIME [epoch: 10.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3237631185814487		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 1.3237631185814487 | validation: 2.5483335074960163]
	TIME [epoch: 10.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3119297790608453		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 1.3119297790608453 | validation: 2.506799281707765]
	TIME [epoch: 10.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3068713156977592		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 1.3068713156977592 | validation: 2.530532307243513]
	TIME [epoch: 10.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3035782768696556		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 1.3035782768696556 | validation: 2.543649249293716]
	TIME [epoch: 10.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007605077381517		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 1.3007605077381517 | validation: 2.5800997001178816]
	TIME [epoch: 10.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3025100832502985		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 1.3025100832502985 | validation: 2.5073464949611832]
	TIME [epoch: 10.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053920064144902		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 1.3053920064144902 | validation: 2.5155670534106047]
	TIME [epoch: 10.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024185029372721		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 1.3024185029372721 | validation: 2.5650956193622023]
	TIME [epoch: 10.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.308109313982707		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 1.308109313982707 | validation: 2.524294624220668]
	TIME [epoch: 10.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3140653786685497		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 1.3140653786685497 | validation: 2.590963854396932]
	TIME [epoch: 10.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3076345816113417		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 1.3076345816113417 | validation: 2.517440244334987]
	TIME [epoch: 10.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3014620601280646		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 1.3014620601280646 | validation: 2.5201488909046588]
	TIME [epoch: 10.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995258631056275		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 1.2995258631056275 | validation: 2.5128823590876297]
	TIME [epoch: 10.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2981926009744549		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 1.2981926009744549 | validation: 2.5337648162153705]
	TIME [epoch: 10.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3081001212277212		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 1.3081001212277212 | validation: 2.507001183529076]
	TIME [epoch: 10.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3057086685854606		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 1.3057086685854606 | validation: 2.554898500916748]
	TIME [epoch: 10.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3107362127275248		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 1.3107362127275248 | validation: 2.507660849949214]
	TIME [epoch: 10.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3096704014550813		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 1.3096704014550813 | validation: 2.5315790420681177]
	TIME [epoch: 10.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297728455188718		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 1.297728455188718 | validation: 2.5546487800484052]
	TIME [epoch: 10.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.311958807607425		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 1.311958807607425 | validation: 2.5201068007439402]
	TIME [epoch: 10.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3099515934005919		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 1.3099515934005919 | validation: 2.5587661070806855]
	TIME [epoch: 10.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307757416075366		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 1.307757416075366 | validation: 2.5229908154728413]
	TIME [epoch: 10.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.311142633253059		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 1.311142633253059 | validation: 2.5328376164118125]
	TIME [epoch: 10.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3189742807840419		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 1.3189742807840419 | validation: 2.517559983081152]
	TIME [epoch: 10.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305724316382085		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 1.305724316382085 | validation: 2.518727674705802]
	TIME [epoch: 10.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.306319989236833		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 1.306319989236833 | validation: 2.5604821091506778]
	TIME [epoch: 10.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982928122227786		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 1.2982928122227786 | validation: 2.56025715002687]
	TIME [epoch: 10.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3091684385381988		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 1.3091684385381988 | validation: 2.512502265283715]
	TIME [epoch: 10.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2991150549931765		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 1.2991150549931765 | validation: 2.5486020838985977]
	TIME [epoch: 10.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3111567199603131		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 1.3111567199603131 | validation: 2.5272691763367057]
	TIME [epoch: 10.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049621156999711		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 1.3049621156999711 | validation: 2.528625819383548]
	TIME [epoch: 10.4 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3147286013005028		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 1.3147286013005028 | validation: 2.5338078943961357]
	TIME [epoch: 10.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299050857707948		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 1.299050857707948 | validation: 2.515282669315432]
	TIME [epoch: 10.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3209209950088325		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 1.3209209950088325 | validation: 2.5356496384163716]
	TIME [epoch: 10.4 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3120390176517052		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 1.3120390176517052 | validation: 2.5183983655621525]
	TIME [epoch: 10.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.310107200462142		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 1.310107200462142 | validation: 2.5205406965481383]
	TIME [epoch: 10.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2945305004704264		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 1.2945305004704264 | validation: 2.5050762607700388]
	TIME [epoch: 10.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3051103066953267		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 1.3051103066953267 | validation: 2.49899492958612]
	TIME [epoch: 10.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3002077934265972		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 1.3002077934265972 | validation: 2.5171617306182346]
	TIME [epoch: 10.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977908834883523		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 1.2977908834883523 | validation: 2.514460233910729]
	TIME [epoch: 10.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3105329254184008		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 1.3105329254184008 | validation: 2.548403871427195]
	TIME [epoch: 10.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.343888507702692		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 1.343888507702692 | validation: 2.5169278407377984]
	TIME [epoch: 10.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3119894843520334		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 1.3119894843520334 | validation: 2.516966483240083]
	TIME [epoch: 10.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304139168313902		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 1.304139168313902 | validation: 2.508260380056865]
	TIME [epoch: 10.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3039332187134591		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 1.3039332187134591 | validation: 2.534434863989533]
	TIME [epoch: 10.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3128813273579547		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 1.3128813273579547 | validation: 2.51715989716391]
	TIME [epoch: 10.4 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3037938927268389		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 1.3037938927268389 | validation: 2.4986340616834686]
	TIME [epoch: 10.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3032440815572943		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 1.3032440815572943 | validation: 2.5348449481870303]
	TIME [epoch: 10.4 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3075259852146899		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 1.3075259852146899 | validation: 2.511422754213507]
	TIME [epoch: 10.4 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3036245638385466		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 1.3036245638385466 | validation: 2.5185108528578364]
	TIME [epoch: 10.4 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3145680565559132		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 1.3145680565559132 | validation: 2.5343726568294778]
	TIME [epoch: 10.4 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3204107436230221		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 1.3204107436230221 | validation: 2.520277047823613]
	TIME [epoch: 10.4 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3069835065968245		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 1.3069835065968245 | validation: 2.5001041378461926]
	TIME [epoch: 10.4 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973368490531123		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 1.2973368490531123 | validation: 2.515228780729016]
	TIME [epoch: 10.4 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3050641463389767		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 1.3050641463389767 | validation: 2.510125239198332]
	TIME [epoch: 10.4 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.314208473092738		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 1.314208473092738 | validation: 2.565430845653397]
	TIME [epoch: 10.4 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007246883654089		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 1.3007246883654089 | validation: 2.5160400632467623]
	TIME [epoch: 10.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3031305765481391		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 1.3031305765481391 | validation: 2.527024592448788]
	TIME [epoch: 10.4 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3134426195813413		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 1.3134426195813413 | validation: 2.5283958992518443]
	TIME [epoch: 10.3 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303335177313152		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 1.303335177313152 | validation: 2.5239096112342057]
	TIME [epoch: 10.4 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049360280374243		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 1.3049360280374243 | validation: 2.5253359132524373]
	TIME [epoch: 10.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.306451019940581		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 1.306451019940581 | validation: 2.573579828258521]
	TIME [epoch: 10.4 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2987811346502647		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 1.2987811346502647 | validation: 2.5153959635175402]
	TIME [epoch: 10.4 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30595925089957		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 1.30595925089957 | validation: 2.508142074963437]
	TIME [epoch: 10.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3006245022493637		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 1.3006245022493637 | validation: 2.4982326780812563]
	TIME [epoch: 10.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.311093986495085		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 1.311093986495085 | validation: 2.5261483915157403]
	TIME [epoch: 10.4 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307007267483544		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 1.307007267483544 | validation: 2.4851971796322516]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_1196.pth
	Model improved!!!
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3087510445016106		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 1.3087510445016106 | validation: 2.53074376253334]
	TIME [epoch: 10.4 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.308228616443008		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 1.308228616443008 | validation: 2.5651351945208054]
	TIME [epoch: 10.3 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3032660499305508		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 1.3032660499305508 | validation: 2.5045238380466146]
	TIME [epoch: 10.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.31447852556296		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 1.31447852556296 | validation: 2.515306845223194]
	TIME [epoch: 10.3 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3137235927671025		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 1.3137235927671025 | validation: 2.508179276568538]
	TIME [epoch: 10.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3034311350856045		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 1.3034311350856045 | validation: 2.557111210816046]
	TIME [epoch: 10.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3026574304806295		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 1.3026574304806295 | validation: 2.5109753755974458]
	TIME [epoch: 10.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296877299386686		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 1.296877299386686 | validation: 2.5147853653872936]
	TIME [epoch: 10.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3062579414188387		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 1.3062579414188387 | validation: 2.514735267955156]
	TIME [epoch: 10.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3110197746514427		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 1.3110197746514427 | validation: 2.5192301575968634]
	TIME [epoch: 10.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3234102385339654		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 1.3234102385339654 | validation: 2.5237332206011627]
	TIME [epoch: 10.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3070005586313589		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 1.3070005586313589 | validation: 2.5093120450460864]
	TIME [epoch: 10.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300117751373113		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 1.300117751373113 | validation: 2.504624669945259]
	TIME [epoch: 10.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3003471663310813		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 1.3003471663310813 | validation: 2.520112401826029]
	TIME [epoch: 10.4 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007426788342948		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 1.3007426788342948 | validation: 2.566638004940667]
	TIME [epoch: 10.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053953248320807		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 1.3053953248320807 | validation: 2.5306336581739517]
	TIME [epoch: 10.4 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3111152264116932		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 1.3111152264116932 | validation: 2.5172343883605626]
	TIME [epoch: 10.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.314081318982939		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 1.314081318982939 | validation: 2.5114209994193426]
	TIME [epoch: 10.4 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3063950810671745		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 1.3063950810671745 | validation: 2.521025542940123]
	TIME [epoch: 10.4 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3076699828324263		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 1.3076699828324263 | validation: 2.516956667227018]
	TIME [epoch: 10.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302006107664916		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 1.302006107664916 | validation: 2.5252925047894212]
	TIME [epoch: 10.4 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3199373837323318		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 1.3199373837323318 | validation: 2.52324220804019]
	TIME [epoch: 10.4 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3087717838742057		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 1.3087717838742057 | validation: 2.5029470783459438]
	TIME [epoch: 10.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2963825181618174		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 1.2963825181618174 | validation: 2.5062377353846426]
	TIME [epoch: 10.4 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993372497697806		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 1.2993372497697806 | validation: 2.517511433626341]
	TIME [epoch: 10.4 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3042344783312791		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 1.3042344783312791 | validation: 2.5014185738210575]
	TIME [epoch: 10.4 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3050053630877843		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 1.3050053630877843 | validation: 2.512987691166072]
	TIME [epoch: 10.4 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3047200952769202		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 1.3047200952769202 | validation: 2.5087749794270566]
	TIME [epoch: 10.4 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3040407279160853		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 1.3040407279160853 | validation: 2.5404983272060533]
	TIME [epoch: 10.4 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3174534289414053		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 1.3174534289414053 | validation: 2.506754326886581]
	TIME [epoch: 10.4 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294984120490882		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 1.294984120490882 | validation: 2.5103868731175]
	TIME [epoch: 10.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3003267010884794		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 1.3003267010884794 | validation: 2.5553884306511954]
	TIME [epoch: 10.4 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001019994735228		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 1.3001019994735228 | validation: 2.564488837014023]
	TIME [epoch: 10.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2989999777553245		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 1.2989999777553245 | validation: 2.515873298478611]
	TIME [epoch: 10.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3139148329355403		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 1.3139148329355403 | validation: 2.5588117053386608]
	TIME [epoch: 10.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294471262456169		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 1.294471262456169 | validation: 2.5089209823740193]
	TIME [epoch: 10.4 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3038472049779293		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 1.3038472049779293 | validation: 2.5072642215672873]
	TIME [epoch: 10.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2986531137247421		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 1.2986531137247421 | validation: 2.534625790990597]
	TIME [epoch: 10.4 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305635047333074		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 1.305635047333074 | validation: 2.56000375816233]
	TIME [epoch: 10.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2947569004127055		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 1.2947569004127055 | validation: 2.512895543392816]
	TIME [epoch: 10.4 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053361374825256		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 1.3053361374825256 | validation: 2.5267154129709053]
	TIME [epoch: 10.4 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3038051009629334		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 1.3038051009629334 | validation: 2.5084474180175995]
	TIME [epoch: 10.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3030879065547385		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 1.3030879065547385 | validation: 2.5072985200935474]
	TIME [epoch: 10.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3034083541463648		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 1.3034083541463648 | validation: 2.5389861774628355]
	TIME [epoch: 10.3 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.306055389732955		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 1.306055389732955 | validation: 2.520261278838632]
	TIME [epoch: 10.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943933549921662		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 1.2943933549921662 | validation: 2.5053943365100393]
	TIME [epoch: 10.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305874251935754		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 1.305874251935754 | validation: 2.5209731272128297]
	TIME [epoch: 10.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3073922814736325		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 1.3073922814736325 | validation: 2.5225381094554282]
	TIME [epoch: 10.4 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299289073427887		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 1.299289073427887 | validation: 2.518215194480943]
	TIME [epoch: 10.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3019137893718116		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 1.3019137893718116 | validation: 2.5296564126666237]
	TIME [epoch: 10.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3040143696886137		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 1.3040143696886137 | validation: 2.5123609185538927]
	TIME [epoch: 10.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3064647913089207		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 1.3064647913089207 | validation: 2.5119424051577686]
	TIME [epoch: 10.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3086527121622669		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 1.3086527121622669 | validation: 2.5149561815446115]
	TIME [epoch: 10.4 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2985536979102317		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 1.2985536979102317 | validation: 2.501355743055079]
	TIME [epoch: 10.4 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301808855579288		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 1.301808855579288 | validation: 2.5271124524644395]
	TIME [epoch: 10.4 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303000059067593		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 1.303000059067593 | validation: 2.526931365886066]
	TIME [epoch: 10.4 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3044067370031471		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 1.3044067370031471 | validation: 2.556150828402203]
	TIME [epoch: 10.4 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3008273956035175		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 1.3008273956035175 | validation: 2.4995949186540973]
	TIME [epoch: 10.4 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024437402493365		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 1.3024437402493365 | validation: 2.5087796959217044]
	TIME [epoch: 10.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301605345311049		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 1.301605345311049 | validation: 2.5163070132475287]
	TIME [epoch: 10.4 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3096722482402838		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 1.3096722482402838 | validation: 2.5143472170532677]
	TIME [epoch: 10.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305626029122965		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 1.305626029122965 | validation: 2.4962660446493246]
	TIME [epoch: 10.4 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3150329812867718		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 1.3150329812867718 | validation: 2.504198850062927]
	TIME [epoch: 10.4 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007959902831845		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 1.3007959902831845 | validation: 2.5500648893085724]
	TIME [epoch: 10.4 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053780559943617		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 1.3053780559943617 | validation: 2.5575479766993716]
	TIME [epoch: 10.4 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3119958943484673		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 1.3119958943484673 | validation: 2.522278067601334]
	TIME [epoch: 10.4 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3014616077088959		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 1.3014616077088959 | validation: 2.5141459463295788]
	TIME [epoch: 10.4 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2999728935422588		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 1.2999728935422588 | validation: 2.51269678516383]
	TIME [epoch: 10.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2944679549689375		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 1.2944679549689375 | validation: 2.50005289775901]
	TIME [epoch: 10.4 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982822336183988		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 1.2982822336183988 | validation: 2.512692621447805]
	TIME [epoch: 10.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3059430677137516		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 1.3059430677137516 | validation: 2.5143975672015255]
	TIME [epoch: 10.4 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3057328829802426		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 1.3057328829802426 | validation: 2.5266572780627623]
	TIME [epoch: 10.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304357675437591		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 1.304357675437591 | validation: 2.4966083438836857]
	TIME [epoch: 10.4 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2991596219647228		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 1.2991596219647228 | validation: 2.4999942789824674]
	TIME [epoch: 10.4 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2964384526559312		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 1.2964384526559312 | validation: 2.5052490871197572]
	TIME [epoch: 10.4 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956070388768661		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 1.2956070388768661 | validation: 2.510218565886351]
	TIME [epoch: 10.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982863508515692		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 1.2982863508515692 | validation: 2.5210812140440373]
	TIME [epoch: 10.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3058063420679258		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 1.3058063420679258 | validation: 2.518001102852518]
	TIME [epoch: 10.4 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3002918133783945		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 1.3002918133783945 | validation: 2.5032385423476047]
	TIME [epoch: 10.4 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3028876272136878		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 1.3028876272136878 | validation: 2.5614220858127874]
	TIME [epoch: 10.4 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980797729018974		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 1.2980797729018974 | validation: 2.4998861988419963]
	TIME [epoch: 10.4 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3041737735527619		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 1.3041737735527619 | validation: 2.4991727166083515]
	TIME [epoch: 10.4 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3056197272746668		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 1.3056197272746668 | validation: 2.518074912471726]
	TIME [epoch: 10.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980756183558029		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 1.2980756183558029 | validation: 2.495062228541485]
	TIME [epoch: 10.4 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297016143054713		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 1.297016143054713 | validation: 2.5200346752905194]
	TIME [epoch: 10.4 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3056097061966763		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 1.3056097061966763 | validation: 2.5097747680753635]
	TIME [epoch: 10.4 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300865856867587		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 1.300865856867587 | validation: 2.4978698073770147]
	TIME [epoch: 10.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988832888010493		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 1.2988832888010493 | validation: 2.510438498643367]
	TIME [epoch: 10.4 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.318702578435412		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 1.318702578435412 | validation: 2.543333471469047]
	TIME [epoch: 10.4 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3168090492136746		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 1.3168090492136746 | validation: 2.534917579687356]
	TIME [epoch: 10.4 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018273029025356		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 1.3018273029025356 | validation: 2.5004750423216318]
	TIME [epoch: 10.4 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3022154438914526		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 1.3022154438914526 | validation: 2.5003735390534394]
	TIME [epoch: 10.4 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2969717428300496		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 1.2969717428300496 | validation: 2.5159930619730067]
	TIME [epoch: 10.4 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049802589185915		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 1.3049802589185915 | validation: 2.5239746495886055]
	TIME [epoch: 10.4 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305307685804245		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 1.305307685804245 | validation: 2.5200151981271337]
	TIME [epoch: 10.4 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302749330811364		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 1.302749330811364 | validation: 2.5130475111835087]
	TIME [epoch: 10.4 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2963117218177693		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 1.2963117218177693 | validation: 2.511182469521325]
	TIME [epoch: 10.4 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299423334867559		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 1.299423334867559 | validation: 2.517501164568908]
	TIME [epoch: 10.4 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005819096851787		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 1.3005819096851787 | validation: 2.519200038138291]
	TIME [epoch: 10.4 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3059555223135373		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 1.3059555223135373 | validation: 2.5268398032603474]
	TIME [epoch: 10.4 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3146680919809905		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 1.3146680919809905 | validation: 2.5311505193947608]
	TIME [epoch: 10.4 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3088905861895068		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 1.3088905861895068 | validation: 2.518035305862465]
	TIME [epoch: 10.4 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3012349820201385		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 1.3012349820201385 | validation: 2.487480963082558]
	TIME [epoch: 10.4 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977283198221885		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 1.2977283198221885 | validation: 2.5283458213575605]
	TIME [epoch: 10.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30647203784715		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 1.30647203784715 | validation: 2.5031667751028897]
	TIME [epoch: 10.4 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3072828516358646		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 1.3072828516358646 | validation: 2.5343579883524945]
	TIME [epoch: 10.4 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3006938451083658		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 1.3006938451083658 | validation: 2.5001027000884455]
	TIME [epoch: 10.4 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3082743013809786		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 1.3082743013809786 | validation: 2.5143889559362127]
	TIME [epoch: 10.4 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005008093791768		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 1.3005008093791768 | validation: 2.5189844781003377]
	TIME [epoch: 10.4 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3044456880604165		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 1.3044456880604165 | validation: 2.5724027197975063]
	TIME [epoch: 10.4 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3074568292843678		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 1.3074568292843678 | validation: 2.514945622742289]
	TIME [epoch: 10.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2918324728579826		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 1.2918324728579826 | validation: 2.520516928455404]
	TIME [epoch: 10.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2999776501074771		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 1.2999776501074771 | validation: 2.516893350024831]
	TIME [epoch: 10.4 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2978849842351345		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 1.2978849842351345 | validation: 2.5020665871469574]
	TIME [epoch: 10.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3031708834779656		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 1.3031708834779656 | validation: 2.5015933003301734]
	TIME [epoch: 10.4 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3146842233318405		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 1.3146842233318405 | validation: 2.5004192224415527]
	TIME [epoch: 10.4 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972107880307056		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 1.2972107880307056 | validation: 2.5185574641965123]
	TIME [epoch: 10.4 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3044977141303558		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 1.3044977141303558 | validation: 2.513274486000919]
	TIME [epoch: 10.4 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995725245195369		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 1.2995725245195369 | validation: 2.555996973333049]
	TIME [epoch: 10.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3120563427493983		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 1.3120563427493983 | validation: 2.517512294768651]
	TIME [epoch: 10.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3015128233443185		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 1.3015128233443185 | validation: 2.4960406895439777]
	TIME [epoch: 10.4 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29863615054952		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 1.29863615054952 | validation: 2.5260626214262656]
	TIME [epoch: 10.4 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3008672739630294		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 1.3008672739630294 | validation: 2.551226235913094]
	TIME [epoch: 10.4 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303182362792821		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 1.303182362792821 | validation: 2.512183852720065]
	TIME [epoch: 10.4 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3003244756435532		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 1.3003244756435532 | validation: 2.5139527608901377]
	TIME [epoch: 10.4 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298263809973117		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 1.298263809973117 | validation: 2.5068794560640963]
	TIME [epoch: 10.3 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3019564358060078		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 1.3019564358060078 | validation: 2.5143101298818613]
	TIME [epoch: 10.4 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304355035567745		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 1.304355035567745 | validation: 2.5040779009631082]
	TIME [epoch: 10.4 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996244083159076		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 1.2996244083159076 | validation: 2.517246350535938]
	TIME [epoch: 10.4 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3021731655941315		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 1.3021731655941315 | validation: 2.4999877153538477]
	TIME [epoch: 10.4 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960125888092275		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 1.2960125888092275 | validation: 2.5585014415529073]
	TIME [epoch: 10.4 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018949270256268		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 1.3018949270256268 | validation: 2.5149063693256393]
	TIME [epoch: 10.4 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3026633685536322		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 1.3026633685536322 | validation: 2.5119878301890814]
	TIME [epoch: 10.4 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3060028449658703		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 1.3060028449658703 | validation: 2.4849817292671195]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r5_20240219_233643/states/model_tr_study206_1330.pth
	Model improved!!!
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3039221834182613		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 1.3039221834182613 | validation: 2.519829041732321]
	TIME [epoch: 10.3 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301627263733895		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 1.301627263733895 | validation: 2.5070883209727173]
	TIME [epoch: 10.3 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968314853018015		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 1.2968314853018015 | validation: 2.50583680370056]
	TIME [epoch: 10.4 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3022285701161842		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 1.3022285701161842 | validation: 2.5405877949369944]
	TIME [epoch: 10.4 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3082553020836571		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 1.3082553020836571 | validation: 2.505742705691322]
	TIME [epoch: 10.4 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3075865991384639		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 1.3075865991384639 | validation: 2.533043920573357]
	TIME [epoch: 10.4 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2948121611990118		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 1.2948121611990118 | validation: 2.508304335406821]
	TIME [epoch: 10.4 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2986840596868		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 1.2986840596868 | validation: 2.507525549159625]
	TIME [epoch: 10.4 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3014195386486374		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 1.3014195386486374 | validation: 2.524059830357124]
	TIME [epoch: 10.4 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2994651105671744		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 1.2994651105671744 | validation: 2.505559237491652]
	TIME [epoch: 10.4 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053445558329575		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 1.3053445558329575 | validation: 2.495881372151471]
	TIME [epoch: 10.4 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3029098438982196		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 1.3029098438982196 | validation: 2.5089530687398343]
	TIME [epoch: 10.4 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3098475673715075		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 1.3098475673715075 | validation: 2.5058396097117384]
	TIME [epoch: 10.4 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3025177703284978		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 1.3025177703284978 | validation: 2.5061975698108694]
	TIME [epoch: 10.4 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3059700696375849		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 1.3059700696375849 | validation: 2.52710414680615]
	TIME [epoch: 10.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294417069713545		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 1.294417069713545 | validation: 2.5084843490048643]
	TIME [epoch: 10.4 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2981818052651724		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 1.2981818052651724 | validation: 2.5619263409182946]
	TIME [epoch: 10.4 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3112270535451178		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 1.3112270535451178 | validation: 2.5393871730331603]
	TIME [epoch: 10.4 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053624581230832		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 1.3053624581230832 | validation: 2.517515229703321]
	TIME [epoch: 10.4 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996555491352322		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 1.2996555491352322 | validation: 2.507135139956006]
	TIME [epoch: 10.4 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302034739439918		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 1.302034739439918 | validation: 2.5217211080302993]
	TIME [epoch: 10.4 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2984509580739985		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 1.2984509580739985 | validation: 2.520365336697429]
	TIME [epoch: 10.4 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3026188646490524		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 1.3026188646490524 | validation: 2.508811542773504]
	TIME [epoch: 10.4 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3036077157049275		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 1.3036077157049275 | validation: 2.4975621369768044]
	TIME [epoch: 10.4 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3019147054678768		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 1.3019147054678768 | validation: 2.507937726356077]
	TIME [epoch: 10.4 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293783114195493		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 1.293783114195493 | validation: 2.4948504509599183]
	TIME [epoch: 10.4 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29555257533973		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 1.29555257533973 | validation: 2.548004983966247]
	TIME [epoch: 10.4 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303355533731073		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 1.303355533731073 | validation: 2.5055950233997453]
	TIME [epoch: 10.4 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2994387110617616		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 1.2994387110617616 | validation: 2.4979311652243554]
	TIME [epoch: 10.4 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2924728926115363		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 1.2924728926115363 | validation: 2.507161585755112]
	TIME [epoch: 10.4 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3046344936672756		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 1.3046344936672756 | validation: 2.501669274250365]
	TIME [epoch: 10.4 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.306896291819902		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 1.306896291819902 | validation: 2.507732343745429]
	TIME [epoch: 10.4 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3020758848647194		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 1.3020758848647194 | validation: 2.51729246263555]
	TIME [epoch: 10.4 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3034878989091943		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 1.3034878989091943 | validation: 2.5008499962111386]
	TIME [epoch: 10.4 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993662320565131		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 1.2993662320565131 | validation: 2.4911638459260597]
	TIME [epoch: 10.4 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295127956711744		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 1.295127956711744 | validation: 2.5091718496207607]
	TIME [epoch: 10.4 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3022326396661223		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 1.3022326396661223 | validation: 2.5062157130530798]
	TIME [epoch: 10.4 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3027063438604713		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 1.3027063438604713 | validation: 2.5107110070209404]
	TIME [epoch: 10.4 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304310766662454		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 1.304310766662454 | validation: 2.4993890228931543]
	TIME [epoch: 10.4 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007079250033593		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 1.3007079250033593 | validation: 2.5445703803611486]
	TIME [epoch: 10.4 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993219007880055		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 1.2993219007880055 | validation: 2.5027870879028358]
	TIME [epoch: 10.4 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294239762386755		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 1.294239762386755 | validation: 2.51465056896724]
	TIME [epoch: 10.4 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001802545542256		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 1.3001802545542256 | validation: 2.5119192372026142]
	TIME [epoch: 10.4 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3043968485275077		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 1.3043968485275077 | validation: 2.5226261232334752]
	TIME [epoch: 10.4 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2997137672953039		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 1.2997137672953039 | validation: 2.570909481298606]
	TIME [epoch: 10.4 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2903832119053218		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 1.2903832119053218 | validation: 2.5064945365699787]
	TIME [epoch: 10.4 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3015060728366417		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 1.3015060728366417 | validation: 2.5036295631615766]
	TIME [epoch: 10.4 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972634184879408		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 1.2972634184879408 | validation: 2.5352008949444826]
	TIME [epoch: 10.4 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977070172645528		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 1.2977070172645528 | validation: 2.5009927207251748]
	TIME [epoch: 10.4 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2932908132454213		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 1.2932908132454213 | validation: 2.513395043924908]
	TIME [epoch: 10.4 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3034476708975007		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 1.3034476708975007 | validation: 2.5009198550680174]
	TIME [epoch: 10.4 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005624415923362		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 1.3005624415923362 | validation: 2.5427694933526963]
	TIME [epoch: 10.4 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3022928184079645		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 1.3022928184079645 | validation: 2.5296581148609425]
	TIME [epoch: 10.4 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004039754426024		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 1.3004039754426024 | validation: 2.504778652533574]
	TIME [epoch: 10.4 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3000781361128062		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 1.3000781361128062 | validation: 2.5089780638832355]
	TIME [epoch: 10.4 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966757704802174		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 1.2966757704802174 | validation: 2.5137972993072304]
	TIME [epoch: 10.4 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995639841743682		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 1.2995639841743682 | validation: 2.5072640124142906]
	TIME [epoch: 10.4 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2963522709533637		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 1.2963522709533637 | validation: 2.5074117691174624]
	TIME [epoch: 10.4 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3011264154636402		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 1.3011264154636402 | validation: 2.5149221834464672]
	TIME [epoch: 10.4 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301556791746575		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 1.301556791746575 | validation: 2.5009790298427483]
	TIME [epoch: 10.4 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307362163670138		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 1.307362163670138 | validation: 2.546917106723357]
	TIME [epoch: 10.4 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3075881655676573		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 1.3075881655676573 | validation: 2.567684945459147]
	TIME [epoch: 10.4 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958727918020811		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 1.2958727918020811 | validation: 2.505026400365581]
	TIME [epoch: 10.4 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298797013941562		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 1.298797013941562 | validation: 2.5142031289374542]
	TIME [epoch: 10.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007799997518465		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 1.3007799997518465 | validation: 2.5046134794058084]
	TIME [epoch: 10.4 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977801836924912		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 1.2977801836924912 | validation: 2.506076354627579]
	TIME [epoch: 10.4 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995666836149735		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 1.2995666836149735 | validation: 2.497291925391201]
	TIME [epoch: 10.4 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993426030237694		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 1.2993426030237694 | validation: 2.5152527873887682]
	TIME [epoch: 10.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3038113450889657		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 1.3038113450889657 | validation: 2.50773282313176]
	TIME [epoch: 10.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3035147908816334		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 1.3035147908816334 | validation: 2.5314284911645446]
	TIME [epoch: 10.4 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297978738380351		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 1.297978738380351 | validation: 2.5069285044762997]
	TIME [epoch: 10.4 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299975376454738		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 1.299975376454738 | validation: 2.509221315386146]
	TIME [epoch: 10.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3057105325594205		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 1.3057105325594205 | validation: 2.547537270495508]
	TIME [epoch: 10.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972421758413122		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 1.2972421758413122 | validation: 2.5129222683895662]
	TIME [epoch: 10.4 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966774168676665		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 1.2966774168676665 | validation: 2.507096753354807]
	TIME [epoch: 10.4 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3032467734030386		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 1.3032467734030386 | validation: 2.566161145365348]
	TIME [epoch: 10.4 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2962453614789635		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 1.2962453614789635 | validation: 2.503116393517432]
	TIME [epoch: 10.4 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2952519655809727		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 1.2952519655809727 | validation: 2.5179565827772628]
	TIME [epoch: 10.4 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3038242949719532		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 1.3038242949719532 | validation: 2.530288619246464]
	TIME [epoch: 10.4 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3017555042064461		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 1.3017555042064461 | validation: 2.523738538040254]
	TIME [epoch: 10.4 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300895090589627		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 1.300895090589627 | validation: 2.50075754250471]
	TIME [epoch: 10.4 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304969680107107		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 1.304969680107107 | validation: 2.50451111523124]
	TIME [epoch: 10.4 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972804182486521		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 1.2972804182486521 | validation: 2.5171448820489335]
	TIME [epoch: 10.4 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297158843012754		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 1.297158843012754 | validation: 2.517887242139543]
	TIME [epoch: 10.4 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300847664441211		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 1.300847664441211 | validation: 2.5540531008782845]
	TIME [epoch: 10.4 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295165699071447		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 1.295165699071447 | validation: 2.509483003239352]
	TIME [epoch: 10.4 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301751051248724		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 1.301751051248724 | validation: 2.5081787372157986]
	TIME [epoch: 10.4 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3025696586531381		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 1.3025696586531381 | validation: 2.5109233757340856]
	TIME [epoch: 10.4 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996225285022374		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 1.2996225285022374 | validation: 2.5602699136267577]
	TIME [epoch: 10.4 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992721875197972		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 1.2992721875197972 | validation: 2.548844093980175]
	TIME [epoch: 10.4 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304891946665864		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 1.304891946665864 | validation: 2.5015722351701157]
	TIME [epoch: 10.4 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2997789226376804		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 1.2997789226376804 | validation: 2.5054822013256834]
	TIME [epoch: 10.4 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995897043832045		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 1.2995897043832045 | validation: 2.533230497639071]
	TIME [epoch: 10.4 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296996300406049		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 1.296996300406049 | validation: 2.517563681973326]
	TIME [epoch: 10.4 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2911481686111896		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 1.2911481686111896 | validation: 2.5169498025300694]
	TIME [epoch: 10.4 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291779051924068		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 1.291779051924068 | validation: 2.5132733828447726]
	TIME [epoch: 10.4 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2987768992069968		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 1.2987768992069968 | validation: 2.5115686772113057]
	TIME [epoch: 10.4 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968969994786548		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 1.2968969994786548 | validation: 2.513088223365996]
	TIME [epoch: 10.4 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3048915768072065		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 1.3048915768072065 | validation: 2.55061323792156]
	TIME [epoch: 10.4 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.309422687377112		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 1.309422687377112 | validation: 2.5076174332312497]
	TIME [epoch: 10.4 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3033348055245564		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 1.3033348055245564 | validation: 2.500777729256584]
	TIME [epoch: 10.4 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291384559981525		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 1.291384559981525 | validation: 2.5063511923413815]
	TIME [epoch: 10.4 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004681468793875		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 1.3004681468793875 | validation: 2.497338251641744]
	TIME [epoch: 10.4 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3015211679535106		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 1.3015211679535106 | validation: 2.5136635350471788]
	TIME [epoch: 10.4 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996731464895095		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 1.2996731464895095 | validation: 2.4987136417043794]
	TIME [epoch: 10.4 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3036079832316565		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 1.3036079832316565 | validation: 2.500620818239473]
	TIME [epoch: 10.4 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307719145615534		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 1.307719145615534 | validation: 2.504858088656785]
	TIME [epoch: 10.4 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2971652801901374		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 1.2971652801901374 | validation: 2.5006981583552825]
	TIME [epoch: 10.4 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300951410027139		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 1.300951410027139 | validation: 2.5127550747391836]
	TIME [epoch: 10.4 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2945637495926796		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 1.2945637495926796 | validation: 2.5128890915003264]
	TIME [epoch: 10.4 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2948341616116967		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 1.2948341616116967 | validation: 2.500750587995669]
	TIME [epoch: 10.4 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2987003754660762		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 1.2987003754660762 | validation: 2.4975442946193867]
	TIME [epoch: 10.4 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293226271170988		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 1.293226271170988 | validation: 2.5547036933345004]
	TIME [epoch: 10.4 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982660277483922		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 1.2982660277483922 | validation: 2.507662198338932]
	TIME [epoch: 10.4 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3031459856459402		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 1.3031459856459402 | validation: 2.4992445930293035]
	TIME [epoch: 10.4 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3041726441920198		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 1.3041726441920198 | validation: 2.496549691701354]
	TIME [epoch: 10.4 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976848402421914		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 1.2976848402421914 | validation: 2.505487495317341]
	TIME [epoch: 10.4 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2936756213619527		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 1.2936756213619527 | validation: 2.5275419801136523]
	TIME [epoch: 10.4 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303758569725344		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 1.303758569725344 | validation: 2.5031452898743094]
	TIME [epoch: 10.4 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2969042388845584		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 1.2969042388845584 | validation: 2.5159955018570823]
	TIME [epoch: 10.4 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3049449105010111		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 1.3049449105010111 | validation: 2.521431613653625]
	TIME [epoch: 10.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29738178423584		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 1.29738178423584 | validation: 2.4903903717620715]
	TIME [epoch: 10.4 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3060046446292461		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 1.3060046446292461 | validation: 2.5325367513237103]
	TIME [epoch: 10.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29410598361749		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 1.29410598361749 | validation: 2.511148561770531]
	TIME [epoch: 10.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2912613312750536		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 1.2912613312750536 | validation: 2.499296650516904]
	TIME [epoch: 10.4 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300486448794061		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 1.300486448794061 | validation: 2.504131796637816]
	TIME [epoch: 10.4 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2990593154088406		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 1.2990593154088406 | validation: 2.5013521241700483]
	TIME [epoch: 10.4 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3033473652180416		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 1.3033473652180416 | validation: 2.5032859484323375]
	TIME [epoch: 10.4 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992501920790769		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 1.2992501920790769 | validation: 2.4998834550337383]
	TIME [epoch: 10.4 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2974030001761663		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 1.2974030001761663 | validation: 2.507625986442826]
	TIME [epoch: 10.4 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977882895250965		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 1.2977882895250965 | validation: 2.524191391755712]
	TIME [epoch: 10.4 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988780472685504		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 1.2988780472685504 | validation: 2.5600819084215725]
	TIME [epoch: 10.4 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3075979237051298		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 1.3075979237051298 | validation: 2.5023904131006622]
	TIME [epoch: 10.4 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303798162591402		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 1.303798162591402 | validation: 2.5097621954919123]
	TIME [epoch: 10.4 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988512674258466		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 1.2988512674258466 | validation: 2.4963781057333954]
	TIME [epoch: 10.4 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297238529880385		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 1.297238529880385 | validation: 2.5041022833616835]
	TIME [epoch: 10.4 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2971552431161681		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 1.2971552431161681 | validation: 2.50805327481927]
	TIME [epoch: 10.4 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960334806449523		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 1.2960334806449523 | validation: 2.5150643102984866]
	TIME [epoch: 10.4 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294654954729945		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 1.294654954729945 | validation: 2.5123476208180233]
	TIME [epoch: 10.4 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2997251814051531		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 1.2997251814051531 | validation: 2.5119876911793146]
	TIME [epoch: 10.4 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3107213762349386		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 1.3107213762349386 | validation: 2.509662613775648]
	TIME [epoch: 10.4 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303256156930555		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 1.303256156930555 | validation: 2.49322663874795]
	TIME [epoch: 10.4 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958562239198768		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 1.2958562239198768 | validation: 2.5045807564037954]
	TIME [epoch: 10.4 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297617868633633		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 1.297617868633633 | validation: 2.5065778726760817]
	TIME [epoch: 10.4 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3021282051589693		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 1.3021282051589693 | validation: 2.5220300396757196]
	TIME [epoch: 10.4 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3065660016760425		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 1.3065660016760425 | validation: 2.500902323037995]
	TIME [epoch: 10.4 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004802766444077		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 1.3004802766444077 | validation: 2.4997787768018678]
	TIME [epoch: 10.4 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299445020000177		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 1.299445020000177 | validation: 2.5155740530334594]
	TIME [epoch: 10.4 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2999630007361684		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 1.2999630007361684 | validation: 2.560249980109715]
	TIME [epoch: 10.4 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2964321385447788		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 1.2964321385447788 | validation: 2.5201200535782555]
	TIME [epoch: 10.4 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005911818468143		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 1.3005911818468143 | validation: 2.521028393203073]
	TIME [epoch: 10.4 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3043047237988905		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 1.3043047237988905 | validation: 2.5123899739832223]
	TIME [epoch: 10.4 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.306553953815361		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 1.306553953815361 | validation: 2.525589450247447]
	TIME [epoch: 10.4 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966399705493639		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 1.2966399705493639 | validation: 2.512586917026653]
	TIME [epoch: 10.4 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294854106322984		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 1.294854106322984 | validation: 2.504613791995148]
	TIME [epoch: 10.4 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992248925569467		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 1.2992248925569467 | validation: 2.5038731251194317]
	TIME [epoch: 10.4 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2911817040538778		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 1.2911817040538778 | validation: 2.5506598825439797]
	TIME [epoch: 10.4 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3009491446433548		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 1.3009491446433548 | validation: 2.5036699569935066]
	TIME [epoch: 10.4 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2969051591972367		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 1.2969051591972367 | validation: 2.5506043417288837]
	TIME [epoch: 10.4 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956578779886647		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 1.2956578779886647 | validation: 2.556470885409371]
	TIME [epoch: 10.4 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2944826276066697		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 1.2944826276066697 | validation: 2.494518523894188]
	TIME [epoch: 10.4 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996180978033824		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 1.2996180978033824 | validation: 2.5113825054981254]
	TIME [epoch: 10.4 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2928871081755093		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 1.2928871081755093 | validation: 2.515956600818836]
	TIME [epoch: 10.4 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29264211396987		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 1.29264211396987 | validation: 2.5334197180019964]
	TIME [epoch: 10.4 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3014216469211617		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 1.3014216469211617 | validation: 2.5441177153292838]
	TIME [epoch: 10.4 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973881165394494		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 1.2973881165394494 | validation: 2.513847948804843]
	TIME [epoch: 10.4 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966598872043575		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 1.2966598872043575 | validation: 2.515008431936627]
	TIME [epoch: 10.4 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018801803236744		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 1.3018801803236744 | validation: 2.510356379094132]
	TIME [epoch: 10.4 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.307083645354201		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 1.307083645354201 | validation: 2.5159208805446402]
	TIME [epoch: 10.4 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304823887490373		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 1.304823887490373 | validation: 2.500148888017613]
	TIME [epoch: 10.4 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2940518823296325		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 1.2940518823296325 | validation: 2.5223513848466794]
	TIME [epoch: 10.4 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303021509630465		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 1.303021509630465 | validation: 2.5104874717244323]
	TIME [epoch: 10.4 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303474837254556		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 1.303474837254556 | validation: 2.5496570418328854]
	TIME [epoch: 10.4 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2987888648056782		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 1.2987888648056782 | validation: 2.50624490030396]
	TIME [epoch: 10.4 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2938922587224335		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 1.2938922587224335 | validation: 2.5071426591673425]
	TIME [epoch: 10.4 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933456397782812		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 1.2933456397782812 | validation: 2.504418976914692]
	TIME [epoch: 10.4 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3063958136144262		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 1.3063958136144262 | validation: 2.537871561256999]
	TIME [epoch: 10.4 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.289354959307354		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 1.289354959307354 | validation: 2.511006551143736]
	TIME [epoch: 10.4 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2969891407180554		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 1.2969891407180554 | validation: 2.543042730745651]
	TIME [epoch: 10.4 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2970824356004385		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 1.2970824356004385 | validation: 2.5107460452760884]
	TIME [epoch: 10.4 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972747665887114		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 1.2972747665887114 | validation: 2.5061846990443337]
	TIME [epoch: 10.4 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302316195000261		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 1.302316195000261 | validation: 2.5025706840660664]
	TIME [epoch: 10.4 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005377402742158		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 1.3005377402742158 | validation: 2.511396660833497]
	TIME [epoch: 10.4 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996815295677766		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 1.2996815295677766 | validation: 2.503579802869738]
	TIME [epoch: 10.4 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296303288610772		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 1.296303288610772 | validation: 2.5081592622339355]
	TIME [epoch: 10.4 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302087946729387		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 1.302087946729387 | validation: 2.554990309692598]
	TIME [epoch: 10.4 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024317522607867		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 1.3024317522607867 | validation: 2.529092771969914]
	TIME [epoch: 10.4 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2962168830467724		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 1.2962168830467724 | validation: 2.501942297902142]
	TIME [epoch: 10.4 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3063809212214836		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 1.3063809212214836 | validation: 2.5114536881484186]
	TIME [epoch: 10.4 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299607790920143		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 1.299607790920143 | validation: 2.517980779715394]
	TIME [epoch: 10.4 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2975290803852653		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 1.2975290803852653 | validation: 2.505256562761381]
	TIME [epoch: 10.4 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967744279997837		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 1.2967744279997837 | validation: 2.494104488944127]
	TIME [epoch: 10.4 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988615692664913		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 1.2988615692664913 | validation: 2.501600999564744]
	TIME [epoch: 10.4 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30098666001418		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 1.30098666001418 | validation: 2.520359100506512]
	TIME [epoch: 10.4 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3009799395227446		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 1.3009799395227446 | validation: 2.505648406019209]
	TIME [epoch: 10.4 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294680447396806		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 1.294680447396806 | validation: 2.5037852835655894]
	TIME [epoch: 10.4 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958524798344615		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 1.2958524798344615 | validation: 2.5041073362789406]
	TIME [epoch: 10.4 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3006442356527799		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 1.3006442356527799 | validation: 2.511011545602866]
	TIME [epoch: 10.4 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3003224564350018		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 1.3003224564350018 | validation: 2.5119638149543375]
	TIME [epoch: 10.4 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3070281474154677		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 1.3070281474154677 | validation: 2.500873438861735]
	TIME [epoch: 10.4 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2975160777603312		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 1.2975160777603312 | validation: 2.5081082753451818]
	TIME [epoch: 10.4 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993937794436932		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 1.2993937794436932 | validation: 2.5544716643760554]
	TIME [epoch: 10.4 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2948948824469217		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 1.2948948824469217 | validation: 2.5129503075290427]
	TIME [epoch: 10.4 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3064030917980947		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 1.3064030917980947 | validation: 2.519852344277958]
	TIME [epoch: 10.5 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3030800245023548		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 1.3030800245023548 | validation: 2.5566258654038982]
	TIME [epoch: 10.4 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2924843037982183		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 1.2924843037982183 | validation: 2.506274711825315]
	TIME [epoch: 10.4 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2955010847191293		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 1.2955010847191293 | validation: 2.5490037989658396]
	TIME [epoch: 10.4 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2929218409410992		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 1.2929218409410992 | validation: 2.5058555252114196]
	TIME [epoch: 10.4 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3038710276054344		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 1.3038710276054344 | validation: 2.5824096586232907]
	TIME [epoch: 10.4 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2946605435815504		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 1.2946605435815504 | validation: 2.5065154344314795]
	TIME [epoch: 10.4 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3014129418096179		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 1.3014129418096179 | validation: 2.501177743908246]
	TIME [epoch: 10.4 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956308306287734		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 1.2956308306287734 | validation: 2.5081359755250126]
	TIME [epoch: 10.4 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3027924624324334		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 1.3027924624324334 | validation: 2.515974743401263]
	TIME [epoch: 10.4 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3079099769232483		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 1.3079099769232483 | validation: 2.5177335724421552]
	TIME [epoch: 10.4 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018049273141958		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 1.3018049273141958 | validation: 2.506370882537173]
	TIME [epoch: 10.4 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972373787842535		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 1.2972373787842535 | validation: 2.510346095097954]
	TIME [epoch: 10.4 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29682828724915		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 1.29682828724915 | validation: 2.54461677920056]
	TIME [epoch: 10.4 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2971330847291196		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 1.2971330847291196 | validation: 2.5132123341895736]
	TIME [epoch: 10.4 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3011511103961182		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 1.3011511103961182 | validation: 2.5089725058584884]
	TIME [epoch: 10.4 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3016525229625822		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 1.3016525229625822 | validation: 2.50292795140327]
	TIME [epoch: 10.4 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299406140867292		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 1.299406140867292 | validation: 2.497759727788926]
	TIME [epoch: 10.4 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3042896344163757		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 1.3042896344163757 | validation: 2.5577054390194784]
	TIME [epoch: 10.4 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995224746756422		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 1.2995224746756422 | validation: 2.4924993267872178]
	TIME [epoch: 10.4 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302662488860986		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 1.302662488860986 | validation: 2.5151198025328436]
	TIME [epoch: 10.4 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293487402303499		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 1.293487402303499 | validation: 2.4896143585443187]
	TIME [epoch: 10.3 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2981054288414577		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 1.2981054288414577 | validation: 2.504867601459728]
	TIME [epoch: 10.4 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3043093170085727		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 1.3043093170085727 | validation: 2.5131580601653134]
	TIME [epoch: 10.4 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3047216271139344		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 1.3047216271139344 | validation: 2.5178295215759268]
	TIME [epoch: 10.4 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303556900899571		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 1.303556900899571 | validation: 2.5173516399297946]
	TIME [epoch: 10.4 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967329023418777		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 1.2967329023418777 | validation: 2.528098554583467]
	TIME [epoch: 10.4 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3017949054740394		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 1.3017949054740394 | validation: 2.507837162982046]
	TIME [epoch: 10.4 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977064008888062		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 1.2977064008888062 | validation: 2.5195726084287613]
	TIME [epoch: 10.4 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2983378941233075		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 1.2983378941233075 | validation: 2.511429308132625]
	TIME [epoch: 10.4 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973842731142955		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 1.2973842731142955 | validation: 2.5135955098041367]
	TIME [epoch: 10.4 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299860203492246		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 1.299860203492246 | validation: 2.51080870997918]
	TIME [epoch: 10.4 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2911923832351995		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 1.2911923832351995 | validation: 2.508796821346722]
	TIME [epoch: 10.4 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2916031063874414		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 1.2916031063874414 | validation: 2.5066143313991187]
	TIME [epoch: 10.4 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3055006342820756		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 1.3055006342820756 | validation: 2.5020282952526216]
	TIME [epoch: 10.4 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291860033666515		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 1.291860033666515 | validation: 2.512088927258612]
	TIME [epoch: 10.4 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3009764077785941		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 1.3009764077785941 | validation: 2.5118050450731593]
	TIME [epoch: 10.4 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293826440733263		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 1.293826440733263 | validation: 2.548485675700839]
	TIME [epoch: 10.4 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3037724903494694		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 1.3037724903494694 | validation: 2.5508632747701494]
	TIME [epoch: 10.4 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295561296664775		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 1.295561296664775 | validation: 2.5084454671499072]
	TIME [epoch: 10.4 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2979142548025453		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 1.2979142548025453 | validation: 2.500990269977417]
	TIME [epoch: 10.4 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943880094246174		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 1.2943880094246174 | validation: 2.5091857333056713]
	TIME [epoch: 10.4 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977268053852329		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 1.2977268053852329 | validation: 2.5123217311722086]
	TIME [epoch: 10.4 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2937660969165823		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 1.2937660969165823 | validation: 2.5177066374204364]
	TIME [epoch: 10.4 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2942794116051857		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 1.2942794116051857 | validation: 2.5124370523197017]
	TIME [epoch: 10.4 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982029223284717		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 1.2982029223284717 | validation: 2.502938843216069]
	TIME [epoch: 10.4 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2985121227633318		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 1.2985121227633318 | validation: 2.522850056520169]
	TIME [epoch: 10.4 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960819553595262		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 1.2960819553595262 | validation: 2.5112451949274353]
	TIME [epoch: 10.4 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007219414325646		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 1.3007219414325646 | validation: 2.5157149218016865]
	TIME [epoch: 10.4 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3034159824288687		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 1.3034159824288687 | validation: 2.5607522338238415]
	TIME [epoch: 10.4 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2990705024323839		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 1.2990705024323839 | validation: 2.5182410735487317]
	TIME [epoch: 10.4 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2955876773452293		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 1.2955876773452293 | validation: 2.5573797694768388]
	TIME [epoch: 10.4 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294916916974883		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 1.294916916974883 | validation: 2.4999296620536997]
	TIME [epoch: 10.4 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2997055918574245		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 1.2997055918574245 | validation: 2.5003592626475477]
	TIME [epoch: 10.4 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2979870152838848		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 1.2979870152838848 | validation: 2.5130476306016067]
	TIME [epoch: 10.4 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3033112808119673		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 1.3033112808119673 | validation: 2.5160843037010925]
	TIME [epoch: 10.4 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960410636427941		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 1.2960410636427941 | validation: 2.5104278721655486]
	TIME [epoch: 10.4 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2946310357321704		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 1.2946310357321704 | validation: 2.5110891753726476]
	TIME [epoch: 10.4 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976064903735878		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 1.2976064903735878 | validation: 2.5097894410774106]
	TIME [epoch: 10.4 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298424362795412		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 1.298424362795412 | validation: 2.5091027749964514]
	TIME [epoch: 10.4 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2959669474986937		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 1.2959669474986937 | validation: 2.4991491011357714]
	TIME [epoch: 10.4 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973813542640487		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 1.2973813542640487 | validation: 2.50318129191082]
	TIME [epoch: 10.4 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2975879892712412		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 1.2975879892712412 | validation: 2.516063343244003]
	TIME [epoch: 10.4 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001210331082311		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 1.3001210331082311 | validation: 2.5825274568318024]
	TIME [epoch: 10.4 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293562146941158		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 1.293562146941158 | validation: 2.500045598616411]
	TIME [epoch: 10.4 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.304748811011014		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 1.304748811011014 | validation: 2.5165527936629304]
	TIME [epoch: 10.4 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29859506526823		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 1.29859506526823 | validation: 2.5175808552818513]
	TIME [epoch: 10.4 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966860555304531		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 1.2966860555304531 | validation: 2.5090313216520004]
	TIME [epoch: 10.4 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977983560799697		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 1.2977983560799697 | validation: 2.5065316837652536]
	TIME [epoch: 10.4 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3046602153956712		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 1.3046602153956712 | validation: 2.5128651249966953]
	TIME [epoch: 10.4 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024852682230876		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 1.3024852682230876 | validation: 2.5000158494119264]
	TIME [epoch: 10.4 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2917952916866657		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 1.2917952916866657 | validation: 2.518302942293761]
	TIME [epoch: 10.4 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004458826170502		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 1.3004458826170502 | validation: 2.5128843934602494]
	TIME [epoch: 10.4 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3016868192962838		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 1.3016868192962838 | validation: 2.5174296605702526]
	TIME [epoch: 10.4 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3006724451597846		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 1.3006724451597846 | validation: 2.4996505953636587]
	TIME [epoch: 10.4 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2962504246369462		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 1.2962504246369462 | validation: 2.5305435987995986]
	TIME [epoch: 10.4 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2931473783389478		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 1.2931473783389478 | validation: 2.504457166926463]
	TIME [epoch: 10.4 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2915914913694828		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 1.2915914913694828 | validation: 2.5177973072746656]
	TIME [epoch: 10.4 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958767161991704		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 1.2958767161991704 | validation: 2.511672447604164]
	TIME [epoch: 10.4 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2984978896132702		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 1.2984978896132702 | validation: 2.4971483673001287]
	TIME [epoch: 10.4 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996611790462318		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 1.2996611790462318 | validation: 2.5123546572328563]
	TIME [epoch: 10.4 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3029605830683768		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 1.3029605830683768 | validation: 2.4959705280869153]
	TIME [epoch: 10.4 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996939373006373		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 1.2996939373006373 | validation: 2.5002298968850174]
	TIME [epoch: 10.4 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2893161680064487		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 1.2893161680064487 | validation: 2.5038639164231795]
	TIME [epoch: 10.4 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.290942134287901		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 1.290942134287901 | validation: 2.517708247338471]
	TIME [epoch: 10.4 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3021427460951522		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 1.3021427460951522 | validation: 2.506320633583517]
	TIME [epoch: 10.4 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2981115230332942		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 1.2981115230332942 | validation: 2.5035653441797865]
	TIME [epoch: 10.4 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294096196869097		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 1.294096196869097 | validation: 2.5157555855344955]
	TIME [epoch: 10.4 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295961382985091		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 1.295961382985091 | validation: 2.5041698478655996]
	TIME [epoch: 10.4 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2985248865833612		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 1.2985248865833612 | validation: 2.510924640266191]
	TIME [epoch: 10.3 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3002368571246374		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 1.3002368571246374 | validation: 2.50640767258755]
	TIME [epoch: 10.4 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2991666401484154		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 1.2991666401484154 | validation: 2.520337287750523]
	TIME [epoch: 10.4 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2940559306598904		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 1.2940559306598904 | validation: 2.500338074060856]
	TIME [epoch: 10.4 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001129699725094		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 1.3001129699725094 | validation: 2.5037545722297376]
	TIME [epoch: 10.4 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977213689881228		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 1.2977213689881228 | validation: 2.5202996622642053]
	TIME [epoch: 10.4 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2963578978390173		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 1.2963578978390173 | validation: 2.5193103913529313]
	TIME [epoch: 10.4 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2923869184142747		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 1.2923869184142747 | validation: 2.56132360093518]
	TIME [epoch: 10.4 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2991343704224785		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 1.2991343704224785 | validation: 2.49214421760302]
	TIME [epoch: 10.4 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3039328442573634		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 1.3039328442573634 | validation: 2.5059878889165894]
	TIME [epoch: 10.4 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302334709652537		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 1.302334709652537 | validation: 2.514490188126175]
	TIME [epoch: 10.4 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2979085573081757		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 1.2979085573081757 | validation: 2.5018979252432048]
	TIME [epoch: 10.4 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993963338970478		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 1.2993963338970478 | validation: 2.5093743806428725]
	TIME [epoch: 10.4 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3027372945833378		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 1.3027372945833378 | validation: 2.4960880930253597]
	TIME [epoch: 10.4 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3039763252997487		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 1.3039763252997487 | validation: 2.4932468867840387]
	TIME [epoch: 10.4 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29676078415562		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 1.29676078415562 | validation: 2.5284421409830884]
	TIME [epoch: 10.4 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3032565616946645		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 1.3032565616946645 | validation: 2.5539438653072075]
	TIME [epoch: 10.4 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291752320175506		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 1.291752320175506 | validation: 2.5014343979656033]
	TIME [epoch: 10.4 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2997912223829196		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 1.2997912223829196 | validation: 2.52610041237873]
	TIME [epoch: 10.4 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296568972252933		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 1.296568972252933 | validation: 2.5007145340837393]
	TIME [epoch: 10.4 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977046425789145		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 1.2977046425789145 | validation: 2.5078375286376264]
	TIME [epoch: 10.4 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297762090327424		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 1.297762090327424 | validation: 2.5499891306754523]
	TIME [epoch: 10.4 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980655795897733		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 1.2980655795897733 | validation: 2.5040495983955404]
	TIME [epoch: 10.4 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2948575927218118		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 1.2948575927218118 | validation: 2.526455052306893]
	TIME [epoch: 10.4 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296490009616552		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 1.296490009616552 | validation: 2.5050937070985375]
	TIME [epoch: 10.4 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2941623588116289		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 1.2941623588116289 | validation: 2.501656791809186]
	TIME [epoch: 10.4 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2930653073835303		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 1.2930653073835303 | validation: 2.5111306058771987]
	TIME [epoch: 10.4 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956984566869711		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 1.2956984566869711 | validation: 2.5025826795856325]
	TIME [epoch: 10.4 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3048120840893818		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 1.3048120840893818 | validation: 2.5099577790863687]
	TIME [epoch: 10.4 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001622034193099		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 1.3001622034193099 | validation: 2.4994219898142025]
	TIME [epoch: 10.4 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988824571238262		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 1.2988824571238262 | validation: 2.509751727205844]
	TIME [epoch: 10.4 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933285039857902		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 1.2933285039857902 | validation: 2.5044026341510177]
	TIME [epoch: 10.4 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299722930792638		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 1.299722930792638 | validation: 2.512119814470845]
	TIME [epoch: 10.4 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976981500748024		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 1.2976981500748024 | validation: 2.528402504744643]
	TIME [epoch: 10.4 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3010901888178938		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 1.3010901888178938 | validation: 2.5870109860736914]
	TIME [epoch: 10.4 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2953688379863295		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 1.2953688379863295 | validation: 2.500911131608233]
	TIME [epoch: 10.4 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2927708441058638		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 1.2927708441058638 | validation: 2.502784809585623]
	TIME [epoch: 10.4 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3055101945740173		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 1.3055101945740173 | validation: 2.5182369118814125]
	TIME [epoch: 10.4 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972166959806848		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 1.2972166959806848 | validation: 2.50858732761168]
	TIME [epoch: 10.4 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3020587977405425		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 1.3020587977405425 | validation: 2.5091199428598934]
	TIME [epoch: 10.4 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3054060896612452		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 1.3054060896612452 | validation: 2.5064768321162694]
	TIME [epoch: 10.4 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3012416473163242		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 1.3012416473163242 | validation: 2.5096838983115703]
	TIME [epoch: 10.4 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976282974905158		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 1.2976282974905158 | validation: 2.536314563717685]
	TIME [epoch: 10.4 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966296808220579		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 1.2966296808220579 | validation: 2.509330553259787]
	TIME [epoch: 10.4 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.290860375863062		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 1.290860375863062 | validation: 2.5196339247236894]
	TIME [epoch: 10.4 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3032480339100507		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 1.3032480339100507 | validation: 2.497217015611319]
	TIME [epoch: 10.4 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298767360836262		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 1.298767360836262 | validation: 2.5683176257460594]
	TIME [epoch: 10.4 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29788723175546		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 1.29788723175546 | validation: 2.5124687634247795]
	TIME [epoch: 10.4 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2936113250728174		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 1.2936113250728174 | validation: 2.565805693214985]
	TIME [epoch: 10.4 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2948132919041628		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 1.2948132919041628 | validation: 2.510289168297848]
	TIME [epoch: 10.4 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3019098331609495		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 1.3019098331609495 | validation: 2.51529503675822]
	TIME [epoch: 10.4 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966349975200495		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 1.2966349975200495 | validation: 2.521115911525053]
	TIME [epoch: 10.4 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.292322929850784		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 1.292322929850784 | validation: 2.503999741504966]
	TIME [epoch: 10.4 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005045825620332		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 1.3005045825620332 | validation: 2.518280444061842]
	TIME [epoch: 10.4 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2981340986826724		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 1.2981340986826724 | validation: 2.5030545044475705]
	TIME [epoch: 10.4 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2938042934275022		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 1.2938042934275022 | validation: 2.515018769513728]
	TIME [epoch: 10.4 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3013685980125596		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 1.3013685980125596 | validation: 2.4978392125918742]
	TIME [epoch: 10.4 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967193409574522		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 1.2967193409574522 | validation: 2.5133874130251317]
	TIME [epoch: 10.4 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3040255877825366		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 1.3040255877825366 | validation: 2.5035296287013367]
	TIME [epoch: 10.4 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298025778469123		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 1.298025778469123 | validation: 2.5172281747588956]
	TIME [epoch: 10.4 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966406982396275		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 1.2966406982396275 | validation: 2.547709230965029]
	TIME [epoch: 10.4 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988304411685634		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 1.2988304411685634 | validation: 2.507617723268662]
	TIME [epoch: 10.4 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3000912868076484		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 1.3000912868076484 | validation: 2.4883251232135435]
	TIME [epoch: 10.4 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2939479284110682		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 1.2939479284110682 | validation: 2.5025572923551995]
	TIME [epoch: 10.4 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2926410629204592		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 1.2926410629204592 | validation: 2.5022975594281]
	TIME [epoch: 10.4 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2969640643491647		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 1.2969640643491647 | validation: 2.5112853499759384]
	TIME [epoch: 10.4 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2954919589035523		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 1.2954919589035523 | validation: 2.520270738713676]
	TIME [epoch: 10.4 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2887609734232464		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 1.2887609734232464 | validation: 2.5055670628349707]
	TIME [epoch: 10.4 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2963936441562514		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 1.2963936441562514 | validation: 2.5035979210207264]
	TIME [epoch: 10.4 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2926092630878525		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 1.2926092630878525 | validation: 2.502532854959886]
	TIME [epoch: 10.4 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.290615503211944		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 1.290615503211944 | validation: 2.5056727316952037]
	TIME [epoch: 10.4 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992457666060375		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 1.2992457666060375 | validation: 2.5384911404655273]
	TIME [epoch: 10.4 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3043979951833313		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 1.3043979951833313 | validation: 2.5523646348586886]
	TIME [epoch: 10.4 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2999998264986163		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 1.2999998264986163 | validation: 2.511331911498495]
	TIME [epoch: 10.4 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2962210235403664		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 1.2962210235403664 | validation: 2.505710473158095]
	TIME [epoch: 10.4 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2984545985059408		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 1.2984545985059408 | validation: 2.5013158338649477]
	TIME [epoch: 10.4 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29675525487818		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 1.29675525487818 | validation: 2.506518585805455]
	TIME [epoch: 10.4 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024388309558987		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 1.3024388309558987 | validation: 2.502232314034736]
	TIME [epoch: 10.4 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3015841121627574		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 1.3015841121627574 | validation: 2.513989817591919]
	TIME [epoch: 10.4 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2975091311094507		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 1.2975091311094507 | validation: 2.5165422569309985]
	TIME [epoch: 10.4 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3009056370370096		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 1.3009056370370096 | validation: 2.512556591092167]
	TIME [epoch: 10.4 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305267072922109		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 1.305267072922109 | validation: 2.528121109510176]
	TIME [epoch: 10.4 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2939880579829732		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 1.2939880579829732 | validation: 2.514563872002915]
	TIME [epoch: 10.4 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2955226979438295		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 1.2955226979438295 | validation: 2.492544482344373]
	TIME [epoch: 10.4 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2906670098263662		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 1.2906670098263662 | validation: 2.5337661639364617]
	TIME [epoch: 10.4 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3031672576697249		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 1.3031672576697249 | validation: 2.5103375690662255]
	TIME [epoch: 10.4 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982101539881645		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 1.2982101539881645 | validation: 2.5031381309072804]
	TIME [epoch: 10.4 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2950369373104493		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 1.2950369373104493 | validation: 2.5006333156504916]
	TIME [epoch: 10.4 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001076083165528		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 1.3001076083165528 | validation: 2.5076268550985557]
	TIME [epoch: 10.4 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2974763247763743		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 1.2974763247763743 | validation: 2.54648883525065]
	TIME [epoch: 10.4 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966721237900707		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 1.2966721237900707 | validation: 2.5159413266756827]
	TIME [epoch: 10.4 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296257682109469		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 1.296257682109469 | validation: 2.5079546692382775]
	TIME [epoch: 10.4 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2920376407872596		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 1.2920376407872596 | validation: 2.512515062476388]
	TIME [epoch: 10.4 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302873724201292		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 1.302873724201292 | validation: 2.5054197757531975]
	TIME [epoch: 10.4 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2999378637816001		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 1.2999378637816001 | validation: 2.51775417103998]
	TIME [epoch: 10.4 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2961699694556237		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 1.2961699694556237 | validation: 2.4989772723404147]
	TIME [epoch: 10.4 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.292771909399693		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 1.292771909399693 | validation: 2.512576004285669]
	TIME [epoch: 10.4 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2918486213259845		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 1.2918486213259845 | validation: 2.50331021284583]
	TIME [epoch: 10.4 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2953116950226986		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 1.2953116950226986 | validation: 2.5108175425405843]
	TIME [epoch: 10.4 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2891751654069508		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 1.2891751654069508 | validation: 2.5084261383623345]
	TIME [epoch: 10.4 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3006113943962627		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 1.3006113943962627 | validation: 2.5050074502974478]
	TIME [epoch: 10.4 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2977034399038019		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 1.2977034399038019 | validation: 2.5055068344895255]
	TIME [epoch: 10.4 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2955759155336295		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 1.2955759155336295 | validation: 2.5098047713124236]
	TIME [epoch: 10.4 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3040848131866805		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 1.3040848131866805 | validation: 2.5124632998311807]
	TIME [epoch: 10.4 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966394583169265		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 1.2966394583169265 | validation: 2.508010837449073]
	TIME [epoch: 10.4 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2987424936092953		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 1.2987424936092953 | validation: 2.51684726593805]
	TIME [epoch: 10.4 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302003540569002		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 1.302003540569002 | validation: 2.564454484754951]
	TIME [epoch: 10.4 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2937320535579537		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 1.2937320535579537 | validation: 2.4995554401028026]
	TIME [epoch: 10.4 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943968815727498		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 1.2943968815727498 | validation: 2.5332764523786055]
	TIME [epoch: 10.4 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3000203336047351		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 1.3000203336047351 | validation: 2.5114413272611906]
	TIME [epoch: 10.4 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966553887271952		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 1.2966553887271952 | validation: 2.5095135877193724]
	TIME [epoch: 10.4 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996989413448106		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 1.2996989413448106 | validation: 2.511567863393476]
	TIME [epoch: 10.4 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982689733438786		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 1.2982689733438786 | validation: 2.504577978898152]
	TIME [epoch: 10.4 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976971025022763		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 1.2976971025022763 | validation: 2.504494237022891]
	TIME [epoch: 10.4 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3016855114597714		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 1.3016855114597714 | validation: 2.5149337251243473]
	TIME [epoch: 10.4 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2892360435889698		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 1.2892360435889698 | validation: 2.510539284734162]
	TIME [epoch: 10.4 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2931462936831277		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 1.2931462936831277 | validation: 2.5516649531865023]
	TIME [epoch: 10.4 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2957210473962153		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 1.2957210473962153 | validation: 2.5083471193548683]
	TIME [epoch: 10.4 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967404273964367		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 1.2967404273964367 | validation: 2.50078006015581]
	TIME [epoch: 10.4 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300099818074727		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 1.300099818074727 | validation: 2.5046256255949313]
	TIME [epoch: 10.4 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2920787604771748		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 1.2920787604771748 | validation: 2.562205363089901]
	TIME [epoch: 10.4 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297046221945742		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 1.297046221945742 | validation: 2.5285706596752626]
	TIME [epoch: 10.4 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295836020595051		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 1.295836020595051 | validation: 2.5188475676748996]
	TIME [epoch: 10.4 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2955483325085069		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 1.2955483325085069 | validation: 2.516236968978207]
	TIME [epoch: 10.4 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.292033851034716		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 1.292033851034716 | validation: 2.5090871545149245]
	TIME [epoch: 10.4 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982766235682455		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 1.2982766235682455 | validation: 2.4985694079122918]
	TIME [epoch: 10.4 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968475984584529		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 1.2968475984584529 | validation: 2.500303829835393]
	TIME [epoch: 10.4 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958388878384453		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 1.2958388878384453 | validation: 2.509191864864838]
	TIME [epoch: 10.4 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933047744759207		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 1.2933047744759207 | validation: 2.4922127809358297]
	TIME [epoch: 10.4 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2927706366512677		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 1.2927706366512677 | validation: 2.5514292426124547]
	TIME [epoch: 10.4 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.292894596051012		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 1.292894596051012 | validation: 2.511890612900682]
	TIME [epoch: 10.4 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2931282758684162		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 1.2931282758684162 | validation: 2.50382170158098]
	TIME [epoch: 10.4 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018085467986897		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 1.3018085467986897 | validation: 2.5100434001169254]
	TIME [epoch: 10.4 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2952543507863221		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 1.2952543507863221 | validation: 2.5626149804338945]
	TIME [epoch: 10.4 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2951768280359932		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 1.2951768280359932 | validation: 2.504380994306927]
	TIME [epoch: 10.4 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3000955473513052		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 1.3000955473513052 | validation: 2.5005813391859797]
	TIME [epoch: 10.4 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958754786543594		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 1.2958754786543594 | validation: 2.556372746256753]
	TIME [epoch: 10.4 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2986759444591023		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 1.2986759444591023 | validation: 2.508235179957606]
	TIME [epoch: 10.4 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3000925242303345		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 1.3000925242303345 | validation: 2.543158066369357]
	TIME [epoch: 10.4 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2920594121556275		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 1.2920594121556275 | validation: 2.5021221649217726]
	TIME [epoch: 10.4 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2952585990559686		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 1.2952585990559686 | validation: 2.4967714519204907]
	TIME [epoch: 10.4 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2983444900306529		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 1.2983444900306529 | validation: 2.5013461560246286]
	TIME [epoch: 10.4 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2937432780610791		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 1.2937432780610791 | validation: 2.5073081331226343]
	TIME [epoch: 10.4 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973748963372604		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 1.2973748963372604 | validation: 2.4948612123892704]
	TIME [epoch: 10.4 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2918092963224985		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 1.2918092963224985 | validation: 2.518059261983051]
	TIME [epoch: 10.4 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2970784907918322		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 1.2970784907918322 | validation: 2.515648988678641]
	TIME [epoch: 10.4 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3032758363921304		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 1.3032758363921304 | validation: 2.515688232827065]
	TIME [epoch: 10.4 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300820241316402		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 1.300820241316402 | validation: 2.501505938363344]
	TIME [epoch: 10.4 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2883307693332902		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 1.2883307693332902 | validation: 2.511284550936882]
	TIME [epoch: 10.4 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297473539070742		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 1.297473539070742 | validation: 2.5078258667870923]
	TIME [epoch: 10.4 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2939599801203974		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 1.2939599801203974 | validation: 2.4980468334010193]
	TIME [epoch: 10.4 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298568529056611		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 1.298568529056611 | validation: 2.499490430608003]
	TIME [epoch: 10.4 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973222224393992		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 1.2973222224393992 | validation: 2.508600744322605]
	TIME [epoch: 10.4 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973201194314314		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 1.2973201194314314 | validation: 2.505588340276771]
	TIME [epoch: 10.4 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2922149322397618		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 1.2922149322397618 | validation: 2.5025227386548408]
	TIME [epoch: 10.4 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2975452730885793		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 1.2975452730885793 | validation: 2.50896491395974]
	TIME [epoch: 10.4 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004190316807356		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 1.3004190316807356 | validation: 2.514736142056083]
	TIME [epoch: 10.4 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2987733130072647		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 1.2987733130072647 | validation: 2.5244210776791007]
	TIME [epoch: 10.4 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2908390415672966		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 1.2908390415672966 | validation: 2.515693669255602]
	TIME [epoch: 10.4 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2945837788187837		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 1.2945837788187837 | validation: 2.494277270434311]
	TIME [epoch: 10.4 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2986844921382679		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 1.2986844921382679 | validation: 2.5036944211069727]
	TIME [epoch: 10.4 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2970465519697183		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 1.2970465519697183 | validation: 2.558019465306383]
	TIME [epoch: 10.4 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007904618379416		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 1.3007904618379416 | validation: 2.5135572305753926]
	TIME [epoch: 10.4 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2947677501813402		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 1.2947677501813402 | validation: 2.5613190190574744]
	TIME [epoch: 10.4 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301764221702747		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 1.301764221702747 | validation: 2.5147523727716443]
	TIME [epoch: 10.4 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2946586755271858		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 1.2946586755271858 | validation: 2.509937073897742]
	TIME [epoch: 10.4 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024748055056867		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 1.3024748055056867 | validation: 2.5028840230364517]
	TIME [epoch: 10.4 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.290766223412581		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 1.290766223412581 | validation: 2.5004601061891685]
	TIME [epoch: 10.4 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2938106549964954		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 1.2938106549964954 | validation: 2.500775874295471]
	TIME [epoch: 10.4 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2923134799532066		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 1.2923134799532066 | validation: 2.506988138643421]
	TIME [epoch: 10.4 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2989331058373144		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 1.2989331058373144 | validation: 2.5088907069805653]
	TIME [epoch: 10.4 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2911155298436172		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 1.2911155298436172 | validation: 2.5105792756236287]
	TIME [epoch: 10.4 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2945072237551165		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 1.2945072237551165 | validation: 2.561919199906992]
	TIME [epoch: 10.4 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296237839168492		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 1.296237839168492 | validation: 2.5103916828615542]
	TIME [epoch: 10.4 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3034868779499056		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 1.3034868779499056 | validation: 2.5028394277698034]
	TIME [epoch: 10.4 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2959524055312333		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 1.2959524055312333 | validation: 2.5066546076739353]
	TIME [epoch: 10.4 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2892079170221622		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 1.2892079170221622 | validation: 2.559499958944759]
	TIME [epoch: 10.4 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3013798079438006		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 1.3013798079438006 | validation: 2.515515056706348]
	TIME [epoch: 10.4 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2926722557881294		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 1.2926722557881294 | validation: 2.5065351576953923]
	TIME [epoch: 10.4 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.30618744541956		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 1.30618744541956 | validation: 2.5075153976497733]
	TIME [epoch: 10.4 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2947267982785202		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 1.2947267982785202 | validation: 2.509156171669616]
	TIME [epoch: 10.4 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968761916341145		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 1.2968761916341145 | validation: 2.5109903023503892]
	TIME [epoch: 10.4 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972401137821672		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 1.2972401137821672 | validation: 2.4901092379051466]
	TIME [epoch: 10.4 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2922126922353931		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 1.2922126922353931 | validation: 2.50353921581828]
	TIME [epoch: 10.4 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980640260342677		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 1.2980640260342677 | validation: 2.4930722367024054]
	TIME [epoch: 10.4 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300371727547925		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 1.300371727547925 | validation: 2.4925492997002623]
	TIME [epoch: 10.4 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300575773165738		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 1.300575773165738 | validation: 2.504676507776979]
	TIME [epoch: 10.4 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.288193271188069		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 1.288193271188069 | validation: 2.510182588655039]
	TIME [epoch: 10.4 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956803582640557		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 1.2956803582640557 | validation: 2.544898481241484]
	TIME [epoch: 10.4 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3017434535292038		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 1.3017434535292038 | validation: 2.561484245838442]
	TIME [epoch: 10.4 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2903897823773023		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 1.2903897823773023 | validation: 2.5007087429052666]
	TIME [epoch: 10.4 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2902416064282203		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 1.2902416064282203 | validation: 2.515647641520271]
	TIME [epoch: 10.4 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29339675924202		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 1.29339675924202 | validation: 2.5208316830826325]
	TIME [epoch: 10.4 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298836047471915		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 1.298836047471915 | validation: 2.507045481511435]
	TIME [epoch: 10.4 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004160886044727		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 1.3004160886044727 | validation: 2.498888220219788]
	TIME [epoch: 10.4 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2941845839969668		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 1.2941845839969668 | validation: 2.5131102843495947]
	TIME [epoch: 10.4 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2894083007268413		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 1.2894083007268413 | validation: 2.5142137384571046]
	TIME [epoch: 10.3 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976557159881725		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 1.2976557159881725 | validation: 2.5056649817689136]
	TIME [epoch: 10.3 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297866876369135		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 1.297866876369135 | validation: 2.5104698623083865]
	TIME [epoch: 10.4 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992391031728676		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 1.2992391031728676 | validation: 2.50934933718269]
	TIME [epoch: 10.4 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2966943161070583		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 1.2966943161070583 | validation: 2.500286628723499]
	TIME [epoch: 10.4 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3027524025743378		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 1.3027524025743378 | validation: 2.503647905838651]
	TIME [epoch: 10.4 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2952363242265086		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 1.2952363242265086 | validation: 2.496356611830149]
	TIME [epoch: 10.4 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2932602017434156		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 1.2932602017434156 | validation: 2.497781881810051]
	TIME [epoch: 10.4 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2904954752944255		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 1.2904954752944255 | validation: 2.5647881624116953]
	TIME [epoch: 10.4 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2895820928484163		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 1.2895820928484163 | validation: 2.5591020603048005]
	TIME [epoch: 10.4 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988423281374322		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 1.2988423281374322 | validation: 2.4976501041185197]
	TIME [epoch: 10.4 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2944809850540906		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 1.2944809850540906 | validation: 2.5112372994885908]
	TIME [epoch: 10.4 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2949154171387984		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 1.2949154171387984 | validation: 2.5068044220262893]
	TIME [epoch: 10.4 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960684351673393		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 1.2960684351673393 | validation: 2.5029569165292225]
	TIME [epoch: 10.4 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302407966155099		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 1.302407966155099 | validation: 2.504655739084265]
	TIME [epoch: 10.4 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2949749447422536		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 1.2949749447422536 | validation: 2.496072987717485]
	TIME [epoch: 10.4 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2950053248788083		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 1.2950053248788083 | validation: 2.504391227291993]
	TIME [epoch: 10.4 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303390820012535		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 1.303390820012535 | validation: 2.57613277253742]
	TIME [epoch: 10.4 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295049166301164		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 1.295049166301164 | validation: 2.506811546563292]
	TIME [epoch: 10.4 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2989042905806152		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 1.2989042905806152 | validation: 2.506479030550647]
	TIME [epoch: 10.4 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293341145377044		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 1.293341145377044 | validation: 2.506475426712703]
	TIME [epoch: 10.4 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297116275013814		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 1.297116275013814 | validation: 2.5126243737585545]
	TIME [epoch: 10.3 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.298521294397504		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 1.298521294397504 | validation: 2.4981576843981257]
	TIME [epoch: 10.4 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992496143230805		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 1.2992496143230805 | validation: 2.5056882150159363]
	TIME [epoch: 10.4 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299279276009692		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 1.299279276009692 | validation: 2.5092382332149303]
	TIME [epoch: 10.4 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29787392471578		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 1.29787392471578 | validation: 2.5246583453470444]
	TIME [epoch: 10.4 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2956269306661974		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 1.2956269306661974 | validation: 2.513844913918013]
	TIME [epoch: 10.4 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2928515719069178		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 1.2928515719069178 | validation: 2.5107047673824954]
	TIME [epoch: 10.4 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018057453609237		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 1.3018057453609237 | validation: 2.5089582840833344]
	TIME [epoch: 10.4 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007292903509502		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 1.3007292903509502 | validation: 2.50721881260279]
	TIME [epoch: 10.4 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2940238361421108		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 1.2940238361421108 | validation: 2.5144686789379653]
	TIME [epoch: 10.4 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053673625744853		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 1.3053673625744853 | validation: 2.548830249772624]
	TIME [epoch: 10.4 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302226282359007		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 1.302226282359007 | validation: 2.4935675977120493]
	TIME [epoch: 10.4 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295675134125893		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 1.295675134125893 | validation: 2.553652983852379]
	TIME [epoch: 10.4 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295042525241479		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 1.295042525241479 | validation: 2.51042956770686]
	TIME [epoch: 10.4 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3033743616075735		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 1.3033743616075735 | validation: 2.5010723221519338]
	TIME [epoch: 10.4 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299707543109029		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 1.299707543109029 | validation: 2.5137254700785387]
	TIME [epoch: 10.4 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2989823198951824		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 1.2989823198951824 | validation: 2.4971844073202702]
	TIME [epoch: 10.4 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3018963622483857		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 1.3018963622483857 | validation: 2.5159269469954513]
	TIME [epoch: 10.4 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2939036059359332		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 1.2939036059359332 | validation: 2.5674772288999237]
	TIME [epoch: 10.4 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2926600479849228		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 1.2926600479849228 | validation: 2.502883631789179]
	TIME [epoch: 10.4 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980102938451168		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 1.2980102938451168 | validation: 2.5063012762736396]
	TIME [epoch: 10.4 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3005689919290722		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 1.3005689919290722 | validation: 2.4979726145548593]
	TIME [epoch: 10.4 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933005130012158		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 1.2933005130012158 | validation: 2.5071738255627163]
	TIME [epoch: 10.4 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2917476303895712		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 1.2917476303895712 | validation: 2.4980408151482796]
	TIME [epoch: 10.4 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960497300460898		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 1.2960497300460898 | validation: 2.5136103265348835]
	TIME [epoch: 10.4 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3002780300212942		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 1.3002780300212942 | validation: 2.5591113658649145]
	TIME [epoch: 10.4 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2937205191745036		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 1.2937205191745036 | validation: 2.496351221353212]
	TIME [epoch: 10.4 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3003865210507681		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 1.3003865210507681 | validation: 2.5134086887539495]
	TIME [epoch: 10.4 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293900412916172		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 1.293900412916172 | validation: 2.5146933492523917]
	TIME [epoch: 10.4 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.303374049645958		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 1.303374049645958 | validation: 2.5645036476790755]
	TIME [epoch: 10.4 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2892590882131774		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 1.2892590882131774 | validation: 2.4975084208688934]
	TIME [epoch: 10.4 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2954349248430703		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 1.2954349248430703 | validation: 2.5087516281464217]
	TIME [epoch: 10.4 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300774973504039		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 1.300774973504039 | validation: 2.517917980474579]
	TIME [epoch: 10.4 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.289989006659854		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 1.289989006659854 | validation: 2.506302208771538]
	TIME [epoch: 10.4 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2910335310004126		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 1.2910335310004126 | validation: 2.501012166159388]
	TIME [epoch: 10.4 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2989071770678362		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 1.2989071770678362 | validation: 2.5146431787249877]
	TIME [epoch: 10.4 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2998085811902793		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 1.2998085811902793 | validation: 2.5580471090698165]
	TIME [epoch: 10.4 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2940551747907558		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 1.2940551747907558 | validation: 2.512812279543362]
	TIME [epoch: 10.4 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943508345553125		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 1.2943508345553125 | validation: 2.5058901847332415]
	TIME [epoch: 10.4 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2996377459991162		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 1.2996377459991162 | validation: 2.5074624132473042]
	TIME [epoch: 10.4 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3041296960062048		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 1.3041296960062048 | validation: 2.5561371342143757]
	TIME [epoch: 10.4 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.293098893953552		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 1.293098893953552 | validation: 2.557134108893611]
	TIME [epoch: 10.4 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2994731387156757		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 1.2994731387156757 | validation: 2.5021925557126803]
	TIME [epoch: 10.4 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2916455094397619		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 1.2916455094397619 | validation: 2.5499861715989747]
	TIME [epoch: 10.4 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2992346998457784		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 1.2992346998457784 | validation: 2.5238809858659845]
	TIME [epoch: 10.4 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.29495781069062		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 1.29495781069062 | validation: 2.5080913034385195]
	TIME [epoch: 10.4 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2865902427674283		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 1.2865902427674283 | validation: 2.5017321316253205]
	TIME [epoch: 10.4 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968100895774155		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 1.2968100895774155 | validation: 2.4986151671549055]
	TIME [epoch: 10.4 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.305032130048181		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 1.305032130048181 | validation: 2.4958599359846323]
	TIME [epoch: 10.4 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2931321202097623		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 1.2931321202097623 | validation: 2.50753805631571]
	TIME [epoch: 10.4 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3050001752348876		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 1.3050001752348876 | validation: 2.4978891267370438]
	TIME [epoch: 10.4 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.302291417452382		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 1.302291417452382 | validation: 2.5326023056289837]
	TIME [epoch: 10.4 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2903149999027614		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 1.2903149999027614 | validation: 2.5026537225963628]
	TIME [epoch: 10.4 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2975291117548806		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 1.2975291117548806 | validation: 2.513358526757376]
	TIME [epoch: 10.4 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2921861782847182		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 1.2921861782847182 | validation: 2.504280762579346]
	TIME [epoch: 10.4 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958565233051849		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 1.2958565233051849 | validation: 2.5555786894947534]
	TIME [epoch: 10.4 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001632512934678		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 1.3001632512934678 | validation: 2.513236946194002]
	TIME [epoch: 10.4 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2914790354857375		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 1.2914790354857375 | validation: 2.5094712595146964]
	TIME [epoch: 10.4 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995554140641628		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 1.2995554140641628 | validation: 2.50103833011751]
	TIME [epoch: 10.4 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.301033890995566		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 1.301033890995566 | validation: 2.5549191229927]
	TIME [epoch: 10.4 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3039048349092817		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 1.3039048349092817 | validation: 2.5100412833565766]
	TIME [epoch: 10.4 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980859635530193		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 1.2980859635530193 | validation: 2.513246128602641]
	TIME [epoch: 10.4 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2917560580806045		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 1.2917560580806045 | validation: 2.50209537909759]
	TIME [epoch: 10.4 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2970314927444033		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 1.2970314927444033 | validation: 2.5085211638043816]
	TIME [epoch: 10.4 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299576080517649		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 1.299576080517649 | validation: 2.560153647169222]
	TIME [epoch: 10.4 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294414364302398		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 1.294414364302398 | validation: 2.5134290481634287]
	TIME [epoch: 10.4 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2976965752662526		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 1.2976965752662526 | validation: 2.5175916930143427]
	TIME [epoch: 10.4 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3013265210605576		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 1.3013265210605576 | validation: 2.499854141493124]
	TIME [epoch: 10.4 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2944237970370467		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 1.2944237970370467 | validation: 2.518693352218476]
	TIME [epoch: 10.4 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007923757043158		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 1.3007923757043158 | validation: 2.561814247529925]
	TIME [epoch: 10.4 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2930063210597795		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 1.2930063210597795 | validation: 2.5218537561481313]
	TIME [epoch: 10.4 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.296407147049089		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 1.296407147049089 | validation: 2.5013756432456096]
	TIME [epoch: 10.4 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2983226533189325		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 1.2983226533189325 | validation: 2.5154154340279877]
	TIME [epoch: 10.4 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967482065884295		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 1.2967482065884295 | validation: 2.505975610956457]
	TIME [epoch: 10.4 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2982857315976275		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 1.2982857315976275 | validation: 2.5256327593619123]
	TIME [epoch: 10.4 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2925563310526462		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 1.2925563310526462 | validation: 2.4850471140153467]
	TIME [epoch: 10.4 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3043970542872274		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 1.3043970542872274 | validation: 2.519974989549446]
	TIME [epoch: 10.4 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2965402778318456		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 1.2965402778318456 | validation: 2.507816779686426]
	TIME [epoch: 10.4 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2994735409640135		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 1.2994735409640135 | validation: 2.5494441502688896]
	TIME [epoch: 10.4 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295101803655029		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 1.295101803655029 | validation: 2.506355954245155]
	TIME [epoch: 10.4 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973353833126295		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 1.2973353833126295 | validation: 2.5050815069703196]
	TIME [epoch: 10.4 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2883999963784605		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 1.2883999963784605 | validation: 2.5093381515268955]
	TIME [epoch: 10.4 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.28467938436642		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 1.28467938436642 | validation: 2.5013700438560513]
	TIME [epoch: 10.4 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3008881278998863		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 1.3008881278998863 | validation: 2.5008011272339368]
	TIME [epoch: 10.4 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299773113983005		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 1.299773113983005 | validation: 2.4996686040977028]
	TIME [epoch: 10.4 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933988477819955		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 1.2933988477819955 | validation: 2.497771587655215]
	TIME [epoch: 10.4 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291374295177212		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 1.291374295177212 | validation: 2.514620769108266]
	TIME [epoch: 10.4 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.292062216483076		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 1.292062216483076 | validation: 2.508522974374087]
	TIME [epoch: 10.4 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933568538192741		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 1.2933568538192741 | validation: 2.5137323445193194]
	TIME [epoch: 10.4 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2925009114679644		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 1.2925009114679644 | validation: 2.5018109683830447]
	TIME [epoch: 10.4 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988696515252964		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 1.2988696515252964 | validation: 2.507494196605274]
	TIME [epoch: 10.4 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2985143584862864		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 1.2985143584862864 | validation: 2.492509355277598]
	TIME [epoch: 10.4 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2932641582281197		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 1.2932641582281197 | validation: 2.5011700737523683]
	TIME [epoch: 10.4 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.297759836897805		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 1.297759836897805 | validation: 2.51248983064625]
	TIME [epoch: 10.4 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3007556170857946		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 1.3007556170857946 | validation: 2.507937825288421]
	TIME [epoch: 10.4 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299127108175265		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 1.299127108175265 | validation: 2.5096690805014354]
	TIME [epoch: 10.4 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2974200417701585		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 1.2974200417701585 | validation: 2.4988531396908593]
	TIME [epoch: 10.4 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2951567616175237		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 1.2951567616175237 | validation: 2.51751446125806]
	TIME [epoch: 10.4 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958420927528305		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 1.2958420927528305 | validation: 2.5433409235669773]
	TIME [epoch: 10.4 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2985983817957512		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 1.2985983817957512 | validation: 2.558318988605443]
	TIME [epoch: 10.4 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.299690578399952		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 1.299690578399952 | validation: 2.505264760298399]
	TIME [epoch: 10.4 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995000900220206		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 1.2995000900220206 | validation: 2.5168996706475837]
	TIME [epoch: 10.4 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943743959613871		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 1.2943743959613871 | validation: 2.5001490230141905]
	TIME [epoch: 10.4 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2926041416146532		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 1.2926041416146532 | validation: 2.5033284054624443]
	TIME [epoch: 10.4 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001178515886995		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 1.3001178515886995 | validation: 2.5041849107645913]
	TIME [epoch: 10.4 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.28835484077851		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 1.28835484077851 | validation: 2.4997184563905774]
	TIME [epoch: 10.4 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2980026535254336		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 1.2980026535254336 | validation: 2.5098773158866914]
	TIME [epoch: 10.4 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294725241038447		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 1.294725241038447 | validation: 2.5016220703828735]
	TIME [epoch: 10.4 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2927584536679004		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 1.2927584536679004 | validation: 2.4937347117903634]
	TIME [epoch: 10.4 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968190932099186		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 1.2968190932099186 | validation: 2.515666001174388]
	TIME [epoch: 10.4 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2968784777675049		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 1.2968784777675049 | validation: 2.560666898803096]
	TIME [epoch: 10.4 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2948837171555723		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 1.2948837171555723 | validation: 2.4917374766897913]
	TIME [epoch: 10.4 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2971137830049462		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 1.2971137830049462 | validation: 2.4961272612377203]
	TIME [epoch: 10.4 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3017801192246288		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 1.3017801192246288 | validation: 2.501835002941565]
	TIME [epoch: 10.4 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2918929833356367		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 1.2918929833356367 | validation: 2.4970504552844273]
	TIME [epoch: 10.4 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3013487364467675		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 1.3013487364467675 | validation: 2.5040688339251065]
	TIME [epoch: 10.4 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2960770811751243		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 1.2960770811751243 | validation: 2.509253810795946]
	TIME [epoch: 10.4 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2938966727520365		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 1.2938966727520365 | validation: 2.510096205222631]
	TIME [epoch: 10.4 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295115930279064		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 1.295115930279064 | validation: 2.51389995164792]
	TIME [epoch: 10.4 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3010224582095198		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 1.3010224582095198 | validation: 2.5189293775995716]
	TIME [epoch: 10.4 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933760166921586		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 1.2933760166921586 | validation: 2.5047856361626715]
	TIME [epoch: 10.4 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993355544274063		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 1.2993355544274063 | validation: 2.513499823798191]
	TIME [epoch: 10.4 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3051638815324706		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 1.3051638815324706 | validation: 2.508762374619675]
	TIME [epoch: 10.4 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.300087125718763		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 1.300087125718763 | validation: 2.499483067049549]
	TIME [epoch: 10.4 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3056439263066282		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 1.3056439263066282 | validation: 2.550861225386551]
	TIME [epoch: 10.4 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2949227982316083		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 1.2949227982316083 | validation: 2.512306043209526]
	TIME [epoch: 10.4 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2955730387141648		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 1.2955730387141648 | validation: 2.5048273123484184]
	TIME [epoch: 10.4 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2971999145008555		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 1.2971999145008555 | validation: 2.563088516111666]
	TIME [epoch: 10.4 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2970162746770346		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 1.2970162746770346 | validation: 2.5158124866649816]
	TIME [epoch: 10.4 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2952629800894386		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 1.2952629800894386 | validation: 2.5139032606021234]
	TIME [epoch: 10.4 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.295447256108554		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 1.295447256108554 | validation: 2.5493485199337265]
	TIME [epoch: 10.4 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2988045800372028		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 1.2988045800372028 | validation: 2.562302113449423]
	TIME [epoch: 10.4 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2973815629445078		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 1.2973815629445078 | validation: 2.5032065662519853]
	TIME [epoch: 10.4 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2993740342048308		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 1.2993740342048308 | validation: 2.5057350689081583]
	TIME [epoch: 10.4 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3053316552378598		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 1.3053316552378598 | validation: 2.5049432653873174]
	TIME [epoch: 10.4 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2978077184980792		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 1.2978077184980792 | validation: 2.510568663366567]
	TIME [epoch: 10.4 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2928989575810568		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 1.2928989575810568 | validation: 2.49930160449568]
	TIME [epoch: 10.4 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.291925923794461		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 1.291925923794461 | validation: 2.5021945482320955]
	TIME [epoch: 10.4 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2912817350114407		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 1.2912817350114407 | validation: 2.5032876671017563]
	TIME [epoch: 10.4 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943964561063446		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 1.2943964561063446 | validation: 2.505876480991086]
	TIME [epoch: 10.4 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.292258235976875		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 1.292258235976875 | validation: 2.5166837428017237]
	TIME [epoch: 10.4 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2908265870930493		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 1.2908265870930493 | validation: 2.5058256215867285]
	TIME [epoch: 10.4 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2995755085471883		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 1.2995755085471883 | validation: 2.5032487170967155]
	TIME [epoch: 10.4 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294663305692388		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 1.294663305692388 | validation: 2.5038531942226547]
	TIME [epoch: 10.4 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2958961328027852		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 1.2958961328027852 | validation: 2.502546024488688]
	TIME [epoch: 10.4 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2949882731314233		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 1.2949882731314233 | validation: 2.5557871477188683]
	TIME [epoch: 10.4 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2967342106873017		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 1.2967342106873017 | validation: 2.5563184058852286]
	TIME [epoch: 10.4 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3004832707936451		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 1.3004832707936451 | validation: 2.499470023919846]
	TIME [epoch: 10.4 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2994911276845216		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 1.2994911276845216 | validation: 2.5178718237905775]
	TIME [epoch: 10.4 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3001484963908365		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 1.3001484963908365 | validation: 2.55912420084673]
	TIME [epoch: 10.4 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3024659601995547		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 1.3024659601995547 | validation: 2.5014750219020976]
	TIME [epoch: 10.4 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2911823512186114		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 1.2911823512186114 | validation: 2.50878333473335]
	TIME [epoch: 10.4 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2943302424701135		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 1.2943302424701135 | validation: 2.512492829776689]
	TIME [epoch: 10.4 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2949512507280005		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 1.2949512507280005 | validation: 2.5092914904811594]
	TIME [epoch: 10.4 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.294603407482073		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 1.294603407482073 | validation: 2.5016508467892664]
	TIME [epoch: 10.4 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3048794286636902		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 1.3048794286636902 | validation: 2.5035106419412267]
	TIME [epoch: 10.4 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2933688631837426		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 1.2933688631837426 | validation: 2.5122659477266542]
	TIME [epoch: 10.4 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2972358293949289		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 1.2972358293949289 | validation: 2.4976231056481373]
	TIME [epoch: 10.4 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2957999453820432		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 1.2957999453820432 | validation: 2.507637829861612]
	TIME [epoch: 10.4 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.2901190304192087		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 1.2901190304192087 | validation: 2.5110970609330137]
	TIME [epoch: 10.4 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 1.3050619925872602		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 1.3050619925872602 | validation: 2.5081620554377078]
	TIME [epoch: 10.4 sec]
Finished training in 20881.034 seconds.
