Args:
Namespace(name='model_tr_study206', outdir='out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3', training_data='data/transition_rate_studies/tr_study206/tr_study206_training/r3', validation_data='data/transition_rate_studies/tr_study206/tr_study206_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1440577491

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 5/5] avg loss: 10.438013079067463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.438013079067463 | validation: 10.646883565034555]
	TIME [epoch: 49.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 5/5] avg loss: 9.352880493952132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.352880493952132 | validation: 7.996583891735108]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 5/5] avg loss: 7.483207646411863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.483207646411863 | validation: 10.134518011016208]
	TIME [epoch: 10.4 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 5/5] avg loss: 8.576827682694645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.576827682694645 | validation: 7.154183327269615]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.079767082792761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.079767082792761 | validation: 8.100283944407888]
	TIME [epoch: 10.4 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 5/5] avg loss: 6.243982297406654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.243982297406654 | validation: 6.209383917705895]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.450641945808643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.450641945808643 | validation: 6.169419055750416]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.167664492922924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.167664492922924 | validation: 6.0998994872831895]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.253919689635824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.253919689635824 | validation: 6.180985397609816]
	TIME [epoch: 10.4 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.530413228862446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.530413228862446 | validation: 6.4333265836393405]
	TIME [epoch: 10.4 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.954211776621404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.954211776621404 | validation: 6.484780894915469]
	TIME [epoch: 10.4 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.156496460005322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.156496460005322 | validation: 6.212288921916892]
	TIME [epoch: 10.4 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.805331106529963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.805331106529963 | validation: 6.370029373006625]
	TIME [epoch: 10.4 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.88694790669214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.88694790669214 | validation: 6.580871183922284]
	TIME [epoch: 10.4 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.968982589664167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.968982589664167 | validation: 5.9535051527144445]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.065143221363551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.065143221363551 | validation: 7.867745359937528]
	TIME [epoch: 10.4 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 5/5] avg loss: 5.30633842058576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.30633842058576 | validation: 6.041457174019541]
	TIME [epoch: 10.4 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.759385713880566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.759385713880566 | validation: 6.380328175599398]
	TIME [epoch: 10.4 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.812010216815905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.812010216815905 | validation: 5.906711021421127]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.6040541174313185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6040541174313185 | validation: 6.990284138916632]
	TIME [epoch: 10.4 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.76759007951914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.76759007951914 | validation: 5.826712334348788]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.486226231887931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.486226231887931 | validation: 6.725002673886202]
	TIME [epoch: 10.4 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.733099934201046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.733099934201046 | validation: 5.880123137353046]
	TIME [epoch: 10.4 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.524343581732676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.524343581732676 | validation: 6.1616945721491225]
	TIME [epoch: 10.4 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.647212154311985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.647212154311985 | validation: 5.915978109698367]
	TIME [epoch: 10.4 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.642017314413764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.642017314413764 | validation: 5.825820963073291]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.392916759238032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.392916759238032 | validation: 6.073374952331886]
	TIME [epoch: 10.4 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.489886426995346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.489886426995346 | validation: 5.961768473265081]
	TIME [epoch: 10.4 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.498984246615314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.498984246615314 | validation: 5.705865658818971]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.43098480592408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.43098480592408 | validation: 5.776900369074371]
	TIME [epoch: 10.4 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.3576157812447445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3576157812447445 | validation: 5.759749681929073]
	TIME [epoch: 10.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.397331341557693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.397331341557693 | validation: 5.740609315307725]
	TIME [epoch: 10.4 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374983067844582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.374983067844582 | validation: 6.046907171636582]
	TIME [epoch: 10.4 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.312885169282505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.312885169282505 | validation: 6.167363905665013]
	TIME [epoch: 10.4 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.323300477191158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.323300477191158 | validation: 6.562076555560992]
	TIME [epoch: 10.4 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.398016011669878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.398016011669878 | validation: 6.014628199040326]
	TIME [epoch: 10.4 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.393138797161585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.393138797161585 | validation: 5.868716265058647]
	TIME [epoch: 10.4 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.437920163779979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.437920163779979 | validation: 5.733446364271969]
	TIME [epoch: 10.4 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.168995510552649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.168995510552649 | validation: 5.717734123266835]
	TIME [epoch: 10.4 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.405230779226543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.405230779226543 | validation: 5.6094814204250225]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.292291811589349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.292291811589349 | validation: 5.711929277864647]
	TIME [epoch: 10.4 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.365507018949531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.365507018949531 | validation: 5.729826989644232]
	TIME [epoch: 10.4 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0450261314686475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0450261314686475 | validation: 5.875319643282774]
	TIME [epoch: 10.4 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.374951268428273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.374951268428273 | validation: 5.699470510394437]
	TIME [epoch: 10.4 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.124818271486049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124818271486049 | validation: 5.823375556421824]
	TIME [epoch: 10.4 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1644482224426955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1644482224426955 | validation: 5.8657225086689095]
	TIME [epoch: 10.4 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.224405246920244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.224405246920244 | validation: 5.7368117225127095]
	TIME [epoch: 10.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.076016334191707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.076016334191707 | validation: 5.891556530297826]
	TIME [epoch: 10.4 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.23539422695343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.23539422695343 | validation: 5.649929733875986]
	TIME [epoch: 10.4 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.099957091628743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.099957091628743 | validation: 5.486337654271463]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.281622074230507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.281622074230507 | validation: 6.208416080020463]
	TIME [epoch: 10.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.10915402640411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.10915402640411 | validation: 5.496355893885855]
	TIME [epoch: 10.4 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.194377521156595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.194377521156595 | validation: 5.734710222383675]
	TIME [epoch: 10.4 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0904994821895615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0904994821895615 | validation: 5.9053086442942435]
	TIME [epoch: 10.4 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.244204366836092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.244204366836092 | validation: 5.5019646833909475]
	TIME [epoch: 10.4 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.057933576873318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.057933576873318 | validation: 5.607008709622447]
	TIME [epoch: 10.4 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.027858139718721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.027858139718721 | validation: 5.670848023378858]
	TIME [epoch: 10.4 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.163664190200648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163664190200648 | validation: 5.720574402987567]
	TIME [epoch: 10.4 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.016411158293599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.016411158293599 | validation: 5.859854938265792]
	TIME [epoch: 10.4 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.106115280731178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.106115280731178 | validation: 5.545729765591729]
	TIME [epoch: 10.4 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.008769668971939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008769668971939 | validation: 5.862652363899612]
	TIME [epoch: 10.4 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.179239555580381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.179239555580381 | validation: 6.0924452936847215]
	TIME [epoch: 10.4 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.073034142629202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.073034142629202 | validation: 5.476080218163552]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.059462038141001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.059462038141001 | validation: 5.455048755711075]
	TIME [epoch: 10.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.071717984875361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.071717984875361 | validation: 5.506209340717578]
	TIME [epoch: 10.4 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9949743712456636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9949743712456636 | validation: 5.781422666735019]
	TIME [epoch: 10.4 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0714466212721705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0714466212721705 | validation: 5.715997475685859]
	TIME [epoch: 10.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.968601750278945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.968601750278945 | validation: 5.638205332125461]
	TIME [epoch: 10.4 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.065684814954943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.065684814954943 | validation: 5.618680081717778]
	TIME [epoch: 10.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8955109195036726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8955109195036726 | validation: 5.495074291433069]
	TIME [epoch: 10.4 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.1281184199685725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1281184199685725 | validation: 5.510162598196769]
	TIME [epoch: 10.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9328562274816505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9328562274816505 | validation: 6.000664261003699]
	TIME [epoch: 10.4 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.200252460461543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.200252460461543 | validation: 5.615649807907853]
	TIME [epoch: 10.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.005055319965146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.005055319965146 | validation: 5.4812598593166015]
	TIME [epoch: 10.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9660539530795518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9660539530795518 | validation: 5.531883646363999]
	TIME [epoch: 10.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.074970163823595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.074970163823595 | validation: 5.627803790625462]
	TIME [epoch: 10.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.176895743241252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.176895743241252 | validation: 5.7614853772573]
	TIME [epoch: 10.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.348235825617761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.348235825617761 | validation: 5.5312307718619875]
	TIME [epoch: 10.4 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9893034573863346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9893034573863346 | validation: 5.724600669637082]
	TIME [epoch: 10.4 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.025388560658785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025388560658785 | validation: 5.5295285482564225]
	TIME [epoch: 10.4 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.094119623142139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.094119623142139 | validation: 5.5449029099482585]
	TIME [epoch: 10.4 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.974353826241186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.974353826241186 | validation: 5.651169980810564]
	TIME [epoch: 10.4 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.99495632824832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.99495632824832 | validation: 5.488211710452021]
	TIME [epoch: 10.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9432681774907223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9432681774907223 | validation: 5.852271239918045]
	TIME [epoch: 10.4 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9933470791065324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9933470791065324 | validation: 5.529373690706708]
	TIME [epoch: 10.4 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.025221637867571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025221637867571 | validation: 5.75733101594794]
	TIME [epoch: 10.4 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.060598815210727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.060598815210727 | validation: 5.521371140588747]
	TIME [epoch: 10.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9003839062635732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9003839062635732 | validation: 5.677518779486735]
	TIME [epoch: 10.4 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.131511996578408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.131511996578408 | validation: 5.592268480958307]
	TIME [epoch: 10.4 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.950753550295012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.950753550295012 | validation: 5.448836225403103]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.849062367280881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.849062367280881 | validation: 5.662807595938146]
	TIME [epoch: 10.4 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.060963055041313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.060963055041313 | validation: 5.460634222983881]
	TIME [epoch: 10.4 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.857237636831237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.857237636831237 | validation: 5.683326425388591]
	TIME [epoch: 10.4 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.0103060787917375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0103060787917375 | validation: 5.589367685025133]
	TIME [epoch: 10.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.011712202531713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.011712202531713 | validation: 7.525019289801109]
	TIME [epoch: 10.4 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.8172318782685455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.8172318782685455 | validation: 5.58208504123092]
	TIME [epoch: 10.4 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8779945773251456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8779945773251456 | validation: 5.492234284604113]
	TIME [epoch: 10.4 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8990016990738807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8990016990738807 | validation: 5.641714598823496]
	TIME [epoch: 10.4 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.999325629294853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.999325629294853 | validation: 5.554931663926374]
	TIME [epoch: 10.4 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.037678918597047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.037678918597047 | validation: 5.44907060244823]
	TIME [epoch: 10.4 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.926283811920937		[learning rate: 0.009971]
	Learning Rate: 0.00997096
	LOSS [training: 3.926283811920937 | validation: 5.576187656922902]
	TIME [epoch: 10.4 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9314111247474712		[learning rate: 0.0099348]
	Learning Rate: 0.00993477
	LOSS [training: 3.9314111247474712 | validation: 5.489204712328297]
	TIME [epoch: 10.4 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.921861813647839		[learning rate: 0.0098987]
	Learning Rate: 0.00989872
	LOSS [training: 3.921861813647839 | validation: 5.466972098983538]
	TIME [epoch: 10.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9498482969174207		[learning rate: 0.0098628]
	Learning Rate: 0.00986279
	LOSS [training: 3.9498482969174207 | validation: 5.552631562515925]
	TIME [epoch: 10.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.881093255328029		[learning rate: 0.009827]
	Learning Rate: 0.009827
	LOSS [training: 3.881093255328029 | validation: 5.584368197512356]
	TIME [epoch: 10.4 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.103376994364231		[learning rate: 0.0097913]
	Learning Rate: 0.00979134
	LOSS [training: 4.103376994364231 | validation: 5.466191353231633]
	TIME [epoch: 10.4 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.879699969450198		[learning rate: 0.0097558]
	Learning Rate: 0.00975581
	LOSS [training: 3.879699969450198 | validation: 5.4601453144476775]
	TIME [epoch: 10.4 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9856085019671106		[learning rate: 0.0097204]
	Learning Rate: 0.0097204
	LOSS [training: 3.9856085019671106 | validation: 5.568657580382517]
	TIME [epoch: 10.4 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8435629090622734		[learning rate: 0.0096851]
	Learning Rate: 0.00968513
	LOSS [training: 3.8435629090622734 | validation: 5.75176128227877]
	TIME [epoch: 10.4 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9107837470289035		[learning rate: 0.00965]
	Learning Rate: 0.00964998
	LOSS [training: 3.9107837470289035 | validation: 5.44259127783118]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.921315867004349		[learning rate: 0.009615]
	Learning Rate: 0.00961496
	LOSS [training: 3.921315867004349 | validation: 5.429442671236483]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.846693487570967		[learning rate: 0.0095801]
	Learning Rate: 0.00958006
	LOSS [training: 3.846693487570967 | validation: 5.741961066727847]
	TIME [epoch: 10.4 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9560066611927005		[learning rate: 0.0095453]
	Learning Rate: 0.0095453
	LOSS [training: 3.9560066611927005 | validation: 5.470877814516185]
	TIME [epoch: 10.4 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.819351209875509		[learning rate: 0.0095107]
	Learning Rate: 0.00951066
	LOSS [training: 3.819351209875509 | validation: 5.49887792853744]
	TIME [epoch: 10.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9600628045712094		[learning rate: 0.0094761]
	Learning Rate: 0.00947614
	LOSS [training: 3.9600628045712094 | validation: 5.7690101692523506]
	TIME [epoch: 10.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.934411353759728		[learning rate: 0.0094418]
	Learning Rate: 0.00944175
	LOSS [training: 3.934411353759728 | validation: 5.632449497975045]
	TIME [epoch: 10.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.894118073224823		[learning rate: 0.0094075]
	Learning Rate: 0.00940749
	LOSS [training: 3.894118073224823 | validation: 5.427529422155228]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8529946835367475		[learning rate: 0.0093733]
	Learning Rate: 0.00937335
	LOSS [training: 3.8529946835367475 | validation: 5.698165230357675]
	TIME [epoch: 10.4 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8877769014853407		[learning rate: 0.0093393]
	Learning Rate: 0.00933933
	LOSS [training: 3.8877769014853407 | validation: 5.674765026716833]
	TIME [epoch: 10.4 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8186336269337873		[learning rate: 0.0093054]
	Learning Rate: 0.00930544
	LOSS [training: 3.8186336269337873 | validation: 5.5488539490382145]
	TIME [epoch: 10.4 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.002846487943826		[learning rate: 0.0092717]
	Learning Rate: 0.00927167
	LOSS [training: 4.002846487943826 | validation: 5.433231808387486]
	TIME [epoch: 10.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.81001370344283		[learning rate: 0.009238]
	Learning Rate: 0.00923802
	LOSS [training: 3.81001370344283 | validation: 5.600693902726821]
	TIME [epoch: 10.4 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8086475177251744		[learning rate: 0.0092045]
	Learning Rate: 0.0092045
	LOSS [training: 3.8086475177251744 | validation: 5.670136645557964]
	TIME [epoch: 10.4 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.218015817388619		[learning rate: 0.0091711]
	Learning Rate: 0.00917109
	LOSS [training: 4.218015817388619 | validation: 5.406795008420918]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8920017217508365		[learning rate: 0.0091378]
	Learning Rate: 0.00913781
	LOSS [training: 3.8920017217508365 | validation: 5.482980786698708]
	TIME [epoch: 10.4 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8484163639628113		[learning rate: 0.0091046]
	Learning Rate: 0.00910465
	LOSS [training: 3.8484163639628113 | validation: 5.405581932630912]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8246842462473323		[learning rate: 0.0090716]
	Learning Rate: 0.00907161
	LOSS [training: 3.8246842462473323 | validation: 5.599398894802951]
	TIME [epoch: 10.4 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.944574224468873		[learning rate: 0.0090387]
	Learning Rate: 0.00903868
	LOSS [training: 3.944574224468873 | validation: 5.391849734966452]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8740840325479966		[learning rate: 0.0090059]
	Learning Rate: 0.00900588
	LOSS [training: 3.8740840325479966 | validation: 5.580757640135592]
	TIME [epoch: 10.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.89825043697851		[learning rate: 0.0089732]
	Learning Rate: 0.0089732
	LOSS [training: 3.89825043697851 | validation: 5.585494997397307]
	TIME [epoch: 10.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.088777179404465		[learning rate: 0.0089406]
	Learning Rate: 0.00894064
	LOSS [training: 4.088777179404465 | validation: 6.124805931426699]
	TIME [epoch: 10.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.152241183549307		[learning rate: 0.0089082]
	Learning Rate: 0.00890819
	LOSS [training: 4.152241183549307 | validation: 5.409055781591667]
	TIME [epoch: 10.4 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8133450114648633		[learning rate: 0.0088759]
	Learning Rate: 0.00887586
	LOSS [training: 3.8133450114648633 | validation: 5.468262461467923]
	TIME [epoch: 10.4 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.814928836939564		[learning rate: 0.0088437]
	Learning Rate: 0.00884365
	LOSS [training: 3.814928836939564 | validation: 5.596687408132076]
	TIME [epoch: 10.4 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.20290689694564		[learning rate: 0.0088116]
	Learning Rate: 0.00881156
	LOSS [training: 4.20290689694564 | validation: 6.0406241316045435]
	TIME [epoch: 10.4 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 5/5] avg loss: 4.135146262118694		[learning rate: 0.0087796]
	Learning Rate: 0.00877958
	LOSS [training: 4.135146262118694 | validation: 5.55913472071815]
	TIME [epoch: 10.4 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8134773358918688		[learning rate: 0.0087477]
	Learning Rate: 0.00874772
	LOSS [training: 3.8134773358918688 | validation: 5.433295949314258]
	TIME [epoch: 10.4 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7505800444121826		[learning rate: 0.008716]
	Learning Rate: 0.00871597
	LOSS [training: 3.7505800444121826 | validation: 5.958527943540527]
	TIME [epoch: 10.4 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.882927833265069		[learning rate: 0.0086843]
	Learning Rate: 0.00868434
	LOSS [training: 3.882927833265069 | validation: 5.600853733178658]
	TIME [epoch: 10.4 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7457863374798364		[learning rate: 0.0086528]
	Learning Rate: 0.00865282
	LOSS [training: 3.7457863374798364 | validation: 5.477596690086882]
	TIME [epoch: 10.4 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7855873420820294		[learning rate: 0.0086214]
	Learning Rate: 0.00862142
	LOSS [training: 3.7855873420820294 | validation: 5.481359059803004]
	TIME [epoch: 10.4 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7862428224884654		[learning rate: 0.0085901]
	Learning Rate: 0.00859013
	LOSS [training: 3.7862428224884654 | validation: 5.556722740558924]
	TIME [epoch: 10.4 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8268429185166504		[learning rate: 0.008559]
	Learning Rate: 0.00855896
	LOSS [training: 3.8268429185166504 | validation: 5.411471951463295]
	TIME [epoch: 10.4 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.738247298536141		[learning rate: 0.0085279]
	Learning Rate: 0.0085279
	LOSS [training: 3.738247298536141 | validation: 5.4160434252002165]
	TIME [epoch: 10.4 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.773846923176503		[learning rate: 0.008497]
	Learning Rate: 0.00849695
	LOSS [training: 3.773846923176503 | validation: 5.580946428195215]
	TIME [epoch: 10.4 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.847037668118221		[learning rate: 0.0084661]
	Learning Rate: 0.00846612
	LOSS [training: 3.847037668118221 | validation: 5.442019122494594]
	TIME [epoch: 10.4 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.74700110362784		[learning rate: 0.0084354]
	Learning Rate: 0.00843539
	LOSS [training: 3.74700110362784 | validation: 5.439400908002162]
	TIME [epoch: 10.4 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8100434425921508		[learning rate: 0.0084048]
	Learning Rate: 0.00840478
	LOSS [training: 3.8100434425921508 | validation: 5.509666359304722]
	TIME [epoch: 10.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.753201501313337		[learning rate: 0.0083743]
	Learning Rate: 0.00837428
	LOSS [training: 3.753201501313337 | validation: 5.3897379028637475]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7753806630741367		[learning rate: 0.0083439]
	Learning Rate: 0.00834389
	LOSS [training: 3.7753806630741367 | validation: 5.494263898636699]
	TIME [epoch: 10.4 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.838716846605963		[learning rate: 0.0083136]
	Learning Rate: 0.00831361
	LOSS [training: 3.838716846605963 | validation: 5.4926520480361685]
	TIME [epoch: 10.4 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7945006738615383		[learning rate: 0.0082834]
	Learning Rate: 0.00828344
	LOSS [training: 3.7945006738615383 | validation: 5.511190209426152]
	TIME [epoch: 10.4 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7395204874060477		[learning rate: 0.0082534]
	Learning Rate: 0.00825338
	LOSS [training: 3.7395204874060477 | validation: 5.398528516817713]
	TIME [epoch: 10.4 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.820608887023563		[learning rate: 0.0082234]
	Learning Rate: 0.00822342
	LOSS [training: 3.820608887023563 | validation: 5.627791827607925]
	TIME [epoch: 10.4 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8533446071548823		[learning rate: 0.0081936]
	Learning Rate: 0.00819358
	LOSS [training: 3.8533446071548823 | validation: 5.421769787272354]
	TIME [epoch: 10.4 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7012887220754727		[learning rate: 0.0081638]
	Learning Rate: 0.00816384
	LOSS [training: 3.7012887220754727 | validation: 5.5901947320451155]
	TIME [epoch: 10.3 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8771541731066095		[learning rate: 0.0081342]
	Learning Rate: 0.00813422
	LOSS [training: 3.8771541731066095 | validation: 5.374655760994816]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7949047364932276		[learning rate: 0.0081047]
	Learning Rate: 0.0081047
	LOSS [training: 3.7949047364932276 | validation: 5.4400527029646435]
	TIME [epoch: 10.4 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7168401285794075		[learning rate: 0.0080753]
	Learning Rate: 0.00807529
	LOSS [training: 3.7168401285794075 | validation: 5.535417273998083]
	TIME [epoch: 10.4 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7730413607237656		[learning rate: 0.008046]
	Learning Rate: 0.00804598
	LOSS [training: 3.7730413607237656 | validation: 5.43162015856091]
	TIME [epoch: 10.4 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7831959846893204		[learning rate: 0.0080168]
	Learning Rate: 0.00801678
	LOSS [training: 3.7831959846893204 | validation: 5.428890827000823]
	TIME [epoch: 10.4 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.766144712902866		[learning rate: 0.0079877]
	Learning Rate: 0.00798769
	LOSS [training: 3.766144712902866 | validation: 5.435384826988809]
	TIME [epoch: 10.4 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7335588963816386		[learning rate: 0.0079587]
	Learning Rate: 0.0079587
	LOSS [training: 3.7335588963816386 | validation: 5.473801486959628]
	TIME [epoch: 10.3 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.762648295874462		[learning rate: 0.0079298]
	Learning Rate: 0.00792982
	LOSS [training: 3.762648295874462 | validation: 5.495125503121963]
	TIME [epoch: 10.4 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.830943205578265		[learning rate: 0.007901]
	Learning Rate: 0.00790104
	LOSS [training: 3.830943205578265 | validation: 5.445878529330427]
	TIME [epoch: 10.4 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8629966460025615		[learning rate: 0.0078724]
	Learning Rate: 0.00787237
	LOSS [training: 3.8629966460025615 | validation: 5.706817600986536]
	TIME [epoch: 10.4 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.801044709646368		[learning rate: 0.0078438]
	Learning Rate: 0.0078438
	LOSS [training: 3.801044709646368 | validation: 5.7292201868375905]
	TIME [epoch: 10.4 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7880864556321776		[learning rate: 0.0078153]
	Learning Rate: 0.00781533
	LOSS [training: 3.7880864556321776 | validation: 5.395414704859858]
	TIME [epoch: 10.4 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.750974707320224		[learning rate: 0.007787]
	Learning Rate: 0.00778697
	LOSS [training: 3.750974707320224 | validation: 5.639214682372413]
	TIME [epoch: 10.4 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.82375422945547		[learning rate: 0.0077587]
	Learning Rate: 0.00775871
	LOSS [training: 3.82375422945547 | validation: 5.374581004325355]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7576699690024937		[learning rate: 0.0077306]
	Learning Rate: 0.00773055
	LOSS [training: 3.7576699690024937 | validation: 5.632670388602846]
	TIME [epoch: 10.4 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8475091845777634		[learning rate: 0.0077025]
	Learning Rate: 0.0077025
	LOSS [training: 3.8475091845777634 | validation: 5.504042408807329]
	TIME [epoch: 10.4 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.770427835785951		[learning rate: 0.0076745]
	Learning Rate: 0.00767455
	LOSS [training: 3.770427835785951 | validation: 5.415164995657387]
	TIME [epoch: 10.4 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7306582211854518		[learning rate: 0.0076467]
	Learning Rate: 0.00764669
	LOSS [training: 3.7306582211854518 | validation: 5.59616021862426]
	TIME [epoch: 10.4 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8821161858765643		[learning rate: 0.0076189]
	Learning Rate: 0.00761894
	LOSS [training: 3.8821161858765643 | validation: 5.400279421151644]
	TIME [epoch: 10.4 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7431068844396322		[learning rate: 0.0075913]
	Learning Rate: 0.00759129
	LOSS [training: 3.7431068844396322 | validation: 5.556824763217279]
	TIME [epoch: 10.4 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8002680052744147		[learning rate: 0.0075637]
	Learning Rate: 0.00756374
	LOSS [training: 3.8002680052744147 | validation: 5.377192057778875]
	TIME [epoch: 10.4 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.734208977352709		[learning rate: 0.0075363]
	Learning Rate: 0.00753629
	LOSS [training: 3.734208977352709 | validation: 5.519834680257406]
	TIME [epoch: 10.4 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.77689388864341		[learning rate: 0.0075089]
	Learning Rate: 0.00750895
	LOSS [training: 3.77689388864341 | validation: 5.620641568354463]
	TIME [epoch: 10.4 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7329760247512573		[learning rate: 0.0074817]
	Learning Rate: 0.00748169
	LOSS [training: 3.7329760247512573 | validation: 5.416596301263089]
	TIME [epoch: 10.4 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.714566802327745		[learning rate: 0.0074545]
	Learning Rate: 0.00745454
	LOSS [training: 3.714566802327745 | validation: 5.4883464582473485]
	TIME [epoch: 10.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7377232435492225		[learning rate: 0.0074275]
	Learning Rate: 0.00742749
	LOSS [training: 3.7377232435492225 | validation: 5.418407303028339]
	TIME [epoch: 10.4 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772520073068686		[learning rate: 0.0074005]
	Learning Rate: 0.00740054
	LOSS [training: 3.772520073068686 | validation: 5.424041900607879]
	TIME [epoch: 10.4 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7221372581389125		[learning rate: 0.0073737]
	Learning Rate: 0.00737368
	LOSS [training: 3.7221372581389125 | validation: 5.4919032154569]
	TIME [epoch: 10.4 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7376608488939267		[learning rate: 0.0073469]
	Learning Rate: 0.00734692
	LOSS [training: 3.7376608488939267 | validation: 5.417966515688329]
	TIME [epoch: 10.4 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7318506005060312		[learning rate: 0.0073203]
	Learning Rate: 0.00732026
	LOSS [training: 3.7318506005060312 | validation: 5.435863064158905]
	TIME [epoch: 10.4 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.90673841272307		[learning rate: 0.0072937]
	Learning Rate: 0.00729369
	LOSS [training: 3.90673841272307 | validation: 5.760966766045255]
	TIME [epoch: 10.4 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.857090124233568		[learning rate: 0.0072672]
	Learning Rate: 0.00726722
	LOSS [training: 3.857090124233568 | validation: 5.3638806996707995]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.728267578179344		[learning rate: 0.0072408]
	Learning Rate: 0.00724085
	LOSS [training: 3.728267578179344 | validation: 5.389390617826023]
	TIME [epoch: 10.4 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.745447876245058		[learning rate: 0.0072146]
	Learning Rate: 0.00721457
	LOSS [training: 3.745447876245058 | validation: 5.384514623138987]
	TIME [epoch: 10.4 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.767072277956848		[learning rate: 0.0071884]
	Learning Rate: 0.00718839
	LOSS [training: 3.767072277956848 | validation: 5.382088893726709]
	TIME [epoch: 10.4 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.784883101437309		[learning rate: 0.0071623]
	Learning Rate: 0.0071623
	LOSS [training: 3.784883101437309 | validation: 5.390998034795002]
	TIME [epoch: 10.4 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.750335385234686		[learning rate: 0.0071363]
	Learning Rate: 0.00713631
	LOSS [training: 3.750335385234686 | validation: 5.372625042301116]
	TIME [epoch: 10.4 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7617472703717865		[learning rate: 0.0071104]
	Learning Rate: 0.00711041
	LOSS [training: 3.7617472703717865 | validation: 5.352965518001105]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.674511541754277		[learning rate: 0.0070846]
	Learning Rate: 0.00708461
	LOSS [training: 3.674511541754277 | validation: 5.419841473304259]
	TIME [epoch: 10.4 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.730042107613921		[learning rate: 0.0070589]
	Learning Rate: 0.0070589
	LOSS [training: 3.730042107613921 | validation: 5.356480557388445]
	TIME [epoch: 10.4 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.723262858183896		[learning rate: 0.0070333]
	Learning Rate: 0.00703328
	LOSS [training: 3.723262858183896 | validation: 5.355567163670278]
	TIME [epoch: 10.4 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.701642976792484		[learning rate: 0.0070078]
	Learning Rate: 0.00700776
	LOSS [training: 3.701642976792484 | validation: 5.508234981714636]
	TIME [epoch: 10.4 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7420154637116645		[learning rate: 0.0069823]
	Learning Rate: 0.00698232
	LOSS [training: 3.7420154637116645 | validation: 5.424304797900013]
	TIME [epoch: 10.4 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.726712389518834		[learning rate: 0.006957]
	Learning Rate: 0.00695698
	LOSS [training: 3.726712389518834 | validation: 5.3801727372321055]
	TIME [epoch: 10.4 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6851525631139395		[learning rate: 0.0069317]
	Learning Rate: 0.00693174
	LOSS [training: 3.6851525631139395 | validation: 5.417224175291371]
	TIME [epoch: 10.4 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7066829643717476		[learning rate: 0.0069066]
	Learning Rate: 0.00690658
	LOSS [training: 3.7066829643717476 | validation: 5.368488858493683]
	TIME [epoch: 10.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.71900286240639		[learning rate: 0.0068815]
	Learning Rate: 0.00688152
	LOSS [training: 3.71900286240639 | validation: 5.525460214341256]
	TIME [epoch: 10.4 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7157501513067137		[learning rate: 0.0068565]
	Learning Rate: 0.00685654
	LOSS [training: 3.7157501513067137 | validation: 5.520759655193577]
	TIME [epoch: 10.4 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.761586578159606		[learning rate: 0.0068317]
	Learning Rate: 0.00683166
	LOSS [training: 3.761586578159606 | validation: 5.348140236110766]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.710657646028467		[learning rate: 0.0068069]
	Learning Rate: 0.00680687
	LOSS [training: 3.710657646028467 | validation: 5.416845138430141]
	TIME [epoch: 10.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6708677040640305		[learning rate: 0.0067822]
	Learning Rate: 0.00678217
	LOSS [training: 3.6708677040640305 | validation: 5.4189411105416445]
	TIME [epoch: 10.4 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7833191125130603		[learning rate: 0.0067576]
	Learning Rate: 0.00675755
	LOSS [training: 3.7833191125130603 | validation: 5.351955753799539]
	TIME [epoch: 10.4 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662960245893621		[learning rate: 0.006733]
	Learning Rate: 0.00673303
	LOSS [training: 3.662960245893621 | validation: 5.35952379676122]
	TIME [epoch: 10.4 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.720309681059524		[learning rate: 0.0067086]
	Learning Rate: 0.00670859
	LOSS [training: 3.720309681059524 | validation: 5.411541229433849]
	TIME [epoch: 10.4 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7346109756394013		[learning rate: 0.0066842]
	Learning Rate: 0.00668425
	LOSS [training: 3.7346109756394013 | validation: 5.509382678395095]
	TIME [epoch: 10.4 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7876510952671354		[learning rate: 0.00666]
	Learning Rate: 0.00665999
	LOSS [training: 3.7876510952671354 | validation: 5.354838397518274]
	TIME [epoch: 10.4 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6587913088626975		[learning rate: 0.0066358]
	Learning Rate: 0.00663582
	LOSS [training: 3.6587913088626975 | validation: 5.489355866528838]
	TIME [epoch: 10.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.724127648277176		[learning rate: 0.0066117]
	Learning Rate: 0.00661174
	LOSS [training: 3.724127648277176 | validation: 5.357648164568066]
	TIME [epoch: 10.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7422951679169216		[learning rate: 0.0065877]
	Learning Rate: 0.00658775
	LOSS [training: 3.7422951679169216 | validation: 5.493808878280568]
	TIME [epoch: 10.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.726286666085337		[learning rate: 0.0065638]
	Learning Rate: 0.00656384
	LOSS [training: 3.726286666085337 | validation: 5.405872513646573]
	TIME [epoch: 10.4 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6993628471747186		[learning rate: 0.00654]
	Learning Rate: 0.00654002
	LOSS [training: 3.6993628471747186 | validation: 5.4784521763411105]
	TIME [epoch: 10.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.8060477951102576		[learning rate: 0.0065163]
	Learning Rate: 0.00651628
	LOSS [training: 3.8060477951102576 | validation: 5.366239781280799]
	TIME [epoch: 10.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.673803469497382		[learning rate: 0.0064926]
	Learning Rate: 0.00649264
	LOSS [training: 3.673803469497382 | validation: 5.449512744119352]
	TIME [epoch: 10.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.658957823717843		[learning rate: 0.0064691]
	Learning Rate: 0.00646907
	LOSS [training: 3.658957823717843 | validation: 5.4582353650171775]
	TIME [epoch: 10.4 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.760876313002707		[learning rate: 0.0064456]
	Learning Rate: 0.0064456
	LOSS [training: 3.760876313002707 | validation: 5.4497925911857195]
	TIME [epoch: 10.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7099586537537506		[learning rate: 0.0064222]
	Learning Rate: 0.00642221
	LOSS [training: 3.7099586537537506 | validation: 5.572081116574773]
	TIME [epoch: 10.4 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7141509769370566		[learning rate: 0.0063989]
	Learning Rate: 0.0063989
	LOSS [training: 3.7141509769370566 | validation: 5.484194913436181]
	TIME [epoch: 10.4 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7401520440954883		[learning rate: 0.0063757]
	Learning Rate: 0.00637568
	LOSS [training: 3.7401520440954883 | validation: 5.422658128282023]
	TIME [epoch: 10.4 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.690026729039315		[learning rate: 0.0063525]
	Learning Rate: 0.00635254
	LOSS [training: 3.690026729039315 | validation: 5.486896849581221]
	TIME [epoch: 10.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.728582357687673		[learning rate: 0.0063295]
	Learning Rate: 0.00632949
	LOSS [training: 3.728582357687673 | validation: 5.4158077029020095]
	TIME [epoch: 10.4 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.689678695777853		[learning rate: 0.0063065]
	Learning Rate: 0.00630652
	LOSS [training: 3.689678695777853 | validation: 5.398226076502823]
	TIME [epoch: 10.4 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6986760438443023		[learning rate: 0.0062836]
	Learning Rate: 0.00628363
	LOSS [training: 3.6986760438443023 | validation: 5.431581985169689]
	TIME [epoch: 10.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.65675920200518		[learning rate: 0.0062608]
	Learning Rate: 0.00626082
	LOSS [training: 3.65675920200518 | validation: 5.339622941094453]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.709446040464176		[learning rate: 0.0062381]
	Learning Rate: 0.0062381
	LOSS [training: 3.709446040464176 | validation: 5.371766698021941]
	TIME [epoch: 10.4 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.707668601689613		[learning rate: 0.0062155]
	Learning Rate: 0.00621547
	LOSS [training: 3.707668601689613 | validation: 5.338044869079729]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6667218011618283		[learning rate: 0.0061929]
	Learning Rate: 0.00619291
	LOSS [training: 3.6667218011618283 | validation: 5.350102952920294]
	TIME [epoch: 10.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6873442941891574		[learning rate: 0.0061704]
	Learning Rate: 0.00617043
	LOSS [training: 3.6873442941891574 | validation: 5.522943023942501]
	TIME [epoch: 10.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.701194269163448		[learning rate: 0.006148]
	Learning Rate: 0.00614804
	LOSS [training: 3.701194269163448 | validation: 5.362805304681972]
	TIME [epoch: 10.4 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.774369687950461		[learning rate: 0.0061257]
	Learning Rate: 0.00612573
	LOSS [training: 3.774369687950461 | validation: 5.402685659252529]
	TIME [epoch: 10.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.687899883797079		[learning rate: 0.0061035]
	Learning Rate: 0.0061035
	LOSS [training: 3.687899883797079 | validation: 5.3388441012604675]
	TIME [epoch: 10.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6347251242387357		[learning rate: 0.0060814]
	Learning Rate: 0.00608135
	LOSS [training: 3.6347251242387357 | validation: 5.341363490137954]
	TIME [epoch: 10.4 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.715156319498224		[learning rate: 0.0060593]
	Learning Rate: 0.00605928
	LOSS [training: 3.715156319498224 | validation: 5.5578255691262415]
	TIME [epoch: 10.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7139735346583356		[learning rate: 0.0060373]
	Learning Rate: 0.00603729
	LOSS [training: 3.7139735346583356 | validation: 5.35170701930098]
	TIME [epoch: 10.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.678870888958638		[learning rate: 0.0060154]
	Learning Rate: 0.00601538
	LOSS [training: 3.678870888958638 | validation: 5.334533625479369]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6446876103670447		[learning rate: 0.0059936]
	Learning Rate: 0.00599355
	LOSS [training: 3.6446876103670447 | validation: 5.356510119776234]
	TIME [epoch: 10.4 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.663096922277448		[learning rate: 0.0059718]
	Learning Rate: 0.0059718
	LOSS [training: 3.663096922277448 | validation: 5.456958955587802]
	TIME [epoch: 10.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.763222904773458		[learning rate: 0.0059501]
	Learning Rate: 0.00595013
	LOSS [training: 3.763222904773458 | validation: 5.454798600378009]
	TIME [epoch: 10.4 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.669417905175122		[learning rate: 0.0059285]
	Learning Rate: 0.00592853
	LOSS [training: 3.669417905175122 | validation: 5.339249107237184]
	TIME [epoch: 10.4 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.66277908798225		[learning rate: 0.005907]
	Learning Rate: 0.00590702
	LOSS [training: 3.66277908798225 | validation: 5.353050843341894]
	TIME [epoch: 10.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6983975072679236		[learning rate: 0.0058856]
	Learning Rate: 0.00588558
	LOSS [training: 3.6983975072679236 | validation: 5.395989400962397]
	TIME [epoch: 10.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6576811706836736		[learning rate: 0.0058642]
	Learning Rate: 0.00586422
	LOSS [training: 3.6576811706836736 | validation: 5.4455770051312475]
	TIME [epoch: 10.4 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6590184402932016		[learning rate: 0.0058429]
	Learning Rate: 0.00584294
	LOSS [training: 3.6590184402932016 | validation: 5.393821376918222]
	TIME [epoch: 10.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6951194724774012		[learning rate: 0.0058217]
	Learning Rate: 0.00582174
	LOSS [training: 3.6951194724774012 | validation: 5.351331706741166]
	TIME [epoch: 10.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6428914645716484		[learning rate: 0.0058006]
	Learning Rate: 0.00580061
	LOSS [training: 3.6428914645716484 | validation: 5.36674380626346]
	TIME [epoch: 10.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.687405974076529		[learning rate: 0.0057796]
	Learning Rate: 0.00577956
	LOSS [training: 3.687405974076529 | validation: 5.349083402191789]
	TIME [epoch: 10.4 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.639402411992269		[learning rate: 0.0057586]
	Learning Rate: 0.00575859
	LOSS [training: 3.639402411992269 | validation: 5.366387555102218]
	TIME [epoch: 10.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.664242799531242		[learning rate: 0.0057377]
	Learning Rate: 0.00573769
	LOSS [training: 3.664242799531242 | validation: 5.976643140917986]
	TIME [epoch: 10.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9619363896857758		[learning rate: 0.0057169]
	Learning Rate: 0.00571686
	LOSS [training: 3.9619363896857758 | validation: 5.396672557637483]
	TIME [epoch: 10.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.65870283570771		[learning rate: 0.0056961]
	Learning Rate: 0.00569612
	LOSS [training: 3.65870283570771 | validation: 6.174175547171851]
	TIME [epoch: 10.4 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.9515846134038677		[learning rate: 0.0056754]
	Learning Rate: 0.00567545
	LOSS [training: 3.9515846134038677 | validation: 5.351638831551315]
	TIME [epoch: 10.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.641905764284407		[learning rate: 0.0056548]
	Learning Rate: 0.00565485
	LOSS [training: 3.641905764284407 | validation: 5.399140530702207]
	TIME [epoch: 10.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.672541731487663		[learning rate: 0.0056343]
	Learning Rate: 0.00563433
	LOSS [training: 3.672541731487663 | validation: 5.3357757154836225]
	TIME [epoch: 10.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6664273431625007		[learning rate: 0.0056139]
	Learning Rate: 0.00561388
	LOSS [training: 3.6664273431625007 | validation: 5.337628420293612]
	TIME [epoch: 10.4 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.667706744159024		[learning rate: 0.0055935]
	Learning Rate: 0.00559351
	LOSS [training: 3.667706744159024 | validation: 5.394176546523182]
	TIME [epoch: 10.4 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.644348455286168		[learning rate: 0.0055732]
	Learning Rate: 0.00557321
	LOSS [training: 3.644348455286168 | validation: 5.436473791990003]
	TIME [epoch: 10.4 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6972710747782918		[learning rate: 0.005553]
	Learning Rate: 0.00555298
	LOSS [training: 3.6972710747782918 | validation: 5.338995705882564]
	TIME [epoch: 10.4 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.680555609352281		[learning rate: 0.0055328]
	Learning Rate: 0.00553283
	LOSS [training: 3.680555609352281 | validation: 5.435745172149916]
	TIME [epoch: 10.4 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.672786309673269		[learning rate: 0.0055128]
	Learning Rate: 0.00551275
	LOSS [training: 3.672786309673269 | validation: 5.359778016835114]
	TIME [epoch: 10.4 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6283244491669735		[learning rate: 0.0054927]
	Learning Rate: 0.00549274
	LOSS [training: 3.6283244491669735 | validation: 5.33973906401908]
	TIME [epoch: 10.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.641481962073641		[learning rate: 0.0054728]
	Learning Rate: 0.00547281
	LOSS [training: 3.641481962073641 | validation: 5.330264634409075]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6427243823232764		[learning rate: 0.005453]
	Learning Rate: 0.00545295
	LOSS [training: 3.6427243823232764 | validation: 5.446568919116394]
	TIME [epoch: 10.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.713636173625838		[learning rate: 0.0054332]
	Learning Rate: 0.00543316
	LOSS [training: 3.713636173625838 | validation: 5.3231047892202845]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6462630662071875		[learning rate: 0.0054134]
	Learning Rate: 0.00541344
	LOSS [training: 3.6462630662071875 | validation: 5.4326124164082215]
	TIME [epoch: 10.4 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.63123745835061		[learning rate: 0.0053938]
	Learning Rate: 0.0053938
	LOSS [training: 3.63123745835061 | validation: 5.321841846972982]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662439137964899		[learning rate: 0.0053742]
	Learning Rate: 0.00537422
	LOSS [training: 3.662439137964899 | validation: 5.3399091982807025]
	TIME [epoch: 10.4 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.616220832860518		[learning rate: 0.0053547]
	Learning Rate: 0.00535472
	LOSS [training: 3.616220832860518 | validation: 5.351079609957749]
	TIME [epoch: 10.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.636491728688182		[learning rate: 0.0053353]
	Learning Rate: 0.00533529
	LOSS [training: 3.636491728688182 | validation: 5.356746728311119]
	TIME [epoch: 10.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625741464080859		[learning rate: 0.0053159]
	Learning Rate: 0.00531593
	LOSS [training: 3.625741464080859 | validation: 5.348332767570951]
	TIME [epoch: 10.4 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.676006881773044		[learning rate: 0.0052966]
	Learning Rate: 0.00529663
	LOSS [training: 3.676006881773044 | validation: 5.461046764082444]
	TIME [epoch: 10.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7126444456514265		[learning rate: 0.0052774]
	Learning Rate: 0.00527741
	LOSS [training: 3.7126444456514265 | validation: 5.356255389191733]
	TIME [epoch: 10.4 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.647424869875523		[learning rate: 0.0052583]
	Learning Rate: 0.00525826
	LOSS [training: 3.647424869875523 | validation: 5.332486615562178]
	TIME [epoch: 10.4 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6763845651948346		[learning rate: 0.0052392]
	Learning Rate: 0.00523918
	LOSS [training: 3.6763845651948346 | validation: 5.446798837485744]
	TIME [epoch: 10.4 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.772985908513886		[learning rate: 0.0052202]
	Learning Rate: 0.00522017
	LOSS [training: 3.772985908513886 | validation: 5.332970794242615]
	TIME [epoch: 10.4 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6228249216162354		[learning rate: 0.0052012]
	Learning Rate: 0.00520122
	LOSS [training: 3.6228249216162354 | validation: 5.409679960201109]
	TIME [epoch: 10.4 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.741937826326706		[learning rate: 0.0051823]
	Learning Rate: 0.00518234
	LOSS [training: 3.741937826326706 | validation: 5.389423506471411]
	TIME [epoch: 10.4 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.655471424912311		[learning rate: 0.0051635]
	Learning Rate: 0.00516354
	LOSS [training: 3.655471424912311 | validation: 5.401267363838063]
	TIME [epoch: 10.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6459375801628675		[learning rate: 0.0051448]
	Learning Rate: 0.0051448
	LOSS [training: 3.6459375801628675 | validation: 5.385073205147091]
	TIME [epoch: 10.4 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.626303305972729		[learning rate: 0.0051261]
	Learning Rate: 0.00512613
	LOSS [training: 3.626303305972729 | validation: 5.3523379942120926]
	TIME [epoch: 10.4 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6666942401751585		[learning rate: 0.0051075]
	Learning Rate: 0.00510753
	LOSS [training: 3.6666942401751585 | validation: 5.433695823398348]
	TIME [epoch: 10.4 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6682954500052745		[learning rate: 0.005089]
	Learning Rate: 0.00508899
	LOSS [training: 3.6682954500052745 | validation: 5.3190052014620735]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.670366653423612		[learning rate: 0.0050705]
	Learning Rate: 0.00507052
	LOSS [training: 3.670366653423612 | validation: 5.559065724170513]
	TIME [epoch: 10.4 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.710220611127981		[learning rate: 0.0050521]
	Learning Rate: 0.00505212
	LOSS [training: 3.710220611127981 | validation: 5.503507975016628]
	TIME [epoch: 10.4 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6685891865160167		[learning rate: 0.0050338]
	Learning Rate: 0.00503379
	LOSS [training: 3.6685891865160167 | validation: 5.315779891195731]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.706076986028902		[learning rate: 0.0050155]
	Learning Rate: 0.00501552
	LOSS [training: 3.706076986028902 | validation: 5.389567594772895]
	TIME [epoch: 10.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.644150909158492		[learning rate: 0.0049973]
	Learning Rate: 0.00499732
	LOSS [training: 3.644150909158492 | validation: 5.372418117350305]
	TIME [epoch: 10.4 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6042447659903116		[learning rate: 0.0049792]
	Learning Rate: 0.00497918
	LOSS [training: 3.6042447659903116 | validation: 5.356208489076986]
	TIME [epoch: 10.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602873896384087		[learning rate: 0.0049611]
	Learning Rate: 0.00496111
	LOSS [training: 3.602873896384087 | validation: 5.479460248433031]
	TIME [epoch: 10.4 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.665840278288261		[learning rate: 0.0049431]
	Learning Rate: 0.00494311
	LOSS [training: 3.665840278288261 | validation: 5.363652982436957]
	TIME [epoch: 10.4 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6159059524565995		[learning rate: 0.0049252]
	Learning Rate: 0.00492517
	LOSS [training: 3.6159059524565995 | validation: 5.381857753643961]
	TIME [epoch: 10.4 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635176414646241		[learning rate: 0.0049073]
	Learning Rate: 0.00490729
	LOSS [training: 3.635176414646241 | validation: 5.368228237725041]
	TIME [epoch: 10.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6260418955489255		[learning rate: 0.0048895]
	Learning Rate: 0.00488948
	LOSS [training: 3.6260418955489255 | validation: 5.323737814471319]
	TIME [epoch: 10.4 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5788527472014615		[learning rate: 0.0048717]
	Learning Rate: 0.00487174
	LOSS [training: 3.5788527472014615 | validation: 5.41749581101643]
	TIME [epoch: 10.4 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6018249225661663		[learning rate: 0.0048541]
	Learning Rate: 0.00485406
	LOSS [training: 3.6018249225661663 | validation: 5.3200445150937]
	TIME [epoch: 10.4 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.660211239693234		[learning rate: 0.0048364]
	Learning Rate: 0.00483645
	LOSS [training: 3.660211239693234 | validation: 5.361353482091566]
	TIME [epoch: 10.4 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635447896984015		[learning rate: 0.0048189]
	Learning Rate: 0.00481889
	LOSS [training: 3.635447896984015 | validation: 5.352290354900162]
	TIME [epoch: 10.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.643886251002459		[learning rate: 0.0048014]
	Learning Rate: 0.00480141
	LOSS [training: 3.643886251002459 | validation: 5.387038142180806]
	TIME [epoch: 10.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.624861018256367		[learning rate: 0.004784]
	Learning Rate: 0.00478398
	LOSS [training: 3.624861018256367 | validation: 5.471319855442669]
	TIME [epoch: 10.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625428768321774		[learning rate: 0.0047666]
	Learning Rate: 0.00476662
	LOSS [training: 3.625428768321774 | validation: 5.343751612962745]
	TIME [epoch: 10.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6080088546743854		[learning rate: 0.0047493]
	Learning Rate: 0.00474932
	LOSS [training: 3.6080088546743854 | validation: 5.3003865518865565]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599430486930234		[learning rate: 0.0047321]
	Learning Rate: 0.00473209
	LOSS [training: 3.599430486930234 | validation: 5.305674542159278]
	TIME [epoch: 10.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6473750659471236		[learning rate: 0.0047149]
	Learning Rate: 0.00471491
	LOSS [training: 3.6473750659471236 | validation: 5.521781181882518]
	TIME [epoch: 10.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.676104046774551		[learning rate: 0.0046978]
	Learning Rate: 0.0046978
	LOSS [training: 3.676104046774551 | validation: 5.33210647440163]
	TIME [epoch: 10.4 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5797993351606165		[learning rate: 0.0046808]
	Learning Rate: 0.00468075
	LOSS [training: 3.5797993351606165 | validation: 5.392274227035874]
	TIME [epoch: 10.4 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.737972864643718		[learning rate: 0.0046638]
	Learning Rate: 0.00466377
	LOSS [training: 3.737972864643718 | validation: 5.340940033951833]
	TIME [epoch: 10.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.626306476010152		[learning rate: 0.0046468]
	Learning Rate: 0.00464684
	LOSS [training: 3.626306476010152 | validation: 5.365317123296897]
	TIME [epoch: 10.4 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.652497868405675		[learning rate: 0.00463]
	Learning Rate: 0.00462998
	LOSS [training: 3.652497868405675 | validation: 5.326325571432576]
	TIME [epoch: 10.4 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.633023514628772		[learning rate: 0.0046132]
	Learning Rate: 0.00461318
	LOSS [training: 3.633023514628772 | validation: 5.437283899890958]
	TIME [epoch: 10.4 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.712233647384582		[learning rate: 0.0045964]
	Learning Rate: 0.00459643
	LOSS [training: 3.712233647384582 | validation: 5.349042418120815]
	TIME [epoch: 10.4 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6436362552189707		[learning rate: 0.0045798]
	Learning Rate: 0.00457975
	LOSS [training: 3.6436362552189707 | validation: 5.437352828910568]
	TIME [epoch: 10.4 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.713326837573716		[learning rate: 0.0045631]
	Learning Rate: 0.00456313
	LOSS [training: 3.713326837573716 | validation: 5.332019045651464]
	TIME [epoch: 10.4 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6382375157019355		[learning rate: 0.0045466]
	Learning Rate: 0.00454657
	LOSS [training: 3.6382375157019355 | validation: 5.345484142654311]
	TIME [epoch: 10.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.598268484816348		[learning rate: 0.0045301]
	Learning Rate: 0.00453007
	LOSS [training: 3.598268484816348 | validation: 5.31745561727027]
	TIME [epoch: 10.4 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.636043751386057		[learning rate: 0.0045136]
	Learning Rate: 0.00451363
	LOSS [training: 3.636043751386057 | validation: 5.307829721702976]
	TIME [epoch: 10.4 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5847579070463764		[learning rate: 0.0044973]
	Learning Rate: 0.00449725
	LOSS [training: 3.5847579070463764 | validation: 5.328018235596154]
	TIME [epoch: 10.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596329342718114		[learning rate: 0.0044809]
	Learning Rate: 0.00448093
	LOSS [training: 3.596329342718114 | validation: 5.409019093312475]
	TIME [epoch: 10.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6185996893874313		[learning rate: 0.0044647]
	Learning Rate: 0.00446467
	LOSS [training: 3.6185996893874313 | validation: 5.392970018802775]
	TIME [epoch: 10.4 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.614269702534288		[learning rate: 0.0044485]
	Learning Rate: 0.00444847
	LOSS [training: 3.614269702534288 | validation: 5.363309058067755]
	TIME [epoch: 10.4 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6146537443454094		[learning rate: 0.0044323]
	Learning Rate: 0.00443232
	LOSS [training: 3.6146537443454094 | validation: 5.309265348620756]
	TIME [epoch: 10.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6067240926265343		[learning rate: 0.0044162]
	Learning Rate: 0.00441624
	LOSS [training: 3.6067240926265343 | validation: 5.3143693493272455]
	TIME [epoch: 10.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6258104661950186		[learning rate: 0.0044002]
	Learning Rate: 0.00440021
	LOSS [training: 3.6258104661950186 | validation: 5.315466938308199]
	TIME [epoch: 10.4 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6256276015336963		[learning rate: 0.0043842]
	Learning Rate: 0.00438424
	LOSS [training: 3.6256276015336963 | validation: 5.4044332158475745]
	TIME [epoch: 10.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6333149811555616		[learning rate: 0.0043683]
	Learning Rate: 0.00436833
	LOSS [training: 3.6333149811555616 | validation: 5.330498157277576]
	TIME [epoch: 10.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5998892413333072		[learning rate: 0.0043525]
	Learning Rate: 0.00435248
	LOSS [training: 3.5998892413333072 | validation: 5.331104743530651]
	TIME [epoch: 10.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615183470288959		[learning rate: 0.0043367]
	Learning Rate: 0.00433668
	LOSS [training: 3.615183470288959 | validation: 5.305477121276567]
	TIME [epoch: 10.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6616965179831817		[learning rate: 0.0043209]
	Learning Rate: 0.00432095
	LOSS [training: 3.6616965179831817 | validation: 5.311976293859296]
	TIME [epoch: 10.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6123054168333626		[learning rate: 0.0043053]
	Learning Rate: 0.00430527
	LOSS [training: 3.6123054168333626 | validation: 5.30923406074901]
	TIME [epoch: 10.4 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.61651572778415		[learning rate: 0.0042896]
	Learning Rate: 0.00428964
	LOSS [training: 3.61651572778415 | validation: 5.3676099347972785]
	TIME [epoch: 10.4 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6010006014400395		[learning rate: 0.0042741]
	Learning Rate: 0.00427407
	LOSS [training: 3.6010006014400395 | validation: 5.373310385212965]
	TIME [epoch: 10.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.601135785765939		[learning rate: 0.0042586]
	Learning Rate: 0.00425856
	LOSS [training: 3.601135785765939 | validation: 5.371974279290461]
	TIME [epoch: 10.4 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5923711265703524		[learning rate: 0.0042431]
	Learning Rate: 0.00424311
	LOSS [training: 3.5923711265703524 | validation: 5.291481314379819]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.662824599331258		[learning rate: 0.0042277]
	Learning Rate: 0.00422771
	LOSS [training: 3.662824599331258 | validation: 5.309716419387972]
	TIME [epoch: 10.4 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5987860522858384		[learning rate: 0.0042124]
	Learning Rate: 0.00421237
	LOSS [training: 3.5987860522858384 | validation: 5.310487481153484]
	TIME [epoch: 10.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5737878481854053		[learning rate: 0.0041971]
	Learning Rate: 0.00419708
	LOSS [training: 3.5737878481854053 | validation: 5.334580321245077]
	TIME [epoch: 10.4 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6739797674607724		[learning rate: 0.0041818]
	Learning Rate: 0.00418185
	LOSS [training: 3.6739797674607724 | validation: 5.393614988389506]
	TIME [epoch: 10.4 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6081259371537513		[learning rate: 0.0041667]
	Learning Rate: 0.00416667
	LOSS [training: 3.6081259371537513 | validation: 5.415196430707099]
	TIME [epoch: 10.4 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6270891790259947		[learning rate: 0.0041516]
	Learning Rate: 0.00415155
	LOSS [training: 3.6270891790259947 | validation: 5.323490547793108]
	TIME [epoch: 10.4 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6074865363367126		[learning rate: 0.0041365]
	Learning Rate: 0.00413649
	LOSS [training: 3.6074865363367126 | validation: 5.300142932402596]
	TIME [epoch: 10.4 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.592361180667408		[learning rate: 0.0041215]
	Learning Rate: 0.00412147
	LOSS [training: 3.592361180667408 | validation: 5.33857170753447]
	TIME [epoch: 10.4 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6805931682230657		[learning rate: 0.0041065]
	Learning Rate: 0.00410652
	LOSS [training: 3.6805931682230657 | validation: 5.461713381403975]
	TIME [epoch: 10.4 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6284120250250944		[learning rate: 0.0040916]
	Learning Rate: 0.00409161
	LOSS [training: 3.6284120250250944 | validation: 5.336676215547887]
	TIME [epoch: 10.4 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5900618819622174		[learning rate: 0.0040768]
	Learning Rate: 0.00407677
	LOSS [training: 3.5900618819622174 | validation: 5.3256938103982865]
	TIME [epoch: 10.4 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5698455422934545		[learning rate: 0.004062]
	Learning Rate: 0.00406197
	LOSS [training: 3.5698455422934545 | validation: 5.341097846081505]
	TIME [epoch: 10.4 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.579733951803994		[learning rate: 0.0040472]
	Learning Rate: 0.00404723
	LOSS [training: 3.579733951803994 | validation: 5.286167951872597]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5847499853468996		[learning rate: 0.0040325]
	Learning Rate: 0.00403254
	LOSS [training: 3.5847499853468996 | validation: 5.290617874837851]
	TIME [epoch: 10.4 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6615965885886887		[learning rate: 0.0040179]
	Learning Rate: 0.00401791
	LOSS [training: 3.6615965885886887 | validation: 5.482666314983288]
	TIME [epoch: 10.4 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6261879698020083		[learning rate: 0.0040033]
	Learning Rate: 0.00400333
	LOSS [training: 3.6261879698020083 | validation: 5.340211043916269]
	TIME [epoch: 10.4 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6048476558337397		[learning rate: 0.0039888]
	Learning Rate: 0.0039888
	LOSS [training: 3.6048476558337397 | validation: 5.355208764309136]
	TIME [epoch: 10.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5929803992662324		[learning rate: 0.0039743]
	Learning Rate: 0.00397432
	LOSS [training: 3.5929803992662324 | validation: 5.305482672479273]
	TIME [epoch: 10.4 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6383518955645577		[learning rate: 0.0039599]
	Learning Rate: 0.0039599
	LOSS [training: 3.6383518955645577 | validation: 5.314036088946268]
	TIME [epoch: 10.4 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5979533149441445		[learning rate: 0.0039455]
	Learning Rate: 0.00394553
	LOSS [training: 3.5979533149441445 | validation: 5.363158957019655]
	TIME [epoch: 10.4 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.617192594112228		[learning rate: 0.0039312]
	Learning Rate: 0.00393121
	LOSS [training: 3.617192594112228 | validation: 5.341608043909356]
	TIME [epoch: 10.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5794301456507838		[learning rate: 0.0039169]
	Learning Rate: 0.00391694
	LOSS [training: 3.5794301456507838 | validation: 5.330812751952305]
	TIME [epoch: 10.4 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5847176236449263		[learning rate: 0.0039027]
	Learning Rate: 0.00390273
	LOSS [training: 3.5847176236449263 | validation: 5.285142508991961]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5948765466298114		[learning rate: 0.0038886]
	Learning Rate: 0.00388857
	LOSS [training: 3.5948765466298114 | validation: 5.293785934423677]
	TIME [epoch: 10.4 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.621739624749935		[learning rate: 0.0038745]
	Learning Rate: 0.00387445
	LOSS [training: 3.621739624749935 | validation: 5.380883308047248]
	TIME [epoch: 10.4 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6363986170305287		[learning rate: 0.0038604]
	Learning Rate: 0.00386039
	LOSS [training: 3.6363986170305287 | validation: 5.312952050511665]
	TIME [epoch: 10.4 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5788894186879965		[learning rate: 0.0038464]
	Learning Rate: 0.00384638
	LOSS [training: 3.5788894186879965 | validation: 5.341518596338447]
	TIME [epoch: 10.4 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625605436049805		[learning rate: 0.0038324]
	Learning Rate: 0.00383242
	LOSS [training: 3.625605436049805 | validation: 5.298275739498974]
	TIME [epoch: 10.4 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6001136730248553		[learning rate: 0.0038185]
	Learning Rate: 0.00381852
	LOSS [training: 3.6001136730248553 | validation: 5.303430406673993]
	TIME [epoch: 10.4 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5700365969475087		[learning rate: 0.0038047]
	Learning Rate: 0.00380466
	LOSS [training: 3.5700365969475087 | validation: 5.304152694537557]
	TIME [epoch: 10.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.602215206103672		[learning rate: 0.0037909]
	Learning Rate: 0.00379085
	LOSS [training: 3.602215206103672 | validation: 5.303451450526368]
	TIME [epoch: 10.4 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5905225620150523		[learning rate: 0.0037771]
	Learning Rate: 0.00377709
	LOSS [training: 3.5905225620150523 | validation: 5.29856251814914]
	TIME [epoch: 10.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.635151528653222		[learning rate: 0.0037634]
	Learning Rate: 0.00376339
	LOSS [training: 3.635151528653222 | validation: 5.330110740149412]
	TIME [epoch: 10.4 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5858394468106822		[learning rate: 0.0037497]
	Learning Rate: 0.00374973
	LOSS [training: 3.5858394468106822 | validation: 5.371201835658623]
	TIME [epoch: 10.4 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6508763205808896		[learning rate: 0.0037361]
	Learning Rate: 0.00373612
	LOSS [training: 3.6508763205808896 | validation: 5.493329644858658]
	TIME [epoch: 10.4 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6597588369300253		[learning rate: 0.0037226]
	Learning Rate: 0.00372256
	LOSS [training: 3.6597588369300253 | validation: 5.3184336352502894]
	TIME [epoch: 10.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6433518391700055		[learning rate: 0.0037091]
	Learning Rate: 0.00370905
	LOSS [training: 3.6433518391700055 | validation: 5.311482292965593]
	TIME [epoch: 10.4 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566187645231082		[learning rate: 0.0036956]
	Learning Rate: 0.00369559
	LOSS [training: 3.566187645231082 | validation: 5.28899790535433]
	TIME [epoch: 10.4 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5713906810681677		[learning rate: 0.0036822]
	Learning Rate: 0.00368218
	LOSS [training: 3.5713906810681677 | validation: 5.345965695788607]
	TIME [epoch: 10.4 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6600790773589997		[learning rate: 0.0036688]
	Learning Rate: 0.00366882
	LOSS [training: 3.6600790773589997 | validation: 5.455096715634447]
	TIME [epoch: 10.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5969797393029816		[learning rate: 0.0036555]
	Learning Rate: 0.0036555
	LOSS [training: 3.5969797393029816 | validation: 5.284145668325143]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.574691559951642		[learning rate: 0.0036422]
	Learning Rate: 0.00364224
	LOSS [training: 3.574691559951642 | validation: 5.309961936823786]
	TIME [epoch: 10.4 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.593098029370932		[learning rate: 0.003629]
	Learning Rate: 0.00362902
	LOSS [training: 3.593098029370932 | validation: 5.352321318272839]
	TIME [epoch: 10.4 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6171569358166407		[learning rate: 0.0036159]
	Learning Rate: 0.00361585
	LOSS [training: 3.6171569358166407 | validation: 5.311641456054905]
	TIME [epoch: 10.4 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.580794849879377		[learning rate: 0.0036027]
	Learning Rate: 0.00360273
	LOSS [training: 3.580794849879377 | validation: 5.297072922323588]
	TIME [epoch: 10.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.605649347520642		[learning rate: 0.0035897]
	Learning Rate: 0.00358965
	LOSS [training: 3.605649347520642 | validation: 5.5630229412181]
	TIME [epoch: 10.4 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.683608257094658		[learning rate: 0.0035766]
	Learning Rate: 0.00357663
	LOSS [training: 3.683608257094658 | validation: 5.382933803739483]
	TIME [epoch: 10.4 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6179471382641126		[learning rate: 0.0035636]
	Learning Rate: 0.00356365
	LOSS [training: 3.6179471382641126 | validation: 5.33909939654878]
	TIME [epoch: 10.4 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6060270847443108		[learning rate: 0.0035507]
	Learning Rate: 0.00355072
	LOSS [training: 3.6060270847443108 | validation: 5.294356263412774]
	TIME [epoch: 10.4 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5674754786942344		[learning rate: 0.0035378]
	Learning Rate: 0.00353783
	LOSS [training: 3.5674754786942344 | validation: 5.356508840236577]
	TIME [epoch: 10.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5871629715010878		[learning rate: 0.003525]
	Learning Rate: 0.00352499
	LOSS [training: 3.5871629715010878 | validation: 5.286897188061282]
	TIME [epoch: 10.4 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5720479058443795		[learning rate: 0.0035122]
	Learning Rate: 0.0035122
	LOSS [training: 3.5720479058443795 | validation: 5.2886105018381935]
	TIME [epoch: 10.4 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6300205452326453		[learning rate: 0.0034995]
	Learning Rate: 0.00349945
	LOSS [training: 3.6300205452326453 | validation: 5.320854064449635]
	TIME [epoch: 10.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.595375762021709		[learning rate: 0.0034868]
	Learning Rate: 0.00348675
	LOSS [training: 3.595375762021709 | validation: 5.521166853277252]
	TIME [epoch: 10.4 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6626984293248173		[learning rate: 0.0034741]
	Learning Rate: 0.0034741
	LOSS [training: 3.6626984293248173 | validation: 5.31529176193329]
	TIME [epoch: 10.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5862568428090675		[learning rate: 0.0034615]
	Learning Rate: 0.00346149
	LOSS [training: 3.5862568428090675 | validation: 5.329306180729125]
	TIME [epoch: 10.4 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6183293289362615		[learning rate: 0.0034489]
	Learning Rate: 0.00344893
	LOSS [training: 3.6183293289362615 | validation: 5.420620288994146]
	TIME [epoch: 10.4 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6040418924548328		[learning rate: 0.0034364]
	Learning Rate: 0.00343641
	LOSS [training: 3.6040418924548328 | validation: 5.3607835523112595]
	TIME [epoch: 10.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6099587061222342		[learning rate: 0.0034239]
	Learning Rate: 0.00342394
	LOSS [training: 3.6099587061222342 | validation: 5.29213260086004]
	TIME [epoch: 10.4 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6134973497131697		[learning rate: 0.0034115]
	Learning Rate: 0.00341152
	LOSS [training: 3.6134973497131697 | validation: 5.311739441085559]
	TIME [epoch: 10.4 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559168642269249		[learning rate: 0.0033991]
	Learning Rate: 0.00339914
	LOSS [training: 3.559168642269249 | validation: 5.293408484518205]
	TIME [epoch: 10.4 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5985505546223067		[learning rate: 0.0033868]
	Learning Rate: 0.0033868
	LOSS [training: 3.5985505546223067 | validation: 5.28693125181879]
	TIME [epoch: 10.4 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.558921396513383		[learning rate: 0.0033745]
	Learning Rate: 0.00337451
	LOSS [training: 3.558921396513383 | validation: 5.408832108141224]
	TIME [epoch: 10.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.589443926262061		[learning rate: 0.0033623]
	Learning Rate: 0.00336226
	LOSS [training: 3.589443926262061 | validation: 5.369531844180615]
	TIME [epoch: 10.4 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.625258995925987		[learning rate: 0.0033501]
	Learning Rate: 0.00335006
	LOSS [training: 3.625258995925987 | validation: 5.310104159788161]
	TIME [epoch: 10.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.549070864920329		[learning rate: 0.0033379]
	Learning Rate: 0.0033379
	LOSS [training: 3.549070864920329 | validation: 5.355577920172031]
	TIME [epoch: 10.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.610668845915813		[learning rate: 0.0033258]
	Learning Rate: 0.00332579
	LOSS [training: 3.610668845915813 | validation: 5.319429475639239]
	TIME [epoch: 10.4 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.60216965432832		[learning rate: 0.0033137]
	Learning Rate: 0.00331372
	LOSS [training: 3.60216965432832 | validation: 5.271119760050943]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.629378159718725		[learning rate: 0.0033017]
	Learning Rate: 0.00330169
	LOSS [training: 3.629378159718725 | validation: 5.301294638824518]
	TIME [epoch: 10.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5712954844351765		[learning rate: 0.0032897]
	Learning Rate: 0.00328971
	LOSS [training: 3.5712954844351765 | validation: 5.28240913645348]
	TIME [epoch: 10.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.57087400335636		[learning rate: 0.0032778]
	Learning Rate: 0.00327777
	LOSS [training: 3.57087400335636 | validation: 5.288917619443301]
	TIME [epoch: 10.4 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.588818266547856		[learning rate: 0.0032659]
	Learning Rate: 0.00326588
	LOSS [training: 3.588818266547856 | validation: 5.3450387576953835]
	TIME [epoch: 10.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5786953720916097		[learning rate: 0.003254]
	Learning Rate: 0.00325403
	LOSS [training: 3.5786953720916097 | validation: 5.293909537827289]
	TIME [epoch: 10.4 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.571090444603064		[learning rate: 0.0032422]
	Learning Rate: 0.00324222
	LOSS [training: 3.571090444603064 | validation: 5.308997517173172]
	TIME [epoch: 10.4 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6102208892737693		[learning rate: 0.0032305]
	Learning Rate: 0.00323045
	LOSS [training: 3.6102208892737693 | validation: 5.275943912502819]
	TIME [epoch: 10.4 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591561817618161		[learning rate: 0.0032187]
	Learning Rate: 0.00321873
	LOSS [training: 3.591561817618161 | validation: 5.324212978440894]
	TIME [epoch: 10.4 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5720806508820653		[learning rate: 0.003207]
	Learning Rate: 0.00320705
	LOSS [training: 3.5720806508820653 | validation: 5.339832398300587]
	TIME [epoch: 10.4 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.572874302822555		[learning rate: 0.0031954]
	Learning Rate: 0.00319541
	LOSS [training: 3.572874302822555 | validation: 5.356761023493516]
	TIME [epoch: 10.4 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6173438945604857		[learning rate: 0.0031838]
	Learning Rate: 0.00318381
	LOSS [training: 3.6173438945604857 | validation: 5.279359131772886]
	TIME [epoch: 10.4 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5569986392577078		[learning rate: 0.0031723]
	Learning Rate: 0.00317226
	LOSS [training: 3.5569986392577078 | validation: 5.307638678898477]
	TIME [epoch: 10.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.600884703012754		[learning rate: 0.0031607]
	Learning Rate: 0.00316075
	LOSS [training: 3.600884703012754 | validation: 5.306853320461693]
	TIME [epoch: 10.4 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.578996145344567		[learning rate: 0.0031493]
	Learning Rate: 0.00314927
	LOSS [training: 3.578996145344567 | validation: 5.3250291274605175]
	TIME [epoch: 10.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5715488359788443		[learning rate: 0.0031378]
	Learning Rate: 0.00313785
	LOSS [training: 3.5715488359788443 | validation: 5.292951475200177]
	TIME [epoch: 10.4 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.618459628826952		[learning rate: 0.0031265]
	Learning Rate: 0.00312646
	LOSS [training: 3.618459628826952 | validation: 5.277752679743637]
	TIME [epoch: 10.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.577869910741184		[learning rate: 0.0031151]
	Learning Rate: 0.00311511
	LOSS [training: 3.577869910741184 | validation: 5.312675003701095]
	TIME [epoch: 10.4 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5612783859690453		[learning rate: 0.0031038]
	Learning Rate: 0.00310381
	LOSS [training: 3.5612783859690453 | validation: 5.2849716083493465]
	TIME [epoch: 10.4 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5649333203837026		[learning rate: 0.0030925]
	Learning Rate: 0.00309254
	LOSS [training: 3.5649333203837026 | validation: 5.30952094116727]
	TIME [epoch: 10.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6418614828999183		[learning rate: 0.0030813]
	Learning Rate: 0.00308132
	LOSS [training: 3.6418614828999183 | validation: 5.328638994795581]
	TIME [epoch: 10.4 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.556044353808157		[learning rate: 0.0030701]
	Learning Rate: 0.00307014
	LOSS [training: 3.556044353808157 | validation: 5.300837950354453]
	TIME [epoch: 10.4 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7198548469392683		[learning rate: 0.003059]
	Learning Rate: 0.003059
	LOSS [training: 3.7198548469392683 | validation: 5.3344771488951075]
	TIME [epoch: 10.4 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5706732273368607		[learning rate: 0.0030479]
	Learning Rate: 0.0030479
	LOSS [training: 3.5706732273368607 | validation: 5.291516842501318]
	TIME [epoch: 10.4 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.571759636358511		[learning rate: 0.0030368]
	Learning Rate: 0.00303683
	LOSS [training: 3.571759636358511 | validation: 5.300603891729367]
	TIME [epoch: 10.4 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.582294511984351		[learning rate: 0.0030258]
	Learning Rate: 0.00302581
	LOSS [training: 3.582294511984351 | validation: 5.308847478105658]
	TIME [epoch: 10.4 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6076946010974047		[learning rate: 0.0030148]
	Learning Rate: 0.00301483
	LOSS [training: 3.6076946010974047 | validation: 5.281530242531144]
	TIME [epoch: 10.4 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5797806711560782		[learning rate: 0.0030039]
	Learning Rate: 0.00300389
	LOSS [training: 3.5797806711560782 | validation: 5.2828019076828765]
	TIME [epoch: 10.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.579099919134017		[learning rate: 0.002993]
	Learning Rate: 0.00299299
	LOSS [training: 3.579099919134017 | validation: 5.326503900730078]
	TIME [epoch: 10.4 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5747372848293857		[learning rate: 0.0029821]
	Learning Rate: 0.00298213
	LOSS [training: 3.5747372848293857 | validation: 5.304176394384671]
	TIME [epoch: 10.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566321353579562		[learning rate: 0.0029713]
	Learning Rate: 0.00297131
	LOSS [training: 3.566321353579562 | validation: 5.340816957668474]
	TIME [epoch: 10.4 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5840884383599203		[learning rate: 0.0029605]
	Learning Rate: 0.00296052
	LOSS [training: 3.5840884383599203 | validation: 5.3359760282931585]
	TIME [epoch: 10.4 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5889654520738334		[learning rate: 0.0029498]
	Learning Rate: 0.00294978
	LOSS [training: 3.5889654520738334 | validation: 5.3535045493108715]
	TIME [epoch: 10.4 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599595093337615		[learning rate: 0.0029391]
	Learning Rate: 0.00293907
	LOSS [training: 3.599595093337615 | validation: 5.297614225721902]
	TIME [epoch: 10.4 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.590888431129057		[learning rate: 0.0029284]
	Learning Rate: 0.00292841
	LOSS [training: 3.590888431129057 | validation: 5.366138318820994]
	TIME [epoch: 10.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5639360884600912		[learning rate: 0.0029178]
	Learning Rate: 0.00291778
	LOSS [training: 3.5639360884600912 | validation: 5.280376993551833]
	TIME [epoch: 10.4 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5536375895949908		[learning rate: 0.0029072]
	Learning Rate: 0.00290719
	LOSS [training: 3.5536375895949908 | validation: 5.290164752873763]
	TIME [epoch: 10.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5778346696687167		[learning rate: 0.0028966]
	Learning Rate: 0.00289664
	LOSS [training: 3.5778346696687167 | validation: 5.299695624220692]
	TIME [epoch: 10.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.564064132081062		[learning rate: 0.0028861]
	Learning Rate: 0.00288613
	LOSS [training: 3.564064132081062 | validation: 5.279944179747349]
	TIME [epoch: 10.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5368623354084976		[learning rate: 0.0028757]
	Learning Rate: 0.00287566
	LOSS [training: 3.5368623354084976 | validation: 5.298671050155515]
	TIME [epoch: 10.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5589776233800428		[learning rate: 0.0028652]
	Learning Rate: 0.00286522
	LOSS [training: 3.5589776233800428 | validation: 5.325444092682339]
	TIME [epoch: 10.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.579155363022256		[learning rate: 0.0028548]
	Learning Rate: 0.00285482
	LOSS [training: 3.579155363022256 | validation: 5.289435388099073]
	TIME [epoch: 10.4 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.559623428046747		[learning rate: 0.0028445]
	Learning Rate: 0.00284446
	LOSS [training: 3.559623428046747 | validation: 5.311818373262743]
	TIME [epoch: 10.4 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6850240005198898		[learning rate: 0.0028341]
	Learning Rate: 0.00283414
	LOSS [training: 3.6850240005198898 | validation: 5.2973711672294215]
	TIME [epoch: 10.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6098362819238488		[learning rate: 0.0028239]
	Learning Rate: 0.00282385
	LOSS [training: 3.6098362819238488 | validation: 5.365432544826724]
	TIME [epoch: 10.4 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5936805544417076		[learning rate: 0.0028136]
	Learning Rate: 0.00281361
	LOSS [training: 3.5936805544417076 | validation: 5.2918500629809095]
	TIME [epoch: 10.4 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.544339017883824		[learning rate: 0.0028034]
	Learning Rate: 0.00280339
	LOSS [training: 3.544339017883824 | validation: 5.382595440625606]
	TIME [epoch: 10.4 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.567361114746954		[learning rate: 0.0027932]
	Learning Rate: 0.00279322
	LOSS [training: 3.567361114746954 | validation: 5.371138080450901]
	TIME [epoch: 10.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.599471003861221		[learning rate: 0.0027831]
	Learning Rate: 0.00278308
	LOSS [training: 3.599471003861221 | validation: 5.30976255909338]
	TIME [epoch: 10.4 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5546167428761017		[learning rate: 0.002773]
	Learning Rate: 0.00277298
	LOSS [training: 3.5546167428761017 | validation: 5.316082206610453]
	TIME [epoch: 10.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5535558642433003		[learning rate: 0.0027629]
	Learning Rate: 0.00276292
	LOSS [training: 3.5535558642433003 | validation: 5.2869832437322]
	TIME [epoch: 10.4 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5583714801728874		[learning rate: 0.0027529]
	Learning Rate: 0.00275289
	LOSS [training: 3.5583714801728874 | validation: 5.368475293833255]
	TIME [epoch: 10.4 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591165088989033		[learning rate: 0.0027429]
	Learning Rate: 0.0027429
	LOSS [training: 3.591165088989033 | validation: 5.3733300522164065]
	TIME [epoch: 10.4 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5610308874547436		[learning rate: 0.0027329]
	Learning Rate: 0.00273295
	LOSS [training: 3.5610308874547436 | validation: 5.278386509881229]
	TIME [epoch: 10.4 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.58100894489858		[learning rate: 0.002723]
	Learning Rate: 0.00272303
	LOSS [training: 3.58100894489858 | validation: 5.279731768673334]
	TIME [epoch: 10.4 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5546815189735588		[learning rate: 0.0027131]
	Learning Rate: 0.00271315
	LOSS [training: 3.5546815189735588 | validation: 5.3093065941906294]
	TIME [epoch: 10.4 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5906186696203832		[learning rate: 0.0027033]
	Learning Rate: 0.0027033
	LOSS [training: 3.5906186696203832 | validation: 5.386888976368368]
	TIME [epoch: 10.4 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615222866643049		[learning rate: 0.0026935]
	Learning Rate: 0.00269349
	LOSS [training: 3.615222866643049 | validation: 5.276925560618349]
	TIME [epoch: 10.4 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.606761862225931		[learning rate: 0.0026837]
	Learning Rate: 0.00268372
	LOSS [training: 3.606761862225931 | validation: 5.303508709187433]
	TIME [epoch: 10.4 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.626453474677701		[learning rate: 0.002674]
	Learning Rate: 0.00267398
	LOSS [training: 3.626453474677701 | validation: 5.401820439610231]
	TIME [epoch: 10.4 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6159320706570304		[learning rate: 0.0026643]
	Learning Rate: 0.00266427
	LOSS [training: 3.6159320706570304 | validation: 5.278540733951267]
	TIME [epoch: 10.4 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.570832551207939		[learning rate: 0.0026546]
	Learning Rate: 0.00265461
	LOSS [training: 3.570832551207939 | validation: 5.295621964283074]
	TIME [epoch: 10.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5668657374092563		[learning rate: 0.002645]
	Learning Rate: 0.00264497
	LOSS [training: 3.5668657374092563 | validation: 5.287281948536469]
	TIME [epoch: 10.4 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.561564581323706		[learning rate: 0.0026354]
	Learning Rate: 0.00263537
	LOSS [training: 3.561564581323706 | validation: 5.4095163639835935]
	TIME [epoch: 10.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.586292489033972		[learning rate: 0.0026258]
	Learning Rate: 0.00262581
	LOSS [training: 3.586292489033972 | validation: 5.353422012273427]
	TIME [epoch: 10.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.585639818431491		[learning rate: 0.0026163]
	Learning Rate: 0.00261628
	LOSS [training: 3.585639818431491 | validation: 5.278872272015276]
	TIME [epoch: 10.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5597038793926883		[learning rate: 0.0026068]
	Learning Rate: 0.00260679
	LOSS [training: 3.5597038793926883 | validation: 5.327024643586005]
	TIME [epoch: 10.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5806632019920683		[learning rate: 0.0025973]
	Learning Rate: 0.00259733
	LOSS [training: 3.5806632019920683 | validation: 5.306772126671051]
	TIME [epoch: 10.4 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5619914224466585		[learning rate: 0.0025879]
	Learning Rate: 0.0025879
	LOSS [training: 3.5619914224466585 | validation: 5.276266833915163]
	TIME [epoch: 10.4 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5527460881657427		[learning rate: 0.0025785]
	Learning Rate: 0.00257851
	LOSS [training: 3.5527460881657427 | validation: 5.31400758171292]
	TIME [epoch: 10.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5594770535020728		[learning rate: 0.0025691]
	Learning Rate: 0.00256915
	LOSS [training: 3.5594770535020728 | validation: 5.308166293237466]
	TIME [epoch: 10.4 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.553628512346054		[learning rate: 0.0025598]
	Learning Rate: 0.00255983
	LOSS [training: 3.553628512346054 | validation: 5.278811116892155]
	TIME [epoch: 10.4 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5606369543730123		[learning rate: 0.0025505]
	Learning Rate: 0.00255054
	LOSS [training: 3.5606369543730123 | validation: 5.2954518270936966]
	TIME [epoch: 10.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6065427027036505		[learning rate: 0.0025413]
	Learning Rate: 0.00254128
	LOSS [training: 3.6065427027036505 | validation: 5.2930852573334235]
	TIME [epoch: 10.4 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.59053166537245		[learning rate: 0.0025321]
	Learning Rate: 0.00253206
	LOSS [training: 3.59053166537245 | validation: 5.268196267739531]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.551941829095425		[learning rate: 0.0025229]
	Learning Rate: 0.00252287
	LOSS [training: 3.551941829095425 | validation: 5.272523147015729]
	TIME [epoch: 10.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5814484747931927		[learning rate: 0.0025137]
	Learning Rate: 0.00251371
	LOSS [training: 3.5814484747931927 | validation: 5.342147833230858]
	TIME [epoch: 10.4 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5546947219818392		[learning rate: 0.0025046]
	Learning Rate: 0.00250459
	LOSS [training: 3.5546947219818392 | validation: 5.285798807284907]
	TIME [epoch: 10.4 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.558563674796217		[learning rate: 0.0024955]
	Learning Rate: 0.0024955
	LOSS [training: 3.558563674796217 | validation: 5.285451899636144]
	TIME [epoch: 10.4 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.58421024294751		[learning rate: 0.0024864]
	Learning Rate: 0.00248645
	LOSS [training: 3.58421024294751 | validation: 5.274677931478566]
	TIME [epoch: 10.4 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5567628736334114		[learning rate: 0.0024774]
	Learning Rate: 0.00247742
	LOSS [training: 3.5567628736334114 | validation: 5.3059644066483225]
	TIME [epoch: 10.4 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.557015195950021		[learning rate: 0.0024684]
	Learning Rate: 0.00246843
	LOSS [training: 3.557015195950021 | validation: 5.308896161379602]
	TIME [epoch: 10.4 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5596837982339737		[learning rate: 0.0024595]
	Learning Rate: 0.00245947
	LOSS [training: 3.5596837982339737 | validation: 5.6839296239689325]
	TIME [epoch: 10.4 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.7353738679574526		[learning rate: 0.0024505]
	Learning Rate: 0.00245055
	LOSS [training: 3.7353738679574526 | validation: 5.278557506824801]
	TIME [epoch: 10.4 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5686441684925834		[learning rate: 0.0024417]
	Learning Rate: 0.00244165
	LOSS [training: 3.5686441684925834 | validation: 5.309361436777335]
	TIME [epoch: 10.4 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.615598919004016		[learning rate: 0.0024328]
	Learning Rate: 0.00243279
	LOSS [training: 3.615598919004016 | validation: 5.293991625248351]
	TIME [epoch: 10.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5462016724715952		[learning rate: 0.002424]
	Learning Rate: 0.00242396
	LOSS [training: 3.5462016724715952 | validation: 5.3568041012622]
	TIME [epoch: 10.4 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5739215873804993		[learning rate: 0.0024152]
	Learning Rate: 0.00241517
	LOSS [training: 3.5739215873804993 | validation: 5.306157566766648]
	TIME [epoch: 10.4 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.585509146974019		[learning rate: 0.0024064]
	Learning Rate: 0.0024064
	LOSS [training: 3.585509146974019 | validation: 5.292053480275148]
	TIME [epoch: 10.4 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5571043393671298		[learning rate: 0.0023977]
	Learning Rate: 0.00239767
	LOSS [training: 3.5571043393671298 | validation: 5.297198453754451]
	TIME [epoch: 10.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5697651829257238		[learning rate: 0.002389]
	Learning Rate: 0.00238897
	LOSS [training: 3.5697651829257238 | validation: 5.317287463832546]
	TIME [epoch: 10.5 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5538799236634544		[learning rate: 0.0023803]
	Learning Rate: 0.0023803
	LOSS [training: 3.5538799236634544 | validation: 5.333906254366147]
	TIME [epoch: 10.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5631727460243723		[learning rate: 0.0023717]
	Learning Rate: 0.00237166
	LOSS [training: 3.5631727460243723 | validation: 5.3018437925556885]
	TIME [epoch: 10.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.572077635595406		[learning rate: 0.0023631]
	Learning Rate: 0.00236305
	LOSS [training: 3.572077635595406 | validation: 5.319198943735282]
	TIME [epoch: 10.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.55735181830554		[learning rate: 0.0023545]
	Learning Rate: 0.00235448
	LOSS [training: 3.55735181830554 | validation: 5.327860397260593]
	TIME [epoch: 10.4 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596696229359014		[learning rate: 0.0023459]
	Learning Rate: 0.00234593
	LOSS [training: 3.596696229359014 | validation: 5.332190249623284]
	TIME [epoch: 10.4 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6108621339929328		[learning rate: 0.0023374]
	Learning Rate: 0.00233742
	LOSS [training: 3.6108621339929328 | validation: 5.416658926016776]
	TIME [epoch: 10.4 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.596251500318062		[learning rate: 0.0023289]
	Learning Rate: 0.00232894
	LOSS [training: 3.596251500318062 | validation: 5.275766816469036]
	TIME [epoch: 10.4 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5511459900939832		[learning rate: 0.0023205]
	Learning Rate: 0.00232049
	LOSS [training: 3.5511459900939832 | validation: 5.276832747832405]
	TIME [epoch: 10.4 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5488917458271247		[learning rate: 0.0023121]
	Learning Rate: 0.00231206
	LOSS [training: 3.5488917458271247 | validation: 5.296458241455737]
	TIME [epoch: 10.4 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5790963189614793		[learning rate: 0.0023037]
	Learning Rate: 0.00230367
	LOSS [training: 3.5790963189614793 | validation: 5.288822280392976]
	TIME [epoch: 10.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5687612606250205		[learning rate: 0.0022953]
	Learning Rate: 0.00229531
	LOSS [training: 3.5687612606250205 | validation: 5.342706203499061]
	TIME [epoch: 10.4 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5684248433461194		[learning rate: 0.002287]
	Learning Rate: 0.00228698
	LOSS [training: 3.5684248433461194 | validation: 5.341619584966477]
	TIME [epoch: 10.4 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.565363331642576		[learning rate: 0.0022787]
	Learning Rate: 0.00227868
	LOSS [training: 3.565363331642576 | validation: 5.301557305893656]
	TIME [epoch: 10.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.564028250555387		[learning rate: 0.0022704]
	Learning Rate: 0.00227042
	LOSS [training: 3.564028250555387 | validation: 5.288396816728166]
	TIME [epoch: 10.4 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547422046286196		[learning rate: 0.0022622]
	Learning Rate: 0.00226218
	LOSS [training: 3.547422046286196 | validation: 5.307578829018212]
	TIME [epoch: 10.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5856234499056265		[learning rate: 0.002254]
	Learning Rate: 0.00225397
	LOSS [training: 3.5856234499056265 | validation: 5.267629666071533]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591013952911678		[learning rate: 0.0022458]
	Learning Rate: 0.00224579
	LOSS [training: 3.591013952911678 | validation: 5.30390487464677]
	TIME [epoch: 10.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5428459608203466		[learning rate: 0.0022376]
	Learning Rate: 0.00223764
	LOSS [training: 3.5428459608203466 | validation: 5.2842658453107525]
	TIME [epoch: 10.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.55363388284353		[learning rate: 0.0022295]
	Learning Rate: 0.00222952
	LOSS [training: 3.55363388284353 | validation: 5.270927858673467]
	TIME [epoch: 10.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5354204438040426		[learning rate: 0.0022214]
	Learning Rate: 0.00222142
	LOSS [training: 3.5354204438040426 | validation: 5.287890973120343]
	TIME [epoch: 10.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.555071499850107		[learning rate: 0.0022134]
	Learning Rate: 0.00221336
	LOSS [training: 3.555071499850107 | validation: 5.350498251935001]
	TIME [epoch: 10.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5772173062537704		[learning rate: 0.0022053]
	Learning Rate: 0.00220533
	LOSS [training: 3.5772173062537704 | validation: 5.312679667646384]
	TIME [epoch: 10.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5392592300827297		[learning rate: 0.0021973]
	Learning Rate: 0.00219733
	LOSS [training: 3.5392592300827297 | validation: 5.284644124622597]
	TIME [epoch: 10.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547400060812405		[learning rate: 0.0021894]
	Learning Rate: 0.00218935
	LOSS [training: 3.547400060812405 | validation: 5.29194570410875]
	TIME [epoch: 10.4 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5578086344205175		[learning rate: 0.0021814]
	Learning Rate: 0.00218141
	LOSS [training: 3.5578086344205175 | validation: 5.2790574761642315]
	TIME [epoch: 10.4 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5563835781401982		[learning rate: 0.0021735]
	Learning Rate: 0.00217349
	LOSS [training: 3.5563835781401982 | validation: 5.304203408787423]
	TIME [epoch: 10.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5507745864260585		[learning rate: 0.0021656]
	Learning Rate: 0.0021656
	LOSS [training: 3.5507745864260585 | validation: 5.329405753198103]
	TIME [epoch: 10.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5534216080665963		[learning rate: 0.0021577]
	Learning Rate: 0.00215774
	LOSS [training: 3.5534216080665963 | validation: 5.273297952174202]
	TIME [epoch: 10.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5365304754006024		[learning rate: 0.0021499]
	Learning Rate: 0.00214991
	LOSS [training: 3.5365304754006024 | validation: 5.2978475668108125]
	TIME [epoch: 10.4 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.54538395994346		[learning rate: 0.0021421]
	Learning Rate: 0.00214211
	LOSS [training: 3.54538395994346 | validation: 5.28769316032366]
	TIME [epoch: 10.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6047595840988187		[learning rate: 0.0021343]
	Learning Rate: 0.00213434
	LOSS [training: 3.6047595840988187 | validation: 5.411312556357352]
	TIME [epoch: 10.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591673111006636		[learning rate: 0.0021266]
	Learning Rate: 0.00212659
	LOSS [training: 3.591673111006636 | validation: 5.310348940625822]
	TIME [epoch: 10.4 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.553483772384331		[learning rate: 0.0021189]
	Learning Rate: 0.00211887
	LOSS [training: 3.553483772384331 | validation: 5.285627237243919]
	TIME [epoch: 10.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.571210633803877		[learning rate: 0.0021112]
	Learning Rate: 0.00211119
	LOSS [training: 3.571210633803877 | validation: 5.302149334393696]
	TIME [epoch: 10.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.557617411442881		[learning rate: 0.0021035]
	Learning Rate: 0.00210352
	LOSS [training: 3.557617411442881 | validation: 5.304168631935282]
	TIME [epoch: 10.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5604342848678243		[learning rate: 0.0020959]
	Learning Rate: 0.00209589
	LOSS [training: 3.5604342848678243 | validation: 5.329312810627839]
	TIME [epoch: 10.4 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5467187613752933		[learning rate: 0.0020883]
	Learning Rate: 0.00208828
	LOSS [training: 3.5467187613752933 | validation: 5.283623353143817]
	TIME [epoch: 10.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5463565875498104		[learning rate: 0.0020807]
	Learning Rate: 0.00208071
	LOSS [training: 3.5463565875498104 | validation: 5.286848392346384]
	TIME [epoch: 10.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5576576720168425		[learning rate: 0.0020732]
	Learning Rate: 0.00207315
	LOSS [training: 3.5576576720168425 | validation: 5.4005389189572774]
	TIME [epoch: 10.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6116307187953693		[learning rate: 0.0020656]
	Learning Rate: 0.00206563
	LOSS [training: 3.6116307187953693 | validation: 5.383898732960786]
	TIME [epoch: 10.4 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.584481548656335		[learning rate: 0.0020581]
	Learning Rate: 0.00205813
	LOSS [training: 3.584481548656335 | validation: 5.292868814225585]
	TIME [epoch: 10.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5515391265352507		[learning rate: 0.0020507]
	Learning Rate: 0.00205067
	LOSS [training: 3.5515391265352507 | validation: 5.265980937309174]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5479594494887197		[learning rate: 0.0020432]
	Learning Rate: 0.00204322
	LOSS [training: 3.5479594494887197 | validation: 5.29996485897148]
	TIME [epoch: 10.4 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5506734636974704		[learning rate: 0.0020358]
	Learning Rate: 0.00203581
	LOSS [training: 3.5506734636974704 | validation: 5.32673094311157]
	TIME [epoch: 10.4 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5578527397943818		[learning rate: 0.0020284]
	Learning Rate: 0.00202842
	LOSS [training: 3.5578527397943818 | validation: 5.289737631619204]
	TIME [epoch: 10.4 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5495248544668607		[learning rate: 0.0020211]
	Learning Rate: 0.00202106
	LOSS [training: 3.5495248544668607 | validation: 5.301912897138372]
	TIME [epoch: 10.4 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.538146123718712		[learning rate: 0.0020137]
	Learning Rate: 0.00201372
	LOSS [training: 3.538146123718712 | validation: 5.270947556558233]
	TIME [epoch: 10.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.539918077489618		[learning rate: 0.0020064]
	Learning Rate: 0.00200642
	LOSS [training: 3.539918077489618 | validation: 5.338055770244109]
	TIME [epoch: 10.4 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5768368766925307		[learning rate: 0.0019991]
	Learning Rate: 0.00199913
	LOSS [training: 3.5768368766925307 | validation: 5.270848415577311]
	TIME [epoch: 10.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531926373596997		[learning rate: 0.0019919]
	Learning Rate: 0.00199188
	LOSS [training: 3.531926373596997 | validation: 5.290551384547585]
	TIME [epoch: 10.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5397155681041994		[learning rate: 0.0019847]
	Learning Rate: 0.00198465
	LOSS [training: 3.5397155681041994 | validation: 5.286858715337648]
	TIME [epoch: 10.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5545974883443363		[learning rate: 0.0019774]
	Learning Rate: 0.00197745
	LOSS [training: 3.5545974883443363 | validation: 5.289164280346012]
	TIME [epoch: 10.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.55091973635151		[learning rate: 0.0019703]
	Learning Rate: 0.00197027
	LOSS [training: 3.55091973635151 | validation: 5.2728386756585115]
	TIME [epoch: 10.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5401612922870966		[learning rate: 0.0019631]
	Learning Rate: 0.00196312
	LOSS [training: 3.5401612922870966 | validation: 5.272087870273678]
	TIME [epoch: 10.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.552973406804542		[learning rate: 0.001956]
	Learning Rate: 0.001956
	LOSS [training: 3.552973406804542 | validation: 5.26801960302032]
	TIME [epoch: 10.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550311491390533		[learning rate: 0.0019489]
	Learning Rate: 0.0019489
	LOSS [training: 3.550311491390533 | validation: 5.279428060973787]
	TIME [epoch: 10.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5343279785734483		[learning rate: 0.0019418]
	Learning Rate: 0.00194183
	LOSS [training: 3.5343279785734483 | validation: 5.269468910391002]
	TIME [epoch: 10.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.548282691627752		[learning rate: 0.0019348]
	Learning Rate: 0.00193478
	LOSS [training: 3.548282691627752 | validation: 5.346018288194438]
	TIME [epoch: 10.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6160195439837843		[learning rate: 0.0019278]
	Learning Rate: 0.00192776
	LOSS [training: 3.6160195439837843 | validation: 5.352373517121626]
	TIME [epoch: 10.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.577202741730787		[learning rate: 0.0019208]
	Learning Rate: 0.00192076
	LOSS [training: 3.577202741730787 | validation: 5.274036151277379]
	TIME [epoch: 10.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.573050676855342		[learning rate: 0.0019138]
	Learning Rate: 0.00191379
	LOSS [training: 3.573050676855342 | validation: 5.329839927931659]
	TIME [epoch: 10.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.58868016993456		[learning rate: 0.0019068]
	Learning Rate: 0.00190685
	LOSS [training: 3.58868016993456 | validation: 5.301831662439626]
	TIME [epoch: 10.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.547201784839853		[learning rate: 0.0018999]
	Learning Rate: 0.00189993
	LOSS [training: 3.547201784839853 | validation: 5.262201990211781]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_557.pth
	Model improved!!!
EPOCH 558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5396879950401314		[learning rate: 0.001893]
	Learning Rate: 0.00189303
	LOSS [training: 3.5396879950401314 | validation: 5.260654249736498]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5311005094862815		[learning rate: 0.0018862]
	Learning Rate: 0.00188616
	LOSS [training: 3.5311005094862815 | validation: 5.284291762908869]
	TIME [epoch: 10.4 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5562915686703436		[learning rate: 0.0018793]
	Learning Rate: 0.00187932
	LOSS [training: 3.5562915686703436 | validation: 5.342573274405826]
	TIME [epoch: 10.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5620245955158927		[learning rate: 0.0018725]
	Learning Rate: 0.0018725
	LOSS [training: 3.5620245955158927 | validation: 5.305184206551602]
	TIME [epoch: 10.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5624071157239507		[learning rate: 0.0018657]
	Learning Rate: 0.0018657
	LOSS [training: 3.5624071157239507 | validation: 5.260388413704849]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5801781802516666		[learning rate: 0.0018589]
	Learning Rate: 0.00185893
	LOSS [training: 3.5801781802516666 | validation: 5.322844875117519]
	TIME [epoch: 10.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5624980608443613		[learning rate: 0.0018522]
	Learning Rate: 0.00185218
	LOSS [training: 3.5624980608443613 | validation: 5.2737752990316915]
	TIME [epoch: 10.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5321993497731454		[learning rate: 0.0018455]
	Learning Rate: 0.00184546
	LOSS [training: 3.5321993497731454 | validation: 5.281448182384325]
	TIME [epoch: 10.4 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524941678479357		[learning rate: 0.0018388]
	Learning Rate: 0.00183877
	LOSS [training: 3.524941678479357 | validation: 5.300915752105489]
	TIME [epoch: 10.4 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5784161171803506		[learning rate: 0.0018321]
	Learning Rate: 0.00183209
	LOSS [training: 3.5784161171803506 | validation: 5.299883037573052]
	TIME [epoch: 10.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.557364918304658		[learning rate: 0.0018254]
	Learning Rate: 0.00182544
	LOSS [training: 3.557364918304658 | validation: 5.275396172117275]
	TIME [epoch: 10.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.542106211066573		[learning rate: 0.0018188]
	Learning Rate: 0.00181882
	LOSS [training: 3.542106211066573 | validation: 5.2684592734851785]
	TIME [epoch: 10.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5669751065887674		[learning rate: 0.0018122]
	Learning Rate: 0.00181222
	LOSS [training: 3.5669751065887674 | validation: 5.2689058237381845]
	TIME [epoch: 10.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5641434466122335		[learning rate: 0.0018056]
	Learning Rate: 0.00180564
	LOSS [training: 3.5641434466122335 | validation: 5.308160047135]
	TIME [epoch: 10.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.552750292552519		[learning rate: 0.0017991]
	Learning Rate: 0.00179909
	LOSS [training: 3.552750292552519 | validation: 5.275604271706523]
	TIME [epoch: 10.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5495329524441646		[learning rate: 0.0017926]
	Learning Rate: 0.00179256
	LOSS [training: 3.5495329524441646 | validation: 5.274268074686987]
	TIME [epoch: 10.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5422630443726453		[learning rate: 0.0017861]
	Learning Rate: 0.00178605
	LOSS [training: 3.5422630443726453 | validation: 5.3912113455700865]
	TIME [epoch: 10.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5714307456579575		[learning rate: 0.0017796]
	Learning Rate: 0.00177957
	LOSS [training: 3.5714307456579575 | validation: 5.270709751297949]
	TIME [epoch: 10.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5395283577690817		[learning rate: 0.0017731]
	Learning Rate: 0.00177311
	LOSS [training: 3.5395283577690817 | validation: 5.277585480296598]
	TIME [epoch: 10.4 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550051204936154		[learning rate: 0.0017667]
	Learning Rate: 0.00176668
	LOSS [training: 3.550051204936154 | validation: 5.303130460841885]
	TIME [epoch: 10.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.531904503416034		[learning rate: 0.0017603]
	Learning Rate: 0.00176027
	LOSS [training: 3.531904503416034 | validation: 5.276809987164751]
	TIME [epoch: 10.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.532475539337708		[learning rate: 0.0017539]
	Learning Rate: 0.00175388
	LOSS [training: 3.532475539337708 | validation: 5.2840925213763965]
	TIME [epoch: 10.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5330509247493254		[learning rate: 0.0017475]
	Learning Rate: 0.00174752
	LOSS [training: 3.5330509247493254 | validation: 5.267683387013326]
	TIME [epoch: 10.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5710069404168627		[learning rate: 0.0017412]
	Learning Rate: 0.00174117
	LOSS [training: 3.5710069404168627 | validation: 5.279811216673129]
	TIME [epoch: 10.4 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5538981148203903		[learning rate: 0.0017349]
	Learning Rate: 0.00173486
	LOSS [training: 3.5538981148203903 | validation: 5.274753559217445]
	TIME [epoch: 10.4 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527113362523861		[learning rate: 0.0017286]
	Learning Rate: 0.00172856
	LOSS [training: 3.527113362523861 | validation: 5.275024834807891]
	TIME [epoch: 10.4 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5611767098521647		[learning rate: 0.0017223]
	Learning Rate: 0.00172229
	LOSS [training: 3.5611767098521647 | validation: 5.277059802353001]
	TIME [epoch: 10.4 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535046844169083		[learning rate: 0.001716]
	Learning Rate: 0.00171604
	LOSS [training: 3.535046844169083 | validation: 5.263073983909412]
	TIME [epoch: 10.4 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5612635951592297		[learning rate: 0.0017098]
	Learning Rate: 0.00170981
	LOSS [training: 3.5612635951592297 | validation: 5.258271349349955]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5702708351444983		[learning rate: 0.0017036]
	Learning Rate: 0.0017036
	LOSS [training: 3.5702708351444983 | validation: 5.2755921487046065]
	TIME [epoch: 10.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5340813622995952		[learning rate: 0.0016974]
	Learning Rate: 0.00169742
	LOSS [training: 3.5340813622995952 | validation: 5.2882018050582085]
	TIME [epoch: 10.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525998564805108		[learning rate: 0.0016913]
	Learning Rate: 0.00169126
	LOSS [training: 3.525998564805108 | validation: 5.256148198010404]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527142398508121		[learning rate: 0.0016851]
	Learning Rate: 0.00168512
	LOSS [training: 3.527142398508121 | validation: 5.257006935704427]
	TIME [epoch: 10.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.567130789041216		[learning rate: 0.001679]
	Learning Rate: 0.00167901
	LOSS [training: 3.567130789041216 | validation: 5.27890463462197]
	TIME [epoch: 10.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5426993319071043		[learning rate: 0.0016729]
	Learning Rate: 0.00167291
	LOSS [training: 3.5426993319071043 | validation: 5.253408593478735]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_592.pth
	Model improved!!!
EPOCH 593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5245245298394634		[learning rate: 0.0016668]
	Learning Rate: 0.00166684
	LOSS [training: 3.5245245298394634 | validation: 5.282425831842629]
	TIME [epoch: 10.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5366362151960673		[learning rate: 0.0016608]
	Learning Rate: 0.00166079
	LOSS [training: 3.5366362151960673 | validation: 5.253171021385472]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5290918515575074		[learning rate: 0.0016548]
	Learning Rate: 0.00165477
	LOSS [training: 3.5290918515575074 | validation: 5.247099646663215]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.554984434769242		[learning rate: 0.0016488]
	Learning Rate: 0.00164876
	LOSS [training: 3.554984434769242 | validation: 5.2574929992476225]
	TIME [epoch: 10.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5729498962323647		[learning rate: 0.0016428]
	Learning Rate: 0.00164278
	LOSS [training: 3.5729498962323647 | validation: 5.288552824218814]
	TIME [epoch: 10.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5566446088954278		[learning rate: 0.0016368]
	Learning Rate: 0.00163682
	LOSS [training: 3.5566446088954278 | validation: 5.268868655214365]
	TIME [epoch: 10.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.533884747712426		[learning rate: 0.0016309]
	Learning Rate: 0.00163088
	LOSS [training: 3.533884747712426 | validation: 5.280856255945716]
	TIME [epoch: 10.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5417152906727907		[learning rate: 0.001625]
	Learning Rate: 0.00162496
	LOSS [training: 3.5417152906727907 | validation: 5.269398109267593]
	TIME [epoch: 10.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5405807296585814		[learning rate: 0.0016191]
	Learning Rate: 0.00161906
	LOSS [training: 3.5405807296585814 | validation: 5.2629032506416955]
	TIME [epoch: 10.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5271595584271096		[learning rate: 0.0016132]
	Learning Rate: 0.00161319
	LOSS [training: 3.5271595584271096 | validation: 5.26003854223666]
	TIME [epoch: 10.4 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.560513714980305		[learning rate: 0.0016073]
	Learning Rate: 0.00160733
	LOSS [training: 3.560513714980305 | validation: 5.50576211726939]
	TIME [epoch: 10.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.6245334494428088		[learning rate: 0.0016015]
	Learning Rate: 0.0016015
	LOSS [training: 3.6245334494428088 | validation: 5.2605499711606605]
	TIME [epoch: 10.4 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5266664667895404		[learning rate: 0.0015957]
	Learning Rate: 0.00159569
	LOSS [training: 3.5266664667895404 | validation: 5.272504845010019]
	TIME [epoch: 10.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515191171602236		[learning rate: 0.0015899]
	Learning Rate: 0.00158989
	LOSS [training: 3.515191171602236 | validation: 5.264184204464256]
	TIME [epoch: 10.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5217769496009863		[learning rate: 0.0015841]
	Learning Rate: 0.00158413
	LOSS [training: 3.5217769496009863 | validation: 5.2640278008356995]
	TIME [epoch: 10.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529749485045906		[learning rate: 0.0015784]
	Learning Rate: 0.00157838
	LOSS [training: 3.529749485045906 | validation: 5.281193717006906]
	TIME [epoch: 10.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5570572709847723		[learning rate: 0.0015726]
	Learning Rate: 0.00157265
	LOSS [training: 3.5570572709847723 | validation: 5.2774976693606845]
	TIME [epoch: 10.4 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520433461340482		[learning rate: 0.0015669]
	Learning Rate: 0.00156694
	LOSS [training: 3.520433461340482 | validation: 5.274244398812556]
	TIME [epoch: 10.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566897329927076		[learning rate: 0.0015613]
	Learning Rate: 0.00156125
	LOSS [training: 3.566897329927076 | validation: 5.264214695558642]
	TIME [epoch: 10.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527863600256058		[learning rate: 0.0015556]
	Learning Rate: 0.00155559
	LOSS [training: 3.527863600256058 | validation: 5.252221388681128]
	TIME [epoch: 10.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5450565280309982		[learning rate: 0.0015499]
	Learning Rate: 0.00154994
	LOSS [training: 3.5450565280309982 | validation: 5.300279369585532]
	TIME [epoch: 10.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.556371923849003		[learning rate: 0.0015443]
	Learning Rate: 0.00154432
	LOSS [training: 3.556371923849003 | validation: 5.294389813812862]
	TIME [epoch: 10.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5519412430377457		[learning rate: 0.0015387]
	Learning Rate: 0.00153871
	LOSS [training: 3.5519412430377457 | validation: 5.271924619880976]
	TIME [epoch: 10.4 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5360715153175		[learning rate: 0.0015331]
	Learning Rate: 0.00153313
	LOSS [training: 3.5360715153175 | validation: 5.254885015827974]
	TIME [epoch: 10.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524616138472801		[learning rate: 0.0015276]
	Learning Rate: 0.00152757
	LOSS [training: 3.524616138472801 | validation: 5.293160379975625]
	TIME [epoch: 10.4 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530871424825365		[learning rate: 0.001522]
	Learning Rate: 0.00152202
	LOSS [training: 3.530871424825365 | validation: 5.291246626372189]
	TIME [epoch: 10.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5364450020042826		[learning rate: 0.0015165]
	Learning Rate: 0.0015165
	LOSS [training: 3.5364450020042826 | validation: 5.285124636193502]
	TIME [epoch: 10.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5314740898848824		[learning rate: 0.001511]
	Learning Rate: 0.001511
	LOSS [training: 3.5314740898848824 | validation: 5.282306233423416]
	TIME [epoch: 10.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.591194070097893		[learning rate: 0.0015055]
	Learning Rate: 0.00150551
	LOSS [training: 3.591194070097893 | validation: 5.255487078481023]
	TIME [epoch: 10.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225866432538786		[learning rate: 0.0015]
	Learning Rate: 0.00150005
	LOSS [training: 3.5225866432538786 | validation: 5.318604669511256]
	TIME [epoch: 10.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541095511137523		[learning rate: 0.0014946]
	Learning Rate: 0.0014946
	LOSS [training: 3.541095511137523 | validation: 5.242190122641185]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5429771913216435		[learning rate: 0.0014892]
	Learning Rate: 0.00148918
	LOSS [training: 3.5429771913216435 | validation: 5.244273001932856]
	TIME [epoch: 10.4 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516917677219886		[learning rate: 0.0014838]
	Learning Rate: 0.00148378
	LOSS [training: 3.516917677219886 | validation: 5.263104747341436]
	TIME [epoch: 10.4 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5335152962506067		[learning rate: 0.0014784]
	Learning Rate: 0.00147839
	LOSS [training: 3.5335152962506067 | validation: 5.2670487468463545]
	TIME [epoch: 10.4 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5286130379578644		[learning rate: 0.001473]
	Learning Rate: 0.00147303
	LOSS [training: 3.5286130379578644 | validation: 5.287380392148102]
	TIME [epoch: 10.4 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541687146486003		[learning rate: 0.0014677]
	Learning Rate: 0.00146768
	LOSS [training: 3.541687146486003 | validation: 5.347252621122993]
	TIME [epoch: 10.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5649887618590315		[learning rate: 0.0014624]
	Learning Rate: 0.00146235
	LOSS [training: 3.5649887618590315 | validation: 5.271844150476437]
	TIME [epoch: 10.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5369325398366023		[learning rate: 0.001457]
	Learning Rate: 0.00145705
	LOSS [training: 3.5369325398366023 | validation: 5.244570096256848]
	TIME [epoch: 10.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5215762661983208		[learning rate: 0.0014518]
	Learning Rate: 0.00145176
	LOSS [training: 3.5215762661983208 | validation: 5.2550357487515225]
	TIME [epoch: 10.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525588558052891		[learning rate: 0.0014465]
	Learning Rate: 0.00144649
	LOSS [training: 3.525588558052891 | validation: 5.308336437343221]
	TIME [epoch: 10.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5390414658779066		[learning rate: 0.0014412]
	Learning Rate: 0.00144124
	LOSS [training: 3.5390414658779066 | validation: 5.266688467554925]
	TIME [epoch: 10.4 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5552903835324776		[learning rate: 0.001436]
	Learning Rate: 0.00143601
	LOSS [training: 3.5552903835324776 | validation: 5.247815844557006]
	TIME [epoch: 10.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.537808619306476		[learning rate: 0.0014308]
	Learning Rate: 0.0014308
	LOSS [training: 3.537808619306476 | validation: 5.266734759399251]
	TIME [epoch: 10.4 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.536174262691218		[learning rate: 0.0014256]
	Learning Rate: 0.00142561
	LOSS [training: 3.536174262691218 | validation: 5.294510865585551]
	TIME [epoch: 10.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527754763060055		[learning rate: 0.0014204]
	Learning Rate: 0.00142043
	LOSS [training: 3.527754763060055 | validation: 5.258046569841225]
	TIME [epoch: 10.4 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225278000421967		[learning rate: 0.0014153]
	Learning Rate: 0.00141528
	LOSS [training: 3.5225278000421967 | validation: 5.264040874275004]
	TIME [epoch: 10.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5402110708851695		[learning rate: 0.0014101]
	Learning Rate: 0.00141014
	LOSS [training: 3.5402110708851695 | validation: 5.314009192456219]
	TIME [epoch: 10.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5447501797148866		[learning rate: 0.001405]
	Learning Rate: 0.00140503
	LOSS [training: 3.5447501797148866 | validation: 5.248438330300389]
	TIME [epoch: 10.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5224142471891113		[learning rate: 0.0013999]
	Learning Rate: 0.00139993
	LOSS [training: 3.5224142471891113 | validation: 5.26434664442849]
	TIME [epoch: 10.4 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5217314696976403		[learning rate: 0.0013948]
	Learning Rate: 0.00139485
	LOSS [training: 3.5217314696976403 | validation: 5.2848023652007985]
	TIME [epoch: 10.4 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5241418361325296		[learning rate: 0.0013898]
	Learning Rate: 0.00138978
	LOSS [training: 3.5241418361325296 | validation: 5.263872286422952]
	TIME [epoch: 10.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530382641375133		[learning rate: 0.0013847]
	Learning Rate: 0.00138474
	LOSS [training: 3.530382641375133 | validation: 5.261222648417379]
	TIME [epoch: 10.4 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518153208899831		[learning rate: 0.0013797]
	Learning Rate: 0.00137972
	LOSS [training: 3.518153208899831 | validation: 5.264097876560981]
	TIME [epoch: 10.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5213967123743815		[learning rate: 0.0013747]
	Learning Rate: 0.00137471
	LOSS [training: 3.5213967123743815 | validation: 5.255292877237787]
	TIME [epoch: 10.4 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529492480684162		[learning rate: 0.0013697]
	Learning Rate: 0.00136972
	LOSS [training: 3.529492480684162 | validation: 5.261943020334857]
	TIME [epoch: 10.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.543665686369091		[learning rate: 0.0013647]
	Learning Rate: 0.00136475
	LOSS [training: 3.543665686369091 | validation: 5.269540747247199]
	TIME [epoch: 10.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5426609578441095		[learning rate: 0.0013598]
	Learning Rate: 0.0013598
	LOSS [training: 3.5426609578441095 | validation: 5.255740539126719]
	TIME [epoch: 10.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5517113674240135		[learning rate: 0.0013549]
	Learning Rate: 0.00135486
	LOSS [training: 3.5517113674240135 | validation: 5.282339163098145]
	TIME [epoch: 10.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5300137560343527		[learning rate: 0.0013499]
	Learning Rate: 0.00134994
	LOSS [training: 3.5300137560343527 | validation: 5.263236182858433]
	TIME [epoch: 10.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510638684876285		[learning rate: 0.001345]
	Learning Rate: 0.00134505
	LOSS [training: 3.510638684876285 | validation: 5.272109578655538]
	TIME [epoch: 10.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517892884718355		[learning rate: 0.0013402]
	Learning Rate: 0.00134016
	LOSS [training: 3.517892884718355 | validation: 5.266437920211493]
	TIME [epoch: 10.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5254344544955662		[learning rate: 0.0013353]
	Learning Rate: 0.0013353
	LOSS [training: 3.5254344544955662 | validation: 5.271477483561498]
	TIME [epoch: 10.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529832582829149		[learning rate: 0.0013305]
	Learning Rate: 0.00133045
	LOSS [training: 3.529832582829149 | validation: 5.26351966333159]
	TIME [epoch: 10.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5246067966988397		[learning rate: 0.0013256]
	Learning Rate: 0.00132563
	LOSS [training: 3.5246067966988397 | validation: 5.302782893209513]
	TIME [epoch: 10.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.566666836520188		[learning rate: 0.0013208]
	Learning Rate: 0.00132082
	LOSS [training: 3.566666836520188 | validation: 5.26647029499375]
	TIME [epoch: 10.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530619510168317		[learning rate: 0.001316]
	Learning Rate: 0.00131602
	LOSS [training: 3.530619510168317 | validation: 5.255412463246532]
	TIME [epoch: 10.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5508386327204904		[learning rate: 0.0013112]
	Learning Rate: 0.00131125
	LOSS [training: 3.5508386327204904 | validation: 5.279534033514257]
	TIME [epoch: 10.4 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5279135662199446		[learning rate: 0.0013065]
	Learning Rate: 0.00130649
	LOSS [training: 3.5279135662199446 | validation: 5.263805149699393]
	TIME [epoch: 10.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523666673511663		[learning rate: 0.0013017]
	Learning Rate: 0.00130175
	LOSS [training: 3.523666673511663 | validation: 5.265665866207858]
	TIME [epoch: 10.4 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223881389688936		[learning rate: 0.001297]
	Learning Rate: 0.00129702
	LOSS [training: 3.5223881389688936 | validation: 5.300556801107843]
	TIME [epoch: 10.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5530882837010322		[learning rate: 0.0012923]
	Learning Rate: 0.00129232
	LOSS [training: 3.5530882837010322 | validation: 5.2808420196365295]
	TIME [epoch: 10.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519483574236001		[learning rate: 0.0012876]
	Learning Rate: 0.00128763
	LOSS [training: 3.519483574236001 | validation: 5.24564963953989]
	TIME [epoch: 10.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5219503231443916		[learning rate: 0.001283]
	Learning Rate: 0.00128295
	LOSS [training: 3.5219503231443916 | validation: 5.274356537039314]
	TIME [epoch: 10.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5405212674977165		[learning rate: 0.0012783]
	Learning Rate: 0.0012783
	LOSS [training: 3.5405212674977165 | validation: 5.284242898212121]
	TIME [epoch: 10.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5819602811868188		[learning rate: 0.0012737]
	Learning Rate: 0.00127366
	LOSS [training: 3.5819602811868188 | validation: 5.2780109525055945]
	TIME [epoch: 10.4 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5294405182853623		[learning rate: 0.001269]
	Learning Rate: 0.00126904
	LOSS [training: 3.5294405182853623 | validation: 5.302332289621534]
	TIME [epoch: 10.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535203780693319		[learning rate: 0.0012644]
	Learning Rate: 0.00126443
	LOSS [training: 3.535203780693319 | validation: 5.274668549824299]
	TIME [epoch: 10.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5271667669105646		[learning rate: 0.0012598]
	Learning Rate: 0.00125984
	LOSS [training: 3.5271667669105646 | validation: 5.257259270014853]
	TIME [epoch: 10.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.533632512684389		[learning rate: 0.0012553]
	Learning Rate: 0.00125527
	LOSS [training: 3.533632512684389 | validation: 5.297259057611057]
	TIME [epoch: 10.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5543943450278115		[learning rate: 0.0012507]
	Learning Rate: 0.00125071
	LOSS [training: 3.5543943450278115 | validation: 5.263893295088153]
	TIME [epoch: 10.4 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5307228541788307		[learning rate: 0.0012462]
	Learning Rate: 0.00124617
	LOSS [training: 3.5307228541788307 | validation: 5.289574577326402]
	TIME [epoch: 10.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5381789320367303		[learning rate: 0.0012417]
	Learning Rate: 0.00124165
	LOSS [training: 3.5381789320367303 | validation: 5.321866787809973]
	TIME [epoch: 10.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5231626239396503		[learning rate: 0.0012371]
	Learning Rate: 0.00123715
	LOSS [training: 3.5231626239396503 | validation: 5.262859716680801]
	TIME [epoch: 10.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.543880910321304		[learning rate: 0.0012327]
	Learning Rate: 0.00123266
	LOSS [training: 3.543880910321304 | validation: 5.2744625391625695]
	TIME [epoch: 10.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5254615154558757		[learning rate: 0.0012282]
	Learning Rate: 0.00122818
	LOSS [training: 3.5254615154558757 | validation: 5.281472684434057]
	TIME [epoch: 10.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5293869716347492		[learning rate: 0.0012237]
	Learning Rate: 0.00122373
	LOSS [training: 3.5293869716347492 | validation: 5.254036648351696]
	TIME [epoch: 10.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5203057399498845		[learning rate: 0.0012193]
	Learning Rate: 0.00121929
	LOSS [training: 3.5203057399498845 | validation: 5.288638125921814]
	TIME [epoch: 10.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52243092377764		[learning rate: 0.0012149]
	Learning Rate: 0.00121486
	LOSS [training: 3.52243092377764 | validation: 5.319557100008279]
	TIME [epoch: 10.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529680198852816		[learning rate: 0.0012105]
	Learning Rate: 0.00121045
	LOSS [training: 3.529680198852816 | validation: 5.2750714294293175]
	TIME [epoch: 10.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521165885597475		[learning rate: 0.0012061]
	Learning Rate: 0.00120606
	LOSS [training: 3.521165885597475 | validation: 5.2718755811841165]
	TIME [epoch: 10.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5224472445722306		[learning rate: 0.0012017]
	Learning Rate: 0.00120168
	LOSS [training: 3.5224472445722306 | validation: 5.284502879889601]
	TIME [epoch: 10.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.535310331588427		[learning rate: 0.0011973]
	Learning Rate: 0.00119732
	LOSS [training: 3.535310331588427 | validation: 5.270857130299554]
	TIME [epoch: 10.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529086308042353		[learning rate: 0.001193]
	Learning Rate: 0.00119298
	LOSS [training: 3.529086308042353 | validation: 5.2742184347749905]
	TIME [epoch: 10.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5547833077436906		[learning rate: 0.0011886]
	Learning Rate: 0.00118865
	LOSS [training: 3.5547833077436906 | validation: 5.281154753949545]
	TIME [epoch: 10.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5277004493340263		[learning rate: 0.0011843]
	Learning Rate: 0.00118433
	LOSS [training: 3.5277004493340263 | validation: 5.265613235476094]
	TIME [epoch: 10.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515989044232902		[learning rate: 0.00118]
	Learning Rate: 0.00118003
	LOSS [training: 3.515989044232902 | validation: 5.24819414076554]
	TIME [epoch: 10.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51870025691017		[learning rate: 0.0011758]
	Learning Rate: 0.00117575
	LOSS [training: 3.51870025691017 | validation: 5.3048047703011685]
	TIME [epoch: 10.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5519578968079295		[learning rate: 0.0011715]
	Learning Rate: 0.00117149
	LOSS [training: 3.5519578968079295 | validation: 5.263513091986713]
	TIME [epoch: 10.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225063407161756		[learning rate: 0.0011672]
	Learning Rate: 0.00116723
	LOSS [training: 3.5225063407161756 | validation: 5.279783943999641]
	TIME [epoch: 10.4 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166796059349408		[learning rate: 0.001163]
	Learning Rate: 0.001163
	LOSS [training: 3.5166796059349408 | validation: 5.247519664864771]
	TIME [epoch: 10.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5072212382760704		[learning rate: 0.0011588]
	Learning Rate: 0.00115878
	LOSS [training: 3.5072212382760704 | validation: 5.272756225466938]
	TIME [epoch: 10.4 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52099545025034		[learning rate: 0.0011546]
	Learning Rate: 0.00115457
	LOSS [training: 3.52099545025034 | validation: 5.322410608749417]
	TIME [epoch: 10.4 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5374070248431364		[learning rate: 0.0011504]
	Learning Rate: 0.00115038
	LOSS [training: 3.5374070248431364 | validation: 5.253037958126272]
	TIME [epoch: 10.4 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5233904258945428		[learning rate: 0.0011462]
	Learning Rate: 0.00114621
	LOSS [training: 3.5233904258945428 | validation: 5.304858118860678]
	TIME [epoch: 10.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.537616142493885		[learning rate: 0.001142]
	Learning Rate: 0.00114205
	LOSS [training: 3.537616142493885 | validation: 5.317548105683304]
	TIME [epoch: 10.4 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5508826292914586		[learning rate: 0.0011379]
	Learning Rate: 0.0011379
	LOSS [training: 3.5508826292914586 | validation: 5.259248314607358]
	TIME [epoch: 10.4 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5315039683763714		[learning rate: 0.0011338]
	Learning Rate: 0.00113377
	LOSS [training: 3.5315039683763714 | validation: 5.263085428029501]
	TIME [epoch: 10.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522984012415537		[learning rate: 0.0011297]
	Learning Rate: 0.00112966
	LOSS [training: 3.522984012415537 | validation: 5.28990518038434]
	TIME [epoch: 10.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524221502571064		[learning rate: 0.0011256]
	Learning Rate: 0.00112556
	LOSS [training: 3.524221502571064 | validation: 5.260623005705661]
	TIME [epoch: 10.4 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5360998382268973		[learning rate: 0.0011215]
	Learning Rate: 0.00112147
	LOSS [training: 3.5360998382268973 | validation: 5.247555559807479]
	TIME [epoch: 10.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5104854979519162		[learning rate: 0.0011174]
	Learning Rate: 0.0011174
	LOSS [training: 3.5104854979519162 | validation: 5.26118585425538]
	TIME [epoch: 10.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529267203341239		[learning rate: 0.0011133]
	Learning Rate: 0.00111335
	LOSS [training: 3.529267203341239 | validation: 5.2551709007088645]
	TIME [epoch: 10.4 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527702981405677		[learning rate: 0.0011093]
	Learning Rate: 0.00110931
	LOSS [training: 3.527702981405677 | validation: 5.248598923346741]
	TIME [epoch: 10.4 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513779785734395		[learning rate: 0.0011053]
	Learning Rate: 0.00110528
	LOSS [training: 3.513779785734395 | validation: 5.264301295011271]
	TIME [epoch: 10.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5147086473617		[learning rate: 0.0011013]
	Learning Rate: 0.00110127
	LOSS [training: 3.5147086473617 | validation: 5.256900512769039]
	TIME [epoch: 10.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518659319015055		[learning rate: 0.0010973]
	Learning Rate: 0.00109728
	LOSS [training: 3.518659319015055 | validation: 5.293415153976983]
	TIME [epoch: 10.4 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5198588834795808		[learning rate: 0.0010933]
	Learning Rate: 0.00109329
	LOSS [training: 3.5198588834795808 | validation: 5.295626418754678]
	TIME [epoch: 10.4 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.54930328753377		[learning rate: 0.0010893]
	Learning Rate: 0.00108933
	LOSS [training: 3.54930328753377 | validation: 5.24937046213936]
	TIME [epoch: 10.4 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5211574067192144		[learning rate: 0.0010854]
	Learning Rate: 0.00108537
	LOSS [training: 3.5211574067192144 | validation: 5.28979160047154]
	TIME [epoch: 10.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.541973893457443		[learning rate: 0.0010814]
	Learning Rate: 0.00108143
	LOSS [training: 3.541973893457443 | validation: 5.263358397266143]
	TIME [epoch: 10.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5416636907959678		[learning rate: 0.0010775]
	Learning Rate: 0.00107751
	LOSS [training: 3.5416636907959678 | validation: 5.27127919761645]
	TIME [epoch: 10.4 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5274360659032515		[learning rate: 0.0010736]
	Learning Rate: 0.0010736
	LOSS [training: 3.5274360659032515 | validation: 5.261703241201433]
	TIME [epoch: 10.4 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519900034630859		[learning rate: 0.0010697]
	Learning Rate: 0.0010697
	LOSS [training: 3.519900034630859 | validation: 5.251580002263331]
	TIME [epoch: 10.4 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182535120066185		[learning rate: 0.0010658]
	Learning Rate: 0.00106582
	LOSS [training: 3.5182535120066185 | validation: 5.252065486825111]
	TIME [epoch: 10.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5279018175049304		[learning rate: 0.001062]
	Learning Rate: 0.00106195
	LOSS [training: 3.5279018175049304 | validation: 5.289424027781056]
	TIME [epoch: 10.4 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5432673574599454		[learning rate: 0.0010581]
	Learning Rate: 0.0010581
	LOSS [training: 3.5432673574599454 | validation: 5.264323756600841]
	TIME [epoch: 10.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5218305730717923		[learning rate: 0.0010543]
	Learning Rate: 0.00105426
	LOSS [training: 3.5218305730717923 | validation: 5.247817732497668]
	TIME [epoch: 10.4 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513296655681846		[learning rate: 0.0010504]
	Learning Rate: 0.00105043
	LOSS [training: 3.513296655681846 | validation: 5.259608628458996]
	TIME [epoch: 10.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515335143224482		[learning rate: 0.0010466]
	Learning Rate: 0.00104662
	LOSS [training: 3.515335143224482 | validation: 5.251503750067448]
	TIME [epoch: 10.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512712325164307		[learning rate: 0.0010428]
	Learning Rate: 0.00104282
	LOSS [training: 3.512712325164307 | validation: 5.2854818459524155]
	TIME [epoch: 10.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5377546090658045		[learning rate: 0.001039]
	Learning Rate: 0.00103904
	LOSS [training: 3.5377546090658045 | validation: 5.262515274580195]
	TIME [epoch: 10.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5256845912582238		[learning rate: 0.0010353]
	Learning Rate: 0.00103527
	LOSS [training: 3.5256845912582238 | validation: 5.273673062689397]
	TIME [epoch: 10.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.529471079798115		[learning rate: 0.0010315]
	Learning Rate: 0.00103151
	LOSS [training: 3.529471079798115 | validation: 5.277383081252606]
	TIME [epoch: 10.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188016303522494		[learning rate: 0.0010278]
	Learning Rate: 0.00102777
	LOSS [training: 3.5188016303522494 | validation: 5.255070408149456]
	TIME [epoch: 10.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517922547364028		[learning rate: 0.001024]
	Learning Rate: 0.00102404
	LOSS [training: 3.517922547364028 | validation: 5.268655925740315]
	TIME [epoch: 10.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202688929905532		[learning rate: 0.0010203]
	Learning Rate: 0.00102032
	LOSS [training: 3.5202688929905532 | validation: 5.258830422308097]
	TIME [epoch: 10.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514257895132173		[learning rate: 0.0010166]
	Learning Rate: 0.00101662
	LOSS [training: 3.514257895132173 | validation: 5.253031958126603]
	TIME [epoch: 10.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516491911067358		[learning rate: 0.0010129]
	Learning Rate: 0.00101293
	LOSS [training: 3.516491911067358 | validation: 5.2655545410227855]
	TIME [epoch: 10.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5331784807851427		[learning rate: 0.0010093]
	Learning Rate: 0.00100925
	LOSS [training: 3.5331784807851427 | validation: 5.248433899613761]
	TIME [epoch: 10.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5240581640000395		[learning rate: 0.0010056]
	Learning Rate: 0.00100559
	LOSS [training: 3.5240581640000395 | validation: 5.257066447287662]
	TIME [epoch: 10.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507125761029937		[learning rate: 0.0010019]
	Learning Rate: 0.00100194
	LOSS [training: 3.507125761029937 | validation: 5.254204811526117]
	TIME [epoch: 10.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190475058204838		[learning rate: 0.0009983]
	Learning Rate: 0.000998305
	LOSS [training: 3.5190475058204838 | validation: 5.274294906850748]
	TIME [epoch: 10.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5468232512988527		[learning rate: 0.00099468]
	Learning Rate: 0.000994682
	LOSS [training: 3.5468232512988527 | validation: 5.268427187861146]
	TIME [epoch: 10.4 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.544889731523136		[learning rate: 0.00099107]
	Learning Rate: 0.000991072
	LOSS [training: 3.544889731523136 | validation: 5.269573328949778]
	TIME [epoch: 10.4 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5063318713149725		[learning rate: 0.00098748]
	Learning Rate: 0.000987475
	LOSS [training: 3.5063318713149725 | validation: 5.253773920782701]
	TIME [epoch: 10.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190702723912524		[learning rate: 0.00098389]
	Learning Rate: 0.000983892
	LOSS [training: 3.5190702723912524 | validation: 5.248432517016961]
	TIME [epoch: 10.4 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514680218976088		[learning rate: 0.00098032]
	Learning Rate: 0.000980321
	LOSS [training: 3.514680218976088 | validation: 5.258178357611578]
	TIME [epoch: 10.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5155530533476904		[learning rate: 0.00097676]
	Learning Rate: 0.000976764
	LOSS [training: 3.5155530533476904 | validation: 5.283061554156916]
	TIME [epoch: 10.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5100009451597116		[learning rate: 0.00097322]
	Learning Rate: 0.000973219
	LOSS [training: 3.5100009451597116 | validation: 5.261525074646259]
	TIME [epoch: 10.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5165231535028973		[learning rate: 0.00096969]
	Learning Rate: 0.000969687
	LOSS [training: 3.5165231535028973 | validation: 5.2682623552520145]
	TIME [epoch: 10.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5269162619538124		[learning rate: 0.00096617]
	Learning Rate: 0.000966168
	LOSS [training: 3.5269162619538124 | validation: 5.318158131205873]
	TIME [epoch: 10.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5329179289209955		[learning rate: 0.00096266]
	Learning Rate: 0.000962662
	LOSS [training: 3.5329179289209955 | validation: 5.26991719339046]
	TIME [epoch: 10.4 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170943052895622		[learning rate: 0.00095917]
	Learning Rate: 0.000959168
	LOSS [training: 3.5170943052895622 | validation: 5.267714071727815]
	TIME [epoch: 10.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5195825224179247		[learning rate: 0.00095569]
	Learning Rate: 0.000955687
	LOSS [training: 3.5195825224179247 | validation: 5.255699392499001]
	TIME [epoch: 10.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5156343189449566		[learning rate: 0.00095222]
	Learning Rate: 0.000952219
	LOSS [training: 3.5156343189449566 | validation: 5.255098724732585]
	TIME [epoch: 10.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512054037162959		[learning rate: 0.00094876]
	Learning Rate: 0.000948763
	LOSS [training: 3.512054037162959 | validation: 5.256390597838353]
	TIME [epoch: 10.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512781659206084		[learning rate: 0.00094532]
	Learning Rate: 0.00094532
	LOSS [training: 3.512781659206084 | validation: 5.251549595616768]
	TIME [epoch: 10.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5255317983899737		[learning rate: 0.00094189]
	Learning Rate: 0.000941889
	LOSS [training: 3.5255317983899737 | validation: 5.308482708397453]
	TIME [epoch: 10.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.543539660216942		[learning rate: 0.00093847]
	Learning Rate: 0.000938471
	LOSS [training: 3.543539660216942 | validation: 5.2464951184868065]
	TIME [epoch: 10.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5166822201975316		[learning rate: 0.00093507]
	Learning Rate: 0.000935066
	LOSS [training: 3.5166822201975316 | validation: 5.25529525984633]
	TIME [epoch: 10.4 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51072628113889		[learning rate: 0.00093167]
	Learning Rate: 0.000931672
	LOSS [training: 3.51072628113889 | validation: 5.261757645595442]
	TIME [epoch: 10.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5125058300293537		[learning rate: 0.00092829]
	Learning Rate: 0.000928291
	LOSS [training: 3.5125058300293537 | validation: 5.270672420843529]
	TIME [epoch: 10.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520715792937781		[learning rate: 0.00092492]
	Learning Rate: 0.000924922
	LOSS [training: 3.520715792937781 | validation: 5.259417690676437]
	TIME [epoch: 10.4 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5099360031566973		[learning rate: 0.00092157]
	Learning Rate: 0.000921566
	LOSS [training: 3.5099360031566973 | validation: 5.2673548244102575]
	TIME [epoch: 10.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5188090070444753		[learning rate: 0.00091822]
	Learning Rate: 0.000918221
	LOSS [training: 3.5188090070444753 | validation: 5.256851427367334]
	TIME [epoch: 10.4 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5225551089059906		[learning rate: 0.00091489]
	Learning Rate: 0.000914889
	LOSS [training: 3.5225551089059906 | validation: 5.2672636212039725]
	TIME [epoch: 10.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5698503574551568		[learning rate: 0.00091157]
	Learning Rate: 0.000911569
	LOSS [training: 3.5698503574551568 | validation: 5.308878176132842]
	TIME [epoch: 10.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5397569412740735		[learning rate: 0.00090826]
	Learning Rate: 0.000908261
	LOSS [training: 3.5397569412740735 | validation: 5.244993785365205]
	TIME [epoch: 10.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522719401961262		[learning rate: 0.00090496]
	Learning Rate: 0.000904965
	LOSS [training: 3.522719401961262 | validation: 5.261035270517052]
	TIME [epoch: 10.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5065394538906993		[learning rate: 0.00090168]
	Learning Rate: 0.00090168
	LOSS [training: 3.5065394538906993 | validation: 5.262272905037398]
	TIME [epoch: 10.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5308881579834397		[learning rate: 0.00089841]
	Learning Rate: 0.000898408
	LOSS [training: 3.5308881579834397 | validation: 5.299458444788692]
	TIME [epoch: 10.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519753740632635		[learning rate: 0.00089515]
	Learning Rate: 0.000895148
	LOSS [training: 3.519753740632635 | validation: 5.2609593232628455]
	TIME [epoch: 10.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5154913732124613		[learning rate: 0.0008919]
	Learning Rate: 0.000891899
	LOSS [training: 3.5154913732124613 | validation: 5.265330434355298]
	TIME [epoch: 10.4 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5221814224352768		[learning rate: 0.00088866]
	Learning Rate: 0.000888663
	LOSS [training: 3.5221814224352768 | validation: 5.250050224038894]
	TIME [epoch: 10.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511200446888119		[learning rate: 0.00088544]
	Learning Rate: 0.000885438
	LOSS [training: 3.511200446888119 | validation: 5.268158212638368]
	TIME [epoch: 10.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5185670674142053		[learning rate: 0.00088222]
	Learning Rate: 0.000882224
	LOSS [training: 3.5185670674142053 | validation: 5.307703385975317]
	TIME [epoch: 10.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5317464194339983		[learning rate: 0.00087902]
	Learning Rate: 0.000879022
	LOSS [training: 3.5317464194339983 | validation: 5.268743893991773]
	TIME [epoch: 10.4 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5140196575591873		[learning rate: 0.00087583]
	Learning Rate: 0.000875833
	LOSS [training: 3.5140196575591873 | validation: 5.24637353319235]
	TIME [epoch: 10.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5671222119005948		[learning rate: 0.00087265]
	Learning Rate: 0.000872654
	LOSS [training: 3.5671222119005948 | validation: 5.2699861769724965]
	TIME [epoch: 10.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517178825131137		[learning rate: 0.00086949]
	Learning Rate: 0.000869487
	LOSS [training: 3.517178825131137 | validation: 5.253503034381088]
	TIME [epoch: 10.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5196946000054816		[learning rate: 0.00086633]
	Learning Rate: 0.000866332
	LOSS [training: 3.5196946000054816 | validation: 5.269451483489747]
	TIME [epoch: 10.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5283069807412644		[learning rate: 0.00086319]
	Learning Rate: 0.000863188
	LOSS [training: 3.5283069807412644 | validation: 5.2680045220530225]
	TIME [epoch: 10.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5367104364854143		[learning rate: 0.00086006]
	Learning Rate: 0.000860055
	LOSS [training: 3.5367104364854143 | validation: 5.261591845118903]
	TIME [epoch: 10.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50684152266978		[learning rate: 0.00085693]
	Learning Rate: 0.000856934
	LOSS [training: 3.50684152266978 | validation: 5.265569907115746]
	TIME [epoch: 10.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5239430343030422		[learning rate: 0.00085382]
	Learning Rate: 0.000853824
	LOSS [training: 3.5239430343030422 | validation: 5.257627690633523]
	TIME [epoch: 10.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517469176234016		[learning rate: 0.00085073]
	Learning Rate: 0.000850726
	LOSS [training: 3.517469176234016 | validation: 5.270158687738207]
	TIME [epoch: 10.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514225619189742		[learning rate: 0.00084764]
	Learning Rate: 0.000847638
	LOSS [training: 3.514225619189742 | validation: 5.274060447815116]
	TIME [epoch: 10.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5090959437836085		[learning rate: 0.00084456]
	Learning Rate: 0.000844562
	LOSS [training: 3.5090959437836085 | validation: 5.254751601304925]
	TIME [epoch: 10.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5224135769718288		[learning rate: 0.0008415]
	Learning Rate: 0.000841497
	LOSS [training: 3.5224135769718288 | validation: 5.26498535966133]
	TIME [epoch: 10.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524573196733069		[learning rate: 0.00083844]
	Learning Rate: 0.000838443
	LOSS [training: 3.524573196733069 | validation: 5.275681922860173]
	TIME [epoch: 10.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5470871853492447		[learning rate: 0.0008354]
	Learning Rate: 0.000835401
	LOSS [training: 3.5470871853492447 | validation: 5.286327074946936]
	TIME [epoch: 10.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518965138030537		[learning rate: 0.00083237]
	Learning Rate: 0.000832369
	LOSS [training: 3.518965138030537 | validation: 5.256161900851176]
	TIME [epoch: 10.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5177648005528077		[learning rate: 0.00082935]
	Learning Rate: 0.000829348
	LOSS [training: 3.5177648005528077 | validation: 5.255939362004003]
	TIME [epoch: 10.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523001414569984		[learning rate: 0.00082634]
	Learning Rate: 0.000826338
	LOSS [training: 3.523001414569984 | validation: 5.281760729760253]
	TIME [epoch: 10.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5169402901984617		[learning rate: 0.00082334]
	Learning Rate: 0.00082334
	LOSS [training: 3.5169402901984617 | validation: 5.2429577659031485]
	TIME [epoch: 10.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511730397678923		[learning rate: 0.00082035]
	Learning Rate: 0.000820352
	LOSS [training: 3.511730397678923 | validation: 5.257003403472354]
	TIME [epoch: 10.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5384903516127886		[learning rate: 0.00081737]
	Learning Rate: 0.000817375
	LOSS [training: 3.5384903516127886 | validation: 5.271750037926527]
	TIME [epoch: 10.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516835525532849		[learning rate: 0.00081441]
	Learning Rate: 0.000814408
	LOSS [training: 3.516835525532849 | validation: 5.255140617627197]
	TIME [epoch: 10.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5201665244842815		[learning rate: 0.00081145]
	Learning Rate: 0.000811453
	LOSS [training: 3.5201665244842815 | validation: 5.26190878385758]
	TIME [epoch: 10.4 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512496070949024		[learning rate: 0.00080851]
	Learning Rate: 0.000808508
	LOSS [training: 3.512496070949024 | validation: 5.252384347680491]
	TIME [epoch: 10.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504700064356239		[learning rate: 0.00080557]
	Learning Rate: 0.000805574
	LOSS [training: 3.504700064356239 | validation: 5.25142438024745]
	TIME [epoch: 10.4 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50632355958793		[learning rate: 0.00080265]
	Learning Rate: 0.00080265
	LOSS [training: 3.50632355958793 | validation: 5.2869880747549285]
	TIME [epoch: 10.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5170499473246126		[learning rate: 0.00079974]
	Learning Rate: 0.000799737
	LOSS [training: 3.5170499473246126 | validation: 5.243436414978122]
	TIME [epoch: 10.4 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5207555137930084		[learning rate: 0.00079684]
	Learning Rate: 0.000796835
	LOSS [training: 3.5207555137930084 | validation: 5.257084389707183]
	TIME [epoch: 10.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5267268534544476		[learning rate: 0.00079394]
	Learning Rate: 0.000793943
	LOSS [training: 3.5267268534544476 | validation: 5.2639293277117325]
	TIME [epoch: 10.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182104650704353		[learning rate: 0.00079106]
	Learning Rate: 0.000791062
	LOSS [training: 3.5182104650704353 | validation: 5.255483115691503]
	TIME [epoch: 10.4 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5141032504751566		[learning rate: 0.00078819]
	Learning Rate: 0.000788191
	LOSS [training: 3.5141032504751566 | validation: 5.276093968945609]
	TIME [epoch: 10.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512030357387048		[learning rate: 0.00078533]
	Learning Rate: 0.000785331
	LOSS [training: 3.512030357387048 | validation: 5.246328212483627]
	TIME [epoch: 10.4 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511672467749565		[learning rate: 0.00078248]
	Learning Rate: 0.000782481
	LOSS [training: 3.511672467749565 | validation: 5.250943785907407]
	TIME [epoch: 10.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223719527997397		[learning rate: 0.00077964]
	Learning Rate: 0.000779641
	LOSS [training: 3.5223719527997397 | validation: 5.294305969838486]
	TIME [epoch: 10.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5182193806554842		[learning rate: 0.00077681]
	Learning Rate: 0.000776812
	LOSS [training: 3.5182193806554842 | validation: 5.256358354979032]
	TIME [epoch: 10.4 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5067085619253966		[learning rate: 0.00077399]
	Learning Rate: 0.000773993
	LOSS [training: 3.5067085619253966 | validation: 5.258788580435927]
	TIME [epoch: 10.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5318943564110348		[learning rate: 0.00077118]
	Learning Rate: 0.000771184
	LOSS [training: 3.5318943564110348 | validation: 5.262039605108632]
	TIME [epoch: 10.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5092656440041323		[learning rate: 0.00076839]
	Learning Rate: 0.000768385
	LOSS [training: 3.5092656440041323 | validation: 5.24891094494388]
	TIME [epoch: 10.4 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5236492100665173		[learning rate: 0.0007656]
	Learning Rate: 0.000765597
	LOSS [training: 3.5236492100665173 | validation: 5.261133895460919]
	TIME [epoch: 10.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5198571620367156		[learning rate: 0.00076282]
	Learning Rate: 0.000762818
	LOSS [training: 3.5198571620367156 | validation: 5.265270014390668]
	TIME [epoch: 10.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5104628888698897		[learning rate: 0.00076005]
	Learning Rate: 0.00076005
	LOSS [training: 3.5104628888698897 | validation: 5.250482736174827]
	TIME [epoch: 10.4 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505939872604506		[learning rate: 0.00075729]
	Learning Rate: 0.000757292
	LOSS [training: 3.505939872604506 | validation: 5.26266392444448]
	TIME [epoch: 10.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142132887300206		[learning rate: 0.00075454]
	Learning Rate: 0.000754543
	LOSS [training: 3.5142132887300206 | validation: 5.250655205651767]
	TIME [epoch: 10.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508241182077196		[learning rate: 0.00075181]
	Learning Rate: 0.000751805
	LOSS [training: 3.508241182077196 | validation: 5.249997468830453]
	TIME [epoch: 10.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.534711140021257		[learning rate: 0.00074908]
	Learning Rate: 0.000749077
	LOSS [training: 3.534711140021257 | validation: 5.254739294366637]
	TIME [epoch: 10.4 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516678539413154		[learning rate: 0.00074636]
	Learning Rate: 0.000746358
	LOSS [training: 3.516678539413154 | validation: 5.2442346698900915]
	TIME [epoch: 10.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149234976256998		[learning rate: 0.00074365]
	Learning Rate: 0.00074365
	LOSS [training: 3.5149234976256998 | validation: 5.269762900102826]
	TIME [epoch: 10.4 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5110507412467564		[learning rate: 0.00074095]
	Learning Rate: 0.000740951
	LOSS [training: 3.5110507412467564 | validation: 5.279970340120322]
	TIME [epoch: 10.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.540883047121669		[learning rate: 0.00073826]
	Learning Rate: 0.000738262
	LOSS [training: 3.540883047121669 | validation: 5.259093525883759]
	TIME [epoch: 10.4 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5096191383099793		[learning rate: 0.00073558]
	Learning Rate: 0.000735583
	LOSS [training: 3.5096191383099793 | validation: 5.245253595746276]
	TIME [epoch: 10.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.520725468169476		[learning rate: 0.00073291]
	Learning Rate: 0.000732913
	LOSS [training: 3.520725468169476 | validation: 5.263762818687854]
	TIME [epoch: 10.4 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5238298856852297		[learning rate: 0.00073025]
	Learning Rate: 0.000730254
	LOSS [training: 3.5238298856852297 | validation: 5.242060036115131]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5405541528737836		[learning rate: 0.0007276]
	Learning Rate: 0.000727603
	LOSS [training: 3.5405541528737836 | validation: 5.255207698532452]
	TIME [epoch: 10.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507799531001642		[learning rate: 0.00072496]
	Learning Rate: 0.000724963
	LOSS [training: 3.507799531001642 | validation: 5.246152132099528]
	TIME [epoch: 10.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501602587016724		[learning rate: 0.00072233]
	Learning Rate: 0.000722332
	LOSS [training: 3.501602587016724 | validation: 5.289085858327008]
	TIME [epoch: 10.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5741533582737377		[learning rate: 0.00071971]
	Learning Rate: 0.000719711
	LOSS [training: 3.5741533582737377 | validation: 5.2708064525884355]
	TIME [epoch: 10.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.522959135131032		[learning rate: 0.0007171]
	Learning Rate: 0.000717099
	LOSS [training: 3.522959135131032 | validation: 5.243963555660675]
	TIME [epoch: 10.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519440451956239		[learning rate: 0.0007145]
	Learning Rate: 0.000714496
	LOSS [training: 3.519440451956239 | validation: 5.270206661920437]
	TIME [epoch: 10.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5145941941422776		[learning rate: 0.0007119]
	Learning Rate: 0.000711903
	LOSS [training: 3.5145941941422776 | validation: 5.252229219778339]
	TIME [epoch: 10.4 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.523512156162276		[learning rate: 0.00070932]
	Learning Rate: 0.00070932
	LOSS [training: 3.523512156162276 | validation: 5.278237266079677]
	TIME [epoch: 10.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513485787031331		[learning rate: 0.00070675]
	Learning Rate: 0.000706746
	LOSS [training: 3.513485787031331 | validation: 5.270484208704852]
	TIME [epoch: 10.4 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505707639102917		[learning rate: 0.00070418]
	Learning Rate: 0.000704181
	LOSS [training: 3.505707639102917 | validation: 5.25567916472895]
	TIME [epoch: 10.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5024561757924717		[learning rate: 0.00070163]
	Learning Rate: 0.000701625
	LOSS [training: 3.5024561757924717 | validation: 5.2585139086749235]
	TIME [epoch: 10.4 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.550548016381673		[learning rate: 0.00069908]
	Learning Rate: 0.000699079
	LOSS [training: 3.550548016381673 | validation: 5.30331337315264]
	TIME [epoch: 10.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5305847910996384		[learning rate: 0.00069654]
	Learning Rate: 0.000696542
	LOSS [training: 3.5305847910996384 | validation: 5.2521874671397715]
	TIME [epoch: 10.4 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509918646229644		[learning rate: 0.00069401]
	Learning Rate: 0.000694014
	LOSS [training: 3.509918646229644 | validation: 5.249413671813705]
	TIME [epoch: 10.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50612635651627		[learning rate: 0.0006915]
	Learning Rate: 0.000691496
	LOSS [training: 3.50612635651627 | validation: 5.253805875949275]
	TIME [epoch: 10.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5105506366121384		[learning rate: 0.00068899]
	Learning Rate: 0.000688986
	LOSS [training: 3.5105506366121384 | validation: 5.259581420369953]
	TIME [epoch: 10.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5135744934058737		[learning rate: 0.00068649]
	Learning Rate: 0.000686486
	LOSS [training: 3.5135744934058737 | validation: 5.261810925224036]
	TIME [epoch: 10.4 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5039624975606274		[learning rate: 0.00068399]
	Learning Rate: 0.000683994
	LOSS [training: 3.5039624975606274 | validation: 5.25342245684247]
	TIME [epoch: 10.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521672645867606		[learning rate: 0.00068151]
	Learning Rate: 0.000681512
	LOSS [training: 3.521672645867606 | validation: 5.24507866924951]
	TIME [epoch: 10.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5052019560694334		[learning rate: 0.00067904]
	Learning Rate: 0.000679039
	LOSS [training: 3.5052019560694334 | validation: 5.271775841014458]
	TIME [epoch: 10.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.524229361289		[learning rate: 0.00067657]
	Learning Rate: 0.000676575
	LOSS [training: 3.524229361289 | validation: 5.251858303069411]
	TIME [epoch: 10.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503942071244522		[learning rate: 0.00067412]
	Learning Rate: 0.00067412
	LOSS [training: 3.503942071244522 | validation: 5.2421802314207655]
	TIME [epoch: 10.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5040404066634947		[learning rate: 0.00067167]
	Learning Rate: 0.000671673
	LOSS [training: 3.5040404066634947 | validation: 5.267073107637898]
	TIME [epoch: 10.4 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5172546953720163		[learning rate: 0.00066924]
	Learning Rate: 0.000669235
	LOSS [training: 3.5172546953720163 | validation: 5.2479313561954255]
	TIME [epoch: 10.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508670149940717		[learning rate: 0.00066681]
	Learning Rate: 0.000666807
	LOSS [training: 3.508670149940717 | validation: 5.239231251419069]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510122929581494		[learning rate: 0.00066439]
	Learning Rate: 0.000664387
	LOSS [training: 3.510122929581494 | validation: 5.268681318701559]
	TIME [epoch: 10.4 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5122624534421902		[learning rate: 0.00066198]
	Learning Rate: 0.000661976
	LOSS [training: 3.5122624534421902 | validation: 5.258186860249703]
	TIME [epoch: 10.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5129629936293023		[learning rate: 0.00065957]
	Learning Rate: 0.000659573
	LOSS [training: 3.5129629936293023 | validation: 5.257404818317846]
	TIME [epoch: 10.4 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518121793337587		[learning rate: 0.00065718]
	Learning Rate: 0.00065718
	LOSS [training: 3.518121793337587 | validation: 5.267340259243928]
	TIME [epoch: 10.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5163061120530243		[learning rate: 0.00065479]
	Learning Rate: 0.000654795
	LOSS [training: 3.5163061120530243 | validation: 5.258767778964613]
	TIME [epoch: 10.4 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5223146025304977		[learning rate: 0.00065242]
	Learning Rate: 0.000652419
	LOSS [training: 3.5223146025304977 | validation: 5.256531980261619]
	TIME [epoch: 10.4 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503236440426055		[learning rate: 0.00065005]
	Learning Rate: 0.000650051
	LOSS [training: 3.503236440426055 | validation: 5.272245500885233]
	TIME [epoch: 10.4 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171466206674977		[learning rate: 0.00064769]
	Learning Rate: 0.000647692
	LOSS [training: 3.5171466206674977 | validation: 5.267908089220799]
	TIME [epoch: 10.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.530664018282561		[learning rate: 0.00064534]
	Learning Rate: 0.000645341
	LOSS [training: 3.530664018282561 | validation: 5.324183050013055]
	TIME [epoch: 10.4 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.552300664689109		[learning rate: 0.000643]
	Learning Rate: 0.000642999
	LOSS [training: 3.552300664689109 | validation: 5.2592006818662576]
	TIME [epoch: 10.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504286587330025		[learning rate: 0.00064067]
	Learning Rate: 0.000640666
	LOSS [training: 3.504286587330025 | validation: 5.257017054878989]
	TIME [epoch: 10.4 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5071238309036543		[learning rate: 0.00063834]
	Learning Rate: 0.000638341
	LOSS [training: 3.5071238309036543 | validation: 5.256294105814884]
	TIME [epoch: 10.4 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5108497969057026		[learning rate: 0.00063602]
	Learning Rate: 0.000636024
	LOSS [training: 3.5108497969057026 | validation: 5.256261435655692]
	TIME [epoch: 10.4 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501331171133171		[learning rate: 0.00063372]
	Learning Rate: 0.000633716
	LOSS [training: 3.501331171133171 | validation: 5.2542806017459975]
	TIME [epoch: 10.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5070051149885146		[learning rate: 0.00063142]
	Learning Rate: 0.000631416
	LOSS [training: 3.5070051149885146 | validation: 5.250324487001667]
	TIME [epoch: 10.4 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500812872344509		[learning rate: 0.00062912]
	Learning Rate: 0.000629125
	LOSS [training: 3.500812872344509 | validation: 5.250108093043449]
	TIME [epoch: 10.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4994372437977064		[learning rate: 0.00062684]
	Learning Rate: 0.000626842
	LOSS [training: 3.4994372437977064 | validation: 5.248784802646535]
	TIME [epoch: 10.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5080484904759173		[learning rate: 0.00062457]
	Learning Rate: 0.000624567
	LOSS [training: 3.5080484904759173 | validation: 5.246869853760102]
	TIME [epoch: 10.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5046867964601693		[learning rate: 0.0006223]
	Learning Rate: 0.0006223
	LOSS [training: 3.5046867964601693 | validation: 5.25555774576015]
	TIME [epoch: 10.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5095396524202016		[learning rate: 0.00062004]
	Learning Rate: 0.000620042
	LOSS [training: 3.5095396524202016 | validation: 5.264623856794956]
	TIME [epoch: 10.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5167399653435525		[learning rate: 0.00061779]
	Learning Rate: 0.000617792
	LOSS [training: 3.5167399653435525 | validation: 5.257612811417173]
	TIME [epoch: 10.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5090625716983297		[learning rate: 0.00061555]
	Learning Rate: 0.00061555
	LOSS [training: 3.5090625716983297 | validation: 5.243983237274597]
	TIME [epoch: 10.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500591576316296		[learning rate: 0.00061332]
	Learning Rate: 0.000613316
	LOSS [training: 3.500591576316296 | validation: 5.258024678353243]
	TIME [epoch: 10.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5116904131051188		[learning rate: 0.00061109]
	Learning Rate: 0.00061109
	LOSS [training: 3.5116904131051188 | validation: 5.264309316864168]
	TIME [epoch: 10.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513792005248251		[learning rate: 0.00060887]
	Learning Rate: 0.000608872
	LOSS [training: 3.513792005248251 | validation: 5.247674177107412]
	TIME [epoch: 10.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5175106978171717		[learning rate: 0.00060666]
	Learning Rate: 0.000606663
	LOSS [training: 3.5175106978171717 | validation: 5.2626533759363]
	TIME [epoch: 10.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517900043823752		[learning rate: 0.00060446]
	Learning Rate: 0.000604461
	LOSS [training: 3.517900043823752 | validation: 5.2904376573841585]
	TIME [epoch: 10.4 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521860040738647		[learning rate: 0.00060227]
	Learning Rate: 0.000602268
	LOSS [training: 3.521860040738647 | validation: 5.263630843033264]
	TIME [epoch: 10.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5072979943923692		[learning rate: 0.00060008]
	Learning Rate: 0.000600082
	LOSS [training: 3.5072979943923692 | validation: 5.251244846607488]
	TIME [epoch: 10.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508819781739328		[learning rate: 0.0005979]
	Learning Rate: 0.000597904
	LOSS [training: 3.508819781739328 | validation: 5.256551000551367]
	TIME [epoch: 10.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517521179453625		[learning rate: 0.00059573]
	Learning Rate: 0.000595734
	LOSS [training: 3.517521179453625 | validation: 5.258893845272407]
	TIME [epoch: 10.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516085324380515		[learning rate: 0.00059357]
	Learning Rate: 0.000593572
	LOSS [training: 3.516085324380515 | validation: 5.260480820471613]
	TIME [epoch: 10.4 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5233313492956775		[learning rate: 0.00059142]
	Learning Rate: 0.000591418
	LOSS [training: 3.5233313492956775 | validation: 5.248443324932532]
	TIME [epoch: 10.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52264702793594		[learning rate: 0.00058927]
	Learning Rate: 0.000589272
	LOSS [training: 3.52264702793594 | validation: 5.25662031916962]
	TIME [epoch: 10.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512173305516602		[learning rate: 0.00058713]
	Learning Rate: 0.000587133
	LOSS [training: 3.512173305516602 | validation: 5.25886139921367]
	TIME [epoch: 10.4 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.516308359085383		[learning rate: 0.000585]
	Learning Rate: 0.000585003
	LOSS [training: 3.516308359085383 | validation: 5.25061719784216]
	TIME [epoch: 10.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5034771103448987		[learning rate: 0.00058288]
	Learning Rate: 0.00058288
	LOSS [training: 3.5034771103448987 | validation: 5.251155277754133]
	TIME [epoch: 10.4 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5073926277150678		[learning rate: 0.00058076]
	Learning Rate: 0.000580764
	LOSS [training: 3.5073926277150678 | validation: 5.264544805131972]
	TIME [epoch: 10.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153641525002683		[learning rate: 0.00057866]
	Learning Rate: 0.000578657
	LOSS [training: 3.5153641525002683 | validation: 5.266499675613647]
	TIME [epoch: 10.4 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5153976752236034		[learning rate: 0.00057656]
	Learning Rate: 0.000576557
	LOSS [training: 3.5153976752236034 | validation: 5.256802367649917]
	TIME [epoch: 10.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5202087144948293		[learning rate: 0.00057446]
	Learning Rate: 0.000574465
	LOSS [training: 3.5202087144948293 | validation: 5.2698569149632295]
	TIME [epoch: 10.4 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513883999727086		[learning rate: 0.00057238]
	Learning Rate: 0.00057238
	LOSS [training: 3.513883999727086 | validation: 5.259103202899448]
	TIME [epoch: 10.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.506197497305422		[learning rate: 0.0005703]
	Learning Rate: 0.000570303
	LOSS [training: 3.506197497305422 | validation: 5.240815002807619]
	TIME [epoch: 10.4 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5083670354198473		[learning rate: 0.00056823]
	Learning Rate: 0.000568233
	LOSS [training: 3.5083670354198473 | validation: 5.248901107528951]
	TIME [epoch: 10.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5151397678539906		[learning rate: 0.00056617]
	Learning Rate: 0.000566171
	LOSS [training: 3.5151397678539906 | validation: 5.255173491967514]
	TIME [epoch: 10.4 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510738809070015		[learning rate: 0.00056412]
	Learning Rate: 0.000564116
	LOSS [training: 3.510738809070015 | validation: 5.252409160342865]
	TIME [epoch: 10.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5063946767945913		[learning rate: 0.00056207]
	Learning Rate: 0.000562069
	LOSS [training: 3.5063946767945913 | validation: 5.252854070870024]
	TIME [epoch: 10.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503077288979763		[learning rate: 0.00056003]
	Learning Rate: 0.000560029
	LOSS [training: 3.503077288979763 | validation: 5.257385945977664]
	TIME [epoch: 10.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5032437537747945		[learning rate: 0.000558]
	Learning Rate: 0.000557997
	LOSS [training: 3.5032437537747945 | validation: 5.270077241956321]
	TIME [epoch: 10.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5109061037854064		[learning rate: 0.00055597]
	Learning Rate: 0.000555972
	LOSS [training: 3.5109061037854064 | validation: 5.2628743332577415]
	TIME [epoch: 10.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512624970348989		[learning rate: 0.00055395]
	Learning Rate: 0.000553954
	LOSS [training: 3.512624970348989 | validation: 5.250572902270667]
	TIME [epoch: 10.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510659987225586		[learning rate: 0.00055194]
	Learning Rate: 0.000551944
	LOSS [training: 3.510659987225586 | validation: 5.253669993974195]
	TIME [epoch: 10.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5081880306519224		[learning rate: 0.00054994]
	Learning Rate: 0.000549941
	LOSS [training: 3.5081880306519224 | validation: 5.25482405398101]
	TIME [epoch: 10.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.515398688443465		[learning rate: 0.00054794]
	Learning Rate: 0.000547945
	LOSS [training: 3.515398688443465 | validation: 5.240958341703515]
	TIME [epoch: 10.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503520992963827		[learning rate: 0.00054596]
	Learning Rate: 0.000545956
	LOSS [training: 3.503520992963827 | validation: 5.243441899201305]
	TIME [epoch: 10.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504255347884		[learning rate: 0.00054397]
	Learning Rate: 0.000543975
	LOSS [training: 3.504255347884 | validation: 5.258965748397275]
	TIME [epoch: 10.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5103515261002274		[learning rate: 0.000542]
	Learning Rate: 0.000542001
	LOSS [training: 3.5103515261002274 | validation: 5.2506691963070535]
	TIME [epoch: 10.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521515429077732		[learning rate: 0.00054003]
	Learning Rate: 0.000540034
	LOSS [training: 3.521515429077732 | validation: 5.258124248361206]
	TIME [epoch: 10.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5119015036023384		[learning rate: 0.00053807]
	Learning Rate: 0.000538074
	LOSS [training: 3.5119015036023384 | validation: 5.252589912178092]
	TIME [epoch: 10.4 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509874155144253		[learning rate: 0.00053612]
	Learning Rate: 0.000536121
	LOSS [training: 3.509874155144253 | validation: 5.265537862411122]
	TIME [epoch: 10.4 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5069040247026004		[learning rate: 0.00053418]
	Learning Rate: 0.000534176
	LOSS [training: 3.5069040247026004 | validation: 5.2656855611087865]
	TIME [epoch: 10.4 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5054643457632095		[learning rate: 0.00053224]
	Learning Rate: 0.000532237
	LOSS [training: 3.5054643457632095 | validation: 5.255434891745436]
	TIME [epoch: 10.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5056766140669544		[learning rate: 0.00053031]
	Learning Rate: 0.000530306
	LOSS [training: 3.5056766140669544 | validation: 5.242170886324122]
	TIME [epoch: 10.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5081666103823466		[learning rate: 0.00052838]
	Learning Rate: 0.000528381
	LOSS [training: 3.5081666103823466 | validation: 5.240820725352307]
	TIME [epoch: 10.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508508557978215		[learning rate: 0.00052646]
	Learning Rate: 0.000526464
	LOSS [training: 3.508508557978215 | validation: 5.256441769220312]
	TIME [epoch: 10.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5126436302048027		[learning rate: 0.00052455]
	Learning Rate: 0.000524553
	LOSS [training: 3.5126436302048027 | validation: 5.2586096127295985]
	TIME [epoch: 10.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5127830715419863		[learning rate: 0.00052265]
	Learning Rate: 0.000522649
	LOSS [training: 3.5127830715419863 | validation: 5.276334118451792]
	TIME [epoch: 10.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.514647603567125		[learning rate: 0.00052075]
	Learning Rate: 0.000520753
	LOSS [training: 3.514647603567125 | validation: 5.246435635286255]
	TIME [epoch: 10.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.526845706498291		[learning rate: 0.00051886]
	Learning Rate: 0.000518863
	LOSS [training: 3.526845706498291 | validation: 5.252099660749111]
	TIME [epoch: 10.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508304175001956		[learning rate: 0.00051698]
	Learning Rate: 0.00051698
	LOSS [training: 3.508304175001956 | validation: 5.252046799825287]
	TIME [epoch: 10.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.52304782042449		[learning rate: 0.0005151]
	Learning Rate: 0.000515104
	LOSS [training: 3.52304782042449 | validation: 5.266212747653974]
	TIME [epoch: 10.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518591609483395		[learning rate: 0.00051323]
	Learning Rate: 0.000513235
	LOSS [training: 3.518591609483395 | validation: 5.244521104811841]
	TIME [epoch: 10.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504022167754324		[learning rate: 0.00051137]
	Learning Rate: 0.000511372
	LOSS [training: 3.504022167754324 | validation: 5.260216398435046]
	TIME [epoch: 10.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511795005732613		[learning rate: 0.00050952]
	Learning Rate: 0.000509516
	LOSS [training: 3.511795005732613 | validation: 5.281443261532604]
	TIME [epoch: 10.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5266809719392		[learning rate: 0.00050767]
	Learning Rate: 0.000507667
	LOSS [training: 3.5266809719392 | validation: 5.255617376018515]
	TIME [epoch: 10.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518855836085712		[learning rate: 0.00050582]
	Learning Rate: 0.000505825
	LOSS [training: 3.518855836085712 | validation: 5.244966717001075]
	TIME [epoch: 10.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5019697534198237		[learning rate: 0.00050399]
	Learning Rate: 0.000503989
	LOSS [training: 3.5019697534198237 | validation: 5.263738841562122]
	TIME [epoch: 10.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512175632062454		[learning rate: 0.00050216]
	Learning Rate: 0.00050216
	LOSS [training: 3.512175632062454 | validation: 5.249567337067576]
	TIME [epoch: 10.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510394490559288		[learning rate: 0.00050034]
	Learning Rate: 0.000500338
	LOSS [training: 3.510394490559288 | validation: 5.2477476860238355]
	TIME [epoch: 10.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507225472056132		[learning rate: 0.00049852]
	Learning Rate: 0.000498522
	LOSS [training: 3.507225472056132 | validation: 5.282585051263466]
	TIME [epoch: 10.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5194996378938312		[learning rate: 0.00049671]
	Learning Rate: 0.000496713
	LOSS [training: 3.5194996378938312 | validation: 5.246960044701626]
	TIME [epoch: 10.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149780522623515		[learning rate: 0.00049491]
	Learning Rate: 0.00049491
	LOSS [training: 3.5149780522623515 | validation: 5.26169079415151]
	TIME [epoch: 10.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5149427517946776		[learning rate: 0.00049311]
	Learning Rate: 0.000493114
	LOSS [training: 3.5149427517946776 | validation: 5.277602788240036]
	TIME [epoch: 10.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.521458179776836		[learning rate: 0.00049132]
	Learning Rate: 0.000491325
	LOSS [training: 3.521458179776836 | validation: 5.259227529990137]
	TIME [epoch: 10.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5085559779521787		[learning rate: 0.00048954]
	Learning Rate: 0.000489542
	LOSS [training: 3.5085559779521787 | validation: 5.254249985480408]
	TIME [epoch: 10.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.525059946470054		[learning rate: 0.00048776]
	Learning Rate: 0.000487765
	LOSS [training: 3.525059946470054 | validation: 5.249847373753607]
	TIME [epoch: 10.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.506412155121957		[learning rate: 0.00048599]
	Learning Rate: 0.000485995
	LOSS [training: 3.506412155121957 | validation: 5.255897674845801]
	TIME [epoch: 10.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513860895153916		[learning rate: 0.00048423]
	Learning Rate: 0.000484231
	LOSS [training: 3.513860895153916 | validation: 5.28107482214862]
	TIME [epoch: 10.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510456285120912		[learning rate: 0.00048247]
	Learning Rate: 0.000482474
	LOSS [training: 3.510456285120912 | validation: 5.243532461014557]
	TIME [epoch: 10.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507658869149779		[learning rate: 0.00048072]
	Learning Rate: 0.000480723
	LOSS [training: 3.507658869149779 | validation: 5.250248015210933]
	TIME [epoch: 10.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503130403458666		[learning rate: 0.00047898]
	Learning Rate: 0.000478978
	LOSS [training: 3.503130403458666 | validation: 5.245843665457888]
	TIME [epoch: 10.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5016840160158886		[learning rate: 0.00047724]
	Learning Rate: 0.00047724
	LOSS [training: 3.5016840160158886 | validation: 5.247061794446836]
	TIME [epoch: 10.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5217504766719094		[learning rate: 0.00047551]
	Learning Rate: 0.000475508
	LOSS [training: 3.5217504766719094 | validation: 5.2558911854365284]
	TIME [epoch: 10.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5017428045262724		[learning rate: 0.00047378]
	Learning Rate: 0.000473782
	LOSS [training: 3.5017428045262724 | validation: 5.248182893595897]
	TIME [epoch: 10.4 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510598034968747		[learning rate: 0.00047206]
	Learning Rate: 0.000472063
	LOSS [training: 3.510598034968747 | validation: 5.252062501928858]
	TIME [epoch: 10.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5105125891473463		[learning rate: 0.00047035]
	Learning Rate: 0.00047035
	LOSS [training: 3.5105125891473463 | validation: 5.250254474083106]
	TIME [epoch: 10.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513454191053408		[learning rate: 0.00046864]
	Learning Rate: 0.000468643
	LOSS [training: 3.513454191053408 | validation: 5.250022291618493]
	TIME [epoch: 10.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511558830267495		[learning rate: 0.00046694]
	Learning Rate: 0.000466942
	LOSS [training: 3.511558830267495 | validation: 5.2371082147985035]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5105006603151345		[learning rate: 0.00046525]
	Learning Rate: 0.000465248
	LOSS [training: 3.5105006603151345 | validation: 5.251260072153152]
	TIME [epoch: 10.4 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5012682801866313		[learning rate: 0.00046356]
	Learning Rate: 0.000463559
	LOSS [training: 3.5012682801866313 | validation: 5.241834488018615]
	TIME [epoch: 10.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5245246359636027		[learning rate: 0.00046188]
	Learning Rate: 0.000461877
	LOSS [training: 3.5245246359636027 | validation: 5.263708833988021]
	TIME [epoch: 10.4 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5033682712430028		[learning rate: 0.0004602]
	Learning Rate: 0.000460201
	LOSS [training: 3.5033682712430028 | validation: 5.241803613516441]
	TIME [epoch: 10.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5029883058723343		[learning rate: 0.00045853]
	Learning Rate: 0.000458531
	LOSS [training: 3.5029883058723343 | validation: 5.258531037071358]
	TIME [epoch: 10.4 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507607607287487		[learning rate: 0.00045687]
	Learning Rate: 0.000456867
	LOSS [training: 3.507607607287487 | validation: 5.254377647461033]
	TIME [epoch: 10.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503918483768193		[learning rate: 0.00045521]
	Learning Rate: 0.000455209
	LOSS [training: 3.503918483768193 | validation: 5.251189482138834]
	TIME [epoch: 10.4 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4986863836511475		[learning rate: 0.00045356]
	Learning Rate: 0.000453557
	LOSS [training: 3.4986863836511475 | validation: 5.237999535243719]
	TIME [epoch: 10.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5039337615353703		[learning rate: 0.00045191]
	Learning Rate: 0.000451911
	LOSS [training: 3.5039337615353703 | validation: 5.25129029577635]
	TIME [epoch: 10.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507252982950976		[learning rate: 0.00045027]
	Learning Rate: 0.000450271
	LOSS [training: 3.507252982950976 | validation: 5.2542413334116045]
	TIME [epoch: 10.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5009934997256513		[learning rate: 0.00044864]
	Learning Rate: 0.000448637
	LOSS [training: 3.5009934997256513 | validation: 5.24307701475251]
	TIME [epoch: 10.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5097917141222887		[learning rate: 0.00044701]
	Learning Rate: 0.000447009
	LOSS [training: 3.5097917141222887 | validation: 5.25362211667992]
	TIME [epoch: 10.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509003490130843		[learning rate: 0.00044539]
	Learning Rate: 0.000445386
	LOSS [training: 3.509003490130843 | validation: 5.241848525672359]
	TIME [epoch: 10.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510466980649455		[learning rate: 0.00044377]
	Learning Rate: 0.00044377
	LOSS [training: 3.510466980649455 | validation: 5.262848303229072]
	TIME [epoch: 10.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505716862373606		[learning rate: 0.00044216]
	Learning Rate: 0.00044216
	LOSS [training: 3.505716862373606 | validation: 5.267614351764371]
	TIME [epoch: 10.4 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509258686886956		[learning rate: 0.00044055]
	Learning Rate: 0.000440555
	LOSS [training: 3.509258686886956 | validation: 5.2443674545753804]
	TIME [epoch: 10.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.517843924364519		[learning rate: 0.00043896]
	Learning Rate: 0.000438956
	LOSS [training: 3.517843924364519 | validation: 5.245334493362159]
	TIME [epoch: 10.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50251193040008		[learning rate: 0.00043736]
	Learning Rate: 0.000437363
	LOSS [training: 3.50251193040008 | validation: 5.245528979089109]
	TIME [epoch: 10.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5062327048990434		[learning rate: 0.00043578]
	Learning Rate: 0.000435776
	LOSS [training: 3.5062327048990434 | validation: 5.263278655265712]
	TIME [epoch: 10.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5013342033897006		[learning rate: 0.00043419]
	Learning Rate: 0.000434194
	LOSS [training: 3.5013342033897006 | validation: 5.244906382635146]
	TIME [epoch: 10.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507123918933002		[learning rate: 0.00043262]
	Learning Rate: 0.000432619
	LOSS [training: 3.507123918933002 | validation: 5.254208967215454]
	TIME [epoch: 10.4 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51282384526517		[learning rate: 0.00043105]
	Learning Rate: 0.000431049
	LOSS [training: 3.51282384526517 | validation: 5.269560085360967]
	TIME [epoch: 10.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505380645466738		[learning rate: 0.00042948]
	Learning Rate: 0.000429484
	LOSS [training: 3.505380645466738 | validation: 5.26091277837427]
	TIME [epoch: 10.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504707569526226		[learning rate: 0.00042793]
	Learning Rate: 0.000427926
	LOSS [training: 3.504707569526226 | validation: 5.249103387914146]
	TIME [epoch: 10.4 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5128553869606263		[learning rate: 0.00042637]
	Learning Rate: 0.000426373
	LOSS [training: 3.5128553869606263 | validation: 5.250942437590119]
	TIME [epoch: 10.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5069588470171325		[learning rate: 0.00042483]
	Learning Rate: 0.000424825
	LOSS [training: 3.5069588470171325 | validation: 5.291808992952154]
	TIME [epoch: 10.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5371346050702512		[learning rate: 0.00042328]
	Learning Rate: 0.000423284
	LOSS [training: 3.5371346050702512 | validation: 5.255765087061099]
	TIME [epoch: 10.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5033624035395023		[learning rate: 0.00042175]
	Learning Rate: 0.000421748
	LOSS [training: 3.5033624035395023 | validation: 5.245021112217422]
	TIME [epoch: 10.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504337879629457		[learning rate: 0.00042022]
	Learning Rate: 0.000420217
	LOSS [training: 3.504337879629457 | validation: 5.2535095117091375]
	TIME [epoch: 10.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5062787047417343		[learning rate: 0.00041869]
	Learning Rate: 0.000418692
	LOSS [training: 3.5062787047417343 | validation: 5.249363152600357]
	TIME [epoch: 10.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5055379256317645		[learning rate: 0.00041717]
	Learning Rate: 0.000417173
	LOSS [training: 3.5055379256317645 | validation: 5.246342966092741]
	TIME [epoch: 10.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5164155189513253		[learning rate: 0.00041566]
	Learning Rate: 0.000415659
	LOSS [training: 3.5164155189513253 | validation: 5.249166334310878]
	TIME [epoch: 10.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.511915360356216		[learning rate: 0.00041415]
	Learning Rate: 0.00041415
	LOSS [training: 3.511915360356216 | validation: 5.256094466434838]
	TIME [epoch: 10.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5076718024918163		[learning rate: 0.00041265]
	Learning Rate: 0.000412647
	LOSS [training: 3.5076718024918163 | validation: 5.246929793845764]
	TIME [epoch: 10.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5082371468808224		[learning rate: 0.00041115]
	Learning Rate: 0.00041115
	LOSS [training: 3.5082371468808224 | validation: 5.252630144038447]
	TIME [epoch: 10.4 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5079883312934683		[learning rate: 0.00040966]
	Learning Rate: 0.000409658
	LOSS [training: 3.5079883312934683 | validation: 5.268459094729114]
	TIME [epoch: 10.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5262083104800817		[learning rate: 0.00040817]
	Learning Rate: 0.000408171
	LOSS [training: 3.5262083104800817 | validation: 5.2546590036998]
	TIME [epoch: 10.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5025711268808792		[learning rate: 0.00040669]
	Learning Rate: 0.00040669
	LOSS [training: 3.5025711268808792 | validation: 5.252832053428693]
	TIME [epoch: 10.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5094197019018907		[learning rate: 0.00040521]
	Learning Rate: 0.000405214
	LOSS [training: 3.5094197019018907 | validation: 5.241340474770222]
	TIME [epoch: 10.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.54729410992183		[learning rate: 0.00040374]
	Learning Rate: 0.000403743
	LOSS [training: 3.54729410992183 | validation: 5.277435318114972]
	TIME [epoch: 10.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5206978135279186		[learning rate: 0.00040228]
	Learning Rate: 0.000402278
	LOSS [training: 3.5206978135279186 | validation: 5.262319263036504]
	TIME [epoch: 10.4 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5065197565518367		[learning rate: 0.00040082]
	Learning Rate: 0.000400818
	LOSS [training: 3.5065197565518367 | validation: 5.264484539345453]
	TIME [epoch: 10.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5140021646205417		[learning rate: 0.00039936]
	Learning Rate: 0.000399364
	LOSS [training: 3.5140021646205417 | validation: 5.2487682245158105]
	TIME [epoch: 10.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508975597612978		[learning rate: 0.00039791]
	Learning Rate: 0.000397914
	LOSS [training: 3.508975597612978 | validation: 5.240882483254134]
	TIME [epoch: 10.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5013394451214794		[learning rate: 0.00039647]
	Learning Rate: 0.00039647
	LOSS [training: 3.5013394451214794 | validation: 5.251251483895201]
	TIME [epoch: 10.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5063476589953737		[learning rate: 0.00039503]
	Learning Rate: 0.000395031
	LOSS [training: 3.5063476589953737 | validation: 5.24236057621994]
	TIME [epoch: 10.4 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5088272766837227		[learning rate: 0.0003936]
	Learning Rate: 0.000393598
	LOSS [training: 3.5088272766837227 | validation: 5.279953091160585]
	TIME [epoch: 10.4 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5250410225946505		[learning rate: 0.00039217]
	Learning Rate: 0.000392169
	LOSS [training: 3.5250410225946505 | validation: 5.254373395136537]
	TIME [epoch: 10.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5070325409960916		[learning rate: 0.00039075]
	Learning Rate: 0.000390746
	LOSS [training: 3.5070325409960916 | validation: 5.256226901142418]
	TIME [epoch: 10.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507014966055184		[learning rate: 0.00038933]
	Learning Rate: 0.000389328
	LOSS [training: 3.507014966055184 | validation: 5.260421949797692]
	TIME [epoch: 10.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5023707149133587		[learning rate: 0.00038792]
	Learning Rate: 0.000387915
	LOSS [training: 3.5023707149133587 | validation: 5.247651701424891]
	TIME [epoch: 10.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4981868482264993		[learning rate: 0.00038651]
	Learning Rate: 0.000386508
	LOSS [training: 3.4981868482264993 | validation: 5.235773897579788]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5012840635486113		[learning rate: 0.0003851]
	Learning Rate: 0.000385105
	LOSS [training: 3.5012840635486113 | validation: 5.2489202791049]
	TIME [epoch: 10.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5028930973394514		[learning rate: 0.00038371]
	Learning Rate: 0.000383707
	LOSS [training: 3.5028930973394514 | validation: 5.25245263985556]
	TIME [epoch: 10.4 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5064641906367777		[learning rate: 0.00038231]
	Learning Rate: 0.000382315
	LOSS [training: 3.5064641906367777 | validation: 5.276868454508713]
	TIME [epoch: 10.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5190958820135037		[learning rate: 0.00038093]
	Learning Rate: 0.000380927
	LOSS [training: 3.5190958820135037 | validation: 5.251289985130732]
	TIME [epoch: 10.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507699262909303		[learning rate: 0.00037954]
	Learning Rate: 0.000379545
	LOSS [training: 3.507699262909303 | validation: 5.249355111916291]
	TIME [epoch: 10.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5054081630452245		[learning rate: 0.00037817]
	Learning Rate: 0.000378167
	LOSS [training: 3.5054081630452245 | validation: 5.237085193341889]
	TIME [epoch: 10.4 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49985328420579		[learning rate: 0.0003768]
	Learning Rate: 0.000376795
	LOSS [training: 3.49985328420579 | validation: 5.250009099041101]
	TIME [epoch: 10.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504595240748017		[learning rate: 0.00037543]
	Learning Rate: 0.000375428
	LOSS [training: 3.504595240748017 | validation: 5.2524242895714615]
	TIME [epoch: 10.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509298858900286		[learning rate: 0.00037407]
	Learning Rate: 0.000374065
	LOSS [training: 3.509298858900286 | validation: 5.269871852411136]
	TIME [epoch: 10.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5091391724795145		[learning rate: 0.00037271]
	Learning Rate: 0.000372708
	LOSS [training: 3.5091391724795145 | validation: 5.247392548417744]
	TIME [epoch: 10.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500774233624193		[learning rate: 0.00037136]
	Learning Rate: 0.000371355
	LOSS [training: 3.500774233624193 | validation: 5.25532450506351]
	TIME [epoch: 10.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5096423638703294		[learning rate: 0.00037001]
	Learning Rate: 0.000370008
	LOSS [training: 3.5096423638703294 | validation: 5.253247672184599]
	TIME [epoch: 10.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5064972198913424		[learning rate: 0.00036866]
	Learning Rate: 0.000368665
	LOSS [training: 3.5064972198913424 | validation: 5.2497008173428075]
	TIME [epoch: 10.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500942974265142		[learning rate: 0.00036733]
	Learning Rate: 0.000367327
	LOSS [training: 3.500942974265142 | validation: 5.253629336119777]
	TIME [epoch: 10.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508861232303327		[learning rate: 0.00036599]
	Learning Rate: 0.000365994
	LOSS [training: 3.508861232303327 | validation: 5.245427774496284]
	TIME [epoch: 10.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510611727091807		[learning rate: 0.00036467]
	Learning Rate: 0.000364666
	LOSS [training: 3.510611727091807 | validation: 5.25636548707342]
	TIME [epoch: 10.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501738847970033		[learning rate: 0.00036334]
	Learning Rate: 0.000363342
	LOSS [training: 3.501738847970033 | validation: 5.267324921249574]
	TIME [epoch: 10.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5099043169910074		[learning rate: 0.00036202]
	Learning Rate: 0.000362024
	LOSS [training: 3.5099043169910074 | validation: 5.255353489109678]
	TIME [epoch: 10.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.512550682558655		[learning rate: 0.00036071]
	Learning Rate: 0.00036071
	LOSS [training: 3.512550682558655 | validation: 5.233118524171326]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_1014.pth
	Model improved!!!
EPOCH 1015/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503761218121093		[learning rate: 0.0003594]
	Learning Rate: 0.000359401
	LOSS [training: 3.503761218121093 | validation: 5.260337145335893]
	TIME [epoch: 10.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4988083051274876		[learning rate: 0.0003581]
	Learning Rate: 0.000358096
	LOSS [training: 3.4988083051274876 | validation: 5.252667579568111]
	TIME [epoch: 10.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5032877832374374		[learning rate: 0.0003568]
	Learning Rate: 0.000356797
	LOSS [training: 3.5032877832374374 | validation: 5.249936518273891]
	TIME [epoch: 10.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5098941424210017		[learning rate: 0.0003555]
	Learning Rate: 0.000355502
	LOSS [training: 3.5098941424210017 | validation: 5.253944883850474]
	TIME [epoch: 10.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507551330231821		[learning rate: 0.00035421]
	Learning Rate: 0.000354212
	LOSS [training: 3.507551330231821 | validation: 5.25099719342029]
	TIME [epoch: 10.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503942447602693		[learning rate: 0.00035293]
	Learning Rate: 0.000352926
	LOSS [training: 3.503942447602693 | validation: 5.266058203435769]
	TIME [epoch: 10.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5124705932046174		[learning rate: 0.00035165]
	Learning Rate: 0.000351646
	LOSS [training: 3.5124705932046174 | validation: 5.267303142593798]
	TIME [epoch: 10.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.513839725552214		[learning rate: 0.00035037]
	Learning Rate: 0.00035037
	LOSS [training: 3.513839725552214 | validation: 5.248499019098438]
	TIME [epoch: 10.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504904228801763		[learning rate: 0.0003491]
	Learning Rate: 0.000349098
	LOSS [training: 3.504904228801763 | validation: 5.256796286757358]
	TIME [epoch: 10.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505089960226411		[learning rate: 0.00034783]
	Learning Rate: 0.000347831
	LOSS [training: 3.505089960226411 | validation: 5.2481034514741625]
	TIME [epoch: 10.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5007790307139346		[learning rate: 0.00034657]
	Learning Rate: 0.000346569
	LOSS [training: 3.5007790307139346 | validation: 5.249877775431384]
	TIME [epoch: 10.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5010112438707686		[learning rate: 0.00034531]
	Learning Rate: 0.000345311
	LOSS [training: 3.5010112438707686 | validation: 5.253252274880315]
	TIME [epoch: 10.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5030866627393875		[learning rate: 0.00034406]
	Learning Rate: 0.000344058
	LOSS [training: 3.5030866627393875 | validation: 5.261774934170208]
	TIME [epoch: 10.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.510310392009255		[learning rate: 0.00034281]
	Learning Rate: 0.000342809
	LOSS [training: 3.510310392009255 | validation: 5.249603285128922]
	TIME [epoch: 10.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138909308195467		[learning rate: 0.00034157]
	Learning Rate: 0.000341565
	LOSS [training: 3.5138909308195467 | validation: 5.243404932439836]
	TIME [epoch: 10.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4994549451922325		[learning rate: 0.00034033]
	Learning Rate: 0.000340326
	LOSS [training: 3.4994549451922325 | validation: 5.25722119173296]
	TIME [epoch: 10.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5023025244409745		[learning rate: 0.00033909]
	Learning Rate: 0.000339091
	LOSS [training: 3.5023025244409745 | validation: 5.260244239641711]
	TIME [epoch: 10.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501516711607058		[learning rate: 0.00033786]
	Learning Rate: 0.00033786
	LOSS [training: 3.501516711607058 | validation: 5.252339320380706]
	TIME [epoch: 10.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503318965183193		[learning rate: 0.00033663]
	Learning Rate: 0.000336634
	LOSS [training: 3.503318965183193 | validation: 5.2508971864400635]
	TIME [epoch: 10.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.51026190094884		[learning rate: 0.00033541]
	Learning Rate: 0.000335412
	LOSS [training: 3.51026190094884 | validation: 5.237910096834121]
	TIME [epoch: 10.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5124074613464082		[learning rate: 0.0003342]
	Learning Rate: 0.000334195
	LOSS [training: 3.5124074613464082 | validation: 5.255044553637439]
	TIME [epoch: 10.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505454855386249		[learning rate: 0.00033298]
	Learning Rate: 0.000332982
	LOSS [training: 3.505454855386249 | validation: 5.258113917596652]
	TIME [epoch: 10.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5103959074353774		[learning rate: 0.00033177]
	Learning Rate: 0.000331774
	LOSS [training: 3.5103959074353774 | validation: 5.262282935341761]
	TIME [epoch: 10.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5073439095975685		[learning rate: 0.00033057]
	Learning Rate: 0.00033057
	LOSS [training: 3.5073439095975685 | validation: 5.256422853094537]
	TIME [epoch: 10.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5142541413223207		[learning rate: 0.00032937]
	Learning Rate: 0.00032937
	LOSS [training: 3.5142541413223207 | validation: 5.2527415271225895]
	TIME [epoch: 10.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5023941918229795		[learning rate: 0.00032817]
	Learning Rate: 0.000328175
	LOSS [training: 3.5023941918229795 | validation: 5.242034411137677]
	TIME [epoch: 10.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5065364683965754		[learning rate: 0.00032698]
	Learning Rate: 0.000326984
	LOSS [training: 3.5065364683965754 | validation: 5.244374412338743]
	TIME [epoch: 10.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5029011696111434		[learning rate: 0.0003258]
	Learning Rate: 0.000325797
	LOSS [training: 3.5029011696111434 | validation: 5.240392114703399]
	TIME [epoch: 10.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503896703645662		[learning rate: 0.00032461]
	Learning Rate: 0.000324615
	LOSS [training: 3.503896703645662 | validation: 5.251822314601611]
	TIME [epoch: 10.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5119215285832697		[learning rate: 0.00032344]
	Learning Rate: 0.000323437
	LOSS [training: 3.5119215285832697 | validation: 5.258978154174806]
	TIME [epoch: 10.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5067048543325017		[learning rate: 0.00032226]
	Learning Rate: 0.000322263
	LOSS [training: 3.5067048543325017 | validation: 5.251013579518122]
	TIME [epoch: 10.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5062965450673844		[learning rate: 0.00032109]
	Learning Rate: 0.000321094
	LOSS [training: 3.5062965450673844 | validation: 5.249583244514699]
	TIME [epoch: 10.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498999060576361		[learning rate: 0.00031993]
	Learning Rate: 0.000319928
	LOSS [training: 3.498999060576361 | validation: 5.237894323318954]
	TIME [epoch: 10.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500135459007333		[learning rate: 0.00031877]
	Learning Rate: 0.000318767
	LOSS [training: 3.500135459007333 | validation: 5.256759263700538]
	TIME [epoch: 10.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5026586359692145		[learning rate: 0.00031761]
	Learning Rate: 0.000317611
	LOSS [training: 3.5026586359692145 | validation: 5.255988038584975]
	TIME [epoch: 10.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502621210731604		[learning rate: 0.00031646]
	Learning Rate: 0.000316458
	LOSS [training: 3.502621210731604 | validation: 5.251939711689198]
	TIME [epoch: 10.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5100160284766444		[learning rate: 0.00031531]
	Learning Rate: 0.000315309
	LOSS [training: 3.5100160284766444 | validation: 5.246486354468935]
	TIME [epoch: 10.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503692380493926		[learning rate: 0.00031417]
	Learning Rate: 0.000314165
	LOSS [training: 3.503692380493926 | validation: 5.248355835662045]
	TIME [epoch: 10.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503093345524276		[learning rate: 0.00031302]
	Learning Rate: 0.000313025
	LOSS [training: 3.503093345524276 | validation: 5.261871786013098]
	TIME [epoch: 10.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5100665460280496		[learning rate: 0.00031189]
	Learning Rate: 0.000311889
	LOSS [training: 3.5100665460280496 | validation: 5.248097633526363]
	TIME [epoch: 10.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504537547085687		[learning rate: 0.00031076]
	Learning Rate: 0.000310757
	LOSS [training: 3.504537547085687 | validation: 5.2383624366087655]
	TIME [epoch: 10.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501595667799267		[learning rate: 0.00030963]
	Learning Rate: 0.000309629
	LOSS [training: 3.501595667799267 | validation: 5.247892122011125]
	TIME [epoch: 10.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5034383530931024		[learning rate: 0.00030851]
	Learning Rate: 0.000308506
	LOSS [training: 3.5034383530931024 | validation: 5.253319955163483]
	TIME [epoch: 10.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5038315574904133		[learning rate: 0.00030739]
	Learning Rate: 0.000307386
	LOSS [training: 3.5038315574904133 | validation: 5.261229839675722]
	TIME [epoch: 10.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5003982149238624		[learning rate: 0.00030627]
	Learning Rate: 0.000306271
	LOSS [training: 3.5003982149238624 | validation: 5.251510193044549]
	TIME [epoch: 10.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501261630207735		[learning rate: 0.00030516]
	Learning Rate: 0.000305159
	LOSS [training: 3.501261630207735 | validation: 5.240413558261069]
	TIME [epoch: 10.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505579375076985		[learning rate: 0.00030405]
	Learning Rate: 0.000304052
	LOSS [training: 3.505579375076985 | validation: 5.257426212390628]
	TIME [epoch: 10.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500164581335917		[learning rate: 0.00030295]
	Learning Rate: 0.000302948
	LOSS [training: 3.500164581335917 | validation: 5.23912000597461]
	TIME [epoch: 10.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499632689817065		[learning rate: 0.00030185]
	Learning Rate: 0.000301849
	LOSS [training: 3.499632689817065 | validation: 5.262230738910131]
	TIME [epoch: 10.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5138501437332925		[learning rate: 0.00030075]
	Learning Rate: 0.000300753
	LOSS [training: 3.5138501437332925 | validation: 5.248801636007752]
	TIME [epoch: 10.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509296503085983		[learning rate: 0.00029966]
	Learning Rate: 0.000299662
	LOSS [training: 3.509296503085983 | validation: 5.2605313719606785]
	TIME [epoch: 10.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5106181021007643		[learning rate: 0.00029857]
	Learning Rate: 0.000298574
	LOSS [training: 3.5106181021007643 | validation: 5.238154688829594]
	TIME [epoch: 10.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5008473699597764		[learning rate: 0.00029749]
	Learning Rate: 0.000297491
	LOSS [training: 3.5008473699597764 | validation: 5.239257921430652]
	TIME [epoch: 10.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498600202686		[learning rate: 0.00029641]
	Learning Rate: 0.000296411
	LOSS [training: 3.498600202686 | validation: 5.251945760890039]
	TIME [epoch: 10.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5069706862661265		[learning rate: 0.00029534]
	Learning Rate: 0.000295336
	LOSS [training: 3.5069706862661265 | validation: 5.240742082196373]
	TIME [epoch: 10.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502004595399974		[learning rate: 0.00029426]
	Learning Rate: 0.000294264
	LOSS [training: 3.502004595399974 | validation: 5.242731015955972]
	TIME [epoch: 10.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5058394215833375		[learning rate: 0.0002932]
	Learning Rate: 0.000293196
	LOSS [training: 3.5058394215833375 | validation: 5.236201168135418]
	TIME [epoch: 10.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.527596428333016		[learning rate: 0.00029213]
	Learning Rate: 0.000292132
	LOSS [training: 3.527596428333016 | validation: 5.273573738577991]
	TIME [epoch: 10.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5201933211983096		[learning rate: 0.00029107]
	Learning Rate: 0.000291072
	LOSS [training: 3.5201933211983096 | validation: 5.247744733081618]
	TIME [epoch: 10.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5101984093368914		[learning rate: 0.00029002]
	Learning Rate: 0.000290015
	LOSS [training: 3.5101984093368914 | validation: 5.255365103909819]
	TIME [epoch: 10.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.506474272562877		[learning rate: 0.00028896]
	Learning Rate: 0.000288963
	LOSS [training: 3.506474272562877 | validation: 5.254161935589788]
	TIME [epoch: 10.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5040311428532775		[learning rate: 0.00028791]
	Learning Rate: 0.000287914
	LOSS [training: 3.5040311428532775 | validation: 5.240543885388352]
	TIME [epoch: 10.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5036888451452044		[learning rate: 0.00028687]
	Learning Rate: 0.000286869
	LOSS [training: 3.5036888451452044 | validation: 5.2335329460550915]
	TIME [epoch: 10.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497369360524489		[learning rate: 0.00028583]
	Learning Rate: 0.000285828
	LOSS [training: 3.497369360524489 | validation: 5.251178241970761]
	TIME [epoch: 10.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999521352427685		[learning rate: 0.00028479]
	Learning Rate: 0.000284791
	LOSS [training: 3.4999521352427685 | validation: 5.250871965880681]
	TIME [epoch: 10.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5030440089921187		[learning rate: 0.00028376]
	Learning Rate: 0.000283758
	LOSS [training: 3.5030440089921187 | validation: 5.233450630424255]
	TIME [epoch: 10.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5019831303570306		[learning rate: 0.00028273]
	Learning Rate: 0.000282728
	LOSS [training: 3.5019831303570306 | validation: 5.260986158314396]
	TIME [epoch: 10.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504948213229679		[learning rate: 0.0002817]
	Learning Rate: 0.000281702
	LOSS [training: 3.504948213229679 | validation: 5.246003990532037]
	TIME [epoch: 10.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5046517280844647		[learning rate: 0.00028068]
	Learning Rate: 0.000280679
	LOSS [training: 3.5046517280844647 | validation: 5.234512074785775]
	TIME [epoch: 10.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499276269765663		[learning rate: 0.00027966]
	Learning Rate: 0.000279661
	LOSS [training: 3.499276269765663 | validation: 5.268832860470809]
	TIME [epoch: 10.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501827183719391		[learning rate: 0.00027865]
	Learning Rate: 0.000278646
	LOSS [training: 3.501827183719391 | validation: 5.242983681304442]
	TIME [epoch: 10.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501997437125142		[learning rate: 0.00027763]
	Learning Rate: 0.000277635
	LOSS [training: 3.501997437125142 | validation: 5.252573240844194]
	TIME [epoch: 10.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5035458654091207		[learning rate: 0.00027663]
	Learning Rate: 0.000276627
	LOSS [training: 3.5035458654091207 | validation: 5.2489145135918145]
	TIME [epoch: 10.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500545862344346		[learning rate: 0.00027562]
	Learning Rate: 0.000275623
	LOSS [training: 3.500545862344346 | validation: 5.248759041477659]
	TIME [epoch: 10.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5005926270092416		[learning rate: 0.00027462]
	Learning Rate: 0.000274623
	LOSS [training: 3.5005926270092416 | validation: 5.244730604444497]
	TIME [epoch: 10.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5007199928879826		[learning rate: 0.00027363]
	Learning Rate: 0.000273626
	LOSS [training: 3.5007199928879826 | validation: 5.243380800377245]
	TIME [epoch: 10.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5011821271432795		[learning rate: 0.00027263]
	Learning Rate: 0.000272633
	LOSS [training: 3.5011821271432795 | validation: 5.247673773408795]
	TIME [epoch: 10.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5001153520719996		[learning rate: 0.00027164]
	Learning Rate: 0.000271644
	LOSS [training: 3.5001153520719996 | validation: 5.264855653449871]
	TIME [epoch: 10.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5055105795018533		[learning rate: 0.00027066]
	Learning Rate: 0.000270658
	LOSS [training: 3.5055105795018533 | validation: 5.2473503258601015]
	TIME [epoch: 10.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5034451012762324		[learning rate: 0.00026968]
	Learning Rate: 0.000269676
	LOSS [training: 3.5034451012762324 | validation: 5.2464411969848195]
	TIME [epoch: 10.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50646229724403		[learning rate: 0.0002687]
	Learning Rate: 0.000268697
	LOSS [training: 3.50646229724403 | validation: 5.241747036832435]
	TIME [epoch: 10.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5018454704458835		[learning rate: 0.00026772]
	Learning Rate: 0.000267722
	LOSS [training: 3.5018454704458835 | validation: 5.243035276496808]
	TIME [epoch: 10.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991285233924065		[learning rate: 0.00026675]
	Learning Rate: 0.000266751
	LOSS [training: 3.4991285233924065 | validation: 5.248604839866882]
	TIME [epoch: 10.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507908494722939		[learning rate: 0.00026578]
	Learning Rate: 0.000265782
	LOSS [training: 3.507908494722939 | validation: 5.248031085458374]
	TIME [epoch: 10.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502060534373814		[learning rate: 0.00026482]
	Learning Rate: 0.000264818
	LOSS [training: 3.502060534373814 | validation: 5.239307844173794]
	TIME [epoch: 10.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4984818862872267		[learning rate: 0.00026386]
	Learning Rate: 0.000263857
	LOSS [training: 3.4984818862872267 | validation: 5.245173880033733]
	TIME [epoch: 10.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501172066813846		[learning rate: 0.0002629]
	Learning Rate: 0.000262899
	LOSS [training: 3.501172066813846 | validation: 5.261380634983327]
	TIME [epoch: 10.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.518937514434164		[learning rate: 0.00026195]
	Learning Rate: 0.000261945
	LOSS [training: 3.518937514434164 | validation: 5.26989736838535]
	TIME [epoch: 10.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5171730705861983		[learning rate: 0.00026099]
	Learning Rate: 0.000260995
	LOSS [training: 3.5171730705861983 | validation: 5.257259629258053]
	TIME [epoch: 10.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5027403183971897		[learning rate: 0.00026005]
	Learning Rate: 0.000260047
	LOSS [training: 3.5027403183971897 | validation: 5.245959934848998]
	TIME [epoch: 10.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500625577762537		[learning rate: 0.0002591]
	Learning Rate: 0.000259104
	LOSS [training: 3.500625577762537 | validation: 5.254120324891897]
	TIME [epoch: 10.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50275075436405		[learning rate: 0.00025816]
	Learning Rate: 0.000258163
	LOSS [training: 3.50275075436405 | validation: 5.241651423402362]
	TIME [epoch: 10.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502801071907322		[learning rate: 0.00025723]
	Learning Rate: 0.000257227
	LOSS [training: 3.502801071907322 | validation: 5.250256922391287]
	TIME [epoch: 10.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5009744537304877		[learning rate: 0.00025629]
	Learning Rate: 0.000256293
	LOSS [training: 3.5009744537304877 | validation: 5.2392501320562]
	TIME [epoch: 10.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499645267494811		[learning rate: 0.00025536]
	Learning Rate: 0.000255363
	LOSS [training: 3.499645267494811 | validation: 5.245581597316872]
	TIME [epoch: 10.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502637770642437		[learning rate: 0.00025444]
	Learning Rate: 0.000254436
	LOSS [training: 3.502637770642437 | validation: 5.245618244368199]
	TIME [epoch: 10.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4933489765503736		[learning rate: 0.00025351]
	Learning Rate: 0.000253513
	LOSS [training: 3.4933489765503736 | validation: 5.245609303407158]
	TIME [epoch: 10.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5015940328610045		[learning rate: 0.00025259]
	Learning Rate: 0.000252593
	LOSS [training: 3.5015940328610045 | validation: 5.247583812428035]
	TIME [epoch: 10.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.506007577473727		[learning rate: 0.00025168]
	Learning Rate: 0.000251676
	LOSS [training: 3.506007577473727 | validation: 5.252220755822434]
	TIME [epoch: 10.4 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991515645505524		[learning rate: 0.00025076]
	Learning Rate: 0.000250763
	LOSS [training: 3.4991515645505524 | validation: 5.260463229074507]
	TIME [epoch: 10.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.519029314390373		[learning rate: 0.00024985]
	Learning Rate: 0.000249853
	LOSS [training: 3.519029314390373 | validation: 5.249526438614696]
	TIME [epoch: 10.4 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500687226178692		[learning rate: 0.00024895]
	Learning Rate: 0.000248946
	LOSS [training: 3.500687226178692 | validation: 5.240715119299413]
	TIME [epoch: 10.4 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5014192494874523		[learning rate: 0.00024804]
	Learning Rate: 0.000248043
	LOSS [training: 3.5014192494874523 | validation: 5.2439990533925025]
	TIME [epoch: 10.4 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503484658061308		[learning rate: 0.00024714]
	Learning Rate: 0.000247142
	LOSS [training: 3.503484658061308 | validation: 5.239972805301411]
	TIME [epoch: 10.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990539044885516		[learning rate: 0.00024625]
	Learning Rate: 0.000246246
	LOSS [training: 3.4990539044885516 | validation: 5.255109608037748]
	TIME [epoch: 10.4 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5006934503409832		[learning rate: 0.00024535]
	Learning Rate: 0.000245352
	LOSS [training: 3.5006934503409832 | validation: 5.244957491546696]
	TIME [epoch: 10.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968826047555153		[learning rate: 0.00024446]
	Learning Rate: 0.000244462
	LOSS [training: 3.4968826047555153 | validation: 5.2424244946638865]
	TIME [epoch: 10.4 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4985788934984114		[learning rate: 0.00024357]
	Learning Rate: 0.000243574
	LOSS [training: 3.4985788934984114 | validation: 5.243855103746376]
	TIME [epoch: 10.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999965515918587		[learning rate: 0.00024269]
	Learning Rate: 0.00024269
	LOSS [training: 3.4999965515918587 | validation: 5.26384196135725]
	TIME [epoch: 10.4 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5027189768053097		[learning rate: 0.00024181]
	Learning Rate: 0.00024181
	LOSS [training: 3.5027189768053097 | validation: 5.252211137113124]
	TIME [epoch: 10.4 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500912038753829		[learning rate: 0.00024093]
	Learning Rate: 0.000240932
	LOSS [training: 3.500912038753829 | validation: 5.23830181643051]
	TIME [epoch: 10.4 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5041140884391724		[learning rate: 0.00024006]
	Learning Rate: 0.000240058
	LOSS [training: 3.5041140884391724 | validation: 5.239451594747734]
	TIME [epoch: 10.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999421103983694		[learning rate: 0.00023919]
	Learning Rate: 0.000239187
	LOSS [training: 3.4999421103983694 | validation: 5.235727272535312]
	TIME [epoch: 10.4 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5032982333909892		[learning rate: 0.00023832]
	Learning Rate: 0.000238319
	LOSS [training: 3.5032982333909892 | validation: 5.243557692503014]
	TIME [epoch: 10.4 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49923463592948		[learning rate: 0.00023745]
	Learning Rate: 0.000237454
	LOSS [training: 3.49923463592948 | validation: 5.258090561144037]
	TIME [epoch: 10.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5015764843231523		[learning rate: 0.00023659]
	Learning Rate: 0.000236592
	LOSS [training: 3.5015764843231523 | validation: 5.255752958685146]
	TIME [epoch: 10.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5022834020735836		[learning rate: 0.00023573]
	Learning Rate: 0.000235733
	LOSS [training: 3.5022834020735836 | validation: 5.260701660213276]
	TIME [epoch: 10.4 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5029880889480736		[learning rate: 0.00023488]
	Learning Rate: 0.000234878
	LOSS [training: 3.5029880889480736 | validation: 5.243439923615864]
	TIME [epoch: 10.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500342910821223		[learning rate: 0.00023403]
	Learning Rate: 0.000234026
	LOSS [training: 3.500342910821223 | validation: 5.2476732404388535]
	TIME [epoch: 10.4 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4947287505702436		[learning rate: 0.00023318]
	Learning Rate: 0.000233176
	LOSS [training: 3.4947287505702436 | validation: 5.23715923172883]
	TIME [epoch: 10.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502426081641539		[learning rate: 0.00023233]
	Learning Rate: 0.00023233
	LOSS [training: 3.502426081641539 | validation: 5.241804739547704]
	TIME [epoch: 10.4 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4997324032770103		[learning rate: 0.00023149]
	Learning Rate: 0.000231487
	LOSS [training: 3.4997324032770103 | validation: 5.247852316095592]
	TIME [epoch: 10.4 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495284539126152		[learning rate: 0.00023065]
	Learning Rate: 0.000230647
	LOSS [training: 3.495284539126152 | validation: 5.25376651896732]
	TIME [epoch: 10.4 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505585785160116		[learning rate: 0.00022981]
	Learning Rate: 0.00022981
	LOSS [training: 3.505585785160116 | validation: 5.252292481331495]
	TIME [epoch: 10.4 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5085907617657552		[learning rate: 0.00022898]
	Learning Rate: 0.000228976
	LOSS [training: 3.5085907617657552 | validation: 5.251758477593946]
	TIME [epoch: 10.4 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5065982099645603		[learning rate: 0.00022814]
	Learning Rate: 0.000228145
	LOSS [training: 3.5065982099645603 | validation: 5.258250803631371]
	TIME [epoch: 10.4 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5007340420845665		[learning rate: 0.00022732]
	Learning Rate: 0.000227317
	LOSS [training: 3.5007340420845665 | validation: 5.248767420762668]
	TIME [epoch: 10.4 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498746781800733		[learning rate: 0.00022649]
	Learning Rate: 0.000226492
	LOSS [training: 3.498746781800733 | validation: 5.243968164519511]
	TIME [epoch: 10.4 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499739047458026		[learning rate: 0.00022567]
	Learning Rate: 0.00022567
	LOSS [training: 3.499739047458026 | validation: 5.246695168030226]
	TIME [epoch: 10.4 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5027936891836378		[learning rate: 0.00022485]
	Learning Rate: 0.000224851
	LOSS [training: 3.5027936891836378 | validation: 5.24763595320914]
	TIME [epoch: 10.4 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5059182340861326		[learning rate: 0.00022403]
	Learning Rate: 0.000224035
	LOSS [training: 3.5059182340861326 | validation: 5.244028394474175]
	TIME [epoch: 10.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501887438423771		[learning rate: 0.00022322]
	Learning Rate: 0.000223222
	LOSS [training: 3.501887438423771 | validation: 5.25172006447855]
	TIME [epoch: 10.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969387388955218		[learning rate: 0.00022241]
	Learning Rate: 0.000222412
	LOSS [training: 3.4969387388955218 | validation: 5.248979390102157]
	TIME [epoch: 10.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507214021236237		[learning rate: 0.0002216]
	Learning Rate: 0.000221605
	LOSS [training: 3.507214021236237 | validation: 5.244769696144499]
	TIME [epoch: 10.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499556317497814		[learning rate: 0.0002208]
	Learning Rate: 0.0002208
	LOSS [training: 3.499556317497814 | validation: 5.240508584527686]
	TIME [epoch: 10.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501679669781803		[learning rate: 0.00022]
	Learning Rate: 0.000219999
	LOSS [training: 3.501679669781803 | validation: 5.238865687915427]
	TIME [epoch: 10.4 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5026487884412303		[learning rate: 0.0002192]
	Learning Rate: 0.000219201
	LOSS [training: 3.5026487884412303 | validation: 5.253031273235668]
	TIME [epoch: 10.4 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5031734217391906		[learning rate: 0.00021841]
	Learning Rate: 0.000218405
	LOSS [training: 3.5031734217391906 | validation: 5.236468670742915]
	TIME [epoch: 10.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.509085409267204		[learning rate: 0.00021761]
	Learning Rate: 0.000217613
	LOSS [training: 3.509085409267204 | validation: 5.236349200480119]
	TIME [epoch: 10.4 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498636054302098		[learning rate: 0.00021682]
	Learning Rate: 0.000216823
	LOSS [training: 3.498636054302098 | validation: 5.237079319532554]
	TIME [epoch: 10.4 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499462004083244		[learning rate: 0.00021604]
	Learning Rate: 0.000216036
	LOSS [training: 3.499462004083244 | validation: 5.2472406456805745]
	TIME [epoch: 10.4 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499082508493153		[learning rate: 0.00021525]
	Learning Rate: 0.000215252
	LOSS [training: 3.499082508493153 | validation: 5.249075825075253]
	TIME [epoch: 10.4 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502411453219609		[learning rate: 0.00021447]
	Learning Rate: 0.000214471
	LOSS [training: 3.502411453219609 | validation: 5.237734631529134]
	TIME [epoch: 10.4 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5006060646691552		[learning rate: 0.00021369]
	Learning Rate: 0.000213693
	LOSS [training: 3.5006060646691552 | validation: 5.24876838294659]
	TIME [epoch: 10.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501833356476129		[learning rate: 0.00021292]
	Learning Rate: 0.000212917
	LOSS [training: 3.501833356476129 | validation: 5.24764119748322]
	TIME [epoch: 10.4 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5027082985543827		[learning rate: 0.00021214]
	Learning Rate: 0.000212144
	LOSS [training: 3.5027082985543827 | validation: 5.248746903220475]
	TIME [epoch: 10.4 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5053070709869765		[learning rate: 0.00021137]
	Learning Rate: 0.000211375
	LOSS [training: 3.5053070709869765 | validation: 5.235082675211763]
	TIME [epoch: 10.4 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497631210418768		[learning rate: 0.00021061]
	Learning Rate: 0.000210607
	LOSS [training: 3.497631210418768 | validation: 5.244227345245823]
	TIME [epoch: 10.4 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5023664316467915		[learning rate: 0.00020984]
	Learning Rate: 0.000209843
	LOSS [training: 3.5023664316467915 | validation: 5.233019076555776]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5047155766124582		[learning rate: 0.00020908]
	Learning Rate: 0.000209082
	LOSS [training: 3.5047155766124582 | validation: 5.246525477429442]
	TIME [epoch: 10.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5007235083135173		[learning rate: 0.00020832]
	Learning Rate: 0.000208323
	LOSS [training: 3.5007235083135173 | validation: 5.249075804678012]
	TIME [epoch: 10.4 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504206413536653		[learning rate: 0.00020757]
	Learning Rate: 0.000207567
	LOSS [training: 3.504206413536653 | validation: 5.250904554768084]
	TIME [epoch: 10.4 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500726831408749		[learning rate: 0.00020681]
	Learning Rate: 0.000206814
	LOSS [training: 3.500726831408749 | validation: 5.258788847037072]
	TIME [epoch: 10.4 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961848783679934		[learning rate: 0.00020606]
	Learning Rate: 0.000206063
	LOSS [training: 3.4961848783679934 | validation: 5.236972394142042]
	TIME [epoch: 10.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5014679074096646		[learning rate: 0.00020532]
	Learning Rate: 0.000205315
	LOSS [training: 3.5014679074096646 | validation: 5.241506266680481]
	TIME [epoch: 10.4 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4974394215265825		[learning rate: 0.00020457]
	Learning Rate: 0.00020457
	LOSS [training: 3.4974394215265825 | validation: 5.2387098700535555]
	TIME [epoch: 10.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989094527922098		[learning rate: 0.00020383]
	Learning Rate: 0.000203828
	LOSS [training: 3.4989094527922098 | validation: 5.240259665374099]
	TIME [epoch: 10.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500516003902098		[learning rate: 0.00020309]
	Learning Rate: 0.000203088
	LOSS [training: 3.500516003902098 | validation: 5.25588386383685]
	TIME [epoch: 10.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5021677511739613		[learning rate: 0.00020235]
	Learning Rate: 0.000202351
	LOSS [training: 3.5021677511739613 | validation: 5.2483304791141565]
	TIME [epoch: 10.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499879177740076		[learning rate: 0.00020162]
	Learning Rate: 0.000201617
	LOSS [training: 3.499879177740076 | validation: 5.252233367031927]
	TIME [epoch: 10.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5036987174124787		[learning rate: 0.00020088]
	Learning Rate: 0.000200885
	LOSS [training: 3.5036987174124787 | validation: 5.252426847302737]
	TIME [epoch: 10.4 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5001161288565505		[learning rate: 0.00020016]
	Learning Rate: 0.000200156
	LOSS [training: 3.5001161288565505 | validation: 5.251250563622889]
	TIME [epoch: 10.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501844147432218		[learning rate: 0.00019943]
	Learning Rate: 0.00019943
	LOSS [training: 3.501844147432218 | validation: 5.247464843441183]
	TIME [epoch: 10.4 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500154196830868		[learning rate: 0.00019871]
	Learning Rate: 0.000198706
	LOSS [training: 3.500154196830868 | validation: 5.249121061498482]
	TIME [epoch: 10.4 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4973398773024607		[learning rate: 0.00019798]
	Learning Rate: 0.000197985
	LOSS [training: 3.4973398773024607 | validation: 5.249272669710873]
	TIME [epoch: 10.4 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989727663450445		[learning rate: 0.00019727]
	Learning Rate: 0.000197266
	LOSS [training: 3.4989727663450445 | validation: 5.24684325543302]
	TIME [epoch: 10.4 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5021980008441758		[learning rate: 0.00019655]
	Learning Rate: 0.00019655
	LOSS [training: 3.5021980008441758 | validation: 5.245773686183672]
	TIME [epoch: 10.4 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5004687502798775		[learning rate: 0.00019584]
	Learning Rate: 0.000195837
	LOSS [training: 3.5004687502798775 | validation: 5.250923507306409]
	TIME [epoch: 10.4 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5097101218242828		[learning rate: 0.00019513]
	Learning Rate: 0.000195126
	LOSS [training: 3.5097101218242828 | validation: 5.260384115240034]
	TIME [epoch: 10.4 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50380809406953		[learning rate: 0.00019442]
	Learning Rate: 0.000194418
	LOSS [training: 3.50380809406953 | validation: 5.242626020256767]
	TIME [epoch: 10.4 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5026220109917667		[learning rate: 0.00019371]
	Learning Rate: 0.000193713
	LOSS [training: 3.5026220109917667 | validation: 5.249895998073383]
	TIME [epoch: 10.4 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5012463372695626		[learning rate: 0.00019301]
	Learning Rate: 0.00019301
	LOSS [training: 3.5012463372695626 | validation: 5.235831206114585]
	TIME [epoch: 10.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49843898287604		[learning rate: 0.00019231]
	Learning Rate: 0.000192309
	LOSS [training: 3.49843898287604 | validation: 5.245124315347498]
	TIME [epoch: 10.4 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505336477579057		[learning rate: 0.00019161]
	Learning Rate: 0.000191611
	LOSS [training: 3.505336477579057 | validation: 5.246394693335127]
	TIME [epoch: 10.4 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49653708752907		[learning rate: 0.00019092]
	Learning Rate: 0.000190916
	LOSS [training: 3.49653708752907 | validation: 5.236657895034536]
	TIME [epoch: 10.4 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500060214465271		[learning rate: 0.00019022]
	Learning Rate: 0.000190223
	LOSS [training: 3.500060214465271 | validation: 5.261746643302302]
	TIME [epoch: 10.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5060817450926995		[learning rate: 0.00018953]
	Learning Rate: 0.000189533
	LOSS [training: 3.5060817450926995 | validation: 5.2471620846334455]
	TIME [epoch: 10.4 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502875118677616		[learning rate: 0.00018884]
	Learning Rate: 0.000188845
	LOSS [training: 3.502875118677616 | validation: 5.244258605653122]
	TIME [epoch: 10.4 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5043248403558325		[learning rate: 0.00018816]
	Learning Rate: 0.00018816
	LOSS [training: 3.5043248403558325 | validation: 5.2494945123554]
	TIME [epoch: 10.4 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49219496836218		[learning rate: 0.00018748]
	Learning Rate: 0.000187477
	LOSS [training: 3.49219496836218 | validation: 5.252559431685341]
	TIME [epoch: 10.4 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975641698921494		[learning rate: 0.0001868]
	Learning Rate: 0.000186796
	LOSS [training: 3.4975641698921494 | validation: 5.244374595459856]
	TIME [epoch: 10.4 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4994624697568133		[learning rate: 0.00018612]
	Learning Rate: 0.000186119
	LOSS [training: 3.4994624697568133 | validation: 5.251275087751012]
	TIME [epoch: 10.4 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501267909365958		[learning rate: 0.00018544]
	Learning Rate: 0.000185443
	LOSS [training: 3.501267909365958 | validation: 5.2431589438342785]
	TIME [epoch: 10.4 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499385109239533		[learning rate: 0.00018477]
	Learning Rate: 0.00018477
	LOSS [training: 3.499385109239533 | validation: 5.23804498782982]
	TIME [epoch: 10.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498500803988042		[learning rate: 0.0001841]
	Learning Rate: 0.000184099
	LOSS [training: 3.498500803988042 | validation: 5.243491876998662]
	TIME [epoch: 10.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496745760657087		[learning rate: 0.00018343]
	Learning Rate: 0.000183431
	LOSS [training: 3.496745760657087 | validation: 5.227769019448977]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_1200.pth
	Model improved!!!
EPOCH 1201/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504946608038155		[learning rate: 0.00018277]
	Learning Rate: 0.000182766
	LOSS [training: 3.504946608038155 | validation: 5.242663722823307]
	TIME [epoch: 10.4 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496559231377803		[learning rate: 0.0001821]
	Learning Rate: 0.000182102
	LOSS [training: 3.496559231377803 | validation: 5.241397363357851]
	TIME [epoch: 10.4 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501502024044322		[learning rate: 0.00018144]
	Learning Rate: 0.000181442
	LOSS [training: 3.501502024044322 | validation: 5.262921272651083]
	TIME [epoch: 10.4 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507838798177159		[learning rate: 0.00018078]
	Learning Rate: 0.000180783
	LOSS [training: 3.507838798177159 | validation: 5.239511760312659]
	TIME [epoch: 10.4 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5026471139516437		[learning rate: 0.00018013]
	Learning Rate: 0.000180127
	LOSS [training: 3.5026471139516437 | validation: 5.241381452826346]
	TIME [epoch: 10.4 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4993438250176157		[learning rate: 0.00017947]
	Learning Rate: 0.000179473
	LOSS [training: 3.4993438250176157 | validation: 5.246294297199272]
	TIME [epoch: 10.4 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976608356918817		[learning rate: 0.00017882]
	Learning Rate: 0.000178822
	LOSS [training: 3.4976608356918817 | validation: 5.253901875152608]
	TIME [epoch: 10.4 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503064068912713		[learning rate: 0.00017817]
	Learning Rate: 0.000178173
	LOSS [training: 3.503064068912713 | validation: 5.244163335271129]
	TIME [epoch: 10.4 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5012718200338755		[learning rate: 0.00017753]
	Learning Rate: 0.000177526
	LOSS [training: 3.5012718200338755 | validation: 5.240318974477171]
	TIME [epoch: 10.4 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502440012642908		[learning rate: 0.00017688]
	Learning Rate: 0.000176882
	LOSS [training: 3.502440012642908 | validation: 5.242968932238132]
	TIME [epoch: 10.4 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5060754203770683		[learning rate: 0.00017624]
	Learning Rate: 0.00017624
	LOSS [training: 3.5060754203770683 | validation: 5.245415785319782]
	TIME [epoch: 10.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4962588588882824		[learning rate: 0.0001756]
	Learning Rate: 0.000175601
	LOSS [training: 3.4962588588882824 | validation: 5.251498757506702]
	TIME [epoch: 10.4 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5000170468758802		[learning rate: 0.00017496]
	Learning Rate: 0.000174964
	LOSS [training: 3.5000170468758802 | validation: 5.24243999217833]
	TIME [epoch: 10.4 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500369985735831		[learning rate: 0.00017433]
	Learning Rate: 0.000174329
	LOSS [training: 3.500369985735831 | validation: 5.252846210616556]
	TIME [epoch: 10.4 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.506082847362645		[learning rate: 0.0001737]
	Learning Rate: 0.000173696
	LOSS [training: 3.506082847362645 | validation: 5.26917836326137]
	TIME [epoch: 10.4 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5012638026048384		[learning rate: 0.00017307]
	Learning Rate: 0.000173066
	LOSS [training: 3.5012638026048384 | validation: 5.242019620550119]
	TIME [epoch: 10.4 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4945044438860386		[learning rate: 0.00017244]
	Learning Rate: 0.000172437
	LOSS [training: 3.4945044438860386 | validation: 5.240646518723691]
	TIME [epoch: 10.4 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4980833341327773		[learning rate: 0.00017181]
	Learning Rate: 0.000171812
	LOSS [training: 3.4980833341327773 | validation: 5.249363721128873]
	TIME [epoch: 10.4 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5022770693681977		[learning rate: 0.00017119]
	Learning Rate: 0.000171188
	LOSS [training: 3.5022770693681977 | validation: 5.2646328675448775]
	TIME [epoch: 10.4 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982748175010427		[learning rate: 0.00017057]
	Learning Rate: 0.000170567
	LOSS [training: 3.4982748175010427 | validation: 5.2539872036068305]
	TIME [epoch: 10.4 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.507224411882496		[learning rate: 0.00016995]
	Learning Rate: 0.000169948
	LOSS [training: 3.507224411882496 | validation: 5.251538980588785]
	TIME [epoch: 10.4 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5022729182178596		[learning rate: 0.00016933]
	Learning Rate: 0.000169331
	LOSS [training: 3.5022729182178596 | validation: 5.260283500786746]
	TIME [epoch: 10.4 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971120252637675		[learning rate: 0.00016872]
	Learning Rate: 0.000168717
	LOSS [training: 3.4971120252637675 | validation: 5.249511335754028]
	TIME [epoch: 10.4 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500614549537974		[learning rate: 0.0001681]
	Learning Rate: 0.000168104
	LOSS [training: 3.500614549537974 | validation: 5.257531577648652]
	TIME [epoch: 10.4 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5048796755883336		[learning rate: 0.00016749]
	Learning Rate: 0.000167494
	LOSS [training: 3.5048796755883336 | validation: 5.254037880122881]
	TIME [epoch: 10.4 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5019197700412397		[learning rate: 0.00016689]
	Learning Rate: 0.000166886
	LOSS [training: 3.5019197700412397 | validation: 5.255412858267798]
	TIME [epoch: 10.4 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989364369177665		[learning rate: 0.00016628]
	Learning Rate: 0.000166281
	LOSS [training: 3.4989364369177665 | validation: 5.240182596309637]
	TIME [epoch: 10.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991247291674012		[learning rate: 0.00016568]
	Learning Rate: 0.000165677
	LOSS [training: 3.4991247291674012 | validation: 5.2439306810450965]
	TIME [epoch: 10.4 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498787354934541		[learning rate: 0.00016508]
	Learning Rate: 0.000165076
	LOSS [training: 3.498787354934541 | validation: 5.238098897302284]
	TIME [epoch: 10.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4974122593377635		[learning rate: 0.00016448]
	Learning Rate: 0.000164477
	LOSS [training: 3.4974122593377635 | validation: 5.248015350783706]
	TIME [epoch: 10.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976094856525783		[learning rate: 0.00016388]
	Learning Rate: 0.00016388
	LOSS [training: 3.4976094856525783 | validation: 5.244712400781358]
	TIME [epoch: 10.4 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498409092021764		[learning rate: 0.00016329]
	Learning Rate: 0.000163285
	LOSS [training: 3.498409092021764 | validation: 5.248322613630113]
	TIME [epoch: 10.4 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5034835349626428		[learning rate: 0.00016269]
	Learning Rate: 0.000162693
	LOSS [training: 3.5034835349626428 | validation: 5.24413837824746]
	TIME [epoch: 10.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5005739431026655		[learning rate: 0.0001621]
	Learning Rate: 0.000162102
	LOSS [training: 3.5005739431026655 | validation: 5.243012193482103]
	TIME [epoch: 10.4 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497408537912745		[learning rate: 0.00016151]
	Learning Rate: 0.000161514
	LOSS [training: 3.497408537912745 | validation: 5.243292549293162]
	TIME [epoch: 10.4 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494795772899905		[learning rate: 0.00016093]
	Learning Rate: 0.000160928
	LOSS [training: 3.494795772899905 | validation: 5.2508608007297815]
	TIME [epoch: 10.4 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499977581725532		[learning rate: 0.00016034]
	Learning Rate: 0.000160344
	LOSS [training: 3.499977581725532 | validation: 5.255898553350346]
	TIME [epoch: 10.4 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5071579924994025		[learning rate: 0.00015976]
	Learning Rate: 0.000159762
	LOSS [training: 3.5071579924994025 | validation: 5.252415205043962]
	TIME [epoch: 10.4 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504029474321814		[learning rate: 0.00015918]
	Learning Rate: 0.000159182
	LOSS [training: 3.504029474321814 | validation: 5.244517410457398]
	TIME [epoch: 10.4 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5030664432194585		[learning rate: 0.0001586]
	Learning Rate: 0.000158605
	LOSS [training: 3.5030664432194585 | validation: 5.242185658368217]
	TIME [epoch: 10.4 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4995872413015063		[learning rate: 0.00015803]
	Learning Rate: 0.000158029
	LOSS [training: 3.4995872413015063 | validation: 5.2559321418230995]
	TIME [epoch: 10.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5060362037918673		[learning rate: 0.00015746]
	Learning Rate: 0.000157456
	LOSS [training: 3.5060362037918673 | validation: 5.235888442020257]
	TIME [epoch: 10.4 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4997420320922403		[learning rate: 0.00015688]
	Learning Rate: 0.000156884
	LOSS [training: 3.4997420320922403 | validation: 5.239939203233619]
	TIME [epoch: 10.4 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972370345471333		[learning rate: 0.00015631]
	Learning Rate: 0.000156315
	LOSS [training: 3.4972370345471333 | validation: 5.2500825374387565]
	TIME [epoch: 10.4 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496085437101109		[learning rate: 0.00015575]
	Learning Rate: 0.000155748
	LOSS [training: 3.496085437101109 | validation: 5.251190956898047]
	TIME [epoch: 10.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496447569697998		[learning rate: 0.00015518]
	Learning Rate: 0.000155182
	LOSS [training: 3.496447569697998 | validation: 5.242180949368747]
	TIME [epoch: 10.4 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5069053581098486		[learning rate: 0.00015462]
	Learning Rate: 0.000154619
	LOSS [training: 3.5069053581098486 | validation: 5.2329623812291475]
	TIME [epoch: 10.4 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501884473885127		[learning rate: 0.00015406]
	Learning Rate: 0.000154058
	LOSS [training: 3.501884473885127 | validation: 5.244019888944738]
	TIME [epoch: 10.4 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4998215436474736		[learning rate: 0.0001535]
	Learning Rate: 0.000153499
	LOSS [training: 3.4998215436474736 | validation: 5.247813515756544]
	TIME [epoch: 10.4 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4994833722272047		[learning rate: 0.00015294]
	Learning Rate: 0.000152942
	LOSS [training: 3.4994833722272047 | validation: 5.246880270327669]
	TIME [epoch: 10.4 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999183550208386		[learning rate: 0.00015239]
	Learning Rate: 0.000152387
	LOSS [training: 3.4999183550208386 | validation: 5.241712821496296]
	TIME [epoch: 10.4 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498737649200682		[learning rate: 0.00015183]
	Learning Rate: 0.000151834
	LOSS [training: 3.498737649200682 | validation: 5.245727827287813]
	TIME [epoch: 10.4 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4981370941743477		[learning rate: 0.00015128]
	Learning Rate: 0.000151283
	LOSS [training: 3.4981370941743477 | validation: 5.245613766066488]
	TIME [epoch: 10.4 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4993419703526896		[learning rate: 0.00015073]
	Learning Rate: 0.000150734
	LOSS [training: 3.4993419703526896 | validation: 5.246039714175929]
	TIME [epoch: 10.4 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50214511014889		[learning rate: 0.00015019]
	Learning Rate: 0.000150187
	LOSS [training: 3.50214511014889 | validation: 5.241419094692873]
	TIME [epoch: 10.4 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499635063027911		[learning rate: 0.00014964]
	Learning Rate: 0.000149642
	LOSS [training: 3.499635063027911 | validation: 5.246777001917824]
	TIME [epoch: 10.4 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5060267796100506		[learning rate: 0.0001491]
	Learning Rate: 0.000149099
	LOSS [training: 3.5060267796100506 | validation: 5.241245842445859]
	TIME [epoch: 10.4 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495334673496567		[learning rate: 0.00014856]
	Learning Rate: 0.000148558
	LOSS [training: 3.495334673496567 | validation: 5.248625484782891]
	TIME [epoch: 10.4 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4994774869891723		[learning rate: 0.00014802]
	Learning Rate: 0.000148018
	LOSS [training: 3.4994774869891723 | validation: 5.235783906810793]
	TIME [epoch: 10.4 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502588682585177		[learning rate: 0.00014748]
	Learning Rate: 0.000147481
	LOSS [training: 3.502588682585177 | validation: 5.236798700667392]
	TIME [epoch: 10.4 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975532588353344		[learning rate: 0.00014695]
	Learning Rate: 0.000146946
	LOSS [training: 3.4975532588353344 | validation: 5.243030958435092]
	TIME [epoch: 10.4 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4993417397529867		[learning rate: 0.00014641]
	Learning Rate: 0.000146413
	LOSS [training: 3.4993417397529867 | validation: 5.243120217467025]
	TIME [epoch: 10.4 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5040492318227834		[learning rate: 0.00014588]
	Learning Rate: 0.000145881
	LOSS [training: 3.5040492318227834 | validation: 5.2422710241137835]
	TIME [epoch: 10.4 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500820808857817		[learning rate: 0.00014535]
	Learning Rate: 0.000145352
	LOSS [training: 3.500820808857817 | validation: 5.250002799370916]
	TIME [epoch: 10.4 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989219553927304		[learning rate: 0.00014482]
	Learning Rate: 0.000144825
	LOSS [training: 3.4989219553927304 | validation: 5.2425636884072]
	TIME [epoch: 10.4 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4984920316017982		[learning rate: 0.0001443]
	Learning Rate: 0.000144299
	LOSS [training: 3.4984920316017982 | validation: 5.2361162861168635]
	TIME [epoch: 10.4 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498825404963344		[learning rate: 0.00014378]
	Learning Rate: 0.000143775
	LOSS [training: 3.498825404963344 | validation: 5.253723209205377]
	TIME [epoch: 10.4 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952622578173935		[learning rate: 0.00014325]
	Learning Rate: 0.000143253
	LOSS [training: 3.4952622578173935 | validation: 5.25384929578951]
	TIME [epoch: 10.4 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4985042132232858		[learning rate: 0.00014273]
	Learning Rate: 0.000142734
	LOSS [training: 3.4985042132232858 | validation: 5.244538424728353]
	TIME [epoch: 10.4 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504410400951742		[learning rate: 0.00014222]
	Learning Rate: 0.000142216
	LOSS [training: 3.504410400951742 | validation: 5.246772484526303]
	TIME [epoch: 10.4 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496374113123609		[learning rate: 0.0001417]
	Learning Rate: 0.0001417
	LOSS [training: 3.496374113123609 | validation: 5.244017824803156]
	TIME [epoch: 10.4 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5031530375805398		[learning rate: 0.00014119]
	Learning Rate: 0.000141185
	LOSS [training: 3.5031530375805398 | validation: 5.241742924798517]
	TIME [epoch: 10.4 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500452910989387		[learning rate: 0.00014067]
	Learning Rate: 0.000140673
	LOSS [training: 3.500452910989387 | validation: 5.242360034344773]
	TIME [epoch: 10.4 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5033835937528197		[learning rate: 0.00014016]
	Learning Rate: 0.000140162
	LOSS [training: 3.5033835937528197 | validation: 5.240074158589482]
	TIME [epoch: 10.4 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4998345897647836		[learning rate: 0.00013965]
	Learning Rate: 0.000139654
	LOSS [training: 3.4998345897647836 | validation: 5.251345379412351]
	TIME [epoch: 10.4 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4993664820613204		[learning rate: 0.00013915]
	Learning Rate: 0.000139147
	LOSS [training: 3.4993664820613204 | validation: 5.236973438432536]
	TIME [epoch: 10.4 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498697278008526		[learning rate: 0.00013864]
	Learning Rate: 0.000138642
	LOSS [training: 3.498697278008526 | validation: 5.241820918971369]
	TIME [epoch: 10.4 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498145927007362		[learning rate: 0.00013814]
	Learning Rate: 0.000138139
	LOSS [training: 3.498145927007362 | validation: 5.241258736129471]
	TIME [epoch: 10.4 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500567778413923		[learning rate: 0.00013764]
	Learning Rate: 0.000137638
	LOSS [training: 3.500567778413923 | validation: 5.250182020135221]
	TIME [epoch: 10.4 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4993250187788183		[learning rate: 0.00013714]
	Learning Rate: 0.000137138
	LOSS [training: 3.4993250187788183 | validation: 5.253381132032855]
	TIME [epoch: 10.4 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5027652446453184		[learning rate: 0.00013664]
	Learning Rate: 0.00013664
	LOSS [training: 3.5027652446453184 | validation: 5.243459416665364]
	TIME [epoch: 10.4 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50057555773572		[learning rate: 0.00013614]
	Learning Rate: 0.000136144
	LOSS [training: 3.50057555773572 | validation: 5.24801902653043]
	TIME [epoch: 10.4 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508673649584955		[learning rate: 0.00013565]
	Learning Rate: 0.00013565
	LOSS [training: 3.508673649584955 | validation: 5.245763731775762]
	TIME [epoch: 10.4 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.508403071351373		[learning rate: 0.00013516]
	Learning Rate: 0.000135158
	LOSS [training: 3.508403071351373 | validation: 5.241966852432115]
	TIME [epoch: 10.4 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50018882026552		[learning rate: 0.00013467]
	Learning Rate: 0.000134668
	LOSS [training: 3.50018882026552 | validation: 5.247899153795032]
	TIME [epoch: 10.4 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499472546700504		[learning rate: 0.00013418]
	Learning Rate: 0.000134179
	LOSS [training: 3.499472546700504 | validation: 5.239828221349463]
	TIME [epoch: 10.4 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4992675030822538		[learning rate: 0.00013369]
	Learning Rate: 0.000133692
	LOSS [training: 3.4992675030822538 | validation: 5.253681546695957]
	TIME [epoch: 10.4 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5000490445142844		[learning rate: 0.00013321]
	Learning Rate: 0.000133207
	LOSS [training: 3.5000490445142844 | validation: 5.247615798502448]
	TIME [epoch: 10.4 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4988360524579902		[learning rate: 0.00013272]
	Learning Rate: 0.000132723
	LOSS [training: 3.4988360524579902 | validation: 5.246282796111832]
	TIME [epoch: 10.4 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500690586542446		[learning rate: 0.00013224]
	Learning Rate: 0.000132242
	LOSS [training: 3.500690586542446 | validation: 5.244440883577136]
	TIME [epoch: 10.4 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5011559811187767		[learning rate: 0.00013176]
	Learning Rate: 0.000131762
	LOSS [training: 3.5011559811187767 | validation: 5.247463337591212]
	TIME [epoch: 10.4 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502271376915572		[learning rate: 0.00013128]
	Learning Rate: 0.000131284
	LOSS [training: 3.502271376915572 | validation: 5.233306143077075]
	TIME [epoch: 10.4 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495986795559799		[learning rate: 0.00013081]
	Learning Rate: 0.000130807
	LOSS [training: 3.495986795559799 | validation: 5.244359502287325]
	TIME [epoch: 10.4 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497875096129061		[learning rate: 0.00013033]
	Learning Rate: 0.000130332
	LOSS [training: 3.497875096129061 | validation: 5.247197235446606]
	TIME [epoch: 10.4 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495713368116945		[learning rate: 0.00012986]
	Learning Rate: 0.000129859
	LOSS [training: 3.495713368116945 | validation: 5.244645167290652]
	TIME [epoch: 10.4 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5025320122783996		[learning rate: 0.00012939]
	Learning Rate: 0.000129388
	LOSS [training: 3.5025320122783996 | validation: 5.252044782509838]
	TIME [epoch: 10.4 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4985254905773955		[learning rate: 0.00012892]
	Learning Rate: 0.000128919
	LOSS [training: 3.4985254905773955 | validation: 5.241254608808711]
	TIME [epoch: 10.4 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497829776649901		[learning rate: 0.00012845]
	Learning Rate: 0.000128451
	LOSS [training: 3.497829776649901 | validation: 5.247302376692087]
	TIME [epoch: 10.4 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498369168549817		[learning rate: 0.00012798]
	Learning Rate: 0.000127985
	LOSS [training: 3.498369168549817 | validation: 5.248142523670497]
	TIME [epoch: 10.4 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500536910315958		[learning rate: 0.00012752]
	Learning Rate: 0.00012752
	LOSS [training: 3.500536910315958 | validation: 5.259472879286432]
	TIME [epoch: 10.4 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5005979844085937		[learning rate: 0.00012706]
	Learning Rate: 0.000127057
	LOSS [training: 3.5005979844085937 | validation: 5.250467429207142]
	TIME [epoch: 10.4 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5047365368100323		[learning rate: 0.0001266]
	Learning Rate: 0.000126596
	LOSS [training: 3.5047365368100323 | validation: 5.245785269881703]
	TIME [epoch: 10.4 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505484416136958		[learning rate: 0.00012614]
	Learning Rate: 0.000126137
	LOSS [training: 3.505484416136958 | validation: 5.24620145078416]
	TIME [epoch: 10.4 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5000342940072406		[learning rate: 0.00012568]
	Learning Rate: 0.000125679
	LOSS [training: 3.5000342940072406 | validation: 5.240048124725522]
	TIME [epoch: 10.4 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4988426552108485		[learning rate: 0.00012522]
	Learning Rate: 0.000125223
	LOSS [training: 3.4988426552108485 | validation: 5.252593968679382]
	TIME [epoch: 10.4 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968043425152295		[learning rate: 0.00012477]
	Learning Rate: 0.000124769
	LOSS [training: 3.4968043425152295 | validation: 5.240565389982212]
	TIME [epoch: 10.4 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497686283306481		[learning rate: 0.00012432]
	Learning Rate: 0.000124316
	LOSS [training: 3.497686283306481 | validation: 5.2383952009402]
	TIME [epoch: 10.4 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496849145427157		[learning rate: 0.00012386]
	Learning Rate: 0.000123865
	LOSS [training: 3.496849145427157 | validation: 5.241545951441156]
	TIME [epoch: 10.4 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493330481396287		[learning rate: 0.00012342]
	Learning Rate: 0.000123415
	LOSS [training: 3.493330481396287 | validation: 5.251455002077446]
	TIME [epoch: 10.4 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4942752466299596		[learning rate: 0.00012297]
	Learning Rate: 0.000122967
	LOSS [training: 3.4942752466299596 | validation: 5.236261158236637]
	TIME [epoch: 10.4 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961340847567812		[learning rate: 0.00012252]
	Learning Rate: 0.000122521
	LOSS [training: 3.4961340847567812 | validation: 5.250969585607227]
	TIME [epoch: 10.4 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987328615872073		[learning rate: 0.00012208]
	Learning Rate: 0.000122076
	LOSS [training: 3.4987328615872073 | validation: 5.237831222574839]
	TIME [epoch: 10.4 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500701463313227		[learning rate: 0.00012163]
	Learning Rate: 0.000121633
	LOSS [training: 3.500701463313227 | validation: 5.253034847007073]
	TIME [epoch: 10.4 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4924473621534013		[learning rate: 0.00012119]
	Learning Rate: 0.000121192
	LOSS [training: 3.4924473621534013 | validation: 5.241203682010446]
	TIME [epoch: 10.4 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497485139186112		[learning rate: 0.00012075]
	Learning Rate: 0.000120752
	LOSS [training: 3.497485139186112 | validation: 5.250343735267518]
	TIME [epoch: 10.4 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494552796131857		[learning rate: 0.00012031]
	Learning Rate: 0.000120314
	LOSS [training: 3.494552796131857 | validation: 5.238168467357361]
	TIME [epoch: 10.4 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496971753132887		[learning rate: 0.00011988]
	Learning Rate: 0.000119877
	LOSS [training: 3.496971753132887 | validation: 5.2438980261787105]
	TIME [epoch: 10.4 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495156719985019		[learning rate: 0.00011944]
	Learning Rate: 0.000119442
	LOSS [training: 3.495156719985019 | validation: 5.239712106084405]
	TIME [epoch: 10.4 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496676090559087		[learning rate: 0.00011901]
	Learning Rate: 0.000119009
	LOSS [training: 3.496676090559087 | validation: 5.247334893797692]
	TIME [epoch: 10.4 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498578507441934		[learning rate: 0.00011858]
	Learning Rate: 0.000118577
	LOSS [training: 3.498578507441934 | validation: 5.244370352594606]
	TIME [epoch: 10.4 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499564024685391		[learning rate: 0.00011815]
	Learning Rate: 0.000118147
	LOSS [training: 3.499564024685391 | validation: 5.235720138396086]
	TIME [epoch: 10.4 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495110577762852		[learning rate: 0.00011772]
	Learning Rate: 0.000117718
	LOSS [training: 3.495110577762852 | validation: 5.245593427685929]
	TIME [epoch: 10.4 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972723889522586		[learning rate: 0.00011729]
	Learning Rate: 0.000117291
	LOSS [training: 3.4972723889522586 | validation: 5.249867918533051]
	TIME [epoch: 10.4 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5011725759758576		[learning rate: 0.00011686]
	Learning Rate: 0.000116865
	LOSS [training: 3.5011725759758576 | validation: 5.245255916889007]
	TIME [epoch: 10.4 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504574189162618		[learning rate: 0.00011644]
	Learning Rate: 0.000116441
	LOSS [training: 3.504574189162618 | validation: 5.257343371675661]
	TIME [epoch: 10.4 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505516122355447		[learning rate: 0.00011602]
	Learning Rate: 0.000116018
	LOSS [training: 3.505516122355447 | validation: 5.239711742062787]
	TIME [epoch: 10.4 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500927458219029		[learning rate: 0.0001156]
	Learning Rate: 0.000115597
	LOSS [training: 3.500927458219029 | validation: 5.247073578400353]
	TIME [epoch: 10.4 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5011604724431216		[learning rate: 0.00011518]
	Learning Rate: 0.000115178
	LOSS [training: 3.5011604724431216 | validation: 5.2452501599099035]
	TIME [epoch: 10.4 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498054193781497		[learning rate: 0.00011476]
	Learning Rate: 0.00011476
	LOSS [training: 3.498054193781497 | validation: 5.250294481876404]
	TIME [epoch: 10.4 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987279652521885		[learning rate: 0.00011434]
	Learning Rate: 0.000114343
	LOSS [training: 3.4987279652521885 | validation: 5.238041304807845]
	TIME [epoch: 10.4 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991544076582954		[learning rate: 0.00011393]
	Learning Rate: 0.000113928
	LOSS [training: 3.4991544076582954 | validation: 5.23552074117712]
	TIME [epoch: 10.4 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498856659899142		[learning rate: 0.00011351]
	Learning Rate: 0.000113515
	LOSS [training: 3.498856659899142 | validation: 5.245007666619972]
	TIME [epoch: 10.4 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500862332523473		[learning rate: 0.0001131]
	Learning Rate: 0.000113103
	LOSS [training: 3.500862332523473 | validation: 5.257923649579399]
	TIME [epoch: 10.4 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499104695878846		[learning rate: 0.00011269]
	Learning Rate: 0.000112692
	LOSS [training: 3.499104695878846 | validation: 5.241533429449289]
	TIME [epoch: 10.4 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958635038129655		[learning rate: 0.00011228]
	Learning Rate: 0.000112283
	LOSS [training: 3.4958635038129655 | validation: 5.248184106562064]
	TIME [epoch: 10.4 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493894421671075		[learning rate: 0.00011188]
	Learning Rate: 0.000111876
	LOSS [training: 3.493894421671075 | validation: 5.229057088867128]
	TIME [epoch: 10.4 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498119118810081		[learning rate: 0.00011147]
	Learning Rate: 0.00011147
	LOSS [training: 3.498119118810081 | validation: 5.242180913554446]
	TIME [epoch: 10.4 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501451495494045		[learning rate: 0.00011107]
	Learning Rate: 0.000111065
	LOSS [training: 3.501451495494045 | validation: 5.243394778481582]
	TIME [epoch: 10.4 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4944248951288386		[learning rate: 0.00011066]
	Learning Rate: 0.000110662
	LOSS [training: 3.4944248951288386 | validation: 5.256901610945915]
	TIME [epoch: 10.4 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5001094274791194		[learning rate: 0.00011026]
	Learning Rate: 0.000110261
	LOSS [training: 3.5001094274791194 | validation: 5.252348121465505]
	TIME [epoch: 10.4 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982509253871923		[learning rate: 0.00010986]
	Learning Rate: 0.000109861
	LOSS [training: 3.4982509253871923 | validation: 5.252779544170505]
	TIME [epoch: 10.4 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502698120475016		[learning rate: 0.00010946]
	Learning Rate: 0.000109462
	LOSS [training: 3.502698120475016 | validation: 5.246527126831711]
	TIME [epoch: 10.4 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5055886662108144		[learning rate: 0.00010906]
	Learning Rate: 0.000109065
	LOSS [training: 3.5055886662108144 | validation: 5.239549213384988]
	TIME [epoch: 10.4 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5011472163110384		[learning rate: 0.00010867]
	Learning Rate: 0.000108669
	LOSS [training: 3.5011472163110384 | validation: 5.239516581035148]
	TIME [epoch: 10.4 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499078755647838		[learning rate: 0.00010827]
	Learning Rate: 0.000108275
	LOSS [training: 3.499078755647838 | validation: 5.253829734343297]
	TIME [epoch: 10.4 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4970257634523363		[learning rate: 0.00010788]
	Learning Rate: 0.000107882
	LOSS [training: 3.4970257634523363 | validation: 5.2417232369912]
	TIME [epoch: 10.4 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953610177208625		[learning rate: 0.00010749]
	Learning Rate: 0.00010749
	LOSS [training: 3.4953610177208625 | validation: 5.237452156325117]
	TIME [epoch: 10.4 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953990914123367		[learning rate: 0.0001071]
	Learning Rate: 0.0001071
	LOSS [training: 3.4953990914123367 | validation: 5.244513959193831]
	TIME [epoch: 10.4 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493548128856413		[learning rate: 0.00010671]
	Learning Rate: 0.000106711
	LOSS [training: 3.493548128856413 | validation: 5.251063608444903]
	TIME [epoch: 10.4 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494546934969045		[learning rate: 0.00010632]
	Learning Rate: 0.000106324
	LOSS [training: 3.494546934969045 | validation: 5.245367328363902]
	TIME [epoch: 10.4 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975370393669203		[learning rate: 0.00010594]
	Learning Rate: 0.000105938
	LOSS [training: 3.4975370393669203 | validation: 5.245179354699097]
	TIME [epoch: 10.4 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989578904795353		[learning rate: 0.00010555]
	Learning Rate: 0.000105554
	LOSS [training: 3.4989578904795353 | validation: 5.245567399711643]
	TIME [epoch: 10.4 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494740859746161		[learning rate: 0.00010517]
	Learning Rate: 0.000105171
	LOSS [training: 3.494740859746161 | validation: 5.242553001296801]
	TIME [epoch: 10.4 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4962451427315067		[learning rate: 0.00010479]
	Learning Rate: 0.000104789
	LOSS [training: 3.4962451427315067 | validation: 5.239909533472522]
	TIME [epoch: 10.4 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49993524781215		[learning rate: 0.00010441]
	Learning Rate: 0.000104409
	LOSS [training: 3.49993524781215 | validation: 5.236844647925848]
	TIME [epoch: 10.4 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961832921070295		[learning rate: 0.00010403]
	Learning Rate: 0.00010403
	LOSS [training: 3.4961832921070295 | validation: 5.246298412880215]
	TIME [epoch: 10.4 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4970706080238614		[learning rate: 0.00010365]
	Learning Rate: 0.000103652
	LOSS [training: 3.4970706080238614 | validation: 5.242568927770935]
	TIME [epoch: 10.4 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500850814437176		[learning rate: 0.00010328]
	Learning Rate: 0.000103276
	LOSS [training: 3.500850814437176 | validation: 5.233555091938479]
	TIME [epoch: 10.4 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4926081776661526		[learning rate: 0.0001029]
	Learning Rate: 0.000102901
	LOSS [training: 3.4926081776661526 | validation: 5.241434603121766]
	TIME [epoch: 10.4 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4930886137151687		[learning rate: 0.00010253]
	Learning Rate: 0.000102528
	LOSS [training: 3.4930886137151687 | validation: 5.239420819051625]
	TIME [epoch: 10.4 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498283010549249		[learning rate: 0.00010216]
	Learning Rate: 0.000102156
	LOSS [training: 3.498283010549249 | validation: 5.254645131728397]
	TIME [epoch: 10.4 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4992800228256096		[learning rate: 0.00010179]
	Learning Rate: 0.000101785
	LOSS [training: 3.4992800228256096 | validation: 5.248368319551105]
	TIME [epoch: 10.4 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958911675560786		[learning rate: 0.00010142]
	Learning Rate: 0.000101416
	LOSS [training: 3.4958911675560786 | validation: 5.2423412878518825]
	TIME [epoch: 10.4 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4988979356475602		[learning rate: 0.00010105]
	Learning Rate: 0.000101048
	LOSS [training: 3.4988979356475602 | validation: 5.238913942035007]
	TIME [epoch: 10.4 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5018603679471836		[learning rate: 0.00010068]
	Learning Rate: 0.000100681
	LOSS [training: 3.5018603679471836 | validation: 5.258011815534182]
	TIME [epoch: 10.4 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5026261033439043		[learning rate: 0.00010032]
	Learning Rate: 0.000100316
	LOSS [training: 3.5026261033439043 | validation: 5.249438942536983]
	TIME [epoch: 10.4 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5024219391148406		[learning rate: 9.9952e-05]
	Learning Rate: 9.99515e-05
	LOSS [training: 3.5024219391148406 | validation: 5.234740810203423]
	TIME [epoch: 10.4 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5016983381959363		[learning rate: 9.9589e-05]
	Learning Rate: 9.95888e-05
	LOSS [training: 3.5016983381959363 | validation: 5.245015847265097]
	TIME [epoch: 10.4 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5025316562757696		[learning rate: 9.9227e-05]
	Learning Rate: 9.92274e-05
	LOSS [training: 3.5025316562757696 | validation: 5.2450660773172775]
	TIME [epoch: 10.4 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504501871261529		[learning rate: 9.8867e-05]
	Learning Rate: 9.88673e-05
	LOSS [training: 3.504501871261529 | validation: 5.264665136788233]
	TIME [epoch: 10.4 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505519495379448		[learning rate: 9.8509e-05]
	Learning Rate: 9.85085e-05
	LOSS [training: 3.505519495379448 | validation: 5.249342617491827]
	TIME [epoch: 10.4 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987740903861386		[learning rate: 9.8151e-05]
	Learning Rate: 9.8151e-05
	LOSS [training: 3.4987740903861386 | validation: 5.237319644771835]
	TIME [epoch: 10.4 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500972129357362		[learning rate: 9.7795e-05]
	Learning Rate: 9.77948e-05
	LOSS [training: 3.500972129357362 | validation: 5.248268746496203]
	TIME [epoch: 10.4 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961587923629045		[learning rate: 9.744e-05]
	Learning Rate: 9.74399e-05
	LOSS [training: 3.4961587923629045 | validation: 5.247971813864403]
	TIME [epoch: 10.4 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966523211779252		[learning rate: 9.7086e-05]
	Learning Rate: 9.70863e-05
	LOSS [training: 3.4966523211779252 | validation: 5.246873077611558]
	TIME [epoch: 10.4 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991281372862653		[learning rate: 9.6734e-05]
	Learning Rate: 9.6734e-05
	LOSS [training: 3.4991281372862653 | validation: 5.254921788835416]
	TIME [epoch: 10.4 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5008221979289993		[learning rate: 9.6383e-05]
	Learning Rate: 9.63829e-05
	LOSS [training: 3.5008221979289993 | validation: 5.2546028488186565]
	TIME [epoch: 10.4 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499171322408543		[learning rate: 9.6033e-05]
	Learning Rate: 9.60331e-05
	LOSS [training: 3.499171322408543 | validation: 5.243674862146143]
	TIME [epoch: 10.4 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968875035932774		[learning rate: 9.5685e-05]
	Learning Rate: 9.56846e-05
	LOSS [training: 3.4968875035932774 | validation: 5.252994532147859]
	TIME [epoch: 10.4 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4935577458853913		[learning rate: 9.5337e-05]
	Learning Rate: 9.53374e-05
	LOSS [training: 3.4935577458853913 | validation: 5.246252007507327]
	TIME [epoch: 10.4 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4979813522362475		[learning rate: 9.4991e-05]
	Learning Rate: 9.49914e-05
	LOSS [training: 3.4979813522362475 | validation: 5.241907147156123]
	TIME [epoch: 10.4 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4921944423350055		[learning rate: 9.4647e-05]
	Learning Rate: 9.46466e-05
	LOSS [training: 3.4921944423350055 | validation: 5.251018234692943]
	TIME [epoch: 10.4 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4948312586074763		[learning rate: 9.4303e-05]
	Learning Rate: 9.43032e-05
	LOSS [training: 3.4948312586074763 | validation: 5.23963286549284]
	TIME [epoch: 10.4 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4980400272877383		[learning rate: 9.3961e-05]
	Learning Rate: 9.39609e-05
	LOSS [training: 3.4980400272877383 | validation: 5.240113214134229]
	TIME [epoch: 10.4 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499109568658208		[learning rate: 9.362e-05]
	Learning Rate: 9.362e-05
	LOSS [training: 3.499109568658208 | validation: 5.246824299315468]
	TIME [epoch: 10.4 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982982364384947		[learning rate: 9.328e-05]
	Learning Rate: 9.32802e-05
	LOSS [training: 3.4982982364384947 | validation: 5.244155778313952]
	TIME [epoch: 10.4 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498444982991605		[learning rate: 9.2942e-05]
	Learning Rate: 9.29417e-05
	LOSS [training: 3.498444982991605 | validation: 5.244257369396794]
	TIME [epoch: 10.4 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491454435577512		[learning rate: 9.2604e-05]
	Learning Rate: 9.26044e-05
	LOSS [training: 3.491454435577512 | validation: 5.2431090609592115]
	TIME [epoch: 10.4 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4980866776823922		[learning rate: 9.2268e-05]
	Learning Rate: 9.22683e-05
	LOSS [training: 3.4980866776823922 | validation: 5.246174902827743]
	TIME [epoch: 10.4 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49673094725501		[learning rate: 9.1933e-05]
	Learning Rate: 9.19335e-05
	LOSS [training: 3.49673094725501 | validation: 5.246905003147126]
	TIME [epoch: 10.4 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5023372063885416		[learning rate: 9.16e-05]
	Learning Rate: 9.15998e-05
	LOSS [training: 3.5023372063885416 | validation: 5.2358900496984955]
	TIME [epoch: 10.4 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5002489139057458		[learning rate: 9.1267e-05]
	Learning Rate: 9.12674e-05
	LOSS [training: 3.5002489139057458 | validation: 5.244197334538703]
	TIME [epoch: 10.4 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4978141996378795		[learning rate: 9.0936e-05]
	Learning Rate: 9.09362e-05
	LOSS [training: 3.4978141996378795 | validation: 5.248330439205366]
	TIME [epoch: 10.4 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4992468464875515		[learning rate: 9.0606e-05]
	Learning Rate: 9.06062e-05
	LOSS [training: 3.4992468464875515 | validation: 5.24490133270901]
	TIME [epoch: 10.4 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4935339040597784		[learning rate: 9.0277e-05]
	Learning Rate: 9.02774e-05
	LOSS [training: 3.4935339040597784 | validation: 5.231707644836804]
	TIME [epoch: 10.4 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492318704936028		[learning rate: 8.995e-05]
	Learning Rate: 8.99498e-05
	LOSS [training: 3.492318704936028 | validation: 5.243464640538973]
	TIME [epoch: 10.4 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975691218969915		[learning rate: 8.9623e-05]
	Learning Rate: 8.96233e-05
	LOSS [training: 3.4975691218969915 | validation: 5.241006136346179]
	TIME [epoch: 10.4 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4955151429857225		[learning rate: 8.9298e-05]
	Learning Rate: 8.92981e-05
	LOSS [training: 3.4955151429857225 | validation: 5.242414430993224]
	TIME [epoch: 10.4 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498346286784863		[learning rate: 8.8974e-05]
	Learning Rate: 8.8974e-05
	LOSS [training: 3.498346286784863 | validation: 5.244151985294811]
	TIME [epoch: 10.4 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971739756786038		[learning rate: 8.8651e-05]
	Learning Rate: 8.86511e-05
	LOSS [training: 3.4971739756786038 | validation: 5.233969420530359]
	TIME [epoch: 10.4 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5048617042329155		[learning rate: 8.8329e-05]
	Learning Rate: 8.83294e-05
	LOSS [training: 3.5048617042329155 | validation: 5.244949198390966]
	TIME [epoch: 10.4 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4992188922204597		[learning rate: 8.8009e-05]
	Learning Rate: 8.80088e-05
	LOSS [training: 3.4992188922204597 | validation: 5.2449493749275415]
	TIME [epoch: 10.4 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4964280916889714		[learning rate: 8.7689e-05]
	Learning Rate: 8.76895e-05
	LOSS [training: 3.4964280916889714 | validation: 5.242146808470807]
	TIME [epoch: 10.4 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492881906220614		[learning rate: 8.7371e-05]
	Learning Rate: 8.73712e-05
	LOSS [training: 3.492881906220614 | validation: 5.243374065646033]
	TIME [epoch: 10.4 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492790386231723		[learning rate: 8.7054e-05]
	Learning Rate: 8.70542e-05
	LOSS [training: 3.492790386231723 | validation: 5.236044572456434]
	TIME [epoch: 10.4 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960672374699584		[learning rate: 8.6738e-05]
	Learning Rate: 8.67382e-05
	LOSS [training: 3.4960672374699584 | validation: 5.242075343982001]
	TIME [epoch: 10.4 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498070733048432		[learning rate: 8.6423e-05]
	Learning Rate: 8.64235e-05
	LOSS [training: 3.498070733048432 | validation: 5.238810504904804]
	TIME [epoch: 10.4 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4946534359104993		[learning rate: 8.611e-05]
	Learning Rate: 8.61098e-05
	LOSS [training: 3.4946534359104993 | validation: 5.24236474506592]
	TIME [epoch: 10.4 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960151484775395		[learning rate: 8.5797e-05]
	Learning Rate: 8.57973e-05
	LOSS [training: 3.4960151484775395 | validation: 5.249121107825348]
	TIME [epoch: 10.4 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4957689051904937		[learning rate: 8.5486e-05]
	Learning Rate: 8.54859e-05
	LOSS [training: 3.4957689051904937 | validation: 5.244671571868171]
	TIME [epoch: 10.4 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4965371144103727		[learning rate: 8.5176e-05]
	Learning Rate: 8.51757e-05
	LOSS [training: 3.4965371144103727 | validation: 5.244899713125334]
	TIME [epoch: 10.4 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4995770516842946		[learning rate: 8.4867e-05]
	Learning Rate: 8.48666e-05
	LOSS [training: 3.4995770516842946 | validation: 5.2394167901636415]
	TIME [epoch: 10.4 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5043293313890787		[learning rate: 8.4559e-05]
	Learning Rate: 8.45586e-05
	LOSS [training: 3.5043293313890787 | validation: 5.2325036728935945]
	TIME [epoch: 10.4 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5001144258981873		[learning rate: 8.4252e-05]
	Learning Rate: 8.42518e-05
	LOSS [training: 3.5001144258981873 | validation: 5.239201064125707]
	TIME [epoch: 10.4 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5013070402271493		[learning rate: 8.3946e-05]
	Learning Rate: 8.3946e-05
	LOSS [training: 3.5013070402271493 | validation: 5.247165820505172]
	TIME [epoch: 10.4 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5003237160352136		[learning rate: 8.3641e-05]
	Learning Rate: 8.36414e-05
	LOSS [training: 3.5003237160352136 | validation: 5.2497365240224285]
	TIME [epoch: 10.4 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961268132752084		[learning rate: 8.3338e-05]
	Learning Rate: 8.33378e-05
	LOSS [training: 3.4961268132752084 | validation: 5.234977496310877]
	TIME [epoch: 10.4 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961026836695543		[learning rate: 8.3035e-05]
	Learning Rate: 8.30354e-05
	LOSS [training: 3.4961026836695543 | validation: 5.237786604262612]
	TIME [epoch: 10.4 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50120044079051		[learning rate: 8.2734e-05]
	Learning Rate: 8.2734e-05
	LOSS [training: 3.50120044079051 | validation: 5.232423034070942]
	TIME [epoch: 10.4 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983999519383495		[learning rate: 8.2434e-05]
	Learning Rate: 8.24338e-05
	LOSS [training: 3.4983999519383495 | validation: 5.240794494846309]
	TIME [epoch: 10.4 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501749214263902		[learning rate: 8.2135e-05]
	Learning Rate: 8.21346e-05
	LOSS [training: 3.501749214263902 | validation: 5.257859274320329]
	TIME [epoch: 10.4 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502798501488757		[learning rate: 8.1837e-05]
	Learning Rate: 8.18366e-05
	LOSS [training: 3.502798501488757 | validation: 5.243135100351987]
	TIME [epoch: 10.4 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983931328791504		[learning rate: 8.154e-05]
	Learning Rate: 8.15396e-05
	LOSS [training: 3.4983931328791504 | validation: 5.23752904365686]
	TIME [epoch: 10.4 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492604264258329		[learning rate: 8.1244e-05]
	Learning Rate: 8.12437e-05
	LOSS [training: 3.492604264258329 | validation: 5.2486240676373965]
	TIME [epoch: 10.4 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4936368947928953		[learning rate: 8.0949e-05]
	Learning Rate: 8.09488e-05
	LOSS [training: 3.4936368947928953 | validation: 5.243414604004048]
	TIME [epoch: 10.4 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999314990778574		[learning rate: 8.0655e-05]
	Learning Rate: 8.0655e-05
	LOSS [training: 3.4999314990778574 | validation: 5.23661950944436]
	TIME [epoch: 10.4 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4956543234524076		[learning rate: 8.0362e-05]
	Learning Rate: 8.03623e-05
	LOSS [training: 3.4956543234524076 | validation: 5.2403859137153574]
	TIME [epoch: 10.4 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963773441495207		[learning rate: 8.0071e-05]
	Learning Rate: 8.00707e-05
	LOSS [training: 3.4963773441495207 | validation: 5.244772255651514]
	TIME [epoch: 10.4 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500653892598835		[learning rate: 7.978e-05]
	Learning Rate: 7.97801e-05
	LOSS [training: 3.500653892598835 | validation: 5.241667303199522]
	TIME [epoch: 10.4 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4964014743062735		[learning rate: 7.9491e-05]
	Learning Rate: 7.94906e-05
	LOSS [training: 3.4964014743062735 | validation: 5.25041724194501]
	TIME [epoch: 10.4 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990532783948787		[learning rate: 7.9202e-05]
	Learning Rate: 7.92021e-05
	LOSS [training: 3.4990532783948787 | validation: 5.245844391921776]
	TIME [epoch: 10.4 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497989201753459		[learning rate: 7.8915e-05]
	Learning Rate: 7.89147e-05
	LOSS [training: 3.497989201753459 | validation: 5.2391832753053205]
	TIME [epoch: 10.4 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497090407455643		[learning rate: 7.8628e-05]
	Learning Rate: 7.86283e-05
	LOSS [training: 3.497090407455643 | validation: 5.249162856520847]
	TIME [epoch: 10.4 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49105443745012		[learning rate: 7.8343e-05]
	Learning Rate: 7.8343e-05
	LOSS [training: 3.49105443745012 | validation: 5.243200116787256]
	TIME [epoch: 10.4 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4981229192102683		[learning rate: 7.8059e-05]
	Learning Rate: 7.80586e-05
	LOSS [training: 3.4981229192102683 | validation: 5.255968038682912]
	TIME [epoch: 10.4 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987757168844333		[learning rate: 7.7775e-05]
	Learning Rate: 7.77754e-05
	LOSS [training: 3.4987757168844333 | validation: 5.250441411534577]
	TIME [epoch: 10.4 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49580540025417		[learning rate: 7.7493e-05]
	Learning Rate: 7.74931e-05
	LOSS [training: 3.49580540025417 | validation: 5.248913547006716]
	TIME [epoch: 10.4 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500949146520905		[learning rate: 7.7212e-05]
	Learning Rate: 7.72119e-05
	LOSS [training: 3.500949146520905 | validation: 5.243470074085576]
	TIME [epoch: 10.4 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972420513832425		[learning rate: 7.6932e-05]
	Learning Rate: 7.69317e-05
	LOSS [training: 3.4972420513832425 | validation: 5.2368881918353365]
	TIME [epoch: 10.4 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5023037293845007		[learning rate: 7.6653e-05]
	Learning Rate: 7.66525e-05
	LOSS [training: 3.5023037293845007 | validation: 5.239729385208282]
	TIME [epoch: 10.4 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953275671049227		[learning rate: 7.6374e-05]
	Learning Rate: 7.63743e-05
	LOSS [training: 3.4953275671049227 | validation: 5.242292123962816]
	TIME [epoch: 10.4 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4984306255552995		[learning rate: 7.6097e-05]
	Learning Rate: 7.60972e-05
	LOSS [training: 3.4984306255552995 | validation: 5.2470587307797745]
	TIME [epoch: 10.4 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4992651921213076		[learning rate: 7.5821e-05]
	Learning Rate: 7.5821e-05
	LOSS [training: 3.4992651921213076 | validation: 5.246006149459017]
	TIME [epoch: 10.4 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963929537799885		[learning rate: 7.5546e-05]
	Learning Rate: 7.55458e-05
	LOSS [training: 3.4963929537799885 | validation: 5.244496360147348]
	TIME [epoch: 10.4 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493661496563047		[learning rate: 7.5272e-05]
	Learning Rate: 7.52717e-05
	LOSS [training: 3.493661496563047 | validation: 5.246303728042769]
	TIME [epoch: 10.4 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4948322885652394		[learning rate: 7.4999e-05]
	Learning Rate: 7.49985e-05
	LOSS [training: 3.4948322885652394 | validation: 5.24534621956261]
	TIME [epoch: 10.4 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.504495757235263		[learning rate: 7.4726e-05]
	Learning Rate: 7.47263e-05
	LOSS [training: 3.504495757235263 | validation: 5.259497297395007]
	TIME [epoch: 10.4 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5035501968187397		[learning rate: 7.4455e-05]
	Learning Rate: 7.44552e-05
	LOSS [training: 3.5035501968187397 | validation: 5.240563463443957]
	TIME [epoch: 10.4 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4942261407153863		[learning rate: 7.4185e-05]
	Learning Rate: 7.4185e-05
	LOSS [training: 3.4942261407153863 | validation: 5.234352947160501]
	TIME [epoch: 10.4 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4949982890497777		[learning rate: 7.3916e-05]
	Learning Rate: 7.39157e-05
	LOSS [training: 3.4949982890497777 | validation: 5.246543700109716]
	TIME [epoch: 10.4 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493731923239114		[learning rate: 7.3647e-05]
	Learning Rate: 7.36475e-05
	LOSS [training: 3.493731923239114 | validation: 5.234970939056543]
	TIME [epoch: 10.4 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497704440062754		[learning rate: 7.338e-05]
	Learning Rate: 7.33802e-05
	LOSS [training: 3.497704440062754 | validation: 5.2389158823929325]
	TIME [epoch: 10.4 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498872900550537		[learning rate: 7.3114e-05]
	Learning Rate: 7.31139e-05
	LOSS [training: 3.498872900550537 | validation: 5.235149361871643]
	TIME [epoch: 10.4 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4979615407691833		[learning rate: 7.2849e-05]
	Learning Rate: 7.28486e-05
	LOSS [training: 3.4979615407691833 | validation: 5.24030000984306]
	TIME [epoch: 10.4 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5003071559597663		[learning rate: 7.2584e-05]
	Learning Rate: 7.25842e-05
	LOSS [training: 3.5003071559597663 | validation: 5.2294578439068236]
	TIME [epoch: 10.4 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.505011528750304		[learning rate: 7.2321e-05]
	Learning Rate: 7.23208e-05
	LOSS [training: 3.505011528750304 | validation: 5.235612995000584]
	TIME [epoch: 10.4 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500131847501313		[learning rate: 7.2058e-05]
	Learning Rate: 7.20583e-05
	LOSS [training: 3.500131847501313 | validation: 5.242233438072033]
	TIME [epoch: 10.4 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983127383606183		[learning rate: 7.1797e-05]
	Learning Rate: 7.17968e-05
	LOSS [training: 3.4983127383606183 | validation: 5.239881658671339]
	TIME [epoch: 10.4 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502251296122921		[learning rate: 7.1536e-05]
	Learning Rate: 7.15363e-05
	LOSS [training: 3.502251296122921 | validation: 5.233203416675858]
	TIME [epoch: 10.4 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5016087133060703		[learning rate: 7.1277e-05]
	Learning Rate: 7.12767e-05
	LOSS [training: 3.5016087133060703 | validation: 5.228835344958707]
	TIME [epoch: 10.4 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4970673876736447		[learning rate: 7.1018e-05]
	Learning Rate: 7.1018e-05
	LOSS [training: 3.4970673876736447 | validation: 5.239637052753149]
	TIME [epoch: 10.4 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494900863887742		[learning rate: 7.076e-05]
	Learning Rate: 7.07603e-05
	LOSS [training: 3.494900863887742 | validation: 5.244793041719791]
	TIME [epoch: 10.4 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4951483503841025		[learning rate: 7.0503e-05]
	Learning Rate: 7.05035e-05
	LOSS [training: 3.4951483503841025 | validation: 5.249223020899001]
	TIME [epoch: 10.4 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494903538871294		[learning rate: 7.0248e-05]
	Learning Rate: 7.02476e-05
	LOSS [training: 3.494903538871294 | validation: 5.242651605320076]
	TIME [epoch: 10.4 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987968534122773		[learning rate: 6.9993e-05]
	Learning Rate: 6.99927e-05
	LOSS [training: 3.4987968534122773 | validation: 5.247060299440725]
	TIME [epoch: 10.4 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497020080383996		[learning rate: 6.9739e-05]
	Learning Rate: 6.97387e-05
	LOSS [training: 3.497020080383996 | validation: 5.244429631016253]
	TIME [epoch: 10.4 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499503865311103		[learning rate: 6.9486e-05]
	Learning Rate: 6.94856e-05
	LOSS [training: 3.499503865311103 | validation: 5.235821514087697]
	TIME [epoch: 10.4 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497745999186107		[learning rate: 6.9233e-05]
	Learning Rate: 6.92334e-05
	LOSS [training: 3.497745999186107 | validation: 5.2426488998372145]
	TIME [epoch: 10.4 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501635729850193		[learning rate: 6.8982e-05]
	Learning Rate: 6.89822e-05
	LOSS [training: 3.501635729850193 | validation: 5.241305247921398]
	TIME [epoch: 10.4 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498592567881733		[learning rate: 6.8732e-05]
	Learning Rate: 6.87318e-05
	LOSS [training: 3.498592567881733 | validation: 5.230677347961704]
	TIME [epoch: 10.4 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989041412006885		[learning rate: 6.8482e-05]
	Learning Rate: 6.84824e-05
	LOSS [training: 3.4989041412006885 | validation: 5.23955145477765]
	TIME [epoch: 10.4 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503679008569825		[learning rate: 6.8234e-05]
	Learning Rate: 6.82339e-05
	LOSS [training: 3.503679008569825 | validation: 5.2385136220171535]
	TIME [epoch: 10.4 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498592834229945		[learning rate: 6.7986e-05]
	Learning Rate: 6.79862e-05
	LOSS [training: 3.498592834229945 | validation: 5.236485442377008]
	TIME [epoch: 10.4 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500948868709268		[learning rate: 6.774e-05]
	Learning Rate: 6.77395e-05
	LOSS [training: 3.500948868709268 | validation: 5.239641556515667]
	TIME [epoch: 10.4 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500703869281838		[learning rate: 6.7494e-05]
	Learning Rate: 6.74937e-05
	LOSS [training: 3.500703869281838 | validation: 5.233685740708072]
	TIME [epoch: 10.4 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495555051461295		[learning rate: 6.7249e-05]
	Learning Rate: 6.72488e-05
	LOSS [training: 3.495555051461295 | validation: 5.239194062025383]
	TIME [epoch: 10.4 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496807117996069		[learning rate: 6.7005e-05]
	Learning Rate: 6.70047e-05
	LOSS [training: 3.496807117996069 | validation: 5.2493465342729015]
	TIME [epoch: 10.4 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969468562235733		[learning rate: 6.6762e-05]
	Learning Rate: 6.67616e-05
	LOSS [training: 3.4969468562235733 | validation: 5.240537108650004]
	TIME [epoch: 10.4 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4957708783214043		[learning rate: 6.6519e-05]
	Learning Rate: 6.65192e-05
	LOSS [training: 3.4957708783214043 | validation: 5.255853292035629]
	TIME [epoch: 10.4 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4950079655847572		[learning rate: 6.6278e-05]
	Learning Rate: 6.62778e-05
	LOSS [training: 3.4950079655847572 | validation: 5.232623753828075]
	TIME [epoch: 10.4 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4995830015444263		[learning rate: 6.6037e-05]
	Learning Rate: 6.60373e-05
	LOSS [training: 3.4995830015444263 | validation: 5.250728417107985]
	TIME [epoch: 10.4 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987448621643624		[learning rate: 6.5798e-05]
	Learning Rate: 6.57977e-05
	LOSS [training: 3.4987448621643624 | validation: 5.245464930855213]
	TIME [epoch: 10.4 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497091277149759		[learning rate: 6.5559e-05]
	Learning Rate: 6.55589e-05
	LOSS [training: 3.497091277149759 | validation: 5.235333094487002]
	TIME [epoch: 10.4 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987430670229065		[learning rate: 6.5321e-05]
	Learning Rate: 6.5321e-05
	LOSS [training: 3.4987430670229065 | validation: 5.252492962587483]
	TIME [epoch: 10.4 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5029800965727595		[learning rate: 6.5084e-05]
	Learning Rate: 6.50839e-05
	LOSS [training: 3.5029800965727595 | validation: 5.242380144205438]
	TIME [epoch: 10.4 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497054981139526		[learning rate: 6.4848e-05]
	Learning Rate: 6.48477e-05
	LOSS [training: 3.497054981139526 | validation: 5.253029689771144]
	TIME [epoch: 10.4 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498039894486189		[learning rate: 6.4612e-05]
	Learning Rate: 6.46124e-05
	LOSS [training: 3.498039894486189 | validation: 5.240421477734701]
	TIME [epoch: 10.4 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4978691832392337		[learning rate: 6.4378e-05]
	Learning Rate: 6.43779e-05
	LOSS [training: 3.4978691832392337 | validation: 5.250914459936167]
	TIME [epoch: 10.4 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499141587982012		[learning rate: 6.4144e-05]
	Learning Rate: 6.41443e-05
	LOSS [training: 3.499141587982012 | validation: 5.2452900849211765]
	TIME [epoch: 10.4 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4931292233417954		[learning rate: 6.3911e-05]
	Learning Rate: 6.39115e-05
	LOSS [training: 3.4931292233417954 | validation: 5.237817145505797]
	TIME [epoch: 10.4 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495249529700858		[learning rate: 6.368e-05]
	Learning Rate: 6.36795e-05
	LOSS [training: 3.495249529700858 | validation: 5.2422672309801746]
	TIME [epoch: 10.4 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500147658509745		[learning rate: 6.3448e-05]
	Learning Rate: 6.34485e-05
	LOSS [training: 3.500147658509745 | validation: 5.243448133012793]
	TIME [epoch: 10.4 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982980785825655		[learning rate: 6.3218e-05]
	Learning Rate: 6.32182e-05
	LOSS [training: 3.4982980785825655 | validation: 5.237373294574884]
	TIME [epoch: 10.4 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4977811441904394		[learning rate: 6.2989e-05]
	Learning Rate: 6.29888e-05
	LOSS [training: 3.4977811441904394 | validation: 5.245095539481591]
	TIME [epoch: 10.4 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501819894401149		[learning rate: 6.276e-05]
	Learning Rate: 6.27602e-05
	LOSS [training: 3.501819894401149 | validation: 5.240422248140151]
	TIME [epoch: 10.3 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4986331415674625		[learning rate: 6.2532e-05]
	Learning Rate: 6.25324e-05
	LOSS [training: 3.4986331415674625 | validation: 5.230106763032384]
	TIME [epoch: 10.4 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496354975914106		[learning rate: 6.2305e-05]
	Learning Rate: 6.23055e-05
	LOSS [training: 3.496354975914106 | validation: 5.240175508530797]
	TIME [epoch: 10.4 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501538937982744		[learning rate: 6.2079e-05]
	Learning Rate: 6.20794e-05
	LOSS [training: 3.501538937982744 | validation: 5.247452791820633]
	TIME [epoch: 10.4 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497123084034233		[learning rate: 6.1854e-05]
	Learning Rate: 6.18541e-05
	LOSS [training: 3.497123084034233 | validation: 5.245137664019187]
	TIME [epoch: 10.4 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497388878228447		[learning rate: 6.163e-05]
	Learning Rate: 6.16296e-05
	LOSS [training: 3.497388878228447 | validation: 5.2408843379435455]
	TIME [epoch: 10.4 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987553945361185		[learning rate: 6.1406e-05]
	Learning Rate: 6.1406e-05
	LOSS [training: 3.4987553945361185 | validation: 5.241696266948802]
	TIME [epoch: 10.4 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4993083663677544		[learning rate: 6.1183e-05]
	Learning Rate: 6.11831e-05
	LOSS [training: 3.4993083663677544 | validation: 5.2335406394602915]
	TIME [epoch: 10.4 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497436219868675		[learning rate: 6.0961e-05]
	Learning Rate: 6.09611e-05
	LOSS [training: 3.497436219868675 | validation: 5.23960257239681]
	TIME [epoch: 10.4 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4967128870980644		[learning rate: 6.074e-05]
	Learning Rate: 6.07399e-05
	LOSS [training: 3.4967128870980644 | validation: 5.234353840647405]
	TIME [epoch: 10.3 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499182376309912		[learning rate: 6.0519e-05]
	Learning Rate: 6.05194e-05
	LOSS [training: 3.499182376309912 | validation: 5.249281946835201]
	TIME [epoch: 10.4 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976337954514927		[learning rate: 6.03e-05]
	Learning Rate: 6.02998e-05
	LOSS [training: 3.4976337954514927 | validation: 5.238640026484205]
	TIME [epoch: 10.4 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.50029273619581		[learning rate: 6.0081e-05]
	Learning Rate: 6.0081e-05
	LOSS [training: 3.50029273619581 | validation: 5.2435473788721145]
	TIME [epoch: 10.4 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969368040119875		[learning rate: 5.9863e-05]
	Learning Rate: 5.98629e-05
	LOSS [training: 3.4969368040119875 | validation: 5.233668557438691]
	TIME [epoch: 10.4 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4954434814805166		[learning rate: 5.9646e-05]
	Learning Rate: 5.96457e-05
	LOSS [training: 3.4954434814805166 | validation: 5.2363902830222]
	TIME [epoch: 10.4 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966448002461603		[learning rate: 5.9429e-05]
	Learning Rate: 5.94292e-05
	LOSS [training: 3.4966448002461603 | validation: 5.242552541650789]
	TIME [epoch: 10.4 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4913202155439125		[learning rate: 5.9214e-05]
	Learning Rate: 5.92136e-05
	LOSS [training: 3.4913202155439125 | validation: 5.2347307705588255]
	TIME [epoch: 10.4 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4946553338604227		[learning rate: 5.8999e-05]
	Learning Rate: 5.89987e-05
	LOSS [training: 3.4946553338604227 | validation: 5.24879730302924]
	TIME [epoch: 10.4 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492920412718159		[learning rate: 5.8785e-05]
	Learning Rate: 5.87846e-05
	LOSS [training: 3.492920412718159 | validation: 5.25046856957956]
	TIME [epoch: 10.4 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493536316380588		[learning rate: 5.8571e-05]
	Learning Rate: 5.85712e-05
	LOSS [training: 3.493536316380588 | validation: 5.241547347744629]
	TIME [epoch: 10.4 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500995602555738		[learning rate: 5.8359e-05]
	Learning Rate: 5.83586e-05
	LOSS [training: 3.500995602555738 | validation: 5.2488378067945645]
	TIME [epoch: 10.4 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49675831234977		[learning rate: 5.8147e-05]
	Learning Rate: 5.81469e-05
	LOSS [training: 3.49675831234977 | validation: 5.234400148909888]
	TIME [epoch: 10.4 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4936812261785866		[learning rate: 5.7936e-05]
	Learning Rate: 5.79358e-05
	LOSS [training: 3.4936812261785866 | validation: 5.240122666092814]
	TIME [epoch: 10.4 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4914375018999726		[learning rate: 5.7726e-05]
	Learning Rate: 5.77256e-05
	LOSS [training: 3.4914375018999726 | validation: 5.246790962404629]
	TIME [epoch: 10.4 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966329419302618		[learning rate: 5.7516e-05]
	Learning Rate: 5.75161e-05
	LOSS [training: 3.4966329419302618 | validation: 5.244227440585985]
	TIME [epoch: 10.4 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4954726122796558		[learning rate: 5.7307e-05]
	Learning Rate: 5.73074e-05
	LOSS [training: 3.4954726122796558 | validation: 5.241901738559481]
	TIME [epoch: 10.4 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498533332584349		[learning rate: 5.7099e-05]
	Learning Rate: 5.70994e-05
	LOSS [training: 3.498533332584349 | validation: 5.24308884313854]
	TIME [epoch: 10.4 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4985680784198827		[learning rate: 5.6892e-05]
	Learning Rate: 5.68922e-05
	LOSS [training: 3.4985680784198827 | validation: 5.244291109539577]
	TIME [epoch: 10.4 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4995991898325363		[learning rate: 5.6686e-05]
	Learning Rate: 5.66857e-05
	LOSS [training: 3.4995991898325363 | validation: 5.240778982540574]
	TIME [epoch: 10.4 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4938903504878156		[learning rate: 5.648e-05]
	Learning Rate: 5.648e-05
	LOSS [training: 3.4938903504878156 | validation: 5.239868039394558]
	TIME [epoch: 10.4 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499971923540675		[learning rate: 5.6275e-05]
	Learning Rate: 5.6275e-05
	LOSS [training: 3.499971923540675 | validation: 5.252749765358069]
	TIME [epoch: 10.4 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493968580983271		[learning rate: 5.6071e-05]
	Learning Rate: 5.60708e-05
	LOSS [training: 3.493968580983271 | validation: 5.242786837280896]
	TIME [epoch: 10.4 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4926955744847383		[learning rate: 5.5867e-05]
	Learning Rate: 5.58673e-05
	LOSS [training: 3.4926955744847383 | validation: 5.232910309737797]
	TIME [epoch: 10.4 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968075958230207		[learning rate: 5.5665e-05]
	Learning Rate: 5.56646e-05
	LOSS [training: 3.4968075958230207 | validation: 5.242253319176834]
	TIME [epoch: 10.4 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4978202641505236		[learning rate: 5.5463e-05]
	Learning Rate: 5.54626e-05
	LOSS [training: 3.4978202641505236 | validation: 5.2372879828324725]
	TIME [epoch: 10.4 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500688909617795		[learning rate: 5.5261e-05]
	Learning Rate: 5.52613e-05
	LOSS [training: 3.500688909617795 | validation: 5.244463542387]
	TIME [epoch: 10.4 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495763131999668		[learning rate: 5.5061e-05]
	Learning Rate: 5.50608e-05
	LOSS [training: 3.495763131999668 | validation: 5.234966499556899]
	TIME [epoch: 10.4 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963526343054148		[learning rate: 5.4861e-05]
	Learning Rate: 5.48609e-05
	LOSS [training: 3.4963526343054148 | validation: 5.240230597806697]
	TIME [epoch: 10.4 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983562986764825		[learning rate: 5.4662e-05]
	Learning Rate: 5.46618e-05
	LOSS [training: 3.4983562986764825 | validation: 5.244882600112146]
	TIME [epoch: 10.4 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491604434908253		[learning rate: 5.4463e-05]
	Learning Rate: 5.44635e-05
	LOSS [training: 3.491604434908253 | validation: 5.235984347693278]
	TIME [epoch: 10.3 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495071852387503		[learning rate: 5.4266e-05]
	Learning Rate: 5.42658e-05
	LOSS [training: 3.495071852387503 | validation: 5.24054032285862]
	TIME [epoch: 10.4 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496992361769948		[learning rate: 5.4069e-05]
	Learning Rate: 5.40689e-05
	LOSS [training: 3.496992361769948 | validation: 5.230980105493679]
	TIME [epoch: 10.4 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4956574414294055		[learning rate: 5.3873e-05]
	Learning Rate: 5.38727e-05
	LOSS [training: 3.4956574414294055 | validation: 5.243534453679802]
	TIME [epoch: 10.4 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4938669076362183		[learning rate: 5.3677e-05]
	Learning Rate: 5.36772e-05
	LOSS [training: 3.4938669076362183 | validation: 5.243650412014172]
	TIME [epoch: 10.4 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969530436110823		[learning rate: 5.3482e-05]
	Learning Rate: 5.34824e-05
	LOSS [training: 3.4969530436110823 | validation: 5.237571565081432]
	TIME [epoch: 10.4 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987117800195278		[learning rate: 5.3288e-05]
	Learning Rate: 5.32883e-05
	LOSS [training: 3.4987117800195278 | validation: 5.239136354121383]
	TIME [epoch: 10.4 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49637667561442		[learning rate: 5.3095e-05]
	Learning Rate: 5.30949e-05
	LOSS [training: 3.49637667561442 | validation: 5.24181554904151]
	TIME [epoch: 10.4 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4931114211470815		[learning rate: 5.2902e-05]
	Learning Rate: 5.29022e-05
	LOSS [training: 3.4931114211470815 | validation: 5.240595879370463]
	TIME [epoch: 10.4 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975668947562304		[learning rate: 5.271e-05]
	Learning Rate: 5.27102e-05
	LOSS [training: 3.4975668947562304 | validation: 5.25140839868087]
	TIME [epoch: 10.4 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497800670778864		[learning rate: 5.2519e-05]
	Learning Rate: 5.25189e-05
	LOSS [training: 3.497800670778864 | validation: 5.2498540345899976]
	TIME [epoch: 10.4 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4992062887425406		[learning rate: 5.2328e-05]
	Learning Rate: 5.23283e-05
	LOSS [training: 3.4992062887425406 | validation: 5.247018675092874]
	TIME [epoch: 10.4 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4986751563505605		[learning rate: 5.2138e-05]
	Learning Rate: 5.21384e-05
	LOSS [training: 3.4986751563505605 | validation: 5.240454854226443]
	TIME [epoch: 10.4 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500020935518355		[learning rate: 5.1949e-05]
	Learning Rate: 5.19492e-05
	LOSS [training: 3.500020935518355 | validation: 5.242824235205658]
	TIME [epoch: 10.4 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498025519932226		[learning rate: 5.1761e-05]
	Learning Rate: 5.17607e-05
	LOSS [training: 3.498025519932226 | validation: 5.248521825057976]
	TIME [epoch: 10.4 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971506321192236		[learning rate: 5.1573e-05]
	Learning Rate: 5.15729e-05
	LOSS [training: 3.4971506321192236 | validation: 5.244923623308759]
	TIME [epoch: 10.4 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4945489396884652		[learning rate: 5.1386e-05]
	Learning Rate: 5.13857e-05
	LOSS [training: 3.4945489396884652 | validation: 5.235756748458581]
	TIME [epoch: 10.4 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49675328276284		[learning rate: 5.1199e-05]
	Learning Rate: 5.11992e-05
	LOSS [training: 3.49675328276284 | validation: 5.241196777876751]
	TIME [epoch: 10.4 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495895774441398		[learning rate: 5.1013e-05]
	Learning Rate: 5.10134e-05
	LOSS [training: 3.495895774441398 | validation: 5.2394783364540105]
	TIME [epoch: 10.4 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495551468818101		[learning rate: 5.0828e-05]
	Learning Rate: 5.08283e-05
	LOSS [training: 3.495551468818101 | validation: 5.2363445856388395]
	TIME [epoch: 10.4 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4931406963420173		[learning rate: 5.0644e-05]
	Learning Rate: 5.06438e-05
	LOSS [training: 3.4931406963420173 | validation: 5.2446072996622375]
	TIME [epoch: 10.4 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4942379292488743		[learning rate: 5.046e-05]
	Learning Rate: 5.046e-05
	LOSS [training: 3.4942379292488743 | validation: 5.248639039041723]
	TIME [epoch: 10.4 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971958128993648		[learning rate: 5.0277e-05]
	Learning Rate: 5.02769e-05
	LOSS [training: 3.4971958128993648 | validation: 5.24377941953704]
	TIME [epoch: 10.4 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.503325553130009		[learning rate: 5.0094e-05]
	Learning Rate: 5.00944e-05
	LOSS [training: 3.503325553130009 | validation: 5.241285580805685]
	TIME [epoch: 10.4 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497039332271478		[learning rate: 4.9913e-05]
	Learning Rate: 4.99127e-05
	LOSS [training: 3.497039332271478 | validation: 5.244085685020979]
	TIME [epoch: 10.4 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4920350574204635		[learning rate: 4.9732e-05]
	Learning Rate: 4.97315e-05
	LOSS [training: 3.4920350574204635 | validation: 5.247041047001603]
	TIME [epoch: 10.4 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968809344358127		[learning rate: 4.9551e-05]
	Learning Rate: 4.9551e-05
	LOSS [training: 3.4968809344358127 | validation: 5.248281549266801]
	TIME [epoch: 10.4 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983754218892122		[learning rate: 4.9371e-05]
	Learning Rate: 4.93712e-05
	LOSS [training: 3.4983754218892122 | validation: 5.246103432317325]
	TIME [epoch: 10.4 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5009881178298854		[learning rate: 4.9192e-05]
	Learning Rate: 4.9192e-05
	LOSS [training: 3.5009881178298854 | validation: 5.2359285053272275]
	TIME [epoch: 10.4 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498903891695581		[learning rate: 4.9014e-05]
	Learning Rate: 4.90135e-05
	LOSS [training: 3.498903891695581 | validation: 5.2324160669592015]
	TIME [epoch: 10.4 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4957980310791448		[learning rate: 4.8836e-05]
	Learning Rate: 4.88356e-05
	LOSS [training: 3.4957980310791448 | validation: 5.241221182981056]
	TIME [epoch: 10.4 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4940941841863995		[learning rate: 4.8658e-05]
	Learning Rate: 4.86584e-05
	LOSS [training: 3.4940941841863995 | validation: 5.241179280743186]
	TIME [epoch: 10.4 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961515776588867		[learning rate: 4.8482e-05]
	Learning Rate: 4.84818e-05
	LOSS [training: 3.4961515776588867 | validation: 5.248203818122484]
	TIME [epoch: 10.4 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4962354725919917		[learning rate: 4.8306e-05]
	Learning Rate: 4.83059e-05
	LOSS [training: 3.4962354725919917 | validation: 5.246556697609645]
	TIME [epoch: 10.4 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4997165444844462		[learning rate: 4.8131e-05]
	Learning Rate: 4.81306e-05
	LOSS [training: 3.4997165444844462 | validation: 5.248856328483563]
	TIME [epoch: 10.4 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991867614754106		[learning rate: 4.7956e-05]
	Learning Rate: 4.79559e-05
	LOSS [training: 3.4991867614754106 | validation: 5.250328157523109]
	TIME [epoch: 10.4 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502486723053593		[learning rate: 4.7782e-05]
	Learning Rate: 4.77819e-05
	LOSS [training: 3.502486723053593 | validation: 5.251567485505625]
	TIME [epoch: 10.4 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4964619168831894		[learning rate: 4.7608e-05]
	Learning Rate: 4.76085e-05
	LOSS [training: 3.4964619168831894 | validation: 5.251018576388076]
	TIME [epoch: 10.4 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4933724114416456		[learning rate: 4.7436e-05]
	Learning Rate: 4.74357e-05
	LOSS [training: 3.4933724114416456 | validation: 5.239378481812277]
	TIME [epoch: 10.4 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4986285886056145		[learning rate: 4.7264e-05]
	Learning Rate: 4.72636e-05
	LOSS [training: 3.4986285886056145 | validation: 5.238412254397148]
	TIME [epoch: 10.4 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497689650317441		[learning rate: 4.7092e-05]
	Learning Rate: 4.7092e-05
	LOSS [training: 3.497689650317441 | validation: 5.240023858304425]
	TIME [epoch: 10.4 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493281004117373		[learning rate: 4.6921e-05]
	Learning Rate: 4.69211e-05
	LOSS [training: 3.493281004117373 | validation: 5.25080203755898]
	TIME [epoch: 10.4 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49137084498982		[learning rate: 4.6751e-05]
	Learning Rate: 4.67508e-05
	LOSS [training: 3.49137084498982 | validation: 5.250853206622977]
	TIME [epoch: 10.3 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969345255281503		[learning rate: 4.6581e-05]
	Learning Rate: 4.65812e-05
	LOSS [training: 3.4969345255281503 | validation: 5.2341138675533365]
	TIME [epoch: 10.4 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497849207749334		[learning rate: 4.6412e-05]
	Learning Rate: 4.64121e-05
	LOSS [training: 3.497849207749334 | validation: 5.241801048174725]
	TIME [epoch: 10.4 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976277622078458		[learning rate: 4.6244e-05]
	Learning Rate: 4.62437e-05
	LOSS [training: 3.4976277622078458 | validation: 5.2384740827056735]
	TIME [epoch: 10.4 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958396733699244		[learning rate: 4.6076e-05]
	Learning Rate: 4.60759e-05
	LOSS [training: 3.4958396733699244 | validation: 5.244463064789361]
	TIME [epoch: 10.4 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961304409713954		[learning rate: 4.5909e-05]
	Learning Rate: 4.59087e-05
	LOSS [training: 3.4961304409713954 | validation: 5.237010187275441]
	TIME [epoch: 10.4 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501539815579271		[learning rate: 4.5742e-05]
	Learning Rate: 4.57421e-05
	LOSS [training: 3.501539815579271 | validation: 5.233318608721645]
	TIME [epoch: 10.4 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496881514676553		[learning rate: 4.5576e-05]
	Learning Rate: 4.55761e-05
	LOSS [training: 3.496881514676553 | validation: 5.236163538246272]
	TIME [epoch: 10.4 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501583968452097		[learning rate: 4.5411e-05]
	Learning Rate: 4.54107e-05
	LOSS [training: 3.501583968452097 | validation: 5.246839100527591]
	TIME [epoch: 10.4 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500139782980189		[learning rate: 4.5246e-05]
	Learning Rate: 4.52459e-05
	LOSS [training: 3.500139782980189 | validation: 5.23942160073417]
	TIME [epoch: 10.4 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4944455432566643		[learning rate: 4.5082e-05]
	Learning Rate: 4.50817e-05
	LOSS [training: 3.4944455432566643 | validation: 5.246720470694588]
	TIME [epoch: 10.4 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499840140268264		[learning rate: 4.4918e-05]
	Learning Rate: 4.49181e-05
	LOSS [training: 3.499840140268264 | validation: 5.236132813419895]
	TIME [epoch: 10.4 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4980868852027327		[learning rate: 4.4755e-05]
	Learning Rate: 4.47551e-05
	LOSS [training: 3.4980868852027327 | validation: 5.232445602016141]
	TIME [epoch: 10.4 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966710441152076		[learning rate: 4.4593e-05]
	Learning Rate: 4.45926e-05
	LOSS [training: 3.4966710441152076 | validation: 5.233688080645377]
	TIME [epoch: 10.4 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499285775575575		[learning rate: 4.4431e-05]
	Learning Rate: 4.44308e-05
	LOSS [training: 3.499285775575575 | validation: 5.233126757414438]
	TIME [epoch: 10.4 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4945936271025544		[learning rate: 4.427e-05]
	Learning Rate: 4.42696e-05
	LOSS [training: 3.4945936271025544 | validation: 5.245145388802932]
	TIME [epoch: 10.4 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493841375921174		[learning rate: 4.4109e-05]
	Learning Rate: 4.41089e-05
	LOSS [training: 3.493841375921174 | validation: 5.238174982401863]
	TIME [epoch: 10.4 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4931230375230635		[learning rate: 4.3949e-05]
	Learning Rate: 4.39489e-05
	LOSS [training: 3.4931230375230635 | validation: 5.2414888607807715]
	TIME [epoch: 10.4 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969550001125187		[learning rate: 4.3789e-05]
	Learning Rate: 4.37893e-05
	LOSS [training: 3.4969550001125187 | validation: 5.237708791900559]
	TIME [epoch: 10.4 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493944569301818		[learning rate: 4.363e-05]
	Learning Rate: 4.36304e-05
	LOSS [training: 3.493944569301818 | validation: 5.242088776580031]
	TIME [epoch: 10.4 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492364809080905		[learning rate: 4.3472e-05]
	Learning Rate: 4.34721e-05
	LOSS [training: 3.492364809080905 | validation: 5.2396978349673855]
	TIME [epoch: 10.4 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4941097250362234		[learning rate: 4.3314e-05]
	Learning Rate: 4.33143e-05
	LOSS [training: 3.4941097250362234 | validation: 5.241176006203847]
	TIME [epoch: 10.4 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953879268083172		[learning rate: 4.3157e-05]
	Learning Rate: 4.31571e-05
	LOSS [training: 3.4953879268083172 | validation: 5.243648448237904]
	TIME [epoch: 10.4 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968058522250827		[learning rate: 4.3001e-05]
	Learning Rate: 4.30005e-05
	LOSS [training: 3.4968058522250827 | validation: 5.236502400500719]
	TIME [epoch: 10.4 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493420933328351		[learning rate: 4.2844e-05]
	Learning Rate: 4.28445e-05
	LOSS [training: 3.493420933328351 | validation: 5.2537371486478115]
	TIME [epoch: 10.4 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4921929581652256		[learning rate: 4.2689e-05]
	Learning Rate: 4.2689e-05
	LOSS [training: 3.4921929581652256 | validation: 5.24472997205824]
	TIME [epoch: 10.4 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990896835530108		[learning rate: 4.2534e-05]
	Learning Rate: 4.25341e-05
	LOSS [training: 3.4990896835530108 | validation: 5.244017605148529]
	TIME [epoch: 10.4 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990049174803275		[learning rate: 4.238e-05]
	Learning Rate: 4.23797e-05
	LOSS [training: 3.4990049174803275 | validation: 5.242270002470048]
	TIME [epoch: 10.4 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499245845364193		[learning rate: 4.2226e-05]
	Learning Rate: 4.22259e-05
	LOSS [training: 3.499245845364193 | validation: 5.232952928358949]
	TIME [epoch: 10.4 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4941857826695903		[learning rate: 4.2073e-05]
	Learning Rate: 4.20727e-05
	LOSS [training: 3.4941857826695903 | validation: 5.2464761613068935]
	TIME [epoch: 10.4 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4925652498224933		[learning rate: 4.192e-05]
	Learning Rate: 4.192e-05
	LOSS [training: 3.4925652498224933 | validation: 5.248005447190296]
	TIME [epoch: 10.4 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495884706650579		[learning rate: 4.1768e-05]
	Learning Rate: 4.17679e-05
	LOSS [training: 3.495884706650579 | validation: 5.244305994059548]
	TIME [epoch: 10.4 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499313184932956		[learning rate: 4.1616e-05]
	Learning Rate: 4.16163e-05
	LOSS [training: 3.499313184932956 | validation: 5.2426079996776185]
	TIME [epoch: 10.4 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990424303187124		[learning rate: 4.1465e-05]
	Learning Rate: 4.14652e-05
	LOSS [training: 3.4990424303187124 | validation: 5.244542740090167]
	TIME [epoch: 10.4 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972210921223614		[learning rate: 4.1315e-05]
	Learning Rate: 4.13148e-05
	LOSS [training: 3.4972210921223614 | validation: 5.235464198457655]
	TIME [epoch: 10.4 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5009557334182446		[learning rate: 4.1165e-05]
	Learning Rate: 4.11648e-05
	LOSS [training: 3.5009557334182446 | validation: 5.232956355260692]
	TIME [epoch: 10.4 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499166110156603		[learning rate: 4.1015e-05]
	Learning Rate: 4.10154e-05
	LOSS [training: 3.499166110156603 | validation: 5.242912307279911]
	TIME [epoch: 10.4 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4959256843586353		[learning rate: 4.0867e-05]
	Learning Rate: 4.08666e-05
	LOSS [training: 3.4959256843586353 | validation: 5.239277488733443]
	TIME [epoch: 10.4 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493144765415346		[learning rate: 4.0718e-05]
	Learning Rate: 4.07183e-05
	LOSS [training: 3.493144765415346 | validation: 5.245868861737136]
	TIME [epoch: 10.4 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496820405201796		[learning rate: 4.0571e-05]
	Learning Rate: 4.05705e-05
	LOSS [training: 3.496820405201796 | validation: 5.23839123865419]
	TIME [epoch: 10.4 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4964406191181636		[learning rate: 4.0423e-05]
	Learning Rate: 4.04233e-05
	LOSS [training: 3.4964406191181636 | validation: 5.239214694645564]
	TIME [epoch: 10.4 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4977172174104694		[learning rate: 4.0277e-05]
	Learning Rate: 4.02766e-05
	LOSS [training: 3.4977172174104694 | validation: 5.2491560600987714]
	TIME [epoch: 10.4 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496538171093456		[learning rate: 4.013e-05]
	Learning Rate: 4.01304e-05
	LOSS [training: 3.496538171093456 | validation: 5.251637276292635]
	TIME [epoch: 10.4 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495961877841129		[learning rate: 3.9985e-05]
	Learning Rate: 3.99848e-05
	LOSS [training: 3.495961877841129 | validation: 5.241984069509787]
	TIME [epoch: 10.4 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4954221080820624		[learning rate: 3.984e-05]
	Learning Rate: 3.98397e-05
	LOSS [training: 3.4954221080820624 | validation: 5.239641467633368]
	TIME [epoch: 10.4 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495211250495152		[learning rate: 3.9695e-05]
	Learning Rate: 3.96951e-05
	LOSS [training: 3.495211250495152 | validation: 5.235703528505328]
	TIME [epoch: 10.4 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496793264155644		[learning rate: 3.9551e-05]
	Learning Rate: 3.9551e-05
	LOSS [training: 3.496793264155644 | validation: 5.252339357995829]
	TIME [epoch: 10.4 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963699027157964		[learning rate: 3.9408e-05]
	Learning Rate: 3.94075e-05
	LOSS [training: 3.4963699027157964 | validation: 5.239346231206309]
	TIME [epoch: 10.4 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4959295625170457		[learning rate: 3.9264e-05]
	Learning Rate: 3.92645e-05
	LOSS [training: 3.4959295625170457 | validation: 5.250733372482808]
	TIME [epoch: 10.4 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4979505674649767		[learning rate: 3.9122e-05]
	Learning Rate: 3.9122e-05
	LOSS [training: 3.4979505674649767 | validation: 5.236604495715884]
	TIME [epoch: 10.4 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4950698082979264		[learning rate: 3.898e-05]
	Learning Rate: 3.898e-05
	LOSS [training: 3.4950698082979264 | validation: 5.239309923897748]
	TIME [epoch: 10.4 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961282756299177		[learning rate: 3.8839e-05]
	Learning Rate: 3.88386e-05
	LOSS [training: 3.4961282756299177 | validation: 5.227093929439117]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_1627.pth
	Model improved!!!
EPOCH 1628/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4932526322205035		[learning rate: 3.8698e-05]
	Learning Rate: 3.86976e-05
	LOSS [training: 3.4932526322205035 | validation: 5.23628340889788]
	TIME [epoch: 10.4 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4938353234176143		[learning rate: 3.8557e-05]
	Learning Rate: 3.85572e-05
	LOSS [training: 3.4938353234176143 | validation: 5.250152162072655]
	TIME [epoch: 10.4 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497886817144225		[learning rate: 3.8417e-05]
	Learning Rate: 3.84173e-05
	LOSS [training: 3.497886817144225 | validation: 5.243320241469812]
	TIME [epoch: 10.4 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953404931655307		[learning rate: 3.8278e-05]
	Learning Rate: 3.82778e-05
	LOSS [training: 3.4953404931655307 | validation: 5.239160460521821]
	TIME [epoch: 10.4 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969441103120067		[learning rate: 3.8139e-05]
	Learning Rate: 3.81389e-05
	LOSS [training: 3.4969441103120067 | validation: 5.2437515217773525]
	TIME [epoch: 10.4 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493131362634911		[learning rate: 3.8001e-05]
	Learning Rate: 3.80005e-05
	LOSS [training: 3.493131362634911 | validation: 5.236548173906563]
	TIME [epoch: 10.4 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4986599090672614		[learning rate: 3.7863e-05]
	Learning Rate: 3.78626e-05
	LOSS [training: 3.4986599090672614 | validation: 5.239940307442983]
	TIME [epoch: 10.4 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982821620708338		[learning rate: 3.7725e-05]
	Learning Rate: 3.77252e-05
	LOSS [training: 3.4982821620708338 | validation: 5.251880536121031]
	TIME [epoch: 10.4 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963456501500128		[learning rate: 3.7588e-05]
	Learning Rate: 3.75883e-05
	LOSS [training: 3.4963456501500128 | validation: 5.239429455016807]
	TIME [epoch: 10.4 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498718973689477		[learning rate: 3.7452e-05]
	Learning Rate: 3.74519e-05
	LOSS [training: 3.498718973689477 | validation: 5.24623402686341]
	TIME [epoch: 10.4 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494213044622492		[learning rate: 3.7316e-05]
	Learning Rate: 3.7316e-05
	LOSS [training: 3.494213044622492 | validation: 5.233960525609759]
	TIME [epoch: 10.4 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4930733638062734		[learning rate: 3.7181e-05]
	Learning Rate: 3.71805e-05
	LOSS [training: 3.4930733638062734 | validation: 5.239245588963008]
	TIME [epoch: 10.4 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968808760463235		[learning rate: 3.7046e-05]
	Learning Rate: 3.70456e-05
	LOSS [training: 3.4968808760463235 | validation: 5.246753741833183]
	TIME [epoch: 10.4 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494500600783758		[learning rate: 3.6911e-05]
	Learning Rate: 3.69112e-05
	LOSS [training: 3.494500600783758 | validation: 5.25257390520301]
	TIME [epoch: 10.4 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497536051451404		[learning rate: 3.6777e-05]
	Learning Rate: 3.67772e-05
	LOSS [training: 3.497536051451404 | validation: 5.245757702765878]
	TIME [epoch: 10.4 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4996639727194365		[learning rate: 3.6644e-05]
	Learning Rate: 3.66438e-05
	LOSS [training: 3.4996639727194365 | validation: 5.239054579218527]
	TIME [epoch: 10.4 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496011205550952		[learning rate: 3.6511e-05]
	Learning Rate: 3.65108e-05
	LOSS [training: 3.496011205550952 | validation: 5.243929731159432]
	TIME [epoch: 10.4 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496459918781853		[learning rate: 3.6378e-05]
	Learning Rate: 3.63783e-05
	LOSS [training: 3.496459918781853 | validation: 5.2461321491496085]
	TIME [epoch: 10.4 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5014879775861836		[learning rate: 3.6246e-05]
	Learning Rate: 3.62463e-05
	LOSS [training: 3.5014879775861836 | validation: 5.247420666080788]
	TIME [epoch: 10.4 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498838605844939		[learning rate: 3.6115e-05]
	Learning Rate: 3.61147e-05
	LOSS [training: 3.498838605844939 | validation: 5.238603860795104]
	TIME [epoch: 10.4 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494156121782086		[learning rate: 3.5984e-05]
	Learning Rate: 3.59837e-05
	LOSS [training: 3.494156121782086 | validation: 5.243341695702274]
	TIME [epoch: 10.4 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492292676777595		[learning rate: 3.5853e-05]
	Learning Rate: 3.58531e-05
	LOSS [training: 3.492292676777595 | validation: 5.2369130752751225]
	TIME [epoch: 10.4 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.488316874483837		[learning rate: 3.5723e-05]
	Learning Rate: 3.5723e-05
	LOSS [training: 3.488316874483837 | validation: 5.237847375976598]
	TIME [epoch: 10.4 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495496147798929		[learning rate: 3.5593e-05]
	Learning Rate: 3.55933e-05
	LOSS [training: 3.495496147798929 | validation: 5.23837844410597]
	TIME [epoch: 10.4 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4949089463858156		[learning rate: 3.5464e-05]
	Learning Rate: 3.54641e-05
	LOSS [training: 3.4949089463858156 | validation: 5.228677536239906]
	TIME [epoch: 10.4 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497438283525409		[learning rate: 3.5335e-05]
	Learning Rate: 3.53354e-05
	LOSS [training: 3.497438283525409 | validation: 5.251337130503578]
	TIME [epoch: 10.4 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989719276394724		[learning rate: 3.5207e-05]
	Learning Rate: 3.52072e-05
	LOSS [training: 3.4989719276394724 | validation: 5.232267389442941]
	TIME [epoch: 10.4 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4933091675140195		[learning rate: 3.5079e-05]
	Learning Rate: 3.50794e-05
	LOSS [training: 3.4933091675140195 | validation: 5.238938940561213]
	TIME [epoch: 10.4 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4970689932084698		[learning rate: 3.4952e-05]
	Learning Rate: 3.49521e-05
	LOSS [training: 3.4970689932084698 | validation: 5.23154055668758]
	TIME [epoch: 10.4 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4888123183383897		[learning rate: 3.4825e-05]
	Learning Rate: 3.48253e-05
	LOSS [training: 3.4888123183383897 | validation: 5.237304606216339]
	TIME [epoch: 10.4 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4927654689499557		[learning rate: 3.4699e-05]
	Learning Rate: 3.46989e-05
	LOSS [training: 3.4927654689499557 | validation: 5.2353086177615085]
	TIME [epoch: 10.4 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4970318613087983		[learning rate: 3.4573e-05]
	Learning Rate: 3.4573e-05
	LOSS [training: 3.4970318613087983 | validation: 5.239705821958962]
	TIME [epoch: 10.4 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972559834914407		[learning rate: 3.4448e-05]
	Learning Rate: 3.44475e-05
	LOSS [training: 3.4972559834914407 | validation: 5.239711764076146]
	TIME [epoch: 10.4 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4898328122854645		[learning rate: 3.4323e-05]
	Learning Rate: 3.43225e-05
	LOSS [training: 3.4898328122854645 | validation: 5.24114406633134]
	TIME [epoch: 10.4 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498128600446664		[learning rate: 3.4198e-05]
	Learning Rate: 3.4198e-05
	LOSS [training: 3.498128600446664 | validation: 5.243870264229822]
	TIME [epoch: 10.4 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4973481657260166		[learning rate: 3.4074e-05]
	Learning Rate: 3.40738e-05
	LOSS [training: 3.4973481657260166 | validation: 5.23886708204655]
	TIME [epoch: 10.4 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4939770492368645		[learning rate: 3.395e-05]
	Learning Rate: 3.39502e-05
	LOSS [training: 3.4939770492368645 | validation: 5.241106323746412]
	TIME [epoch: 10.4 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497575937976729		[learning rate: 3.3827e-05]
	Learning Rate: 3.3827e-05
	LOSS [training: 3.497575937976729 | validation: 5.243994569912728]
	TIME [epoch: 10.4 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49133304753951		[learning rate: 3.3704e-05]
	Learning Rate: 3.37042e-05
	LOSS [training: 3.49133304753951 | validation: 5.238688053380367]
	TIME [epoch: 10.4 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49683285594337		[learning rate: 3.3582e-05]
	Learning Rate: 3.35819e-05
	LOSS [training: 3.49683285594337 | validation: 5.235325941961128]
	TIME [epoch: 10.4 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495059355875958		[learning rate: 3.346e-05]
	Learning Rate: 3.346e-05
	LOSS [training: 3.495059355875958 | validation: 5.245578168960588]
	TIME [epoch: 10.4 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495453838537329		[learning rate: 3.3339e-05]
	Learning Rate: 3.33386e-05
	LOSS [training: 3.495453838537329 | validation: 5.2364618981513615]
	TIME [epoch: 10.4 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495440776057245		[learning rate: 3.3218e-05]
	Learning Rate: 3.32176e-05
	LOSS [training: 3.495440776057245 | validation: 5.246228031785222]
	TIME [epoch: 10.4 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4912078514491798		[learning rate: 3.3097e-05]
	Learning Rate: 3.30971e-05
	LOSS [training: 3.4912078514491798 | validation: 5.23433911229824]
	TIME [epoch: 10.4 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953059957818184		[learning rate: 3.2977e-05]
	Learning Rate: 3.2977e-05
	LOSS [training: 3.4953059957818184 | validation: 5.2441388472760275]
	TIME [epoch: 10.4 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498648987065967		[learning rate: 3.2857e-05]
	Learning Rate: 3.28573e-05
	LOSS [training: 3.498648987065967 | validation: 5.236925390118854]
	TIME [epoch: 10.4 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4974800537556745		[learning rate: 3.2738e-05]
	Learning Rate: 3.2738e-05
	LOSS [training: 3.4974800537556745 | validation: 5.2401529693817945]
	TIME [epoch: 10.4 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496960382178795		[learning rate: 3.2619e-05]
	Learning Rate: 3.26192e-05
	LOSS [training: 3.496960382178795 | validation: 5.23383119556122]
	TIME [epoch: 10.4 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983094991601327		[learning rate: 3.2501e-05]
	Learning Rate: 3.25009e-05
	LOSS [training: 3.4983094991601327 | validation: 5.2321849772526114]
	TIME [epoch: 10.4 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4965556892102567		[learning rate: 3.2383e-05]
	Learning Rate: 3.23829e-05
	LOSS [training: 3.4965556892102567 | validation: 5.25827752805407]
	TIME [epoch: 10.4 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494629203380813		[learning rate: 3.2265e-05]
	Learning Rate: 3.22654e-05
	LOSS [training: 3.494629203380813 | validation: 5.244333634001556]
	TIME [epoch: 10.4 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4955546389091396		[learning rate: 3.2148e-05]
	Learning Rate: 3.21483e-05
	LOSS [training: 3.4955546389091396 | validation: 5.233159149362116]
	TIME [epoch: 10.4 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4985941645448038		[learning rate: 3.2032e-05]
	Learning Rate: 3.20316e-05
	LOSS [training: 3.4985941645448038 | validation: 5.23874044135506]
	TIME [epoch: 10.4 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494919571774088		[learning rate: 3.1915e-05]
	Learning Rate: 3.19154e-05
	LOSS [training: 3.494919571774088 | validation: 5.237055925007262]
	TIME [epoch: 10.4 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491866136604004		[learning rate: 3.18e-05]
	Learning Rate: 3.17996e-05
	LOSS [training: 3.491866136604004 | validation: 5.243007312841549]
	TIME [epoch: 10.4 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495543098739136		[learning rate: 3.1684e-05]
	Learning Rate: 3.16842e-05
	LOSS [training: 3.495543098739136 | validation: 5.235791441826714]
	TIME [epoch: 10.4 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4988974322299184		[learning rate: 3.1569e-05]
	Learning Rate: 3.15692e-05
	LOSS [training: 3.4988974322299184 | validation: 5.239519515005672]
	TIME [epoch: 10.4 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5003658739579167		[learning rate: 3.1455e-05]
	Learning Rate: 3.14546e-05
	LOSS [training: 3.5003658739579167 | validation: 5.24706129994951]
	TIME [epoch: 10.4 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991183022050256		[learning rate: 3.134e-05]
	Learning Rate: 3.13405e-05
	LOSS [training: 3.4991183022050256 | validation: 5.244030251358598]
	TIME [epoch: 10.4 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49520119039353		[learning rate: 3.1227e-05]
	Learning Rate: 3.12267e-05
	LOSS [training: 3.49520119039353 | validation: 5.240975038688329]
	TIME [epoch: 10.4 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4974795455930234		[learning rate: 3.1113e-05]
	Learning Rate: 3.11134e-05
	LOSS [training: 3.4974795455930234 | validation: 5.244422995335959]
	TIME [epoch: 10.4 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497462814380417		[learning rate: 3.1e-05]
	Learning Rate: 3.10005e-05
	LOSS [training: 3.497462814380417 | validation: 5.238868877801065]
	TIME [epoch: 10.4 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971054235289634		[learning rate: 3.0888e-05]
	Learning Rate: 3.0888e-05
	LOSS [training: 3.4971054235289634 | validation: 5.228810283528926]
	TIME [epoch: 10.4 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4951097404971536		[learning rate: 3.0776e-05]
	Learning Rate: 3.07759e-05
	LOSS [training: 3.4951097404971536 | validation: 5.233427653486493]
	TIME [epoch: 10.4 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495686767688361		[learning rate: 3.0664e-05]
	Learning Rate: 3.06642e-05
	LOSS [training: 3.495686767688361 | validation: 5.233362700238019]
	TIME [epoch: 10.4 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966437565542647		[learning rate: 3.0553e-05]
	Learning Rate: 3.05529e-05
	LOSS [training: 3.4966437565542647 | validation: 5.238635953210011]
	TIME [epoch: 10.4 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493610002294522		[learning rate: 3.0442e-05]
	Learning Rate: 3.0442e-05
	LOSS [training: 3.493610002294522 | validation: 5.241381527173331]
	TIME [epoch: 10.4 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496837215728285		[learning rate: 3.0332e-05]
	Learning Rate: 3.03316e-05
	LOSS [training: 3.496837215728285 | validation: 5.241743405567555]
	TIME [epoch: 10.4 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497152698917007		[learning rate: 3.0221e-05]
	Learning Rate: 3.02215e-05
	LOSS [training: 3.497152698917007 | validation: 5.239989952440039]
	TIME [epoch: 10.4 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495233623169848		[learning rate: 3.0112e-05]
	Learning Rate: 3.01118e-05
	LOSS [training: 3.495233623169848 | validation: 5.234748754062395]
	TIME [epoch: 10.4 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4911185513453704		[learning rate: 3.0003e-05]
	Learning Rate: 3.00025e-05
	LOSS [training: 3.4911185513453704 | validation: 5.246483052547746]
	TIME [epoch: 10.4 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493795896493812		[learning rate: 2.9894e-05]
	Learning Rate: 2.98936e-05
	LOSS [training: 3.493795896493812 | validation: 5.234106196925691]
	TIME [epoch: 10.4 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4950786565751586		[learning rate: 2.9785e-05]
	Learning Rate: 2.97852e-05
	LOSS [training: 3.4950786565751586 | validation: 5.241045594402517]
	TIME [epoch: 10.4 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494026672443411		[learning rate: 2.9677e-05]
	Learning Rate: 2.96771e-05
	LOSS [training: 3.494026672443411 | validation: 5.242790198293083]
	TIME [epoch: 10.4 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495311325478511		[learning rate: 2.9569e-05]
	Learning Rate: 2.95694e-05
	LOSS [training: 3.495311325478511 | validation: 5.232029235970904]
	TIME [epoch: 10.4 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4989140503776257		[learning rate: 2.9462e-05]
	Learning Rate: 2.94621e-05
	LOSS [training: 3.4989140503776257 | validation: 5.240409214290139]
	TIME [epoch: 10.4 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49655760483173		[learning rate: 2.9355e-05]
	Learning Rate: 2.93551e-05
	LOSS [training: 3.49655760483173 | validation: 5.232879725954622]
	TIME [epoch: 10.4 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976759727800575		[learning rate: 2.9249e-05]
	Learning Rate: 2.92486e-05
	LOSS [training: 3.4976759727800575 | validation: 5.237231215986129]
	TIME [epoch: 10.4 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4925729390034386		[learning rate: 2.9142e-05]
	Learning Rate: 2.91425e-05
	LOSS [training: 3.4925729390034386 | validation: 5.233130945617423]
	TIME [epoch: 10.4 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501919826697204		[learning rate: 2.9037e-05]
	Learning Rate: 2.90367e-05
	LOSS [training: 3.501919826697204 | validation: 5.236426176507937]
	TIME [epoch: 10.4 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497257729305268		[learning rate: 2.8931e-05]
	Learning Rate: 2.89313e-05
	LOSS [training: 3.497257729305268 | validation: 5.232649841572234]
	TIME [epoch: 10.4 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982655391349575		[learning rate: 2.8826e-05]
	Learning Rate: 2.88263e-05
	LOSS [training: 3.4982655391349575 | validation: 5.245453904467936]
	TIME [epoch: 10.4 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4921334708590948		[learning rate: 2.8722e-05]
	Learning Rate: 2.87217e-05
	LOSS [training: 3.4921334708590948 | validation: 5.239778177881647]
	TIME [epoch: 10.4 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4938793054368262		[learning rate: 2.8617e-05]
	Learning Rate: 2.86175e-05
	LOSS [training: 3.4938793054368262 | validation: 5.241734178955223]
	TIME [epoch: 10.4 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496005734333088		[learning rate: 2.8514e-05]
	Learning Rate: 2.85136e-05
	LOSS [training: 3.496005734333088 | validation: 5.244100765419957]
	TIME [epoch: 10.4 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492350447485792		[learning rate: 2.841e-05]
	Learning Rate: 2.84102e-05
	LOSS [training: 3.492350447485792 | validation: 5.231596143868678]
	TIME [epoch: 10.4 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500685148189666		[learning rate: 2.8307e-05]
	Learning Rate: 2.83071e-05
	LOSS [training: 3.500685148189666 | validation: 5.236550138578137]
	TIME [epoch: 10.4 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493603198133397		[learning rate: 2.8204e-05]
	Learning Rate: 2.82043e-05
	LOSS [training: 3.493603198133397 | validation: 5.248358940403425]
	TIME [epoch: 10.4 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971512219493817		[learning rate: 2.8102e-05]
	Learning Rate: 2.8102e-05
	LOSS [training: 3.4971512219493817 | validation: 5.244888814200622]
	TIME [epoch: 10.4 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990229981186567		[learning rate: 2.8e-05]
	Learning Rate: 2.8e-05
	LOSS [training: 3.4990229981186567 | validation: 5.2382265303748365]
	TIME [epoch: 10.4 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961122382519116		[learning rate: 2.7898e-05]
	Learning Rate: 2.78984e-05
	LOSS [training: 3.4961122382519116 | validation: 5.233454550212964]
	TIME [epoch: 10.4 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497820020333053		[learning rate: 2.7797e-05]
	Learning Rate: 2.77971e-05
	LOSS [training: 3.497820020333053 | validation: 5.249635711880713]
	TIME [epoch: 10.4 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966001172338337		[learning rate: 2.7696e-05]
	Learning Rate: 2.76963e-05
	LOSS [training: 3.4966001172338337 | validation: 5.235490124500959]
	TIME [epoch: 10.4 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4945719122520793		[learning rate: 2.7596e-05]
	Learning Rate: 2.75957e-05
	LOSS [training: 3.4945719122520793 | validation: 5.240173819836559]
	TIME [epoch: 10.4 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952353189770813		[learning rate: 2.7496e-05]
	Learning Rate: 2.74956e-05
	LOSS [training: 3.4952353189770813 | validation: 5.243331761384]
	TIME [epoch: 10.4 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4926964634348074		[learning rate: 2.7396e-05]
	Learning Rate: 2.73958e-05
	LOSS [training: 3.4926964634348074 | validation: 5.238243978771184]
	TIME [epoch: 10.4 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498083948392906		[learning rate: 2.7296e-05]
	Learning Rate: 2.72964e-05
	LOSS [training: 3.498083948392906 | validation: 5.241418483454229]
	TIME [epoch: 10.4 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4936222439781597		[learning rate: 2.7197e-05]
	Learning Rate: 2.71973e-05
	LOSS [training: 3.4936222439781597 | validation: 5.23971059640783]
	TIME [epoch: 10.4 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495755293572362		[learning rate: 2.7099e-05]
	Learning Rate: 2.70986e-05
	LOSS [training: 3.495755293572362 | validation: 5.233573775742679]
	TIME [epoch: 10.4 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493437681126605		[learning rate: 2.7e-05]
	Learning Rate: 2.70003e-05
	LOSS [training: 3.493437681126605 | validation: 5.234540924355956]
	TIME [epoch: 10.4 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491131120550923		[learning rate: 2.6902e-05]
	Learning Rate: 2.69023e-05
	LOSS [training: 3.491131120550923 | validation: 5.234638677284409]
	TIME [epoch: 10.4 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4922085550276436		[learning rate: 2.6805e-05]
	Learning Rate: 2.68047e-05
	LOSS [training: 3.4922085550276436 | validation: 5.232047484293155]
	TIME [epoch: 10.4 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4944154057410977		[learning rate: 2.6707e-05]
	Learning Rate: 2.67074e-05
	LOSS [training: 3.4944154057410977 | validation: 5.235113236441434]
	TIME [epoch: 10.4 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976261561231303		[learning rate: 2.661e-05]
	Learning Rate: 2.66105e-05
	LOSS [training: 3.4976261561231303 | validation: 5.230169640035512]
	TIME [epoch: 10.4 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4913932085617803		[learning rate: 2.6514e-05]
	Learning Rate: 2.65139e-05
	LOSS [training: 3.4913932085617803 | validation: 5.244616732602062]
	TIME [epoch: 10.4 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968035552463093		[learning rate: 2.6418e-05]
	Learning Rate: 2.64177e-05
	LOSS [training: 3.4968035552463093 | validation: 5.2479038198205785]
	TIME [epoch: 10.4 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499783363421412		[learning rate: 2.6322e-05]
	Learning Rate: 2.63218e-05
	LOSS [training: 3.499783363421412 | validation: 5.242383661009353]
	TIME [epoch: 10.4 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493855451984074		[learning rate: 2.6226e-05]
	Learning Rate: 2.62263e-05
	LOSS [training: 3.493855451984074 | validation: 5.24726684902203]
	TIME [epoch: 10.3 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494767882110949		[learning rate: 2.6131e-05]
	Learning Rate: 2.61311e-05
	LOSS [training: 3.494767882110949 | validation: 5.246501255187649]
	TIME [epoch: 10.4 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492585575775057		[learning rate: 2.6036e-05]
	Learning Rate: 2.60363e-05
	LOSS [training: 3.492585575775057 | validation: 5.239314848782972]
	TIME [epoch: 10.4 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495440025313532		[learning rate: 2.5942e-05]
	Learning Rate: 2.59418e-05
	LOSS [training: 3.495440025313532 | validation: 5.2359296459147675]
	TIME [epoch: 10.4 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491952410954881		[learning rate: 2.5848e-05]
	Learning Rate: 2.58477e-05
	LOSS [training: 3.491952410954881 | validation: 5.238177180848862]
	TIME [epoch: 10.4 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497788593685705		[learning rate: 2.5754e-05]
	Learning Rate: 2.57539e-05
	LOSS [training: 3.497788593685705 | validation: 5.22967560255628]
	TIME [epoch: 10.4 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497461954165496		[learning rate: 2.566e-05]
	Learning Rate: 2.56604e-05
	LOSS [training: 3.497461954165496 | validation: 5.243656890289308]
	TIME [epoch: 10.4 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500162571155607		[learning rate: 2.5567e-05]
	Learning Rate: 2.55673e-05
	LOSS [training: 3.500162571155607 | validation: 5.238249033562214]
	TIME [epoch: 10.4 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983726846540506		[learning rate: 2.5474e-05]
	Learning Rate: 2.54745e-05
	LOSS [training: 3.4983726846540506 | validation: 5.242208696533245]
	TIME [epoch: 10.4 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4903640004047247		[learning rate: 2.5382e-05]
	Learning Rate: 2.5382e-05
	LOSS [training: 3.4903640004047247 | validation: 5.235816668645314]
	TIME [epoch: 10.4 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4956710546328758		[learning rate: 2.529e-05]
	Learning Rate: 2.52899e-05
	LOSS [training: 3.4956710546328758 | validation: 5.236222481209796]
	TIME [epoch: 10.4 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500140085862513		[learning rate: 2.5198e-05]
	Learning Rate: 2.51981e-05
	LOSS [training: 3.500140085862513 | validation: 5.24851070197847]
	TIME [epoch: 10.4 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4950470660513595		[learning rate: 2.5107e-05]
	Learning Rate: 2.51067e-05
	LOSS [training: 3.4950470660513595 | validation: 5.2444658934881305]
	TIME [epoch: 10.4 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501678536119138		[learning rate: 2.5016e-05]
	Learning Rate: 2.50156e-05
	LOSS [training: 3.501678536119138 | validation: 5.235568838372069]
	TIME [epoch: 10.4 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500672523075072		[learning rate: 2.4925e-05]
	Learning Rate: 2.49248e-05
	LOSS [training: 3.500672523075072 | validation: 5.244974575408142]
	TIME [epoch: 10.4 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496020575043633		[learning rate: 2.4834e-05]
	Learning Rate: 2.48343e-05
	LOSS [training: 3.496020575043633 | validation: 5.251270799155577]
	TIME [epoch: 10.4 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500984696392287		[learning rate: 2.4744e-05]
	Learning Rate: 2.47442e-05
	LOSS [training: 3.500984696392287 | validation: 5.235014351056325]
	TIME [epoch: 10.4 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983868294610803		[learning rate: 2.4654e-05]
	Learning Rate: 2.46544e-05
	LOSS [training: 3.4983868294610803 | validation: 5.2424740108163395]
	TIME [epoch: 10.4 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496811248295687		[learning rate: 2.4565e-05]
	Learning Rate: 2.45649e-05
	LOSS [training: 3.496811248295687 | validation: 5.241384663859371]
	TIME [epoch: 10.4 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497484982267463		[learning rate: 2.4476e-05]
	Learning Rate: 2.44758e-05
	LOSS [training: 3.497484982267463 | validation: 5.239740505723214]
	TIME [epoch: 10.4 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972790381488137		[learning rate: 2.4387e-05]
	Learning Rate: 2.4387e-05
	LOSS [training: 3.4972790381488137 | validation: 5.238644126726001]
	TIME [epoch: 10.4 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4926010920180124		[learning rate: 2.4298e-05]
	Learning Rate: 2.42985e-05
	LOSS [training: 3.4926010920180124 | validation: 5.2383180990753635]
	TIME [epoch: 10.4 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963797598545456		[learning rate: 2.421e-05]
	Learning Rate: 2.42103e-05
	LOSS [training: 3.4963797598545456 | validation: 5.241713896141905]
	TIME [epoch: 10.4 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963600747224035		[learning rate: 2.4122e-05]
	Learning Rate: 2.41224e-05
	LOSS [training: 3.4963600747224035 | validation: 5.238162275309799]
	TIME [epoch: 10.4 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5004828300075572		[learning rate: 2.4035e-05]
	Learning Rate: 2.40349e-05
	LOSS [training: 3.5004828300075572 | validation: 5.2456051749682295]
	TIME [epoch: 10.4 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496685908241168		[learning rate: 2.3948e-05]
	Learning Rate: 2.39477e-05
	LOSS [training: 3.496685908241168 | validation: 5.233471045587483]
	TIME [epoch: 10.4 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495783021407912		[learning rate: 2.3861e-05]
	Learning Rate: 2.38608e-05
	LOSS [training: 3.495783021407912 | validation: 5.24719847503293]
	TIME [epoch: 10.4 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500582391921278		[learning rate: 2.3774e-05]
	Learning Rate: 2.37742e-05
	LOSS [training: 3.500582391921278 | validation: 5.2454379120124806]
	TIME [epoch: 10.4 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4957282098842013		[learning rate: 2.3688e-05]
	Learning Rate: 2.36879e-05
	LOSS [training: 3.4957282098842013 | validation: 5.238741651778658]
	TIME [epoch: 10.4 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4986741849919545		[learning rate: 2.3602e-05]
	Learning Rate: 2.36019e-05
	LOSS [training: 3.4986741849919545 | validation: 5.236917598240491]
	TIME [epoch: 10.4 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495910182514735		[learning rate: 2.3516e-05]
	Learning Rate: 2.35163e-05
	LOSS [training: 3.495910182514735 | validation: 5.2400452021292585]
	TIME [epoch: 10.4 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497546794710158		[learning rate: 2.3431e-05]
	Learning Rate: 2.34309e-05
	LOSS [training: 3.497546794710158 | validation: 5.239694592644094]
	TIME [epoch: 10.4 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953662496537428		[learning rate: 2.3346e-05]
	Learning Rate: 2.33459e-05
	LOSS [training: 3.4953662496537428 | validation: 5.241951171348072]
	TIME [epoch: 10.4 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495648373711542		[learning rate: 2.3261e-05]
	Learning Rate: 2.32612e-05
	LOSS [training: 3.495648373711542 | validation: 5.238659343409291]
	TIME [epoch: 10.4 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969294034408582		[learning rate: 2.3177e-05]
	Learning Rate: 2.31768e-05
	LOSS [training: 3.4969294034408582 | validation: 5.239825569399641]
	TIME [epoch: 10.4 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501774231829394		[learning rate: 2.3093e-05]
	Learning Rate: 2.30926e-05
	LOSS [training: 3.501774231829394 | validation: 5.23484168356144]
	TIME [epoch: 10.4 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49642921763184		[learning rate: 2.3009e-05]
	Learning Rate: 2.30088e-05
	LOSS [training: 3.49642921763184 | validation: 5.232411342864875]
	TIME [epoch: 10.4 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987628758884695		[learning rate: 2.2925e-05]
	Learning Rate: 2.29253e-05
	LOSS [training: 3.4987628758884695 | validation: 5.238465295649546]
	TIME [epoch: 10.4 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501607141480348		[learning rate: 2.2842e-05]
	Learning Rate: 2.28421e-05
	LOSS [training: 3.501607141480348 | validation: 5.243759862794027]
	TIME [epoch: 10.4 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4932568568214633		[learning rate: 2.2759e-05]
	Learning Rate: 2.27592e-05
	LOSS [training: 3.4932568568214633 | validation: 5.240468947230697]
	TIME [epoch: 10.4 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4940531028321984		[learning rate: 2.2677e-05]
	Learning Rate: 2.26767e-05
	LOSS [training: 3.4940531028321984 | validation: 5.23927951418659]
	TIME [epoch: 10.4 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4965468796169836		[learning rate: 2.2594e-05]
	Learning Rate: 2.25944e-05
	LOSS [training: 3.4965468796169836 | validation: 5.247685226107045]
	TIME [epoch: 10.4 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4947968437025274		[learning rate: 2.2512e-05]
	Learning Rate: 2.25124e-05
	LOSS [training: 3.4947968437025274 | validation: 5.254926491079459]
	TIME [epoch: 10.4 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495510158760804		[learning rate: 2.2431e-05]
	Learning Rate: 2.24307e-05
	LOSS [training: 3.495510158760804 | validation: 5.238051090077595]
	TIME [epoch: 10.4 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498791515572794		[learning rate: 2.2349e-05]
	Learning Rate: 2.23493e-05
	LOSS [training: 3.498791515572794 | validation: 5.2412024884524895]
	TIME [epoch: 10.4 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990919445836384		[learning rate: 2.2268e-05]
	Learning Rate: 2.22682e-05
	LOSS [training: 3.4990919445836384 | validation: 5.2363816145897095]
	TIME [epoch: 10.4 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4940890848814545		[learning rate: 2.2187e-05]
	Learning Rate: 2.21873e-05
	LOSS [training: 3.4940890848814545 | validation: 5.23939218405958]
	TIME [epoch: 10.4 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492909229473489		[learning rate: 2.2107e-05]
	Learning Rate: 2.21068e-05
	LOSS [training: 3.492909229473489 | validation: 5.245225884584673]
	TIME [epoch: 10.4 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971910080791098		[learning rate: 2.2027e-05]
	Learning Rate: 2.20266e-05
	LOSS [training: 3.4971910080791098 | validation: 5.235845181386513]
	TIME [epoch: 10.4 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4962069542942493		[learning rate: 2.1947e-05]
	Learning Rate: 2.19467e-05
	LOSS [training: 3.4962069542942493 | validation: 5.239369866891578]
	TIME [epoch: 10.4 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4988389987551534		[learning rate: 2.1867e-05]
	Learning Rate: 2.1867e-05
	LOSS [training: 3.4988389987551534 | validation: 5.251351016852477]
	TIME [epoch: 10.4 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4998287815989686		[learning rate: 2.1788e-05]
	Learning Rate: 2.17877e-05
	LOSS [training: 3.4998287815989686 | validation: 5.237673129003217]
	TIME [epoch: 10.4 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49498002099687		[learning rate: 2.1709e-05]
	Learning Rate: 2.17086e-05
	LOSS [training: 3.49498002099687 | validation: 5.241312255332612]
	TIME [epoch: 10.4 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5007165081284675		[learning rate: 2.163e-05]
	Learning Rate: 2.16298e-05
	LOSS [training: 3.5007165081284675 | validation: 5.239907030394028]
	TIME [epoch: 10.4 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999136849118444		[learning rate: 2.1551e-05]
	Learning Rate: 2.15513e-05
	LOSS [training: 3.4999136849118444 | validation: 5.242240760514392]
	TIME [epoch: 10.4 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4898975221545676		[learning rate: 2.1473e-05]
	Learning Rate: 2.14731e-05
	LOSS [training: 3.4898975221545676 | validation: 5.238816285424763]
	TIME [epoch: 10.4 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491437974252731		[learning rate: 2.1395e-05]
	Learning Rate: 2.13952e-05
	LOSS [training: 3.491437974252731 | validation: 5.229888479362317]
	TIME [epoch: 10.4 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495969972655428		[learning rate: 2.1318e-05]
	Learning Rate: 2.13175e-05
	LOSS [training: 3.495969972655428 | validation: 5.243635162796265]
	TIME [epoch: 10.4 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497879335112816		[learning rate: 2.124e-05]
	Learning Rate: 2.12402e-05
	LOSS [training: 3.497879335112816 | validation: 5.247999065388159]
	TIME [epoch: 10.4 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958551588099964		[learning rate: 2.1163e-05]
	Learning Rate: 2.11631e-05
	LOSS [training: 3.4958551588099964 | validation: 5.24716165992087]
	TIME [epoch: 10.4 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49481159146776		[learning rate: 2.1086e-05]
	Learning Rate: 2.10863e-05
	LOSS [training: 3.49481159146776 | validation: 5.241436788525777]
	TIME [epoch: 10.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4972606397098254		[learning rate: 2.101e-05]
	Learning Rate: 2.10098e-05
	LOSS [training: 3.4972606397098254 | validation: 5.238850012490336]
	TIME [epoch: 10.4 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4946918278905854		[learning rate: 2.0934e-05]
	Learning Rate: 2.09335e-05
	LOSS [training: 3.4946918278905854 | validation: 5.2283524312277425]
	TIME [epoch: 10.4 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493599337495535		[learning rate: 2.0858e-05]
	Learning Rate: 2.08575e-05
	LOSS [training: 3.493599337495535 | validation: 5.241076921730037]
	TIME [epoch: 10.4 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4956973717655844		[learning rate: 2.0782e-05]
	Learning Rate: 2.07819e-05
	LOSS [training: 3.4956973717655844 | validation: 5.239299143307453]
	TIME [epoch: 10.4 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4941081023416216		[learning rate: 2.0706e-05]
	Learning Rate: 2.07064e-05
	LOSS [training: 3.4941081023416216 | validation: 5.239372181059582]
	TIME [epoch: 10.4 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4973720437171947		[learning rate: 2.0631e-05]
	Learning Rate: 2.06313e-05
	LOSS [training: 3.4973720437171947 | validation: 5.233542104464057]
	TIME [epoch: 10.4 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49841688841934		[learning rate: 2.0556e-05]
	Learning Rate: 2.05564e-05
	LOSS [training: 3.49841688841934 | validation: 5.239735523723319]
	TIME [epoch: 10.4 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4999502950688295		[learning rate: 2.0482e-05]
	Learning Rate: 2.04818e-05
	LOSS [training: 3.4999502950688295 | validation: 5.242278847996109]
	TIME [epoch: 10.4 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4928023847563714		[learning rate: 2.0407e-05]
	Learning Rate: 2.04075e-05
	LOSS [training: 3.4928023847563714 | validation: 5.233286519547895]
	TIME [epoch: 10.4 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969642047694363		[learning rate: 2.0333e-05]
	Learning Rate: 2.03334e-05
	LOSS [training: 3.4969642047694363 | validation: 5.23410463124917]
	TIME [epoch: 10.4 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492008883089467		[learning rate: 2.026e-05]
	Learning Rate: 2.02596e-05
	LOSS [training: 3.492008883089467 | validation: 5.235208203557396]
	TIME [epoch: 10.4 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4929935406086825		[learning rate: 2.0186e-05]
	Learning Rate: 2.01861e-05
	LOSS [training: 3.4929935406086825 | validation: 5.241532288222183]
	TIME [epoch: 10.4 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49490177035537		[learning rate: 2.0113e-05]
	Learning Rate: 2.01129e-05
	LOSS [training: 3.49490177035537 | validation: 5.239694121638173]
	TIME [epoch: 10.4 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4981673128601463		[learning rate: 2.004e-05]
	Learning Rate: 2.00399e-05
	LOSS [training: 3.4981673128601463 | validation: 5.233511042317004]
	TIME [epoch: 10.4 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497238565086415		[learning rate: 1.9967e-05]
	Learning Rate: 1.99671e-05
	LOSS [training: 3.497238565086415 | validation: 5.235720883980655]
	TIME [epoch: 10.4 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4959188914050863		[learning rate: 1.9895e-05]
	Learning Rate: 1.98947e-05
	LOSS [training: 3.4959188914050863 | validation: 5.242515266597547]
	TIME [epoch: 10.3 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960362450472835		[learning rate: 1.9822e-05]
	Learning Rate: 1.98225e-05
	LOSS [training: 3.4960362450472835 | validation: 5.23826796601903]
	TIME [epoch: 10.4 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4947281375928276		[learning rate: 1.9751e-05]
	Learning Rate: 1.97505e-05
	LOSS [training: 3.4947281375928276 | validation: 5.242602343509185]
	TIME [epoch: 10.4 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492766948518626		[learning rate: 1.9679e-05]
	Learning Rate: 1.96789e-05
	LOSS [training: 3.492766948518626 | validation: 5.2388562501857345]
	TIME [epoch: 10.4 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5008371366194715		[learning rate: 1.9607e-05]
	Learning Rate: 1.96074e-05
	LOSS [training: 3.5008371366194715 | validation: 5.244420084116463]
	TIME [epoch: 10.4 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4964550074133394		[learning rate: 1.9536e-05]
	Learning Rate: 1.95363e-05
	LOSS [training: 3.4964550074133394 | validation: 5.254780180954832]
	TIME [epoch: 10.4 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4932831593340326		[learning rate: 1.9465e-05]
	Learning Rate: 1.94654e-05
	LOSS [training: 3.4932831593340326 | validation: 5.248694989048907]
	TIME [epoch: 10.4 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966170552435516		[learning rate: 1.9395e-05]
	Learning Rate: 1.93948e-05
	LOSS [training: 3.4966170552435516 | validation: 5.242604657835292]
	TIME [epoch: 10.4 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494251869503041		[learning rate: 1.9324e-05]
	Learning Rate: 1.93244e-05
	LOSS [training: 3.494251869503041 | validation: 5.2386880681864865]
	TIME [epoch: 10.4 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4934163605916737		[learning rate: 1.9254e-05]
	Learning Rate: 1.92542e-05
	LOSS [training: 3.4934163605916737 | validation: 5.240141148476077]
	TIME [epoch: 10.4 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498010717334739		[learning rate: 1.9184e-05]
	Learning Rate: 1.91844e-05
	LOSS [training: 3.498010717334739 | validation: 5.2436142300631206]
	TIME [epoch: 10.4 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497022220720203		[learning rate: 1.9115e-05]
	Learning Rate: 1.91147e-05
	LOSS [training: 3.497022220720203 | validation: 5.236528255232502]
	TIME [epoch: 10.4 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975283783838096		[learning rate: 1.9045e-05]
	Learning Rate: 1.90454e-05
	LOSS [training: 3.4975283783838096 | validation: 5.236901963860586]
	TIME [epoch: 10.4 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.490850842999521		[learning rate: 1.8976e-05]
	Learning Rate: 1.89763e-05
	LOSS [training: 3.490850842999521 | validation: 5.2360584766766145]
	TIME [epoch: 10.4 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4959962181838455		[learning rate: 1.8907e-05]
	Learning Rate: 1.89074e-05
	LOSS [training: 3.4959962181838455 | validation: 5.23753761073466]
	TIME [epoch: 10.4 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961014411250617		[learning rate: 1.8839e-05]
	Learning Rate: 1.88388e-05
	LOSS [training: 3.4961014411250617 | validation: 5.2365548487483125]
	TIME [epoch: 10.4 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499666093568203		[learning rate: 1.877e-05]
	Learning Rate: 1.87704e-05
	LOSS [training: 3.499666093568203 | validation: 5.243570271220003]
	TIME [epoch: 10.3 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952896481372697		[learning rate: 1.8702e-05]
	Learning Rate: 1.87023e-05
	LOSS [training: 3.4952896481372697 | validation: 5.231537688059473]
	TIME [epoch: 10.4 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496265201437975		[learning rate: 1.8634e-05]
	Learning Rate: 1.86344e-05
	LOSS [training: 3.496265201437975 | validation: 5.22996004736772]
	TIME [epoch: 10.4 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497547498646854		[learning rate: 1.8567e-05]
	Learning Rate: 1.85668e-05
	LOSS [training: 3.497547498646854 | validation: 5.238212222723077]
	TIME [epoch: 10.4 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494524584417571		[learning rate: 1.8499e-05]
	Learning Rate: 1.84994e-05
	LOSS [training: 3.494524584417571 | validation: 5.245471978066054]
	TIME [epoch: 10.4 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4962096733499264		[learning rate: 1.8432e-05]
	Learning Rate: 1.84323e-05
	LOSS [training: 3.4962096733499264 | validation: 5.247133175292703]
	TIME [epoch: 10.4 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4934574233690534		[learning rate: 1.8365e-05]
	Learning Rate: 1.83654e-05
	LOSS [training: 3.4934574233690534 | validation: 5.236264635970092]
	TIME [epoch: 10.4 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498749523041039		[learning rate: 1.8299e-05]
	Learning Rate: 1.82987e-05
	LOSS [training: 3.498749523041039 | validation: 5.240786822854602]
	TIME [epoch: 10.4 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4908487512961175		[learning rate: 1.8232e-05]
	Learning Rate: 1.82323e-05
	LOSS [training: 3.4908487512961175 | validation: 5.24800015526331]
	TIME [epoch: 10.4 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497420527062734		[learning rate: 1.8166e-05]
	Learning Rate: 1.81662e-05
	LOSS [training: 3.497420527062734 | validation: 5.245051955614106]
	TIME [epoch: 10.4 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498429970206513		[learning rate: 1.81e-05]
	Learning Rate: 1.81002e-05
	LOSS [training: 3.498429970206513 | validation: 5.237903369382681]
	TIME [epoch: 10.4 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.490991121895456		[learning rate: 1.8035e-05]
	Learning Rate: 1.80346e-05
	LOSS [training: 3.490991121895456 | validation: 5.236953464231035]
	TIME [epoch: 10.4 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4910489791161785		[learning rate: 1.7969e-05]
	Learning Rate: 1.79691e-05
	LOSS [training: 3.4910489791161785 | validation: 5.23705678977188]
	TIME [epoch: 10.4 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960063635277385		[learning rate: 1.7904e-05]
	Learning Rate: 1.79039e-05
	LOSS [training: 3.4960063635277385 | validation: 5.251219687162277]
	TIME [epoch: 10.4 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4943901641172572		[learning rate: 1.7839e-05]
	Learning Rate: 1.78389e-05
	LOSS [training: 3.4943901641172572 | validation: 5.236338449211348]
	TIME [epoch: 10.4 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4892023106533054		[learning rate: 1.7774e-05]
	Learning Rate: 1.77742e-05
	LOSS [training: 3.4892023106533054 | validation: 5.236889120892292]
	TIME [epoch: 10.4 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493959902452771		[learning rate: 1.771e-05]
	Learning Rate: 1.77097e-05
	LOSS [training: 3.493959902452771 | validation: 5.246229056395054]
	TIME [epoch: 10.4 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953799786170903		[learning rate: 1.7645e-05]
	Learning Rate: 1.76454e-05
	LOSS [training: 3.4953799786170903 | validation: 5.230001323097443]
	TIME [epoch: 10.4 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953642259974407		[learning rate: 1.7581e-05]
	Learning Rate: 1.75814e-05
	LOSS [training: 3.4953642259974407 | validation: 5.238166070034854]
	TIME [epoch: 10.4 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493947692665305		[learning rate: 1.7518e-05]
	Learning Rate: 1.75176e-05
	LOSS [training: 3.493947692665305 | validation: 5.234942040450197]
	TIME [epoch: 10.4 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958613711412143		[learning rate: 1.7454e-05]
	Learning Rate: 1.7454e-05
	LOSS [training: 3.4958613711412143 | validation: 5.2331465891812154]
	TIME [epoch: 10.4 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494611400411967		[learning rate: 1.7391e-05]
	Learning Rate: 1.73907e-05
	LOSS [training: 3.494611400411967 | validation: 5.250314043338265]
	TIME [epoch: 10.4 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494920966159493		[learning rate: 1.7328e-05]
	Learning Rate: 1.73275e-05
	LOSS [training: 3.494920966159493 | validation: 5.233149962513373]
	TIME [epoch: 10.4 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960313129701914		[learning rate: 1.7265e-05]
	Learning Rate: 1.72647e-05
	LOSS [training: 3.4960313129701914 | validation: 5.232829360106345]
	TIME [epoch: 10.4 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496064121521612		[learning rate: 1.7202e-05]
	Learning Rate: 1.7202e-05
	LOSS [training: 3.496064121521612 | validation: 5.251556521098434]
	TIME [epoch: 10.4 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496124117749703		[learning rate: 1.714e-05]
	Learning Rate: 1.71396e-05
	LOSS [training: 3.496124117749703 | validation: 5.239212939399481]
	TIME [epoch: 10.4 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492218983287556		[learning rate: 1.7077e-05]
	Learning Rate: 1.70774e-05
	LOSS [training: 3.492218983287556 | validation: 5.237913857489367]
	TIME [epoch: 10.4 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4956011228026673		[learning rate: 1.7015e-05]
	Learning Rate: 1.70154e-05
	LOSS [training: 3.4956011228026673 | validation: 5.237485259212158]
	TIME [epoch: 10.4 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961573346860737		[learning rate: 1.6954e-05]
	Learning Rate: 1.69537e-05
	LOSS [training: 3.4961573346860737 | validation: 5.237745485655715]
	TIME [epoch: 10.4 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4948718690482288		[learning rate: 1.6892e-05]
	Learning Rate: 1.68921e-05
	LOSS [training: 3.4948718690482288 | validation: 5.247656024454013]
	TIME [epoch: 10.4 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493673141531103		[learning rate: 1.6831e-05]
	Learning Rate: 1.68308e-05
	LOSS [training: 3.493673141531103 | validation: 5.249709173516189]
	TIME [epoch: 10.4 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49751498420407		[learning rate: 1.677e-05]
	Learning Rate: 1.67697e-05
	LOSS [training: 3.49751498420407 | validation: 5.2278970849860045]
	TIME [epoch: 10.4 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952794305444472		[learning rate: 1.6709e-05]
	Learning Rate: 1.67089e-05
	LOSS [training: 3.4952794305444472 | validation: 5.242137048071915]
	TIME [epoch: 10.4 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496933730891333		[learning rate: 1.6648e-05]
	Learning Rate: 1.66482e-05
	LOSS [training: 3.496933730891333 | validation: 5.237517632017648]
	TIME [epoch: 10.4 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4936070940909287		[learning rate: 1.6588e-05]
	Learning Rate: 1.65878e-05
	LOSS [training: 3.4936070940909287 | validation: 5.243431853590241]
	TIME [epoch: 10.4 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4931795433106503		[learning rate: 1.6528e-05]
	Learning Rate: 1.65276e-05
	LOSS [training: 3.4931795433106503 | validation: 5.2481959366427]
	TIME [epoch: 10.4 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4929410697478467		[learning rate: 1.6468e-05]
	Learning Rate: 1.64677e-05
	LOSS [training: 3.4929410697478467 | validation: 5.23992028287689]
	TIME [epoch: 10.4 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960062751077503		[learning rate: 1.6408e-05]
	Learning Rate: 1.64079e-05
	LOSS [training: 3.4960062751077503 | validation: 5.239318978894864]
	TIME [epoch: 10.4 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49310428114437		[learning rate: 1.6348e-05]
	Learning Rate: 1.63483e-05
	LOSS [training: 3.49310428114437 | validation: 5.225790520625565]
	TIME [epoch: 10.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study206/model_tr_study206_r3_20240219_233643/states/model_tr_study206_1865.pth
	Model improved!!!
EPOCH 1866/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4974700441248077		[learning rate: 1.6289e-05]
	Learning Rate: 1.6289e-05
	LOSS [training: 3.4974700441248077 | validation: 5.2406747307189745]
	TIME [epoch: 10.4 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496664973864642		[learning rate: 1.623e-05]
	Learning Rate: 1.62299e-05
	LOSS [training: 3.496664973864642 | validation: 5.242981968850427]
	TIME [epoch: 10.4 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499509923354691		[learning rate: 1.6171e-05]
	Learning Rate: 1.6171e-05
	LOSS [training: 3.499509923354691 | validation: 5.242467513099403]
	TIME [epoch: 10.4 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4964880279205324		[learning rate: 1.6112e-05]
	Learning Rate: 1.61123e-05
	LOSS [training: 3.4964880279205324 | validation: 5.2483100919008185]
	TIME [epoch: 10.4 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.500124437411592		[learning rate: 1.6054e-05]
	Learning Rate: 1.60538e-05
	LOSS [training: 3.500124437411592 | validation: 5.239028421567212]
	TIME [epoch: 10.4 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494214775272552		[learning rate: 1.5996e-05]
	Learning Rate: 1.59956e-05
	LOSS [training: 3.494214775272552 | validation: 5.241706799695144]
	TIME [epoch: 10.4 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969517023006746		[learning rate: 1.5938e-05]
	Learning Rate: 1.59375e-05
	LOSS [training: 3.4969517023006746 | validation: 5.232146681612745]
	TIME [epoch: 10.4 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4953604618311607		[learning rate: 1.588e-05]
	Learning Rate: 1.58797e-05
	LOSS [training: 3.4953604618311607 | validation: 5.235651873473944]
	TIME [epoch: 10.4 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5010853305559673		[learning rate: 1.5822e-05]
	Learning Rate: 1.58221e-05
	LOSS [training: 3.5010853305559673 | validation: 5.24874634723694]
	TIME [epoch: 10.4 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4963564659282427		[learning rate: 1.5765e-05]
	Learning Rate: 1.57647e-05
	LOSS [training: 3.4963564659282427 | validation: 5.239380104071894]
	TIME [epoch: 10.4 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4917948620744412		[learning rate: 1.5707e-05]
	Learning Rate: 1.57074e-05
	LOSS [training: 3.4917948620744412 | validation: 5.241571843328734]
	TIME [epoch: 10.4 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491935160973597		[learning rate: 1.565e-05]
	Learning Rate: 1.56504e-05
	LOSS [training: 3.491935160973597 | validation: 5.235316279984134]
	TIME [epoch: 10.4 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5016633408241384		[learning rate: 1.5594e-05]
	Learning Rate: 1.55936e-05
	LOSS [training: 3.5016633408241384 | validation: 5.245511645706849]
	TIME [epoch: 10.4 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4991764404285286		[learning rate: 1.5537e-05]
	Learning Rate: 1.5537e-05
	LOSS [training: 3.4991764404285286 | validation: 5.238055112079154]
	TIME [epoch: 10.4 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499025313266801		[learning rate: 1.5481e-05]
	Learning Rate: 1.54807e-05
	LOSS [training: 3.499025313266801 | validation: 5.2392309291505805]
	TIME [epoch: 10.4 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5000763680075098		[learning rate: 1.5424e-05]
	Learning Rate: 1.54245e-05
	LOSS [training: 3.5000763680075098 | validation: 5.239509843993943]
	TIME [epoch: 10.4 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495290351484043		[learning rate: 1.5369e-05]
	Learning Rate: 1.53685e-05
	LOSS [training: 3.495290351484043 | validation: 5.2412557985986234]
	TIME [epoch: 10.4 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491256961908971		[learning rate: 1.5313e-05]
	Learning Rate: 1.53127e-05
	LOSS [training: 3.491256961908971 | validation: 5.230158409654457]
	TIME [epoch: 10.4 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966116644216223		[learning rate: 1.5257e-05]
	Learning Rate: 1.52572e-05
	LOSS [training: 3.4966116644216223 | validation: 5.240832251981837]
	TIME [epoch: 10.4 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4891915305459698		[learning rate: 1.5202e-05]
	Learning Rate: 1.52018e-05
	LOSS [training: 3.4891915305459698 | validation: 5.240163346889877]
	TIME [epoch: 10.4 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4955796023548693		[learning rate: 1.5147e-05]
	Learning Rate: 1.51466e-05
	LOSS [training: 3.4955796023548693 | validation: 5.240530349470811]
	TIME [epoch: 10.4 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493631693792171		[learning rate: 1.5092e-05]
	Learning Rate: 1.50917e-05
	LOSS [training: 3.493631693792171 | validation: 5.233746995161439]
	TIME [epoch: 10.4 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494735143413872		[learning rate: 1.5037e-05]
	Learning Rate: 1.50369e-05
	LOSS [training: 3.494735143413872 | validation: 5.248380493188306]
	TIME [epoch: 10.4 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983217848334376		[learning rate: 1.4982e-05]
	Learning Rate: 1.49823e-05
	LOSS [training: 3.4983217848334376 | validation: 5.248009298942618]
	TIME [epoch: 10.4 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968849148915266		[learning rate: 1.4928e-05]
	Learning Rate: 1.49279e-05
	LOSS [training: 3.4968849148915266 | validation: 5.2401247798301815]
	TIME [epoch: 10.4 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4951207580384827		[learning rate: 1.4874e-05]
	Learning Rate: 1.48738e-05
	LOSS [training: 3.4951207580384827 | validation: 5.235388515776945]
	TIME [epoch: 10.4 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4987120320122087		[learning rate: 1.482e-05]
	Learning Rate: 1.48198e-05
	LOSS [training: 3.4987120320122087 | validation: 5.238948985054201]
	TIME [epoch: 10.4 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4975201838051952		[learning rate: 1.4766e-05]
	Learning Rate: 1.4766e-05
	LOSS [training: 3.4975201838051952 | validation: 5.244718161845642]
	TIME [epoch: 10.4 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495132483038799		[learning rate: 1.4712e-05]
	Learning Rate: 1.47124e-05
	LOSS [training: 3.495132483038799 | validation: 5.247272159459284]
	TIME [epoch: 10.4 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5029474648126877		[learning rate: 1.4659e-05]
	Learning Rate: 1.4659e-05
	LOSS [training: 3.5029474648126877 | validation: 5.244981232148903]
	TIME [epoch: 10.4 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493972437049963		[learning rate: 1.4606e-05]
	Learning Rate: 1.46058e-05
	LOSS [training: 3.493972437049963 | validation: 5.240704684998394]
	TIME [epoch: 10.4 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958979183715924		[learning rate: 1.4553e-05]
	Learning Rate: 1.45528e-05
	LOSS [training: 3.4958979183715924 | validation: 5.2471791831817844]
	TIME [epoch: 10.4 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4990404350589857		[learning rate: 1.45e-05]
	Learning Rate: 1.45e-05
	LOSS [training: 3.4990404350589857 | validation: 5.237558265534349]
	TIME [epoch: 10.4 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494548436688935		[learning rate: 1.4447e-05]
	Learning Rate: 1.44474e-05
	LOSS [training: 3.494548436688935 | validation: 5.243697993795264]
	TIME [epoch: 10.4 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952957661673594		[learning rate: 1.4395e-05]
	Learning Rate: 1.4395e-05
	LOSS [training: 3.4952957661673594 | validation: 5.238435434098232]
	TIME [epoch: 10.4 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4969643013591836		[learning rate: 1.4343e-05]
	Learning Rate: 1.43427e-05
	LOSS [training: 3.4969643013591836 | validation: 5.241716885132213]
	TIME [epoch: 10.4 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4985843490048465		[learning rate: 1.4291e-05]
	Learning Rate: 1.42907e-05
	LOSS [training: 3.4985843490048465 | validation: 5.237349726372072]
	TIME [epoch: 10.4 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.490302614726631		[learning rate: 1.4239e-05]
	Learning Rate: 1.42388e-05
	LOSS [training: 3.490302614726631 | validation: 5.240140108601588]
	TIME [epoch: 10.4 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492599692843959		[learning rate: 1.4187e-05]
	Learning Rate: 1.41871e-05
	LOSS [training: 3.492599692843959 | validation: 5.243416687797962]
	TIME [epoch: 10.4 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49269033242389		[learning rate: 1.4136e-05]
	Learning Rate: 1.41357e-05
	LOSS [training: 3.49269033242389 | validation: 5.247617015670726]
	TIME [epoch: 10.4 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499184031327727		[learning rate: 1.4084e-05]
	Learning Rate: 1.40844e-05
	LOSS [training: 3.499184031327727 | validation: 5.232804184531556]
	TIME [epoch: 10.4 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497684082839638		[learning rate: 1.4033e-05]
	Learning Rate: 1.40332e-05
	LOSS [training: 3.497684082839638 | validation: 5.242918243451463]
	TIME [epoch: 10.4 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4950467012503403		[learning rate: 1.3982e-05]
	Learning Rate: 1.39823e-05
	LOSS [training: 3.4950467012503403 | validation: 5.238534330230444]
	TIME [epoch: 10.4 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493487790407093		[learning rate: 1.3932e-05]
	Learning Rate: 1.39316e-05
	LOSS [training: 3.493487790407093 | validation: 5.237798570074874]
	TIME [epoch: 10.4 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4959194432464455		[learning rate: 1.3881e-05]
	Learning Rate: 1.3881e-05
	LOSS [training: 3.4959194432464455 | validation: 5.236291874893109]
	TIME [epoch: 10.4 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501826289407467		[learning rate: 1.3831e-05]
	Learning Rate: 1.38306e-05
	LOSS [training: 3.501826289407467 | validation: 5.233628830913074]
	TIME [epoch: 10.4 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971435605366947		[learning rate: 1.378e-05]
	Learning Rate: 1.37804e-05
	LOSS [training: 3.4971435605366947 | validation: 5.239106344161594]
	TIME [epoch: 10.4 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49828303229422		[learning rate: 1.373e-05]
	Learning Rate: 1.37304e-05
	LOSS [training: 3.49828303229422 | validation: 5.235445933416274]
	TIME [epoch: 10.4 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49862738573871		[learning rate: 1.3681e-05]
	Learning Rate: 1.36806e-05
	LOSS [training: 3.49862738573871 | validation: 5.243864195505004]
	TIME [epoch: 10.4 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501286606129836		[learning rate: 1.3631e-05]
	Learning Rate: 1.3631e-05
	LOSS [training: 3.501286606129836 | validation: 5.229127486075822]
	TIME [epoch: 10.4 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494807251675732		[learning rate: 1.3581e-05]
	Learning Rate: 1.35815e-05
	LOSS [training: 3.494807251675732 | validation: 5.241978769081779]
	TIME [epoch: 10.4 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49430403727076		[learning rate: 1.3532e-05]
	Learning Rate: 1.35322e-05
	LOSS [training: 3.49430403727076 | validation: 5.239395606948095]
	TIME [epoch: 10.4 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4965801223046897		[learning rate: 1.3483e-05]
	Learning Rate: 1.34831e-05
	LOSS [training: 3.4965801223046897 | validation: 5.231358533718712]
	TIME [epoch: 10.4 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49919737340299		[learning rate: 1.3434e-05]
	Learning Rate: 1.34342e-05
	LOSS [training: 3.49919737340299 | validation: 5.234856484630054]
	TIME [epoch: 10.4 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4945279997969765		[learning rate: 1.3385e-05]
	Learning Rate: 1.33854e-05
	LOSS [training: 3.4945279997969765 | validation: 5.24263474121161]
	TIME [epoch: 10.4 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497639143010138		[learning rate: 1.3337e-05]
	Learning Rate: 1.33368e-05
	LOSS [training: 3.497639143010138 | validation: 5.24010067496908]
	TIME [epoch: 10.4 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.49528869452797		[learning rate: 1.3288e-05]
	Learning Rate: 1.32884e-05
	LOSS [training: 3.49528869452797 | validation: 5.239310856371315]
	TIME [epoch: 10.4 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.5016226218527136		[learning rate: 1.324e-05]
	Learning Rate: 1.32402e-05
	LOSS [training: 3.5016226218527136 | validation: 5.2491760334149085]
	TIME [epoch: 10.4 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4918635882707947		[learning rate: 1.3192e-05]
	Learning Rate: 1.31922e-05
	LOSS [training: 3.4918635882707947 | validation: 5.242753918441845]
	TIME [epoch: 10.4 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4950786443353445		[learning rate: 1.3144e-05]
	Learning Rate: 1.31443e-05
	LOSS [training: 3.4950786443353445 | validation: 5.2326387929838845]
	TIME [epoch: 10.4 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4971116451424145		[learning rate: 1.3097e-05]
	Learning Rate: 1.30966e-05
	LOSS [training: 3.4971116451424145 | validation: 5.23492347144854]
	TIME [epoch: 10.4 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496602942013451		[learning rate: 1.3049e-05]
	Learning Rate: 1.30491e-05
	LOSS [training: 3.496602942013451 | validation: 5.235892649799629]
	TIME [epoch: 10.4 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4947909783546627		[learning rate: 1.3002e-05]
	Learning Rate: 1.30017e-05
	LOSS [training: 3.4947909783546627 | validation: 5.237221760735206]
	TIME [epoch: 10.4 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494175172747724		[learning rate: 1.2955e-05]
	Learning Rate: 1.29545e-05
	LOSS [training: 3.494175172747724 | validation: 5.238323582617313]
	TIME [epoch: 10.4 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4995064378933955		[learning rate: 1.2907e-05]
	Learning Rate: 1.29075e-05
	LOSS [training: 3.4995064378933955 | validation: 5.241722896690171]
	TIME [epoch: 10.4 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4957954533165463		[learning rate: 1.2861e-05]
	Learning Rate: 1.28607e-05
	LOSS [training: 3.4957954533165463 | validation: 5.233240693668927]
	TIME [epoch: 10.4 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493999136096247		[learning rate: 1.2814e-05]
	Learning Rate: 1.2814e-05
	LOSS [training: 3.493999136096247 | validation: 5.240744933474097]
	TIME [epoch: 10.4 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4903067593541124		[learning rate: 1.2767e-05]
	Learning Rate: 1.27675e-05
	LOSS [training: 3.4903067593541124 | validation: 5.236685841143127]
	TIME [epoch: 10.4 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494793412539863		[learning rate: 1.2721e-05]
	Learning Rate: 1.27212e-05
	LOSS [training: 3.494793412539863 | validation: 5.244240586768374]
	TIME [epoch: 10.4 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497590428995853		[learning rate: 1.2675e-05]
	Learning Rate: 1.2675e-05
	LOSS [training: 3.497590428995853 | validation: 5.245462044098999]
	TIME [epoch: 10.4 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4914199617555313		[learning rate: 1.2629e-05]
	Learning Rate: 1.2629e-05
	LOSS [training: 3.4914199617555313 | validation: 5.244369539479379]
	TIME [epoch: 10.4 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4968147946382446		[learning rate: 1.2583e-05]
	Learning Rate: 1.25832e-05
	LOSS [training: 3.4968147946382446 | validation: 5.230975122375244]
	TIME [epoch: 10.4 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4924773315339395		[learning rate: 1.2537e-05]
	Learning Rate: 1.25375e-05
	LOSS [training: 3.4924773315339395 | validation: 5.2356266012388835]
	TIME [epoch: 10.4 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499675021947631		[learning rate: 1.2492e-05]
	Learning Rate: 1.2492e-05
	LOSS [training: 3.499675021947631 | validation: 5.251299534521499]
	TIME [epoch: 10.4 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4952115230916823		[learning rate: 1.2447e-05]
	Learning Rate: 1.24467e-05
	LOSS [training: 3.4952115230916823 | validation: 5.243143189960489]
	TIME [epoch: 10.4 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494830866576218		[learning rate: 1.2401e-05]
	Learning Rate: 1.24015e-05
	LOSS [training: 3.494830866576218 | validation: 5.2439604461043725]
	TIME [epoch: 10.4 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494630073553229		[learning rate: 1.2356e-05]
	Learning Rate: 1.23565e-05
	LOSS [training: 3.494630073553229 | validation: 5.23043608098328]
	TIME [epoch: 10.4 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4966883276613068		[learning rate: 1.2312e-05]
	Learning Rate: 1.23116e-05
	LOSS [training: 3.4966883276613068 | validation: 5.2372420319420145]
	TIME [epoch: 10.4 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494851322298465		[learning rate: 1.2267e-05]
	Learning Rate: 1.2267e-05
	LOSS [training: 3.494851322298465 | validation: 5.246007282212187]
	TIME [epoch: 10.4 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4955096810692354		[learning rate: 1.2222e-05]
	Learning Rate: 1.22224e-05
	LOSS [training: 3.4955096810692354 | validation: 5.248152216888477]
	TIME [epoch: 10.4 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493716051145161		[learning rate: 1.2178e-05]
	Learning Rate: 1.21781e-05
	LOSS [training: 3.493716051145161 | validation: 5.235596371937677]
	TIME [epoch: 10.4 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493732818121642		[learning rate: 1.2134e-05]
	Learning Rate: 1.21339e-05
	LOSS [training: 3.493732818121642 | validation: 5.242969206010145]
	TIME [epoch: 10.4 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4947963870622956		[learning rate: 1.209e-05]
	Learning Rate: 1.20899e-05
	LOSS [training: 3.4947963870622956 | validation: 5.24097613004341]
	TIME [epoch: 10.4 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4983975294468777		[learning rate: 1.2046e-05]
	Learning Rate: 1.2046e-05
	LOSS [training: 3.4983975294468777 | validation: 5.233065212865947]
	TIME [epoch: 10.4 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4933960888835984		[learning rate: 1.2002e-05]
	Learning Rate: 1.20023e-05
	LOSS [training: 3.4933960888835984 | validation: 5.249773237337279]
	TIME [epoch: 10.4 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494016187178984		[learning rate: 1.1959e-05]
	Learning Rate: 1.19587e-05
	LOSS [training: 3.494016187178984 | validation: 5.234670525687648]
	TIME [epoch: 10.4 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494916260221456		[learning rate: 1.1915e-05]
	Learning Rate: 1.19153e-05
	LOSS [training: 3.494916260221456 | validation: 5.243637236813235]
	TIME [epoch: 10.4 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4958334521504546		[learning rate: 1.1872e-05]
	Learning Rate: 1.18721e-05
	LOSS [training: 3.4958334521504546 | validation: 5.237330384475994]
	TIME [epoch: 10.4 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494834099304086		[learning rate: 1.1829e-05]
	Learning Rate: 1.1829e-05
	LOSS [training: 3.494834099304086 | validation: 5.244550032319394]
	TIME [epoch: 10.4 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495111158854308		[learning rate: 1.1786e-05]
	Learning Rate: 1.17861e-05
	LOSS [training: 3.495111158854308 | validation: 5.243529606614684]
	TIME [epoch: 10.4 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494332891230443		[learning rate: 1.1743e-05]
	Learning Rate: 1.17433e-05
	LOSS [training: 3.494332891230443 | validation: 5.243793617681954]
	TIME [epoch: 10.4 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492053399648899		[learning rate: 1.1701e-05]
	Learning Rate: 1.17007e-05
	LOSS [training: 3.492053399648899 | validation: 5.241934766669804]
	TIME [epoch: 10.4 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498895066079519		[learning rate: 1.1658e-05]
	Learning Rate: 1.16582e-05
	LOSS [training: 3.498895066079519 | validation: 5.236280356279081]
	TIME [epoch: 10.4 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4967141008748337		[learning rate: 1.1616e-05]
	Learning Rate: 1.16159e-05
	LOSS [training: 3.4967141008748337 | validation: 5.234326164165489]
	TIME [epoch: 10.4 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4925364486865633		[learning rate: 1.1574e-05]
	Learning Rate: 1.15737e-05
	LOSS [training: 3.4925364486865633 | validation: 5.249639022459876]
	TIME [epoch: 10.4 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.496941660532362		[learning rate: 1.1532e-05]
	Learning Rate: 1.15317e-05
	LOSS [training: 3.496941660532362 | validation: 5.245680284839666]
	TIME [epoch: 10.4 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494670645120334		[learning rate: 1.149e-05]
	Learning Rate: 1.14899e-05
	LOSS [training: 3.494670645120334 | validation: 5.240499571062168]
	TIME [epoch: 10.4 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498405420939375		[learning rate: 1.1448e-05]
	Learning Rate: 1.14482e-05
	LOSS [training: 3.498405420939375 | validation: 5.244381671562897]
	TIME [epoch: 10.4 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4918731805269245		[learning rate: 1.1407e-05]
	Learning Rate: 1.14066e-05
	LOSS [training: 3.4918731805269245 | validation: 5.2312785888887205]
	TIME [epoch: 10.3 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4948296918973214		[learning rate: 1.1365e-05]
	Learning Rate: 1.13652e-05
	LOSS [training: 3.4948296918973214 | validation: 5.237414105186325]
	TIME [epoch: 10.4 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4960790237940556		[learning rate: 1.1324e-05]
	Learning Rate: 1.1324e-05
	LOSS [training: 3.4960790237940556 | validation: 5.238930838395102]
	TIME [epoch: 10.4 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498778614459637		[learning rate: 1.1283e-05]
	Learning Rate: 1.12829e-05
	LOSS [training: 3.498778614459637 | validation: 5.2453830240362525]
	TIME [epoch: 10.4 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4928519585055944		[learning rate: 1.1242e-05]
	Learning Rate: 1.1242e-05
	LOSS [training: 3.4928519585055944 | validation: 5.2328855248570765]
	TIME [epoch: 10.4 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4941171277166134		[learning rate: 1.1201e-05]
	Learning Rate: 1.12012e-05
	LOSS [training: 3.4941171277166134 | validation: 5.229413429855356]
	TIME [epoch: 10.4 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.492814853326844		[learning rate: 1.1161e-05]
	Learning Rate: 1.11605e-05
	LOSS [training: 3.492814853326844 | validation: 5.244985657771678]
	TIME [epoch: 10.4 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495748343132923		[learning rate: 1.112e-05]
	Learning Rate: 1.112e-05
	LOSS [training: 3.495748343132923 | validation: 5.240908844139861]
	TIME [epoch: 10.4 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495167839790475		[learning rate: 1.108e-05]
	Learning Rate: 1.10797e-05
	LOSS [training: 3.495167839790475 | validation: 5.246524371869311]
	TIME [epoch: 10.4 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4939122898184025		[learning rate: 1.1039e-05]
	Learning Rate: 1.10394e-05
	LOSS [training: 3.4939122898184025 | validation: 5.239178040079328]
	TIME [epoch: 10.4 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493430049178875		[learning rate: 1.0999e-05]
	Learning Rate: 1.09994e-05
	LOSS [training: 3.493430049178875 | validation: 5.245969488724618]
	TIME [epoch: 10.4 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497673368679		[learning rate: 1.0959e-05]
	Learning Rate: 1.09595e-05
	LOSS [training: 3.497673368679 | validation: 5.2436266385731605]
	TIME [epoch: 10.4 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498439081221042		[learning rate: 1.092e-05]
	Learning Rate: 1.09197e-05
	LOSS [training: 3.498439081221042 | validation: 5.244970894883879]
	TIME [epoch: 10.4 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.493953732026603		[learning rate: 1.088e-05]
	Learning Rate: 1.08801e-05
	LOSS [training: 3.493953732026603 | validation: 5.232451992993334]
	TIME [epoch: 10.4 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491887367589824		[learning rate: 1.0841e-05]
	Learning Rate: 1.08406e-05
	LOSS [training: 3.491887367589824 | validation: 5.250656331523573]
	TIME [epoch: 10.4 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491033383647408		[learning rate: 1.0801e-05]
	Learning Rate: 1.08012e-05
	LOSS [training: 3.491033383647408 | validation: 5.236001298090005]
	TIME [epoch: 10.4 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4982762091100015		[learning rate: 1.0762e-05]
	Learning Rate: 1.0762e-05
	LOSS [training: 3.4982762091100015 | validation: 5.240090769699018]
	TIME [epoch: 10.4 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495769888330188		[learning rate: 1.0723e-05]
	Learning Rate: 1.0723e-05
	LOSS [training: 3.495769888330188 | validation: 5.235416381500272]
	TIME [epoch: 10.4 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4931665337251046		[learning rate: 1.0684e-05]
	Learning Rate: 1.06841e-05
	LOSS [training: 3.4931665337251046 | validation: 5.242476297706584]
	TIME [epoch: 10.4 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.498460117913132		[learning rate: 1.0645e-05]
	Learning Rate: 1.06453e-05
	LOSS [training: 3.498460117913132 | validation: 5.243044488595472]
	TIME [epoch: 10.4 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4956343285594187		[learning rate: 1.0607e-05]
	Learning Rate: 1.06067e-05
	LOSS [training: 3.4956343285594187 | validation: 5.246583103706289]
	TIME [epoch: 10.4 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.497245043175559		[learning rate: 1.0568e-05]
	Learning Rate: 1.05682e-05
	LOSS [training: 3.497245043175559 | validation: 5.23438046940428]
	TIME [epoch: 10.4 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4974115494101916		[learning rate: 1.053e-05]
	Learning Rate: 1.05298e-05
	LOSS [training: 3.4974115494101916 | validation: 5.234869342324887]
	TIME [epoch: 10.4 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4961394779660986		[learning rate: 1.0492e-05]
	Learning Rate: 1.04916e-05
	LOSS [training: 3.4961394779660986 | validation: 5.242505269814899]
	TIME [epoch: 10.4 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4962533536137883		[learning rate: 1.0454e-05]
	Learning Rate: 1.04535e-05
	LOSS [training: 3.4962533536137883 | validation: 5.226966680335372]
	TIME [epoch: 10.4 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.501761406425259		[learning rate: 1.0416e-05]
	Learning Rate: 1.04156e-05
	LOSS [training: 3.501761406425259 | validation: 5.235846292568415]
	TIME [epoch: 10.4 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.494660484112149		[learning rate: 1.0378e-05]
	Learning Rate: 1.03778e-05
	LOSS [training: 3.494660484112149 | validation: 5.242671663272151]
	TIME [epoch: 10.4 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4984037886867854		[learning rate: 1.034e-05]
	Learning Rate: 1.03401e-05
	LOSS [training: 3.4984037886867854 | validation: 5.244021380741665]
	TIME [epoch: 10.4 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4923126457513325		[learning rate: 1.0303e-05]
	Learning Rate: 1.03026e-05
	LOSS [training: 3.4923126457513325 | validation: 5.246240229419775]
	TIME [epoch: 10.4 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.495926537004132		[learning rate: 1.0265e-05]
	Learning Rate: 1.02652e-05
	LOSS [training: 3.495926537004132 | validation: 5.245832583900024]
	TIME [epoch: 10.4 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4980696732672514		[learning rate: 1.0228e-05]
	Learning Rate: 1.0228e-05
	LOSS [training: 3.4980696732672514 | validation: 5.233959674441366]
	TIME [epoch: 10.4 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4897009002369366		[learning rate: 1.0191e-05]
	Learning Rate: 1.01909e-05
	LOSS [training: 3.4897009002369366 | validation: 5.240487550542241]
	TIME [epoch: 10.4 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.499951801009491		[learning rate: 1.0154e-05]
	Learning Rate: 1.01539e-05
	LOSS [training: 3.499951801009491 | validation: 5.242801032847406]
	TIME [epoch: 10.4 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4932293323467833		[learning rate: 1.0117e-05]
	Learning Rate: 1.0117e-05
	LOSS [training: 3.4932293323467833 | validation: 5.241180737217705]
	TIME [epoch: 10.4 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.491766170523564		[learning rate: 1.008e-05]
	Learning Rate: 1.00803e-05
	LOSS [training: 3.491766170523564 | validation: 5.228959027908916]
	TIME [epoch: 10.4 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.4976355554031064		[learning rate: 1.0044e-05]
	Learning Rate: 1.00437e-05
	LOSS [training: 3.4976355554031064 | validation: 5.237283086305677]
	TIME [epoch: 10.4 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 5/5] avg loss: 3.502860430926622		[learning rate: 1.0007e-05]
	Learning Rate: 1.00073e-05
	LOSS [training: 3.502860430926622 | validation: 5.23296344962879]
	TIME [epoch: 10.4 sec]
Finished training in 20882.707 seconds.
