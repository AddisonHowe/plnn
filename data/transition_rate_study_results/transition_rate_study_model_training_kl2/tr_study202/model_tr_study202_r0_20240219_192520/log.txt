Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r0', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r0', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2357546095

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.033882389559459		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.1241799532570225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.079031171408241 | validation: 6.184594696178789]
	TIME [epoch: 52.5 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.085238773037316		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.93656034657462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.510899559805969 | validation: 4.3888158778585]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.165597823896147		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.101052538316546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.133325181106347 | validation: 3.8317823375733964]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.781945178702549		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7874104558325556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.784677817267552 | validation: 3.709855645861308]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.737528512094215		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.514734601323215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.626131556708715 | validation: 3.122495730509562]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.394361585780512		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.445209799924322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.419785692852417 | validation: 2.9730287666394846]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1262665245015224		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9944672558976975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.06036689019961 | validation: 3.081684456384387]
	TIME [epoch: 8.25 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.046154164886158		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.921985942206007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.984070053546082 | validation: 2.6909084238141947]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1579590333359606		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.461954696834418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.30995686508519 | validation: 3.7628902799268733]
	TIME [epoch: 8.26 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.074563006672249		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8057382394328534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9401506230525505 | validation: 2.4197481934088585]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6185757255172697		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9628594473134644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7907175864153664 | validation: 2.7011057651256762]
	TIME [epoch: 8.26 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.737102373082347		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7911308623325928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7641166177074696 | validation: 2.3033488771566564]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5222509215974034		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6454826462055467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5838667839014753 | validation: 2.513660170992299]
	TIME [epoch: 8.26 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6782890288523644		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.5246496623382333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.601469345595299 | validation: 3.4639127352156467]
	TIME [epoch: 8.26 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7837495396447225		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.573085731434689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6784176355397054 | validation: 2.0080609608650746]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.563713871945141		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.953514442505823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2586141572254825 | validation: 1.56411602152254]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7028671281079155		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4995159922949992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6011915602014575 | validation: 1.4483695699487646]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6514072759309553		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5609700577524368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6061886668416956 | validation: 1.3580728326583418]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5878907705149665		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4910632853677677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5394770279413668 | validation: 1.244329582882292]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4336480643477725		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5606219584650352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4971350114064037 | validation: 1.4216675623981305]
	TIME [epoch: 8.24 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4895732313690817		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5508871171028455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5202301742359636 | validation: 1.6709021350569966]
	TIME [epoch: 8.23 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4996079820380261		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3899534870806018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.444780734559314 | validation: 1.5910721666683982]
	TIME [epoch: 8.26 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.492998638466944		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.356260720643442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4246296795551934 | validation: 1.1975110197628647]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2891195285979309		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3058165264668724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2974680275324018 | validation: 0.967065454794265]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2407838706383485		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.36835225711314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3045680638757442 | validation: 1.0725931715909214]
	TIME [epoch: 8.24 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3083326761690315		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3430647311270951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3256987036480636 | validation: 1.2825366302532495]
	TIME [epoch: 8.26 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3148386042493496		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.399724197899689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3572814010745193 | validation: 1.5196462464943974]
	TIME [epoch: 8.24 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2999047056098054		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.190846018610006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2453753621099057 | validation: 0.8298976405792945]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1517259407337925		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0827844894999137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1172552151168529 | validation: 0.8498947503134898]
	TIME [epoch: 8.25 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9828555590173064		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1727779669487126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0778167629830095 | validation: 0.8122938389576488]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9524248050406172		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.016290048297945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.984357426669281 | validation: 0.8358422481225933]
	TIME [epoch: 8.24 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3208775286426582		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2640140642930862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2924457964678724 | validation: 0.7728395132749244]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3613199495314956		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9368415085041173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1490807290178064 | validation: 2.232357817469742]
	TIME [epoch: 8.23 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1816371264422092		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.950852153315715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0662446398789622 | validation: 1.1475071525326912]
	TIME [epoch: 8.24 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9377055523994665		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8511398038721367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8944226781358013 | validation: 0.8726066831547965]
	TIME [epoch: 8.23 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8663331324370347		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8729384756331772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.869635804035106 | validation: 0.6262516673705645]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.784195202280946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8539329305915333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190640664362396 | validation: 0.6875407414562162]
	TIME [epoch: 8.23 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8029346335709029		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0873963441740284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451654888724657 | validation: 0.6455429838244402]
	TIME [epoch: 8.24 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0833063720873688		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8831028570421118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.98320461456474 | validation: 0.986830931524147]
	TIME [epoch: 8.23 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7644460920683203		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8027684711789652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7836072816236427 | validation: 0.5295267326168209]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6888245281379974		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.874693040415764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7817587842768806 | validation: 0.5307488287919714]
	TIME [epoch: 8.24 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8084856447034265		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6765659773997127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7425258110515695 | validation: 0.4314502940490033]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7051014729707887		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8142861442952999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596938086330444 | validation: 0.48889900828489585]
	TIME [epoch: 8.24 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9386347636488223		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0352309669402182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.98693286529452 | validation: 0.8438483592401276]
	TIME [epoch: 8.23 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7948609356168557		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.787082480888618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909717082527368 | validation: 0.8641961062011589]
	TIME [epoch: 8.22 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6547059960546575		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8217178456057675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382119208302124 | validation: 0.7657281254981094]
	TIME [epoch: 8.25 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8089294876355654		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6666374843227311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7377834859791481 | validation: 0.49353490549904916]
	TIME [epoch: 8.23 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.707921584186977		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8133556933052123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606386387460947 | validation: 0.9033718154621649]
	TIME [epoch: 8.22 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9931174490822106		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9791819675425633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9861497083123867 | validation: 1.4148482198624435]
	TIME [epoch: 8.22 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.983535295791099		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6943923733133881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8389638345522437 | validation: 0.578498264896999]
	TIME [epoch: 8.24 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8013176772954044		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6510100914358009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7261638843656026 | validation: 0.5963019539630284]
	TIME [epoch: 8.23 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6852474284789775		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6510431702594145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6681452993691959 | validation: 1.140179558580073]
	TIME [epoch: 8.22 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6549761952077834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5770842725434987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6160302338756408 | validation: 0.4375126980409141]
	TIME [epoch: 8.22 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6306054646582238		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1881087251359026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9093570948970633 | validation: 0.660062508743373]
	TIME [epoch: 8.26 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6513982086139241		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.764922608413839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7081604085138815 | validation: 1.1188098706706806]
	TIME [epoch: 8.23 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7652358932387974		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6468632053848813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060495493118393 | validation: 0.4495184113570067]
	TIME [epoch: 8.23 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6192635699360672		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9337074389113438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764855044237057 | validation: 1.3913522203260966]
	TIME [epoch: 8.22 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7933182926176463		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6395905891540846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7164544408858655 | validation: 0.39361353737636595]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6714393026164209		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9682164755909561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8198278891036883 | validation: 0.6058322193479242]
	TIME [epoch: 8.25 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9345026101976041		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0127424301729622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9736225201852831 | validation: 0.533745441300933]
	TIME [epoch: 8.23 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9435614756088275		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8058219650661655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8746917203374964 | validation: 0.4672056191791605]
	TIME [epoch: 8.22 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5856402076443026		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0092749795521765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7974575935982394 | validation: 0.5585860019128268]
	TIME [epoch: 8.25 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6664507254162451		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8342341040229373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503424147195912 | validation: 0.8707326060131063]
	TIME [epoch: 8.22 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8414192951019063		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7620828743048086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8017510847033575 | validation: 0.5407073607410855]
	TIME [epoch: 8.22 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7288625046520993		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9259864941189075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8274244993855036 | validation: 1.2598060869695402]
	TIME [epoch: 8.22 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.904553021130875		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9062624284165715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9054077247737231 | validation: 0.9104105203157986]
	TIME [epoch: 8.23 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6427837884517353		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6023332969206966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.622558542686216 | validation: 0.6231692624241408]
	TIME [epoch: 8.23 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7499604046264		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7105533695162589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7302568870713296 | validation: 1.1272756196564653]
	TIME [epoch: 8.22 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8443559838925495		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5964374447127199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7203967143026346 | validation: 0.5370455429377557]
	TIME [epoch: 8.22 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7363782262252654		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9166312213608505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826504723793058 | validation: 0.5130182558340831]
	TIME [epoch: 8.24 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5434090582189076		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7645789255257253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6539939918723163 | validation: 0.7624691565251314]
	TIME [epoch: 8.23 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6536871398358354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8130295696446955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333583547402653 | validation: 0.369325436807283]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9028629425215746		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.555428193708116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291455681148452 | validation: 0.6388977414930112]
	TIME [epoch: 8.23 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6589268783034777		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7245051068339248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917159925687013 | validation: 0.9099181229550716]
	TIME [epoch: 8.24 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5968370643278391		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5554392379005701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5761381511142047 | validation: 0.4884286321930113]
	TIME [epoch: 8.23 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6252817196846406		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5283215906328925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5768016551587666 | validation: 0.49464853102657896]
	TIME [epoch: 8.23 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8994726359121662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7696296556719562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.834551145792061 | validation: 0.4871961041327684]
	TIME [epoch: 8.22 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5609488554415467		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6642866457289329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6126177505852399 | validation: 0.5800189322575054]
	TIME [epoch: 8.22 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7857491092623662		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5735324660048166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6796407876335913 | validation: 0.3867457283418755]
	TIME [epoch: 8.25 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5083425349473137		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6017083811935362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5550254580704249 | validation: 1.2862762754837074]
	TIME [epoch: 8.22 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6171236791893566		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6061724146011017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6116480468952291 | validation: 0.6811442828468202]
	TIME [epoch: 8.22 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.544931021687945		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6600663046734533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6024986631806992 | validation: 1.4702051191412737]
	TIME [epoch: 8.22 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8552977610685015		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6667458156667824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.761021788367642 | validation: 0.5326577282881995]
	TIME [epoch: 8.25 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7228537844197228		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6667024270470738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6947781057333984 | validation: 0.36627988579313614]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6200751438778067		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.606019813949626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6130474789137164 | validation: 0.5908647343664972]
	TIME [epoch: 8.23 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5784346071046184		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6683167703472156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6233756887259169 | validation: 0.8945360437157781]
	TIME [epoch: 8.23 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7307209044211722		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8426061333889683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7866635189050701 | validation: 0.5827217767347145]
	TIME [epoch: 8.25 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6010453065711433		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6239178744043815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6124815904877623 | validation: 0.5374161152913937]
	TIME [epoch: 8.23 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5390621081868192		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5920919443707968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.565577026278808 | validation: 0.5967536897728635]
	TIME [epoch: 8.22 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5511335701738835		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7715175478622487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6613255590180662 | validation: 0.42201335301063614]
	TIME [epoch: 8.23 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7239861374839148		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5348619859404843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6294240617121996 | validation: 0.3486354318721586]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5520267573687543		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7146941923413581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6333604748550562 | validation: 0.7748205511500514]
	TIME [epoch: 8.27 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.599861522193773		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5031081164500746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5514848193219237 | validation: 0.7971097947420634]
	TIME [epoch: 8.23 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7001726892282032		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5129165601498163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6065446246890098 | validation: 0.42896161899505575]
	TIME [epoch: 8.23 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5360613409490121		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.75290262821074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444819845798764 | validation: 0.473234663809992]
	TIME [epoch: 8.25 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.623008264885024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.65707259451242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6400404296987219 | validation: 0.5395097963208639]
	TIME [epoch: 8.23 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5963839494278004		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.48219182636885466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5392878878983275 | validation: 0.8611961258159688]
	TIME [epoch: 8.22 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5629134634915837		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6076187386296678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5852661010606257 | validation: 0.5004855134985782]
	TIME [epoch: 8.22 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5724773828217024		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6789956941202892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6257365384709958 | validation: 0.6690201416061976]
	TIME [epoch: 8.25 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.706294676485376		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5659957325705428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361452045279594 | validation: 0.6428169116427823]
	TIME [epoch: 8.23 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6056961742167684		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.5614696041301959		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.583582889173482 | validation: 0.813803584935669]
	TIME [epoch: 8.23 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6552828458618152		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.5448115505601198		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.6000471982109675 | validation: 0.5894558851852798]
	TIME [epoch: 8.23 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5353007122918111		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.6692057193061403		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.6022532157989756 | validation: 0.5594848458174267]
	TIME [epoch: 8.25 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5359393803062836		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.6123959904933735		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.5741676853998284 | validation: 0.9212212659544814]
	TIME [epoch: 8.22 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6844501897213944		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.6381442015046614		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.6612971956130279 | validation: 0.9434688314193065]
	TIME [epoch: 8.22 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7395640007650698		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.6303022080430076		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.6849331044040385 | validation: 0.5602853502218675]
	TIME [epoch: 8.23 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5499001148526169		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.5700436182298462		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.5599718665412314 | validation: 0.3744260025748675]
	TIME [epoch: 8.24 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4896393751003628		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.533092213037252		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.5113657940688073 | validation: 0.36604615766299425]
	TIME [epoch: 8.23 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5199022616716544		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.63184240209315		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.5758723318824022 | validation: 0.7370562083012164]
	TIME [epoch: 8.22 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5249973141589362		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.6967753051929734		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.6108863096759547 | validation: 0.5477291093756773]
	TIME [epoch: 8.23 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5653934840752098		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.5397997282467185		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.5525966061609642 | validation: 0.955708232320425]
	TIME [epoch: 8.25 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6212849648911514		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.45628179349533127		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.5387833791932412 | validation: 0.33898176541366926]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5109642580542858		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.4757035731414055		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.4933339155978456 | validation: 0.5180325639032894]
	TIME [epoch: 8.23 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5280553621827334		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.5197411834966212		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.5238982728396773 | validation: 0.318000881203514]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4605215785170926		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.5126934984264861		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.48660753847178917 | validation: 0.551038871690944]
	TIME [epoch: 8.26 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44956166095430944		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.5055289676010646		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.4775453142776872 | validation: 0.9791508143567533]
	TIME [epoch: 8.24 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6890715453398479		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.4081053948824609		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.5485884701111544 | validation: 0.43454715968270974]
	TIME [epoch: 8.23 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4261759856478594		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.6025183099341238		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.5143471477909916 | validation: 0.3441414217393118]
	TIME [epoch: 8.23 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5666582288236699		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.6013798710956251		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.5840190499596475 | validation: 0.8974416122547123]
	TIME [epoch: 8.25 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5132156020083125		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.5455134886320259		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.5293645453201693 | validation: 0.504555303379876]
	TIME [epoch: 8.24 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5700582364026823		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.5475215247496504		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.5587898805761664 | validation: 1.0545230494289308]
	TIME [epoch: 8.23 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.759318942046162		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.46385294865632465		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.6115859453512433 | validation: 0.8475964757198332]
	TIME [epoch: 8.24 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6711276725098209		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.4640644233760932		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.567596047942957 | validation: 0.3214131655969507]
	TIME [epoch: 8.25 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4551188772220948		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.6991801260585475		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.5771495016403212 | validation: 0.40590039581680704]
	TIME [epoch: 8.23 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42576885224443195		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.4360747712866452		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.43092181176553856 | validation: 0.428558550118468]
	TIME [epoch: 8.24 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6915797128527508		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.35797230883766107		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.5247760108452059 | validation: 0.5576214337923904]
	TIME [epoch: 8.23 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3932851292992975		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.5909802551203466		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.492132692209822 | validation: 0.4823791999831656]
	TIME [epoch: 8.25 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4037166714328541		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.47386944650937546		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.43879305897111476 | validation: 0.36602558434467275]
	TIME [epoch: 8.23 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4597970471879288		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.4453190825716426		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.45255806487978567 | validation: 0.36015900134757783]
	TIME [epoch: 8.23 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41061906307948803		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.5537847183818537		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.482201890730671 | validation: 0.46241273575711134]
	TIME [epoch: 8.23 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4003933767433293		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.507071809224549		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.45373259298393903 | validation: 0.4757704895312748]
	TIME [epoch: 8.25 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5869089560906768		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.4962434172095887		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.5415761866501326 | validation: 0.6938480961240192]
	TIME [epoch: 8.23 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.583368023300005		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.43182670961967656		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.507597366459841 | validation: 0.49069699706155534]
	TIME [epoch: 8.23 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4462617286348737		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.4809710314505987		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.46361638004273614 | validation: 0.38449646535691423]
	TIME [epoch: 8.23 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48753958027921085		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.5177316703456047		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.5026356253124078 | validation: 0.766819503734192]
	TIME [epoch: 8.25 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5488850117220873		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.4664930254702151		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.5076890185961511 | validation: 0.2738243481790228]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5829647762504571		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.41640926903630476		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.499687022643381 | validation: 0.38570015921327777]
	TIME [epoch: 8.23 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38925033808364307		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.46592086478706785		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.4275856014353553 | validation: 0.2581353993279832]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40297217823639003		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.5364745432623066		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.4697233607493483 | validation: 0.6280806695797609]
	TIME [epoch: 8.25 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46325830924309874		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.4672921077332995		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.4652752084881991 | validation: 0.3030929877946021]
	TIME [epoch: 8.22 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5331510281680579		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.4124135168192383		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.4727822724936481 | validation: 0.2986746674424111]
	TIME [epoch: 8.22 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46441401301383856		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.49926027623138297		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.48183714462261074 | validation: 0.5132745564588773]
	TIME [epoch: 8.22 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.647145725347533		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.47372348352516047		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.5604346044363467 | validation: 0.4570536590293426]
	TIME [epoch: 8.24 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5056951519215974		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.4153344879579442		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.4605148199397708 | validation: 0.3119468796819351]
	TIME [epoch: 8.22 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42003719152906926		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.5021538888662216		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.4610955401976454 | validation: 0.3069597368556328]
	TIME [epoch: 8.22 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4558052519805512		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.5259035063422653		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.4908543791614083 | validation: 0.3166955848854849]
	TIME [epoch: 8.22 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4530152704366892		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.5013299145479708		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.47717259249232996 | validation: 0.3597531249699929]
	TIME [epoch: 8.24 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41248299145313705		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.512547001513469		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.46251499648330296 | validation: 0.3448366749982208]
	TIME [epoch: 8.23 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5288618852799356		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.5040007961615667		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.516431340720751 | validation: 0.368347537201301]
	TIME [epoch: 8.22 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4367690695953437		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.351873193057307		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.3943211313263254 | validation: 0.3501803135567956]
	TIME [epoch: 8.23 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49659293131329096		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.3325910063928129		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.414591968853052 | validation: 0.503074171704464]
	TIME [epoch: 8.24 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4827268212708967		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.47018930860153524		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.476458064936216 | validation: 0.35150760878072174]
	TIME [epoch: 8.23 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3394026948829161		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.3894881712108395		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.3644454330468778 | validation: 0.3275194323518485]
	TIME [epoch: 8.22 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4188506787624841		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.4858425428260934		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.45234661079428873 | validation: 0.2956989020274141]
	TIME [epoch: 8.22 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3212706243226346		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.47416734472634675		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.3977189845244907 | validation: 0.6644419958744617]
	TIME [epoch: 8.26 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4872690210081473		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.4961292786212116		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.49169914981467955 | validation: 0.6152630605426659]
	TIME [epoch: 8.23 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40732000726352363		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.4203619729276289		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.4138409900955763 | validation: 0.3597805665936728]
	TIME [epoch: 8.23 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34412053548106825		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.3741568734941055		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.35913870448758684 | validation: 0.5138743546829716]
	TIME [epoch: 8.22 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3887787273004826		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.3960032904992535		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.3923910088998681 | validation: 0.35680284421519803]
	TIME [epoch: 8.25 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3912347794403303		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.7177810131791011		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.5545078963097156 | validation: 0.4427551094568791]
	TIME [epoch: 8.22 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47109365160058114		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.3852048298989881		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.4281492407497847 | validation: 0.5407593074638234]
	TIME [epoch: 8.23 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39329060682176586		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.3477118578191321		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.37050123232044907 | validation: 0.4293169513193811]
	TIME [epoch: 8.21 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36205369398138165		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.3297432962659143		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.34589849512364795 | validation: 0.35222513418359713]
	TIME [epoch: 8.25 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43898840582762083		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.4232183421309091		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.43110337397926496 | validation: 0.3360171595520714]
	TIME [epoch: 8.22 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3537950037174961		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.4319260699731496		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.39286053684532285 | validation: 0.40204906232605087]
	TIME [epoch: 8.22 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5086153559682144		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.4559227426932198		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.482269049330717 | validation: 0.4001752164598815]
	TIME [epoch: 8.23 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.442902954920136		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.4926287502579714		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.46776585258905373 | validation: 0.29226144278184896]
	TIME [epoch: 8.25 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47610764655105864		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.5016073977904726		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.4888575221707657 | validation: 1.0497906092923166]
	TIME [epoch: 8.23 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4239559181622491		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.5569682483027301		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.4904620832324896 | validation: 0.4081818467839781]
	TIME [epoch: 8.22 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43841254240147387		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.5578866453860712		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.49814959389377256 | validation: 0.8602186865078619]
	TIME [epoch: 8.23 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.522452194135352		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.3437385527843861		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.43309537345986915 | validation: 0.30711929486256306]
	TIME [epoch: 8.25 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3179788280667145		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.37264977201302185		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.3453143000398682 | validation: 0.41986771717932714]
	TIME [epoch: 8.23 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31331848703684023		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.4088836151548704		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.36110105109585533 | validation: 0.45512550801075363]
	TIME [epoch: 8.22 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45490140362688747		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.39846881658655275		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.42668511010672 | validation: 0.3136615859156163]
	TIME [epoch: 8.23 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4915560689433284		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.347949084074225		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.4197525765087766 | validation: 0.6133692091380962]
	TIME [epoch: 8.25 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.512795630358848		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.41838459890247703		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.4655901146306626 | validation: 0.254227142701273]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31779854042367345		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.4320973066328505		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.3749479235282619 | validation: 0.32913263857632735]
	TIME [epoch: 8.24 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37799211008913935		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.42894181680328225		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.4034669634462108 | validation: 0.26540772141799196]
	TIME [epoch: 8.23 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3524222982623743		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.35833734243685733		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.3553798203496158 | validation: 0.6439084657084552]
	TIME [epoch: 8.25 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.407953894001298		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.3714106534167169		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.38968227370900743 | validation: 0.3941499441060785]
	TIME [epoch: 8.23 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3633088209762184		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.43180419581640594		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.39755650839631207 | validation: 0.3693894456234298]
	TIME [epoch: 8.23 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4720586338527849		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.43614246746338303		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.45410055065808386 | validation: 0.28411596330437777]
	TIME [epoch: 8.22 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33491390603948296		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.3333251464491989		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.33411952624434094 | validation: 0.29136188289548987]
	TIME [epoch: 8.25 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.399558559794777		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.4018686962011988		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.40071362799798793 | validation: 0.3063127522270497]
	TIME [epoch: 8.23 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47056569593014075		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.37515013546236864		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.4228579156962547 | validation: 0.383387800154824]
	TIME [epoch: 8.22 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4015120235162696		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.45629323679386535		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.4289026301550674 | validation: 0.45529405549336865]
	TIME [epoch: 8.24 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36242717549914394		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.3904560541596501		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.376441614829397 | validation: 0.5455960501200232]
	TIME [epoch: 8.25 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5127913748946475		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.35730221160409836		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.43504679324937295 | validation: 0.31199365371002813]
	TIME [epoch: 8.23 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36179315060192985		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.33405123419151705		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.3479221923967234 | validation: 0.4475607144459963]
	TIME [epoch: 8.22 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3366942777863375		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.3189355175492411		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.32781489766778926 | validation: 0.28352173012497284]
	TIME [epoch: 8.23 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3044909434917672		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.37673300739675014		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.34061197544425875 | validation: 0.24831147919118857]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3916344225120735		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.3366877722942147		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.36416109740314406 | validation: 0.2776655723102512]
	TIME [epoch: 8.25 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40327397022328926		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.39286041202966226		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.3980671911264758 | validation: 0.6693892274706426]
	TIME [epoch: 8.23 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.392082409187135		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.345444185444868		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.36876329731600144 | validation: 0.32432366367875576]
	TIME [epoch: 8.23 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4286868970094694		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.36200161826750354		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.3953442576384864 | validation: 0.19468670837101376]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32373775269304284		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.37908446277601743		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.35141110773453005 | validation: 0.24776499278924138]
	TIME [epoch: 8.25 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34932864825609056		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.31408361959907466		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.3317061339275826 | validation: 0.19814882652700108]
	TIME [epoch: 8.23 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35514860973140755		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.35637506400921715		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.3557618368703123 | validation: 0.2818958825049116]
	TIME [epoch: 8.22 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.348235324901719		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.3483836729102049		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.34830949890596197 | validation: 0.2926563647169709]
	TIME [epoch: 8.27 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3809386046791107		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.3773986446049688		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.3791686246420398 | validation: 0.34415025222585277]
	TIME [epoch: 8.24 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3177886719917499		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.33737239311575506		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.32758053255375247 | validation: 1.1217665784361865]
	TIME [epoch: 8.24 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43961059154320425		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.3098720627318973		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.3747413271375507 | validation: 0.4350883343575465]
	TIME [epoch: 8.24 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4116443296904471		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.35557005720417945		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.3836071934473133 | validation: 0.384425688681655]
	TIME [epoch: 8.25 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4051036278232768		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.36126141593461436		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.3831825218789455 | validation: 0.21001554023734845]
	TIME [epoch: 8.24 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3840334197683751		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.2922730265459353		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.3381532231571552 | validation: 0.48580825319556586]
	TIME [epoch: 8.22 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3218621067509884		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.35394399577183866		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.3379030512614135 | validation: 0.256748272197317]
	TIME [epoch: 8.22 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31209480479815244		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.28882156680777776		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.3004581858029651 | validation: 0.3727170513043752]
	TIME [epoch: 8.26 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39637027736962993		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.35268535882922736		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.3745278180994286 | validation: 0.302249755448192]
	TIME [epoch: 8.25 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28364869738331755		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.34474147829376967		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.3141950878385436 | validation: 0.3987879888923176]
	TIME [epoch: 8.24 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32638675348362906		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.25040090077220306		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.28839382712791606 | validation: 0.2840215745230163]
	TIME [epoch: 8.23 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34586163020903865		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.403913114961252		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.3748873725851453 | validation: 0.2169147879083254]
	TIME [epoch: 8.25 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33675231874179934		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.348061631033019		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.34240697488740923 | validation: 0.4144184722778691]
	TIME [epoch: 8.24 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28101030787135417		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.32290525312999246		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.3019577805006733 | validation: 0.40779237525472806]
	TIME [epoch: 8.23 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26239985429938595		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.31396894185509594		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.288184398077241 | validation: 0.24259546163347756]
	TIME [epoch: 8.23 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3152631313747023		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.287002069605593		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.30113260049014756 | validation: 0.39629178742869325]
	TIME [epoch: 8.27 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2581361366808601		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.3680542261982086		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.3130951814395343 | validation: 0.20000514273839468]
	TIME [epoch: 8.23 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29356233688000505		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.3344784671271657		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.3140204020035854 | validation: 0.41618970912077513]
	TIME [epoch: 8.24 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3873059355809169		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.31087659557446157		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.34909126557768916 | validation: 0.3060141129451568]
	TIME [epoch: 8.23 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3231074144965346		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.3015180970989946		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.31231275579776463 | validation: 0.20726505415589605]
	TIME [epoch: 8.26 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27332970721952504		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.3355163689082339		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.3044230380638794 | validation: 0.37322992774226715]
	TIME [epoch: 8.23 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2940342394126679		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.4185146077974905		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.35627442360507916 | validation: 0.6283833051839725]
	TIME [epoch: 8.23 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35935805499903145		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.30072302396716405		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.33004053948309775 | validation: 0.187274103027803]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2830160274304197		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.32186991064935816		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.302442969039889 | validation: 0.2683898625656853]
	TIME [epoch: 8.28 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3227367621470257		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.25187905482415346		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.2873079084855895 | validation: 0.3379858210773752]
	TIME [epoch: 8.23 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31372336025623154		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.29963344676458464		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.3066784035104081 | validation: 0.17891290664121157]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3165339707495633		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.3436296569700327		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.330081813859798 | validation: 0.4641203813767598]
	TIME [epoch: 8.22 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3458941425382093		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.3336600011436761		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.33977707184094286 | validation: 0.16057603968188247]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3003223288443072		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.3273059207844464		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.31381412481437687 | validation: 0.309553635868669]
	TIME [epoch: 8.26 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25701284415862624		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.34804581940351004		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.30252933178106817 | validation: 0.17273771732336354]
	TIME [epoch: 8.24 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3714704344342318		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.2707384657367914		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.3211044500855116 | validation: 0.346934502264574]
	TIME [epoch: 8.25 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24943822826419346		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.32844925185109697		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.2889437400576452 | validation: 0.28660271627395806]
	TIME [epoch: 8.26 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22765368367952116		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.2696901798305468		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.24867193175503402 | validation: 0.15771843636898578]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3317339599503097		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.30160672460898025		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.31667034227964497 | validation: 0.16524091020716583]
	TIME [epoch: 8.25 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2989957220144829		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.339369087393819		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.31918240470415093 | validation: 0.18109998355557294]
	TIME [epoch: 8.24 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2176157638893553		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.2825637566651733		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.2500897602772644 | validation: 0.32246911299821185]
	TIME [epoch: 8.27 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25665166080945767		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.31745219530985236		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.287051928059655 | validation: 0.1842999071010966]
	TIME [epoch: 8.24 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32110982665890736		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.2831212039541327		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.30211551530652003 | validation: 0.3058366291567044]
	TIME [epoch: 8.25 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24417468548467758		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.26204902055275603		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.2531118530187168 | validation: 0.1560311201094881]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2941273965921727		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.26999786210940374		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.28206262935078824 | validation: 0.2540282781921248]
	TIME [epoch: 8.28 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42350252424680407		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.2901690801757534		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.35683580221127875 | validation: 0.2106894093590586]
	TIME [epoch: 8.25 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30310752922065154		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.22416773879941404		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.26363763401003276 | validation: 0.3256172778409477]
	TIME [epoch: 8.24 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2711008879082558		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.35160668541530116		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.3113537866617786 | validation: 0.15858914724406878]
	TIME [epoch: 8.24 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2841004629934071		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.2715528032847967		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.2778266331391019 | validation: 0.1867130405972417]
	TIME [epoch: 8.26 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22874415415686888		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.3088639134391114		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.2688040337979901 | validation: 0.9571020318183181]
	TIME [epoch: 8.25 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4000362633365112		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.24646720988312013		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.32325173660981565 | validation: 0.13028134097489047]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28581785083497946		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.2400597005544543		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.2629387756947168 | validation: 0.18774123220207262]
	TIME [epoch: 8.25 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26924974270120877		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.3135402701971122		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.2913950064491605 | validation: 0.22127476939861093]
	TIME [epoch: 8.26 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3135017186697452		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.24384436733347242		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.2786730430016088 | validation: 0.3470256891292606]
	TIME [epoch: 8.25 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3918019509624166		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.23024559510565817		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.3110237730340374 | validation: 0.33463835621758015]
	TIME [epoch: 8.25 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33871811970968246		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.25025017159142887		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.2944841456505557 | validation: 0.21171656197491034]
	TIME [epoch: 8.24 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32801981625098736		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.2563746648722659		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.29219724056162655 | validation: 0.46302134325071553]
	TIME [epoch: 8.27 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2566140839091999		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.3271131876967894		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.2918636358029946 | validation: 0.12346219603751214]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23050595605381746		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.26691142865487183		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.24870869235434467 | validation: 0.174768598055833]
	TIME [epoch: 8.25 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2730706907788524		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.2926059869880401		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.2828383388834462 | validation: 0.2959992834790133]
	TIME [epoch: 8.24 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24360176095948996		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.21157277514183542		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.22758726805066268 | validation: 0.2371118059939888]
	TIME [epoch: 8.27 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24737228838357317		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.3178658936594519		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.28261909102151245 | validation: 0.29139875916917357]
	TIME [epoch: 8.26 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5555180855536437		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.36798945420557383		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.4617537698796087 | validation: 0.1964129019517495]
	TIME [epoch: 8.24 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2437464512382414		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.2101650748307926		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.22695576303451706 | validation: 0.30892566934978205]
	TIME [epoch: 8.24 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26870954892117566		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.26543664568780734		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.26707309730449147 | validation: 0.3518842747555235]
	TIME [epoch: 8.27 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.270170406830784		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.24592762611995767		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.25804901647537093 | validation: 0.1915885713988271]
	TIME [epoch: 8.24 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21190321838247422		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.32885303959865353		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.2703781289905639 | validation: 0.18543720209171444]
	TIME [epoch: 8.24 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2406857002957558		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.2623026461048807		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.2514941732003183 | validation: 0.41289788499071167]
	TIME [epoch: 8.24 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2555241161893958		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.2989354866929016		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.2772298014411487 | validation: 0.14431172824523136]
	TIME [epoch: 8.25 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30475128517704236		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.3735366761168232		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.33914398064693285 | validation: 0.28017340546755315]
	TIME [epoch: 8.26 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2792755618076851		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.3534828512078824		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.3163792065077838 | validation: 0.2851011602867843]
	TIME [epoch: 8.24 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2295352046987047		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.1745219516547715		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.20202857817673808 | validation: 0.11361109339779546]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22386049011890585		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.190115558257357		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.2069880241881314 | validation: 0.14026570245384276]
	TIME [epoch: 8.28 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1882879823242408		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.21097221715891404		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.19963009974157744 | validation: 0.21949603717107719]
	TIME [epoch: 8.24 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22901569268475344		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.2353919805588108		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.2322038366217821 | validation: 0.40007354654094285]
	TIME [epoch: 8.24 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29618200125578437		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.2302129251594899		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.26319746320763715 | validation: 0.22210300070517688]
	TIME [epoch: 8.26 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20850176126124492		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.25890302872705195		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.23370239499414844 | validation: 0.3983380135671416]
	TIME [epoch: 8.27 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24684139128532903		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.20404851081155684		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.22544495104844292 | validation: 0.18957948879220904]
	TIME [epoch: 8.25 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2171662709091146		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.21676471510485418		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.21696549300698442 | validation: 0.2250760109206734]
	TIME [epoch: 8.24 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23827286405640313		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.27608652074573065		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.257179692401067 | validation: 0.45121714504521293]
	TIME [epoch: 8.24 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24411794839035572		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.31417217577243683		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.2791450620813963 | validation: 0.31795995984882297]
	TIME [epoch: 8.27 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22362793782372464		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.25203800031435064		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.23783296906903773 | validation: 0.32079849991289366]
	TIME [epoch: 8.26 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17994644421136946		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.22315561707794881		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.20155103064465915 | validation: 0.4389977418285138]
	TIME [epoch: 8.24 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22907735587780662		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.46649979959737076		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.3477885777375887 | validation: 0.3149156897823424]
	TIME [epoch: 8.24 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.204601649061629		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.2573963220584571		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.23099898556004309 | validation: 0.319169391729888]
	TIME [epoch: 8.26 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25177032517452236		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.25721049337509705		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.2544904092748097 | validation: 0.12492497047222971]
	TIME [epoch: 8.25 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20902996233157287		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.23199003207412355		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.22050999720284822 | validation: 0.24845856066001532]
	TIME [epoch: 8.24 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2534401284881659		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.22647957779849617		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.23995985314333096 | validation: 0.4314335278461817]
	TIME [epoch: 8.25 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2453802441845559		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.17415172311881805		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.20976598365168703 | validation: 0.21634737056668224]
	TIME [epoch: 8.26 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23570818941647143		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.23510194421210479		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.23540506681428805 | validation: 0.0844576956970046]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28396099944870884		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.20392167131911534		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.24394133538391208 | validation: 0.15477125290744465]
	TIME [epoch: 8.24 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22072385521928087		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.2534535999802528		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.23708872759976685 | validation: 0.16869413988779192]
	TIME [epoch: 8.24 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.221143471296		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.21853875810955917		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.2198411147027796 | validation: 0.4662007967344064]
	TIME [epoch: 8.26 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3296006973576426		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.20732007215558784		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.26846038475661527 | validation: 0.13947217189146693]
	TIME [epoch: 8.24 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17369201998744332		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.24528218999736576		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.20948710499240453 | validation: 0.2311329340977834]
	TIME [epoch: 8.24 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21175253414462336		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.21182555281875048		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.21178904348168692 | validation: 0.685758577059141]
	TIME [epoch: 8.23 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2636626299796142		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.2859355864345666		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.27479910820709036 | validation: 0.1506897834852322]
	TIME [epoch: 8.27 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19504126373731287		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.21760792824341563		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.2063245959903642 | validation: 0.34283202885754566]
	TIME [epoch: 8.25 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22037482475464648		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.2727046506280424		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.24653973769134443 | validation: 0.2260434243108121]
	TIME [epoch: 8.24 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26683047234689933		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.18768887242433527		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.22725967238561728 | validation: 0.14387949350779455]
	TIME [epoch: 8.25 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2211591411000275		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.243506382180045		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.2323327616400362 | validation: 0.14112346107008472]
	TIME [epoch: 8.26 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17169236147905037		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.20275532060788587		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.18722384104346815 | validation: 0.09097632148669789]
	TIME [epoch: 8.24 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2048233465831731		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.17828628911701544		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.1915548178500943 | validation: 0.1492417424963277]
	TIME [epoch: 8.23 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1675122805779952		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.22924101148759016		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.19837664603279265 | validation: 0.20316606826080352]
	TIME [epoch: 8.24 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22732652675815662		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.26104036239521533		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.24418344457668595 | validation: 0.1850003732239686]
	TIME [epoch: 8.26 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2455406251148892		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.3207438631038094		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.28314224410934935 | validation: 0.21786953189569322]
	TIME [epoch: 8.23 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20591891017927075		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.29575606800640797		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.2508374890928394 | validation: 0.15629692668869888]
	TIME [epoch: 8.25 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17485435380820782		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.18449106536834187		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.1796727095882748 | validation: 0.2809134905836322]
	TIME [epoch: 8.24 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22890285159765647		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.21851579472882982		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.2237093231632432 | validation: 0.17956024079019509]
	TIME [epoch: 8.25 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1803039247081743		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.17053143836819704		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.17541768153818568 | validation: 0.1828857524748446]
	TIME [epoch: 8.26 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2217131592483419		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.23344922986997968		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.22758119455916082 | validation: 0.1363414923537289]
	TIME [epoch: 8.24 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20940460660363008		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.18497238544166789		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.197188496022649 | validation: 0.272208084540817]
	TIME [epoch: 8.24 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17826441730315265		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.18302332135831914		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.1806438693307359 | validation: 0.38588662932158]
	TIME [epoch: 8.25 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21054326576151766		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.2133968384105474		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.2119700520860325 | validation: 0.15298761210816858]
	TIME [epoch: 8.26 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18638184036438604		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.21399808972327797		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.200189965043832 | validation: 0.2872200246203368]
	TIME [epoch: 8.24 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22848638736937277		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.19006327036217355		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.20927482886577314 | validation: 0.10274737795850925]
	TIME [epoch: 8.23 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22072291533983832		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.24227865793888648		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.2315007866393624 | validation: 0.12026815193145934]
	TIME [epoch: 8.26 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1502650099731363		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.21534151434829268		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.1828032621607145 | validation: 0.22149627544521036]
	TIME [epoch: 8.25 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18827564852207854		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.1704806235442929		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.1793781360331857 | validation: 0.13605224357024803]
	TIME [epoch: 8.24 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.150601210214641		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.25781009600536875		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.20420565311000488 | validation: 0.1599448322881219]
	TIME [epoch: 8.23 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21510646625700894		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.17741758929536713		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.19626202777618804 | validation: 0.1198645301212301]
	TIME [epoch: 8.25 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2152462280320428		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.1951312889920266		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.20518875851203466 | validation: 0.5057716596660548]
	TIME [epoch: 8.26 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19155698989835707		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.282937521822344		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.23724725586035053 | validation: 0.2021419803918737]
	TIME [epoch: 8.24 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19720966578137408		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.19169541175998195		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.194452538770678 | validation: 0.2013342897404657]
	TIME [epoch: 8.25 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20235136676062804		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.15463074914999428		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.17849105795531117 | validation: 0.15494306294255233]
	TIME [epoch: 8.24 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2307287890910735		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.17842289091472868		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.2045758400029011 | validation: 0.08739717860828089]
	TIME [epoch: 8.27 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18896230692620813		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.18396729841139176		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.1864648026688 | validation: 0.15131537055806218]
	TIME [epoch: 8.25 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17194517849718033		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.19455942614731037		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.18325230232224535 | validation: 0.10023574106047915]
	TIME [epoch: 8.24 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15505248507164088		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.19711237498623269		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.17608243002893678 | validation: 0.09772480273805823]
	TIME [epoch: 8.23 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1973958986133651		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.14888453354054157		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.17314021607695335 | validation: 0.18217853168108003]
	TIME [epoch: 8.26 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44835882014991996		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.20247814848631376		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.3254184843181169 | validation: 0.13736522483366478]
	TIME [epoch: 8.24 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.203498167255324		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.18324510451242326		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.19337163588387365 | validation: 0.5397839720310134]
	TIME [epoch: 8.25 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.225465452855865		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.20059939200420213		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.21303242243003365 | validation: 0.19507585703814917]
	TIME [epoch: 8.25 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21487578275808955		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.15007297557157745		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.1824743791648335 | validation: 0.23821689047863462]
	TIME [epoch: 8.26 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20072454182926394		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.18421599417696646		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.1924702680031152 | validation: 0.19058285465286892]
	TIME [epoch: 8.24 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17607136702215778		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.1946970701302166		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.18538421857618717 | validation: 0.14981546096718346]
	TIME [epoch: 8.25 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16722908637767875		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.18909036519036612		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.17815972578402245 | validation: 0.34940250195922284]
	TIME [epoch: 8.24 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18215287385798085		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.17185608385669385		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.17700447885733736 | validation: 0.14258595135573582]
	TIME [epoch: 8.26 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23033692252357083		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.17669391682582386		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.20351541967469733 | validation: 0.2820195428623079]
	TIME [epoch: 8.24 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15958877145594902		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.22405365503709013		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.19182121324651957 | validation: 0.2707493906956641]
	TIME [epoch: 8.24 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2024946467173027		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.2873601702394599		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.24492740847838132 | validation: 0.6899645816349]
	TIME [epoch: 8.25 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2668720275090989		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.18193620273556682		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.22440411512233283 | validation: 0.2655653935129497]
	TIME [epoch: 8.26 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16976534678848063		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.18724689784226348		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.17850612231537205 | validation: 0.31467984076484457]
	TIME [epoch: 8.24 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18704271918854792		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.15046183151492523		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.1687522753517366 | validation: 0.08292000687869498]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1609253938950786		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.27060288345268124		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.21576413867387992 | validation: 0.11816528601409469]
	TIME [epoch: 8.26 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13081531472057378		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.1812594715249421		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.15603739312275794 | validation: 0.1725247010093677]
	TIME [epoch: 8.26 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17443630510562722		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.1569687853143119		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.16570254520996958 | validation: 0.10266824537187078]
	TIME [epoch: 8.24 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18358310697497965		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.19652455613207018		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.19005383155352493 | validation: 0.6345523441520518]
	TIME [epoch: 8.23 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2396518444561237		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.15744315255691124		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.19854749850651746 | validation: 0.09514149854760504]
	TIME [epoch: 8.23 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.182411534310491		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.1579697570640729		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.17019064568728193 | validation: 0.16716600560394748]
	TIME [epoch: 8.26 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1374846956787632		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.34556224202342006		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.2415234688510916 | validation: 0.1999595487393318]
	TIME [epoch: 8.24 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12319573389460718		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.17362137469063416		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.14840855429262065 | validation: 0.2509793909107334]
	TIME [epoch: 8.23 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1540628234717925		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.155910358062913		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.15498659076735274 | validation: 0.17549299214557645]
	TIME [epoch: 8.24 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19797612356289854		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.1540477161161375		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.176011919839518 | validation: 0.22836339381076778]
	TIME [epoch: 8.26 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18543565198184148		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.13268224002947154		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.15905894600565645 | validation: 0.18683380616148682]
	TIME [epoch: 8.27 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15270823076467088		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.17910151987171877		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.16590487531819476 | validation: 0.10766362408814675]
	TIME [epoch: 8.24 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2022183011676592		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.175527898064038		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.18887309961584864 | validation: 0.08519537533602076]
	TIME [epoch: 8.25 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17217132145963032		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.2484237730285387		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.21029754724408453 | validation: 0.22418268033824554]
	TIME [epoch: 8.27 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15556057018036354		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.24181114623756686		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.19868585820896517 | validation: 0.21987671080134946]
	TIME [epoch: 8.25 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1859256593848746		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.24951772174815545		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.217721690566515 | validation: 0.5020765794893953]
	TIME [epoch: 8.24 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18827613912193816		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.15674137111628864		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.1725087551191134 | validation: 0.17998072151094835]
	TIME [epoch: 8.25 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21428305828361904		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.17639217496367282		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.19533761662364596 | validation: 0.19332467138885556]
	TIME [epoch: 8.25 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16998764904920566		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.19099673956999763		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.18049219430960167 | validation: 0.15457606923680603]
	TIME [epoch: 8.23 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13581136186680506		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.1930099356660574		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.16441064876643124 | validation: 0.18295424576543645]
	TIME [epoch: 8.24 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20051416467589397		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.14796341579646874		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.17423879023618136 | validation: 0.13711158000923204]
	TIME [epoch: 8.23 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19963954103745776		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.19130293793609826		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.19547123948677797 | validation: 0.06772862599094008]
	TIME [epoch: 8.27 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16803797534613565		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.19367620974471614		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.18085709254542587 | validation: 0.35786400477491687]
	TIME [epoch: 8.24 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1479131883929315		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.24753380930560187		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.19772349884926663 | validation: 0.15590606660306872]
	TIME [epoch: 8.24 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23086933449473918		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.20180802223718336		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.21633867836596127 | validation: 0.14652400121151654]
	TIME [epoch: 8.23 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17545497774245872		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.15613758874135092		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.16579628324190487 | validation: 0.23826506378557913]
	TIME [epoch: 8.25 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13791593739940403		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.16523909025832012		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.1515775138288621 | validation: 0.06825961056749953]
	TIME [epoch: 8.24 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12996458819172158		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.1918315325724808		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.1608980603821012 | validation: 0.10326298941013397]
	TIME [epoch: 8.23 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13666316263583841		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.17271086649859926		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.15468701456721887 | validation: 0.30931926672181675]
	TIME [epoch: 8.22 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1936520968798031		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.15630864175913847		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.17498036931947078 | validation: 0.18666612916081213]
	TIME [epoch: 8.26 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17231492033477838		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.18888461824659114		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.18059976929068472 | validation: 0.08178343787808556]
	TIME [epoch: 8.24 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13248245993610924		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.1706021441315018		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.1515423020338055 | validation: 0.2199632002100871]
	TIME [epoch: 8.23 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18766554129981622		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.1422826945787406		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.16497411793927838 | validation: 0.12330597148172168]
	TIME [epoch: 8.23 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14574323317529153		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.14275185552558928		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.14424754435044043 | validation: 0.0985235024061245]
	TIME [epoch: 8.25 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16771318943910807		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.1508935225261347		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.15930335598262138 | validation: 0.10306719603874355]
	TIME [epoch: 8.24 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11796073643747354		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.14567135666576642		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.13181604655162 | validation: 0.08439977415140042]
	TIME [epoch: 8.23 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1275801542616824		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.18661184700577932		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.15709600063373086 | validation: 0.29686798781163753]
	TIME [epoch: 8.21 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17233046296149435		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.13725687866010777		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.15479367081080106 | validation: 0.18500874416021]
	TIME [epoch: 8.25 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16756764605021238		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.1692832555366755		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.1684254507934439 | validation: 0.08408276922876284]
	TIME [epoch: 8.23 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17844205277186914		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.13860349351240261		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.15852277314213586 | validation: 0.06300224869478876]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1437862315057428		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.16098256865613858		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.15238440008094073 | validation: 0.17044332781895905]
	TIME [epoch: 8.47 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13560544460373006		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.16705252973080076		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.15132898716726545 | validation: 0.09715905456033945]
	TIME [epoch: 8.26 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16940097791984166		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.1561317622667038		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.16276637009327272 | validation: 0.13039715914499575]
	TIME [epoch: 8.24 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11710613730783848		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.18394554523001044		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.15052584126892446 | validation: 0.22993781009413847]
	TIME [epoch: 8.24 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2146368371735453		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.15184485913757645		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.18324084815556088 | validation: 0.183932554885937]
	TIME [epoch: 8.23 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17201709218497738		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.12200990964198086		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.14701350091347914 | validation: 0.11515632439012244]
	TIME [epoch: 8.26 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16716588102718147		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.16497165135245362		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.16606876618981753 | validation: 0.23021052899333622]
	TIME [epoch: 8.24 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1579259047754244		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.20409594342851434		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.18101092410196937 | validation: 0.1825301147826165]
	TIME [epoch: 8.23 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1474952056740867		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.17585659660482716		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.16167590113945696 | validation: 0.27213640262721217]
	TIME [epoch: 8.22 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15924968762874073		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.164172552361525		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.16171111999513288 | validation: 0.15303481076922948]
	TIME [epoch: 8.26 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1745412421776786		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.1554752502990205		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.1650082462383496 | validation: 0.1339291054125196]
	TIME [epoch: 8.24 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13336804917142356		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.1878008984260051		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.16058447379871432 | validation: 0.13049137861614132]
	TIME [epoch: 8.24 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15578621612383045		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.1602280883029884		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.15800715221340947 | validation: 0.25471956368372556]
	TIME [epoch: 8.24 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18034179806713121		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.13867734666131482		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.15950957236422297 | validation: 0.261172323325909]
	TIME [epoch: 8.26 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15222195437017647		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.15602271983868193		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.15412233710442919 | validation: 0.11583458370328914]
	TIME [epoch: 8.25 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15369575332770086		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.11614941759295509		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.134922585460328 | validation: 0.09025012200455876]
	TIME [epoch: 8.24 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1324727983584301		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.11077910419248944		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.12162595127545975 | validation: 0.13030699789845013]
	TIME [epoch: 8.23 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18924754202526173		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.16080877228443632		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.175028157154849 | validation: 0.11254233857130608]
	TIME [epoch: 8.26 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14042227973363222		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.16254927926208412		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.1514857794978582 | validation: 0.15686685897059918]
	TIME [epoch: 8.25 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1433656294342879		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.19850055097008978		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.17093309020218883 | validation: 0.12723614060981456]
	TIME [epoch: 8.24 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16507319859319933		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.1604966739426189		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.16278493626790913 | validation: 0.08912477445605477]
	TIME [epoch: 8.24 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15254462684558226		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.17033229737023653		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.16143846210790938 | validation: 0.13166473212321564]
	TIME [epoch: 8.27 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16476417606813953		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.1663513280872004		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.16555775207767 | validation: 0.21880533440645783]
	TIME [epoch: 8.25 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15197450526413803		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.1500521462659545		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.15101332576504628 | validation: 0.17756221684185444]
	TIME [epoch: 8.25 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14735922408706315		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.13285926884549878		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.14010924646628098 | validation: 0.12040465896475563]
	TIME [epoch: 8.24 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15624668937408173		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.3035419526437485		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.2298943210089151 | validation: 0.08174721512388977]
	TIME [epoch: 8.27 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13721906189465785		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.12773296025075243		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.13247601107270512 | validation: 0.07855171184973883]
	TIME [epoch: 8.24 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13179109071450107		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.1831701025908085		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.15748059665265476 | validation: 0.12860863262108643]
	TIME [epoch: 8.23 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12224465507389219		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.12358136580030495		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.12291301043709857 | validation: 0.07985863768066695]
	TIME [epoch: 8.24 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12081821603621165		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.18236964517693344		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.1515939306065725 | validation: 0.12303799086153366]
	TIME [epoch: 8.26 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16176001989093744		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.13823825165418904		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.14999913577256324 | validation: 0.14111609030663846]
	TIME [epoch: 8.24 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1470377403271628		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 0.1333275673721064		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.14018265384963458 | validation: 0.1294069453701196]
	TIME [epoch: 8.24 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16458209543371338		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.12916689490044225		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.1468744951670778 | validation: 0.06930215774216157]
	TIME [epoch: 8.24 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10629247136638731		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.10902477870361157		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.10765862503499948 | validation: 0.09883355220335155]
	TIME [epoch: 8.26 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1395616159511514		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.12752806111894466		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.13354483853504803 | validation: 0.16200517478249474]
	TIME [epoch: 8.24 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20290981998248095		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.13825069138635682		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.17058025568441887 | validation: 0.08312319337089744]
	TIME [epoch: 8.24 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11906866428587015		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.11753477677491056		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.11830172053039034 | validation: 0.24799149309653407]
	TIME [epoch: 8.24 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1369534085170525		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.1497177911387195		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.14333559982788605 | validation: 0.1738059343957032]
	TIME [epoch: 8.26 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14521435297466143		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.11081015863049637		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.12801225580257888 | validation: 0.14283056068703062]
	TIME [epoch: 8.24 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11772584181559768		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.10611543988065661		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.11192064084812717 | validation: 0.14107450332200916]
	TIME [epoch: 8.24 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1605768964788818		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.15388508275021967		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.15723098961455073 | validation: 0.1272977640692894]
	TIME [epoch: 8.23 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14689791458879267		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.11251694873013385		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.12970743165946325 | validation: 0.1496099844537373]
	TIME [epoch: 8.26 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12062648699119723		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.14913470429546127		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.13488059564332927 | validation: 0.1725340241823886]
	TIME [epoch: 8.24 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1653820381139038		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.124421087190509		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.1449015626522064 | validation: 0.13056589568038224]
	TIME [epoch: 8.25 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11573006090632762		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.15152628549576136		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.13362817320104448 | validation: 0.10962439724971264]
	TIME [epoch: 8.24 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12971898711189006		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.10969329828092947		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.11970614269640975 | validation: 0.07313790515148746]
	TIME [epoch: 8.27 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12014726754573916		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.13988836242036115		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.13001781498305018 | validation: 0.15693768884951503]
	TIME [epoch: 8.24 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1389228198516394		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.09473595535351245		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.11682938760257593 | validation: 0.11631120544407632]
	TIME [epoch: 8.24 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13838583654638986		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.1784393343891936		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.15841258546779174 | validation: 0.07867208830605214]
	TIME [epoch: 8.25 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12533651141577629		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.11550955458882066		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.12042303300229848 | validation: 0.14959124407842606]
	TIME [epoch: 8.27 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13464030086606585		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.09798909407507937		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.11631469747057259 | validation: 0.15746169448728367]
	TIME [epoch: 8.26 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14889966389697232		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.1534531120812514		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.15117638798911182 | validation: 0.23157642419381608]
	TIME [epoch: 8.24 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1808104031146148		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.1328132698415269		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.15681183647807084 | validation: 0.14533156540597092]
	TIME [epoch: 8.25 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12206398282830208		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.11806641148802025		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.12006519715816118 | validation: 0.08272691017382675]
	TIME [epoch: 8.26 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11154110967797781		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.12203889642906651		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.11679000305352216 | validation: 0.10235077790066859]
	TIME [epoch: 8.25 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12497163656658321		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.08643069244609515		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.10570116450633918 | validation: 0.133224067680968]
	TIME [epoch: 8.24 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1395046222654811		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.11539621873469974		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.12745042050009042 | validation: 0.08467797433690746]
	TIME [epoch: 8.25 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12171433019644047		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.09526238311433335		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.10848835665538689 | validation: 0.0732778822881602]
	TIME [epoch: 8.27 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1214647921891115		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.13226919186863098		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.12686699202887122 | validation: 0.09834562091446297]
	TIME [epoch: 8.25 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11126398494858054		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 0.15995658777699256		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 0.13561028636278655 | validation: 0.08237832781693194]
	TIME [epoch: 8.25 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1059047848239418		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.12496877137126386		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.11543677809760285 | validation: 0.08819516574392562]
	TIME [epoch: 8.24 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12040764983560384		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.10799834144877585		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.11420299564218986 | validation: 0.09968125600638165]
	TIME [epoch: 8.27 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11096495709649759		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.12488634473943534		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.11792565091796647 | validation: 0.08018763068230769]
	TIME [epoch: 8.24 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.116876690246151		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 0.1349238247452103		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 0.12590025749568068 | validation: 0.1646461117874905]
	TIME [epoch: 8.24 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12542489818499133		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 0.08007176188059209		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 0.10274833003279168 | validation: 0.19916984822212097]
	TIME [epoch: 8.24 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1584500266026968		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.10794081477667157		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.13319542068968418 | validation: 0.16512235808424597]
	TIME [epoch: 8.26 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10487731543215764		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.1465774084273883		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.12572736192977296 | validation: 0.11982233527482945]
	TIME [epoch: 8.25 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10439685872719415		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.12835233911449312		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.11637459892084365 | validation: 0.1663385392565216]
	TIME [epoch: 8.25 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14798515503067128		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.1425149230912614		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.14525003906096637 | validation: 0.16692330146099738]
	TIME [epoch: 8.24 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17292234226315223		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.13280779065168813		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.1528650664574202 | validation: 0.12097250263062455]
	TIME [epoch: 8.26 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13400287747055056		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 0.1173074815905621		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 0.12565517953055633 | validation: 0.15678453015854252]
	TIME [epoch: 8.25 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13737718082534373		[learning rate: 0.002807]
		[batch 20/20] avg loss: 0.16049960583307965		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 0.14893839332921172 | validation: 0.2347755396174212]
	TIME [epoch: 8.24 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17055443984055843		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 0.11996921455244831		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 0.14526182719650332 | validation: 0.12470264693664615]
	TIME [epoch: 8.24 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1292193987633394		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 0.12468632500193874		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 0.12695286188263905 | validation: 0.1713819632827288]
	TIME [epoch: 8.27 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11008996650574668		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 0.11702670489632579		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 0.11355833570103621 | validation: 0.06423542102912919]
	TIME [epoch: 8.25 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14271275811795825		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 0.10760193743253472		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 0.12515734777524648 | validation: 0.08691902914983131]
	TIME [epoch: 8.24 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13274189291819155		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 0.16254450065228201		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 0.14764319678523677 | validation: 0.0899728363614836]
	TIME [epoch: 8.24 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16483582018675574		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 0.10843992227680146		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 0.13663787123177865 | validation: 0.0914821068227809]
	TIME [epoch: 8.27 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12454935066757958		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 0.11924835161268128		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 0.12189885114013044 | validation: 0.11413871216040868]
	TIME [epoch: 8.25 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1300692733392939		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 0.11781760551277405		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 0.12394343942603399 | validation: 0.8421316148408324]
	TIME [epoch: 8.24 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26889773992676325		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 0.10549309679014778		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 0.1871954183584555 | validation: 0.10091038817764701]
	TIME [epoch: 8.25 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12025606586451425		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 0.12804945917290017		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 0.12415276251870722 | validation: 0.0966616107611748]
	TIME [epoch: 8.26 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1130409717251496		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 0.11863509850145279		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 0.1158380351133012 | validation: 0.14776093097949858]
	TIME [epoch: 8.25 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1880335155779646		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 0.16859506323542792		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 0.17831428940669625 | validation: 0.10595668926239375]
	TIME [epoch: 8.25 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16702125612087243		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 0.11159622829917079		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 0.1393087422100216 | validation: 0.0738179369603894]
	TIME [epoch: 8.25 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07883125430074138		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 0.1466012995632942		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 0.1127162769320178 | validation: 0.0930016348625636]
	TIME [epoch: 8.27 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1133715817663673		[learning rate: 0.002658]
		[batch 20/20] avg loss: 0.12464039038019106		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 0.1190059860732792 | validation: 0.1480394508509346]
	TIME [epoch: 8.25 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1531645414245476		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.13683899965408555		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.14500177053931657 | validation: 0.06624157989071178]
	TIME [epoch: 8.24 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12555548669333		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.10503764919557693		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.11529656794445346 | validation: 0.19772710943777733]
	TIME [epoch: 8.24 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14878004765604813		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.09883059985588337		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.12380532375596576 | validation: 0.07353769437930803]
	TIME [epoch: 8.26 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13866452729721895		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.10240059027637134		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.12053255878679514 | validation: 0.08981438512608315]
	TIME [epoch: 8.26 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10366889052606214		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.11773820864924514		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.11070354958765363 | validation: 0.08333726238565693]
	TIME [epoch: 8.24 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13151385726857967		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.09859536979086685		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.11505461352972328 | validation: 0.06318793255966981]
	TIME [epoch: 8.24 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11040689978802229		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.12322494190611818		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.11681592084707024 | validation: 0.14010055030366356]
	TIME [epoch: 8.25 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1216386766306182		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.1130881501111323		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.11736341337087523 | validation: 0.15944123206554262]
	TIME [epoch: 8.26 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14716863787418097		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.10413452258372435		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.12565158022895268 | validation: 0.05275946521987865]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12857220875796066		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.13204784114582435		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.13031002495189248 | validation: 0.11038685640217175]
	TIME [epoch: 8.25 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11344586840859269		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.12078128729218836		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.11711357785039056 | validation: 0.15625255756435388]
	TIME [epoch: 8.26 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09784016029332596		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.10852805353608799		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.10318410691470696 | validation: 0.07462347766283693]
	TIME [epoch: 8.24 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09806465639946138		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.14245382456234273		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.12025924048090211 | validation: 0.07023526659044116]
	TIME [epoch: 8.24 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07747974788025304		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.10363995973533657		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.0905598538077948 | validation: 0.1600333663301488]
	TIME [epoch: 8.23 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10549542199043738		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.11967755571610177		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.11258648885326956 | validation: 0.09574627231740782]
	TIME [epoch: 8.24 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11271142849688834		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.15184591004741319		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.13227866927215076 | validation: 0.09635244305782846]
	TIME [epoch: 8.27 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1211255344138796		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.11021164535665182		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.1156685898852657 | validation: 0.06609305946983082]
	TIME [epoch: 8.24 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09939687746881579		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.10890909823520205		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.10415298785200892 | validation: 0.0886778082303909]
	TIME [epoch: 8.24 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07550306358528071		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.12168160513921193		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.09859233436224632 | validation: 0.08798349060229368]
	TIME [epoch: 8.25 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09553987757075469		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.08593453751099746		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.09073720754087608 | validation: 0.059930031816610524]
	TIME [epoch: 8.27 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10366076309948673		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.10754877068211562		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.10560476689080119 | validation: 0.13593517281699]
	TIME [epoch: 8.24 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08715656035990912		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.09354656339580995		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.09035156187785956 | validation: 0.12987003055720073]
	TIME [epoch: 8.24 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09804282701918839		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.08763546208074302		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.0928391445499657 | validation: 0.0669458924902145]
	TIME [epoch: 8.24 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11155410520213134		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.10535903720718336		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.10845657120465735 | validation: 0.0967429708723122]
	TIME [epoch: 8.27 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08974577524873015		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.08774270209674365		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.08874423867273691 | validation: 0.08644678098070425]
	TIME [epoch: 8.24 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09726350855287995		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.10178211177179386		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.09952281016233691 | validation: 0.11380199561776663]
	TIME [epoch: 8.24 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11802269599114217		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.13947657928687815		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.12874963763901018 | validation: 0.06343931635643749]
	TIME [epoch: 8.24 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11520751723331737		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.08268310243118696		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.09894530983225215 | validation: 0.06452470848113034]
	TIME [epoch: 8.26 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10773285372358396		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.1960702176712686		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.15190153569742626 | validation: 0.15994873980205432]
	TIME [epoch: 8.24 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10257434522690183		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.08306098867395668		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.09281766695042926 | validation: 0.23024928788466623]
	TIME [epoch: 8.24 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08963629091327288		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.1112309195380973		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.10043360522568509 | validation: 0.1628938703738631]
	TIME [epoch: 8.24 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11285490786556554		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.0692843742708839		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.09106964106822472 | validation: 0.04934810186161772]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10045238700696033		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.10093996354236938		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.10069617527466487 | validation: 0.053902239757840635]
	TIME [epoch: 8.25 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10788948489886489		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.09179426596639834		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.0998418754326316 | validation: 0.07642385773668647]
	TIME [epoch: 8.24 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11127021168312734		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.1898284088941359		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.15054931028863167 | validation: 0.05088669051183594]
	TIME [epoch: 8.23 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08859444525332924		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.09898180706223392		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.09378812615778159 | validation: 0.04940242190695933]
	TIME [epoch: 8.25 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0810762009516592		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.11573755887169868		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.09840687991167894 | validation: 0.15056554786515108]
	TIME [epoch: 8.23 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13748260470040277		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.11319704697872261		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.12533982583956266 | validation: 0.08741027285508335]
	TIME [epoch: 8.24 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09852812376922063		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.10091608782079908		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.09972210579500987 | validation: 0.22421153914331246]
	TIME [epoch: 8.23 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12769594584310667		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.10625207159181467		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.11697400871746066 | validation: 0.08544110270970823]
	TIME [epoch: 8.27 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1064735415410177		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.08073486883252401		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.09360420518677084 | validation: 0.05581332005103662]
	TIME [epoch: 8.23 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12020487838776979		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.08863299606737256		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.10441893722757119 | validation: 0.0693736990676771]
	TIME [epoch: 8.23 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10131390817819055		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.0952845828132727		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.09829924549573163 | validation: 0.0852674900379127]
	TIME [epoch: 8.24 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08238793805799777		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.08406860848131273		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.08322827326965525 | validation: 0.1489581470729548]
	TIME [epoch: 8.26 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10144097487215993		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.0709014907561877		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.08617123281417381 | validation: 0.06633643552578948]
	TIME [epoch: 8.24 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08135061151312392		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.10457973257809157		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.09296517204560777 | validation: 0.09970946963325922]
	TIME [epoch: 8.23 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07598945893133316		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.08088238724723922		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.07843592308928618 | validation: 0.045970856530946125]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0898123449889669		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.12910945758384598		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.10946090128640644 | validation: 0.07826847841628089]
	TIME [epoch: 8.26 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0784289032107152		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.11095108047422313		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.09468999184246917 | validation: 0.06248893638461815]
	TIME [epoch: 8.24 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07238007256997715		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.11424962191968646		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.0933148472448318 | validation: 0.08836013806630769]
	TIME [epoch: 8.22 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0844508454422963		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.07407208522702657		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.07926146533466143 | validation: 0.08783820193358838]
	TIME [epoch: 8.23 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07762058192802167		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.087325913772588		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.08247324785030483 | validation: 0.1627230676311967]
	TIME [epoch: 8.25 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10316084771291945		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.0954925763773293		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.09932671204512439 | validation: 0.0975104093828495]
	TIME [epoch: 8.23 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08928062081693039		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.08029101775301224		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.08478581928497132 | validation: 0.19388387056092388]
	TIME [epoch: 8.23 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10478214992060571		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.09418245667222973		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.09948230329641773 | validation: 0.09662938619523426]
	TIME [epoch: 8.24 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08926593891174042		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.08165332565784825		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.08545963228479433 | validation: 0.0920573423384276]
	TIME [epoch: 8.25 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11294478568807435		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.09858299087150103		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.10576388827978767 | validation: 0.07983563152468146]
	TIME [epoch: 8.24 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08137606877361572		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.09908181047620485		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.09022893962491028 | validation: 0.13456825141230827]
	TIME [epoch: 8.23 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1177797820137596		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.08655360210643612		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.10216669206009785 | validation: 0.10019703733535525]
	TIME [epoch: 8.24 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08359188721455799		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.07211857852725481		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.07785523287090641 | validation: 0.105116599351121]
	TIME [epoch: 8.25 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09175861239407501		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.09454487969561896		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.09315174604484701 | validation: 0.03743533374704204]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07137427381843967		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.07619166972160847		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.07378297177002405 | validation: 0.059997599746569936]
	TIME [epoch: 8.24 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1103765955764755		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.08842048235737093		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.0993985389669232 | validation: 0.1505551060272009]
	TIME [epoch: 8.23 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11069773914654302		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.07008796522434509		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.09039285218544404 | validation: 0.12341183831733221]
	TIME [epoch: 8.25 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12069272241864852		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.08931280117971657		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.10500276179918255 | validation: 0.06750976317966725]
	TIME [epoch: 8.24 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08076184808162534		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.07007934043597687		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.0754205942588011 | validation: 0.04975357677935061]
	TIME [epoch: 8.23 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08564951821412439		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.08110900753251064		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.08337926287331751 | validation: 0.10482167354782546]
	TIME [epoch: 8.24 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09883170618853618		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.10006957375739063		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.09945063997296341 | validation: 0.06098581530858011]
	TIME [epoch: 8.26 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08003120417938439		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.11887144956649667		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.09945132687294053 | validation: 0.14680779838841812]
	TIME [epoch: 8.22 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1261201001512574		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.07431254251422162		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.1002163213327395 | validation: 0.054111426898394085]
	TIME [epoch: 8.23 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11355936813699978		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.0896820488136932		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.10162070847534647 | validation: 0.0810923795146744]
	TIME [epoch: 8.23 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08518815095300494		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.10905048411322964		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.0971193175331173 | validation: 0.07715061830784914]
	TIME [epoch: 8.25 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07872144785981355		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.10679154380430844		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.092756495832061 | validation: 0.06195959656488319]
	TIME [epoch: 8.24 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08450877604235729		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.12202214199832702		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.10326545902034216 | validation: 0.08262341010841917]
	TIME [epoch: 8.24 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07599852366871869		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.07594877831821678		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.07597365099346774 | validation: 0.06593775406527826]
	TIME [epoch: 8.24 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0962380448207898		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.11017093578194788		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.10320449030136884 | validation: 0.13151913808591478]
	TIME [epoch: 8.25 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13415633465670668		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.10996962088887856		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.1220629777727926 | validation: 0.13399805369500514]
	TIME [epoch: 8.24 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.103658478763244		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.08528522288769876		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.09447185082547138 | validation: 0.08275281958947234]
	TIME [epoch: 8.24 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07324836870556058		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.07543168142354391		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.07434002506455226 | validation: 0.04307252924815857]
	TIME [epoch: 8.25 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08487177030480089		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.08468861502271395		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.08478019266375743 | validation: 0.051086459301748835]
	TIME [epoch: 8.25 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06754245095979813		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.0894691745263638		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.07850581274308097 | validation: 0.05698994308885583]
	TIME [epoch: 8.25 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10319151770611797		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.07777334166918155		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.09048242968764975 | validation: 0.05589419663278136]
	TIME [epoch: 8.23 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11480218000520806		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.07175506390238397		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.09327862195379602 | validation: 0.08079657019663633]
	TIME [epoch: 8.24 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08462353735368144		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.08125869880812467		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.08294111808090306 | validation: 0.0631149287032714]
	TIME [epoch: 8.26 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07006309001770071		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.06833612259899718		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.06919960630834894 | validation: 0.08977770201468582]
	TIME [epoch: 8.24 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08762143653405415		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.07366244973913134		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.08064194313659273 | validation: 0.044268795775471645]
	TIME [epoch: 8.24 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07987523601069883		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.12862777225373284		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.10425150413221584 | validation: 0.05262707806885886]
	TIME [epoch: 8.24 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08521876943042928		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.0825667347931495		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.0838927521117894 | validation: 0.03679686140741295]
	TIME [epoch: 8.26 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_553.pth
	Model improved!!!
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06368686057310924		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.0845066246834666		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.07409674262828793 | validation: 0.05858230843297675]
	TIME [epoch: 8.25 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07334318844464685		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.07426546533165583		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.07380432688815133 | validation: 0.08973025070460508]
	TIME [epoch: 8.23 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09037750702070535		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.10264713857816499		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.09651232279943517 | validation: 0.06432134658694902]
	TIME [epoch: 8.23 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0936958127823497		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.06943855391106675		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.08156718334670823 | validation: 0.044250617375082534]
	TIME [epoch: 8.27 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.067057417978903		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.09435505559765182		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.0807062367882774 | validation: 0.04414121235057869]
	TIME [epoch: 8.23 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09388375450459693		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.08635957498445926		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.09012166474452808 | validation: 0.07769869215904339]
	TIME [epoch: 8.25 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06090588544853561		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.08846459348453312		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.07468523946653437 | validation: 0.12586186202575717]
	TIME [epoch: 8.22 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10157482471494936		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.10570624621657694		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.10364053546576316 | validation: 0.08707772556327065]
	TIME [epoch: 8.28 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0909506528117717		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.11793490039136659		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.10444277660156913 | validation: 0.18676911923015874]
	TIME [epoch: 8.23 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08987998603368552		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.1223039983175038		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.10609199217559469 | validation: 0.07334358384887087]
	TIME [epoch: 8.24 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10607678661892359		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.07502770921401929		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.09055224791647143 | validation: 0.04714505924136305]
	TIME [epoch: 8.23 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06609866138705459		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.07961043594208324		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.07285454866456892 | validation: 0.12107613372010091]
	TIME [epoch: 8.25 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10258802190248055		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.1057795302235754		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.104183776063028 | validation: 0.09896778287697801]
	TIME [epoch: 8.26 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13816518418783078		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.1472671521544803		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.1427161681711555 | validation: 0.04026792248695346]
	TIME [epoch: 8.23 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06404936814756926		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.05250371272987301		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.058276540438721124 | validation: 0.10515937717862714]
	TIME [epoch: 8.24 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08055358185528096		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.08946501397966602		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.08500929791747348 | validation: 0.08596866577400818]
	TIME [epoch: 8.26 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08391784781693001		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.09885693954885413		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.09138739368289206 | validation: 0.06878518486028032]
	TIME [epoch: 8.24 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08673085124392302		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.10069868964010464		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.09371477044201384 | validation: 0.10872294834269317]
	TIME [epoch: 8.24 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07711915994322044		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.1002435061612629		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.08868133305224166 | validation: 0.0673273182834916]
	TIME [epoch: 8.24 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07886415458386407		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.11302909074979181		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.09594662266682795 | validation: 0.0632530628922148]
	TIME [epoch: 8.26 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10940039306364813		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.06375875264409739		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.08657957285387274 | validation: 0.04836609932546155]
	TIME [epoch: 8.24 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05865549065241203		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.10574839520271921		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.08220194292756564 | validation: 0.047806632070053964]
	TIME [epoch: 8.23 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0631972512694896		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.08617068401274187		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.07468396764111576 | validation: 0.17984768798549983]
	TIME [epoch: 8.21 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10068576056747869		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.07513344747272226		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.08790960402010048 | validation: 0.06941402107065728]
	TIME [epoch: 8.26 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12307799973927347		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.08657027012905268		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.10482413493416307 | validation: 0.041099026690658624]
	TIME [epoch: 8.24 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07899193868997124		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.11409200826758462		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.09654197347877792 | validation: 0.10139373485593726]
	TIME [epoch: 8.23 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07549377019799457		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.06761891957371756		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.07155634488585606 | validation: 0.07676417404990776]
	TIME [epoch: 8.23 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07332449097554813		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.09082155042861434		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.08207302070208124 | validation: 0.06807365036423878]
	TIME [epoch: 8.25 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06193700063435899		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.08468813162252944		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.07331256612844421 | validation: 0.07759416433884224]
	TIME [epoch: 8.23 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15464951419948433		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.12532317955010958		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.139986346874797 | validation: 0.06386890240999438]
	TIME [epoch: 8.24 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08174273820573762		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.08734241753864323		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.08454257787219042 | validation: 0.09548428654425417]
	TIME [epoch: 8.23 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09761771616480668		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.07282690157163937		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.085222308868223 | validation: 0.04351833463903516]
	TIME [epoch: 8.26 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0673821481261925		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.09647970581091471		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.08193092696855361 | validation: 0.05556990227434133]
	TIME [epoch: 8.23 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0692608885433383		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.057364287810267		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.06331258817680264 | validation: 0.032992400071342194]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_587.pth
	Model improved!!!
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05232112424651847		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.07377496263535917		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.06304804344093883 | validation: 0.08932467573081362]
	TIME [epoch: 8.24 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060923254648669566		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.11291131876984695		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.08691728670925825 | validation: 0.047097290525068454]
	TIME [epoch: 8.26 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10596490107155498		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.06623912271491346		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.08610201189323421 | validation: 0.06831900834539092]
	TIME [epoch: 8.24 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06731618534453704		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.06658571430241958		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.06695094982347831 | validation: 0.07508136654308134]
	TIME [epoch: 8.24 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06572080152633167		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.07886242373312087		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.07229161262972626 | validation: 0.039608552143261935]
	TIME [epoch: 8.22 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06358830684840135		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.09123102448071077		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.07740966566455605 | validation: 0.04428644579057057]
	TIME [epoch: 8.25 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0768453669715962		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.0761210580504229		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.07648321251100956 | validation: 0.04081702322483352]
	TIME [epoch: 8.23 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06893039044612406		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.05388298088854197		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.061406685667333025 | validation: 0.0585392170209935]
	TIME [epoch: 8.23 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055660439246962234		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.07017573943354034		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.0629180893402513 | validation: 0.14228286582156852]
	TIME [epoch: 8.24 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08538846792926695		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.08169494417568414		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.08354170605247555 | validation: 0.040347225641723015]
	TIME [epoch: 8.26 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08784526247541546		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.0977758203974132		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.09281054143641433 | validation: 0.0695300979296444]
	TIME [epoch: 8.23 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06333817584064091		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.06559850505939577		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.06446834045001834 | validation: 0.035890349954224174]
	TIME [epoch: 8.23 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07895480065500163		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.10733000475773022		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.09314240270636594 | validation: 0.0529924339368938]
	TIME [epoch: 8.23 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12042638260412972		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.05962662848408203		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.09002650554410588 | validation: 0.09080697201022583]
	TIME [epoch: 8.26 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06403470563966888		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.07846335203776776		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.07124902883871834 | validation: 0.09658139523484538]
	TIME [epoch: 8.24 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09674019586550284		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.07517894033719204		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.08595956810134743 | validation: 0.05477506939413134]
	TIME [epoch: 8.24 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07053596757148416		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.06468918189447773		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.06761257473298096 | validation: 0.09505967391543413]
	TIME [epoch: 8.22 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07854547059964537		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.047833010693007896		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.06318924064632661 | validation: 0.04168811500855041]
	TIME [epoch: 8.25 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07544767658878437		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.059059399844058204		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.0672535382164213 | validation: 0.03403921797319674]
	TIME [epoch: 8.23 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07261956706665637		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.057682878251778355		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.06515122265921738 | validation: 0.04294334548732405]
	TIME [epoch: 8.22 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04515401504222963		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.058502444464098656		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.051828229753164146 | validation: 0.06353448653004114]
	TIME [epoch: 8.24 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13869607804932782		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.10350347881377815		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.12109977843155299 | validation: 0.08976788391306555]
	TIME [epoch: 8.25 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08772401658357723		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.08953515589859284		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.08862958624108504 | validation: 0.1562150711944242]
	TIME [epoch: 8.24 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07593795592572394		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.08023014107913479		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.07808404850242937 | validation: 0.07516147579683871]
	TIME [epoch: 8.22 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06413716786040477		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.08952826646467966		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.07683271716254222 | validation: 0.06663840725036776]
	TIME [epoch: 8.23 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06977152182528759		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.07257768715653908		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.07117460449091331 | validation: 0.07306294764846837]
	TIME [epoch: 8.26 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06876466195202827		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.07573540959406946		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.07225003577304887 | validation: 0.13183764469548936]
	TIME [epoch: 8.23 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.091672373447958		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.06792384715258783		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.07979811030027292 | validation: 0.04719104396520501]
	TIME [epoch: 8.23 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057748963110775584		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.060207236122850685		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.05897809961681314 | validation: 0.07066879430858884]
	TIME [epoch: 8.23 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05988818425544161		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.0714456598055761		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.06566692203050886 | validation: 0.1400623728386855]
	TIME [epoch: 8.25 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0645366817084034		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.063939854017395		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.0642382678628992 | validation: 0.06272405204659998]
	TIME [epoch: 8.24 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06964443445808613		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.08673887765001945		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.07819165605405279 | validation: 0.09593657058238018]
	TIME [epoch: 8.22 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08683525632973194		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.06202507429985937		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.07443016531479565 | validation: 0.05521369009477001]
	TIME [epoch: 8.23 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07410588918938486		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.05376701272091382		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.06393645095514934 | validation: 0.030850576665515234]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0676456495967667		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.06315604919945873		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.06540084939811272 | validation: 0.10005916767329048]
	TIME [epoch: 8.25 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06930143302001038		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.04798956792905537		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.058645500474532875 | validation: 0.0903193662418444]
	TIME [epoch: 8.23 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11188445021451077		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.05405698925877139		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.08297071973664108 | validation: 0.06283176370190238]
	TIME [epoch: 8.22 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062227518988418475		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.05779635609456961		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.060011937541494045 | validation: 0.0507133068333382]
	TIME [epoch: 8.26 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07055701444254918		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.062264008817917826		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.0664105116302335 | validation: 0.02865149302660845]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_626.pth
	Model improved!!!
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049318066352842596		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.05828132298289216		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.05379969466786737 | validation: 0.056600228971878545]
	TIME [epoch: 8.23 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07625882590908627		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.06224159074768019		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.06925020832838322 | validation: 0.038241229912104754]
	TIME [epoch: 8.23 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09132216094792342		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.06578238962154881		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.07855227528473611 | validation: 0.04285599165459052]
	TIME [epoch: 8.26 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05240569391291346		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.05780697951499364		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.055106336713953555 | validation: 0.125283883288451]
	TIME [epoch: 8.23 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06363183720915959		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.08439828169117969		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.07401505945016963 | validation: 0.05666338879390559]
	TIME [epoch: 8.24 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05705589116532854		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.07616202687224757		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.06660895901878806 | validation: 0.0698782553079998]
	TIME [epoch: 8.23 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07423637984568379		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.07805078539999509		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.07614358262283943 | validation: 0.06841154127275885]
	TIME [epoch: 8.25 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07318039691928041		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.10280077539820835		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.08799058615874437 | validation: 0.04991923127755103]
	TIME [epoch: 8.23 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062476779414968674		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.058027299082313445		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.060252039248641066 | validation: 0.06609770062297278]
	TIME [epoch: 8.23 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06846648895100213		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.06806180364011764		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.06826414629555991 | validation: 0.028550133363170475]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08408456864415585		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.06790923119061196		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.07599689991738391 | validation: 0.08617663423409871]
	TIME [epoch: 8.27 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06849083307370815		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.05039981522939533		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.05944532415155174 | validation: 0.04403217019564064]
	TIME [epoch: 8.25 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07651131773548557		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.05480239166724723		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.0656568547013664 | validation: 0.06579802071117616]
	TIME [epoch: 8.23 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07127064149364062		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.07837289917647403		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.07482177033505732 | validation: 0.05981020507045444]
	TIME [epoch: 8.23 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06762369029665241		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.0852067104285647		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.07641520036260854 | validation: 0.0764153157738256]
	TIME [epoch: 8.26 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07373943017651019		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.06307901949842483		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.0684092248374675 | validation: 0.06755681547187115]
	TIME [epoch: 8.24 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06635492221248043		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.057833394026725916		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.06209415811960317 | validation: 0.08813721696511555]
	TIME [epoch: 8.23 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08144578956798827		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.05530872456572076		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.0683772570668545 | validation: 0.09293397310893509]
	TIME [epoch: 8.24 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0711801114874322		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.05732528394156942		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.0642526977145008 | validation: 0.035893538872598094]
	TIME [epoch: 8.26 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06731214333347635		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.07518870601564578		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.07125042467456107 | validation: 0.04886899527131899]
	TIME [epoch: 8.24 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0463427478030096		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.058969255051510606		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.0526560014272601 | validation: 0.03710111901406491]
	TIME [epoch: 8.23 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046719900946919066		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.06409364643358666		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.05540677369025286 | validation: 0.04281921393967848]
	TIME [epoch: 8.23 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08585606013637433		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.05787503494961303		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.07186554754299368 | validation: 0.11333392660134385]
	TIME [epoch: 8.26 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06290484697804033		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.09173400072722318		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.07731942385263176 | validation: 0.03669091338122554]
	TIME [epoch: 8.24 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05359238566484966		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.05664714133278981		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.05511976349881973 | validation: 0.0716529609059016]
	TIME [epoch: 8.24 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07497857787918878		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.0614590909490207		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.06821883441410474 | validation: 0.03933611696683539]
	TIME [epoch: 8.24 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07205930206352418		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.060421588983261575		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.06624044552339288 | validation: 0.03792896299741882]
	TIME [epoch: 8.26 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051045180840435835		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.05646688819683506		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.05375603451863544 | validation: 0.04193097123454417]
	TIME [epoch: 8.24 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05141079582128814		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.07455902373384402		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.06298490977756607 | validation: 0.03441802367635978]
	TIME [epoch: 8.23 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05086310479701319		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.08925223636653906		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.07005767058177612 | validation: 0.13848063053523899]
	TIME [epoch: 8.23 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06727082699748929		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.054821732300225565		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.061046279648857435 | validation: 0.043702416751785725]
	TIME [epoch: 8.25 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.057866176811713854		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.07099606316012111		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.06443111998591747 | validation: 0.05130679973363839]
	TIME [epoch: 8.23 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05356926978506591		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.07177150753082172		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.0626703886579438 | validation: 0.1410704099191103]
	TIME [epoch: 8.24 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07798446000029813		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.0543562219364598		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.06617034096837895 | validation: 0.043616677540980225]
	TIME [epoch: 8.23 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04392985636434512		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.047316975692556155		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.045623416028450645 | validation: 0.05401491982295631]
	TIME [epoch: 8.25 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06627984651983522		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.06103567071590484		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.06365775861787003 | validation: 0.04650967192780065]
	TIME [epoch: 8.23 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055230357907756045		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.07222416938060791		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.06372726364418198 | validation: 0.0893323347942353]
	TIME [epoch: 8.23 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05474826696762162		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.05572350778034013		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.05523588737398087 | validation: 0.04392814961759472]
	TIME [epoch: 8.23 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04633282488468211		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.05041625206843593		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.048374538476559016 | validation: 0.04787722005727492]
	TIME [epoch: 8.25 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.058197552395818565		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.07341934075084125		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.06580844657332989 | validation: 0.14272324778907128]
	TIME [epoch: 8.25 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09792902393956376		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.05940292144352925		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.07866597269154653 | validation: 0.05060129681721599]
	TIME [epoch: 8.23 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06449006342034286		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.07542578134999396		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.06995792238516842 | validation: 0.05876431560109338]
	TIME [epoch: 8.23 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05346923194677833		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.06259177425987898		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.058030503103328655 | validation: 0.05048683440327096]
	TIME [epoch: 8.24 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04214297484859382		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.06367624908944448		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.052909611969019145 | validation: 0.03380876323870975]
	TIME [epoch: 8.24 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04106289725327695		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.056583825507649885		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.04882336138046341 | validation: 0.05641044078234182]
	TIME [epoch: 8.24 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04264940918095122		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.07364991173253198		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.05814966045674158 | validation: 0.12320400018757302]
	TIME [epoch: 8.23 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07073238632057145		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.05138590678310563		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.06105914655183854 | validation: 0.03446499795209318]
	TIME [epoch: 8.24 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04057542608238		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.0436591546445256		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.042117290363452804 | validation: 0.04455284414112661]
	TIME [epoch: 8.25 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0695211991745551		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.06326118531419185		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.0663911922443735 | validation: 0.02711280649968382]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051654394955322626		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.0627311123000731		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.057192753627697865 | validation: 0.08889637609347072]
	TIME [epoch: 8.23 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06189799519733018		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.04710625075095246		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.05450212297414132 | validation: 0.05362786069571389]
	TIME [epoch: 8.24 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.048751494709056035		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.05837647751012698		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.053563986109591524 | validation: 0.04798185850249346]
	TIME [epoch: 8.23 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05545696097400016		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.06957579749971039		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.06251637923685527 | validation: 0.02635703373881626]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06181781587298		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.04251870522318168		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.052168260548080846 | validation: 0.020059133304677096]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04242570476286455		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.05155789945181003		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.04699180210733729 | validation: 0.02795337778187543]
	TIME [epoch: 8.27 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05210471599992918		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.05078052100509487		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.05144261850251203 | validation: 0.035081496688959615]
	TIME [epoch: 8.24 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.062255348233849296		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.05451530339774278		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.058385325815796044 | validation: 0.041474274509777406]
	TIME [epoch: 8.22 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05182619821572363		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.05644959128104902		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.05413789474838632 | validation: 0.03389740469963885]
	TIME [epoch: 8.22 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047247402542132035		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 0.05602478294236313		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.0516360927422476 | validation: 0.13401096280172642]
	TIME [epoch: 8.23 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07779240627483028		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.04875317593099048		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.06327279110291038 | validation: 0.0448793221731266]
	TIME [epoch: 8.24 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0629909719947246		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.046953511014742745		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.054972241504733656 | validation: 0.07112070973524938]
	TIME [epoch: 8.23 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06451203771629799		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.04229468714328314		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.05340336242979056 | validation: 0.04972103451955237]
	TIME [epoch: 8.22 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05182541463299382		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.06761586623017471		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.059720640431584274 | validation: 0.028837118122005158]
	TIME [epoch: 8.24 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055370486085442594		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.07378277818944892		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.06457663213744577 | validation: 0.04410659296390453]
	TIME [epoch: 8.24 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06704161784299478		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.05692535528364562		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.061983486563320186 | validation: 0.04213969970615091]
	TIME [epoch: 8.24 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07098155924936454		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.03740426403203807		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.054192911640701315 | validation: 0.04768359902287983]
	TIME [epoch: 8.23 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06661233859926138		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.08482198686155366		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.07571716273040753 | validation: 0.06181014679701025]
	TIME [epoch: 8.24 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04915132196606713		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.07487585751522113		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.06201358974064414 | validation: 0.10626732342332545]
	TIME [epoch: 8.23 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07450073158774805		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.05862929365026742		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.06656501261900774 | validation: 0.06321150893276326]
	TIME [epoch: 8.24 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06264200118649531		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.043144579616110616		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.05289329040130296 | validation: 0.05031831866883538]
	TIME [epoch: 8.23 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04304231323795325		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.05848749319599174		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.05076490321697249 | validation: 0.03962389994042592]
	TIME [epoch: 8.23 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056686933354678205		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.06368171702064529		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.06018432518766176 | validation: 0.02901851450699844]
	TIME [epoch: 8.25 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04471736473362732		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.05960304034657461		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.05216020254010097 | validation: 0.05486763858909982]
	TIME [epoch: 8.23 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049452743730461095		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.03704112748990069		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.0432469356101809 | validation: 0.058003055003525365]
	TIME [epoch: 8.22 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05542565512898987		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.05799745153062628		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.056711553329808075 | validation: 0.032407819772666956]
	TIME [epoch: 8.24 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05359694521904642		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.032343344568151675		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.042970144893599044 | validation: 0.035968622886284045]
	TIME [epoch: 8.26 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05002973119344025		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.046417253070250346		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.048223492131845305 | validation: 0.026134191563828155]
	TIME [epoch: 8.22 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05816971658352825		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.04985044883877719		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.05401008271115273 | validation: 0.05784508077342254]
	TIME [epoch: 8.22 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.061494705974299534		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.04939443038921692		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.055444568181758226 | validation: 0.03953808974864412]
	TIME [epoch: 8.23 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05972707574802555		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.04555925741374521		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.052643166580885394 | validation: 0.048269468389907956]
	TIME [epoch: 8.25 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05398458346505545		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.04436500861015432		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.04917479603760488 | validation: 0.04845637687413723]
	TIME [epoch: 8.24 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05167318881382891		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.061848765344875024		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.056760977079351974 | validation: 0.021633654493422576]
	TIME [epoch: 8.23 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04492724138012518		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.07281064878591914		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.05886894508302217 | validation: 0.06441635492822582]
	TIME [epoch: 8.24 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03926668887732535		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.04687374186802223		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.0430702153726738 | validation: 0.06543318168780071]
	TIME [epoch: 8.25 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06027658432516299		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 0.06036126310587623		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.0603189237155196 | validation: 0.03782095601689346]
	TIME [epoch: 8.23 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051399591498234275		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.03712719847188532		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.044263394985059795 | validation: 0.04435876055005067]
	TIME [epoch: 8.23 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06365945631094888		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.05756572755906777		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.06061259193500833 | validation: 0.037877017934859314]
	TIME [epoch: 8.23 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051095052806821514		[learning rate: 0.001075]
		[batch 20/20] avg loss: 0.03840552751389657		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 0.04475029016035905 | validation: 0.07617829214454874]
	TIME [epoch: 8.26 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.059136472499494275		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 0.05140142480072435		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 0.055268948650109305 | validation: 0.03617003414360172]
	TIME [epoch: 8.24 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04753414263740764		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 0.045848881210242706		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 0.046691511923825174 | validation: 0.050711542374405276]
	TIME [epoch: 8.23 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0625935617452164		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 0.04202428030471354		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 0.05230892102496496 | validation: 0.01701048967754657]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046081188154186305		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 0.06465774178967514		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 0.055369464971930724 | validation: 0.03879062946990671]
	TIME [epoch: 8.26 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050947365479352066		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 0.05195376948148365		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 0.05145056748041786 | validation: 0.06958144633791291]
	TIME [epoch: 8.23 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06331987109719373		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 0.04261490793199625		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 0.052967389514594995 | validation: 0.023634486681639113]
	TIME [epoch: 8.23 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039050220051788063		[learning rate: 0.001048]
		[batch 20/20] avg loss: 0.046342005035452674		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 0.042696112543620365 | validation: 0.0398696186679713]
	TIME [epoch: 8.22 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09386844779466727		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 0.061236587383146204		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 0.07755251758890673 | validation: 0.054397477642314826]
	TIME [epoch: 8.26 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04568857472717953		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 0.059997291614920234		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 0.05284293317104988 | validation: 0.03335201396634035]
	TIME [epoch: 8.22 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05313489309921484		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 0.07777128573057576		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 0.0654530894148953 | validation: 0.03192601197360823]
	TIME [epoch: 8.23 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04064967895252042		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 0.05129502076860091		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 0.04597234986056067 | validation: 0.04961582896931128]
	TIME [epoch: 8.23 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04935769470136172		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 0.05753870650370263		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 0.05344820060253217 | validation: 0.0670563479632546]
	TIME [epoch: 8.26 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047527728190484425		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 0.04847759556524658		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 0.04800266187786549 | validation: 0.08504216166690676]
	TIME [epoch: 8.22 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.047079383292679786		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 0.05232070761260752		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 0.04970004545264365 | validation: 0.033361822877557824]
	TIME [epoch: 8.22 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0463091606329256		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 0.05495761539304378		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 0.050633388012984684 | validation: 0.025826440767607154]
	TIME [epoch: 8.24 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060191841777301344		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 0.05880733957195454		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 0.05949959067462794 | validation: 0.012925892373694577]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_730.pth
	Model improved!!!
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04030596664289998		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 0.045408352484138506		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 0.042857159563519244 | validation: 0.05928850122226506]
	TIME [epoch: 8.22 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06190903208416311		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 0.051683833768404366		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 0.05679643292628374 | validation: 0.04647702281796157]
	TIME [epoch: 8.23 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06250456466401182		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 0.05668389526808766		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 0.05959422996604975 | validation: 0.06987472183945193]
	TIME [epoch: 8.23 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.052751275750505235		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 0.0403545725179296		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 0.04655292413421742 | validation: 0.02862131079597839]
	TIME [epoch: 8.24 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04126527725383577		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 0.04969655821818962		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 0.04548091773601269 | validation: 0.034151248776385916]
	TIME [epoch: 8.21 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046005528716119834		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 0.06015778145981047		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 0.05308165508796516 | validation: 0.08299942184805927]
	TIME [epoch: 8.23 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07613456532114672		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 0.03485724785021971		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 0.05549590658568322 | validation: 0.07367725749056357]
	TIME [epoch: 8.23 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05508396953122642		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 0.044811106812769286		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 0.04994753817199785 | validation: 0.03358509252538258]
	TIME [epoch: 8.25 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04209741117737321		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 0.052264388362870985		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 0.04718089977012211 | validation: 0.024598399647914777]
	TIME [epoch: 8.23 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04687260769804209		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 0.05238061919951483		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 0.049626613448778466 | validation: 0.027777439240382697]
	TIME [epoch: 8.21 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04454472902020741		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 0.04069448308264782		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 0.042619606051427616 | validation: 0.038083012253318685]
	TIME [epoch: 8.23 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05046963498152516		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 0.031019091338820047		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 0.0407443631601726 | validation: 0.025018806778643578]
	TIME [epoch: 8.26 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046396443582216104		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 0.044323761032065444		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 0.04536010230714077 | validation: 0.06770645559360768]
	TIME [epoch: 8.22 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03631518285414206		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 0.040103814572045315		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 0.03820949871309368 | validation: 0.030026543271070426]
	TIME [epoch: 8.24 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03663987349003591		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 0.04664929451267445		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 0.04164458400135518 | validation: 0.03869536505422254]
	TIME [epoch: 8.23 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035077031725845594		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 0.06220401776061714		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 0.04864052474323136 | validation: 0.030673618631181815]
	TIME [epoch: 8.25 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04041624932174083		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 0.06334799577986495		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 0.0518821225508029 | validation: 0.056915641349565865]
	TIME [epoch: 8.22 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04559116224326004		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 0.04115960766599697		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 0.043375384954628504 | validation: 0.02702021892620515]
	TIME [epoch: 8.23 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04450386346612015		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 0.04704800963384786		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 0.045775936549984006 | validation: 0.061023985628631525]
	TIME [epoch: 8.23 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.043454986677451166		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 0.03903544124909441		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 0.04124521396327279 | validation: 0.11154618190835078]
	TIME [epoch: 8.26 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06478620783501463		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 0.04738586560768585		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 0.05608603672135023 | validation: 0.03686976563911689]
	TIME [epoch: 8.22 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04909151423985558		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 0.0439835872728142		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 0.0465375507563349 | validation: 0.01821895007014258]
	TIME [epoch: 8.23 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041284992128557754		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 0.06926177154510496		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 0.05527338183683136 | validation: 0.03250639327563341]
	TIME [epoch: 8.23 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04710153817463886		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 0.03520087650074393		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 0.0411512073376914 | validation: 0.019406300587118508]
	TIME [epoch: 8.24 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04928212261306793		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 0.0405011917369044		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 0.044891657174986166 | validation: 0.025775582276377832]
	TIME [epoch: 8.24 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04311418511870363		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 0.061294758376733136		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 0.05220447174771838 | validation: 0.03334518324198471]
	TIME [epoch: 8.24 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036108912586064815		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 0.06049543575376235		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 0.048302174169913586 | validation: 0.04588188671809732]
	TIME [epoch: 8.23 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04997075567579992		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 0.0554897189067999		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 0.05273023729129991 | validation: 0.06401666445983198]
	TIME [epoch: 8.26 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05199662949792009		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 0.045793909458128924		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 0.048895269478024514 | validation: 0.04253274286814461]
	TIME [epoch: 8.23 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037806175547822764		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 0.033354733089576157		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 0.03558045431869946 | validation: 0.04175395698571232]
	TIME [epoch: 8.22 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031638551083756		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 0.031002114908140005		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 0.03132033299594801 | validation: 0.03388713178051133]
	TIME [epoch: 8.23 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03607706009990466		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 0.04374887620847354		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 0.0399129681541891 | validation: 0.018885571792435683]
	TIME [epoch: 8.26 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033295706524073675		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 0.03639935399727101		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 0.034847530260672346 | validation: 0.045505736859807175]
	TIME [epoch: 8.23 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04233303026595847		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 0.04453415555338606		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 0.04343359290967227 | validation: 0.038855681260920455]
	TIME [epoch: 8.22 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034422835394725675		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 0.038143266297646986		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 0.036283050846186334 | validation: 0.0336366937364086]
	TIME [epoch: 8.22 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04431368965798543		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 0.039546319796601864		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 0.04193000472729364 | validation: 0.03868422191492442]
	TIME [epoch: 8.26 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04449102876792037		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 0.051454417419275825		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 0.047972723093598096 | validation: 0.05280505050348623]
	TIME [epoch: 8.23 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.049716590933976		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 0.04584572869899766		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 0.04778115981648684 | validation: 0.026675241398090924]
	TIME [epoch: 8.23 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033778570916612995		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 0.05085427910487254		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 0.04231642501074277 | validation: 0.07222074516649497]
	TIME [epoch: 8.22 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.046604990385016906		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 0.04443215153231905		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 0.04551857095866798 | validation: 0.024962589431986745]
	TIME [epoch: 8.24 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05242772446142768		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 0.037869250631129024		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 0.04514848754627836 | validation: 0.04756733540559492]
	TIME [epoch: 8.23 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05921681214064424		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 0.04799098656742874		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 0.05360389935403648 | validation: 0.03555699256962064]
	TIME [epoch: 8.23 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04562523423309775		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 0.038440205058769654		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 0.0420327196459337 | validation: 0.032284109989727826]
	TIME [epoch: 8.21 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034871865307762304		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 0.03451107397087081		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 0.03469146963931656 | validation: 0.021891598719990024]
	TIME [epoch: 8.25 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033770335649586654		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 0.034103863837898964		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 0.0339370997437428 | validation: 0.01674903312937746]
	TIME [epoch: 8.23 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03686695890056647		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 0.06014473083947881		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 0.048505844870022644 | validation: 0.022760366125720213]
	TIME [epoch: 8.22 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04255233636998131		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 0.039805728698768514		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 0.04117903253437492 | validation: 0.04482106125999002]
	TIME [epoch: 8.23 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05612658011492282		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 0.037216506083739366		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 0.046671543099331095 | validation: 0.03792492568235601]
	TIME [epoch: 8.24 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04293146231579288		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 0.033556813600446725		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 0.0382441379581198 | validation: 0.03632208125759152]
	TIME [epoch: 8.23 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.039639690047282225		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 0.041417918772025775		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 0.040528804409654 | validation: 0.030915372001364138]
	TIME [epoch: 8.23 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030791870856472513		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 0.05682182790661622		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 0.04380684938154437 | validation: 0.034769831729666585]
	TIME [epoch: 8.24 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.056748403442246374		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 0.02811138970582917		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 0.04242989657403777 | validation: 0.05010673219663688]
	TIME [epoch: 8.25 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04624280509801628		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 0.040227187667045215		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 0.04323499638253075 | validation: 0.0519461355678806]
	TIME [epoch: 8.22 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.053425073495895536		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 0.03867482913868291		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 0.04604995131728923 | validation: 0.03098439670611158]
	TIME [epoch: 8.22 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04010059295911359		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 0.04277370043375515		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 0.04143714669643438 | validation: 0.03581349520838706]
	TIME [epoch: 8.22 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.055021969850977494		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 0.04400940979562918		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 0.049515689823303344 | validation: 0.018432577236533697]
	TIME [epoch: 8.24 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04295126103789425		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 0.040963673012853134		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 0.04195746702537369 | validation: 0.024296885142365717]
	TIME [epoch: 8.23 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03959133496423352		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 0.0314109939411093		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 0.0355011644526714 | validation: 0.021333863650301578]
	TIME [epoch: 8.21 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027333089245490088		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 0.06499007431483932		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 0.046161581780164704 | validation: 0.031802469047401465]
	TIME [epoch: 8.21 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.045699301886172786		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 0.04083250028977241		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 0.04326590108797259 | validation: 0.043065373996989315]
	TIME [epoch: 8.24 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04013461717588805		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 0.056809781950190055		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 0.04847219956303904 | validation: 0.05589577892366853]
	TIME [epoch: 8.23 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.051952511245575574		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 0.04666188496538922		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 0.04930719810548238 | validation: 0.039955724270747034]
	TIME [epoch: 8.22 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026110585064813063		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 0.039527510657640784		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 0.03281904786122693 | validation: 0.03631202195063921]
	TIME [epoch: 8.23 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06490065629120251		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 0.04323400715870391		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 0.054067331724953205 | validation: 0.0402271452891968]
	TIME [epoch: 8.24 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03352554285291986		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 0.026413571111529588		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 0.02996955698222472 | validation: 0.021890140629161815]
	TIME [epoch: 8.23 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02998200301390768		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 0.03242575891226358		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 0.03120388096308564 | validation: 0.02787115051481379]
	TIME [epoch: 8.22 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02975928906613063		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 0.03838272939982788		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 0.03407100923297926 | validation: 0.02898994661424681]
	TIME [epoch: 8.22 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03791451448667783		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 0.059657153005759425		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 0.04878583374621863 | validation: 0.022861763666885224]
	TIME [epoch: 8.24 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040552297027090005		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 0.04902408988770101		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 0.044788193457395514 | validation: 0.02865944428544845]
	TIME [epoch: 8.24 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.050675473169574346		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 0.025528447938394178		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 0.038101960553984265 | validation: 0.015473548078561904]
	TIME [epoch: 8.22 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04665592406727066		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 0.03655962873632104		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 0.041607776401795846 | validation: 0.05817019966340413]
	TIME [epoch: 8.21 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03648678872997868		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 0.06250075987639436		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 0.04949377430318652 | validation: 0.024537205411396965]
	TIME [epoch: 8.24 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.06263277981805863		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 0.05105487946081002		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 0.05684382963943433 | validation: 0.02627448496392395]
	TIME [epoch: 8.23 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034138330859794355		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 0.04085598908041576		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 0.03749715997010506 | validation: 0.051461205535014064]
	TIME [epoch: 8.23 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03427188954629207		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 0.01952242717614722		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 0.026897158361219642 | validation: 0.018253120854254585]
	TIME [epoch: 8.21 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031017604254359298		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 0.04370866783865612		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 0.03736313604650772 | validation: 0.02921397300501998]
	TIME [epoch: 8.24 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029318482377233425		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.04002670295958973		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 0.03467259266841158 | validation: 0.03691112793394417]
	TIME [epoch: 8.23 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029910009812287374		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 0.036652962446997894		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 0.033281486129642636 | validation: 0.027574822534753202]
	TIME [epoch: 8.23 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0346061680494865		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.029705879709099447		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.03215602387929298 | validation: 0.026765445243730836]
	TIME [epoch: 8.22 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.036682629157655834		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.03470637539032299		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.035694502273989404 | validation: 0.03227442755516104]
	TIME [epoch: 8.26 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05578830569020523		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 0.05391184191971839		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.0548500738049618 | validation: 0.050858681494164545]
	TIME [epoch: 8.22 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03932946463671598		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 0.04349712835404612		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 0.041413296495381044 | validation: 0.0362058430205294]
	TIME [epoch: 8.21 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0323448060945406		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 0.03200091087298721		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 0.0321728584837639 | validation: 0.029546431738444002]
	TIME [epoch: 8.21 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037806868817231734		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 0.036269552729218696		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 0.03703821077322521 | validation: 0.030550534834329846]
	TIME [epoch: 8.24 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03477628490425676		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 0.04133918228966145		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 0.038057733596959104 | validation: 0.029038925854434135]
	TIME [epoch: 8.23 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030429855380943054		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 0.030818051203208536		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 0.03062395329207579 | validation: 0.03399399364141592]
	TIME [epoch: 8.22 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03628685761083459		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 0.037396895356697116		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 0.03684187648376585 | validation: 0.025385952030503672]
	TIME [epoch: 8.22 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04423354308988286		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 0.04980444471856441		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 0.047018993904223635 | validation: 0.04590394382072353]
	TIME [epoch: 8.24 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040203946381919306		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 0.05439109799847695		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 0.04729752219019813 | validation: 0.031785325451044955]
	TIME [epoch: 8.23 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04627487211734913		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 0.039583723827384044		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 0.042929297972366574 | validation: 0.039588376457299845]
	TIME [epoch: 8.22 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04805273000427725		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 0.030539276857347843		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 0.03929600343081254 | validation: 0.04312252430125523]
	TIME [epoch: 8.23 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03381593310070216		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 0.028219388418965603		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 0.031017660759833875 | validation: 0.03578403315748726]
	TIME [epoch: 8.24 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03649591202218434		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 0.025204579927177468		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 0.03085024597468091 | validation: 0.02121215585298877]
	TIME [epoch: 8.24 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034272807526710844		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 0.04389334020230948		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 0.03908307386451017 | validation: 0.04271758258724429]
	TIME [epoch: 8.22 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0274500164124044		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 0.03245430286371424		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 0.029952159638059323 | validation: 0.038217002546605455]
	TIME [epoch: 8.24 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04939313202228441		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 0.025466251474108297		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 0.037429691748196346 | validation: 0.028790062705872525]
	TIME [epoch: 8.26 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03249178813398623		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 0.028033409130590325		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 0.030262598632288286 | validation: 0.024206456238371723]
	TIME [epoch: 8.24 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032816050767719776		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 0.047620476739846276		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 0.040218263753783026 | validation: 0.03066396384717635]
	TIME [epoch: 8.23 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03210213688657694		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 0.0349920875280195		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 0.033547112207298224 | validation: 0.019108391780597093]
	TIME [epoch: 8.23 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03130757309597604		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 0.03919115040957773		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 0.03524936175277689 | validation: 0.06159544740757445]
	TIME [epoch: 8.22 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05066654151525315		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 0.04117252853551155		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 0.04591953502538235 | validation: 0.02407383103888847]
	TIME [epoch: 8.25 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032660061986999576		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.025582459001013958		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.029121260494006772 | validation: 0.046343328267833925]
	TIME [epoch: 8.21 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05737015891067584		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.03235464394149602		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.04486240142608593 | validation: 0.016425211105009704]
	TIME [epoch: 8.23 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02924093725149468		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.034331070291815834		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.03178600377165526 | validation: 0.035010208950185806]
	TIME [epoch: 8.21 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02923663651875378		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.03250383722060155		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.030870236869677676 | validation: 0.03708204375644301]
	TIME [epoch: 8.24 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030531063998378172		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.032393532274925904		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.03146229813665204 | validation: 0.042087237637252896]
	TIME [epoch: 8.22 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0372401353471687		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.02649402372433436		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.031867079535751526 | validation: 0.029396452517824335]
	TIME [epoch: 8.22 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025320770439167816		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.03490042344543015		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.030110596942298985 | validation: 0.04784642963058112]
	TIME [epoch: 8.25 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04538257529518767		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.03206634944068485		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.038724462367936245 | validation: 0.025033345767133362]
	TIME [epoch: 8.25 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0353780088242756		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.059064750275811886		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.047221379550043754 | validation: 0.040757947600097384]
	TIME [epoch: 8.22 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026600587701004562		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.03482283198318062		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.030711709842092594 | validation: 0.022914494985277076]
	TIME [epoch: 8.21 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04302430160301283		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.03011170078983441		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.03656800119642361 | validation: 0.03217846133091691]
	TIME [epoch: 8.23 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040238491338369424		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.02838374212948362		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.034311116733926526 | validation: 0.030064394387249078]
	TIME [epoch: 8.25 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03611782239904827		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.030285250692642717		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.0332015365458455 | validation: 0.02557846617799471]
	TIME [epoch: 8.23 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025584673829564718		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.03188765132793557		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.028736162578750148 | validation: 0.019958371236263747]
	TIME [epoch: 8.22 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04277006240735599		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.03660270028634845		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.03968638134685222 | validation: 0.029746522156622088]
	TIME [epoch: 8.23 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03952010050344752		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.033501786060192176		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.03651094328181985 | validation: 0.012917100949129711]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022437387497761425		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.043312492581354424		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.03287494003955792 | validation: 0.029805980521190482]
	TIME [epoch: 8.23 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041596978045343104		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.058671776730951954		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.050134377388147536 | validation: 0.02112637900319485]
	TIME [epoch: 8.23 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024847236624606212		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.04894181464129309		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.03689452563294964 | validation: 0.03483242965921565]
	TIME [epoch: 8.23 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026528905844119755		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.03676058210090169		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.03164474397251073 | validation: 0.03456911840535861]
	TIME [epoch: 8.24 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0234044777702239		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.03488967106117179		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.029147074415697843 | validation: 0.016138665182312065]
	TIME [epoch: 8.22 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035394789698958304		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.03195268193845738		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.033673735818707834 | validation: 0.05145820760019738]
	TIME [epoch: 8.22 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044786397717738666		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.038790061619157705		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.04178822966844818 | validation: 0.024041997502852092]
	TIME [epoch: 8.23 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027201307327155595		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.03540011179258336		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.03130070955986948 | validation: 0.020591465480155644]
	TIME [epoch: 8.25 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038324125010934865		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.03150748793274854		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.034915806471841705 | validation: 0.04753287190455578]
	TIME [epoch: 8.23 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03130856467453371		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.02973357127300682		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.03052106797377027 | validation: 0.018925032919563135]
	TIME [epoch: 8.22 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038323039152445795		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.04583245349617822		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.04207774632431201 | validation: 0.032054451027959896]
	TIME [epoch: 8.23 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.060226139148789014		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.04350829091661759		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.05186721503270331 | validation: 0.037832911323799234]
	TIME [epoch: 8.25 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04909200253782761		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.040110280633472585		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.04460114158565011 | validation: 0.029125520380197206]
	TIME [epoch: 8.23 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04071466792107377		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.03457677323893807		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.03764572058000591 | validation: 0.05140276734838182]
	TIME [epoch: 8.21 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03450384684030279		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.028905740537594003		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.03170479368894839 | validation: 0.016492795146498682]
	TIME [epoch: 8.24 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0350991762070186		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.03421551620754373		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.034657346207281156 | validation: 0.015550742081661077]
	TIME [epoch: 8.25 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03974679006808452		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.03062681361475883		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.03518680184142167 | validation: 0.016410565574721668]
	TIME [epoch: 8.23 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03690119091609287		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.040516631621381444		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.03870891126873716 | validation: 0.057269263638246214]
	TIME [epoch: 8.22 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04155994695133025		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.03229069157921762		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.03692531926527393 | validation: 0.03608899514825994]
	TIME [epoch: 8.23 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02722315874402496		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.02089692283784766		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.02406004079093631 | validation: 0.021927628932880954]
	TIME [epoch: 8.26 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033947277771494686		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.031090005663448807		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.03251864171747175 | validation: 0.028333364251596407]
	TIME [epoch: 8.24 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026439371849222005		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.04474124814010741		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.0355903099946647 | validation: 0.023943220080284603]
	TIME [epoch: 8.22 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030061529955923944		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.030970758221359702		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.030516144088641818 | validation: 0.05554782252105354]
	TIME [epoch: 8.23 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03693581557341689		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.033931680186092		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.03543374787975444 | validation: 0.025531044996745654]
	TIME [epoch: 8.25 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026686331100375597		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.03398586777645633		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.030336099438415965 | validation: 0.020315678267101102]
	TIME [epoch: 8.23 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03088441880511152		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.036687405364616364		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.033785912084863944 | validation: 0.030515654350623256]
	TIME [epoch: 8.23 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0410971796721642		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.02667564267176069		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.033886411171962436 | validation: 0.022473628644495564]
	TIME [epoch: 8.22 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02836940907409411		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.027467002221273108		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.027918205647683603 | validation: 0.025120073759647775]
	TIME [epoch: 8.25 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029027842591344416		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.0509070564924485		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.03996744954189645 | validation: 0.04489550275407761]
	TIME [epoch: 8.23 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04188830258112209		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.021917235935973427		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.03190276925854776 | validation: 0.019329251018696158]
	TIME [epoch: 8.23 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02763685164516732		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.037839590415625214		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.03273822103039627 | validation: 0.053785667753825595]
	TIME [epoch: 8.23 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040770983457489675		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.034791157909635204		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.03778107068356244 | validation: 0.014359950267848115]
	TIME [epoch: 8.24 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028296678643173333		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.035325633621626426		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.03181115613239988 | validation: 0.03861529228570771]
	TIME [epoch: 8.24 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0406338150702996		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.040548833393923456		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.04059132423211152 | validation: 0.026289165822797497]
	TIME [epoch: 8.23 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028820350394458943		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.03696411813353309		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.032892234263996026 | validation: 0.017135492166519047]
	TIME [epoch: 8.23 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02968832951991024		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.03148694554416829		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.030587637532039268 | validation: 0.036274791882129474]
	TIME [epoch: 8.24 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0243917210027104		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.03491358530679273		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.029652653154751567 | validation: 0.02048270650785169]
	TIME [epoch: 8.23 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04027890082976473		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.041631969997940214		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.04095543541385248 | validation: 0.03730455283426529]
	TIME [epoch: 8.22 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03947749342579548		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.05787973527515712		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.048678614350476296 | validation: 0.054792931469928]
	TIME [epoch: 8.22 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03773049841885097		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.03130468444152506		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.03451759143018802 | validation: 0.03293521575158882]
	TIME [epoch: 8.25 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03586665877368371		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.0303727562992106		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.03311970753644715 | validation: 0.023911709445583174]
	TIME [epoch: 8.23 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038638761685441655		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.0315798689148533		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.035109315300147485 | validation: 0.0375060288350495]
	TIME [epoch: 8.22 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035442189485830985		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.027995357171531815		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.0317187733286814 | validation: 0.04503019459745327]
	TIME [epoch: 8.22 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03387807765375342		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.030781125675977704		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.03232960166486555 | validation: 0.023650423766957262]
	TIME [epoch: 8.25 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030092677553849097		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.031122208553343817		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.030607443053596455 | validation: 0.02415252382630226]
	TIME [epoch: 8.24 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02935130381672264		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.036920090652426144		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.03313569723457439 | validation: 0.03667006764577872]
	TIME [epoch: 8.23 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03084696459451921		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.02911210510829428		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.029979534851406746 | validation: 0.04032229025801515]
	TIME [epoch: 8.22 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03482405767222625		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.03162665591404144		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.03322535679313384 | validation: 0.019473645516135014]
	TIME [epoch: 8.25 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02829855357085162		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.0398366151440768		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.03406758435746421 | validation: 0.032821582466185176]
	TIME [epoch: 8.25 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034347823142118386		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.02271715643875618		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.028532489790437283 | validation: 0.027088958343234406]
	TIME [epoch: 8.23 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032705240406399425		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.02298743720121084		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.02784633880380514 | validation: 0.02980931861421373]
	TIME [epoch: 8.23 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03832500460605036		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.027851573408841958		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.03308828900744616 | validation: 0.026993313663090122]
	TIME [epoch: 8.25 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029185669472951987		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.03453012404064441		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.0318578967567982 | validation: 0.01635589549245054]
	TIME [epoch: 8.25 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03293947700020507		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.030624194115357068		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.031781835557781066 | validation: 0.030213817149156524]
	TIME [epoch: 8.23 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034845152721130944		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.03501912339305704		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.03493213805709399 | validation: 0.02236301503022659]
	TIME [epoch: 8.22 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027151209583189624		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.027651367451056376		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.027401288517123 | validation: 0.024426851169640034]
	TIME [epoch: 8.26 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034162382834211955		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.05753375854030568		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.04584807068725881 | validation: 0.011553492623230693]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_904.pth
	Model improved!!!
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03235709966908519		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.0437096323020051		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.038033365985545145 | validation: 0.023545680516671868]
	TIME [epoch: 8.25 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.041967188801119014		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.03428174742789607		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.038124468114507544 | validation: 0.030274980029137994]
	TIME [epoch: 8.22 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04548937497735162		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.03525620476491566		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.04037278987113364 | validation: 0.023002368742484514]
	TIME [epoch: 8.24 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03179843570351599		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 0.030055419831105427		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 0.030926927767310715 | validation: 0.020133527609798904]
	TIME [epoch: 8.23 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03713477049885328		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 0.02609013501470752		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 0.031612452756780395 | validation: 0.01722383578838001]
	TIME [epoch: 8.22 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02286868393406223		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 0.03271301404178346		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 0.027790848987922844 | validation: 0.0276348216311441]
	TIME [epoch: 8.22 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04008509643022363		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 0.029377480945315226		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 0.03473128868776943 | validation: 0.02615310524880192]
	TIME [epoch: 8.26 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023097027703271034		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 0.031340244662821676		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 0.027218636183046353 | validation: 0.017324443029093434]
	TIME [epoch: 8.22 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020809168278519436		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 0.026840233851038647		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 0.023824701064779043 | validation: 0.0179699036195909]
	TIME [epoch: 8.22 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030139019218997115		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 0.04358501976143523		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 0.03686201949021617 | validation: 0.01739376851658819]
	TIME [epoch: 8.22 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0309584328419933		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 0.024682445331301248		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 0.027820439086647275 | validation: 0.014153306302225985]
	TIME [epoch: 8.26 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023160308597203756		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 0.02793176326121639		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 0.02554603592921007 | validation: 0.032420601309021795]
	TIME [epoch: 8.24 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03812223008147279		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 0.033762407562108944		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 0.035942318821790865 | validation: 0.025119560512343347]
	TIME [epoch: 8.23 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02660694539901764		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.03188065652141529		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.02924380096021646 | validation: 0.020832056165561722]
	TIME [epoch: 8.24 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03055112208650676		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.02637378139861631		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.028462451742561533 | validation: 0.022373697996898256]
	TIME [epoch: 8.27 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024847989521281536		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.03870725856742723		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.03177762404435438 | validation: 0.022122320237062825]
	TIME [epoch: 8.24 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034890405559686186		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.037396988119211674		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.03614369683944892 | validation: 0.0423102241045477]
	TIME [epoch: 8.22 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04114389625436905		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.027502395042165083		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.034323145648267064 | validation: 0.027612006729329428]
	TIME [epoch: 8.23 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044737366678556226		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.02569145475363884		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.03521441071609752 | validation: 0.005877451787550636]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026152239777691304		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.033750258815924285		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.029951249296807796 | validation: 0.02079282921502352]
	TIME [epoch: 8.26 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03659469831382781		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.02529417650256289		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.030944437408195347 | validation: 0.03946716069950285]
	TIME [epoch: 8.24 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03571158376197276		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.03477084776790949		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.03524121576494112 | validation: 0.03436210630190961]
	TIME [epoch: 8.24 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.040369740329810076		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.05692656296348111		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.04864815164664559 | validation: 0.04882193842216528]
	TIME [epoch: 8.28 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.05076984243041614		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.03689207143044941		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.04383095693043278 | validation: 0.02752766654328236]
	TIME [epoch: 8.24 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029358308643210762		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.03056875846189138		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.029963533552551075 | validation: 0.03348374761328332]
	TIME [epoch: 8.24 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04908627376055604		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.03955206598180105		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.04431916987117855 | validation: 0.03295762862047658]
	TIME [epoch: 8.25 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032702898256533365		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.029493256545304915		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.03109807740091914 | validation: 0.024777836544409078]
	TIME [epoch: 8.27 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02882246755061369		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.02877255825647762		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.028797512903545652 | validation: 0.06329005266902221]
	TIME [epoch: 8.26 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03655705353019206		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.03036319326491932		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.03346012339755569 | validation: 0.020290154676013056]
	TIME [epoch: 8.25 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0364093509047686		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.043783036398391095		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.040096193651579855 | validation: 0.03222402896048589]
	TIME [epoch: 8.25 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.042223572042936475		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.030082687571185117		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.03615312980706079 | validation: 0.02652413683182878]
	TIME [epoch: 8.27 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03714554689468891		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.031237653323731455		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.03419160010921017 | validation: 0.020617387362394796]
	TIME [epoch: 8.26 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03569775679935929		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.030179786626101872		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.032938771712730575 | validation: 0.02457589809969723]
	TIME [epoch: 8.24 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02531987333659197		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.031053728867198538		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.028186801101895254 | validation: 0.01487594232651118]
	TIME [epoch: 8.24 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025637702996271494		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.03849702509105661		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.03206736404366405 | validation: 0.027327090295087172]
	TIME [epoch: 8.27 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03480573605300883		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.025553036905369742		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.030179386479189284 | validation: 0.021721621405310882]
	TIME [epoch: 8.24 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02362536124327539		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.027047847919385186		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.02533660458133029 | validation: 0.029093738820701688]
	TIME [epoch: 8.24 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031239321692421374		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.026505800576582562		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.028872561134501963 | validation: 0.02606405386988796]
	TIME [epoch: 8.24 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03562889597013125		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.028954020197356843		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.03229145808374405 | validation: 0.023332114395660075]
	TIME [epoch: 8.27 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026376007539157604		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.036143959929206666		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.03125998373418214 | validation: 0.016885673931721176]
	TIME [epoch: 8.25 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03244378150778742		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.02843599344114267		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.03043988747446505 | validation: 0.02431384261074912]
	TIME [epoch: 8.25 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0299417891956525		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.025349900182093116		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.027645844688872812 | validation: 0.017456640940717652]
	TIME [epoch: 8.25 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03612667119982978		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.036401984217561134		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.03626432770869545 | validation: 0.03996706746408314]
	TIME [epoch: 8.27 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03895797239676917		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.043363745567232166		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.04116085898200066 | validation: 0.03642719019436018]
	TIME [epoch: 8.25 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031688375049324105		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.03723956287292562		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.034463968961124865 | validation: 0.041312004768233254]
	TIME [epoch: 8.25 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.034144472525916715		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.04193862323075071		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.038041547878333706 | validation: 0.015053383560375336]
	TIME [epoch: 8.25 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022820213856692136		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.023509805870685		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.023165009863688572 | validation: 0.03295168097203641]
	TIME [epoch: 8.27 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03271248812223114		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.0262268599614368		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.029469674041833965 | validation: 0.03188893986566707]
	TIME [epoch: 8.26 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02602507848070163		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.02977754271553318		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.0279013105981174 | validation: 0.022756590248715236]
	TIME [epoch: 8.26 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022226909836861687		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.02899151760274124		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.025609213719801467 | validation: 0.026782517995080746]
	TIME [epoch: 8.25 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02791648576237536		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.035144254794747774		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.03153037027856157 | validation: 0.04239509075607486]
	TIME [epoch: 8.28 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.037080950553615856		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.03597208494228078		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.036526517747948316 | validation: 0.022283100891712686]
	TIME [epoch: 8.24 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03822125166542441		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.026120082402231952		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.032170667033828176 | validation: 0.02123854014350413]
	TIME [epoch: 8.25 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02756703559974935		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.02111431128687511		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.024340673443312234 | validation: 0.02073493070025328]
	TIME [epoch: 8.25 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019904438763783353		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.030793858756404856		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.025349148760094104 | validation: 0.018593732530451455]
	TIME [epoch: 8.27 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0319713483917031		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.025631071994162896		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.028801210192932997 | validation: 0.015983010589764365]
	TIME [epoch: 8.25 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01948516840658886		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.05392663541488542		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.03670590191073714 | validation: 0.025000822587776948]
	TIME [epoch: 8.25 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03553000137847704		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.03377957439549338		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.034654787886985214 | validation: 0.01403839365512389]
	TIME [epoch: 8.25 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024551418994782557		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.03364359266990545		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.029097505832343995 | validation: 0.022617745708958154]
	TIME [epoch: 8.27 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02555296834898622		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.03126380744937623		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.028408387899181226 | validation: 0.010656347919233814]
	TIME [epoch: 8.25 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023837158294329763		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.03181314159139062		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.027825149942860193 | validation: 0.03730418050984377]
	TIME [epoch: 8.25 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02804470119366363		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.03013221162279519		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.0290884564082294 | validation: 0.020200060306662528]
	TIME [epoch: 8.24 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.035681745594021805		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.031393495412404805		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.03353762050321331 | validation: 0.018001531600051145]
	TIME [epoch: 8.27 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02708872573009503		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.023643114190816984		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.025365919960456014 | validation: 0.012008195392246023]
	TIME [epoch: 8.25 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026038359973296104		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.03349273984969997		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.02976554991149804 | validation: 0.02660346149881582]
	TIME [epoch: 8.25 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03606002583642068		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.027850992255886275		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.031955509046153484 | validation: 0.022479087648313207]
	TIME [epoch: 8.25 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026430041106424617		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.034854403501580505		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.03064222230400256 | validation: 0.02118459815443912]
	TIME [epoch: 8.27 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02605036600320017		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.025751265246871113		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.025900815625035645 | validation: 0.01518594347537474]
	TIME [epoch: 8.25 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021465564542438087		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.028172210552622968		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.024818887547530526 | validation: 0.0084683408094772]
	TIME [epoch: 8.24 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021471395318160492		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.0234468604518026		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.022459127884981545 | validation: 0.022176377399110575]
	TIME [epoch: 8.25 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022066022888489915		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.025931502050313533		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.023998762469401726 | validation: 0.022780518430012185]
	TIME [epoch: 8.27 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025146345228682688		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.024275471828359266		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.024710908528520977 | validation: 0.013126835922609473]
	TIME [epoch: 8.25 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0176954825439851		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.022497292107704885		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.020096387325844994 | validation: 0.02164876904468019]
	TIME [epoch: 8.25 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02359130793987359		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.01985539246930814		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.021723350204590867 | validation: 0.010169239123690148]
	TIME [epoch: 8.25 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021580014322741183		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 0.027134883634485473		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.024357448978613326 | validation: 0.018200988820252156]
	TIME [epoch: 8.27 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03177237513881719		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.02515088157873164		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.028461628358774423 | validation: 0.012836464835943298]
	TIME [epoch: 8.25 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04571415841273743		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.035769905971069486		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.04074203219190346 | validation: 0.02657490610440011]
	TIME [epoch: 8.25 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03175503199585295		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.07787980250027758		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.054817417248065256 | validation: 0.032460582322727576]
	TIME [epoch: 8.25 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.032503947297042		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.04263134937459576		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.03756764833581888 | validation: 0.05500020518229479]
	TIME [epoch: 8.26 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0411976476330738		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.030194640779885523		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.035696144206479666 | validation: 0.018031015251832917]
	TIME [epoch: 8.25 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02663117181719456		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.026303282703527987		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.026467227260361277 | validation: 0.028114756059472966]
	TIME [epoch: 8.24 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026099903667538886		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.021260723651745826		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.02368031365964236 | validation: 0.024910467784553955]
	TIME [epoch: 8.24 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025197765179881797		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 0.03219760911705855		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 0.02869768714847017 | validation: 0.02760807873229114]
	TIME [epoch: 8.26 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02889890181272552		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.03141013720625163		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 0.03015451950948858 | validation: 0.015242401034903784]
	TIME [epoch: 8.25 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02728036237286529		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 0.026994394936953443		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 0.027137378654909373 | validation: 0.020265605742651235]
	TIME [epoch: 8.25 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023075983229733938		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.016915480225569084		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.01999573172765151 | validation: 0.02487777234744784]
	TIME [epoch: 8.25 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03078778972587722		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.03682651487201824		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.03380715229894772 | validation: 0.029466454228863414]
	TIME [epoch: 8.26 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02717022353285652		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.024481051765700725		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.025825637649278622 | validation: 0.015805992999748008]
	TIME [epoch: 8.26 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03380061320814818		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.03192826891436344		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.0328644410612558 | validation: 0.030812385719419643]
	TIME [epoch: 8.25 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.04127317976792572		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.03495088935793959		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.03811203456293265 | validation: 0.02371158443273099]
	TIME [epoch: 8.25 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03126065795490211		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.027548581617562656		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.029404619786232385 | validation: 0.014913614469688165]
	TIME [epoch: 8.26 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016586086642692184		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.036343508654961144		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.02646479764882666 | validation: 0.01729451262672886]
	TIME [epoch: 8.26 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030585832969552928		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 0.03302184647686462		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.03180383972320877 | validation: 0.0281209024940144]
	TIME [epoch: 8.25 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038653359957629904		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 0.02772269730464308		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 0.03318802863113649 | validation: 0.027100557051352486]
	TIME [epoch: 8.25 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028229260982810617		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 0.036269645510150585		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 0.0322494532464806 | validation: 0.028197298005503538]
	TIME [epoch: 8.26 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031069751263556638		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.03661088975920197		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 0.03384032051137931 | validation: 0.03563889518681529]
	TIME [epoch: 8.26 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038630022957384165		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.03894084378048596		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.03878543336893507 | validation: 0.02641721118165092]
	TIME [epoch: 8.24 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02433758375283365		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.018550827579588945		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.021444205666211295 | validation: 0.013675035834366717]
	TIME [epoch: 8.24 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02986423488219154		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.031711175119320284		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.030787705000755915 | validation: 0.014058373711173343]
	TIME [epoch: 8.26 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026109898191201984		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.03217235147713051		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.029141124834166254 | validation: 0.028340933879461315]
	TIME [epoch: 8.26 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03211659380805405		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 0.024378877061912416		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 0.02824773543498324 | validation: 0.014723053371802361]
	TIME [epoch: 8.26 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023384257850285844		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.03158668846607046		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 0.027485473158178148 | validation: 0.018453696206057867]
	TIME [epoch: 8.24 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027911489696945492		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.028619393505754265		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.028265441601349882 | validation: 0.029031871019419404]
	TIME [epoch: 8.25 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02523175692494045		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.03700454506556646		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.031118150995253453 | validation: 0.040837635869606745]
	TIME [epoch: 8.27 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.029848483154677703		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.017788460112557504		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.023818471633617604 | validation: 0.015464556099446199]
	TIME [epoch: 8.25 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021847483823653695		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.027802667273519703		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.024825075548586702 | validation: 0.025739074598661538]
	TIME [epoch: 8.25 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026885428725462512		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.020191584954542215		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.02353850684000236 | validation: 0.018936720883766158]
	TIME [epoch: 8.24 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03011879354513457		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.02479295763039315		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.027455875587763863 | validation: 0.026656347912770717]
	TIME [epoch: 8.27 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02847420218553971		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.034716903950173926		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.031595553067856816 | validation: 0.050409554720324136]
	TIME [epoch: 8.25 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.044854480644158194		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.036536643595030116		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.04069556211959416 | validation: 0.02646127643495039]
	TIME [epoch: 8.24 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028637058681418276		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.02547676923079966		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.027056913956108965 | validation: 0.016796361260633556]
	TIME [epoch: 8.25 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027751417193921418		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.023845544157052916		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.02579848067548716 | validation: 0.01679674565264837]
	TIME [epoch: 8.27 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02225183221056258		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.03142651554015739		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.02683917387535999 | validation: 0.03624845236488234]
	TIME [epoch: 8.24 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027677835435709418		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.02697023333575436		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.027324034385731888 | validation: 0.016410098679727668]
	TIME [epoch: 8.24 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02133482493560324		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.038607943146035786		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.029971384040819515 | validation: 0.035170741908183575]
	TIME [epoch: 8.26 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025397648673799067		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.019930052967036906		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.02266385082041798 | validation: 0.013759039339430509]
	TIME [epoch: 8.26 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025121242772117212		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.02366615113637127		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.02439369695424424 | validation: 0.01943026362213077]
	TIME [epoch: 8.25 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026264593638604276		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.017855598283403936		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.0220600959610041 | validation: 0.01123129951701942]
	TIME [epoch: 8.23 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02240504488375967		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.024495510062211397		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.023450277472985535 | validation: 0.017270315600982463]
	TIME [epoch: 8.25 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02588430895789017		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.02640921288971041		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.026146760923800288 | validation: 0.014135364178716948]
	TIME [epoch: 8.27 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024251845105933062		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.01712192998000879		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.020686887542970926 | validation: 0.022395565932849408]
	TIME [epoch: 8.25 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018814483970759487		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.027955908523043593		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.02338519624690154 | validation: 0.01709109963332846]
	TIME [epoch: 8.24 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026567696573681233		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.03404902484591754		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.030308360709799385 | validation: 0.01947611921325294]
	TIME [epoch: 8.26 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03775390368733521		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.031374645186478055		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.034564274436906635 | validation: 0.013553207522559408]
	TIME [epoch: 8.26 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023886295239267448		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.022508458965145348		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.023197377102206398 | validation: 0.021323330213250453]
	TIME [epoch: 8.25 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023234774355819117		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.037338877703309924		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.030286826029564524 | validation: 0.015336521759478678]
	TIME [epoch: 8.24 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023096497003264414		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.031044202865748955		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.02707034993450668 | validation: 0.021985534021862384]
	TIME [epoch: 8.25 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030242625060425327		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.02014828388486316		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.025195454472644246 | validation: 0.033833374469475944]
	TIME [epoch: 8.27 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024582700482235455		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.02256734602232443		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.02357502325227994 | validation: 0.018222396832880648]
	TIME [epoch: 8.24 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016376987898540647		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.025416237473239		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.02089661268588982 | validation: 0.028779365569283993]
	TIME [epoch: 8.25 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02244752397618924		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.02900996165460538		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.02572874281539731 | validation: 0.0372488384247014]
	TIME [epoch: 8.25 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02881986502701564		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.017609223710106665		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.02321454436856115 | validation: 0.010152705750151446]
	TIME [epoch: 8.27 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02802709254850596		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.021857888514949304		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.024942490531727625 | validation: 0.022442069640844196]
	TIME [epoch: 8.24 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02575336015009757		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.03007313209480224		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.027913246122449907 | validation: 0.028924437216820767]
	TIME [epoch: 8.24 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030224615848140624		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.018897401853976238		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.02456100885105843 | validation: 0.021023614874974512]
	TIME [epoch: 8.25 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027342736855213635		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.027287254791075227		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.027314995823144433 | validation: 0.02219976015314428]
	TIME [epoch: 8.27 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030841231497947125		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.01783322713253544		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.024337229315241283 | validation: 0.02954976127206333]
	TIME [epoch: 8.25 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023754263993353505		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.0238630845824457		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.023808674287899602 | validation: 0.029314872578792464]
	TIME [epoch: 8.24 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.031210446112693226		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.020096346848665063		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.02565339648067914 | validation: 0.009346281117830991]
	TIME [epoch: 8.25 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016797021046861902		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.02537654886388756		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.021086784955374728 | validation: 0.026475693268672745]
	TIME [epoch: 8.27 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02206300436809456		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.024599306934200933		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.023331155651147746 | validation: 0.035091892592014]
	TIME [epoch: 8.25 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.038112341810577106		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.015590175305572399		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.026851258558074752 | validation: 0.011120148854243896]
	TIME [epoch: 8.24 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018188998005244074		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.025176751253576844		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.02168287462941046 | validation: 0.018386617681024647]
	TIME [epoch: 8.24 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01728697363646784		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.03289764307932726		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.025092308357897553 | validation: 0.016831650387088476]
	TIME [epoch: 8.26 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018174716974275994		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.019129261090301518		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.018651989032288756 | validation: 0.006982714041262585]
	TIME [epoch: 8.25 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023907549959223548		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.024274214898472625		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.024090882428848083 | validation: 0.021004639772282178]
	TIME [epoch: 8.25 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018382752284824815		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.031485428266345		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.024934090275584908 | validation: 0.016680916822983255]
	TIME [epoch: 8.25 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026296367721514818		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.01650075774435777		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.021398562732936298 | validation: 0.011557781769491054]
	TIME [epoch: 8.27 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0172579645863117		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.023931035413062997		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.02059449999968735 | validation: 0.009671639115417435]
	TIME [epoch: 8.24 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0182199982041731		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.029027509534146772		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.02362375386915993 | validation: 0.01016901758277051]
	TIME [epoch: 8.25 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022203394172748837		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.020539480731606836		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.021371437452177837 | validation: 0.012292025103950572]
	TIME [epoch: 8.24 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01889554985896662		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.028543906259640307		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.023719728059303464 | validation: 0.015163015536144788]
	TIME [epoch: 8.26 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03980502353312727		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.018864993394290486		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.029335008463708873 | validation: 0.009805280449839927]
	TIME [epoch: 8.25 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02474796082055648		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.025224808150941535		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.02498638448574901 | validation: 0.024471829370757234]
	TIME [epoch: 8.24 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03744428207086994		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.03115102845957548		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.034297655265222715 | validation: 0.022670478329132646]
	TIME [epoch: 8.24 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026353462093743		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.027765082738874347		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.02705927241630867 | validation: 0.020108006774396208]
	TIME [epoch: 8.27 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02023104021254186		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.02188543339732337		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.02105823680493262 | validation: 0.010944370499646007]
	TIME [epoch: 8.25 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016686776586305272		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.019934455574812434		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.018310616080558853 | validation: 0.022976457359232233]
	TIME [epoch: 8.24 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024194286621369524		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.020349372115424806		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.02227182936839716 | validation: 0.027534341891979966]
	TIME [epoch: 8.24 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025296045662789195		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.017191118132843342		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.02124358189781627 | validation: 0.013934067168319884]
	TIME [epoch: 8.26 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02158645886446744		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.02293280583218645		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.022259632348326945 | validation: 0.03281630757341586]
	TIME [epoch: 8.25 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018785978430737402		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.024659104595422567		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.021722541513079986 | validation: 0.026141745760506218]
	TIME [epoch: 8.24 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026472919225974407		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.023701185615573158		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.025087052420773782 | validation: 0.01799575156170703]
	TIME [epoch: 8.24 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02294970050372444		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.022976153966394706		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.02296292723505957 | validation: 0.018604756787204326]
	TIME [epoch: 8.26 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026401097194743466		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.02032848033525756		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.023364788765000512 | validation: 0.009799209082867147]
	TIME [epoch: 8.24 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02051355491194574		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.016774444481709914		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.018643999696827826 | validation: 0.008999427160137281]
	TIME [epoch: 8.24 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016434487908012434		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.024706689228233426		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.020570588568122928 | validation: 0.020103686115094905]
	TIME [epoch: 8.24 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02435318277373677		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.03553935665330794		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.02994626971352235 | validation: 0.037356979405276214]
	TIME [epoch: 8.26 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03852745051014507		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.03097602936925369		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.03475173993969938 | validation: 0.022537422377675545]
	TIME [epoch: 8.25 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019981715723734188		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.029024811011288017		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.0245032633675111 | validation: 0.019199121601468663]
	TIME [epoch: 8.25 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0196471426802927		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.030504492711877253		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.025075817696084975 | validation: 0.03427860281105056]
	TIME [epoch: 8.25 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02464547083974912		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 0.021203596098292577		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.022924533469020847 | validation: 0.019841626546759594]
	TIME [epoch: 8.27 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024311424003335105		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.019011532338257645		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.021661478170796375 | validation: 0.031330770781968276]
	TIME [epoch: 8.25 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02209904529876679		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 0.028499734342334427		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.025299389820550612 | validation: 0.021165958858412784]
	TIME [epoch: 8.25 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022678619878572545		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 0.032544937029597105		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 0.02761177845408483 | validation: 0.01947300199028818]
	TIME [epoch: 8.24 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016452923988426574		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 0.023249121342973064		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 0.019851022665699825 | validation: 0.012715310267186692]
	TIME [epoch: 8.27 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021796291672210644		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 0.026732611052935746		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 0.024264451362573193 | validation: 0.020141843938998608]
	TIME [epoch: 8.25 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02206504689596487		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 0.0172540459766754		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 0.019659546436320136 | validation: 0.012287306828284666]
	TIME [epoch: 8.24 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024844507600508155		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 0.01888846041542886		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 0.02186648400796851 | validation: 0.009022024252126655]
	TIME [epoch: 8.24 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013804472459140505		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 0.018120652021517872		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 0.015962562240329183 | validation: 0.012163884654158007]
	TIME [epoch: 8.27 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02933497032517306		[learning rate: 0.000279]
		[batch 20/20] avg loss: 0.022615719830100953		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 0.02597534507763701 | validation: 0.006818530594182587]
	TIME [epoch: 8.25 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019841211503305586		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 0.021303772272757364		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 0.020572491888031473 | validation: 0.034534527711244656]
	TIME [epoch: 8.25 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020751450353843244		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 0.02917903119332165		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 0.02496524077358245 | validation: 0.009101315790867379]
	TIME [epoch: 8.24 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017226347974928367		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 0.02766969915880902		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 0.022448023566868692 | validation: 0.014205123899864476]
	TIME [epoch: 8.27 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027191494489609558		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 0.026708170495904637		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 0.026949832492757092 | validation: 0.023177905543067784]
	TIME [epoch: 8.24 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028336960411059688		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 0.01810374737636638		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 0.023220353893713037 | validation: 0.021764480406055876]
	TIME [epoch: 8.24 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03244281611065095		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.019146729716097752		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 0.02579477291337435 | validation: 0.014102642830987444]
	TIME [epoch: 8.25 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018548429263504383		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.015295053990807907		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.016921741627156144 | validation: 0.014550779382661989]
	TIME [epoch: 8.27 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01465316883663836		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.023629172687818424		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.019141170762228394 | validation: 0.016202635022994825]
	TIME [epoch: 8.25 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02001049328709713		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.024229235399788322		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.022119864343442725 | validation: 0.010324917451448486]
	TIME [epoch: 8.25 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01808775687542619		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.02938606271537676		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.023736909795401474 | validation: 0.01679220628245409]
	TIME [epoch: 8.24 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01656772070670283		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.019337586885759126		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.017952653796230976 | validation: 0.009351695770555488]
	TIME [epoch: 8.27 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020407375199599696		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.02404553926630825		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.022226457232953974 | validation: 0.010110050284511412]
	TIME [epoch: 8.25 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018739250944329905		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.027127574610965415		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.022933412777647665 | validation: 0.02284153349333948]
	TIME [epoch: 8.24 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03955074093412522		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.026095267296631457		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.03282300411537833 | validation: 0.011679690075328248]
	TIME [epoch: 8.24 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022672440191273492		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.02467450478919681		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.023673472490235145 | validation: 0.017171667951206452]
	TIME [epoch: 8.27 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018674729831991043		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.017282650653740367		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.017978690242865705 | validation: 0.01164012461729294]
	TIME [epoch: 8.25 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.030272147870945426		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.014196358662521997		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.02223425326673371 | validation: 0.018087491827986087]
	TIME [epoch: 8.24 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02208319636025375		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.018074486199859588		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.02007884128005667 | validation: 0.03179360745932851]
	TIME [epoch: 8.25 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026629841687549772		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 0.02421700674042723		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 0.0254234242139885 | validation: 0.020999780625364284]
	TIME [epoch: 8.26 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02702064358219878		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 0.023912578508839495		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 0.025466611045519137 | validation: 0.026438903647595838]
	TIME [epoch: 8.25 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02445107858821804		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 0.019522327308694565		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 0.021986702948456307 | validation: 0.015991897449824766]
	TIME [epoch: 8.24 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023937685947484695		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 0.02399817492915725		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 0.02396793043832098 | validation: 0.010300943995523393]
	TIME [epoch: 8.25 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027540527070382376		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.019683851269451845		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 0.023612189169917114 | validation: 0.006157070728060146]
	TIME [epoch: 8.26 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014512845148268417		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 0.023835356678339546		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 0.01917410091330398 | validation: 0.016964698152425527]
	TIME [epoch: 8.25 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03346349165576178		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 0.023401287090977864		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 0.028432389373369827 | validation: 0.028115860108011113]
	TIME [epoch: 8.24 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022684473320494662		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 0.021116053568053177		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 0.02190026344427392 | validation: 0.019272659208336484]
	TIME [epoch: 8.24 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027041843727594616		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 0.020007414304727376		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 0.023524629016160998 | validation: 0.01235103888715018]
	TIME [epoch: 8.27 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02290048584589918		[learning rate: 0.000252]
		[batch 20/20] avg loss: 0.018875988736741733		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 0.020888237291320454 | validation: 0.016041533540121642]
	TIME [epoch: 8.24 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024573241314860952		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.01566674140489794		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.02011999135987945 | validation: 0.011156488978142002]
	TIME [epoch: 8.25 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018981009966336455		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.021209904297231748		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.0200954571317841 | validation: 0.006604849120704339]
	TIME [epoch: 8.24 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019109008235535525		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.023096179478060783		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.021102593856798153 | validation: 0.006870150255823456]
	TIME [epoch: 8.27 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02095205585878588		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.015854370653634646		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.018403213256210266 | validation: 0.016296280326683055]
	TIME [epoch: 8.25 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027610357802509755		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.016109138004970984		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.021859747903740363 | validation: 0.013793110448640653]
	TIME [epoch: 8.25 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02788752264600309		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.01784996735796517		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.022868745001984124 | validation: 0.01336644546802615]
	TIME [epoch: 8.25 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022723717887526505		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.01999074396947467		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.021357230928500587 | validation: 0.012026474451206205]
	TIME [epoch: 8.27 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01606485411101783		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.026307552189057153		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.021186203150037487 | validation: 0.012063123005522158]
	TIME [epoch: 8.25 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01748027763444589		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.0209270992567284		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.01920368844558714 | validation: 0.011144016428289655]
	TIME [epoch: 8.24 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018271094718116807		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.01834614028197227		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.018308617500044536 | validation: 0.012507731343356565]
	TIME [epoch: 8.24 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015482789756882937		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.015367770487592691		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.015425280122237808 | validation: 0.018775754559125973]
	TIME [epoch: 8.26 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03673813244535597		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.019185209109223826		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.0279616707772899 | validation: 0.008400413896913886]
	TIME [epoch: 8.25 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019856259862911115		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.015597342198100292		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.017726801030505706 | validation: 0.010897801492909944]
	TIME [epoch: 8.24 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020679790528335226		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.025985722908668362		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.023332756718501796 | validation: 0.005207748576776736]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020114785730964018		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.022430093722213003		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.021272439726588512 | validation: 0.012460459443749648]
	TIME [epoch: 8.28 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019739748258270566		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.023743442338692806		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.021741595298481683 | validation: 0.016961195234616217]
	TIME [epoch: 8.25 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019368226768332376		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.016785065232150752		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.018076646000241562 | validation: 0.027220273670068424]
	TIME [epoch: 8.24 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03314030474399644		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.01967063218299793		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.026405468463497183 | validation: 0.028604449283005783]
	TIME [epoch: 8.24 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.03070598218530678		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.018688474847116186		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.02469722851621148 | validation: 0.02095566018986526]
	TIME [epoch: 8.26 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023194929683373708		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.021922011963644294		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.022558470823509 | validation: 0.017497522547504005]
	TIME [epoch: 8.25 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02267129978280408		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.016445280222459023		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.019558290002631554 | validation: 0.005796946541784127]
	TIME [epoch: 8.24 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02292099620480462		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.025541243909525464		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.02423112005716504 | validation: 0.017969512997562193]
	TIME [epoch: 8.24 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022663754945110005		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.018608497966220622		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.020636126455665312 | validation: 0.019034787571930337]
	TIME [epoch: 8.26 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026100253800130018		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.02001291514184323		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.023056584470986626 | validation: 0.010269955649730782]
	TIME [epoch: 8.24 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014641546865990767		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.013566998899236377		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.014104272882613572 | validation: 0.011658524810747393]
	TIME [epoch: 8.24 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018736390826603885		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.02307233969903958		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.020904365262821734 | validation: 0.014459828539689499]
	TIME [epoch: 8.24 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022263033891373178		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.01495069181151774		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.01860686285144546 | validation: 0.013330572944227239]
	TIME [epoch: 8.26 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02429138263858849		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.016106535442633317		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.02019895904061091 | validation: 0.015739943453114184]
	TIME [epoch: 8.24 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020066890763271703		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.018613550070331164		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.019340220416801432 | validation: 0.009149713638349512]
	TIME [epoch: 8.24 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015991970867224516		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.022207145331038968		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.01909955809913174 | validation: 0.0247600138127912]
	TIME [epoch: 8.24 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01875000286966378		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.01568204396304013		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.01721602341635195 | validation: 0.006836902022961637]
	TIME [epoch: 8.26 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019460076380519414		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.020932955839148254		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.020196516109833838 | validation: 0.013501035476971256]
	TIME [epoch: 8.24 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01592778126905531		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.016660077524281787		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.016293929396668547 | validation: 0.012716673580411316]
	TIME [epoch: 8.24 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015985955006292348		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.01973088597829078		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.01785842049229156 | validation: 0.012362055301735392]
	TIME [epoch: 8.24 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021353304967025578		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.020117853241651072		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.02073557910433833 | validation: 0.01026322235841492]
	TIME [epoch: 8.26 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01754069556337285		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.018212028358632372		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.01787636196100261 | validation: 0.01103606344794026]
	TIME [epoch: 8.23 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01858797201266878		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.01812316223639012		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.01835556712452945 | validation: 0.013870413580521187]
	TIME [epoch: 8.24 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013891158474223361		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.0231894795549707		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.01854031901459703 | validation: 0.019215385188148422]
	TIME [epoch: 8.24 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02222717259845052		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.018872875016810915		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.020550023807630716 | validation: 0.007856506754909164]
	TIME [epoch: 8.26 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015507636157751909		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.028680741052079583		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.022094188604915747 | validation: 0.02643621248023276]
	TIME [epoch: 8.24 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017322823788654865		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.029167919341086496		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.023245371564870675 | validation: 0.019916024244943993]
	TIME [epoch: 8.23 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.033146769889369175		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.019673269160681968		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.026410019525025568 | validation: 0.010024781532703309]
	TIME [epoch: 8.23 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023225886131056743		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.015252056479789203		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.019238971305422972 | validation: 0.011732962789735018]
	TIME [epoch: 8.26 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019470467479310204		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.018175529826489744		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.018822998652899974 | validation: 0.018585432587119397]
	TIME [epoch: 8.24 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019672143883539613		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.02690825847190243		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.023290201177721025 | validation: 0.009287853301821237]
	TIME [epoch: 8.24 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02389420164099799		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.02371627970565328		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.023805240673325644 | validation: 0.03685656984538261]
	TIME [epoch: 8.23 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01931865812196825		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.023696627606185003		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.021507642864076623 | validation: 0.016267223913474318]
	TIME [epoch: 8.26 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01912588364142354		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.014979232334413872		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.017052557987918708 | validation: 0.007452001552464607]
	TIME [epoch: 8.24 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016739935959354614		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 0.017024531538639632		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 0.01688223374899712 | validation: 0.023839959886247812]
	TIME [epoch: 8.24 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025738565033465548		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 0.02063389287867396		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 0.023186228956069758 | validation: 0.018350880776848714]
	TIME [epoch: 8.24 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01676176488388068		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 0.018364586969224057		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 0.01756317592655237 | validation: 0.01913575686159421]
	TIME [epoch: 8.25 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.026417724690318112		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 0.02564418447155125		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 0.026030954580934677 | validation: 0.008944943922161844]
	TIME [epoch: 8.24 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021799781649773232		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 0.021724240827907065		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 0.021762011238840147 | validation: 0.014780605693296473]
	TIME [epoch: 8.23 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019556969420992845		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 0.022022586649899523		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 0.02078977803544619 | validation: 0.018780905555542075]
	TIME [epoch: 8.23 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02208004067654816		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 0.016436525415528556		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 0.019258283046038355 | validation: 0.011675830412545002]
	TIME [epoch: 8.27 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01867377526400579		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 0.015849038167971338		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 0.01726140671598856 | validation: 0.013726012236688175]
	TIME [epoch: 8.24 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01649807789774113		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 0.02340399039545269		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 0.019951034146596915 | validation: 0.01707972461423495]
	TIME [epoch: 8.24 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022705962355276266		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 0.019608879700881862		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 0.02115742102807907 | validation: 0.009042674902477142]
	TIME [epoch: 8.23 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016069338707134295		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 0.01795762760166484		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 0.017013483154399565 | validation: 0.01683138075310693]
	TIME [epoch: 8.27 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.028344016263070853		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 0.017721956590633373		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 0.023032986426852115 | validation: 0.010870482856905303]
	TIME [epoch: 8.24 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016520166016080792		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 0.016725533809590895		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 0.016622849912835844 | validation: 0.02446173175431321]
	TIME [epoch: 8.24 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018597414053151587		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 0.028549907203837682		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 0.023573660628494635 | validation: 0.02652329883473778]
	TIME [epoch: 8.23 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02139512343235912		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 0.018791059598106202		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 0.02009309151523266 | validation: 0.007536321183326422]
	TIME [epoch: 8.26 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020942985265572475		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 0.01686723494618799		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 0.01890511010588023 | validation: 0.020165630424032306]
	TIME [epoch: 8.24 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02920738235629457		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 0.01992477922176749		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 0.024566080789031032 | validation: 0.022325239912911143]
	TIME [epoch: 8.24 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01834727019488032		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 0.021773068656844162		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 0.02006016942586224 | validation: 0.015531273642912038]
	TIME [epoch: 8.24 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018760034174986954		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 0.022584773818461183		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 0.020672403996724074 | validation: 0.02512932976964066]
	TIME [epoch: 8.25 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022281066836334754		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 0.01570102033661264		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 0.01899104358647369 | validation: 0.02232126558431622]
	TIME [epoch: 8.25 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01788633666571917		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 0.019427839833817878		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 0.018657088249768528 | validation: 0.013017549291301535]
	TIME [epoch: 8.24 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020285914420669484		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 0.020935343208411717		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 0.0206106288145406 | validation: 0.011700985658018307]
	TIME [epoch: 8.24 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01760885473999496		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 0.02206531161276932		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 0.019837083176382136 | validation: 0.00860713432175975]
	TIME [epoch: 8.25 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019620469350840882		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 0.015153092327623829		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 0.017386780839232355 | validation: 0.013476888723570982]
	TIME [epoch: 8.25 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018812709703972454		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 0.019551780414607277		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 0.01918224505928987 | validation: 0.01188666413236872]
	TIME [epoch: 8.24 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017570724341396938		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 0.01628755196362925		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 0.016929138152513096 | validation: 0.01336354174939103]
	TIME [epoch: 8.24 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020695117266488965		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.01626244217407482		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 0.01847877972028189 | validation: 0.008207754670392515]
	TIME [epoch: 8.25 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016946188650907617		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.01902219559715836		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.01798419212403299 | validation: 0.013839927100345347]
	TIME [epoch: 8.25 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017150764123586203		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.012653483915461564		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.014902124019523882 | validation: 0.01099113999658028]
	TIME [epoch: 8.24 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018939245411915044		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.014320714922453398		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.01662998016718422 | validation: 0.010846786268522143]
	TIME [epoch: 8.24 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020368919784711167		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.019074310434834995		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.019721615109773084 | validation: 0.01851086384655329]
	TIME [epoch: 8.25 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018534862291606696		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.01599330741891337		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.017264084855260035 | validation: 0.012828918039737219]
	TIME [epoch: 8.26 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020357124975806093		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.016276358470880513		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.018316741723343296 | validation: 0.013622618244803424]
	TIME [epoch: 8.24 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019227180980456417		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.01954350578201801		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.01938534338123721 | validation: 0.009386968311642388]
	TIME [epoch: 8.24 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02221916919300112		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.01986796324794563		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.021043566220473375 | validation: 0.01305731196883662]
	TIME [epoch: 8.24 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018318188483391083		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.02341570247186117		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.020866945477626127 | validation: 0.01396605049102271]
	TIME [epoch: 8.26 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01909739906026461		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.017396625356970265		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.018247012208617443 | validation: 0.009006396742069196]
	TIME [epoch: 8.24 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017680620543811015		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.01878911892376996		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.01823486973379049 | validation: 0.006321437986105194]
	TIME [epoch: 8.25 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022381243421651795		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.016556303157317653		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.019468773289484723 | validation: 0.01800438918812872]
	TIME [epoch: 8.24 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.022436408311985206		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.017993427529752484		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.02021491792086885 | validation: 0.020438970042395237]
	TIME [epoch: 8.26 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018487873832373804		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.017395734788628617		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.01794180431050121 | validation: 0.006815380691554258]
	TIME [epoch: 8.23 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021626735014061454		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.01810984001476234		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.0198682875144119 | validation: 0.010392538407915729]
	TIME [epoch: 8.24 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01929698721793069		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.020365787279033994		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.019831387248482345 | validation: 0.008804867193235503]
	TIME [epoch: 8.24 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019495489302577794		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.015893176067596475		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.017694332685087133 | validation: 0.009108038338368075]
	TIME [epoch: 8.26 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01735452007446087		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.01962156055910933		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.018488040316785102 | validation: 0.01081428415494901]
	TIME [epoch: 8.24 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01567142459210938		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.023469298317255798		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.019570361454682588 | validation: 0.011790248231264257]
	TIME [epoch: 8.24 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012985244571032001		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.02478441598903301		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.018884830280032502 | validation: 0.024324386391451323]
	TIME [epoch: 8.25 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01867296049487528		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.016143007690247794		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.017407984092561536 | validation: 0.014950551973010831]
	TIME [epoch: 8.26 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015548714151783235		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.01579327636662032		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.015670995259201778 | validation: 0.011031323402114154]
	TIME [epoch: 8.25 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019795704785799863		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.021964493917582532		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.020880099351691196 | validation: 0.016023320308214326]
	TIME [epoch: 8.23 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01653693344768708		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.015208539044180513		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.015872736245933796 | validation: 0.01207642443012573]
	TIME [epoch: 8.25 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014593367024933199		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.016527857507004362		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.01556061226596878 | validation: 0.0198884203201232]
	TIME [epoch: 8.26 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01755592198381271		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.01713931378874554		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.01734761788627913 | validation: 0.009195401194485529]
	TIME [epoch: 8.23 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01864018856721813		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.017189956602905152		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.017915072585061646 | validation: 0.01158278064694528]
	TIME [epoch: 8.25 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013445985341278102		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.017439995092690192		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.015442990216984148 | validation: 0.014423037669219511]
	TIME [epoch: 8.24 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025205210286318676		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.020935989462864044		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.023070599874591362 | validation: 0.00712454191222198]
	TIME [epoch: 8.27 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01427900228658018		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.012422458305717917		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.013350730296149051 | validation: 0.0036292870383323956]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1218.pth
	Model improved!!!
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015048258335068542		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.020457907562945674		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.017753082949007105 | validation: 0.016811601035640805]
	TIME [epoch: 8.25 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.027626955783596297		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.012158508784457468		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.019892732284026885 | validation: 0.013379563403743935]
	TIME [epoch: 8.24 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018227512141959164		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.021793675524737533		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.02001059383334835 | validation: 0.01230644537810062]
	TIME [epoch: 8.25 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016839193558611416		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.028557482539312277		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.022698338048961843 | validation: 0.02586286352914146]
	TIME [epoch: 8.24 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023344426295462014		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.015077739431361314		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.019211082863411662 | validation: 0.0072046374855301585]
	TIME [epoch: 8.24 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01783869369073205		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.0112124344563943		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.01452556407356317 | validation: 0.01615576439953683]
	TIME [epoch: 8.24 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023048836895300732		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.018173286893980633		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.020611061894640684 | validation: 0.010878823331270975]
	TIME [epoch: 8.26 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016606125862661542		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.014516908742143916		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.015561517302402733 | validation: 0.01105862489195433]
	TIME [epoch: 8.24 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013748001138171654		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.01773009012805052		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.015739045633111088 | validation: 0.021905739066112484]
	TIME [epoch: 8.23 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013589005908170498		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.02040674125170027		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.016997873579935385 | validation: 0.01491737970188124]
	TIME [epoch: 8.24 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015196363394897602		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.018200342011486945		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.016698352703192275 | validation: 0.016323949555177887]
	TIME [epoch: 8.26 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019782864665511962		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.023144850202207575		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.021463857433859767 | validation: 0.01206077590339187]
	TIME [epoch: 8.24 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017766559354939097		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.022213117492272748		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.01998983842360592 | validation: 0.01352744317941301]
	TIME [epoch: 8.23 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017534202049696933		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.01624191112813963		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.01688805658891828 | validation: 0.008294278747555629]
	TIME [epoch: 8.24 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019712880577446298		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.012291266483823364		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.016002073530634833 | validation: 0.016092144024423386]
	TIME [epoch: 8.26 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013767794921267739		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.01469273300368055		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.014230263962474149 | validation: 0.006277727980778041]
	TIME [epoch: 8.24 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01599925371355271		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.019607019614064643		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.01780313666380868 | validation: 0.02157102518322437]
	TIME [epoch: 8.23 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01780389337761168		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.018772662698585813		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.018288278038098744 | validation: 0.009416491088411319]
	TIME [epoch: 8.24 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01775515143540709		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.02648163803251739		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.022118394733962245 | validation: 0.011336616111083673]
	TIME [epoch: 8.26 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019714876778797037		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.016360554610483406		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.01803771569464022 | validation: 0.016346406356325338]
	TIME [epoch: 8.23 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01937025796235442		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.01575396728252475		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.01756211262243959 | validation: 0.010352308172746794]
	TIME [epoch: 8.24 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01697895001127723		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.019900595292618773		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.018439772651947996 | validation: 0.011919809032949294]
	TIME [epoch: 8.24 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019232403790026973		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.017238753910355783		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.018235578850191378 | validation: 0.01713024877385728]
	TIME [epoch: 8.25 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012060066912056302		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.015150154104075816		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.013605110508066056 | validation: 0.008301475914046823]
	TIME [epoch: 8.24 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017630542870312835		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.017421233846842528		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.017525888358577678 | validation: 0.0029794463002876127]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1243.pth
	Model improved!!!
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025910878314767273		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.021221632135890657		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.023566255225328958 | validation: 0.006701336295397435]
	TIME [epoch: 8.26 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017126098616253242		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.016229079331385544		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.016677588973819397 | validation: 0.0135321448942654]
	TIME [epoch: 8.27 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01739616393797391		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.019517030347649712		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.018456597142811813 | validation: 0.010484972473848076]
	TIME [epoch: 8.24 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018144740041993228		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.016800558390457403		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.01747264921622532 | validation: 0.006033267051154462]
	TIME [epoch: 8.24 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016324058520056466		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.019386132889690055		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.017855095704873262 | validation: 0.014536544253815224]
	TIME [epoch: 8.24 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01725366805585437		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.015841319853249568		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.01654749395455197 | validation: 0.004950622521181092]
	TIME [epoch: 8.26 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011357222473302746		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.025887518429774402		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.018622370451538573 | validation: 0.01275124207986989]
	TIME [epoch: 8.24 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018544296234923992		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.02098205406286346		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.019763175148893725 | validation: 0.012326376833478585]
	TIME [epoch: 8.24 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025639982694111628		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.014068570468900254		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.019854276581505938 | validation: 0.010061976810458343]
	TIME [epoch: 8.24 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019052824628596147		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.019322168633077844		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.019187496630836996 | validation: 0.019707187721510915]
	TIME [epoch: 8.27 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017277363369197368		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.013309490395688659		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.015293426882443012 | validation: 0.01614478966854077]
	TIME [epoch: 8.24 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017387063324050954		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.018037676417713092		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.01771236987088203 | validation: 0.013094748118000783]
	TIME [epoch: 8.24 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015576586044893795		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.014213953897284495		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.014895269971089146 | validation: 0.007313396084803804]
	TIME [epoch: 8.25 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01008515185845194		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.01856948139611346		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.014327316627282701 | validation: 0.015497425784935504]
	TIME [epoch: 8.27 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01228811878448903		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.018583941691252503		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.015436030237870765 | validation: 0.016669071525719534]
	TIME [epoch: 8.24 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016818998715042922		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.0174399594521913		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.01712947908361712 | validation: 0.01944804899046245]
	TIME [epoch: 8.25 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02948319842652855		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.021803524533034357		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.025643361479781458 | validation: 0.009919810319343317]
	TIME [epoch: 8.24 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0163277008613336		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.019428448069648115		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.01787807446549086 | validation: 0.013737270946362295]
	TIME [epoch: 8.27 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014031077639979841		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.0171481581262495		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.01558961788311467 | validation: 0.008219553258110617]
	TIME [epoch: 8.24 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023196426525979086		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.012137152399085985		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.017666789462532535 | validation: 0.011516494995541492]
	TIME [epoch: 8.24 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011588677479061064		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.020621187294199674		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.016104932386630372 | validation: 0.015658616484920732]
	TIME [epoch: 8.24 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01777151587374636		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.02797930730658387		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.022875411590165116 | validation: 0.018089784520804245]
	TIME [epoch: 8.26 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01731131611006958		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.0126483514137981		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.014979833761933839 | validation: 0.010807362473883466]
	TIME [epoch: 8.25 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00760719999414487		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.0175646145428115		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.012585907268478183 | validation: 0.009269889163101019]
	TIME [epoch: 8.24 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015743748672906305		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.010589688990887507		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.013166718831896906 | validation: 0.007424743699306268]
	TIME [epoch: 8.25 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011627098613104745		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.011811009504698173		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.01171905405890146 | validation: 0.012016909848159719]
	TIME [epoch: 8.26 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016201428829558977		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.01509132743870647		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.015646378134132728 | validation: 0.01691360444371793]
	TIME [epoch: 8.24 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02229886649825283		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.010860911761845903		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.016579889130049368 | validation: 0.02390990105495453]
	TIME [epoch: 8.22 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01808416688823553		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.014776626376328128		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.016430396632281827 | validation: 0.009547158017171636]
	TIME [epoch: 8.23 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013743620889337796		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.016475867812967754		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.015109744351152777 | validation: 0.005929712773961426]
	TIME [epoch: 8.26 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008090555556613343		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.01966305142347346		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.013876803490043402 | validation: 0.006794490607300683]
	TIME [epoch: 8.24 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010831969009085349		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.018202918395328112		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.014517443702206732 | validation: 0.010980866768913395]
	TIME [epoch: 8.24 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01820948691539588		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.013918622871518932		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.016064054893457407 | validation: 0.015071378162762984]
	TIME [epoch: 8.24 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0123704783518867		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.023433310068090213		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.017901894209988455 | validation: 0.024584807624696794]
	TIME [epoch: 8.27 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016023358570806043		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.018868167105066842		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.01744576283793644 | validation: 0.008846487142369484]
	TIME [epoch: 8.24 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013598528580064675		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.015465026485288868		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.014531777532676773 | validation: 0.01757891554485392]
	TIME [epoch: 8.24 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009536513351509282		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.01480672288333178		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.01217161811742053 | validation: 0.01171534909244327]
	TIME [epoch: 8.24 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021322126424036216		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.014438868352944797		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.017880497388490506 | validation: 0.015235667897904965]
	TIME [epoch: 8.26 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019519156156253548		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.012429456375283237		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.01597430626576839 | validation: 0.014224954662074196]
	TIME [epoch: 8.24 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019700640226582668		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.017868584106103366		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.01878461216634302 | validation: 0.004280267118437532]
	TIME [epoch: 8.24 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013936672695930732		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.012349224134186194		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.013142948415058465 | validation: 0.010644745360998936]
	TIME [epoch: 8.24 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019508039656426206		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.01915405429613946		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.01933104697628283 | validation: 0.006280119970539107]
	TIME [epoch: 8.25 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019984510322174878		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 0.01292318023386296		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.01645384527801892 | validation: 0.011347732264823636]
	TIME [epoch: 8.24 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012878324530629615		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.030079929699025832		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.021479127114827724 | validation: 0.012643210226920849]
	TIME [epoch: 8.23 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016008538084834902		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.020968505277410485		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.018488521681122694 | validation: 0.012002135387383055]
	TIME [epoch: 8.23 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01846191108211597		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.014045257964690657		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.016253584523403315 | validation: 0.011080670012244076]
	TIME [epoch: 8.25 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014255466788164051		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.01153803827231109		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.012896752530237568 | validation: 0.014960575520399583]
	TIME [epoch: 8.23 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01874930098633338		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.016167342776649475		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.017458321881491427 | validation: 0.012215900046255118]
	TIME [epoch: 8.22 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01852675333070952		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.022679530022094413		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.02060314167640196 | validation: 0.020228848514006845]
	TIME [epoch: 8.22 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.024772250867986047		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.018525897111130736		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.02164907398955839 | validation: 0.012734283871055602]
	TIME [epoch: 8.26 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015712150223496812		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.009995459117691381		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.012853804670594096 | validation: 0.011828140369784215]
	TIME [epoch: 8.24 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014966051156774985		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.021170375364184167		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.018068213260479575 | validation: 0.004853764965399232]
	TIME [epoch: 8.24 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01140585685795658		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.019875629510636532		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.015640743184296556 | validation: 0.012588689594586016]
	TIME [epoch: 8.23 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015935140463030607		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.01488403345869073		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.015409586960860668 | validation: 0.009860687385526517]
	TIME [epoch: 8.27 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018611379396577928		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.012902574820468976		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.015756977108523453 | validation: 0.02020288836246817]
	TIME [epoch: 8.24 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019967123057682147		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.016602710587739742		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.018284916822710946 | validation: 0.00986274772491473]
	TIME [epoch: 8.24 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016438473296182317		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.021161960670052347		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.01880021698311733 | validation: 0.013826621415312604]
	TIME [epoch: 8.23 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020336227887412465		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.01688326417720355		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.018609746032308003 | validation: 0.017285541350350472]
	TIME [epoch: 8.26 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015497323777560617		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.01363080706238195		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.014564065419971286 | validation: 0.014588724490054234]
	TIME [epoch: 8.24 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018950645892088194		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.01799516275106215		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.01847290432157517 | validation: 0.013796800681536159]
	TIME [epoch: 8.24 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.023794255487365884		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.017107512337027328		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.020450883912196606 | validation: 0.015501973936259088]
	TIME [epoch: 8.24 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008866283970318462		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.019067152879944455		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.013966718425131457 | validation: 0.014198602513053443]
	TIME [epoch: 8.27 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020190689545747244		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.017023417453945287		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.018607053499846267 | validation: 0.014740299802519296]
	TIME [epoch: 8.24 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018696955464909668		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.014689107736171197		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.016693031600540434 | validation: 0.02208259242168864]
	TIME [epoch: 8.24 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021328357940096312		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.015137093032906505		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.018232725486501407 | validation: 0.011851629892757322]
	TIME [epoch: 8.23 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018363939309913212		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.013477100813880299		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.01592052006189676 | validation: 0.010785967405750269]
	TIME [epoch: 8.25 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02069273308816181		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.013605693806037308		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.01714921344709956 | validation: 0.01104273208540821]
	TIME [epoch: 8.24 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013123423479948701		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.015553090334263029		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.014338256907105865 | validation: 0.016569698362815286]
	TIME [epoch: 8.24 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01562136269293588		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.017736127290328595		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.01667874499163224 | validation: 0.006586073938761202]
	TIME [epoch: 8.23 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01599396853038859		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.011100732503097788		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.01354735051674319 | validation: 0.008997018056765423]
	TIME [epoch: 8.25 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013082585941620834		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.028797032326051693		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.02093980913383626 | validation: 0.01616529512134052]
	TIME [epoch: 8.24 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015931011003785096		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.018344146978727136		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.017137578991256112 | validation: 0.011392754103747398]
	TIME [epoch: 8.24 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017069184075154363		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.01879011534485564		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.017929649710005005 | validation: 0.0188104956245428]
	TIME [epoch: 8.23 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020810903067082047		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.012734285567778367		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.016772594317430205 | validation: 0.006733896476287743]
	TIME [epoch: 8.26 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012524047116513984		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.019603920221716233		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.016063983669115107 | validation: 0.01424552584656103]
	TIME [epoch: 8.23 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01504595842696795		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.019529923499538047		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.017287940963253 | validation: 0.015822997978526897]
	TIME [epoch: 8.24 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015646171871475177		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.013633921497459822		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.0146400466844675 | validation: 0.014543711490351854]
	TIME [epoch: 8.23 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013325319608804065		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.022122089128000077		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.017723704368402073 | validation: 0.011801026533572756]
	TIME [epoch: 8.26 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019864491399052026		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.014588415090569556		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.017226453244810792 | validation: 0.018854271834962185]
	TIME [epoch: 8.24 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016877801365693546		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.01878838439688787		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.017833092881290705 | validation: 0.015419934865056915]
	TIME [epoch: 8.24 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014733829840763577		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.01408126642742515		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.014407548134094362 | validation: 0.003194447164085047]
	TIME [epoch: 8.24 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.020613127510343412		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.0187168933765733		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.019665010443458357 | validation: 0.012419116770533128]
	TIME [epoch: 8.26 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016747595159016488		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.010776095938677514		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.013761845548847004 | validation: 0.0033361591262962365]
	TIME [epoch: 8.24 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01752462596231428		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.011775701333163089		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.014650163647738681 | validation: 0.008538629690182455]
	TIME [epoch: 8.23 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015346837978117653		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.01583650095494355		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.015591669466530598 | validation: 0.013310426507112517]
	TIME [epoch: 8.24 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021554811094897045		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.016977058950656373		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.019265935022776712 | validation: 0.015601396981770418]
	TIME [epoch: 8.25 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016855948482956608		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.013852205777560323		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.015354077130258468 | validation: 0.013748211329784214]
	TIME [epoch: 8.24 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018800508043188276		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.023771118562323522		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.0212858133027559 | validation: 0.01568543716315619]
	TIME [epoch: 8.24 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018939984226967743		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.011409526254718671		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.015174755240843205 | validation: 0.012392395939095632]
	TIME [epoch: 8.23 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017818466413281712		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.016077771127573023		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.01694811877042737 | validation: 0.004945206823138379]
	TIME [epoch: 8.26 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018158682195104133		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.02651834333215053		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.022338512763627333 | validation: 0.01597623634591165]
	TIME [epoch: 8.24 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015058423796783313		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.017084398630480127		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.016071411213631724 | validation: 0.006723131286817415]
	TIME [epoch: 8.23 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015323079269598494		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.0169746873725223		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.016148883321060396 | validation: 0.00837015337095971]
	TIME [epoch: 8.24 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01837024888832077		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.01760053310333668		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.017985390995828725 | validation: 0.012939181498891901]
	TIME [epoch: 8.26 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015235797560806081		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.019562168199080392		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.01739898287994324 | validation: 0.00989915534405686]
	TIME [epoch: 8.23 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013966998171186368		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.015121470589743568		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.014544234380464972 | validation: 0.01050491543037642]
	TIME [epoch: 8.23 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016714785536935436		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.02566249353840703		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.021188639537671233 | validation: 0.010918021126140019]
	TIME [epoch: 8.24 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010377919817927473		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.015157983677562579		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.012767951747745023 | validation: 0.010601939268053875]
	TIME [epoch: 8.25 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018101276564555478		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.013410977788725775		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.015756127176640627 | validation: 0.011791161294050887]
	TIME [epoch: 8.24 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014798827202239542		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.013723147995160756		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.014260987598700148 | validation: 0.008648489006371272]
	TIME [epoch: 8.23 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010170263834262899		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.019106349919395957		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.014638306876829424 | validation: 0.008208238423352714]
	TIME [epoch: 8.24 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.025180068136764867		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.014706144381218056		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.019943106258991462 | validation: 0.01304416017715259]
	TIME [epoch: 8.25 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021287325467284415		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.01868623711580367		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.01998678129154404 | validation: 0.012450153820714894]
	TIME [epoch: 8.24 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01694834826160816		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.014828930974449348		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.015888639618028753 | validation: 0.011743558065522923]
	TIME [epoch: 8.23 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01748324779162543		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.022661199689520264		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.020072223740572846 | validation: 0.010741796712963648]
	TIME [epoch: 8.23 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0165160969015268		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.016614603620810832		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.016565350261168812 | validation: 0.015850191140337203]
	TIME [epoch: 8.25 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0163457742344328		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.013430508757195415		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.01488814149581411 | validation: 0.005148170737484338]
	TIME [epoch: 8.23 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01795657048870236		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.010657188818973031		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.014306879653837697 | validation: 0.00885243935800376]
	TIME [epoch: 8.23 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015064617179131018		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.011280945738357554		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.013172781458744287 | validation: 0.01632893564451017]
	TIME [epoch: 8.23 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019598760186007334		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.014801029410119283		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.01719989479806331 | validation: 0.006500376494185335]
	TIME [epoch: 8.24 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015045535334803045		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.019542147471180556		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.017293841402991797 | validation: 0.011194602726786771]
	TIME [epoch: 8.23 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02365695596942689		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.009136474886149387		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.016396715427788137 | validation: 0.0066496040053502774]
	TIME [epoch: 8.23 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015409454562651137		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.01324721638896419		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.014328335475807663 | validation: 0.00684376329653427]
	TIME [epoch: 8.23 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015160320245620185		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.014627060740387096		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.014893690493003642 | validation: 0.002467137487536948]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1357.pth
	Model improved!!!
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017283658113114296		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.013192440808730613		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.015238049460922453 | validation: 0.011162517486196738]
	TIME [epoch: 8.24 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015611406469140438		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.015784376465510324		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.01569789146732538 | validation: 0.007487638990202137]
	TIME [epoch: 8.23 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01177106194894115		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.01571492929085024		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.013742995619895695 | validation: 0.013949017854117372]
	TIME [epoch: 8.23 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018304840727657305		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.012625383481694577		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.015465112104675946 | validation: 0.010360315423809894]
	TIME [epoch: 8.25 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01588760249075296		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.017184167016968132		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.016535884753860548 | validation: 0.008488132927922694]
	TIME [epoch: 8.23 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014334297198330476		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.014847189971708238		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.014590743585019356 | validation: 0.013284915526559677]
	TIME [epoch: 8.23 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015068455335918251		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.01244309734949666		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.013755776342707454 | validation: 0.009159630980865715]
	TIME [epoch: 8.24 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010165387643880483		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.01920337080643819		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.014684379225159338 | validation: 0.014177507956380002]
	TIME [epoch: 8.26 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010913420723239035		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.014258531628072191		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.012585976175655614 | validation: 0.009742938143080051]
	TIME [epoch: 8.24 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015032923884481356		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.011872346406483844		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.013452635145482597 | validation: 0.005810224215900411]
	TIME [epoch: 8.22 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01502643120498418		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.015667244887497385		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.015346838046240777 | validation: 0.01051741941041302]
	TIME [epoch: 8.22 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018253460147182977		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.01789768940836457		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.018075574777773776 | validation: 0.019396717408224098]
	TIME [epoch: 8.25 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02157577402085093		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.012829158161060356		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.017202466090955642 | validation: 0.010380104279798977]
	TIME [epoch: 8.23 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014881110675895301		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.007308922349523061		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.011095016512709182 | validation: 0.013144424474508023]
	TIME [epoch: 8.22 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017140391252002757		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.015134660838604621		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.01613752604530369 | validation: 0.012399806199608704]
	TIME [epoch: 8.22 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014710578880948632		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.015207663841116808		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.01495912136103272 | validation: 0.006255375716941941]
	TIME [epoch: 8.24 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010680905907259416		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.020406213529324067		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.01554355971829174 | validation: 0.011856723071813631]
	TIME [epoch: 8.23 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017705649429221572		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.014305418275138978		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.01600553385218027 | validation: 0.010634072508615412]
	TIME [epoch: 8.22 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017662020883361666		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.013005630465355699		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.015333825674358679 | validation: 0.011534473721788337]
	TIME [epoch: 8.21 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014706225479130131		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 0.020237973158275406		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.01747209931870277 | validation: 0.003287375574691497]
	TIME [epoch: 8.24 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01202396761101319		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.015454489647638894		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.013739228629326042 | validation: 0.012101991379492113]
	TIME [epoch: 8.22 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016100532191520023		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.014745763469012166		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.015423147830266095 | validation: 0.017739606943327327]
	TIME [epoch: 8.23 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014973806291917436		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.011474466582618567		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.013224136437268 | validation: 0.010971099168606356]
	TIME [epoch: 8.23 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013478334259026632		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.011203609032655961		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.012340971645841298 | validation: 0.012205455815277738]
	TIME [epoch: 8.25 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01338523333998794		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.01564861395794547		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.014516923648966706 | validation: 0.010198046452640691]
	TIME [epoch: 8.24 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015185810023173008		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.010322316184030489		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.012754063103601745 | validation: 0.009956458642206673]
	TIME [epoch: 8.22 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019364384778352263		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.009853888337067403		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.014609136557709835 | validation: 0.011234092681154448]
	TIME [epoch: 8.23 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01557073956044137		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.014454216328109504		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.015012477944275437 | validation: 0.008889916897125478]
	TIME [epoch: 8.23 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010996114606632674		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.018358661538236655		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.014677388072434665 | validation: 0.00924702444953538]
	TIME [epoch: 8.23 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011943554424841603		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.017119146344571776		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.014531350384706692 | validation: 0.006518855586591326]
	TIME [epoch: 8.22 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016303661345683618		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.01555303313140528		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.01592834723854445 | validation: 0.011280771150953547]
	TIME [epoch: 8.24 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014883217447425264		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.017177127268179063		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.016030172357802165 | validation: 0.010762574632624314]
	TIME [epoch: 8.22 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014961401118072726		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.010103446189434163		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.012532423653753445 | validation: 0.008272522258305076]
	TIME [epoch: 8.24 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016411880573501847		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.01077843204020731		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.013595156306854577 | validation: 0.007684527855863137]
	TIME [epoch: 8.21 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01333382009561706		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.010773456572449932		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.012053638334033497 | validation: 0.006660879202044518]
	TIME [epoch: 8.23 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018877394637534718		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.01909275028371705		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.018985072460625883 | validation: 0.00969565497758865]
	TIME [epoch: 8.23 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01834068061762751		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.012242300197841224		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.015291490407734365 | validation: 0.0074008323028353935]
	TIME [epoch: 8.24 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012979708355323246		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.015351336072207959		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.014165522213765602 | validation: 0.006920918283448376]
	TIME [epoch: 8.23 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010388307642732738		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.015250237520388102		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.012819272581560421 | validation: 0.01977598554594807]
	TIME [epoch: 8.22 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013459146229494437		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.013380315681722136		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.013419730955608286 | validation: 0.004413839075881162]
	TIME [epoch: 8.22 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014092250566532242		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 0.016355180036039557		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.015223715301285898 | validation: 0.008375981692358363]
	TIME [epoch: 8.23 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01656428977903805		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 0.014943117913850149		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 0.0157537038464441 | validation: 0.014154633624980744]
	TIME [epoch: 8.23 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010794774195279116		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 0.015252319266356226		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 0.013023546730817672 | validation: 0.005007143029554236]
	TIME [epoch: 8.22 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012156128902515415		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 0.016942920190823385		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 0.014549524546669395 | validation: 0.008516331963017962]
	TIME [epoch: 8.22 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018835063088081697		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 0.013970273590054833		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 0.016402668339068262 | validation: 0.010267706954996545]
	TIME [epoch: 8.24 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006358110720339816		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 0.014239334509865944		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 0.010298722615102878 | validation: 0.009290208803163459]
	TIME [epoch: 8.23 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01935438775546918		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 0.012983721220105512		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 0.016169054487787344 | validation: 0.00896384037606994]
	TIME [epoch: 8.22 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01306900135635344		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 0.016373620765684556		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 0.014721311061019001 | validation: 0.01254913479606382]
	TIME [epoch: 8.23 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00958880132503658		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 0.017381278861426472		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 0.013485040093231523 | validation: 0.006915785027624009]
	TIME [epoch: 8.25 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009867561592952385		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 0.018393483061931972		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 0.01413052232744218 | validation: 0.004781386029279676]
	TIME [epoch: 8.23 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014117221480833006		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 0.008216329912181203		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 0.011166775696507104 | validation: 0.00948528594971224]
	TIME [epoch: 8.24 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016958068486935284		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 0.015747248418632037		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 0.016352658452783657 | validation: 0.009077122168423388]
	TIME [epoch: 8.22 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015383509093553493		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 0.02031791275796855		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 0.01785071092576102 | validation: 0.012190996277960178]
	TIME [epoch: 8.27 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016369624389258215		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 0.014819153406924307		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 0.01559438889809126 | validation: 0.008049818322043156]
	TIME [epoch: 8.24 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012782396871956443		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 0.014568145406718408		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 0.013675271139337428 | validation: 0.006801396922266724]
	TIME [epoch: 8.21 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009763000844862849		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 0.017543798277668388		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 0.01365339956126562 | validation: 0.012754545143291558]
	TIME [epoch: 8.23 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014768683840672195		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 0.012043491655754083		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 0.013406087748213136 | validation: 0.01309452897492804]
	TIME [epoch: 8.25 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015905570850596084		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 0.01505318696121504		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 0.015479378905905557 | validation: 0.0077628509860045575]
	TIME [epoch: 8.21 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01675415070428886		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 0.015810997837878222		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 0.01628257427108354 | validation: 0.011595081103543542]
	TIME [epoch: 8.24 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01796229298454704		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 0.019654062794148754		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 0.018808177889347893 | validation: 0.011960975581709438]
	TIME [epoch: 8.24 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01579854970464325		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 0.019377926713660065		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 0.017588238209151653 | validation: 0.01607431185066133]
	TIME [epoch: 8.26 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010548620902642934		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 0.011967238433905698		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 0.011257929668274314 | validation: 0.002572007944348999]
	TIME [epoch: 8.22 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01031762672128024		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 0.019726078136735548		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 0.015021852429007892 | validation: 0.004129424037706835]
	TIME [epoch: 8.23 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016175217148367955		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.013849863406566406		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.015012540277467179 | validation: 0.011836809646057017]
	TIME [epoch: 8.23 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0196877890725933		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.011845018280386147		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.01576640367648972 | validation: 0.0046584990326708955]
	TIME [epoch: 8.24 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013081809078819243		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.012686159333368165		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.012883984206093704 | validation: 0.013202769210854705]
	TIME [epoch: 8.22 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008218578269073568		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.013560959166890124		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.010889768717981845 | validation: 0.002757106311617089]
	TIME [epoch: 8.22 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009706608922701348		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.016086745827033056		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.012896677374867199 | validation: 0.0007442710870699917]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1425.pth
	Model improved!!!
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01328521020871538		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.00864494171476995		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.010965075961742666 | validation: 0.009707837832947047]
	TIME [epoch: 8.28 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01387324992539255		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.01845185678204747		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.016162553353720015 | validation: 0.016822742339802098]
	TIME [epoch: 8.22 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01589523365131661		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.017357224644534753		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.01662622914792568 | validation: 0.00869296969743735]
	TIME [epoch: 8.23 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016472932357312967		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.008792618301682167		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.012632775329497567 | validation: 0.007493480406835744]
	TIME [epoch: 8.24 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01433915462169135		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.016325560011269457		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.015332357316480402 | validation: 0.0027522224820563314]
	TIME [epoch: 8.26 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012157626214763358		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.016865940017760577		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.014511783116261969 | validation: 0.00850224562622483]
	TIME [epoch: 8.24 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008852647366935968		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.013457840048543395		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.011155243707739682 | validation: 0.013142248828464775]
	TIME [epoch: 8.25 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010259335137620538		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.016697195451587633		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.013478265294604089 | validation: 0.002031974807472275]
	TIME [epoch: 8.24 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011054581205908898		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.014912662715650471		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.012983621960779684 | validation: 0.010531958550517577]
	TIME [epoch: 8.26 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021666105112611446		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.014917845573922627		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.01829197534326704 | validation: 0.00680965555564801]
	TIME [epoch: 8.25 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01435574977642802		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.014038780675748985		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.014197265226088501 | validation: 0.0014745257060310877]
	TIME [epoch: 8.23 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013997361477510123		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.017956631483333683		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.015976996480421897 | validation: 0.009080402665109677]
	TIME [epoch: 8.23 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021134531003146544		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.014634793573272992		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.01788466228820977 | validation: 0.003893808560548342]
	TIME [epoch: 8.23 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01660885519549579		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.015093978679457287		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.01585141693747654 | validation: 0.011488071677041743]
	TIME [epoch: 8.23 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018086354309002788		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.010739302096511602		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.014412828202757194 | validation: 0.010142328548075452]
	TIME [epoch: 8.23 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011075169720880311		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.015797652825227996		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.013436411273054154 | validation: 0.007592255377172755]
	TIME [epoch: 8.23 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013436021774209068		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.011741679616965059		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.012588850695587064 | validation: 0.007011030637466505]
	TIME [epoch: 8.25 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008617347952256346		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.022713003944776614		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.015665175948516477 | validation: 0.01250717792602938]
	TIME [epoch: 8.22 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014768122233448467		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.011055678013851308		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.012911900123649886 | validation: 0.006319679911168993]
	TIME [epoch: 8.23 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014403276282488702		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.013273500388604646		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.013838388335546677 | validation: 0.005461114722514513]
	TIME [epoch: 8.22 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009750811588147891		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.019000676830862966		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.014375744209505428 | validation: 0.009525329048388875]
	TIME [epoch: 8.24 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017162222594195132		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.011555026002528033		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.014358624298361582 | validation: 0.00537528479294926]
	TIME [epoch: 8.23 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017492052969999266		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.013481450703948716		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.015486751836973992 | validation: 0.004951158016468211]
	TIME [epoch: 8.23 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01708693741367403		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.010111445562508868		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.01359919148809145 | validation: 0.0025625995475704383]
	TIME [epoch: 8.22 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018853148670681545		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.01667803237427514		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.01776559052247834 | validation: 0.018353780350727173]
	TIME [epoch: 8.25 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016426241450701973		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 0.016476976629602756		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.016451609040152367 | validation: 0.01178636949648063]
	TIME [epoch: 8.25 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01602912974642991		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.021786640652356637		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.018907885199393272 | validation: 0.007579521572493012]
	TIME [epoch: 8.24 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015442963175539578		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.014795075685693954		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 0.015119019430616765 | validation: 0.003037984749859585]
	TIME [epoch: 8.23 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01364864692613126		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.013392074204619983		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.013520360565375625 | validation: 0.010692905693163123]
	TIME [epoch: 8.26 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016046913279926973		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.015097645309376264		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.015572279294651617 | validation: 0.008544565260947932]
	TIME [epoch: 8.23 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013754664790152703		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.01820302406942525		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.015978844429788978 | validation: 0.013540526483674938]
	TIME [epoch: 8.24 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02204033284063905		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.016987255641875115		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.01951379424125708 | validation: 0.010355815335116082]
	TIME [epoch: 8.24 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01677815980935311		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.014082783850440792		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.015430471829896953 | validation: 0.00892211408331772]
	TIME [epoch: 8.26 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013468604007390042		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.012644279082748774		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.013056441545069408 | validation: 0.009750118059220008]
	TIME [epoch: 8.23 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014661652002572		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.013927976744683082		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.014294814373627545 | validation: 0.014133812629186596]
	TIME [epoch: 8.23 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017609426037471034		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.018597570410298928		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.018103498223884983 | validation: 0.0038838900056329595]
	TIME [epoch: 8.22 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008170122760986356		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.011021208545095102		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.009595665653040727 | validation: 0.006132771300406035]
	TIME [epoch: 8.25 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014979363573931915		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.011134525801733453		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.013056944687832684 | validation: 0.005410067527471942]
	TIME [epoch: 8.24 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021471794479329932		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.010231847754490353		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.015851821116910143 | validation: 0.0074835285672430335]
	TIME [epoch: 8.24 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010091859216309175		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.016674140722959717		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.013382999969634445 | validation: 0.007103148322791083]
	TIME [epoch: 8.24 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01196740729706455		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.01566219826250155		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.01381480277978305 | validation: 0.006898027819194216]
	TIME [epoch: 8.24 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01639470793296103		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.01502845539757858		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.015711581665269807 | validation: 0.012122628730342231]
	TIME [epoch: 8.25 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01254810224537067		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.012513258694790938		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.012530680470080805 | validation: 0.011607429551870014]
	TIME [epoch: 8.23 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012144599900109604		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.012335021292596721		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.012239810596353162 | validation: 0.011793586598946965]
	TIME [epoch: 8.23 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016332719167238746		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.013214450492461744		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.014773584829850245 | validation: 0.012193375286670197]
	TIME [epoch: 8.24 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011616867302006064		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.013483666868042875		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.012550267085024469 | validation: 0.011335804627988047]
	TIME [epoch: 8.23 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011520926263757036		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.01396509847672248		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.012743012370239757 | validation: 0.009300236001716951]
	TIME [epoch: 8.23 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01372513191737022		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.013592306412794486		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.01365871916508235 | validation: 0.007343742238136421]
	TIME [epoch: 8.23 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017013522045083263		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.014880750438871156		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.01594713624197721 | validation: 0.010119080066171075]
	TIME [epoch: 8.25 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008864216324796297		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.012046801129897672		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.010455508727346983 | validation: 0.005435610811604613]
	TIME [epoch: 8.24 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010092120205436842		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.015209073796975534		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.012650597001206187 | validation: 0.012114283940567896]
	TIME [epoch: 8.24 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.021556717831608705		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.009546108840355293		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.015551413335981997 | validation: 0.011371255865368564]
	TIME [epoch: 8.23 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016537256728602436		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.015522454131965577		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.016029855430284005 | validation: 0.0032665040866430035]
	TIME [epoch: 8.26 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010568658792119328		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.01293888319799362		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.011753770995056475 | validation: 0.011354647890429096]
	TIME [epoch: 8.21 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017267897839987674		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.009480948913917783		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.013374423376952733 | validation: 0.006942379171620536]
	TIME [epoch: 8.24 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010533533331676129		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.013269988067222188		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.011901760699449159 | validation: 0.00840573522933592]
	TIME [epoch: 8.22 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011016145789539208		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.01285511596405294		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.011935630876796076 | validation: 0.009141002382008004]
	TIME [epoch: 8.25 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016102581179543378		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.010882813646047252		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.013492697412795315 | validation: 0.004528777760007767]
	TIME [epoch: 8.24 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014898261869476731		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.010899051844478274		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.012898656856977506 | validation: 0.006842855245738944]
	TIME [epoch: 8.22 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011743270188220297		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.010619379855614793		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.011181325021917547 | validation: 0.009897783452746587]
	TIME [epoch: 8.22 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011598463417942486		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.010137621861904538		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.010868042639923511 | validation: 0.008139027897507206]
	TIME [epoch: 8.25 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013379165431501066		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.01394639605939094		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.013662780745446005 | validation: 0.014827367143157015]
	TIME [epoch: 8.23 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013472895865669101		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.01227906382285289		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.012875979844260995 | validation: -0.0014388079456256395]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1488.pth
	Model improved!!!
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015982020069958334		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.011062174366769808		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.01352209721836407 | validation: 0.0099433490381287]
	TIME [epoch: 8.25 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013774621788698893		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.014445190069844082		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.014109905929271489 | validation: 0.009919802084453332]
	TIME [epoch: 8.24 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013188389220627589		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.01330662487620311		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.01324750704841535 | validation: 0.011778672997585702]
	TIME [epoch: 8.23 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017253686419393278		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.010828750389005128		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.0140412184041992 | validation: 0.004820686709392564]
	TIME [epoch: 8.22 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010926363970532467		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.013822052211710118		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.012374208091121292 | validation: 0.006197063119710272]
	TIME [epoch: 8.23 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010502187332623827		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.02032848105509207		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.015415334193857946 | validation: 0.012167136160301023]
	TIME [epoch: 8.24 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008068773296215253		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.012416856761832209		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.01024281502902373 | validation: 0.0020447381873408806]
	TIME [epoch: 8.23 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011759768290146595		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.01256088284558309		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.012160325567864845 | validation: 0.004241387091565338]
	TIME [epoch: 8.23 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009301105656601532		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.012642080747030379		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.010971593201815958 | validation: 0.007612699641090841]
	TIME [epoch: 8.22 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009927741545547014		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.010133468434491647		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.010030604990019329 | validation: 0.010751760022416346]
	TIME [epoch: 8.24 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009589951424533633		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.009509940654090545		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.009549946039312088 | validation: 0.012986743230779712]
	TIME [epoch: 8.22 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013921137265643932		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.015178332738296496		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.014549735001970215 | validation: 0.012136283934675522]
	TIME [epoch: 8.23 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014412116751458292		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.01947589463909087		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.016944005695274583 | validation: 0.009024363984703652]
	TIME [epoch: 8.22 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015528634813321961		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.01451377614236809		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.015021205477845027 | validation: 0.005131874482391492]
	TIME [epoch: 8.25 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015565597960437267		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.01039103668988944		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.012978317325163354 | validation: 0.012864338691977863]
	TIME [epoch: 8.23 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014696067662376602		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.010505139680031252		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.012600603671203928 | validation: 0.014145268347646116]
	TIME [epoch: 8.23 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017692254444616307		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.014538707410907353		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.01611548092776183 | validation: 0.011879621521338236]
	TIME [epoch: 8.23 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014251213011718004		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.012778874574620281		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.013515043793169142 | validation: 0.011523518280955495]
	TIME [epoch: 8.25 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009985695220992113		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.014726551196616786		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.01235612320880445 | validation: 0.008793169433315573]
	TIME [epoch: 8.23 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018952536009505326		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.014734398017661757		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.016843467013583545 | validation: 0.009130769901061247]
	TIME [epoch: 8.23 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015151085165266063		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.018085324493308665		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.016618204829287365 | validation: 0.015043408293699052]
	TIME [epoch: 8.22 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015406442250214184		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.0084654371627539		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.011935939706484041 | validation: 0.009861135684664523]
	TIME [epoch: 8.25 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010870230452849329		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.017202071967269535		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.014036151210059435 | validation: 0.013738499393515473]
	TIME [epoch: 8.24 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0162227206861462		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.01195943131508371		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.014091076000614952 | validation: 0.00569587805162024]
	TIME [epoch: 8.23 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00997949604508715		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.010614051036588978		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.010296773540838064 | validation: 0.011731216509522436]
	TIME [epoch: 8.22 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019003792059402815		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.01545621631749288		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.017230004188447848 | validation: 0.006525205522833521]
	TIME [epoch: 8.25 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01762000316996175		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.008663885553737056		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.0131419443618494 | validation: 0.012303602052357662]
	TIME [epoch: 8.23 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010964656432263692		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.010329043648970766		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.010646850040617228 | validation: 0.01643176469304388]
	TIME [epoch: 8.22 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014165045213698069		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.009764694097712119		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.011964869655705092 | validation: 0.005995838830003887]
	TIME [epoch: 8.22 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01787474683782022		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.011525041673409526		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.014699894255614871 | validation: 0.011252615657729548]
	TIME [epoch: 8.25 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009420473146928664		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.014135111703788627		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.011777792425358645 | validation: 0.009304239027766023]
	TIME [epoch: 8.23 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014464897662465222		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.010889225837960786		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.012677061750213006 | validation: 0.005651297140742701]
	TIME [epoch: 8.23 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0059796615565188		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.014077559290365142		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.01002861042344197 | validation: 0.004360030417894351]
	TIME [epoch: 8.22 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009804372733692961		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.020297691807708497		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.015051032270700728 | validation: 0.011004947985100032]
	TIME [epoch: 8.25 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014374542356368467		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.010938372153639358		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.012656457255003913 | validation: 0.013809450960803438]
	TIME [epoch: 8.23 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00920882078251376		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.016082652145389204		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.012645736463951483 | validation: 0.016084119650014758]
	TIME [epoch: 8.23 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015656206534282846		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 0.016898587723395392		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.01627739712883912 | validation: 0.010391441533233244]
	TIME [epoch: 8.23 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011280245883277714		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.01942660531883545		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.01535342560105658 | validation: 0.008039562344122848]
	TIME [epoch: 8.25 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012261028213475699		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 0.013583970260562466		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.012922499237019084 | validation: 0.009700624831164807]
	TIME [epoch: 8.23 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010194638253101403		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.013141213446963571		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.011667925850032485 | validation: 0.008065008094835504]
	TIME [epoch: 8.22 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012006167961900719		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 0.015640957361466173		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.013823562661683445 | validation: 0.010287981978041006]
	TIME [epoch: 8.23 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012203990501782468		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.01121446363671772		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.011709227069250094 | validation: 0.0049701259152957625]
	TIME [epoch: 8.25 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014482962456637332		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.0077669263577640575		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.011124944407200693 | validation: 0.007464177728858547]
	TIME [epoch: 8.23 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01313509075857094		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.012307897458927885		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.012721494108749416 | validation: 0.006577899784727855]
	TIME [epoch: 8.23 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011854607166294249		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.011723934141152598		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.011789270653723422 | validation: 0.008872647567898736]
	TIME [epoch: 8.22 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015275011087141317		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.01592235950679651		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.015598685296968912 | validation: 0.014643520155340803]
	TIME [epoch: 8.25 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01585431321930951		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.016563231570793563		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.016208772395051536 | validation: 0.013640487451555829]
	TIME [epoch: 8.23 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012686674950042373		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.012053185406328572		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.012369930178185478 | validation: 0.010481883964302684]
	TIME [epoch: 8.23 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00925668146412379		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.014957790049907244		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.012107235757015517 | validation: 0.01464271789533871]
	TIME [epoch: 8.22 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012719135590062752		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.009894186700889838		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.011306661145476297 | validation: 0.010222682190470995]
	TIME [epoch: 8.24 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01968718722612095		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.014422221629265888		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.017054704427693412 | validation: 0.010625032217377473]
	TIME [epoch: 8.24 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013800048446128001		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.012112268907350605		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.012956158676739302 | validation: 0.0077269377358900665]
	TIME [epoch: 8.23 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014690084987615853		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.013105304680742735		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.013897694834179297 | validation: 0.01121487906529488]
	TIME [epoch: 8.23 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015291627180821776		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.013205207471710945		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.01424841732626636 | validation: 0.011968992562336274]
	TIME [epoch: 8.24 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011190497757902535		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.014735687420446137		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.012963092589174335 | validation: 0.011001280003352147]
	TIME [epoch: 8.24 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009695881120851724		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.015903437253077347		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.012799659186964536 | validation: 0.008562177527914615]
	TIME [epoch: 8.23 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01307027521131555		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.012638087537751131		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.012854181374533338 | validation: 0.007340009214401981]
	TIME [epoch: 8.23 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017119530986020766		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.013728874734336085		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.015424202860178426 | validation: 0.013523615948742034]
	TIME [epoch: 8.25 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012377500027341382		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.01359967982651982		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.012988589926930602 | validation: 0.006940500020540314]
	TIME [epoch: 8.24 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012150478163980094		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 0.01368043698973483		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.012915457576857461 | validation: 0.008573114890876749]
	TIME [epoch: 8.23 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011629474916825044		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.014198114840300358		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.012913794878562701 | validation: 0.009549124244070475]
	TIME [epoch: 8.23 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017659181409354675		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.01526592924017097		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.016462555324762825 | validation: 0.012798912290555893]
	TIME [epoch: 8.25 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013329664556374705		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.016706909009285772		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.015018286782830237 | validation: 0.006777079981852603]
	TIME [epoch: 8.24 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011451891250916838		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.012854145208667423		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.012153018229792128 | validation: 0.005151497144712705]
	TIME [epoch: 8.23 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011356358437393231		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.010611857747985053		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.01098410809268914 | validation: 0.007685385011085201]
	TIME [epoch: 8.22 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01784820883824034		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.014495676984655027		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.016171942911447683 | validation: 0.01044064245922609]
	TIME [epoch: 8.24 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013931477385421423		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.01545996513002286		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.01469572125772214 | validation: 0.007869272143495082]
	TIME [epoch: 8.24 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007196482271293492		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.019900011824672586		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.013548247047983039 | validation: 0.016856056821787733]
	TIME [epoch: 8.23 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015624711014537999		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.01468290270360994		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.015153806859073967 | validation: 0.008538920540204168]
	TIME [epoch: 8.22 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01292115982288949		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.013299296449869533		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.013110228136379512 | validation: 0.01390757533046839]
	TIME [epoch: 8.24 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009627414111691589		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.015656119656454533		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.01264176688407306 | validation: 0.011507241107558054]
	TIME [epoch: 8.25 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010817972055751777		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.01776259011572328		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.01429028108573753 | validation: 0.014913683452546489]
	TIME [epoch: 8.23 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013843292771974453		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.011657113887975619		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.012750203329975038 | validation: 0.009086504316913716]
	TIME [epoch: 8.24 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013484255553236232		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.013504351976075352		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.013494303764655793 | validation: 0.011235792428766078]
	TIME [epoch: 8.26 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015724047207315427		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.012605934107631878		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.014164990657473656 | validation: 0.00739806822229971]
	TIME [epoch: 8.27 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01369392996396047		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.014463329376045697		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.014078629670003085 | validation: 0.011652395345281251]
	TIME [epoch: 8.26 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01560890894480404		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.015943535216021182		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.01577622208041261 | validation: 0.008528191355777016]
	TIME [epoch: 8.25 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0105693223978234		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 0.018466275599734232		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.014517798998778817 | validation: 0.013657006210117428]
	TIME [epoch: 8.26 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01620269321774318		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.016444085533275424		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.016323389375509305 | validation: 0.012422127509227023]
	TIME [epoch: 8.27 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013852455978475597		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.011163151813042095		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.012507803895758845 | validation: 0.011898284021617335]
	TIME [epoch: 8.26 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013507927369758999		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 0.012486703581514061		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.012997315475636534 | validation: 0.011168097360257787]
	TIME [epoch: 8.25 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014352552818673375		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.013994487216293792		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.014173520017483584 | validation: 0.010015964690707547]
	TIME [epoch: 8.25 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015628104824610533		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.01130167372171324		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.01346488927316189 | validation: 0.0043605245861355404]
	TIME [epoch: 8.26 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007090099933228677		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 0.013512706071708564		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.01030140300246862 | validation: 0.006419467104507246]
	TIME [epoch: 8.25 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014970539317424017		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.012545442855565674		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.013757991086494841 | validation: 0.010382549667343574]
	TIME [epoch: 8.25 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013697832692687434		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.016027464459358362		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.014862648576022897 | validation: 0.009633524066918617]
	TIME [epoch: 8.25 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013257260938272925		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.01895166529639492		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.016104463117333927 | validation: 0.008490905883111336]
	TIME [epoch: 8.27 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011758539532895558		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.016657533441419018		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.014208036487157289 | validation: 0.014735005175535467]
	TIME [epoch: 8.26 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010816082172573672		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.012050685444727795		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.011433383808650733 | validation: 0.00938220010824499]
	TIME [epoch: 8.24 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016809668838875816		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.010568790930121525		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.013689229884498671 | validation: 0.01573134134953755]
	TIME [epoch: 8.25 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011395099495825543		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.012197024353805404		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.011796061924815476 | validation: 0.011847948651187322]
	TIME [epoch: 8.26 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009460321717354904		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.01390812544789933		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.011684223582627117 | validation: 0.010291818621979774]
	TIME [epoch: 8.25 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01317261844869789		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 0.00746335577847055		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 0.010317987113584219 | validation: 0.012128266400309465]
	TIME [epoch: 8.24 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012039913073547473		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.01648719025457277		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.014263551664060122 | validation: 0.009551500322412244]
	TIME [epoch: 8.24 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010104014642204725		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.014154287624743578		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.012129151133474151 | validation: 0.006246237215387709]
	TIME [epoch: 8.26 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01306233323350794		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.012346183222756966		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.01270425822813245 | validation: 0.0076334425851110966]
	TIME [epoch: 8.24 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010899018658171212		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.008492932393433855		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.009695975525802532 | validation: 0.006763278995788796]
	TIME [epoch: 8.25 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007331767153966143		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 0.01184111839113886		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.009586442772552503 | validation: 0.010747331079710628]
	TIME [epoch: 8.25 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009259650245515037		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.010683762888539977		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.009971706567027505 | validation: 0.007652398354021603]
	TIME [epoch: 8.27 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009710631278401735		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.012294337755506254		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.011002484516953995 | validation: 0.011578790201588406]
	TIME [epoch: 8.24 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012266556520249092		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.013505508423084792		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.012886032471666942 | validation: 0.006348665430477791]
	TIME [epoch: 8.23 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008783696545187337		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.015073583139349122		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.011928639842268229 | validation: 0.008160006403212865]
	TIME [epoch: 8.26 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010144136936884377		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.01458209612867635		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.012363116532780365 | validation: 0.007338823417683483]
	TIME [epoch: 8.26 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010788810262456948		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.015665441871667005		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.013227126067061978 | validation: 0.00935744558765809]
	TIME [epoch: 8.25 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012503646891825228		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.01281481954220936		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.012659233217017295 | validation: 0.007233813989942677]
	TIME [epoch: 8.24 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009441294626296021		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.011747151067355398		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.01059422284682571 | validation: 0.005405482084900167]
	TIME [epoch: 8.24 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014470128488498962		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.008671788953802804		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.011570958721150883 | validation: 0.004889195736114189]
	TIME [epoch: 8.26 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0149795037747598		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 0.014774376611908762		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.014876940193334284 | validation: 0.006431143385656483]
	TIME [epoch: 8.23 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011325862893868693		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.015906632303194546		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.01361624759853162 | validation: 0.011170343639744203]
	TIME [epoch: 8.26 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014242379907818717		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.010188329010453034		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.012215354459135876 | validation: 0.007431707002329671]
	TIME [epoch: 8.25 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009763076152916831		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.014388309567931598		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.012075692860424213 | validation: 0.0069273841973630235]
	TIME [epoch: 8.26 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014151932567741343		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.006111968511978719		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.010131950539860032 | validation: 0.002948042389602333]
	TIME [epoch: 8.24 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013748411648385462		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.011748617155325243		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.012748514401855352 | validation: 0.003186096217957142]
	TIME [epoch: 8.24 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012241807893169457		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.012562274098388077		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.012402040995778768 | validation: 0.01064730469599616]
	TIME [epoch: 8.25 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016505578495648508		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.009666638604663665		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.01308610855015609 | validation: 0.011220550817554768]
	TIME [epoch: 8.27 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008221327774584305		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 0.01763114052590346		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.012926234150243885 | validation: 0.005618232115099502]
	TIME [epoch: 8.24 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010745264134970177		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.016072534131281277		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.013408899133125726 | validation: 0.00421154987014982]
	TIME [epoch: 8.25 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0120627689527969		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 0.013216814016454406		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.012639791484625651 | validation: 0.009671085573105077]
	TIME [epoch: 8.24 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015032285001967668		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.008437009683579117		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.011734647342773394 | validation: -8.706078921877618e-05]
	TIME [epoch: 8.26 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012465100513368762		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.01974320653019388		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.016104153521781323 | validation: 0.008367407480065928]
	TIME [epoch: 8.22 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013818933053778699		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.010609967208659404		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.012214450131219051 | validation: 0.007343387680670794]
	TIME [epoch: 8.24 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012133460682530586		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.010698628024279257		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.01141604435340492 | validation: 0.005106896331900788]
	TIME [epoch: 8.25 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01199785512565033		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.013748403678221627		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.012873129401935978 | validation: 0.009249593348206953]
	TIME [epoch: 8.26 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007230284931183252		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 0.016521965747335474		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.011876125339259361 | validation: 0.004953413554352073]
	TIME [epoch: 8.24 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011134088020659611		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.015006086628330325		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.01307008732449497 | validation: 0.005864836076188816]
	TIME [epoch: 8.24 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010017363394256785		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.013139774420267087		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.011578568907261938 | validation: 0.0035827823950369494]
	TIME [epoch: 8.24 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011876039133530926		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 0.01618086938557078		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.014028454259550854 | validation: 0.00845611692687526]
	TIME [epoch: 8.26 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00808808475886608		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.014684227601526623		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.011386156180196352 | validation: 0.007254962260367065]
	TIME [epoch: 8.25 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013782047278513117		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.013303106837966806		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.013542577058239963 | validation: 0.009856413419988483]
	TIME [epoch: 8.25 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013145531940702993		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.009746542331026214		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.0114460371358646 | validation: 0.005732106964809814]
	TIME [epoch: 8.25 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012864486141047623		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 0.01078067244139859		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.011822579291223106 | validation: 0.007547538951785258]
	TIME [epoch: 8.27 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012703210122594472		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.012118191784083389		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.012410700953338929 | validation: 0.015250678349884783]
	TIME [epoch: 8.25 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01065693745845878		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.01488624659940209		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.012771592028930436 | validation: 0.0075738109330002485]
	TIME [epoch: 8.25 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012787306861578977		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.010622144354379762		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.011704725607979369 | validation: 0.00871979961602255]
	TIME [epoch: 8.24 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01155706286835606		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.011932064346909882		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.01174456360763297 | validation: 0.007925863102622464]
	TIME [epoch: 8.25 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016398247012489004		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.008615817800202132		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.012507032406345566 | validation: 0.0052653831927922]
	TIME [epoch: 8.24 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004556290659577773		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.011531961535899617		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.008044126097738694 | validation: 0.004927742310274452]
	TIME [epoch: 8.23 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011538027607438162		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.012128873832212412		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.011833450719825287 | validation: 0.004114128439833304]
	TIME [epoch: 8.23 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014954938514091682		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.006230886442751767		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.010592912478421725 | validation: 0.00997065436959376]
	TIME [epoch: 8.27 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010354119168429045		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.015682142442732153		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.0130181308055806 | validation: 0.007205604992013217]
	TIME [epoch: 8.23 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01348838444500823		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 0.011471645771183658		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.012480015108095946 | validation: 0.008864065349902675]
	TIME [epoch: 8.23 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014596410906249446		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.014829501447110028		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.01471295617667974 | validation: 0.01024564541950691]
	TIME [epoch: 8.24 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012026971053754502		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.00990430592005107		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.010965638486902785 | validation: 0.004909485241184827]
	TIME [epoch: 8.25 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014623323491150869		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.00761943758317637		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.011121380537163616 | validation: 0.007036035936285781]
	TIME [epoch: 8.24 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013713013529131951		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.011335641834332767		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.012524327681732362 | validation: 0.012922374636408124]
	TIME [epoch: 8.24 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01612612535655475		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.008408462169003613		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.01226729376277918 | validation: 0.005178236636558631]
	TIME [epoch: 8.23 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0161080363978519		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 0.013011383347092929		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.014559709872472414 | validation: 0.007787166841934586]
	TIME [epoch: 8.26 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009166546980526066		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.015329608777513626		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.012248077879019844 | validation: 0.01132035060847454]
	TIME [epoch: 8.24 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009799714765101625		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.010689118178363013		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.010244416471732319 | validation: 0.004306704362762212]
	TIME [epoch: 8.24 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008827199294471743		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.017098426615432517		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.01296281295495213 | validation: 0.009674344291460746]
	TIME [epoch: 8.25 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014142902331813242		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.01270092835733661		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.013421915344574927 | validation: 0.008214301115646142]
	TIME [epoch: 8.28 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012625914285647485		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.013935326603713683		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.013280620444680586 | validation: 0.010681479955099842]
	TIME [epoch: 8.3 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014950823740523287		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.008424160103106208		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.011687491921814749 | validation: 0.00727892747180859]
	TIME [epoch: 8.29 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012420925287585583		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.01628106505547546		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.014350995171530522 | validation: 0.009901265647369574]
	TIME [epoch: 8.29 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010573643726730591		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.016655218694368222		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.013614431210549404 | validation: 0.012262944316247075]
	TIME [epoch: 8.3 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011983203570549578		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.009788356206949988		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.010885779888749782 | validation: 0.012488600200079053]
	TIME [epoch: 8.28 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013269942995457348		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.0154415631046536		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.014355753050055476 | validation: 0.006203515526922003]
	TIME [epoch: 8.29 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012129147143826865		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.008502244397211148		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.010315695770519006 | validation: 0.010742006668187183]
	TIME [epoch: 8.3 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01743790540788461		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.014606257230616204		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.016022081319250408 | validation: 0.012441295613990858]
	TIME [epoch: 8.3 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01186913982435643		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.013982573636505324		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.012925856730430877 | validation: 0.01146708855645521]
	TIME [epoch: 8.29 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01353935698016944		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.01408115574366862		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.01381025636191903 | validation: 0.011420865086129819]
	TIME [epoch: 8.29 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013822187012154319		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.013353652188746304		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.013587919600450313 | validation: 0.004360958772052536]
	TIME [epoch: 8.28 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011622563956138704		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.010741249861891945		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.011181906909015327 | validation: 0.011339968942425635]
	TIME [epoch: 8.3 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0111998552609517		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.010720790300742414		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.010960322780847055 | validation: 0.009877755173160627]
	TIME [epoch: 8.29 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008951971941745714		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.01672339730554337		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.01283768462364454 | validation: 0.00658526055207683]
	TIME [epoch: 8.29 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008737114578628581		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.016962667896648072		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.012849891237638328 | validation: 0.004157321352318466]
	TIME [epoch: 8.29 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.02200395629858884		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.010116491840223486		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.016060224069406166 | validation: 0.01148370580923532]
	TIME [epoch: 8.32 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013004992400564802		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.016239899703396295		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.014622446051980547 | validation: 0.005569835399442248]
	TIME [epoch: 8.28 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01330801695625571		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 0.00802287703333835		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 0.010665446994797028 | validation: 0.008724828185562455]
	TIME [epoch: 8.29 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006517134797020982		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.014993940489005902		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.010755537643013442 | validation: 0.003216790564703742]
	TIME [epoch: 8.3 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01125644373544667		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 0.011063144837110982		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.011159794286278826 | validation: 0.010166560305452606]
	TIME [epoch: 8.32 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010033827214378902		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.012632220827153767		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.011333024020766334 | validation: 0.005545946804554781]
	TIME [epoch: 8.3 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012273470972040738		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.014684496780096945		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.013478983876068839 | validation: 0.005116952050165871]
	TIME [epoch: 8.3 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008606188513631033		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 0.009524119324622543		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 0.009065153919126788 | validation: 0.008909166935177598]
	TIME [epoch: 8.29 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010202776941894622		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.012573325776036686		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.011388051358965657 | validation: 0.013106805261351422]
	TIME [epoch: 8.32 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012680613680630703		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.011533191348835877		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.01210690251473329 | validation: 0.008267325816519044]
	TIME [epoch: 8.3 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012876121221999725		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.009559751772514804		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.011217936497257264 | validation: 0.009793828016606972]
	TIME [epoch: 8.27 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011412559412643302		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.010907685076227507		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.011160122244435406 | validation: 0.012141156812406344]
	TIME [epoch: 8.3 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012058554625829354		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.01407610717683209		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.01306733090133072 | validation: 0.007581243007040401]
	TIME [epoch: 8.31 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014824694008022879		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.009528181225351465		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.012176437616687172 | validation: 0.0037520547628035177]
	TIME [epoch: 8.27 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014350295992459827		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.013025538298576536		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.01368791714551818 | validation: 0.011395778633381353]
	TIME [epoch: 8.27 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013037753296675317		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.012279035212047614		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.012658394254361466 | validation: 0.013617529975093726]
	TIME [epoch: 8.3 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007285436453556017		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.01270920812460744		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.00999732228908173 | validation: 0.006120106823962194]
	TIME [epoch: 8.32 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010552317926703034		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.008568134559741373		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.009560226243222203 | validation: 0.0075317303338891155]
	TIME [epoch: 8.31 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008659024286120268		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 0.012353751730648996		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.01050638800838463 | validation: 0.010912226780186291]
	TIME [epoch: 8.3 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013086931380945627		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.017001064074046432		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.015043997727496029 | validation: 0.010696138522816343]
	TIME [epoch: 8.29 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01253159538306425		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.013418134731940307		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.012974865057502278 | validation: 0.008026452692805428]
	TIME [epoch: 8.31 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00940388341962546		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.012815952935447964		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.01110991817753671 | validation: 0.008367466213566075]
	TIME [epoch: 8.28 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013404359076227657		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.009901379014611533		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.011652869045419593 | validation: 0.005247133716031105]
	TIME [epoch: 8.28 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011004118878833196		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.012443726828119363		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.01172392285347628 | validation: 0.009969968254321886]
	TIME [epoch: 8.3 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010831088237119951		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.014398488559996902		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.012614788398558428 | validation: 0.0011975256720773487]
	TIME [epoch: 8.29 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012565684905261665		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.012888192410210973		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.01272693865773632 | validation: 0.0035229664843370464]
	TIME [epoch: 8.3 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012580219200113174		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.011172636542810276		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.011876427871461724 | validation: 0.005493972386042507]
	TIME [epoch: 8.28 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011454758853753317		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 0.012924487029985544		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.01218962294186943 | validation: 0.007521640234411436]
	TIME [epoch: 8.28 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008157280710451124		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.01776758996002004		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.012962435335235584 | validation: 0.0070722183973183405]
	TIME [epoch: 8.3 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008578593860230114		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.01273557836283501		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.01065708611153256 | validation: 0.012897472261672229]
	TIME [epoch: 8.27 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015453454406225526		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.006243540702072736		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.010848497554149131 | validation: 0.0033267871332880016]
	TIME [epoch: 8.27 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010780159690683381		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 0.01319271034126632		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.01198643501597485 | validation: 0.008323814589866215]
	TIME [epoch: 8.26 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012928329693173424		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.012519474224727581		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.012723901958950502 | validation: 0.0034613988703294943]
	TIME [epoch: 8.29 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013700396465413631		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.009903245711717306		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.011801821088565468 | validation: 0.010861688917215034]
	TIME [epoch: 8.27 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010398736819929387		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 0.013222419602823124		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.011810578211376255 | validation: 0.006638399746995211]
	TIME [epoch: 8.27 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007608667651282819		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.011701005521429966		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.009654836586356392 | validation: 0.005881028528198881]
	TIME [epoch: 8.28 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010020374922793245		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.00994567387566178		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.00998302439922751 | validation: 0.0043556793858615436]
	TIME [epoch: 8.29 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014214229795692793		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.014083067192433876		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.014148648494063332 | validation: 0.012960915701000534]
	TIME [epoch: 8.28 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014131663529481803		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.010768941389074362		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.012450302459278088 | validation: 0.004262817732420264]
	TIME [epoch: 8.27 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01153228893294707		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.009456123183498647		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.010494206058222861 | validation: 0.006595835510685992]
	TIME [epoch: 8.28 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010878246581804241		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.014565145715961663		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.012721696148882952 | validation: 0.00919305717810416]
	TIME [epoch: 8.29 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013041754335189726		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.01182192200741568		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.012431838171302704 | validation: 0.009732317430184816]
	TIME [epoch: 8.28 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014996171976519595		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.01075917689208654		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.012877674434303068 | validation: 0.004126022441406403]
	TIME [epoch: 8.28 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01303052928190345		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 0.009422987905632276		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.011226758593767863 | validation: 0.001631936379394774]
	TIME [epoch: 8.29 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009535852618311488		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.01608092937951373		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.012808390998912608 | validation: 0.006553302217289493]
	TIME [epoch: 8.31 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.003685870289238921		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.013755976799909342		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.008720923544574133 | validation: 0.006288455983156461]
	TIME [epoch: 8.28 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012612851868236536		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.008735106420211431		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.01067397914422398 | validation: 0.00696345938465816]
	TIME [epoch: 8.29 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011539213508021964		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.010914230057778414		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.011226721782900191 | validation: 0.007563220640133278]
	TIME [epoch: 8.29 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017909924974379147		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.010476141812054477		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.014193033393216809 | validation: 0.009413277849246612]
	TIME [epoch: 8.3 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01214210874545114		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.007840722811895334		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.009991415778673237 | validation: 0.0027596215360309964]
	TIME [epoch: 8.27 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008150764749142706		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.012612426778769673		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.010381595763956189 | validation: 0.0040214640935303265]
	TIME [epoch: 8.28 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012127374092688568		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.010297764441586817		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.01121256926713769 | validation: 0.00848125191247514]
	TIME [epoch: 8.29 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009382013687374116		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.01432640611251105		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.011854209899942585 | validation: 0.013683621090586445]
	TIME [epoch: 8.3 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012134389023397394		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.009972483213163323		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.011053436118280358 | validation: 0.009163381774418581]
	TIME [epoch: 8.28 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009745923942731652		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.014293851974370903		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.012019887958551278 | validation: 0.01029624943735171]
	TIME [epoch: 8.28 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013514285268535556		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.01220793310809829		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.012861109188316922 | validation: 0.012480236523424343]
	TIME [epoch: 8.28 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014249291722717643		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.01055634823177417		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.012402819977245908 | validation: 0.006835412283911382]
	TIME [epoch: 8.29 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009912575223005603		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.014062188290467775		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.01198738175673669 | validation: 0.002793420472079879]
	TIME [epoch: 8.28 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01355590094622947		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.01176200328417669		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.01265895211520308 | validation: 0.005988068044592812]
	TIME [epoch: 8.29 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013057522446080725		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.012614458631440121		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.012835990538760422 | validation: 0.005240419415588336]
	TIME [epoch: 8.28 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008558795082705135		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.015612790520860762		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.012085792801782947 | validation: 0.008178023688699401]
	TIME [epoch: 8.3 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019551999948076035		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.011356894665234813		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.015454447306655423 | validation: 0.010646283310273378]
	TIME [epoch: 8.29 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011785212156321398		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 0.011045999184768484		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.011415605670544943 | validation: 0.007790174442442593]
	TIME [epoch: 8.28 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016013193092806566		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.011211357874579274		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.01361227548369292 | validation: 0.007575387420925457]
	TIME [epoch: 8.27 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013055460061799251		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.008019606434300343		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.0105375332480498 | validation: 0.005965099096523314]
	TIME [epoch: 8.3 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0071946896379868315		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 0.014087190448558901		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.010640940043272864 | validation: -0.002317164295313562]
	TIME [epoch: 8.28 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1720.pth
	Model improved!!!
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0049263644596157855		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 0.012151742216993434		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.008539053338304608 | validation: 0.006035918534559207]
	TIME [epoch: 8.29 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009458705437002866		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.011914509114555471		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.010686607275779168 | validation: 0.005585744068041048]
	TIME [epoch: 8.27 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011979738090686146		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 0.010877884513277319		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.011428811301981732 | validation: 0.005466148511358086]
	TIME [epoch: 8.3 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012627789308783504		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.01471382412207196		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.013670806715427735 | validation: 0.001231964390973556]
	TIME [epoch: 8.27 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016255253407395278		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 0.011651222006175212		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.013953237706785243 | validation: 0.012423394721517501]
	TIME [epoch: 8.28 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015485404408094044		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.008051561258591727		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.011768482833342888 | validation: 0.009106638402728375]
	TIME [epoch: 8.27 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013220899459883978		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.011130271988900597		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.01217558572439229 | validation: 0.0035385973569795944]
	TIME [epoch: 8.29 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013488673168793642		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.012811870714725787		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.013150271941759713 | validation: 0.009364519933268796]
	TIME [epoch: 8.27 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007167141772966827		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.010866204631145533		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.00901667320205618 | validation: 0.002018190359822935]
	TIME [epoch: 8.29 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008760716105804148		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 0.012713513008411035		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.010737114557107588 | validation: 0.004004307816114133]
	TIME [epoch: 8.27 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006453445358408726		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 0.01178677572192658		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.009120110540167651 | validation: 0.0060481514423756915]
	TIME [epoch: 8.29 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012289028809888768		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.013894698914450821		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.013091863862169794 | validation: 0.0044384254740599365]
	TIME [epoch: 8.27 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014674602385104993		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 0.013117050150355155		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.013895826267730074 | validation: 0.008017606431506478]
	TIME [epoch: 8.27 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012243652656323097		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 0.00825907041140073		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.010251361533861915 | validation: 0.00802365695748639]
	TIME [epoch: 8.27 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012982362826208793		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.006919788189615236		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.009951075507912011 | validation: 0.010040039796912419]
	TIME [epoch: 8.29 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014035708541413703		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.01114269112020834		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.012589199830811021 | validation: 0.00577095204634977]
	TIME [epoch: 8.27 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012150264058582105		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.01039971540753497		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.011274989733058537 | validation: 0.005101714981011918]
	TIME [epoch: 8.28 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00951440685413232		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.00859738217254281		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.009055894513337564 | validation: 0.006555807286690496]
	TIME [epoch: 8.28 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010939846174829813		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.01075981804227664		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.010849832108553226 | validation: 0.007006686998891428]
	TIME [epoch: 8.3 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008614889227280237		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 0.017501825070975058		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.01305835714912765 | validation: 0.008246331191737614]
	TIME [epoch: 8.29 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015730746445443308		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.01015985675378238		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.012945301599612847 | validation: 0.0049377072385210965]
	TIME [epoch: 8.28 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014241557562441423		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 0.012092945140526293		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.013167251351483857 | validation: 0.009153425454808451]
	TIME [epoch: 8.28 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007683553758797743		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.014149935411799385		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.010916744585298564 | validation: 0.007159987590843907]
	TIME [epoch: 8.31 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009351023070363177		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 0.013594341798806892		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.011472682434585036 | validation: 0.005881283400743736]
	TIME [epoch: 8.28 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01207883579425766		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 0.009354969571930686		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.010716902683094172 | validation: 0.010581152035774935]
	TIME [epoch: 8.27 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011591202469945168		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.009925331855477549		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.010758267162711357 | validation: 0.012282929803795972]
	TIME [epoch: 8.29 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01363708017692813		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.008731597623041192		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.011184338899984662 | validation: 0.008007501066316815]
	TIME [epoch: 8.3 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016514022308814536		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.00876847713294757		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.012641249720881053 | validation: 0.011108843955232785]
	TIME [epoch: 8.28 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014715563261607343		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 0.01023577306287276		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.01247566816224005 | validation: 0.009467170709073677]
	TIME [epoch: 8.28 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01066119928771325		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.016043229238522506		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.013352214263117876 | validation: 0.011510361663158731]
	TIME [epoch: 8.27 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014695593382714981		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 0.013324133276169912		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.014009863329442446 | validation: 0.00596825753809417]
	TIME [epoch: 8.29 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007928463136162158		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.010664474493411734		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.009296468814786945 | validation: 0.009299152951588414]
	TIME [epoch: 8.27 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007643615396330193		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.01842894096380909		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.013036278180069643 | validation: 0.01540380189655728]
	TIME [epoch: 8.27 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009583250606877048		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 0.01247539809476632		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.011029324350821684 | validation: 0.011111440210659558]
	TIME [epoch: 8.28 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009560679713764375		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 0.014460988503344967		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.012010834108554667 | validation: 0.006672853955190543]
	TIME [epoch: 8.3 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01095039192299584		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.013113768075382472		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.012032079999189156 | validation: 0.0022603046356965877]
	TIME [epoch: 8.29 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013551708305264393		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.011197109847368571		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.01237440907631648 | validation: 0.01003487235644312]
	TIME [epoch: 8.29 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018804170499424743		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.005980904703411801		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.01239253760141827 | validation: 0.0051388164674353535]
	TIME [epoch: 8.3 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013123367246161246		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.012418653603582954		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.0127710104248721 | validation: 0.005843814578375971]
	TIME [epoch: 8.31 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010905998725407642		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.01449208913830547		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.012699043931856554 | validation: 0.005460592887484157]
	TIME [epoch: 8.3 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008517392663908186		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.010707338679979505		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.009612365671943844 | validation: 0.008327336069480676]
	TIME [epoch: 8.3 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013018124719009278		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.010614834032970703		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.01181647937598999 | validation: 0.0062907949404389435]
	TIME [epoch: 8.28 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015813889531207326		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 0.009551756023412114		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.012682822777309718 | validation: 0.005051057035836368]
	TIME [epoch: 8.3 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013243217740027874		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.008366369554328755		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.010804793647178314 | validation: 0.012850596771248553]
	TIME [epoch: 8.28 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00944250102736999		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.014891117092125084		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.012166809059747537 | validation: 0.015180747571511836]
	TIME [epoch: 8.28 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012399215848849761		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.012338760048213666		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.012368987948531713 | validation: 0.013860651775208167]
	TIME [epoch: 8.29 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014812801972637618		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.012875290875122347		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.01384404642387998 | validation: 0.010519870335027411]
	TIME [epoch: 8.31 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012391993127576295		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.008639959016518959		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.010515976072047625 | validation: 0.01050578189139581]
	TIME [epoch: 8.3 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015788007232781225		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.015836121185984414		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.01581206420938282 | validation: 0.009284330052311626]
	TIME [epoch: 8.29 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011302950647940152		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.011211330948323345		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.011257140798131749 | validation: 0.009427854109349466]
	TIME [epoch: 8.29 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.018289993287293468		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.010343869065240614		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.014316931176267037 | validation: 0.008752937623252648]
	TIME [epoch: 8.31 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014728907234172839		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.01316038001739607		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.013944643625784459 | validation: 0.008170229768218054]
	TIME [epoch: 8.29 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010878732664314597		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.009891393477428019		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.010385063070871307 | validation: 0.012050317856680561]
	TIME [epoch: 8.29 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010319921852660592		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.011097192535720538		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.010708557194190565 | validation: 0.008026322474875305]
	TIME [epoch: 8.29 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009550269460259556		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.011058471730757188		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.010304370595508372 | validation: 0.008994715820328701]
	TIME [epoch: 8.31 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010727399900771189		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.015180165019441116		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.012953782460106153 | validation: 0.012134904497870018]
	TIME [epoch: 8.3 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010859072774114669		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.010791745901521647		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.010825409337818158 | validation: 0.0035468509457671858]
	TIME [epoch: 8.29 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00996635723772758		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.013838254788082604		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.011902306012905092 | validation: 0.005841037957064624]
	TIME [epoch: 8.3 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007088000118392286		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.00867399795941675		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.007880999038904516 | validation: 0.013927024542808003]
	TIME [epoch: 8.31 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012664807711476864		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.010438496557607004		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.011551652134541934 | validation: 0.005478330574213816]
	TIME [epoch: 8.3 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011354714534640927		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.0106826812411181		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.011018697887879512 | validation: 0.013319654778132278]
	TIME [epoch: 8.29 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0162935335670314		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.012652905346485097		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.014473219456758248 | validation: -0.0022451138378527767]
	TIME [epoch: 8.28 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014692223227075415		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.01153015323865594		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.013111188232865681 | validation: 0.011994265293300325]
	TIME [epoch: 8.31 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006853214708863134		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 0.010410740850233049		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.008631977779548092 | validation: -0.003136630240335108]
	TIME [epoch: 8.29 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r0_20240219_192520/states/model_tr_study202_1784.pth
	Model improved!!!
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011813405562689724		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.011829978990544577		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.011821692276617151 | validation: 0.0038751127818524]
	TIME [epoch: 8.27 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01675547573960407		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.01200656073391331		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.014381018236758688 | validation: 0.01141651422563876]
	TIME [epoch: 8.27 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013934190730907427		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.00884933547792317		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.011391763104415297 | validation: 0.007932233708337764]
	TIME [epoch: 8.29 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01298750458383377		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.016201220017509278		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.014594362300671524 | validation: 0.0040348323263508755]
	TIME [epoch: 8.27 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016297451780659562		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.010182504565410111		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.01323997817303484 | validation: 0.013092828056756009]
	TIME [epoch: 8.28 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006116594392117253		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.017240967842232904		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.01167878111717508 | validation: 0.0054388902537506195]
	TIME [epoch: 8.27 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013387314080983703		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.010017339940232212		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.011702327010607957 | validation: 0.014025630514100908]
	TIME [epoch: 8.29 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009972305362178115		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 0.011993772691692443		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.010983039026935277 | validation: 0.005425348836158602]
	TIME [epoch: 8.28 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013165521093357451		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.013750004072006619		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.013457762582682034 | validation: -0.0018573252216138392]
	TIME [epoch: 8.27 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011888150595410448		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 0.009151429031123818		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.010519789813267134 | validation: 0.010009102879737783]
	TIME [epoch: 8.27 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011031403979847884		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.01040464637404696		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.010718025176947423 | validation: 0.013345570764012972]
	TIME [epoch: 8.3 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013713431659110759		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 0.011715089805464023		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.012714260732287392 | validation: 0.006256347674024202]
	TIME [epoch: 8.27 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010421720299634406		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.011533586346450573		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.01097765332304249 | validation: 0.010216500635212034]
	TIME [epoch: 8.27 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009591915862680992		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.01569408694165344		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.012643001402167214 | validation: 0.003957950319740481]
	TIME [epoch: 8.27 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014918368075352153		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.005633485237173577		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.010275926656262865 | validation: 0.009360662914666359]
	TIME [epoch: 8.29 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011509326496838184		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.010671037426576989		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.011090181961707586 | validation: 0.012804050360239636]
	TIME [epoch: 8.27 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005133261266210917		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 0.01718487277718111		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.011159067021696015 | validation: -0.0017112206341139808]
	TIME [epoch: 8.27 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008264748850160198		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.014464565689517612		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.011364657269838903 | validation: 0.002659601302154389]
	TIME [epoch: 8.28 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011954412179956278		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.012516602974113661		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.012235507577034968 | validation: 0.002848821541342722]
	TIME [epoch: 8.3 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007817678434804748		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 0.01564142523648996		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.011729551835647353 | validation: 0.007283877198484549]
	TIME [epoch: 8.27 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01271271676578602		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.011513632580625535		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.012113174673205778 | validation: 0.006045159902424005]
	TIME [epoch: 8.27 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012613665184357848		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.00967746948926691		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.011145567336812375 | validation: 0.006244683632849701]
	TIME [epoch: 8.27 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008727386764954301		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.012340577441826303		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.010533982103390304 | validation: 0.011715106764992314]
	TIME [epoch: 8.29 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01198215046878256		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 0.012255653192848248		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.012118901830815404 | validation: 0.008876150265794896]
	TIME [epoch: 8.28 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014466039557233029		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.012465373349017872		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.013465706453125453 | validation: 0.006972248105272879]
	TIME [epoch: 8.28 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01115437765700737		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 0.010045170948022094		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.010599774302514731 | validation: 0.007109705549816211]
	TIME [epoch: 8.27 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012281612786833035		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.015394580456910995		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.013838096621872017 | validation: 0.014154895033505453]
	TIME [epoch: 8.27 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.019220663906738985		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 0.005280553968064631		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.01225060893740181 | validation: 0.010014463810881145]
	TIME [epoch: 8.2 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009147346573110976		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 0.012846728656927145		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.010997037615019061 | validation: 0.01124376490912947]
	TIME [epoch: 8.22 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012958945171825931		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.010071972799265975		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.011515458985545953 | validation: 0.009695189329296208]
	TIME [epoch: 8.27 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007850193203128467		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.01403377251930511		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.01094198286121679 | validation: 0.008635356400024812]
	TIME [epoch: 8.29 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01607899018674924		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.00971710699876202		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.012898048592755628 | validation: 0.009627478399179679]
	TIME [epoch: 8.26 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014666520835407657		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.006792714557172226		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.010729617696289941 | validation: 0.01032080816902055]
	TIME [epoch: 8.26 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016840208366289723		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.014173050295254064		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.01550662933077189 | validation: 0.015332596233022484]
	TIME [epoch: 8.27 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009844677443681843		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.014630149264238857		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.012237413353960348 | validation: 0.006446672572868441]
	TIME [epoch: 8.28 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015066173105148892		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 0.007656373352358562		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.011361273228753726 | validation: 0.006517004602914209]
	TIME [epoch: 8.26 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011679384860076967		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.0132708738039714		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.01247512933202418 | validation: 0.00566961493750484]
	TIME [epoch: 8.25 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01097426970777771		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.00884035392601217		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.009907311816894939 | validation: 0.002984372048767805]
	TIME [epoch: 8.25 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0076718109819734915		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.0097695041909418		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.008720657586457646 | validation: 0.007138837762407752]
	TIME [epoch: 8.27 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00813299805678196		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.014768395943430896		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.01145069700010643 | validation: 0.001564175919617333]
	TIME [epoch: 8.27 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007334919744291318		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.01107289813906819		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.009203908941679757 | validation: 0.0023238078387755965]
	TIME [epoch: 8.27 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012102969328721309		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.012969489357317673		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.012536229343019492 | validation: 0.008704426772154439]
	TIME [epoch: 8.23 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014481000922976328		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.007891393494995848		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.011186197208986086 | validation: 0.009885420351554626]
	TIME [epoch: 8.25 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01345654149784416		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 0.009589729598284137		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.011523135548064146 | validation: 0.00734618486198131]
	TIME [epoch: 8.23 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011911054014241576		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.01674957874505751		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.014330316379649544 | validation: 0.0045469823733625455]
	TIME [epoch: 8.23 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010584024933014986		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.010264595275377906		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.010424310104196444 | validation: 0.010477532555202737]
	TIME [epoch: 8.23 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00864034332078402		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.012123698518357337		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.01038202091957068 | validation: 0.011209008148272075]
	TIME [epoch: 8.27 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014454797997847713		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 0.013785759787609247		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.014120278892728478 | validation: 0.007049077672817723]
	TIME [epoch: 8.24 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010625162875740052		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.015310242778706531		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.012967702827223291 | validation: 0.00611541116869372]
	TIME [epoch: 8.23 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011791356905999296		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 0.009260166463878295		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.010525761684938795 | validation: 0.002482280325322728]
	TIME [epoch: 8.23 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012410557676030177		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.010313260014860336		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.011361908845445256 | validation: 0.007568069349337864]
	TIME [epoch: 8.25 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007667357624747487		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.013129926108656797		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.01039864186670214 | validation: 0.012190219723537402]
	TIME [epoch: 8.24 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008626838936228233		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.013870685168935947		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.011248762052582092 | validation: 0.00898056801122309]
	TIME [epoch: 8.25 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011731320975374724		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 0.012266365682911538		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.011998843329143128 | validation: -0.0016362036751614347]
	TIME [epoch: 8.23 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006123008097388826		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.012092918919682227		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.009107963508535526 | validation: 0.00578816122832553]
	TIME [epoch: 8.26 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005740491227403517		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.014698928693119872		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.010219709960261697 | validation: 0.005098973860811456]
	TIME [epoch: 8.24 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0154040999523931		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.005993244947264022		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.01069867244982856 | validation: 0.004136506613769475]
	TIME [epoch: 8.23 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01313985218246809		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.01365478782174435		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.013397320002106217 | validation: 0.008855267631866387]
	TIME [epoch: 8.24 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013170637358144758		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.01406110052860376		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.01361586894337426 | validation: 0.00916330083115351]
	TIME [epoch: 8.25 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0136156195452588		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 0.009626213490843897		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.011620916518051347 | validation: 0.0038903094447129347]
	TIME [epoch: 8.23 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009164040718578192		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.013271438324598258		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.011217739521588227 | validation: 0.007669212426094848]
	TIME [epoch: 8.23 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009609538132139276		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.012771274552683298		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.011190406342411289 | validation: 0.004961071573554341]
	TIME [epoch: 8.23 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010879136025870383		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.010166307939793218		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.010522721982831803 | validation: 0.010245690560005365]
	TIME [epoch: 8.25 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010324646717577662		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 0.015211092490769431		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.012767869604173549 | validation: 0.007157934475024232]
	TIME [epoch: 8.24 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011888881687808944		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 0.017008609862770113		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.01444874577528953 | validation: 0.007365519460016105]
	TIME [epoch: 8.23 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011707273214307434		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.011186725296125992		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.011446999255216712 | validation: 0.012659800583435585]
	TIME [epoch: 8.23 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011706095676552815		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.011931256392402287		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.01181867603447755 | validation: 0.005179038919749914]
	TIME [epoch: 8.26 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.017029532050001035		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.0052273261474125535		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.011128429098706793 | validation: 0.006900138561807363]
	TIME [epoch: 8.23 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01079679471532341		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.008232020218434565		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.009514407466878986 | validation: 0.0037428848550400216]
	TIME [epoch: 8.23 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01071442687893695		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.009727092256106582		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.010220759567521769 | validation: 0.00984426015442833]
	TIME [epoch: 8.23 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016469134704302474		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.013852373807099139		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.015160754255700806 | validation: 0.00012194803949871482]
	TIME [epoch: 8.26 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007583688403734051		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.011395801653115092		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.009489745028424574 | validation: 0.0075425859457296605]
	TIME [epoch: 8.24 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015959529906027746		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.01047904435934248		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.013219287132685113 | validation: 0.007529754441295913]
	TIME [epoch: 8.23 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014334058593700256		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.007663616111333755		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.010998837352517006 | validation: 0.00902822400933006]
	TIME [epoch: 8.24 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013032727042030203		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.013538955333159564		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.013285841187594885 | validation: 0.008190777556390201]
	TIME [epoch: 8.24 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012925420336090041		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.016894677479269718		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.01491004890767988 | validation: 0.005019137185980144]
	TIME [epoch: 8.25 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012634427251114461		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.009917735744431536		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.011276081497772997 | validation: 0.005109122070694477]
	TIME [epoch: 8.23 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011818893578708137		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 0.010419253193804134		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 0.011119073386256135 | validation: 0.00469508610571833]
	TIME [epoch: 8.23 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011818169138778459		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.01233571002473555		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.012076939581757002 | validation: 0.006015083940094846]
	TIME [epoch: 8.25 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015556461070335103		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.011048684346594636		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.013302572708464869 | validation: 0.008264506623962609]
	TIME [epoch: 8.24 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011937613589335924		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.009781650828294432		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.010859632208815178 | validation: 0.006513574194730614]
	TIME [epoch: 8.23 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01333329772218533		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.012715420300482078		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.013024359011333708 | validation: 0.005920144830729916]
	TIME [epoch: 8.23 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012216333982799565		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.01397370218405237		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.013095018083425966 | validation: 0.010400597188235454]
	TIME [epoch: 8.26 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009945543279484865		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.011527966270838394		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.01073675477516163 | validation: 0.0052183804843256906]
	TIME [epoch: 8.24 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011182531946115314		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.013641626217322959		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.012412079081719134 | validation: 0.0010432333913774814]
	TIME [epoch: 8.23 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012194327741840729		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.00971775245289939		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.010956040097370058 | validation: 0.014508881141150688]
	TIME [epoch: 8.23 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013151160978255163		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.008225550474437116		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.01068835572634614 | validation: 0.0008562770960667776]
	TIME [epoch: 8.25 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013790342876965286		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.011205057615684366		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.012497700246324826 | validation: 0.0038736014483913518]
	TIME [epoch: 8.23 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011526208023951007		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.006908943984043679		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.00921757600399734 | validation: 0.009864909284723673]
	TIME [epoch: 8.23 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012987099094717777		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 0.010180494944131943		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.011583797019424861 | validation: 0.005472229840292279]
	TIME [epoch: 8.24 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011440105229053255		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.011667976875806665		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.01155404105242996 | validation: 0.012574751636613674]
	TIME [epoch: 8.25 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011402094932153224		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 0.008091929085898081		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.009747012009025651 | validation: 0.0025820116919035864]
	TIME [epoch: 8.24 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015891251677830717		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.011371810796602159		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.013631531237216438 | validation: 0.001820399056304072]
	TIME [epoch: 8.24 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007192064019220809		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.01424752411789294		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.010719794068556875 | validation: 0.01216328338094252]
	TIME [epoch: 8.25 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010474894295379653		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 0.014284148884754424		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.012379521590067038 | validation: 0.007854602043241114]
	TIME [epoch: 8.26 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011062355240588084		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.012116231038495266		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.011589293139541677 | validation: 0.009150058804116635]
	TIME [epoch: 8.24 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012043717181334165		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.009010231682826497		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.01052697443208033 | validation: 0.008492027008152544]
	TIME [epoch: 8.24 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013742448207156857		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.010184196732631988		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.011963322469894422 | validation: 0.003930800621100114]
	TIME [epoch: 8.24 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007068135254129653		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.00790468512261315		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.0074864101883714024 | validation: 0.006406577758543702]
	TIME [epoch: 8.25 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012287877147015205		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.00782000796603425		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.010053942556524727 | validation: 0.006117291617901903]
	TIME [epoch: 8.24 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011502641949404695		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.007597697659914681		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.009550169804659687 | validation: 0.010981788288827583]
	TIME [epoch: 8.24 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01102782054952348		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.014806125342439275		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.012916972945981379 | validation: 0.013949190892485957]
	TIME [epoch: 8.23 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009773522107467975		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.01586999501075103		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.012821758559109503 | validation: 0.009788990297015153]
	TIME [epoch: 8.26 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011250297024075507		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 0.009310809156436855		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.01028055309025618 | validation: 0.0031984339270038]
	TIME [epoch: 8.24 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015753548164609884		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 0.012057927684287933		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 0.013905737924448913 | validation: 0.0014821932389254648]
	TIME [epoch: 8.24 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008311627192184836		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.013790785862051274		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.011051206527118057 | validation: 0.005316173343140901]
	TIME [epoch: 8.24 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01162385579610184		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.01012039094324663		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.010872123369674237 | validation: 0.006688691361945078]
	TIME [epoch: 8.26 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014022083075659358		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 0.0081200900947778		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.01107108658521858 | validation: 0.0031164034575040154]
	TIME [epoch: 8.24 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011829816200872473		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.010985974694468688		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.011407895447670579 | validation: 0.011483390264753306]
	TIME [epoch: 8.24 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011981014202562855		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.007954456464582567		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.009967735333572711 | validation: 0.00435508985338158]
	TIME [epoch: 8.25 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01085883076866997		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.012465771082185027		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.011662300925427497 | validation: 0.005632616065498447]
	TIME [epoch: 8.26 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007886024595941739		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.013058667678764685		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.01047234613735321 | validation: 0.006701224285721503]
	TIME [epoch: 8.24 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015297017992153262		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.010380652320272143		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.012838835156212702 | validation: 0.001761732152734669]
	TIME [epoch: 8.24 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010529430446684498		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.012186434104249164		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.011357932275466833 | validation: 0.011087510056946723]
	TIME [epoch: 8.23 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00896686211810913		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 0.009778273295110704		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.009372567706609916 | validation: 0.004181252497531276]
	TIME [epoch: 8.26 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008660086110180714		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.008791251078583397		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.008725668594382055 | validation: 0.005883973794350846]
	TIME [epoch: 8.25 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007991512297924884		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.013342944649832936		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.010667228473878913 | validation: 0.0014889717658981222]
	TIME [epoch: 8.24 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01200561913291302		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.011315600859694028		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.011660609996303523 | validation: 0.005819052207520719]
	TIME [epoch: 8.24 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011730921243064166		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.011467793538734939		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.011599357390899552 | validation: 0.009607715639671485]
	TIME [epoch: 8.25 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011521482431979877		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.01395959022930549		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.012740536330642685 | validation: 0.001981510201894166]
	TIME [epoch: 8.24 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012477019377864074		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.007786046357779862		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.01013153286782197 | validation: 0.009439824828367931]
	TIME [epoch: 8.23 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011423970501841034		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 0.011272374043440541		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.011348172272640787 | validation: 0.012685129996000143]
	TIME [epoch: 8.24 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009146214880507117		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 0.005846610787548985		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.0074964128340280515 | validation: 0.007640810598013906]
	TIME [epoch: 8.25 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015444667543603527		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 0.013682498886897262		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.014563583215250392 | validation: 0.0037090470838008235]
	TIME [epoch: 8.23 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01404778618619416		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.006482558302392386		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.010265172244293273 | validation: 0.0075940367249199645]
	TIME [epoch: 8.24 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010174115459931129		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.009565440717735152		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.00986977808883314 | validation: 0.008092160219323878]
	TIME [epoch: 8.23 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009271779440709364		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 0.011607377103854382		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.010439578272281872 | validation: 0.002288743940807513]
	TIME [epoch: 8.25 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006213289216854156		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.01510549427623512		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.01065939174654464 | validation: 0.009682094606789107]
	TIME [epoch: 8.24 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008153159218414694		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.011518325789374234		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.009835742503894464 | validation: 0.008195492582918433]
	TIME [epoch: 8.24 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01349583491878881		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.013415300646171385		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.0134555677824801 | validation: 0.009927451150436568]
	TIME [epoch: 8.24 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010459386234374998		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.012478752518105549		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.011469069376240272 | validation: 0.010925856520963709]
	TIME [epoch: 8.26 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010464137873240235		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.009614736204970123		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.01003943703910518 | validation: 0.0010536094915432619]
	TIME [epoch: 8.24 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01707550101049151		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.008555090084647448		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.012815295547569477 | validation: 0.01252743772159996]
	TIME [epoch: 8.23 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014167381289320121		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.011983542223842835		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.013075461756581478 | validation: 0.006000837335785056]
	TIME [epoch: 8.23 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009252023673727006		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 0.01423440786393591		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.011743215768831458 | validation: 0.011178320815285277]
	TIME [epoch: 8.25 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010512089803197311		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.008721771562196818		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.009616930682697065 | validation: 0.004618143217361983]
	TIME [epoch: 8.24 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008421262499743468		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.013990168056033125		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.011205715277888295 | validation: 0.014375829732763384]
	TIME [epoch: 8.24 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011670273219484883		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.007796378889973432		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.009733326054729157 | validation: 0.0027639106087711147]
	TIME [epoch: 8.24 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013436118025582085		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.012122825802847965		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.012779471914215027 | validation: 0.0076493907018298215]
	TIME [epoch: 8.26 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012387410480522495		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 0.01139024175169949		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.01188882611611099 | validation: 0.006945333666099364]
	TIME [epoch: 8.24 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01279449732129935		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 0.010635441505190994		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.011714969413245175 | validation: 0.007493130411054555]
	TIME [epoch: 8.24 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013714962145212312		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 0.009292245211725853		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.011503603678469082 | validation: -0.0027233382261561013]
	TIME [epoch: 8.25 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005872700748758339		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.013618093243823353		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.009745396996290848 | validation: 0.007206871440309839]
	TIME [epoch: 8.25 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011857600616345623		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.007621476118647735		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.00973953836749668 | validation: 0.0072352324412698656]
	TIME [epoch: 8.24 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009963141438390631		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.016748072908795302		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.013355607173592968 | validation: 0.0026282878896503738]
	TIME [epoch: 8.24 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.014777973427930028		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 0.007299899049088866		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.011038936238509446 | validation: 0.005824536486052146]
	TIME [epoch: 8.24 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01010533410721847		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.010504505703845299		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.010304919905531885 | validation: 0.003669271589091269]
	TIME [epoch: 8.27 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009717502772899338		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 0.009541558484194964		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.00962953062854715 | validation: 0.003500670344371942]
	TIME [epoch: 8.25 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013949670161934446		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.008180889464668903		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.011065279813301675 | validation: 0.00417862282772151]
	TIME [epoch: 8.24 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008051898899710126		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.014757121837643517		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.011404510368676821 | validation: 0.007275840574316575]
	TIME [epoch: 8.23 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01501947890188741		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.011068770201716592		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.013044124551802 | validation: -5.206221200014212e-05]
	TIME [epoch: 8.26 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0075886865825976774		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.01862766671645364		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.01310817664952566 | validation: 0.004272375489757251]
	TIME [epoch: 8.24 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012259500221385815		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.007182630458497886		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.009721065339941852 | validation: 0.007752278288704988]
	TIME [epoch: 8.26 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013081140782982881		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.010234835994358834		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.011657988388670857 | validation: 0.010986164243677321]
	TIME [epoch: 8.24 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009930280076116673		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.014406218771261931		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.012168249423689301 | validation: 0.007679864894820668]
	TIME [epoch: 8.26 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010570564337603002		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 0.015200122564267327		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.012885343450935163 | validation: 0.0043626641225045825]
	TIME [epoch: 8.24 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008550973644250021		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.01217883079143359		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.010364902217841806 | validation: 0.002770667810867282]
	TIME [epoch: 8.25 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01886170914051996		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.010447401913106757		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.01465455552681336 | validation: 0.011075576626209022]
	TIME [epoch: 8.26 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012428152154461008		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.007922790471932274		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.010175471313196639 | validation: 0.010070715903421098]
	TIME [epoch: 8.26 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008888140805630319		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.013816063937711842		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.011352102371671082 | validation: 0.015329165389381538]
	TIME [epoch: 8.25 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005313128977214263		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.013870016042223113		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.009591572509718687 | validation: 0.010529198342415886]
	TIME [epoch: 8.23 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0116364397918711		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.01493951574981669		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.013287977770843895 | validation: 0.008094218740599231]
	TIME [epoch: 8.24 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011101316497925394		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.011124252279812048		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.011112784388868719 | validation: 0.011059437729041168]
	TIME [epoch: 8.26 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010496338294666833		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.016186235864413884		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.013341287079540359 | validation: 0.007958632928405386]
	TIME [epoch: 8.24 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01977603220328712		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 0.009966351254430092		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.014871191728858604 | validation: 0.006606668517046046]
	TIME [epoch: 8.24 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008860704453797533		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 0.015233522849020487		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.01204711365140901 | validation: 0.002630275264301249]
	TIME [epoch: 8.25 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011114326972632229		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.010365976889059013		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.010740151930845623 | validation: 0.011235504052485596]
	TIME [epoch: 8.25 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010807634845263848		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.009668596955774591		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.010238115900519222 | validation: 0.007449529693516304]
	TIME [epoch: 8.24 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007957432593915435		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.01209359760841577		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.010025515101165604 | validation: 0.005283449625996214]
	TIME [epoch: 8.24 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004173357115944344		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 0.012469208617649209		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.008321282866796776 | validation: 0.00652359804276277]
	TIME [epoch: 8.24 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013926929176360963		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.00958291533624776		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.011754922256304362 | validation: 0.00908159517538926]
	TIME [epoch: 8.26 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009987945055248469		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.010867507687725212		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.010427726371486843 | validation: 0.003648612373299764]
	TIME [epoch: 8.25 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.007720343470044529		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.012813115228489283		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.010266729349266906 | validation: 0.0067638483322846575]
	TIME [epoch: 8.24 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008435076182041859		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.014595008213230783		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.01151504219763632 | validation: 0.011908783215687944]
	TIME [epoch: 8.24 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008664710531359109		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: 0.012053609393678173		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.010359159962518641 | validation: 0.0074349682740094325]
	TIME [epoch: 8.26 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016839931390781618		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.00855457312311928		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.01269725225695045 | validation: 0.002557733113151275]
	TIME [epoch: 8.24 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.016345635600982556		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.009610930622715588		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.012978283111849074 | validation: 0.005352886501493565]
	TIME [epoch: 8.25 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015106861194054164		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.008745030054722266		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.011925945624388212 | validation: 0.006035700811057613]
	TIME [epoch: 8.24 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.005686731599071977		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.012845675765177426		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.009266203682124703 | validation: 0.011821322972565104]
	TIME [epoch: 8.26 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010838710648311314		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.013665968306041726		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.01225233947717652 | validation: 0.003955562671724506]
	TIME [epoch: 8.25 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011672242662516716		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.01151751360719953		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.011594878134858124 | validation: 0.00758195492589989]
	TIME [epoch: 8.24 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013082302413867763		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.00935246081005512		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.011217381611961442 | validation: 0.003498645143067064]
	TIME [epoch: 8.25 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009458463721677664		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.00990703537731662		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.009682749549497143 | validation: 0.00922828465381275]
	TIME [epoch: 8.25 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012574541925160715		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.009392230525389105		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.010983386225274911 | validation: 0.009078016116763496]
	TIME [epoch: 8.25 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008677825293391114		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.01438985465485532		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.011533839974123215 | validation: 0.011988220240867301]
	TIME [epoch: 8.24 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012205091674228275		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.013351013105760475		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.012778052389994377 | validation: 0.01164691180998589]
	TIME [epoch: 8.24 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010254559851664739		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.013468545679188643		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.01186155276542669 | validation: 0.013670451043210923]
	TIME [epoch: 8.25 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.004792097915684926		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.013139281257897739		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.008965689586791333 | validation: 0.010241389066982848]
	TIME [epoch: 8.27 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0126807689135846		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.0072407825321097585		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.00996077572284718 | validation: 0.004027202876282979]
	TIME [epoch: 8.23 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012905202135959925		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.009877875109516654		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.01139153862273829 | validation: 0.006408083866992436]
	TIME [epoch: 8.24 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009056037908660464		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.011220143956258906		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.010138090932459685 | validation: 0.0073136630489783645]
	TIME [epoch: 8.24 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013159892603107303		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.009034525555211063		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.01109720907915918 | validation: 0.006543611634493713]
	TIME [epoch: 8.26 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01242151158121588		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.01186461464681517		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.012143063114015525 | validation: 0.009829381645777208]
	TIME [epoch: 8.25 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012414663998418167		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.00751102715470252		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.009962845576560346 | validation: 0.01012429768710169]
	TIME [epoch: 8.23 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009471947003708837		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.012439580448672747		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.010955763726190794 | validation: 0.007937763052453908]
	TIME [epoch: 8.24 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.00954232048805508		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.014233387292611454		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.011887853890333267 | validation: 0.010832056132064668]
	TIME [epoch: 8.26 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008961325049804753		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.009226443730118087		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.009093884389961417 | validation: 0.007378083779332235]
	TIME [epoch: 8.25 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012702718016096681		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.013746705466126535		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.013224711741111611 | validation: 0.0065968243897241395]
	TIME [epoch: 8.24 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012152071084014983		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.010979500856034274		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.01156578597002463 | validation: 0.011033842395329082]
	TIME [epoch: 8.25 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009814578250040791		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.015843760133542963		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.012829169191791879 | validation: 0.010101777671681803]
	TIME [epoch: 8.26 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011913218805647151		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.011870860100049396		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.011892039452848273 | validation: 0.006186022926007305]
	TIME [epoch: 8.24 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01194470638624523		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.010837810264969907		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.011391258325607568 | validation: 0.005847276275473273]
	TIME [epoch: 8.25 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.015086987798761775		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.009579486518984498		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.012333237158873137 | validation: 0.0031787059772453875]
	TIME [epoch: 8.25 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010410452263054985		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.013634424582262583		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.012022438422658783 | validation: 0.007433813531787608]
	TIME [epoch: 8.26 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.013100656449854775		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.009155712638050916		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.011128184543952846 | validation: 0.006185662372409133]
	TIME [epoch: 8.23 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.010413697829120407		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.007090448495727044		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.008752073162423725 | validation: 0.0008304464351262515]
	TIME [epoch: 8.24 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009548893977576686		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.008846966065495562		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.009197930021536125 | validation: 0.008824038189009788]
	TIME [epoch: 8.24 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.006013743977024512		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.013867562438579515		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.009940653207802014 | validation: 0.005945841710220695]
	TIME [epoch: 8.25 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.012011823685411477		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.013408349368877854		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.012710086527144665 | validation: 0.006058275683284342]
	TIME [epoch: 8.24 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011604568693915114		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.00914575286717131		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.010375160780543215 | validation: 0.005491829152371886]
	TIME [epoch: 8.23 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01638324459979442		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.007199316427628374		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.011791280513711398 | validation: 0.004757342493315343]
	TIME [epoch: 8.25 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.01470722945082267		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 0.007222674824240612		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.010964952137531642 | validation: 0.0023937974636220522]
	TIME [epoch: 8.26 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0073691633085183035		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.011611684263519563		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.009490423786018932 | validation: 0.005141742006302717]
	TIME [epoch: 8.24 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.009713898732399893		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.011931453992870416		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.010822676362635156 | validation: 0.003031465469317303]
	TIME [epoch: 8.24 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.011958366677131214		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 0.011922978522832186		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.0119406725999817 | validation: 0.009473156659757112]
	TIME [epoch: 8.25 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.008068796159096498		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.013318400378703821		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.010693598268900161 | validation: 0.005586521526789612]
	TIME [epoch: 8.27 sec]
Finished training in 16623.584 seconds.
