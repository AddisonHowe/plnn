Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r3', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r3', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 142020843

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.267626823372161		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.334946504887439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.301286664129801 | validation: 8.376817240994018]
	TIME [epoch: 52.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.539149279872035		[learning rate: 0.01]
		[batch 20/20] avg loss: 7.2283179761380385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.883733628005037 | validation: 6.787734085400679]
	TIME [epoch: 8.25 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.871157680118496		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.846177832375833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.358667756247166 | validation: 4.780625400126954]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.441836732283319		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.0867213942258465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.264279063254583 | validation: 3.9797231757110154]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.374508397201595		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8880832956048637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1312958464032286 | validation: 5.508206294907381]
	TIME [epoch: 8.22 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6805397882255413		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.16375432552126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.422147056873401 | validation: 2.4700153932558515]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.110552682353144		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0531845270432187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0818686046981814 | validation: 1.8497327584143048]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8678837206970051		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6771451713716758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7725144460343407 | validation: 1.6016012797360362]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7266769624554386		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6663094638353926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6964932131454156 | validation: 1.919015347031978]
	TIME [epoch: 8.22 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5683535307841097		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.418449848784794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.493401689784452 | validation: 1.506659571564]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4798394613929005		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.4013046914629068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4405720764279037 | validation: 1.1048444102163266]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1884521186338226		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3195237420256678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2539879303297452 | validation: 1.2792700855759942]
	TIME [epoch: 8.21 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1843607272919134		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.214001897274906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1991813122834099 | validation: 1.0205379144157962]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1712165029956156		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0694046028438051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1203105529197106 | validation: 1.1852702183127697]
	TIME [epoch: 8.23 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1907836411660078		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0306748879801888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1107292645730984 | validation: 1.2536921486012895]
	TIME [epoch: 8.21 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0468507744955904		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0858682238285677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0663594991620786 | validation: 0.9891792610848004]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0817216012395847		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9871568435278842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0344392223837346 | validation: 1.2101439134857452]
	TIME [epoch: 8.19 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5675491854032078		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1420114662519698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3547803258275883 | validation: 0.6543854822560526]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9600168734694574		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0781606017395264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.019088737604492 | validation: 0.9851499286631189]
	TIME [epoch: 8.21 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.043535058167733		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0510618534334253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0472984558005793 | validation: 0.8854599854014802]
	TIME [epoch: 8.19 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9990885955251588		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0580085030018802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0285485492635196 | validation: 1.0507468052452051]
	TIME [epoch: 8.19 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0705323336039765		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9609420690531744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0157372013285753 | validation: 0.7569964332772593]
	TIME [epoch: 8.23 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8115531496057594		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8681859546332295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8398695521194945 | validation: 1.2605336492466468]
	TIME [epoch: 8.2 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0492051935285889		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7640695093139118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9066373514212505 | validation: 0.5825723062231237]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.835210298352951		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8539433203110164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445768093319839 | validation: 0.6968138715870602]
	TIME [epoch: 8.21 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8686610659596354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7461206223961732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8073908441779041 | validation: 1.3951273785597813]
	TIME [epoch: 8.22 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8027927262779299		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9301089038699363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8664508150739332 | validation: 0.9365854456179674]
	TIME [epoch: 8.21 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7980330640508398		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9245044359290505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8612687499899451 | validation: 1.3065335246175824]
	TIME [epoch: 8.2 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7788951125881074		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8080160596062038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7934555860971557 | validation: 0.48199640228631024]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8599886467898354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8556643687160704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.857826507752953 | validation: 0.9226583148678544]
	TIME [epoch: 8.2 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8312003397468952		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7856218996744848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.80841111971069 | validation: 0.6987738131002008]
	TIME [epoch: 8.22 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8306055373297303		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8343075056443288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8324565214870295 | validation: 0.513164890684411]
	TIME [epoch: 8.2 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.685461756116499		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8451270155172146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7652943858168566 | validation: 0.4812273734784345]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7613974668148955		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7972845176621194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7793409922385075 | validation: 0.7832622460527215]
	TIME [epoch: 8.21 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7863616585036967		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8125161724776107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7994389154906536 | validation: 0.43054588693377915]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8054272230068946		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8230721918418352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8142497074243649 | validation: 1.118076400867673]
	TIME [epoch: 8.21 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9290584895369769		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7064247548915175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177416222142473 | validation: 0.4573165901188144]
	TIME [epoch: 8.22 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9079627449709899		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7004811101025082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8042219275367494 | validation: 0.5097276293034646]
	TIME [epoch: 8.22 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7805148031924345		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6646635579965907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225891805945127 | validation: 0.8086698511879138]
	TIME [epoch: 8.24 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7805797082549664		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7454130376310595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762996372943013 | validation: 0.583443098087369]
	TIME [epoch: 8.22 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7491132468516538		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6882711521588277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186921995052408 | validation: 0.46082841222551574]
	TIME [epoch: 8.22 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7642670617414133		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7682958129718596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7662814373566363 | validation: 0.83139579774248]
	TIME [epoch: 8.22 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7010012901781411		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7964712395603727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7487362648692569 | validation: 0.8236877756438589]
	TIME [epoch: 8.24 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0449793716006766		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7479988181236494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964890948621633 | validation: 0.5194240858216183]
	TIME [epoch: 8.22 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2087773748613182		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7841047443704802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9964410596158993 | validation: 0.6278631902035661]
	TIME [epoch: 8.22 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6724899907880221		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6643408481966068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6684154194923144 | validation: 0.572538323411798]
	TIME [epoch: 8.22 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6330490689174436		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7130953975397867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730722332286152 | validation: 0.4359923898276475]
	TIME [epoch: 8.24 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7566619396516996		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6620594025932307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7093606711224651 | validation: 1.326580541754134]
	TIME [epoch: 8.22 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7675335198857832		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8236450189267057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7955892694062444 | validation: 0.5789096238976713]
	TIME [epoch: 8.21 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6532951172575736		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6253655846454238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6393303509514987 | validation: 0.8449795381020315]
	TIME [epoch: 8.21 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7159184227264768		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6571582032993167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6865383130128968 | validation: 0.5862060961406381]
	TIME [epoch: 8.24 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6773166864453979		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6844146907013735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6808656885733858 | validation: 0.4892776834832067]
	TIME [epoch: 8.21 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6492817937303291		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7015223047611734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6754020492457511 | validation: 0.4816703307724739]
	TIME [epoch: 8.22 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6096810709800042		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8597764301947703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7347287505873872 | validation: 0.44789737999909884]
	TIME [epoch: 8.21 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5899265834892893		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.69023428040369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6400804319464896 | validation: 0.827787359603203]
	TIME [epoch: 8.24 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.615498325985046		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6686510295520905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6420746777685682 | validation: 0.5692570505364359]
	TIME [epoch: 8.22 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6079291066305401		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5803476488635871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5941383777470636 | validation: 0.47438600554567045]
	TIME [epoch: 8.21 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5853987579493185		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6011893420294452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932940499893818 | validation: 0.9864760993329786]
	TIME [epoch: 8.22 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6427488202136159		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6357263773273629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6392375987704894 | validation: 0.7205803585668085]
	TIME [epoch: 8.24 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5534777390154221		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5961390027311336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.574808370873278 | validation: 0.9495592943069615]
	TIME [epoch: 8.21 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6473056351789259		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5936047911767621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6204552131778441 | validation: 1.596224535310907]
	TIME [epoch: 8.21 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8840192961051503		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5357115618824502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098654289938001 | validation: 0.3736473684204059]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7358726891219974		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5978667266742331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668697078981152 | validation: 0.3931897036661949]
	TIME [epoch: 8.25 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4884794557781092		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.529684299239228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5090818775086685 | validation: 0.5807394232179359]
	TIME [epoch: 8.21 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6208289133740472		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5755858102474634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5982073618107553 | validation: 0.965847660527176]
	TIME [epoch: 8.21 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6782669799330291		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7020583953922073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6901626876626181 | validation: 0.6940834174834375]
	TIME [epoch: 8.21 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6247601372944561		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.818436617794484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.72159837754447 | validation: 0.46175688449476265]
	TIME [epoch: 8.23 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.573805396441149		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5624437953720871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.568124595906618 | validation: 0.3961513360892882]
	TIME [epoch: 8.22 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5472884899602715		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4905610806685384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5189247853144049 | validation: 0.3765844309476084]
	TIME [epoch: 8.21 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7077044585318549		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5958826120630797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6517935352974673 | validation: 0.3818566497330141]
	TIME [epoch: 8.21 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5463241808193594		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5705881759262725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5584561783728158 | validation: 0.46344877108682736]
	TIME [epoch: 8.22 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8110519827490407		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5457111569040046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6783815698265225 | validation: 0.48804165465531096]
	TIME [epoch: 8.22 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6304862623967145		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6727474276475954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6516168450221549 | validation: 0.74681987841614]
	TIME [epoch: 8.21 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5082925186613254		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5639760959708935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5361343073161094 | validation: 0.6444668304405405]
	TIME [epoch: 8.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6313794593880006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6359089595980507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6336442094930257 | validation: 0.4298029599012053]
	TIME [epoch: 8.2 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5745295443665355		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5938527980699758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5841911712182556 | validation: 0.47610413813130137]
	TIME [epoch: 8.23 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6687471101183176		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5990307190768303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6338889145975738 | validation: 0.8526095790539552]
	TIME [epoch: 8.21 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8328759479801853		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5834713876398447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708173667810015 | validation: 0.6904746313091432]
	TIME [epoch: 8.21 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6173593216245287		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6060300344917201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6116946780581245 | validation: 0.7580238263778294]
	TIME [epoch: 8.2 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5787593672102498		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5671732020373838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5729662846238168 | validation: 0.7408219416953149]
	TIME [epoch: 8.23 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5994640847227808		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7358480788132626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6676560817680215 | validation: 0.5481579993458194]
	TIME [epoch: 8.21 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6581548996873292		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6437192032952006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6509370514912648 | validation: 0.5818488045372607]
	TIME [epoch: 8.2 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6222452229145519		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7667020338938992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944736284042256 | validation: 0.4217023412337882]
	TIME [epoch: 8.21 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5910169428767028		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6170817355285653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604049339202634 | validation: 0.6107928685862589]
	TIME [epoch: 8.23 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5681628994399622		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5243824864450937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5462726929425278 | validation: 1.1027079249052147]
	TIME [epoch: 8.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7337616789783545		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6163173905522656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.67503953476531 | validation: 0.4042823586323048]
	TIME [epoch: 8.21 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.597275117328681		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6326966476068224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6149858824677519 | validation: 0.81201597530723]
	TIME [epoch: 8.21 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.645197021404953		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5753030270777442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6102500242413486 | validation: 0.47142203204307176]
	TIME [epoch: 8.23 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5709142903907493		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5639706408635267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5674424656271378 | validation: 0.3001240361920171]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5062693912280977		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6149477748755079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5606085830518028 | validation: 0.5240319813986346]
	TIME [epoch: 8.22 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5784656926431357		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.696397278364898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.637431485504017 | validation: 0.3947273492847763]
	TIME [epoch: 8.21 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5284500048511114		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5924132934253942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5604316491382529 | validation: 0.32532432476786255]
	TIME [epoch: 8.24 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4944186611263329		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5658462865367457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5301324738315394 | validation: 0.41831399131605207]
	TIME [epoch: 8.21 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5297611690440116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6029736456123811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5663674073281963 | validation: 0.36725191760654097]
	TIME [epoch: 8.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5309550098818023		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5129998910988803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5219774504903414 | validation: 0.4171321656000945]
	TIME [epoch: 8.21 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46034582029544513		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.47741504081155284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46888043055349904 | validation: 0.9418316354788947]
	TIME [epoch: 8.23 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5522221390321933		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.557384192201014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5548031656166035 | validation: 0.6522993557529423]
	TIME [epoch: 8.2 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6129863609654771		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7451393591806438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6790628600730604 | validation: 0.40918365674689705]
	TIME [epoch: 8.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5651484572111696		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5090625629326483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5371055100719089 | validation: 0.2452304911546619]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5440127026196349		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5355838865179541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5397982945687947 | validation: 0.7589807532644584]
	TIME [epoch: 8.23 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6013425389572691		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.4235611063389409		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.5124518226481048 | validation: 0.5916480774464035]
	TIME [epoch: 8.2 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5690608222460438		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.5141956689566747		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.5416282456013592 | validation: 0.44650726877679947]
	TIME [epoch: 8.2 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4990896003224914		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.43698203306964273		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.4680358166960669 | validation: 0.4188232824890684]
	TIME [epoch: 8.2 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48034533633154475		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.41821462773703477		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.44927998203428965 | validation: 0.7364886518253735]
	TIME [epoch: 8.22 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5231823681724492		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.5710022105127182		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.5470922893425837 | validation: 0.9164778384895916]
	TIME [epoch: 8.2 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5099976515609599		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.7102098447526848		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.6101037481568223 | validation: 0.41034119165144245]
	TIME [epoch: 8.19 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4385803744862856		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.5227433529328316		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.48066186370955855 | validation: 0.9958920907213848]
	TIME [epoch: 8.2 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5234521588899895		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.44254759824279233		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.4829998785663908 | validation: 0.6971499275206177]
	TIME [epoch: 8.23 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5070413485446865		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.44700090908831064		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.4770211288164985 | validation: 0.39797032313589165]
	TIME [epoch: 8.21 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5549933295534222		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.5572852210756115		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.5561392753145169 | validation: 0.31053102714497854]
	TIME [epoch: 8.21 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42563558745929353		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.5025469698704387		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.4640912786648661 | validation: 0.6758625103406071]
	TIME [epoch: 8.2 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47771668103828013		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.5110805861973895		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.4943986336178348 | validation: 0.3486409056573957]
	TIME [epoch: 8.22 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5536676234369446		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.5210799128571775		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.5373737681470611 | validation: 0.6037418709744521]
	TIME [epoch: 8.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6101896877947158		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.5435582419707423		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.576873964882729 | validation: 0.42806890450465596]
	TIME [epoch: 8.2 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5279763528113135		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.4397679991147996		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.4838721759630566 | validation: 0.6047870467263231]
	TIME [epoch: 8.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47613871456057577		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.45802234501315214		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.467080529786864 | validation: 0.4305161317959688]
	TIME [epoch: 8.23 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5320947696769989		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.39758203468681286		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.46483840218190575 | validation: 0.5427376480984927]
	TIME [epoch: 8.2 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47357879615439363		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.4695689843233546		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.4715738902388741 | validation: 0.4324677040872146]
	TIME [epoch: 8.21 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6133498059843501		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.6130350265780443		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.6131924162811973 | validation: 0.29639502599738055]
	TIME [epoch: 8.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45880822635296087		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.5123185957798859		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.48556341106642337 | validation: 0.4146840392984969]
	TIME [epoch: 8.24 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42927498673633196		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.38217637598762433		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.4057256813619781 | validation: 0.2920868889201642]
	TIME [epoch: 8.2 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5453555494409742		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.38766012391336135		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.4665078366771677 | validation: 0.24920164753676863]
	TIME [epoch: 8.19 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41215302097098316		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.43836750401134184		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.4252602624911626 | validation: 0.47068673395895844]
	TIME [epoch: 8.19 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49030191480710067		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.5582216528669371		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.5242617838370188 | validation: 0.4433524855563228]
	TIME [epoch: 8.22 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4912983990253486		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.4444126769352498		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.4678555379802993 | validation: 0.4249111379677753]
	TIME [epoch: 8.2 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4839161415350334		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.5397851919609624		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.5118506667479978 | validation: 0.411659384695595]
	TIME [epoch: 8.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5036881897017751		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.4422552924645641		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.4729717410831696 | validation: 0.26187566743558977]
	TIME [epoch: 8.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5646573758050907		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.4493483195690545		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.5070028476870726 | validation: 0.6736627141625304]
	TIME [epoch: 8.23 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40695689684669956		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.501035763881326		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.453996330364013 | validation: 0.411384641170527]
	TIME [epoch: 8.22 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4502682642185598		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.44752693468101884		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.4488975994497893 | validation: 0.40223600715873054]
	TIME [epoch: 8.21 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.55952265715471		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.4950952053066164		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.5273089312306631 | validation: 0.5248812302979278]
	TIME [epoch: 8.21 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5610451252490283		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.5695662913348859		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.565305708291957 | validation: 0.48735668017749645]
	TIME [epoch: 8.23 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41762181672502335		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.5774579490875443		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.4975398829062838 | validation: 0.5105349523346907]
	TIME [epoch: 8.21 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6096108840264889		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.3969351745082669		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.5032730292673778 | validation: 0.4729245386422734]
	TIME [epoch: 8.2 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44022651901271515		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.376957919454234		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.40859221923347455 | validation: 0.5631559525734848]
	TIME [epoch: 8.21 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46316644076113783		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.4066614666952404		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.4349139537281892 | validation: 0.3213021961958184]
	TIME [epoch: 8.23 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45874496748048177		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.47502393563614576		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.4668844515583138 | validation: 0.4965572790155443]
	TIME [epoch: 8.22 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4438787612125399		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.5065433037244408		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.4752110324684905 | validation: 0.7566510103163879]
	TIME [epoch: 8.2 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7277844813061615		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.37308259080171746		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.5504335360539395 | validation: 0.6377332328728378]
	TIME [epoch: 8.2 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42254835488365317		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.49603900929343336		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.45929368208854326 | validation: 0.4614033542024455]
	TIME [epoch: 8.23 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.488548171403188		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.6192543958251557		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.5539012836141718 | validation: 0.4705875532139499]
	TIME [epoch: 8.22 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4327740791859981		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.906289181141845		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.6695316301639214 | validation: 0.47226924426143635]
	TIME [epoch: 8.2 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33844922513937353		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.468158471972994		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.40330384855618384 | validation: 0.3164484637156999]
	TIME [epoch: 8.2 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39056710263978994		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.5706121220517304		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.4805896123457602 | validation: 0.2531629511304007]
	TIME [epoch: 8.23 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35568476713182523		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.5127384172385567		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.4342115921851909 | validation: 0.36261200469712573]
	TIME [epoch: 8.21 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4925492911990627		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.41944495398413917		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.45599712259160086 | validation: 0.3637414545223382]
	TIME [epoch: 8.21 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5230997520542461		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.45848412127569055		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.4907919366649683 | validation: 0.3266276034928343]
	TIME [epoch: 8.21 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37439593449739683		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.5572910498434946		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.46584349217044574 | validation: 0.4478755926380342]
	TIME [epoch: 8.23 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4235092986083582		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.42777944280352675		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.4256443707059424 | validation: 0.391710527537108]
	TIME [epoch: 8.21 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46716412422235826		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.34346470677494534		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.4053144154986518 | validation: 0.26332776096342275]
	TIME [epoch: 8.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5011449047014497		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.7712529834778216		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.6361989440896356 | validation: 0.3331187258337508]
	TIME [epoch: 8.21 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5388405710668913		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.3666949471261504		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.4527677590965208 | validation: 0.2617057016928756]
	TIME [epoch: 8.22 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47344050478623434		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.4271400630891661		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.4502902839377002 | validation: 0.5650401336645877]
	TIME [epoch: 8.21 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40210354178030716		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.5124225932243431		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.4572630675023251 | validation: 0.6792020821428264]
	TIME [epoch: 8.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5256938635801708		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.44810651883754005		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.48690019120885547 | validation: 0.7806507848004207]
	TIME [epoch: 8.2 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43856156150767134		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.49801139929361105		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.4682864804006412 | validation: 0.7330792623729057]
	TIME [epoch: 8.22 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4434233724691744		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.5474538773236327		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.49543862489640356 | validation: 0.5381847078541013]
	TIME [epoch: 8.21 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5286788183101381		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.4296832622360527		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.47918104027309544 | validation: 0.2867532834251396]
	TIME [epoch: 8.21 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4128556160500117		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.3760160948099814		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.39443585542999654 | validation: 0.39247735618749563]
	TIME [epoch: 8.19 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3755837138494594		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.3722012292705974		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.37389247156002836 | validation: 0.47315091751992305]
	TIME [epoch: 8.23 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5638879249601655		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.3794617973398433		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.47167486115000423 | validation: 0.2834599121469875]
	TIME [epoch: 8.21 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5190963697205003		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.4188690286658704		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.4689826991931853 | validation: 0.29675629283547095]
	TIME [epoch: 8.21 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4455377961272554		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.40412757392677534		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.4248326850270153 | validation: 0.5568115895440686]
	TIME [epoch: 8.2 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5555970875750367		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.44430353188276167		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.4999503097288992 | validation: 0.5961607264904798]
	TIME [epoch: 8.22 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4092353365798731		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.441827669982447		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.42553150328116 | validation: 0.3176233472534533]
	TIME [epoch: 8.21 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4426674193954699		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.5474468574302259		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.4950571384128478 | validation: 0.5983572904413732]
	TIME [epoch: 8.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43658293648839025		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.4048343825228553		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.42070865950562286 | validation: 0.2706896295927093]
	TIME [epoch: 8.18 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39101699502170867		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.4265538467313913		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.40878542087655 | validation: 0.3971785531007789]
	TIME [epoch: 8.21 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.342554316514334		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.346776475735994		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.3446653961251639 | validation: 0.28856054125003583]
	TIME [epoch: 8.23 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38611110577587754		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.39936930325309705		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.3927402045144873 | validation: 0.5082533013901775]
	TIME [epoch: 8.2 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.400775166120004		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.5491823663337934		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.4749787662268986 | validation: 0.41035677413090677]
	TIME [epoch: 8.21 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3691262620884425		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.4086883968969815		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.388907329492712 | validation: 0.5382847854804185]
	TIME [epoch: 8.21 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3448585403518999		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.4235448881361578		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.3842017142440289 | validation: 0.3152335699530225]
	TIME [epoch: 8.23 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5067139215884937		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.37253327411902015		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.439623597853757 | validation: 0.350907768651212]
	TIME [epoch: 8.21 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33117812429652316		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.4399776026104828		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.38557786345350303 | validation: 0.30095347199779005]
	TIME [epoch: 8.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43894115137972217		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.4496557542211631		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.44429845280044267 | validation: 0.38563910756064734]
	TIME [epoch: 8.21 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4439011815513684		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.37908278254440214		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.41149198204788523 | validation: 0.2956466454094038]
	TIME [epoch: 8.23 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42506768139256035		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.3239273350060499		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.3744975081993053 | validation: 0.2595004782810425]
	TIME [epoch: 8.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40090985355155		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.3484740645138472		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.3746919590326986 | validation: 0.2361212170709725]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32460368008623597		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.529946424471171		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.42727505227870344 | validation: 0.32076957898462677]
	TIME [epoch: 8.21 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49077197408423706		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.3469767835266101		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.41887437880542366 | validation: 0.6635447820537828]
	TIME [epoch: 8.22 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40331687697903496		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.34314481362371885		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.37323084530137696 | validation: 0.3461254502944585]
	TIME [epoch: 8.2 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42737982224178434		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.4341755243853795		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.4307776733135819 | validation: 0.4712658375350611]
	TIME [epoch: 8.19 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5436113132438555		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.39758592502699036		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.47059861913542306 | validation: 0.30477001988562874]
	TIME [epoch: 8.19 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4291218339014467		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.3141131754023821		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.3716175046519144 | validation: 0.584190947199063]
	TIME [epoch: 8.21 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.422754605445259		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.4275161380590573		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.42513537175215815 | validation: 0.3171483909559851]
	TIME [epoch: 8.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35351558873346983		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.42524395596589865		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.38937977234968424 | validation: 0.6618276038932727]
	TIME [epoch: 8.19 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36362653925223054		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.3653541301578428		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.3644903347050367 | validation: 0.22079195019820683]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29450965244609684		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.4591529543063279		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.3768313033762124 | validation: 1.236208698426951]
	TIME [epoch: 8.23 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5398229505448662		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.48286936912711376		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.51134615983599 | validation: 0.20549345983929942]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49952942524007315		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.34582377586904695		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.4226766005545601 | validation: 0.1692564280527247]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3185324316726563		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.4036268070747157		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.3610796193736861 | validation: 0.3517845509140028]
	TIME [epoch: 8.2 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3854668427995084		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.3419568642913139		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.363711853545411 | validation: 0.35685963789097813]
	TIME [epoch: 8.22 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3490504111773752		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.40665626032201985		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.3778533357496975 | validation: 0.27907890552827447]
	TIME [epoch: 8.19 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7142061199083399		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.5027351334008867		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.6084706266546134 | validation: 0.21702327187980255]
	TIME [epoch: 8.19 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4595111987735153		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.3765986763587761		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.41805493756614576 | validation: 0.2097774337549083]
	TIME [epoch: 8.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34362712674600726		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.3753695679618898		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.3594983473539485 | validation: 0.23123960891371248]
	TIME [epoch: 8.22 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2797137038739064		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.3199476610250738		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.2998306824494902 | validation: 0.4236964480499546]
	TIME [epoch: 8.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4570840710010665		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.3623501996922572		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.4097171353466618 | validation: 0.340576684574023]
	TIME [epoch: 8.19 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3803461648603873		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.5684164057993524		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.47438128532986984 | validation: 1.8536833134651653]
	TIME [epoch: 8.2 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5596764975142111		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.3214084346905395		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.44054246610237524 | validation: 0.2541222579186824]
	TIME [epoch: 8.22 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3433614303349607		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.3703934779642114		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.35687745414958605 | validation: 0.26031465728444647]
	TIME [epoch: 8.19 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3406382500450652		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.39734872452282555		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.36899348728394543 | validation: 0.35277227238207587]
	TIME [epoch: 8.2 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28194257410628654		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.41534959126721754		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.34864608268675207 | validation: 0.5291767072498587]
	TIME [epoch: 8.2 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4427607501338014		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.2839562715218317		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.3633585108278165 | validation: 0.1708717087608877]
	TIME [epoch: 8.22 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30843784698749593		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.3120299794621288		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.3102339132248124 | validation: 0.3357393290735184]
	TIME [epoch: 8.21 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39422182232140424		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.3610765260904863		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.37764917420594524 | validation: 0.1574095366472194]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2606097984377983		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.29271393723021		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.2766618678340041 | validation: 0.3886866535771807]
	TIME [epoch: 8.2 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3513251818767277		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.3975296945572583		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.374427438216993 | validation: 0.27480118665239933]
	TIME [epoch: 8.22 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34803189778928156		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.32840170471010766		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.3382168012496945 | validation: 0.23175058654609532]
	TIME [epoch: 8.2 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35382653602311687		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.36043753701170467		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.3571320365174108 | validation: 0.35164647848624747]
	TIME [epoch: 8.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31414028641720554		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.29808902635540446		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.30611465638630503 | validation: 0.26423996447671905]
	TIME [epoch: 8.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27627921700911834		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 0.3134416534784824		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 0.2948604352438004 | validation: 0.5648061311864687]
	TIME [epoch: 8.21 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35111311339679435		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 0.45094460338789066		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 0.40102885839234254 | validation: 0.16231526107202354]
	TIME [epoch: 8.19 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2722534136184689		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 0.38011767086595327		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 0.32618554224221114 | validation: 0.4189861674173974]
	TIME [epoch: 8.19 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6610801303829132		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 0.4019965124993844		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 0.5315383214411488 | validation: 0.2311146575792457]
	TIME [epoch: 8.19 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43242045495773807		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.30677391651476305		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.36959718573625067 | validation: 0.2650296509868546]
	TIME [epoch: 8.22 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.339032661390937		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.2970301352552644		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.3180313983231007 | validation: 0.4508325061401167]
	TIME [epoch: 8.2 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37646321760844537		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.34556375270286355		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.3610134851556544 | validation: 0.14136711171705557]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3034702456527221		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.3007470179460922		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.3021086317994071 | validation: 0.16826006181672537]
	TIME [epoch: 8.22 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3147252901063776		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.2593623248403901		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.2870438074733838 | validation: 0.38138356608402646]
	TIME [epoch: 8.23 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5064083677045544		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.5097129582018628		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.5080606629532086 | validation: 2.4032599085354827]
	TIME [epoch: 8.22 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7864913893333552		[learning rate: 0.006407]
		[batch 20/20] avg loss: 0.5079216848854923		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.6472065371094238 | validation: 0.4176845607914938]
	TIME [epoch: 8.21 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36098275326902496		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.3677800677509114		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.36438141050996814 | validation: 0.17218370728295052]
	TIME [epoch: 8.22 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27227701869920024		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.3541312273021055		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.31320412300065287 | validation: 0.5622619128594843]
	TIME [epoch: 8.24 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3174292053733579		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.3245044002712714		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.3209668028223147 | validation: 0.7402219067669471]
	TIME [epoch: 8.22 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.50892281660208		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.34959060331457015		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.4292567099583252 | validation: 0.2756702012816316]
	TIME [epoch: 8.21 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32148760019618916		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.41249444264006696		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.3669910214181281 | validation: 0.29026593878297274]
	TIME [epoch: 8.21 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45004751650160424		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 0.2783357859060266		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.3641916512038154 | validation: 0.15779423029082684]
	TIME [epoch: 8.23 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25730351677265406		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.3416323773940452		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.2994679470833496 | validation: 0.3844650518579429]
	TIME [epoch: 8.22 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3235117826082949		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.30246699286900436		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.3129893877386495 | validation: 0.19462996180424832]
	TIME [epoch: 8.21 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3742912412848785		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.3518328045927886		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.3630620229388334 | validation: 0.25285922794722]
	TIME [epoch: 8.21 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2634936991126748		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.3762443890820643		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.31986904409736955 | validation: 0.34507260395480865]
	TIME [epoch: 8.24 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28838169799379576		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.26979488053740613		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.279088289265601 | validation: 0.18725244073268923]
	TIME [epoch: 8.22 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30150174787652184		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.3057174046093036		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.3036095762429127 | validation: 0.16249095137437744]
	TIME [epoch: 8.22 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2877296064197873		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.2895530251981355		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.28864131580896146 | validation: 0.20723963412883611]
	TIME [epoch: 8.21 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33342155089968756		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.2934639369984828		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.3134427439490851 | validation: 0.13707703099925936]
	TIME [epoch: 8.24 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27099445899837793		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.2807197583408598		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.2758571086696189 | validation: 0.6509637637921786]
	TIME [epoch: 8.23 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.331689727510635		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.25402141986398463		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.29285557368730986 | validation: 0.4025269967579724]
	TIME [epoch: 8.21 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32092748615085026		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.34903115039413957		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.33497931827249494 | validation: 0.13218516662101815]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42978773052930935		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 0.3437863872200603		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 0.38678705887468484 | validation: 0.6954241998430638]
	TIME [epoch: 8.23 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29012709616119065		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 0.2579234381558517		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 0.2740252671585212 | validation: 0.16597716354426303]
	TIME [epoch: 8.21 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30095617453623075		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.36270980153340016		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.3318329880348155 | validation: 0.15827342873450503]
	TIME [epoch: 8.2 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4222901823017948		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.28540961209181404		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.3538498971968045 | validation: 0.12692011663139627]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2980306447384582		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.26495089129743243		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.28149076801794537 | validation: 0.17264738747664204]
	TIME [epoch: 8.23 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34954472913181467		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.3027070011373925		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.3261258651346036 | validation: 0.4215288739481398]
	TIME [epoch: 8.21 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.330675285001767		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.37530324487927114		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.352989264940519 | validation: 0.19607514024779643]
	TIME [epoch: 8.19 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2572975993248742		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.26026234759092104		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.2587799734578976 | validation: 0.26238614633739094]
	TIME [epoch: 8.2 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27810383293053753		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.307015604049366		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.2925597184899517 | validation: 0.3490301174509187]
	TIME [epoch: 8.22 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.303005782210634		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.2677507730847456		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.2853782776476898 | validation: 0.3135691535347995]
	TIME [epoch: 8.21 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3421053553244774		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.6765735746417821		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.5093394649831298 | validation: 1.143660736821992]
	TIME [epoch: 8.21 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8569937448411414		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 0.31896868968634157		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 0.5879812172637415 | validation: 0.27434703262963145]
	TIME [epoch: 8.18 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29412330606676285		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.2724468488709268		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.2832850774688448 | validation: 0.3436939160854473]
	TIME [epoch: 8.23 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4139562294011635		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 0.30336923758532414		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.3586627334932438 | validation: 0.282886200686681]
	TIME [epoch: 8.19 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27742907729982114		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.2694177515061217		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.2734234144029714 | validation: 0.2601845771819933]
	TIME [epoch: 8.21 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2587861056228524		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.3114731329510378		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.28512961928694514 | validation: 0.41508754783335494]
	TIME [epoch: 8.2 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2840952246350876		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.2959606898712685		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.2900279572531781 | validation: 0.31438094089810253]
	TIME [epoch: 8.21 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23634440154332936		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.952892385518244		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.5946183935307867 | validation: 1.5361179957618798]
	TIME [epoch: 8.2 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5123526813826744		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.2764356826360931		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.3943941820093837 | validation: 0.20391015679017224]
	TIME [epoch: 8.2 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2781410377972091		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.26544223278208706		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.2717916352896481 | validation: 0.14766634679635376]
	TIME [epoch: 8.2 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20729915866601106		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.3928994536235098		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.30009930614476044 | validation: 0.15621695159055748]
	TIME [epoch: 8.22 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2945684528468855		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.2410320767798802		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.2678002648133828 | validation: 0.21373724319029122]
	TIME [epoch: 8.2 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34234587632990754		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.3133447407346003		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.32784530853225385 | validation: 0.12572898060465357]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2682452913589312		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 0.25809147775235586		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 0.26316838455564356 | validation: 0.1948003054884459]
	TIME [epoch: 8.22 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24436537215529688		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 0.30255062917362113		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 0.273458000664459 | validation: 0.13855405976622176]
	TIME [epoch: 8.22 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27521094554058934		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 0.21609061629866927		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 0.24565078091962927 | validation: 0.2788535602680069]
	TIME [epoch: 8.2 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2987003190751923		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.28313600012831397		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.2909181596017531 | validation: 0.23504839755769075]
	TIME [epoch: 8.2 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2994641305744824		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.3168147114701679		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.30813942102232517 | validation: 0.24466808111091842]
	TIME [epoch: 8.2 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4765472958044826		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.24854371828324034		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.3625455070438615 | validation: 0.32541891557472913]
	TIME [epoch: 8.23 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.274390081188263		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.2454855626127908		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.25993782190052694 | validation: 0.2347380234075571]
	TIME [epoch: 8.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26208608217960494		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.35649747965788536		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.3092917809187451 | validation: 0.33298167466703377]
	TIME [epoch: 8.19 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2667160774475456		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.32850695744785896		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.2976115174477023 | validation: 0.21993735394237537]
	TIME [epoch: 8.2 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2898156411368552		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.24378719470283036		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.2668014179198428 | validation: 0.47196491518891726]
	TIME [epoch: 8.22 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.268911538548955		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.26406012209979224		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.2664858303243736 | validation: 0.2804021174133827]
	TIME [epoch: 8.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29996257165862017		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.28698334806518616		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.2934729598619032 | validation: 0.38914563646248673]
	TIME [epoch: 8.2 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30321399907157665		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.3092133940181482		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.3062136965448624 | validation: 0.3526494915786852]
	TIME [epoch: 8.2 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2792672474931321		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.2877018755849056		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.2834845615390189 | validation: 0.16412344938475776]
	TIME [epoch: 8.21 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.288190731643077		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.2308288457973283		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.2595097887202027 | validation: 0.31015805759456455]
	TIME [epoch: 8.21 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2901784191273401		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.220780026054661		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.25547922259100053 | validation: 0.11450436283675008]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2644605946775618		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.32599964334154163		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.29523011900955176 | validation: 0.18098482550994255]
	TIME [epoch: 8.21 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.355513565385752		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.1983263762949912		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.2769199708403716 | validation: 0.22957504213761012]
	TIME [epoch: 8.23 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2723873636280457		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.23552233924397487		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.2539548514360103 | validation: 0.197096626188035]
	TIME [epoch: 8.2 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2581715153745524		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.25834891616984235		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.2582602157721973 | validation: 0.23384793115041325]
	TIME [epoch: 8.19 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26784784368547837		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.27524420734298716		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.27154602551423274 | validation: 0.5948669622784034]
	TIME [epoch: 8.19 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29889850779173127		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.2161572558668762		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.2575278818293037 | validation: 0.2395496759421313]
	TIME [epoch: 8.22 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24727796901764726		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.2073566775865971		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.22731732330212212 | validation: 0.2926064351480453]
	TIME [epoch: 8.19 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2577453069254264		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.26742803441809493		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.26258667067176067 | validation: 0.44839940031180187]
	TIME [epoch: 8.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27261636035946935		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.30787944814773593		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.2902479042536027 | validation: 0.2659070715773032]
	TIME [epoch: 8.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30259522333020267		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.24414634016519726		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.27337078174769996 | validation: 0.2326022223788325]
	TIME [epoch: 8.21 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22588924316401693		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.22449237348659284		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.22519080832530483 | validation: 0.2576174981279023]
	TIME [epoch: 8.19 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2793542051117026		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.25006749753043817		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.2647108513210704 | validation: 0.13515674150882934]
	TIME [epoch: 8.21 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21522196686141704		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.22091212975931743		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.21806704831036722 | validation: 0.25480751656077105]
	TIME [epoch: 8.2 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1980707674016706		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.2526527917973678		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.22536177959951925 | validation: 0.14096785456247513]
	TIME [epoch: 8.24 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23546262476923774		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.260874960911559		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.2481687928403984 | validation: 0.31514143826596863]
	TIME [epoch: 8.2 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2325204303583665		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.23321651974700006		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.23286847505268327 | validation: 0.2804752589334235]
	TIME [epoch: 8.2 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25086225539936857		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.24252266500133354		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.24669246020035102 | validation: 0.3124174304260039]
	TIME [epoch: 8.2 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25379486065299645		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.24223487681277517		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.2480148687328858 | validation: 0.29411348101746615]
	TIME [epoch: 8.22 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24531830854223413		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.26409165190583506		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.25470498022403465 | validation: 0.23913701885032196]
	TIME [epoch: 8.21 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2345921011828406		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.22929208845068		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.2319420948167603 | validation: 0.3041893619470768]
	TIME [epoch: 8.19 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32331740669359305		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 0.27622304959550964		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.2997702281445513 | validation: 0.23614367382139914]
	TIME [epoch: 8.2 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3886303701922807		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.229200717582949		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.3089155438876147 | validation: 0.14649899049103648]
	TIME [epoch: 8.22 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24771039679813184		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.272473740875327		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.2600920688367294 | validation: 0.45990039836293983]
	TIME [epoch: 8.21 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3015893716357753		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.27457667801146785		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.2880830248236216 | validation: 0.2724999991442847]
	TIME [epoch: 8.2 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28403526134507107		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.3121355106880982		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.2980853860165846 | validation: 0.3033874655744161]
	TIME [epoch: 8.2 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2839324180162871		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.2708274239872448		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.277379921001766 | validation: 0.09212976416699384]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1854542731918625		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.21266192203131426		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.19905809761158838 | validation: 0.205411583333167]
	TIME [epoch: 8.22 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2227557629556		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.2640262810897557		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.24339102202267782 | validation: 0.30577620479122025]
	TIME [epoch: 8.19 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28773030087505636		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.2599926088939771		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.2738614548845167 | validation: 0.18043678149765932]
	TIME [epoch: 8.2 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23550497721764357		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.26397570153456923		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.24974033937610646 | validation: 0.1878895872223501]
	TIME [epoch: 8.2 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23380276052494703		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.24321446864833485		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.23850861458664094 | validation: 0.2642832701579136]
	TIME [epoch: 8.21 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21510626960881898		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.23931415476666035		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.22721021218773965 | validation: 0.3972962262121088]
	TIME [epoch: 8.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3890308046516421		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.2567557466016263		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.3228932756266342 | validation: 0.2407542266115753]
	TIME [epoch: 8.2 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2521052742613489		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.2835064025536871		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.267805838407518 | validation: 0.20458880297966514]
	TIME [epoch: 8.2 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22124794206330384		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.23368962647026192		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.2274687842667829 | validation: 0.1586262574324147]
	TIME [epoch: 8.2 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3033998971586883		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.22566385102565123		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.2645318740921698 | validation: 0.27208500507577604]
	TIME [epoch: 8.19 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23809690704921693		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.30778010890892504		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.27293850797907104 | validation: 0.18786519801091212]
	TIME [epoch: 8.22 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19563541894149583		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.2918329671279141		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.24373419303470492 | validation: 0.1938202189374932]
	TIME [epoch: 8.22 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22395735646991305		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.25500506396394285		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.23948121021692792 | validation: 0.22211014986485061]
	TIME [epoch: 8.24 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24654327186438324		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.2167206031735775		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.23163193751898037 | validation: 0.1603631288659519]
	TIME [epoch: 8.21 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3635219454507432		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.24529949029237166		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.3044107178715575 | validation: 0.24139587419907987]
	TIME [epoch: 8.22 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21696070498912548		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.3222508502621223		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.2696057776256239 | validation: 0.18896655937593884]
	TIME [epoch: 8.21 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22668801123348117		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.2500390955428924		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.2383635533881868 | validation: 0.2943405403674738]
	TIME [epoch: 8.22 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2465039361458964		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.20971684077399413		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.22811038845994522 | validation: 0.33396995195487356]
	TIME [epoch: 8.21 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2685601759153412		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.265455753953176		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.2670079649342586 | validation: 0.19784732614634126]
	TIME [epoch: 8.21 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24348557699353002		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.2660113222827717		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.2547484496381509 | validation: 0.2367559973695532]
	TIME [epoch: 8.2 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24136266207879092		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.19275956559533275		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.2170611138370618 | validation: 0.1944147204837113]
	TIME [epoch: 8.22 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23280497263788552		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.23492569803594301		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.23386533533691428 | validation: 0.14611577707445264]
	TIME [epoch: 8.2 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16465228152753755		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.2405092998791602		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.20258079070334892 | validation: 0.1732511043470433]
	TIME [epoch: 8.2 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26834007948174415		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.23009206898518003		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.24921607423346206 | validation: 0.18560585134956223]
	TIME [epoch: 8.2 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21587376291539564		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.2417245751628819		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.22879916903913883 | validation: 0.1146595428275748]
	TIME [epoch: 8.22 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23759734556690573		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.26691564574603555		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.2522564956564706 | validation: 0.21843304494158325]
	TIME [epoch: 8.19 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2224555720389544		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.24790808283117075		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.2351818274350626 | validation: 0.2477627810226335]
	TIME [epoch: 8.19 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3374180426186761		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.21930426875658232		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.2783611556876292 | validation: 0.2646054537476529]
	TIME [epoch: 8.19 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20862163756223318		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.2063741366289041		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.20749788709556874 | validation: 0.19130275573298572]
	TIME [epoch: 8.22 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2360371032646456		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.19770276690155095		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.21686993508309826 | validation: 0.21210826434837138]
	TIME [epoch: 8.2 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2080549887015844		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.22547280585062915		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.2167638972761068 | validation: 0.2044186104485366]
	TIME [epoch: 8.2 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25728431414168884		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.24452349713012836		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.25090390563590864 | validation: 0.23495347581942747]
	TIME [epoch: 8.19 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2013862697919285		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.25883246080803746		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.230109365299983 | validation: 0.12629608140275414]
	TIME [epoch: 8.22 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2199949265894677		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.22310404324463745		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.22154948491705256 | validation: 0.2400416275118092]
	TIME [epoch: 8.2 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31003291533680216		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 0.26496195632274355		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.2874974358297729 | validation: 0.15771966364329654]
	TIME [epoch: 8.19 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20489856061047038		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.21085707641174248		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 0.20787781851110645 | validation: 0.1745091773151886]
	TIME [epoch: 8.19 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2477206328023434		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.20199502690976817		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.2248578298560558 | validation: 0.1801918850182415]
	TIME [epoch: 8.22 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3247700993854369		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.19481667355739934		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.2597933864714181 | validation: 0.12092597495735326]
	TIME [epoch: 8.19 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19447986704655165		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 0.21989934144467585		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 0.20718960424561375 | validation: 0.15679313304717435]
	TIME [epoch: 8.19 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18899194269831174		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.21742239391745866		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.20320716830788518 | validation: 0.1505692000123843]
	TIME [epoch: 8.2 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19372360808475853		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.1934500276967142		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.19358681789073634 | validation: 0.14325240702837394]
	TIME [epoch: 8.22 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19784544747715088		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.20809648639382491		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 0.2029709669354879 | validation: 0.13733268900808537]
	TIME [epoch: 8.21 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21509727931614786		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.19836403750411205		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.2067306584101299 | validation: 0.18661027872474964]
	TIME [epoch: 8.19 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2113224899819651		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.3187565760913621		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.2650395330366636 | validation: 0.16154777776401896]
	TIME [epoch: 8.19 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2328706169497588		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.2531133700943278		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.24299199352204331 | validation: 0.21040222994219349]
	TIME [epoch: 8.21 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2253015538448276		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.21103532641729256		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 0.2181684401310601 | validation: 0.1414693948453626]
	TIME [epoch: 8.19 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.252535861707031		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 0.1921241642317227		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 0.22233001296937688 | validation: 0.14247149260325329]
	TIME [epoch: 8.2 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2307919347238913		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.20166431770140597		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.2162281262126487 | validation: 0.16453301663283854]
	TIME [epoch: 8.2 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22961260829790647		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.22619595562135425		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.22790428195963036 | validation: 0.17714493516288582]
	TIME [epoch: 8.21 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19582799303028645		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.17770500637076375		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.18676649970052508 | validation: 0.19821119731478107]
	TIME [epoch: 8.2 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21468061403371602		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 0.18861691855236462		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 0.20164876629304032 | validation: 0.16089276829333427]
	TIME [epoch: 8.2 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18853472801953752		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.18808757359975053		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.18831115080964406 | validation: 0.20971025775686436]
	TIME [epoch: 8.19 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19611723683573684		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.31718641006887816		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.2566518234523075 | validation: 0.1614998754931559]
	TIME [epoch: 8.21 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21759063264394046		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 0.18463770606928326		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.20111416935661186 | validation: 0.09124017315285188]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21147546572892312		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.1892065087737817		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.2003409872513524 | validation: 0.30409660088564616]
	TIME [epoch: 8.2 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2170154010529856		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.22639872454417526		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.22170706279858043 | validation: 0.14084706282940712]
	TIME [epoch: 8.17 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2570781912692378		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.2755796055471528		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.2663288984081953 | validation: 0.11882450587346756]
	TIME [epoch: 8.2 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19896835717894984		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 0.3598021484191226		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.27938525279903625 | validation: 0.2317646666854531]
	TIME [epoch: 8.19 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22079413026896888		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 0.1723467897933676		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 0.19657046003116824 | validation: 0.4545982692867772]
	TIME [epoch: 8.19 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2777931534518289		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 0.2503350003233852		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 0.2640640768876071 | validation: 0.16314395971357543]
	TIME [epoch: 8.19 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22949402472185815		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 0.23649024553013556		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 0.23299213512599692 | validation: 0.14205791662508427]
	TIME [epoch: 8.21 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16501450178161745		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.18781385669771003		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.17641417923966377 | validation: 0.18654898319217014]
	TIME [epoch: 8.19 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22227287240020943		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.19970596201878568		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.21098941720949757 | validation: 0.1676812644211647]
	TIME [epoch: 8.19 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45835475386498636		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.28182908687590286		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.37009192037044464 | validation: 0.22989426634476465]
	TIME [epoch: 8.19 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.557142068827716		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 0.21729370235762838		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 0.3872178855926723 | validation: 0.15015362017676626]
	TIME [epoch: 8.22 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22083075821037013		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 0.20277342325335113		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 0.21180209073186057 | validation: 0.16694600754215988]
	TIME [epoch: 8.18 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19074707685337142		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 0.1698750236537003		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 0.18031105025353586 | validation: 0.1268618903795281]
	TIME [epoch: 8.18 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21547355383796446		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 0.4019105846305891		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 0.3086920692342767 | validation: 0.6022109096475965]
	TIME [epoch: 8.19 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32052606689738256		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 0.20063794584600414		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 0.2605820063716934 | validation: 0.1453027657033017]
	TIME [epoch: 8.21 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18148166792425705		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 0.20459227591650056		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 0.1930369719203788 | validation: 0.5450494681121913]
	TIME [epoch: 8.19 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24694088910167408		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 0.22544310863663797		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 0.2361919988691561 | validation: 0.24625860230569255]
	TIME [epoch: 8.2 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3432856176238272		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 0.18498958735207177		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 0.26413760248794954 | validation: 0.13766913820235205]
	TIME [epoch: 8.19 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2100441885469703		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 0.18812605709380664		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 0.19908512282038848 | validation: 0.14727930178807186]
	TIME [epoch: 8.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22306567641984826		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 0.2291718006982209		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 0.22611873855903455 | validation: 0.24016455748224408]
	TIME [epoch: 8.2 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25094391114410175		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 0.23185781441278638		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 0.24140086277844408 | validation: 0.1585367343393979]
	TIME [epoch: 8.19 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17267225279598855		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 0.2625280728244316		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 0.21760016281021 | validation: 0.17890679174867102]
	TIME [epoch: 8.19 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2891591217618046		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 0.19372118827254897		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 0.2414401550171768 | validation: 0.09489459852162417]
	TIME [epoch: 8.22 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1625134201058361		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 0.21185262015882333		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 0.18718302013232974 | validation: 0.1232343589520046]
	TIME [epoch: 8.21 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1899162806779828		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 0.15986597750312176		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 0.17489112909055227 | validation: 0.15599507818947791]
	TIME [epoch: 8.18 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2278890010680083		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 0.17792453936559377		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 0.2029067702168011 | validation: 0.17513346562238458]
	TIME [epoch: 8.2 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25346420570219974		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 0.21490175386772786		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 0.2341829797849638 | validation: 0.1686722113086275]
	TIME [epoch: 8.23 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18740376996859548		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 0.38860525640410387		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 0.2880045131863496 | validation: 0.6219568119494249]
	TIME [epoch: 8.2 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3206923954296351		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 0.21018407163755973		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 0.26543823353359747 | validation: 0.22870498743720763]
	TIME [epoch: 8.19 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21836008193358708		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 0.19758751774636724		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 0.20797379983997716 | validation: 0.18161406651461748]
	TIME [epoch: 8.18 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1983483400057747		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 0.45977505386391576		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 0.3290616969348452 | validation: 0.3015214806584655]
	TIME [epoch: 8.21 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.293550901809977		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 0.16336076138996156		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 0.22845583159996927 | validation: 0.1263327907409651]
	TIME [epoch: 8.19 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2090374001098124		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 0.23456940980239294		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 0.22180340495610268 | validation: 0.8481468816312729]
	TIME [epoch: 8.19 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.362516119579257		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 0.3181920626498215		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 0.3403540911145393 | validation: 0.16352545480798514]
	TIME [epoch: 8.18 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2597909598406491		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 0.41485589656816346		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 0.3373234282044063 | validation: 0.5186690923633659]
	TIME [epoch: 8.22 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3336193105322348		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 0.24631255808812869		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 0.2899659343101817 | validation: 0.23550570157505143]
	TIME [epoch: 8.21 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23699951873397476		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 0.24595711323649577		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 0.24147831598523523 | validation: 0.4720592914184432]
	TIME [epoch: 8.19 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6276300014419782		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 0.2584247729882722		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 0.44302738721512525 | validation: 0.17545600773178105]
	TIME [epoch: 8.19 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23184803113893793		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 0.3098416968963905		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 0.2708448640176643 | validation: 0.16255527249693286]
	TIME [epoch: 8.2 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2254671265682949		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 0.35982447736050577		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 0.2926458019644004 | validation: 0.14770442858960192]
	TIME [epoch: 8.2 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22905000820273816		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 0.4819441374067061		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 0.35549707280472215 | validation: 0.1984971577532354]
	TIME [epoch: 8.19 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24458758824925741		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 0.35100458064469964		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 0.2977960844469785 | validation: 0.1942500204009203]
	TIME [epoch: 8.19 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22353187982959116		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 0.2923408064792014		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 0.2579363431543963 | validation: 0.14624433664588155]
	TIME [epoch: 8.19 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2282479784347816		[learning rate: 0.00333]
		[batch 20/20] avg loss: 0.21627030359603702		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 0.22225914101540933 | validation: 0.13600184193649287]
	TIME [epoch: 8.21 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29256279284462255		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 0.6977979654506606		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 0.49518037914764157 | validation: 0.3533387196177957]
	TIME [epoch: 8.19 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36560153788689964		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 0.42269936940330827		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 0.3941504536451039 | validation: 0.7274131211866338]
	TIME [epoch: 8.18 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36524440945604153		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 0.26856991675681186		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 0.31690716310642675 | validation: 0.20073786355764076]
	TIME [epoch: 8.2 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2848712265970982		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 0.3066478447395484		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 0.29575953566832325 | validation: 0.16680288279493125]
	TIME [epoch: 8.21 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2990697877466486		[learning rate: 0.00327]
		[batch 20/20] avg loss: 0.26352897635294303		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 0.2812993820497958 | validation: 0.15851434431503858]
	TIME [epoch: 8.19 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24630846889094923		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 0.3168576800185522		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 0.2815830744547507 | validation: 0.22567709747040898]
	TIME [epoch: 8.18 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24843253415720276		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 1.0042153493415031		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 0.626323941749353 | validation: 0.24128950262334922]
	TIME [epoch: 8.19 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35973494581340937		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 0.25986357505572144		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 0.3097992604345654 | validation: 0.1582009350727745]
	TIME [epoch: 8.22 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24514111664087346		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 0.24506412977325936		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 0.24510262320706647 | validation: 0.199312310877268]
	TIME [epoch: 8.19 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.76341817319382		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 0.5953225775735285		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 0.6793703753836743 | validation: 0.28086203827700384]
	TIME [epoch: 8.2 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24044774134019958		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 0.23590664906075873		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 0.23817719520047914 | validation: 0.15108149012654165]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4384193277068585		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 0.5622067626751793		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 0.500313045191019 | validation: 0.169918875562356]
	TIME [epoch: 8.22 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26538976785643864		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 0.33521807734430825		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 0.3003039226003735 | validation: 0.16253955050182048]
	TIME [epoch: 8.19 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33706930348747305		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 0.21301393054530635		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 0.2750416170163897 | validation: 0.2062062560837761]
	TIME [epoch: 8.18 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35926883266096676		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 0.2800962816643594		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 0.31968255716266314 | validation: 0.24834458940951218]
	TIME [epoch: 8.19 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2590114924457271		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 0.29799247393942185		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 0.27850198319257446 | validation: 0.17267555486808342]
	TIME [epoch: 8.21 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24132756441566494		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 0.2748533835764216		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 0.25809047399604323 | validation: 0.18155905280605236]
	TIME [epoch: 8.19 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25637540826951716		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 0.2673372339934706		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 0.2618563211314938 | validation: 0.1716618853305239]
	TIME [epoch: 8.18 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2912005993682844		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 0.24934413401235642		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 0.2702723666903204 | validation: 0.2553299981434217]
	TIME [epoch: 8.19 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47907668506116874		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 0.2308594635161146		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 0.3549680742886417 | validation: 0.12639156476062807]
	TIME [epoch: 8.22 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21784638897372915		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 0.31223684105653754		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 0.26504161501513335 | validation: 0.18245104754047592]
	TIME [epoch: 8.2 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43054044811437536		[learning rate: 0.003074]
		[batch 20/20] avg loss: 0.2995677878160596		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 0.3650541179652175 | validation: 0.27532576248159435]
	TIME [epoch: 8.19 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2071186625680105		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 0.23138973127271534		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 0.2192541969203629 | validation: 0.19598964066015795]
	TIME [epoch: 8.18 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20763280637827658		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 0.27241718034504486		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 0.24002499336166067 | validation: 0.11717754304159599]
	TIME [epoch: 8.21 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.427750428088807		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 0.3627155754772535		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 0.3952330017830302 | validation: 0.12053119877809237]
	TIME [epoch: 8.19 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23219751879853684		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 0.2943782858066123		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 0.26328790230257465 | validation: 0.5408292090772031]
	TIME [epoch: 8.18 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30508941864617956		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 0.20649010402304593		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 0.2557897613346128 | validation: 0.25612776064681725]
	TIME [epoch: 8.19 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20164229487819746		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 0.25392239544664463		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 0.22778234516242102 | validation: 0.2710972430297819]
	TIME [epoch: 8.22 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2691614841413911		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 0.22627668804552128		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 0.24771908609345625 | validation: 0.24229814919929113]
	TIME [epoch: 8.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21150802338657337		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 0.24225966890187314		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 0.22688384614422324 | validation: 0.15851574941100172]
	TIME [epoch: 8.19 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25633199491398095		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 0.38158575714206666		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 0.31895887602802375 | validation: 0.15280846653052949]
	TIME [epoch: 8.19 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20351958692712682		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 0.28776833461818063		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 0.24564396077265377 | validation: 0.6028588938919058]
	TIME [epoch: 8.22 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6774006604708871		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 0.22738431991457203		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 0.45239249019272954 | validation: 0.17424328687930687]
	TIME [epoch: 8.19 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28257918011280203		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 0.45365011031253316		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 0.3681146452126676 | validation: 0.35647789352468345]
	TIME [epoch: 8.19 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1303765466295437		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 1.3718375492284844		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 1.251107047929014 | validation: 0.679408709204953]
	TIME [epoch: 8.19 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7944592852883903		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 0.3187577369809693		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 0.5566085111346799 | validation: 0.20183539455596716]
	TIME [epoch: 8.21 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2383026715877143		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 0.30429964642910456		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 0.27130115900840945 | validation: 0.21192378498144382]
	TIME [epoch: 8.18 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5226368160266776		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 0.3192369796845983		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 0.42093689785563787 | validation: 0.2490723979415123]
	TIME [epoch: 8.2 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5253756288692455		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 4.210041644251538		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 2.8677086365603923 | validation: 2.3574281745672043]
	TIME [epoch: 8.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1442318028655714		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 1.9390112232566967		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 2.041621513061134 | validation: 1.641991625113436]
	TIME [epoch: 8.22 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7239429127163747		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 0.23425460626788613		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 0.47909875949213054 | validation: 0.2069715582146999]
	TIME [epoch: 8.2 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2827256021576926		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 0.23178207971831818		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 0.2572538409380054 | validation: 0.13955487270274958]
	TIME [epoch: 8.18 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23505195135324963		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 0.274618774755071		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 0.2548353630541603 | validation: 0.23566710215616737]
	TIME [epoch: 8.19 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2364601766437739		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 0.24696664200367593		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 0.2417134093237249 | validation: 0.18133412130714588]
	TIME [epoch: 8.21 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.252777880736495		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 0.42292422606599606		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 0.33785105340124555 | validation: 0.23005296300688186]
	TIME [epoch: 8.19 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.469219876267544		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 3.383050334300459		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 2.4261351052840014 | validation: 1.114114500641944]
	TIME [epoch: 8.19 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6896375615351182		[learning rate: 0.002807]
		[batch 20/20] avg loss: 4.890661590412519		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 3.790149575973819 | validation: 5.273288864441487]
	TIME [epoch: 8.19 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.602034457378723		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 5.834766716528184		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 5.718400586953452 | validation: 5.144133774705646]
	TIME [epoch: 8.21 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.023245668829268		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 5.372226420040898		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 5.197736044435083 | validation: 5.3880957935034015]
	TIME [epoch: 8.19 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.736064830102589		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 5.549320667932324		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 5.642692749017458 | validation: 5.143673502658359]
	TIME [epoch: 8.18 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.1662088646557525		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 4.694141911941458		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 4.930175388298604 | validation: 4.904080988674678]
	TIME [epoch: 8.19 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.285919625078828		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 5.27742380300908		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 5.281671714043954 | validation: 5.59639463806538]
	TIME [epoch: 8.21 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.618537099708874		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 5.783153622349323		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 5.7008453610290974 | validation: 6.104187907500147]
	TIME [epoch: 8.2 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.294306154345242		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 6.529687572044173		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 6.411996863194709 | validation: 6.100055328397561]
	TIME [epoch: 8.19 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.973801893168492		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 6.120734576080085		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 6.047268234624289 | validation: 5.7024768594129185]
	TIME [epoch: 8.18 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.818946403251661		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 5.906061717135345		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 5.862504060193503 | validation: 5.77698929608724]
	TIME [epoch: 8.21 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.03648004729349		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 5.791663748788962		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 5.914071898041224 | validation: 5.581097395314916]
	TIME [epoch: 8.19 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.887211451911641		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 4.9248697250024165		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 5.406040588457029 | validation: 4.82982472189845]
	TIME [epoch: 8.19 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.095610871929795		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 4.027858871340927		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 4.06173487163536 | validation: 3.9594101150116945]
	TIME [epoch: 8.18 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.881731760485363		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 5.482304430887394		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 5.182018095686379 | validation: 5.151189357661211]
	TIME [epoch: 8.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.331213389164245		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 5.087051161561622		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 5.209132275362932 | validation: 4.842770198474812]
	TIME [epoch: 8.19 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.615132133457363		[learning rate: 0.002658]
		[batch 20/20] avg loss: 2.0994475810515256		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 3.3572898572544445 | validation: 0.9550713047626218]
	TIME [epoch: 8.19 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8475326279321711		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 0.46026162844700025		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 0.6538971281895857 | validation: 0.38615843810588485]
	TIME [epoch: 8.19 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36943678007256325		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 0.4036106208921285		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 0.38652370048234586 | validation: 0.3401965058312481]
	TIME [epoch: 8.21 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.356751942611388		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 0.28376703223671784		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 0.3202594874240529 | validation: 0.1680595800351613]
	TIME [epoch: 8.19 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29485204711559854		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 0.3120562009967604		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 0.3034541240561795 | validation: 0.1660981443486852]
	TIME [epoch: 8.19 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3410939811078426		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 0.3035810667083059		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 0.3223375239080742 | validation: 0.2947052578383285]
	TIME [epoch: 8.19 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24862350820782134		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 0.28645784399310176		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 0.2675406761004616 | validation: 0.2815528518184115]
	TIME [epoch: 8.21 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26693094907866494		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 0.26355512751091953		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 0.26524303829479223 | validation: 0.14478325860122032]
	TIME [epoch: 8.19 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2702060875322329		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 0.3202334338085159		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 0.2952197606703744 | validation: 0.2376828787220261]
	TIME [epoch: 8.19 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2451491699453851		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 0.23935763323870668		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 0.24225340159204584 | validation: 0.2906121283140035]
	TIME [epoch: 8.19 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2506710873989593		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.23612688286386355		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.2433989851314114 | validation: 0.15088541664441557]
	TIME [epoch: 8.2 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2136487031707023		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.2445718361156651		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.22911026964318376 | validation: 0.29298148504246835]
	TIME [epoch: 8.2 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25099766111111266		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.19099730453112507		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.22099748282111883 | validation: 0.12020647986499072]
	TIME [epoch: 8.2 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22891172282496308		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.28736562511800406		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.25813867397148355 | validation: 0.20948222819960455]
	TIME [epoch: 8.19 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2399414112142672		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.1916372858376454		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.21578934852595627 | validation: 0.11340327846227907]
	TIME [epoch: 8.2 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21790969529287513		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.24317026496595567		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.2305399801294154 | validation: 0.25653209118790654]
	TIME [epoch: 8.21 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21161729617715674		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.2079706298289837		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.20979396300307024 | validation: 0.18856538633113223]
	TIME [epoch: 8.2 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18139966996916349		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.2715347342103313		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.2264672020897474 | validation: 0.28098289099009954]
	TIME [epoch: 8.19 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22960026944300238		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.3976565150328335		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.313628392237918 | validation: 0.4377357784963216]
	TIME [epoch: 8.2 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3166499083889696		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.1951766179729334		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.2559132631809515 | validation: 0.227661340301323]
	TIME [epoch: 8.21 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2737704544064558		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.22536228662054594		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.24956637051350086 | validation: 0.17930434841315626]
	TIME [epoch: 8.21 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27752059294493814		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.18745686100842848		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.23248872697668327 | validation: 0.1135608094859325]
	TIME [epoch: 8.19 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23194616555705166		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.18773744562527078		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.20984180559116122 | validation: 0.15648203831526902]
	TIME [epoch: 8.2 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23756206412199954		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.2559782592268811		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.2467701616744403 | validation: 0.09270809564432594]
	TIME [epoch: 8.21 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1833957249071227		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.21499029324426527		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.19919300907569398 | validation: 0.2683103583526559]
	TIME [epoch: 8.19 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2350032824132567		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.2754854568408765		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.2552443696270666 | validation: 0.341378278902178]
	TIME [epoch: 8.19 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29578564378095284		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.29872646411794757		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.29725605394945026 | validation: 0.17713985164641172]
	TIME [epoch: 8.19 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2512924048618824		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.22368120220427853		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.23748680353308052 | validation: 0.14032100561791247]
	TIME [epoch: 8.22 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20532003543245642		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.19079675133990756		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.198058393386182 | validation: 0.1826171814409557]
	TIME [epoch: 8.19 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18199053056173237		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.23971946629533872		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.2108549984285355 | validation: 0.11881659450616378]
	TIME [epoch: 8.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23079129353519345		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.2653069783591239		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.2480491359471587 | validation: 0.12739704458416545]
	TIME [epoch: 8.19 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1915493946588845		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.19400518177387643		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.1927772882163804 | validation: 0.2711514629669115]
	TIME [epoch: 8.22 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23582582493685242		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.2101341773088655		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.22298000112285896 | validation: 0.148645761819219]
	TIME [epoch: 8.19 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1874797534013598		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.18137420401879917		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.18442697871007954 | validation: 0.1130613620789871]
	TIME [epoch: 8.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1547535559145051		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.2176976941764349		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.18622562504547 | validation: 0.15866548226971444]
	TIME [epoch: 8.19 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16661416058814726		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.23538027618279295		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.2009972183854701 | validation: 0.12109920934176456]
	TIME [epoch: 8.22 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21942873913083663		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.22534653791546813		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.22238763852315238 | validation: 0.13941271689596546]
	TIME [epoch: 8.21 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19442530005655803		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.19553615648082995		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.19498072826869398 | validation: 0.10483536234471963]
	TIME [epoch: 8.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2313743797768526		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.261889431415897		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.24663190559637477 | validation: 0.3616467372916501]
	TIME [epoch: 8.19 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23734006326430604		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.17733417984174052		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.2073371215530233 | validation: 0.08710743436098706]
	TIME [epoch: 8.22 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16289761539060665		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.19545911013193526		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.179178362761271 | validation: 0.17537670169415734]
	TIME [epoch: 8.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17207053543088469		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.25875893473165074		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.2154147350812677 | validation: 0.197106027643286]
	TIME [epoch: 8.19 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1899559202489197		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.19011870788637597		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.19003731406764782 | validation: 0.12692034875704616]
	TIME [epoch: 8.19 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2501538073207491		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.1548267638416728		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.20249028558121096 | validation: 0.08468451851616926]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1550841291505173		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.1555354282198903		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.15530977868520376 | validation: 0.23636993340785867]
	TIME [epoch: 8.22 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22924621269057371		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.5119633765382577		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.3706047946144157 | validation: 0.15565985954696646]
	TIME [epoch: 8.21 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16196875571653485		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.2421404301553538		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.2020545929359443 | validation: 0.271742011757949]
	TIME [epoch: 8.2 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20053601811121294		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.17030749596272066		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.18542175703696678 | validation: 0.14466804311467957]
	TIME [epoch: 8.21 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2602645907751817		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.19222474293541705		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.22624466685529937 | validation: 0.0949351065489395]
	TIME [epoch: 8.19 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1909142176082873		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.22636664333629267		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.20864043047229003 | validation: 0.16938054879305234]
	TIME [epoch: 8.19 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25178459425898553		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.1493680979541528		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.20057634610656913 | validation: 0.10029403542197943]
	TIME [epoch: 8.19 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2295182589986881		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.1577455594458062		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.1936319092222471 | validation: 0.26032490618875465]
	TIME [epoch: 8.21 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20892378383076604		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.18459209914808064		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.19675794148942333 | validation: 0.16123575126070785]
	TIME [epoch: 8.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2002945030052042		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.21911656552528053		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.20970553426524238 | validation: 0.23033057234448168]
	TIME [epoch: 8.19 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19547900489526981		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.1911337013413026		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.1933063531182862 | validation: 0.16652502664951788]
	TIME [epoch: 8.19 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21311001062709325		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.18168600612284452		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.19739800837496885 | validation: 0.14783639821380198]
	TIME [epoch: 8.21 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21385256693020654		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.21142690197130132		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.21263973445075393 | validation: 0.20808411188948608]
	TIME [epoch: 8.19 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2040751631320191		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.22517775592293007		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.21462645952747458 | validation: 0.1429677625795446]
	TIME [epoch: 8.19 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16405824890878282		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.23533836871748873		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.19969830881313574 | validation: 0.1391319792253466]
	TIME [epoch: 8.19 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22975248692011246		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.2292378205893053		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.2294951537547088 | validation: 0.2047991530549243]
	TIME [epoch: 8.22 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19514803114903773		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.1726079800096132		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.1838780055793255 | validation: 0.1144526377055951]
	TIME [epoch: 8.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17728386664860135		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.2075848025523416		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.1924343346004715 | validation: 0.20298156555598124]
	TIME [epoch: 8.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16415932078920475		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.1936836938026748		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.17892150729593975 | validation: 0.16994608306291742]
	TIME [epoch: 8.19 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2588927003443734		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.17185628884115758		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.2153744945927655 | validation: 0.098569327564316]
	TIME [epoch: 8.22 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22488698131490564		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.1961919185396075		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.21053944992725654 | validation: 0.1125459125447939]
	TIME [epoch: 8.19 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16306668464237267		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.3006771667305987		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.23187192568648576 | validation: 0.2645837004086033]
	TIME [epoch: 8.19 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31987379691742934		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.33109348153156704		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.32548363922449813 | validation: 0.15220154932959173]
	TIME [epoch: 8.19 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1839535084067993		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.1956654073269477		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.18980945786687353 | validation: 0.11306342320374274]
	TIME [epoch: 8.22 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24530352313280215		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.28581363834582857		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.2655585807393154 | validation: 0.14504257204728394]
	TIME [epoch: 8.19 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20316112549840032		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.17284529585484937		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.18800321067662482 | validation: 0.16866608709025097]
	TIME [epoch: 8.19 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1941515781079423		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.18998573098171587		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.19206865454482908 | validation: 0.21218596117736643]
	TIME [epoch: 8.19 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16623683629059322		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.18171185236513782		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.17397434432786552 | validation: 0.11610011217941746]
	TIME [epoch: 8.21 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16571991017210558		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.29333571135740444		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.22952781076475498 | validation: 0.3135106449923648]
	TIME [epoch: 8.19 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17651353172711864		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.16051098882578935		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.168512260276454 | validation: 0.1846201779806506]
	TIME [epoch: 8.19 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16867118858212185		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.1923945181402452		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.18053285336118352 | validation: 0.15706497093547808]
	TIME [epoch: 8.18 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24826695347202588		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.2421802453482762		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.24522359941015104 | validation: 0.16404726400164094]
	TIME [epoch: 8.21 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17577535876828757		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.19062626656968473		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.18320081266898613 | validation: 0.14248965131657487]
	TIME [epoch: 8.19 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18359173258626058		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.18982622404552463		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.18670897831589262 | validation: 0.10856068005928116]
	TIME [epoch: 8.19 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18137307114662254		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.18920107781976253		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.18528707448319254 | validation: 0.22591877912982908]
	TIME [epoch: 8.18 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2237208561629936		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.18549884822863144		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.20460985219581249 | validation: 0.3291465333384381]
	TIME [epoch: 8.21 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27666623941904933		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.169001191459987		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.2228337154395182 | validation: 0.18695710002007385]
	TIME [epoch: 8.19 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1614807446521129		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.1684017634922305		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.1649412540721717 | validation: 0.10894785813465926]
	TIME [epoch: 8.18 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21265336418556466		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.22847887104348344		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.22056611761452402 | validation: 0.09573520674386979]
	TIME [epoch: 8.19 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16401209796758692		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.14269771026927447		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.1533549041184307 | validation: 0.2404180689004094]
	TIME [epoch: 8.21 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15477018091271952		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.16174082776902188		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.1582555043408707 | validation: 0.2356955990284046]
	TIME [epoch: 8.19 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21706422543690587		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 0.19479346883293353		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.20592884713491966 | validation: 0.2029888405901527]
	TIME [epoch: 8.19 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2349192738016142		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 0.22910055050160785		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 0.2320099121516111 | validation: 0.09815946401992749]
	TIME [epoch: 8.18 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15339131275753298		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 0.20489701536914257		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 0.17914416406333775 | validation: 0.20892363189048757]
	TIME [epoch: 8.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19050575443144316		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 0.20050211448905458		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 0.19550393446024888 | validation: 0.13652693681772843]
	TIME [epoch: 8.19 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2319588657216259		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 0.15852178925017762		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 0.19524032748590175 | validation: 0.11438902142901936]
	TIME [epoch: 8.18 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22826456573241555		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 0.17773530472972576		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 0.20299993523107066 | validation: 0.13369910909655036]
	TIME [epoch: 8.18 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16999155557302376		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 0.17078193808988124		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 0.17038674683145247 | validation: 0.13652107551399803]
	TIME [epoch: 8.21 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2139364134377253		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 0.14830088062840507		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 0.18111864703306518 | validation: 0.09946564265025239]
	TIME [epoch: 8.19 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22060922673747757		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 0.1473748785864745		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 0.18399205266197605 | validation: 0.15154712882696633]
	TIME [epoch: 8.18 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1891773080159956		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 0.12807668334818298		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 0.15862699568208932 | validation: 0.07104474015447738]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19554194160365906		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 0.2234004100300365		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 0.20947117581684785 | validation: 0.11371752773034222]
	TIME [epoch: 8.21 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22479994530152245		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 0.1481010108576135		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 0.18645047807956797 | validation: 0.14815273329107323]
	TIME [epoch: 8.19 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19443975617306045		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 0.17657993429907623		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 0.18550984523606828 | validation: 0.10128755671887874]
	TIME [epoch: 8.18 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28658762457151005		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 0.20932114092327767		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 0.2479543827473938 | validation: 0.14095988111392105]
	TIME [epoch: 8.18 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16181878283876136		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 0.18700414870838517		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 0.1744114657735733 | validation: 0.11325033281982]
	TIME [epoch: 8.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19846339817268618		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 0.2080772042220386		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 0.2032703011973624 | validation: 0.15596893851557175]
	TIME [epoch: 8.18 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17392709745638926		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 0.20359666844211546		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 0.18876188294925236 | validation: 0.1776879039101458]
	TIME [epoch: 8.17 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15370098339969948		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 0.18514260884875544		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 0.16942179612422748 | validation: 0.2146807254999554]
	TIME [epoch: 8.18 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2050385104512331		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 0.16049810786184185		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 0.18276830915653747 | validation: 0.1292989178242779]
	TIME [epoch: 8.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20299897326154387		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 0.19002876818064157		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 0.19651387072109275 | validation: 0.1038963895675746]
	TIME [epoch: 8.18 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18789797251376558		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 0.1914071596005032		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 0.1896525660571344 | validation: 0.11146580529410399]
	TIME [epoch: 8.17 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20695887099648796		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 0.22203384100864582		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 0.21449635600256692 | validation: 0.08804736607399052]
	TIME [epoch: 8.18 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17805153657168443		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 0.14796038379033194		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 0.1630059601810082 | validation: 0.12645540148815645]
	TIME [epoch: 8.19 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15243071223109453		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 0.19094729477489453		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 0.17168900350299454 | validation: 0.1534128217577145]
	TIME [epoch: 8.19 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19076471937149436		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 0.16345265010714233		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 0.17710868473931837 | validation: 0.08277544608481355]
	TIME [epoch: 8.18 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14926748708982315		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 0.19411647910705176		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 0.17169198309843745 | validation: 0.11172931524561515]
	TIME [epoch: 8.18 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20331073283718748		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 0.192083892315366		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 0.19769731257627676 | validation: 0.10996891541408413]
	TIME [epoch: 8.18 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16015107910604479		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 0.17587440192487483		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 0.16801274051545984 | validation: 0.16051441165213826]
	TIME [epoch: 8.2 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2613489367591799		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 0.167967606458785		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 0.21465827160898243 | validation: 0.07879175809879192]
	TIME [epoch: 8.18 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1938523417986102		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 0.13400594389668224		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 0.16392914284764623 | validation: 0.1129049904726139]
	TIME [epoch: 8.18 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17817099675804765		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 0.1528298577645145		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 0.16550042726128106 | validation: 0.3777003921388668]
	TIME [epoch: 8.18 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1991427026125673		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 0.2571091389137756		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 0.22812592076317148 | validation: 0.19602098077743363]
	TIME [epoch: 8.21 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14818908610566778		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 0.2050313038131236		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 0.17661019495939573 | validation: 0.10449650872260645]
	TIME [epoch: 8.18 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16806857672454364		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 0.16020286975228393		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 0.1641357232384138 | validation: 0.37531740354006415]
	TIME [epoch: 8.18 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.303411911941206		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 0.2780086041363316		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 0.2907102580387688 | validation: 0.10395086202952016]
	TIME [epoch: 8.18 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21288244741827925		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 0.1722698186636447		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 0.192576133040962 | validation: 0.09624360525539852]
	TIME [epoch: 8.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19015147594831447		[learning rate: 0.001712]
		[batch 20/20] avg loss: 0.1528765647390794		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 0.17151402034369692 | validation: 0.21180050704282566]
	TIME [epoch: 8.18 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2223715622570031		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 0.16169461983429442		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 0.19203309104564875 | validation: 0.3986471663244909]
	TIME [epoch: 8.18 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2587985448841815		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 0.32524655109555517		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 0.2920225479898683 | validation: 0.15974868473044104]
	TIME [epoch: 8.18 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19232683647595877		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 0.21480230959602659		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 0.20356457303599268 | validation: 0.10209330477428216]
	TIME [epoch: 8.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13761551255347454		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 0.4737848558085663		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 0.3057001841810204 | validation: 0.46811630957166483]
	TIME [epoch: 8.18 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2022943468123644		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 0.21253993190800782		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 0.2074171393601861 | validation: 0.12089605683625682]
	TIME [epoch: 8.18 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2464120027522893		[learning rate: 0.001675]
		[batch 20/20] avg loss: 0.19006231332898058		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 0.21823715804063495 | validation: 0.10379893831875134]
	TIME [epoch: 8.18 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1688142444336818		[learning rate: 0.001669]
		[batch 20/20] avg loss: 0.1482478965291054		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 0.1585310704813936 | validation: 0.09737741958027599]
	TIME [epoch: 8.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1886321582526289		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 0.2901600791707234		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 0.23939611871167615 | validation: 0.1175757713531563]
	TIME [epoch: 8.17 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20850907001269162		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 0.20628012535131543		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 0.20739459768200352 | validation: 0.19319674360506323]
	TIME [epoch: 8.19 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1925517317422914		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 0.17323845258160425		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 0.1828950921619478 | validation: 0.18628753055117891]
	TIME [epoch: 8.18 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21987927453092446		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 0.17389374681118758		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 0.19688651067105603 | validation: 0.09513851765171757]
	TIME [epoch: 8.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16966769300705878		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 0.15648362160334858		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 0.16307565730520368 | validation: 0.0920036264009563]
	TIME [epoch: 8.18 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19324485632967048		[learning rate: 0.001633]
		[batch 20/20] avg loss: 0.18409179318231375		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 0.18866832475599213 | validation: 0.14787306374675205]
	TIME [epoch: 8.18 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1405704791857234		[learning rate: 0.001627]
		[batch 20/20] avg loss: 0.16974556593631998		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 0.15515802256102168 | validation: 0.14661486670419094]
	TIME [epoch: 8.18 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14370585754687096		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 0.18459805391874107		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 0.164151955732806 | validation: 0.1245180394000009]
	TIME [epoch: 8.2 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18456438144607118		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 0.17808038091060538		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 0.18132238117833827 | validation: 0.2692852563952866]
	TIME [epoch: 8.18 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17262404740420706		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 0.2856453027644741		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 0.22913467508434054 | validation: 0.09992055039266592]
	TIME [epoch: 8.18 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1931339040725883		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 0.19487742068429686		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 0.19400566237844255 | validation: 0.10714861779592791]
	TIME [epoch: 8.18 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16926032810316283		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 0.17598123412669012		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 0.1726207811149265 | validation: 0.33705232672887103]
	TIME [epoch: 8.21 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4126137680856181		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 0.2538890736260929		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 0.3332514208558555 | validation: 0.512685411902475]
	TIME [epoch: 8.19 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2520621488352558		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 0.2681609917647017		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 0.2601115702999787 | validation: 0.12411940870958985]
	TIME [epoch: 8.18 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19451056118399968		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 0.154579297585179		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 0.17454492938458932 | validation: 0.2291603661615169]
	TIME [epoch: 8.18 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33184447337450285		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 0.26928390113551715		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 0.30056418725501005 | validation: 0.40434974424281883]
	TIME [epoch: 8.21 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3412440179866608		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 0.22672034262529275		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 0.2839821803059768 | validation: 0.09674385785224723]
	TIME [epoch: 8.18 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17647434333929368		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 0.18675888196333706		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 0.18161661265131537 | validation: 0.15481018761867313]
	TIME [epoch: 8.18 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.173102567546055		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 0.20970556317280406		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 0.19140406535942953 | validation: 0.1395898167560867]
	TIME [epoch: 8.19 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23073992860165507		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 0.22049392607165128		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 0.2256169273366532 | validation: 0.17036528047421076]
	TIME [epoch: 8.21 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19382071374390938		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 0.18806028263002122		[learning rate: 0.0015435]
	Learning Rate: 0.00154348
	LOSS [training: 0.1909404981869653 | validation: 0.07034806991984814]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21567342589425711		[learning rate: 0.0015407]
		[batch 20/20] avg loss: 0.19001923568291518		[learning rate: 0.0015379]
	Learning Rate: 0.00153788
	LOSS [training: 0.20284633078858616 | validation: 0.11402331025085023]
	TIME [epoch: 8.18 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18526846126906305		[learning rate: 0.0015351]
		[batch 20/20] avg loss: 0.20861714108168491		[learning rate: 0.0015323]
	Learning Rate: 0.00153229
	LOSS [training: 0.19694280117537397 | validation: 0.11247882648702207]
	TIME [epoch: 8.18 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17357830182551098		[learning rate: 0.0015295]
		[batch 20/20] avg loss: 0.21972675096940594		[learning rate: 0.0015267]
	Learning Rate: 0.00152673
	LOSS [training: 0.1966525263974585 | validation: 0.1903904723057308]
	TIME [epoch: 8.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1639558417337192		[learning rate: 0.001524]
		[batch 20/20] avg loss: 0.16739127173911275		[learning rate: 0.0015212]
	Learning Rate: 0.00152119
	LOSS [training: 0.16567355673641598 | validation: 0.10784333595624786]
	TIME [epoch: 8.19 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20474761555765034		[learning rate: 0.0015184]
		[batch 20/20] avg loss: 0.22316463070721762		[learning rate: 0.0015157]
	Learning Rate: 0.00151567
	LOSS [training: 0.21395612313243398 | validation: 0.1459489730420173]
	TIME [epoch: 8.18 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21052335699910985		[learning rate: 0.0015129]
		[batch 20/20] avg loss: 0.33027417001944287		[learning rate: 0.0015102]
	Learning Rate: 0.00151017
	LOSS [training: 0.27039876350927633 | validation: 0.09788241505331471]
	TIME [epoch: 8.2 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17321799700745316		[learning rate: 0.0015074]
		[batch 20/20] avg loss: 0.2013419913872024		[learning rate: 0.0015047]
	Learning Rate: 0.00150469
	LOSS [training: 0.18727999419732777 | validation: 0.1237735063067352]
	TIME [epoch: 8.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1583216389091054		[learning rate: 0.001502]
		[batch 20/20] avg loss: 0.17674229279284323		[learning rate: 0.0014992]
	Learning Rate: 0.00149923
	LOSS [training: 0.16753196585097432 | validation: 0.14963976687566385]
	TIME [epoch: 8.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16908467034272284		[learning rate: 0.0014965]
		[batch 20/20] avg loss: 0.3429247334567493		[learning rate: 0.0014938]
	Learning Rate: 0.00149379
	LOSS [training: 0.2560047018997361 | validation: 0.2637465888631754]
	TIME [epoch: 8.18 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21790273978329142		[learning rate: 0.0014911]
		[batch 20/20] avg loss: 0.22265099616328551		[learning rate: 0.0014884]
	Learning Rate: 0.00148837
	LOSS [training: 0.2202768679732885 | validation: 0.23131104505120514]
	TIME [epoch: 8.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21400084558068005		[learning rate: 0.0014857]
		[batch 20/20] avg loss: 0.16095844928491232		[learning rate: 0.001483]
	Learning Rate: 0.00148297
	LOSS [training: 0.18747964743279621 | validation: 0.1585210584847609]
	TIME [epoch: 8.21 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17042667310799253		[learning rate: 0.0014803]
		[batch 20/20] avg loss: 0.30482519330105246		[learning rate: 0.0014776]
	Learning Rate: 0.00147759
	LOSS [training: 0.23762593320452247 | validation: 0.18319456874011114]
	TIME [epoch: 8.19 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18613873625930294		[learning rate: 0.0014749]
		[batch 20/20] avg loss: 0.19953015889765313		[learning rate: 0.0014722]
	Learning Rate: 0.00147222
	LOSS [training: 0.19283444757847806 | validation: 0.2139136538129555]
	TIME [epoch: 8.18 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15170631869097734		[learning rate: 0.0014695]
		[batch 20/20] avg loss: 0.1832779505880306		[learning rate: 0.0014669]
	Learning Rate: 0.00146688
	LOSS [training: 0.167492134639504 | validation: 0.19993240609733243]
	TIME [epoch: 8.19 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18797363817762786		[learning rate: 0.0014642]
		[batch 20/20] avg loss: 0.19362956124503033		[learning rate: 0.0014616]
	Learning Rate: 0.00146156
	LOSS [training: 0.1908015997113291 | validation: 0.1610035933126357]
	TIME [epoch: 8.21 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18592411461178265		[learning rate: 0.0014589]
		[batch 20/20] avg loss: 0.2311593781728492		[learning rate: 0.0014563]
	Learning Rate: 0.00145625
	LOSS [training: 0.20854174639231587 | validation: 0.12333400820651365]
	TIME [epoch: 8.19 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21774706436606506		[learning rate: 0.0014536]
		[batch 20/20] avg loss: 0.2147684213570677		[learning rate: 0.001451]
	Learning Rate: 0.00145097
	LOSS [training: 0.2162577428615664 | validation: 0.18023956731856777]
	TIME [epoch: 8.18 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30596690134433957		[learning rate: 0.0014483]
		[batch 20/20] avg loss: 0.25379550327079675		[learning rate: 0.0014457]
	Learning Rate: 0.0014457
	LOSS [training: 0.27988120230756824 | validation: 0.13301754559958007]
	TIME [epoch: 8.19 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19754830686517225		[learning rate: 0.0014431]
		[batch 20/20] avg loss: 0.1436099820965942		[learning rate: 0.0014405]
	Learning Rate: 0.00144046
	LOSS [training: 0.17057914448088324 | validation: 0.0864468657680465]
	TIME [epoch: 8.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1616336232764897		[learning rate: 0.0014378]
		[batch 20/20] avg loss: 0.25941280118614307		[learning rate: 0.0014352]
	Learning Rate: 0.00143523
	LOSS [training: 0.2105232122313164 | validation: 0.2232518505990896]
	TIME [epoch: 8.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34970996707325097		[learning rate: 0.0014326]
		[batch 20/20] avg loss: 0.18207201358523928		[learning rate: 0.00143]
	Learning Rate: 0.00143002
	LOSS [training: 0.2658909903292452 | validation: 0.2643885190578459]
	TIME [epoch: 8.19 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2236083015812956		[learning rate: 0.0014274]
		[batch 20/20] avg loss: 0.24746907485275832		[learning rate: 0.0014248]
	Learning Rate: 0.00142483
	LOSS [training: 0.235538688217027 | validation: 1.7680894150285]
	TIME [epoch: 8.21 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5591353631324198		[learning rate: 0.0014222]
		[batch 20/20] avg loss: 0.23687160614937802		[learning rate: 0.0014197]
	Learning Rate: 0.00141966
	LOSS [training: 0.39800348464089896 | validation: 0.10535574915853636]
	TIME [epoch: 8.21 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8168547023381754		[learning rate: 0.0014171]
		[batch 20/20] avg loss: 0.2206399147354551		[learning rate: 0.0014145]
	Learning Rate: 0.00141451
	LOSS [training: 0.5187473085368153 | validation: 0.5856073307585954]
	TIME [epoch: 8.19 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25069284365124755		[learning rate: 0.0014119]
		[batch 20/20] avg loss: 0.21254621022845943		[learning rate: 0.0014094]
	Learning Rate: 0.00140937
	LOSS [training: 0.2316195269398535 | validation: 0.10415933617733059]
	TIME [epoch: 8.19 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2725891079431546		[learning rate: 0.0014068]
		[batch 20/20] avg loss: 0.3407750048947425		[learning rate: 0.0014043]
	Learning Rate: 0.00140426
	LOSS [training: 0.3066820564189485 | validation: 0.22120691287619257]
	TIME [epoch: 8.19 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24445677510374667		[learning rate: 0.0014017]
		[batch 20/20] avg loss: 0.19847394194629167		[learning rate: 0.0013992]
	Learning Rate: 0.00139916
	LOSS [training: 0.2214653585250192 | validation: 0.1994267075434148]
	TIME [epoch: 8.22 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33471795998745824		[learning rate: 0.0013966]
		[batch 20/20] avg loss: 0.24542028329446638		[learning rate: 0.0013941]
	Learning Rate: 0.00139409
	LOSS [training: 0.2900691216409623 | validation: 0.23740896365801967]
	TIME [epoch: 8.19 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24110850608411522		[learning rate: 0.0013916]
		[batch 20/20] avg loss: 0.18062764738053233		[learning rate: 0.001389]
	Learning Rate: 0.00138903
	LOSS [training: 0.2108680767323238 | validation: 0.08911714227873173]
	TIME [epoch: 8.19 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20608452532080404		[learning rate: 0.0013865]
		[batch 20/20] avg loss: 0.17606712157653456		[learning rate: 0.001384]
	Learning Rate: 0.00138399
	LOSS [training: 0.19107582344866927 | validation: 0.12986534347150105]
	TIME [epoch: 8.19 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27279811139101307		[learning rate: 0.0013815]
		[batch 20/20] avg loss: 0.15798730923521878		[learning rate: 0.001379]
	Learning Rate: 0.00137896
	LOSS [training: 0.2153927103131159 | validation: 0.10773675559841883]
	TIME [epoch: 8.21 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2557605919885519		[learning rate: 0.0013765]
		[batch 20/20] avg loss: 0.36186192145417306		[learning rate: 0.001374]
	Learning Rate: 0.00137396
	LOSS [training: 0.30881125672136245 | validation: 0.43974915519879676]
	TIME [epoch: 8.18 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2741383178209743		[learning rate: 0.0013715]
		[batch 20/20] avg loss: 0.20471078350959643		[learning rate: 0.001369]
	Learning Rate: 0.00136897
	LOSS [training: 0.23942455066528537 | validation: 0.16704411675720776]
	TIME [epoch: 8.19 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18964436833475748		[learning rate: 0.0013665]
		[batch 20/20] avg loss: 0.27522742514621423		[learning rate: 0.001364]
	Learning Rate: 0.001364
	LOSS [training: 0.23243589674048587 | validation: 0.06954660462458909]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18673832750967917		[learning rate: 0.0013615]
		[batch 20/20] avg loss: 0.27986267631774		[learning rate: 0.0013591]
	Learning Rate: 0.00135905
	LOSS [training: 0.2333005019137096 | validation: 0.21429680507645169]
	TIME [epoch: 8.22 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2698792018250322		[learning rate: 0.0013566]
		[batch 20/20] avg loss: 0.27494423025974585		[learning rate: 0.0013541]
	Learning Rate: 0.00135412
	LOSS [training: 0.272411716042389 | validation: 0.3423830186276423]
	TIME [epoch: 8.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36576942960133807		[learning rate: 0.0013517]
		[batch 20/20] avg loss: 0.16564005679105082		[learning rate: 0.0013492]
	Learning Rate: 0.00134921
	LOSS [training: 0.2657047431961944 | validation: 0.17085639723542512]
	TIME [epoch: 8.19 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2137729098781403		[learning rate: 0.0013468]
		[batch 20/20] avg loss: 0.19173472687212736		[learning rate: 0.0013443]
	Learning Rate: 0.00134431
	LOSS [training: 0.2027538183751338 | validation: 0.1050653547036182]
	TIME [epoch: 8.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2573345320879411		[learning rate: 0.0013419]
		[batch 20/20] avg loss: 0.3157815038878664		[learning rate: 0.0013394]
	Learning Rate: 0.00133943
	LOSS [training: 0.2865580179879037 | validation: 0.3807891811627924]
	TIME [epoch: 8.22 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22875656034171793		[learning rate: 0.001337]
		[batch 20/20] avg loss: 0.2100039061282774		[learning rate: 0.0013346]
	Learning Rate: 0.00133457
	LOSS [training: 0.21938023323499772 | validation: 0.08335346398733018]
	TIME [epoch: 8.21 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15706462199817792		[learning rate: 0.0013321]
		[batch 20/20] avg loss: 0.21693911596603196		[learning rate: 0.0013297]
	Learning Rate: 0.00132973
	LOSS [training: 0.18700186898210497 | validation: 0.08835090410934299]
	TIME [epoch: 8.19 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23217728093978107		[learning rate: 0.0013273]
		[batch 20/20] avg loss: 0.2170249579608574		[learning rate: 0.0013249]
	Learning Rate: 0.0013249
	LOSS [training: 0.22460111945031924 | validation: 0.1414326973753804]
	TIME [epoch: 8.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30705340603896036		[learning rate: 0.0013225]
		[batch 20/20] avg loss: 0.523164263784606		[learning rate: 0.0013201]
	Learning Rate: 0.0013201
	LOSS [training: 0.4151088349117832 | validation: 0.08869988150726821]
	TIME [epoch: 8.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35180097547367073		[learning rate: 0.0013177]
		[batch 20/20] avg loss: 0.2278117977266687		[learning rate: 0.0013153]
	Learning Rate: 0.0013153
	LOSS [training: 0.2898063866001697 | validation: 0.1819745688379196]
	TIME [epoch: 8.21 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20183530635270602		[learning rate: 0.0013129]
		[batch 20/20] avg loss: 0.2000152727249788		[learning rate: 0.0013105]
	Learning Rate: 0.00131053
	LOSS [training: 0.20092528953884242 | validation: 0.1355717993295268]
	TIME [epoch: 8.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16294985320683625		[learning rate: 0.0013082]
		[batch 20/20] avg loss: 0.26336428745876217		[learning rate: 0.0013058]
	Learning Rate: 0.00130578
	LOSS [training: 0.21315707033279918 | validation: 0.32862786129693133]
	TIME [epoch: 8.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33333094128306406		[learning rate: 0.0013034]
		[batch 20/20] avg loss: 0.18373154018964605		[learning rate: 0.001301]
	Learning Rate: 0.00130104
	LOSS [training: 0.2585312407363551 | validation: 0.20941704763120608]
	TIME [epoch: 8.21 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1913318887392998		[learning rate: 0.0012987]
		[batch 20/20] avg loss: 0.19270570221947883		[learning rate: 0.0012963]
	Learning Rate: 0.00129631
	LOSS [training: 0.19201879547938933 | validation: 0.12918880457283402]
	TIME [epoch: 8.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19720028951301838		[learning rate: 0.001294]
		[batch 20/20] avg loss: 0.19083406466562278		[learning rate: 0.0012916]
	Learning Rate: 0.00129161
	LOSS [training: 0.1940171770893206 | validation: 0.1613403052100694]
	TIME [epoch: 8.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20030081869328833		[learning rate: 0.0012893]
		[batch 20/20] avg loss: 0.1652184358489761		[learning rate: 0.0012869]
	Learning Rate: 0.00128692
	LOSS [training: 0.1827596272711322 | validation: 0.20887516375642906]
	TIME [epoch: 8.19 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16443066497915854		[learning rate: 0.0012846]
		[batch 20/20] avg loss: 0.22484493835320504		[learning rate: 0.0012823]
	Learning Rate: 0.00128225
	LOSS [training: 0.19463780166618178 | validation: 0.16452008295457543]
	TIME [epoch: 8.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31266178940856		[learning rate: 0.0012799]
		[batch 20/20] avg loss: 0.3119252425473507		[learning rate: 0.0012776]
	Learning Rate: 0.0012776
	LOSS [training: 0.31229351597795535 | validation: 0.24257807207561363]
	TIME [epoch: 8.22 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23567367717097115		[learning rate: 0.0012753]
		[batch 20/20] avg loss: 0.200766069960232		[learning rate: 0.001273]
	Learning Rate: 0.00127296
	LOSS [training: 0.21821987356560163 | validation: 0.12963647737779224]
	TIME [epoch: 8.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16711974287986273		[learning rate: 0.0012707]
		[batch 20/20] avg loss: 0.17800534307780783		[learning rate: 0.0012683]
	Learning Rate: 0.00126834
	LOSS [training: 0.17256254297883528 | validation: 0.10376469523035542]
	TIME [epoch: 8.19 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22356748910498672		[learning rate: 0.001266]
		[batch 20/20] avg loss: 0.16677024358290354		[learning rate: 0.0012637]
	Learning Rate: 0.00126374
	LOSS [training: 0.19516886634394517 | validation: 0.10680540885612341]
	TIME [epoch: 8.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18431432507923262		[learning rate: 0.0012614]
		[batch 20/20] avg loss: 0.17929719484948195		[learning rate: 0.0012592]
	Learning Rate: 0.00125915
	LOSS [training: 0.18180575996435727 | validation: 0.1534356334781783]
	TIME [epoch: 8.22 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19443885757731377		[learning rate: 0.0012569]
		[batch 20/20] avg loss: 0.20436193544444398		[learning rate: 0.0012546]
	Learning Rate: 0.00125458
	LOSS [training: 0.19940039651087887 | validation: 0.09386895338510787]
	TIME [epoch: 8.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24195035938900583		[learning rate: 0.0012523]
		[batch 20/20] avg loss: 0.20053098823142684		[learning rate: 0.00125]
	Learning Rate: 0.00125003
	LOSS [training: 0.2212406738102163 | validation: 0.21152249590452116]
	TIME [epoch: 8.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18482851428561253		[learning rate: 0.0012478]
		[batch 20/20] avg loss: 0.30103770246462425		[learning rate: 0.0012455]
	Learning Rate: 0.0012455
	LOSS [training: 0.24293310837511845 | validation: 0.10003110642971005]
	TIME [epoch: 8.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1870674268083175		[learning rate: 0.0012432]
		[batch 20/20] avg loss: 0.1915969957297084		[learning rate: 0.001241]
	Learning Rate: 0.00124098
	LOSS [training: 0.18933221126901295 | validation: 0.168373999043559]
	TIME [epoch: 8.22 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17609763073248824		[learning rate: 0.0012387]
		[batch 20/20] avg loss: 0.18647819106168959		[learning rate: 0.0012365]
	Learning Rate: 0.00123647
	LOSS [training: 0.1812879108970889 | validation: 0.12710624686138716]
	TIME [epoch: 8.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24035701394968037		[learning rate: 0.0012342]
		[batch 20/20] avg loss: 0.1578741299183444		[learning rate: 0.001232]
	Learning Rate: 0.00123198
	LOSS [training: 0.19911557193401236 | validation: 0.14701483196792212]
	TIME [epoch: 8.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26732975959764815		[learning rate: 0.0012297]
		[batch 20/20] avg loss: 0.16263248910246347		[learning rate: 0.0012275]
	Learning Rate: 0.00122751
	LOSS [training: 0.2149811243500558 | validation: 0.21386299288622018]
	TIME [epoch: 8.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14725669621074766		[learning rate: 0.0012253]
		[batch 20/20] avg loss: 0.19098381149056726		[learning rate: 0.0012231]
	Learning Rate: 0.00122306
	LOSS [training: 0.1691202538506575 | validation: 0.5463432243146246]
	TIME [epoch: 8.23 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3211989491796627		[learning rate: 0.0012208]
		[batch 20/20] avg loss: 0.3292330734715049		[learning rate: 0.0012186]
	Learning Rate: 0.00121862
	LOSS [training: 0.32521601132558375 | validation: 0.277321041624943]
	TIME [epoch: 8.19 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13932746401468948		[learning rate: 0.0012164]
		[batch 20/20] avg loss: 0.20319103552262763		[learning rate: 0.0012142]
	Learning Rate: 0.0012142
	LOSS [training: 0.17125924976865853 | validation: 0.30987621097417206]
	TIME [epoch: 8.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22004796913767258		[learning rate: 0.001212]
		[batch 20/20] avg loss: 0.15377488002450362		[learning rate: 0.0012098]
	Learning Rate: 0.00120979
	LOSS [training: 0.18691142458108811 | validation: 0.0829409774351535]
	TIME [epoch: 8.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20861142702009783		[learning rate: 0.0012076]
		[batch 20/20] avg loss: 0.16563981580343853		[learning rate: 0.0012054]
	Learning Rate: 0.0012054
	LOSS [training: 0.1871256214117682 | validation: 0.12839527260559225]
	TIME [epoch: 8.22 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17092133067741594		[learning rate: 0.0012032]
		[batch 20/20] avg loss: 0.3185155166683492		[learning rate: 0.001201]
	Learning Rate: 0.00120103
	LOSS [training: 0.2447184236728826 | validation: 0.11446889683784056]
	TIME [epoch: 8.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21424108589718607		[learning rate: 0.0011988]
		[batch 20/20] avg loss: 0.2384397841817903		[learning rate: 0.0011967]
	Learning Rate: 0.00119667
	LOSS [training: 0.22634043503948814 | validation: 0.09743599403697564]
	TIME [epoch: 8.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2652415506609406		[learning rate: 0.0011945]
		[batch 20/20] avg loss: 1.043551951408433		[learning rate: 0.0011923]
	Learning Rate: 0.00119233
	LOSS [training: 0.6543967510346868 | validation: 0.17553284931710889]
	TIME [epoch: 8.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21449969764013552		[learning rate: 0.0011902]
		[batch 20/20] avg loss: 0.23153406673221144		[learning rate: 0.001188]
	Learning Rate: 0.001188
	LOSS [training: 0.22301688218617347 | validation: 0.21013842996277687]
	TIME [epoch: 8.23 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3317296446509353		[learning rate: 0.0011858]
		[batch 20/20] avg loss: 0.19037340063549762		[learning rate: 0.0011837]
	Learning Rate: 0.00118369
	LOSS [training: 0.2610515226432165 | validation: 0.21470557714466043]
	TIME [epoch: 8.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4041467217289442		[learning rate: 0.0011815]
		[batch 20/20] avg loss: 0.14889367306782164		[learning rate: 0.0011794]
	Learning Rate: 0.00117939
	LOSS [training: 0.2765201973983829 | validation: 0.17015399355230046]
	TIME [epoch: 8.19 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22253860021425376		[learning rate: 0.0011772]
		[batch 20/20] avg loss: 0.22995824381469016		[learning rate: 0.0011751]
	Learning Rate: 0.00117511
	LOSS [training: 0.22624842201447198 | validation: 0.12055518947610219]
	TIME [epoch: 8.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3155952640528044		[learning rate: 0.001173]
		[batch 20/20] avg loss: 0.3198414653169678		[learning rate: 0.0011708]
	Learning Rate: 0.00117085
	LOSS [training: 0.3177183646848861 | validation: 0.3318633978465589]
	TIME [epoch: 8.22 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2973579334613032		[learning rate: 0.0011687]
		[batch 20/20] avg loss: 0.12800541062205964		[learning rate: 0.0011666]
	Learning Rate: 0.0011666
	LOSS [training: 0.2126816720416814 | validation: 0.33168557742130045]
	TIME [epoch: 8.19 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2012564630662932		[learning rate: 0.0011645]
		[batch 20/20] avg loss: 0.22459819447456314		[learning rate: 0.0011624]
	Learning Rate: 0.00116236
	LOSS [training: 0.21292732877042822 | validation: 0.11457277933671639]
	TIME [epoch: 8.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23332179273339254		[learning rate: 0.0011603]
		[batch 20/20] avg loss: 0.3815695802455613		[learning rate: 0.0011581]
	Learning Rate: 0.00115815
	LOSS [training: 0.3074456864894769 | validation: 0.11238452377740983]
	TIME [epoch: 8.19 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2731204509352589		[learning rate: 0.001156]
		[batch 20/20] avg loss: 0.16936647033271585		[learning rate: 0.0011539]
	Learning Rate: 0.00115394
	LOSS [training: 0.2212434606339874 | validation: 0.07890010373831546]
	TIME [epoch: 8.22 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23655198828937923		[learning rate: 0.0011518]
		[batch 20/20] avg loss: 0.22081801966280823		[learning rate: 0.0011498]
	Learning Rate: 0.00114975
	LOSS [training: 0.22868500397609376 | validation: 0.2428771427205797]
	TIME [epoch: 8.19 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19164727612158455		[learning rate: 0.0011477]
		[batch 20/20] avg loss: 0.16043689933130106		[learning rate: 0.0011456]
	Learning Rate: 0.00114558
	LOSS [training: 0.17604208772644275 | validation: 0.11280139044421333]
	TIME [epoch: 8.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2138548395498499		[learning rate: 0.0011435]
		[batch 20/20] avg loss: 0.291858976027382		[learning rate: 0.0011414]
	Learning Rate: 0.00114142
	LOSS [training: 0.25285690778861597 | validation: 1.6215186278949225]
	TIME [epoch: 8.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3211151880321856		[learning rate: 0.0011394]
		[batch 20/20] avg loss: 0.19905903190247573		[learning rate: 0.0011373]
	Learning Rate: 0.00113728
	LOSS [training: 0.2600871099673306 | validation: 0.1217111248096318]
	TIME [epoch: 8.23 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17633247465514312		[learning rate: 0.0011352]
		[batch 20/20] avg loss: 0.27507092777042125		[learning rate: 0.0011332]
	Learning Rate: 0.00113316
	LOSS [training: 0.2257017012127822 | validation: 0.27369541155769783]
	TIME [epoch: 8.19 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30783646829003086		[learning rate: 0.0011311]
		[batch 20/20] avg loss: 0.31974197766760565		[learning rate: 0.001129]
	Learning Rate: 0.00112904
	LOSS [training: 0.3137892229788182 | validation: 0.17953969688603377]
	TIME [epoch: 8.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3005593074986669		[learning rate: 0.001127]
		[batch 20/20] avg loss: 0.21843645077483825		[learning rate: 0.0011249]
	Learning Rate: 0.00112495
	LOSS [training: 0.25949787913675254 | validation: 0.1934495033099558]
	TIME [epoch: 8.19 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2818428090629106		[learning rate: 0.0011229]
		[batch 20/20] avg loss: 0.3116052123236027		[learning rate: 0.0011209]
	Learning Rate: 0.00112086
	LOSS [training: 0.2967240106932566 | validation: 0.15175113492626338]
	TIME [epoch: 8.22 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2853417029419871		[learning rate: 0.0011188]
		[batch 20/20] avg loss: 0.37474539559454745		[learning rate: 0.0011168]
	Learning Rate: 0.0011168
	LOSS [training: 0.3300435492682673 | validation: 0.23042967113556279]
	TIME [epoch: 8.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3276213603922639		[learning rate: 0.0011148]
		[batch 20/20] avg loss: 0.2669209427765124		[learning rate: 0.0011127]
	Learning Rate: 0.00111274
	LOSS [training: 0.29727115158438816 | validation: 0.17814960549879771]
	TIME [epoch: 8.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21683560826279996		[learning rate: 0.0011107]
		[batch 20/20] avg loss: 0.28507197521787975		[learning rate: 0.0011087]
	Learning Rate: 0.0011087
	LOSS [training: 0.2509537917403399 | validation: 0.18483709445157206]
	TIME [epoch: 8.19 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28892702803747844		[learning rate: 0.0011067]
		[batch 20/20] avg loss: 0.2657820559670653		[learning rate: 0.0011047]
	Learning Rate: 0.00110468
	LOSS [training: 0.27735454200227194 | validation: 0.1695304529399667]
	TIME [epoch: 8.22 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4932002333768491		[learning rate: 0.0011027]
		[batch 20/20] avg loss: 0.311487509448986		[learning rate: 0.0011007]
	Learning Rate: 0.00110067
	LOSS [training: 0.4023438714129175 | validation: 0.42100620558540225]
	TIME [epoch: 8.19 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0778350461053734		[learning rate: 0.0010987]
		[batch 20/20] avg loss: 0.5900134659243802		[learning rate: 0.0010967]
	Learning Rate: 0.00109668
	LOSS [training: 0.8339242560148769 | validation: 0.5290280470689067]
	TIME [epoch: 8.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34181228369217803		[learning rate: 0.0010947]
		[batch 20/20] avg loss: 0.26861032539919133		[learning rate: 0.0010927]
	Learning Rate: 0.0010927
	LOSS [training: 0.3052113045456847 | validation: 0.22031163725810593]
	TIME [epoch: 8.19 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2554920391977817		[learning rate: 0.0010907]
		[batch 20/20] avg loss: 0.39159543231726857		[learning rate: 0.0010887]
	Learning Rate: 0.00108873
	LOSS [training: 0.32354373575752515 | validation: 0.24186752190153737]
	TIME [epoch: 8.21 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48416838619923264		[learning rate: 0.0010868]
		[batch 20/20] avg loss: 1.0361907917101025		[learning rate: 0.0010848]
	Learning Rate: 0.00108478
	LOSS [training: 0.7601795889546675 | validation: 0.40901040138427847]
	TIME [epoch: 8.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8056771737755888		[learning rate: 0.0010828]
		[batch 20/20] avg loss: 0.3342961895567381		[learning rate: 0.0010808]
	Learning Rate: 0.00108084
	LOSS [training: 0.5699866816661634 | validation: 0.2085876105126233]
	TIME [epoch: 8.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.235497392707197		[learning rate: 0.0010789]
		[batch 20/20] avg loss: 0.40681375895626193		[learning rate: 0.0010769]
	Learning Rate: 0.00107692
	LOSS [training: 0.3211555758317295 | validation: 0.3265851870790108]
	TIME [epoch: 8.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5789656064618263		[learning rate: 0.001075]
		[batch 20/20] avg loss: 6.91710412636156		[learning rate: 0.001073]
	Learning Rate: 0.00107301
	LOSS [training: 4.248034866411693 | validation: 9.959028622967068]
	TIME [epoch: 8.22 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.413962000094324		[learning rate: 0.0010711]
		[batch 20/20] avg loss: 10.180363209754528		[learning rate: 0.0010691]
	Learning Rate: 0.00106912
	LOSS [training: 10.297162604924427 | validation: 10.18454271732349]
	TIME [epoch: 8.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.788100232074026		[learning rate: 0.0010672]
		[batch 20/20] avg loss: 8.464688112700587		[learning rate: 0.0010652]
	Learning Rate: 0.00106524
	LOSS [training: 9.12639417238731 | validation: 7.620881265229299]
	TIME [epoch: 8.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.174087392894286		[learning rate: 0.0010633]
		[batch 20/20] avg loss: 6.106858734362405		[learning rate: 0.0010614]
	Learning Rate: 0.00106137
	LOSS [training: 6.640473063628346 | validation: 6.377097105407867]
	TIME [epoch: 8.19 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.460809544985399		[learning rate: 0.0010594]
		[batch 20/20] avg loss: 7.9981287578831495		[learning rate: 0.0010575]
	Learning Rate: 0.00105752
	LOSS [training: 7.229469151434275 | validation: 7.058615356237023]
	TIME [epoch: 8.22 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.284938715166135		[learning rate: 0.0010556]
		[batch 20/20] avg loss: 7.859383621403586		[learning rate: 0.0010537]
	Learning Rate: 0.00105368
	LOSS [training: 7.072161168284862 | validation: 9.377529326979204]
	TIME [epoch: 8.19 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.22471710516806		[learning rate: 0.0010518]
		[batch 20/20] avg loss: 7.7782461920565495		[learning rate: 0.0010499]
	Learning Rate: 0.00104986
	LOSS [training: 8.001481648612307 | validation: 9.195971705967537]
	TIME [epoch: 8.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.625695338559527		[learning rate: 0.001048]
		[batch 20/20] avg loss: 8.389887320418849		[learning rate: 0.0010461]
	Learning Rate: 0.00104605
	LOSS [training: 8.507791329489187 | validation: 8.944259630674917]
	TIME [epoch: 8.19 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.034008535857321		[learning rate: 0.0010442]
		[batch 20/20] avg loss: 8.768192119098307		[learning rate: 0.0010423]
	Learning Rate: 0.00104225
	LOSS [training: 8.401100327477815 | validation: 9.569876111836367]
	TIME [epoch: 8.22 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.104737016308125		[learning rate: 0.0010404]
		[batch 20/20] avg loss: 10.127640334333705		[learning rate: 0.0010385]
	Learning Rate: 0.00103847
	LOSS [training: 10.116188675320917 | validation: 9.611122883091117]
	TIME [epoch: 8.19 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.00496117675976		[learning rate: 0.0010366]
		[batch 20/20] avg loss: 10.469991584531504		[learning rate: 0.0010347]
	Learning Rate: 0.0010347
	LOSS [training: 10.23747638064563 | validation: 10.990853768866199]
	TIME [epoch: 8.19 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.670448342251827		[learning rate: 0.0010328]
		[batch 20/20] avg loss: 10.61529183734507		[learning rate: 0.0010309]
	Learning Rate: 0.00103095
	LOSS [training: 10.642870089798446 | validation: 10.638149821098699]
	TIME [epoch: 8.19 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.041373065980352		[learning rate: 0.0010291]
		[batch 20/20] avg loss: 9.312980585757803		[learning rate: 0.0010272]
	Learning Rate: 0.00102721
	LOSS [training: 9.677176825869077 | validation: 8.815462471055763]
	TIME [epoch: 8.21 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.562599472715053		[learning rate: 0.0010253]
		[batch 20/20] avg loss: 5.339953391410617		[learning rate: 0.0010235]
	Learning Rate: 0.00102348
	LOSS [training: 6.951276432062835 | validation: 6.4599964768707965]
	TIME [epoch: 8.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.168759066905376		[learning rate: 0.0010216]
		[batch 20/20] avg loss: 5.890796772101256		[learning rate: 0.0010198]
	Learning Rate: 0.00101976
	LOSS [training: 5.529777919503315 | validation: 6.92018562687346]
	TIME [epoch: 8.19 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.149682903203785		[learning rate: 0.0010179]
		[batch 20/20] avg loss: 5.870482732223219		[learning rate: 0.0010161]
	Learning Rate: 0.00101606
	LOSS [training: 6.010082817713503 | validation: 6.010769326031172]
	TIME [epoch: 8.19 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.553984405028724		[learning rate: 0.0010142]
		[batch 20/20] avg loss: 5.989138407929275		[learning rate: 0.0010124]
	Learning Rate: 0.00101238
	LOSS [training: 6.271561406478999 | validation: 5.462223719361887]
	TIME [epoch: 8.22 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.724716480610939		[learning rate: 0.0010105]
		[batch 20/20] avg loss: 4.644925239955446		[learning rate: 0.0010087]
	Learning Rate: 0.0010087
	LOSS [training: 4.6848208602831924 | validation: 4.859491702308938]
	TIME [epoch: 8.19 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.132666542981776		[learning rate: 0.0010069]
		[batch 20/20] avg loss: 5.0648724938534855		[learning rate: 0.001005]
	Learning Rate: 0.00100504
	LOSS [training: 5.0987695184176305 | validation: 4.808161497157804]
	TIME [epoch: 8.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.099800770590383		[learning rate: 0.0010032]
		[batch 20/20] avg loss: 5.742266903438263		[learning rate: 0.0010014]
	Learning Rate: 0.00100139
	LOSS [training: 5.421033837014322 | validation: 5.599017435077746]
	TIME [epoch: 8.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.861526997885968		[learning rate: 0.00099958]
		[batch 20/20] avg loss: 5.966565245973255		[learning rate: 0.00099776]
	Learning Rate: 0.00099776
	LOSS [training: 5.91404612192961 | validation: 5.2565253860799075]
	TIME [epoch: 8.22 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.523355919483363		[learning rate: 0.00099595]
		[batch 20/20] avg loss: 5.468502848163045		[learning rate: 0.00099414]
	Learning Rate: 0.00099414
	LOSS [training: 5.495929383823204 | validation: 5.185125625493454]
	TIME [epoch: 8.19 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.30101152182462		[learning rate: 0.00099233]
		[batch 20/20] avg loss: 5.431177695893757		[learning rate: 0.00099053]
	Learning Rate: 0.000990532
	LOSS [training: 5.366094608859187 | validation: 5.205151363556334]
	TIME [epoch: 8.19 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.909504630787781		[learning rate: 0.00098873]
		[batch 20/20] avg loss: 4.135095853116533		[learning rate: 0.00098694]
	Learning Rate: 0.000986937
	LOSS [training: 4.5223002419521565 | validation: 4.06126470249945]
	TIME [epoch: 8.19 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.433087906636631		[learning rate: 0.00098514]
		[batch 20/20] avg loss: 4.02409790700787		[learning rate: 0.00098336]
	Learning Rate: 0.000983355
	LOSS [training: 4.228592906822251 | validation: 3.8593597356745453]
	TIME [epoch: 8.21 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.194415209125063		[learning rate: 0.00098157]
		[batch 20/20] avg loss: 4.1801028474459025		[learning rate: 0.00097979]
	Learning Rate: 0.000979787
	LOSS [training: 4.1872590282854825 | validation: 4.300229879554509]
	TIME [epoch: 8.19 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.391459678878816		[learning rate: 0.00097801]
		[batch 20/20] avg loss: 4.081513383294714		[learning rate: 0.00097623]
	Learning Rate: 0.000976231
	LOSS [training: 4.236486531086766 | validation: 4.16848995605911]
	TIME [epoch: 8.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7497899531932397		[learning rate: 0.00097446]
		[batch 20/20] avg loss: 3.5463079811610223		[learning rate: 0.00097269]
	Learning Rate: 0.000972688
	LOSS [training: 3.648048967177131 | validation: 3.208649040803082]
	TIME [epoch: 8.19 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.273514134545119		[learning rate: 0.00097092]
		[batch 20/20] avg loss: 3.840301013023832		[learning rate: 0.00096916]
	Learning Rate: 0.000969158
	LOSS [training: 3.5569075737844758 | validation: 6.5209567250276415]
	TIME [epoch: 8.21 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.423507181617137		[learning rate: 0.0009674]
		[batch 20/20] avg loss: 4.16683238741829		[learning rate: 0.00096564]
	Learning Rate: 0.000965641
	LOSS [training: 4.295169784517713 | validation: 4.893287383023056]
	TIME [epoch: 8.19 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.288960606750061		[learning rate: 0.00096389]
		[batch 20/20] avg loss: 3.788548783990188		[learning rate: 0.00096214]
	Learning Rate: 0.000962137
	LOSS [training: 4.038754695370124 | validation: 6.314769094900625]
	TIME [epoch: 8.19 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.202812633755564		[learning rate: 0.00096039]
		[batch 20/20] avg loss: 7.01034311500577		[learning rate: 0.00095865]
	Learning Rate: 0.000958645
	LOSS [training: 6.606577874380666 | validation: 8.352062408998416]
	TIME [epoch: 8.19 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.698758990857255		[learning rate: 0.0009569]
		[batch 20/20] avg loss: 7.855635563710214		[learning rate: 0.00095517]
	Learning Rate: 0.000955166
	LOSS [training: 7.777197277283735 | validation: 8.88727811756849]
	TIME [epoch: 8.21 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.195345505378228		[learning rate: 0.00095343]
		[batch 20/20] avg loss: 8.485399673717527		[learning rate: 0.0009517]
	Learning Rate: 0.0009517
	LOSS [training: 8.84037258954788 | validation: 8.76239666600344]
	TIME [epoch: 8.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.307631415969563		[learning rate: 0.00094997]
		[batch 20/20] avg loss: 6.261314640287571		[learning rate: 0.00094825]
	Learning Rate: 0.000948246
	LOSS [training: 6.784473028128566 | validation: 7.776460756388526]
	TIME [epoch: 8.19 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.648794089080407		[learning rate: 0.00094652]
		[batch 20/20] avg loss: 6.798517182607526		[learning rate: 0.0009448]
	Learning Rate: 0.000944805
	LOSS [training: 6.723655635843966 | validation: 7.241132171109927]
	TIME [epoch: 8.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.775509500409025		[learning rate: 0.00094309]
		[batch 20/20] avg loss: 7.751010933921205		[learning rate: 0.00094138]
	Learning Rate: 0.000941376
	LOSS [training: 7.263260217165114 | validation: 8.152041863427526]
	TIME [epoch: 8.21 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.7623692678044875		[learning rate: 0.00093967]
		[batch 20/20] avg loss: 7.869258814229481		[learning rate: 0.00093796]
	Learning Rate: 0.00093796
	LOSS [training: 7.815814041016982 | validation: 8.363712180371786]
	TIME [epoch: 8.21 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.048293038681203		[learning rate: 0.00093626]
		[batch 20/20] avg loss: 9.329403505544143		[learning rate: 0.00093456]
	Learning Rate: 0.000934556
	LOSS [training: 8.688848272112672 | validation: 9.415432700072415]
	TIME [epoch: 8.19 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.233115962464804		[learning rate: 0.00093286]
		[batch 20/20] avg loss: 6.72819644209928		[learning rate: 0.00093116]
	Learning Rate: 0.000931164
	LOSS [training: 7.480656202282043 | validation: 6.933251375790013]
	TIME [epoch: 8.19 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.615459261249944		[learning rate: 0.00092947]
		[batch 20/20] avg loss: 5.46851173956691		[learning rate: 0.00092779]
	Learning Rate: 0.000927785
	LOSS [training: 5.041985500408427 | validation: 7.118212562313309]
	TIME [epoch: 8.19 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.51245709218519		[learning rate: 0.0009261]
		[batch 20/20] avg loss: 4.577441993973955		[learning rate: 0.00092442]
	Learning Rate: 0.000924418
	LOSS [training: 4.544949543079572 | validation: 5.143502168267569]
	TIME [epoch: 8.22 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.904078442350757		[learning rate: 0.00092274]
		[batch 20/20] avg loss: 5.566747624285791		[learning rate: 0.00092106]
	Learning Rate: 0.000921063
	LOSS [training: 5.235413033318275 | validation: 6.424889736971757]
	TIME [epoch: 8.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.18960004696638		[learning rate: 0.00091939]
		[batch 20/20] avg loss: 4.241064946497587		[learning rate: 0.00091772]
	Learning Rate: 0.000917721
	LOSS [training: 4.715332496731984 | validation: 5.7937919740082435]
	TIME [epoch: 8.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8351605591757965		[learning rate: 0.00091605]
		[batch 20/20] avg loss: 3.134042762530226		[learning rate: 0.00091439]
	Learning Rate: 0.00091439
	LOSS [training: 3.484601660853012 | validation: 3.191673417156376]
	TIME [epoch: 8.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5823801187283375		[learning rate: 0.00091273]
		[batch 20/20] avg loss: 2.4400173887497063		[learning rate: 0.00091107]
	Learning Rate: 0.000911072
	LOSS [training: 2.5111987537390217 | validation: 2.682158189430207]
	TIME [epoch: 8.22 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4883223148792104		[learning rate: 0.00090942]
		[batch 20/20] avg loss: 2.4049592881851423		[learning rate: 0.00090777]
	Learning Rate: 0.000907766
	LOSS [training: 2.4466408015321766 | validation: 2.8086727043891986]
	TIME [epoch: 8.19 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.454683658271206		[learning rate: 0.00090612]
		[batch 20/20] avg loss: 2.118269590648103		[learning rate: 0.00090447]
	Learning Rate: 0.000904471
	LOSS [training: 2.286476624459655 | validation: 2.8880444697258185]
	TIME [epoch: 8.19 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4408117643226332		[learning rate: 0.00090283]
		[batch 20/20] avg loss: 5.561062214460753		[learning rate: 0.00090119]
	Learning Rate: 0.000901189
	LOSS [training: 4.000936989391693 | validation: 7.2049614749588065]
	TIME [epoch: 8.19 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.861145297322329		[learning rate: 0.00089955]
		[batch 20/20] avg loss: 5.478490507972023		[learning rate: 0.00089792]
	Learning Rate: 0.000897918
	LOSS [training: 6.169817902647176 | validation: 8.115779861779743]
	TIME [epoch: 8.22 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.993397460290839		[learning rate: 0.00089629]
		[batch 20/20] avg loss: 5.533373296416164		[learning rate: 0.00089466]
	Learning Rate: 0.00089466
	LOSS [training: 6.263385378353501 | validation: 8.43758923714865]
	TIME [epoch: 8.19 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.2184280297178365		[learning rate: 0.00089303]
		[batch 20/20] avg loss: 5.343282088171737		[learning rate: 0.00089141]
	Learning Rate: 0.000891413
	LOSS [training: 6.280855058944786 | validation: 5.1182422637869625]
	TIME [epoch: 8.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.096722792195194		[learning rate: 0.00088979]
		[batch 20/20] avg loss: 3.9618009903162745		[learning rate: 0.00088818]
	Learning Rate: 0.000888178
	LOSS [training: 4.029261891255734 | validation: 3.3255335784392392]
	TIME [epoch: 8.19 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.483739579025724		[learning rate: 0.00088656]
		[batch 20/20] avg loss: 2.5793080697028694		[learning rate: 0.00088495]
	Learning Rate: 0.000884955
	LOSS [training: 2.5315238243642963 | validation: 5.251628809119245]
	TIME [epoch: 8.22 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.505561441809496		[learning rate: 0.00088335]
		[batch 20/20] avg loss: 1.9556596607936974		[learning rate: 0.00088174]
	Learning Rate: 0.000881743
	LOSS [training: 3.2306105513015964 | validation: 2.143105514746005]
	TIME [epoch: 8.19 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.034799369587164		[learning rate: 0.00088014]
		[batch 20/20] avg loss: 2.8422708960032144		[learning rate: 0.00087854]
	Learning Rate: 0.000878543
	LOSS [training: 2.438535132795189 | validation: 2.581181537999718]
	TIME [epoch: 8.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.791426135784212		[learning rate: 0.00087695]
		[batch 20/20] avg loss: 6.362869708364359		[learning rate: 0.00087536]
	Learning Rate: 0.000875355
	LOSS [training: 6.0771479220742854 | validation: 7.423378778405947]
	TIME [epoch: 8.19 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.932636465737664		[learning rate: 0.00087377]
		[batch 20/20] avg loss: 6.7476824014596435		[learning rate: 0.00087218]
	Learning Rate: 0.000872178
	LOSS [training: 6.840159433598655 | validation: 6.318494943778173]
	TIME [epoch: 8.22 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.450182529846723		[learning rate: 0.00087059]
		[batch 20/20] avg loss: 4.754455045240463		[learning rate: 0.00086901]
	Learning Rate: 0.000869013
	LOSS [training: 6.102318787543593 | validation: 2.9121477247960925]
	TIME [epoch: 8.19 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3363930078072075		[learning rate: 0.00086743]
		[batch 20/20] avg loss: 2.717619606954216		[learning rate: 0.00086586]
	Learning Rate: 0.000865859
	LOSS [training: 2.527006307380712 | validation: 3.6363314100680726]
	TIME [epoch: 8.19 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.0139817263468425		[learning rate: 0.00086429]
		[batch 20/20] avg loss: 6.2754823577745364		[learning rate: 0.00086272]
	Learning Rate: 0.000862717
	LOSS [training: 5.64473204206069 | validation: 6.459606599247693]
	TIME [epoch: 8.19 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.210732406096158		[learning rate: 0.00086115]
		[batch 20/20] avg loss: 3.2644807796023243		[learning rate: 0.00085959]
	Learning Rate: 0.000859586
	LOSS [training: 4.237606592849241 | validation: 3.0964995961716335]
	TIME [epoch: 8.22 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2876464010848423		[learning rate: 0.00085803]
		[batch 20/20] avg loss: 4.004380477853616		[learning rate: 0.00085647]
	Learning Rate: 0.000856467
	LOSS [training: 3.646013439469229 | validation: 5.17444145567331]
	TIME [epoch: 8.19 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.265283630018751		[learning rate: 0.00085491]
		[batch 20/20] avg loss: 4.542388133408433		[learning rate: 0.00085336]
	Learning Rate: 0.000853359
	LOSS [training: 4.403835881713592 | validation: 4.988201419235764]
	TIME [epoch: 8.19 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.691190131250681		[learning rate: 0.00085181]
		[batch 20/20] avg loss: 5.053238154878244		[learning rate: 0.00085026]
	Learning Rate: 0.000850262
	LOSS [training: 4.872214143064463 | validation: 5.3858719440646325]
	TIME [epoch: 8.19 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.25795612023406		[learning rate: 0.00084872]
		[batch 20/20] avg loss: 5.6134781440367245		[learning rate: 0.00084718]
	Learning Rate: 0.000847176
	LOSS [training: 5.435717132135392 | validation: 6.044763187275082]
	TIME [epoch: 8.21 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.252478713119926		[learning rate: 0.00084564]
		[batch 20/20] avg loss: 5.311655234327392		[learning rate: 0.0008441]
	Learning Rate: 0.000844102
	LOSS [training: 5.282066973723659 | validation: 4.446796694378008]
	TIME [epoch: 8.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.973261929235838		[learning rate: 0.00084257]
		[batch 20/20] avg loss: 4.969064746040196		[learning rate: 0.00084104]
	Learning Rate: 0.000841038
	LOSS [training: 4.471163337638016 | validation: 4.970474942417557]
	TIME [epoch: 8.19 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.486387689504794		[learning rate: 0.00083951]
		[batch 20/20] avg loss: 4.350061170006846		[learning rate: 0.00083799]
	Learning Rate: 0.000837986
	LOSS [training: 4.41822442975582 | validation: 5.324672702807876]
	TIME [epoch: 8.19 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.89153831839865		[learning rate: 0.00083646]
		[batch 20/20] avg loss: 5.84188778167435		[learning rate: 0.00083495]
	Learning Rate: 0.000834945
	LOSS [training: 5.8667130500365 | validation: 6.212326516301575]
	TIME [epoch: 8.21 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.142657184301744		[learning rate: 0.00083343]
		[batch 20/20] avg loss: 5.3838489950228965		[learning rate: 0.00083192]
	Learning Rate: 0.000831915
	LOSS [training: 5.763253089662319 | validation: 5.627635252214684]
	TIME [epoch: 8.19 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.290426321805654		[learning rate: 0.0008304]
		[batch 20/20] avg loss: 5.055752367152534		[learning rate: 0.0008289]
	Learning Rate: 0.000828896
	LOSS [training: 5.173089344479093 | validation: 5.481828507033174]
	TIME [epoch: 8.18 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.767622168533608		[learning rate: 0.00082739]
		[batch 20/20] avg loss: 5.302447660195939		[learning rate: 0.00082589]
	Learning Rate: 0.000825888
	LOSS [training: 5.035034914364774 | validation: 5.379227007811197]
	TIME [epoch: 8.18 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.837048698486276		[learning rate: 0.00082439]
		[batch 20/20] avg loss: 5.09337707391296		[learning rate: 0.00082289]
	Learning Rate: 0.000822891
	LOSS [training: 4.965212886199618 | validation: 5.375740540288723]
	TIME [epoch: 8.21 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.913570020230113		[learning rate: 0.0008214]
		[batch 20/20] avg loss: 5.019837917818522		[learning rate: 0.0008199]
	Learning Rate: 0.000819904
	LOSS [training: 4.966703969024317 | validation: 5.452335183605556]
	TIME [epoch: 8.19 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.8265848993827865		[learning rate: 0.00081842]
		[batch 20/20] avg loss: 4.291997485534125		[learning rate: 0.00081693]
	Learning Rate: 0.000816929
	LOSS [training: 4.5592911924584545 | validation: 4.215110481933976]
	TIME [epoch: 8.19 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.123396580811491		[learning rate: 0.00081545]
		[batch 20/20] avg loss: 2.742075261315954		[learning rate: 0.00081396]
	Learning Rate: 0.000813964
	LOSS [training: 3.4327359210637227 | validation: 2.345228711056168]
	TIME [epoch: 8.18 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1226015363176707		[learning rate: 0.00081249]
		[batch 20/20] avg loss: 2.3110817484184665		[learning rate: 0.00081101]
	Learning Rate: 0.00081101
	LOSS [training: 2.2168416423680686 | validation: 2.6891905222308945]
	TIME [epoch: 8.22 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5367927469291938		[learning rate: 0.00080954]
		[batch 20/20] avg loss: 2.2594594970436277		[learning rate: 0.00080807]
	Learning Rate: 0.000808067
	LOSS [training: 2.3981261219864107 | validation: 2.2970582286799894]
	TIME [epoch: 8.19 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.803187143868706		[learning rate: 0.0008066]
		[batch 20/20] avg loss: 1.7262869954714681		[learning rate: 0.00080513]
	Learning Rate: 0.000805135
	LOSS [training: 1.7647370696700868 | validation: 2.605458418984907]
	TIME [epoch: 8.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4340118869981806		[learning rate: 0.00080367]
		[batch 20/20] avg loss: 2.8448615712876846		[learning rate: 0.00080221]
	Learning Rate: 0.000802213
	LOSS [training: 2.6394367291429317 | validation: 2.923991291113862]
	TIME [epoch: 8.19 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.28084126990948		[learning rate: 0.00080076]
		[batch 20/20] avg loss: 3.5273554659846695		[learning rate: 0.0007993]
	Learning Rate: 0.000799301
	LOSS [training: 3.404098367947074 | validation: 4.181896752499005]
	TIME [epoch: 8.21 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.525642265464929		[learning rate: 0.00079785]
		[batch 20/20] avg loss: 3.859621241114579		[learning rate: 0.0007964]
	Learning Rate: 0.000796401
	LOSS [training: 3.692631753289754 | validation: 4.256986965863492]
	TIME [epoch: 8.19 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3931211564321067		[learning rate: 0.00079495]
		[batch 20/20] avg loss: 3.866900011313638		[learning rate: 0.00079351]
	Learning Rate: 0.000793511
	LOSS [training: 3.6300105838728727 | validation: 4.0624908650544915]
	TIME [epoch: 8.18 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3392329324217678		[learning rate: 0.00079207]
		[batch 20/20] avg loss: 2.9578717340196468		[learning rate: 0.00079063]
	Learning Rate: 0.000790631
	LOSS [training: 3.1485523332207075 | validation: 2.923460235017422]
	TIME [epoch: 8.18 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6764095969730373		[learning rate: 0.00078919]
		[batch 20/20] avg loss: 2.2400907383739623		[learning rate: 0.00078776]
	Learning Rate: 0.000787761
	LOSS [training: 2.4582501676735005 | validation: 1.8380469018631929]
	TIME [epoch: 8.21 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8050129349131048		[learning rate: 0.00078633]
		[batch 20/20] avg loss: 1.5055726120437738		[learning rate: 0.0007849]
	Learning Rate: 0.000784903
	LOSS [training: 1.6552927734784393 | validation: 1.4462008544278335]
	TIME [epoch: 8.19 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.689975580324333		[learning rate: 0.00078348]
		[batch 20/20] avg loss: 1.368351316163254		[learning rate: 0.00078205]
	Learning Rate: 0.000782054
	LOSS [training: 1.5291634482437935 | validation: 1.4847612660249647]
	TIME [epoch: 8.19 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2137574060096534		[learning rate: 0.00078063]
		[batch 20/20] avg loss: 1.65183045278735		[learning rate: 0.00077922]
	Learning Rate: 0.000779216
	LOSS [training: 1.4327939293985017 | validation: 1.7481918063312705]
	TIME [epoch: 8.19 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.265148521331748		[learning rate: 0.0007778]
		[batch 20/20] avg loss: 1.363336820347563		[learning rate: 0.00077639]
	Learning Rate: 0.000776388
	LOSS [training: 1.3142426708396555 | validation: 1.3503630200328656]
	TIME [epoch: 8.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1586445285494928		[learning rate: 0.00077498]
		[batch 20/20] avg loss: 1.1675099724375517		[learning rate: 0.00077357]
	Learning Rate: 0.000773571
	LOSS [training: 1.163077250493522 | validation: 1.1730439317129322]
	TIME [epoch: 8.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1315853784102756		[learning rate: 0.00077217]
		[batch 20/20] avg loss: 1.2609820968238536		[learning rate: 0.00077076]
	Learning Rate: 0.000770763
	LOSS [training: 1.1962837376170647 | validation: 1.3605427803358543]
	TIME [epoch: 8.19 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9006287527865563		[learning rate: 0.00076936]
		[batch 20/20] avg loss: 1.4752658877532892		[learning rate: 0.00076797]
	Learning Rate: 0.000767966
	LOSS [training: 1.187947320269923 | validation: 1.9045768144639288]
	TIME [epoch: 8.19 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2234567374696426		[learning rate: 0.00076657]
		[batch 20/20] avg loss: 0.8679602807900704		[learning rate: 0.00076518]
	Learning Rate: 0.000765179
	LOSS [training: 1.0457085091298564 | validation: 1.205756072423205]
	TIME [epoch: 8.21 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2570930906830682		[learning rate: 0.00076379]
		[batch 20/20] avg loss: 1.3615460855771422		[learning rate: 0.0007624]
	Learning Rate: 0.000762402
	LOSS [training: 1.3093195881301052 | validation: 0.9331474481727661]
	TIME [epoch: 8.19 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8388368210083211		[learning rate: 0.00076102]
		[batch 20/20] avg loss: 0.9570942239375434		[learning rate: 0.00075964]
	Learning Rate: 0.000759636
	LOSS [training: 0.897965522472932 | validation: 1.010067020837476]
	TIME [epoch: 8.19 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9478343570118806		[learning rate: 0.00075826]
		[batch 20/20] avg loss: 0.7947164524713696		[learning rate: 0.00075688]
	Learning Rate: 0.000756879
	LOSS [training: 0.8712754047416251 | validation: 0.9352231962320178]
	TIME [epoch: 8.19 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7238051266830343		[learning rate: 0.0007555]
		[batch 20/20] avg loss: 1.1997289690235566		[learning rate: 0.00075413]
	Learning Rate: 0.000754132
	LOSS [training: 0.9617670478532956 | validation: 1.3381579942662647]
	TIME [epoch: 8.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.003168715027467		[learning rate: 0.00075276]
		[batch 20/20] avg loss: 4.612612310381871		[learning rate: 0.0007514]
	Learning Rate: 0.000751395
	LOSS [training: 4.307890512704669 | validation: 2.5796459924444424]
	TIME [epoch: 8.19 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.541327121371788		[learning rate: 0.00075003]
		[batch 20/20] avg loss: 4.925393821465009		[learning rate: 0.00074867]
	Learning Rate: 0.000748668
	LOSS [training: 4.733360471418398 | validation: 3.499429761350184]
	TIME [epoch: 8.19 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.846972089974434		[learning rate: 0.00074731]
		[batch 20/20] avg loss: 3.5794790549724467		[learning rate: 0.00074595]
	Learning Rate: 0.000745951
	LOSS [training: 4.213225572473441 | validation: 1.6053546967390935]
	TIME [epoch: 8.18 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5638518424660743		[learning rate: 0.0007446]
		[batch 20/20] avg loss: 2.1091660899215725		[learning rate: 0.00074324]
	Learning Rate: 0.000743244
	LOSS [training: 2.336508966193823 | validation: 2.138577264164795]
	TIME [epoch: 8.21 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.025725777880761		[learning rate: 0.00074189]
		[batch 20/20] avg loss: 1.6268457562779948		[learning rate: 0.00074055]
	Learning Rate: 0.000740547
	LOSS [training: 1.8262857670793782 | validation: 1.5345266509139215]
	TIME [epoch: 8.18 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.425647523072581		[learning rate: 0.0007392]
		[batch 20/20] avg loss: 1.308182383914338		[learning rate: 0.00073786]
	Learning Rate: 0.00073786
	LOSS [training: 1.3669149534934597 | validation: 1.298863481029496]
	TIME [epoch: 8.19 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2031301371441763		[learning rate: 0.00073652]
		[batch 20/20] avg loss: 1.2597434023014795		[learning rate: 0.00073518]
	Learning Rate: 0.000735182
	LOSS [training: 1.2314367697228281 | validation: 1.4146922111491431]
	TIME [epoch: 8.19 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4996039960053729		[learning rate: 0.00073385]
		[batch 20/20] avg loss: 2.563969716936479		[learning rate: 0.00073251]
	Learning Rate: 0.000732514
	LOSS [training: 2.031786856470925 | validation: 2.8826536295703877]
	TIME [epoch: 8.21 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.351049212570975		[learning rate: 0.00073118]
		[batch 20/20] avg loss: 2.6773303938773445		[learning rate: 0.00072986]
	Learning Rate: 0.000729855
	LOSS [training: 3.0141898032241596 | validation: 2.487730559626905]
	TIME [epoch: 8.19 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6765696296615027		[learning rate: 0.00072853]
		[batch 20/20] avg loss: 1.582967191828947		[learning rate: 0.00072721]
	Learning Rate: 0.000727207
	LOSS [training: 1.6297684107452248 | validation: 2.8549495068858732]
	TIME [epoch: 8.19 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5401538147441807		[learning rate: 0.00072589]
		[batch 20/20] avg loss: 2.71279198509042		[learning rate: 0.00072457]
	Learning Rate: 0.000724568
	LOSS [training: 3.1264728999173004 | validation: 2.485939164053162]
	TIME [epoch: 8.19 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2308248150463097		[learning rate: 0.00072325]
		[batch 20/20] avg loss: 1.8611551381157212		[learning rate: 0.00072194]
	Learning Rate: 0.000721938
	LOSS [training: 2.0459899765810152 | validation: 2.1331646574138374]
	TIME [epoch: 8.21 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5489324434369809		[learning rate: 0.00072063]
		[batch 20/20] avg loss: 1.2037123954365196		[learning rate: 0.00071932]
	Learning Rate: 0.000719318
	LOSS [training: 1.3763224194367503 | validation: 1.3202358697315932]
	TIME [epoch: 8.19 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2034924050635074		[learning rate: 0.00071801]
		[batch 20/20] avg loss: 1.7607589666535322		[learning rate: 0.00071671]
	Learning Rate: 0.000716708
	LOSS [training: 1.4821256858585197 | validation: 2.0093939322064243]
	TIME [epoch: 8.18 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3943552004948616		[learning rate: 0.00071541]
		[batch 20/20] avg loss: 2.76936417688441		[learning rate: 0.00071411]
	Learning Rate: 0.000714107
	LOSS [training: 2.581859688689636 | validation: 2.1261524432924834]
	TIME [epoch: 8.19 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4265784983036043		[learning rate: 0.00071281]
		[batch 20/20] avg loss: 2.008616221400163		[learning rate: 0.00071152]
	Learning Rate: 0.000711515
	LOSS [training: 2.217597359851883 | validation: 1.3448829509647122]
	TIME [epoch: 8.21 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.346121914882205		[learning rate: 0.00071022]
		[batch 20/20] avg loss: 1.3954696957689299		[learning rate: 0.00070893]
	Learning Rate: 0.000708933
	LOSS [training: 1.370795805325567 | validation: 1.2761714222459717]
	TIME [epoch: 8.19 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.351858926700211		[learning rate: 0.00070765]
		[batch 20/20] avg loss: 1.8030025666187863		[learning rate: 0.00070636]
	Learning Rate: 0.00070636
	LOSS [training: 1.5774307466594988 | validation: 1.4129121647432459]
	TIME [epoch: 8.19 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3976555334901652		[learning rate: 0.00070508]
		[batch 20/20] avg loss: 1.280821027745538		[learning rate: 0.0007038]
	Learning Rate: 0.000703797
	LOSS [training: 1.3392382806178516 | validation: 1.2811763488424988]
	TIME [epoch: 8.18 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1954414849183852		[learning rate: 0.00070252]
		[batch 20/20] avg loss: 1.35017058957993		[learning rate: 0.00070124]
	Learning Rate: 0.000701243
	LOSS [training: 1.2728060372491579 | validation: 1.122489588207841]
	TIME [epoch: 8.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9397627095092673		[learning rate: 0.00069997]
		[batch 20/20] avg loss: 0.6458762654407091		[learning rate: 0.0006987]
	Learning Rate: 0.000698698
	LOSS [training: 0.7928194874749883 | validation: 0.5096555437776858]
	TIME [epoch: 8.19 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4900627419471532		[learning rate: 0.00069743]
		[batch 20/20] avg loss: 0.46944740329542667		[learning rate: 0.00069616]
	Learning Rate: 0.000696162
	LOSS [training: 0.47975507262128997 | validation: 0.32108816400753365]
	TIME [epoch: 8.19 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.330015820560824		[learning rate: 0.0006949]
		[batch 20/20] avg loss: 0.31539501486828725		[learning rate: 0.00069364]
	Learning Rate: 0.000693636
	LOSS [training: 0.3227054177145557 | validation: 0.25625256539377006]
	TIME [epoch: 8.19 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31385766014948907		[learning rate: 0.00069238]
		[batch 20/20] avg loss: 0.39555069041426516		[learning rate: 0.00069112]
	Learning Rate: 0.000691119
	LOSS [training: 0.3547041752818771 | validation: 0.19013396827895074]
	TIME [epoch: 8.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24318179169619344		[learning rate: 0.00068986]
		[batch 20/20] avg loss: 0.22318309423071775		[learning rate: 0.00068861]
	Learning Rate: 0.000688611
	LOSS [training: 0.2331824429634556 | validation: 0.2127563664872481]
	TIME [epoch: 8.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22631544795837022		[learning rate: 0.00068736]
		[batch 20/20] avg loss: 0.2076511578097997		[learning rate: 0.00068611]
	Learning Rate: 0.000686112
	LOSS [training: 0.21698330288408493 | validation: 0.23926518244166262]
	TIME [epoch: 8.19 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2468014930852303		[learning rate: 0.00068487]
		[batch 20/20] avg loss: 0.19770675849057753		[learning rate: 0.00068362]
	Learning Rate: 0.000683622
	LOSS [training: 0.22225412578790396 | validation: 0.14979889563081075]
	TIME [epoch: 8.19 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1694134463066079		[learning rate: 0.00068238]
		[batch 20/20] avg loss: 0.17205205376527472		[learning rate: 0.00068114]
	Learning Rate: 0.000681141
	LOSS [training: 0.1707327500359413 | validation: 0.14380033794602337]
	TIME [epoch: 8.19 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1830938765369959		[learning rate: 0.0006799]
		[batch 20/20] avg loss: 0.18286450883625838		[learning rate: 0.00067867]
	Learning Rate: 0.000678669
	LOSS [training: 0.18297919268662713 | validation: 0.1284298180171432]
	TIME [epoch: 8.21 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15180510628292748		[learning rate: 0.00067744]
		[batch 20/20] avg loss: 0.1962817704954333		[learning rate: 0.00067621]
	Learning Rate: 0.000676206
	LOSS [training: 0.17404343838918038 | validation: 0.1256211933243602]
	TIME [epoch: 8.19 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16522073883453564		[learning rate: 0.00067498]
		[batch 20/20] avg loss: 0.17219696493437792		[learning rate: 0.00067375]
	Learning Rate: 0.000673752
	LOSS [training: 0.16870885188445678 | validation: 0.13484818860195516]
	TIME [epoch: 8.19 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21260263628718784		[learning rate: 0.00067253]
		[batch 20/20] avg loss: 0.15623365053058		[learning rate: 0.00067131]
	Learning Rate: 0.000671307
	LOSS [training: 0.1844181434088839 | validation: 0.15721881793859838]
	TIME [epoch: 8.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2016325977400017		[learning rate: 0.00067009]
		[batch 20/20] avg loss: 0.16845990535746372		[learning rate: 0.00066887]
	Learning Rate: 0.000668871
	LOSS [training: 0.18504625154873272 | validation: 0.19383708491809643]
	TIME [epoch: 8.21 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17763529454758195		[learning rate: 0.00066766]
		[batch 20/20] avg loss: 0.20061641082848616		[learning rate: 0.00066644]
	Learning Rate: 0.000666443
	LOSS [training: 0.18912585268803406 | validation: 0.19036184111599927]
	TIME [epoch: 8.19 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13414631907862976		[learning rate: 0.00066523]
		[batch 20/20] avg loss: 0.14879413417550874		[learning rate: 0.00066402]
	Learning Rate: 0.000664025
	LOSS [training: 0.14147022662706926 | validation: 0.11125105301420771]
	TIME [epoch: 8.18 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14474039823559412		[learning rate: 0.00066282]
		[batch 20/20] avg loss: 0.13669005256994216		[learning rate: 0.00066161]
	Learning Rate: 0.000661615
	LOSS [training: 0.14071522540276812 | validation: 0.1296179250402394]
	TIME [epoch: 8.18 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1562873996484942		[learning rate: 0.00066041]
		[batch 20/20] avg loss: 0.15940869557063989		[learning rate: 0.00065921]
	Learning Rate: 0.000659214
	LOSS [training: 0.15784804760956703 | validation: 0.09507065769138298]
	TIME [epoch: 8.21 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1550332218668748		[learning rate: 0.00065802]
		[batch 20/20] avg loss: 0.15591812181861456		[learning rate: 0.00065682]
	Learning Rate: 0.000656822
	LOSS [training: 0.15547567184274466 | validation: 0.133390009650556]
	TIME [epoch: 8.18 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.151544763491152		[learning rate: 0.00065563]
		[batch 20/20] avg loss: 0.14073817083315882		[learning rate: 0.00065444]
	Learning Rate: 0.000654438
	LOSS [training: 0.14614146716215543 | validation: 0.08815128228558043]
	TIME [epoch: 8.18 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1582305043677778		[learning rate: 0.00065325]
		[batch 20/20] avg loss: 0.1399868616309173		[learning rate: 0.00065206]
	Learning Rate: 0.000652063
	LOSS [training: 0.14910868299934757 | validation: 0.09223781094791572]
	TIME [epoch: 8.19 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15255998358706918		[learning rate: 0.00065088]
		[batch 20/20] avg loss: 0.1303012158159986		[learning rate: 0.0006497]
	Learning Rate: 0.000649697
	LOSS [training: 0.1414305997015339 | validation: 0.09399125937794968]
	TIME [epoch: 8.21 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14645140591828404		[learning rate: 0.00064852]
		[batch 20/20] avg loss: 0.15913490637527314		[learning rate: 0.00064734]
	Learning Rate: 0.000647339
	LOSS [training: 0.1527931561467786 | validation: 0.10063930023732374]
	TIME [epoch: 8.19 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16719605760414313		[learning rate: 0.00064616]
		[batch 20/20] avg loss: 0.16015615861885935		[learning rate: 0.00064499]
	Learning Rate: 0.00064499
	LOSS [training: 0.1636761081115012 | validation: 0.09348995970739679]
	TIME [epoch: 8.19 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1335986050279101		[learning rate: 0.00064382]
		[batch 20/20] avg loss: 0.1301074735688605		[learning rate: 0.00064265]
	Learning Rate: 0.000642649
	LOSS [training: 0.1318530392983853 | validation: 0.14191410722511455]
	TIME [epoch: 8.19 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1891850890533135		[learning rate: 0.00064148]
		[batch 20/20] avg loss: 0.18897951293346515		[learning rate: 0.00064032]
	Learning Rate: 0.000640317
	LOSS [training: 0.18908230099338935 | validation: 0.16147294674617818]
	TIME [epoch: 8.21 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12505067878872766		[learning rate: 0.00063915]
		[batch 20/20] avg loss: 0.12783644073824635		[learning rate: 0.00063799]
	Learning Rate: 0.000637993
	LOSS [training: 0.12644355976348698 | validation: 0.10847933952818922]
	TIME [epoch: 8.18 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15171287895857644		[learning rate: 0.00063683]
		[batch 20/20] avg loss: 0.14869445960068617		[learning rate: 0.00063568]
	Learning Rate: 0.000635677
	LOSS [training: 0.1502036692796313 | validation: 0.11056825623311398]
	TIME [epoch: 8.19 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13119157048810667		[learning rate: 0.00063452]
		[batch 20/20] avg loss: 0.1557908529037187		[learning rate: 0.00063337]
	Learning Rate: 0.000633371
	LOSS [training: 0.14349121169591267 | validation: 0.10345911839762298]
	TIME [epoch: 8.19 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15842842803276702		[learning rate: 0.00063222]
		[batch 20/20] avg loss: 0.20633696749772262		[learning rate: 0.00063107]
	Learning Rate: 0.000631072
	LOSS [training: 0.18238269776524482 | validation: 0.2489948127721982]
	TIME [epoch: 8.21 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1766916960862932		[learning rate: 0.00062993]
		[batch 20/20] avg loss: 0.17278223325971873		[learning rate: 0.00062878]
	Learning Rate: 0.000628782
	LOSS [training: 0.17473696467300592 | validation: 0.12826860084176853]
	TIME [epoch: 8.19 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13436948204970117		[learning rate: 0.00062764]
		[batch 20/20] avg loss: 0.11719516255053189		[learning rate: 0.0006265]
	Learning Rate: 0.0006265
	LOSS [training: 0.12578232230011654 | validation: 0.11407864007166943]
	TIME [epoch: 8.19 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22425237528570893		[learning rate: 0.00062536]
		[batch 20/20] avg loss: 0.12907990460187752		[learning rate: 0.00062423]
	Learning Rate: 0.000624226
	LOSS [training: 0.17666613994379315 | validation: 0.08630483633934569]
	TIME [epoch: 8.18 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1374098876140718		[learning rate: 0.00062309]
		[batch 20/20] avg loss: 0.1349153831182012		[learning rate: 0.00062196]
	Learning Rate: 0.000621961
	LOSS [training: 0.13616263536613651 | validation: 0.16326341510563883]
	TIME [epoch: 8.21 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18810781395198772		[learning rate: 0.00062083]
		[batch 20/20] avg loss: 0.15191643602513658		[learning rate: 0.0006197]
	Learning Rate: 0.000619704
	LOSS [training: 0.17001212498856216 | validation: 0.11292746478452048]
	TIME [epoch: 8.18 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4015812880780141		[learning rate: 0.00061858]
		[batch 20/20] avg loss: 0.21590772199998		[learning rate: 0.00061745]
	Learning Rate: 0.000617455
	LOSS [training: 0.30874450503899703 | validation: 0.27940652048685255]
	TIME [epoch: 8.19 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2223965945321031		[learning rate: 0.00061633]
		[batch 20/20] avg loss: 0.1297699928372939		[learning rate: 0.00061521]
	Learning Rate: 0.000615214
	LOSS [training: 0.1760832936846985 | validation: 0.15156372729404854]
	TIME [epoch: 8.18 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1627884987703188		[learning rate: 0.0006141]
		[batch 20/20] avg loss: 0.16226630359055022		[learning rate: 0.00061298]
	Learning Rate: 0.000612982
	LOSS [training: 0.16252740118043452 | validation: 0.18336943771497727]
	TIME [epoch: 8.21 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17843432932387646		[learning rate: 0.00061187]
		[batch 20/20] avg loss: 0.140910482029363		[learning rate: 0.00061076]
	Learning Rate: 0.000610757
	LOSS [training: 0.15967240567661972 | validation: 0.17228432354004955]
	TIME [epoch: 8.19 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2610842591615644		[learning rate: 0.00060965]
		[batch 20/20] avg loss: 0.15965198929180358		[learning rate: 0.00060854]
	Learning Rate: 0.00060854
	LOSS [training: 0.210368124226684 | validation: 0.11828361156801821]
	TIME [epoch: 8.19 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14074098774890428		[learning rate: 0.00060744]
		[batch 20/20] avg loss: 0.22621845641151758		[learning rate: 0.00060633]
	Learning Rate: 0.000606332
	LOSS [training: 0.18347972208021096 | validation: 0.12964811743234456]
	TIME [epoch: 8.19 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14614196621637593		[learning rate: 0.00060523]
		[batch 20/20] avg loss: 0.15925384274279553		[learning rate: 0.00060413]
	Learning Rate: 0.000604132
	LOSS [training: 0.1526979044795857 | validation: 0.08845602397957887]
	TIME [epoch: 8.21 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12157637062117783		[learning rate: 0.00060303]
		[batch 20/20] avg loss: 0.2844710524367164		[learning rate: 0.00060194]
	Learning Rate: 0.000601939
	LOSS [training: 0.20302371152894705 | validation: 0.16953208489981758]
	TIME [epoch: 8.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1927311329278077		[learning rate: 0.00060085]
		[batch 20/20] avg loss: 0.14711458829393054		[learning rate: 0.00059975]
	Learning Rate: 0.000599755
	LOSS [training: 0.16992286061086911 | validation: 0.24045271720869915]
	TIME [epoch: 8.19 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1517181058235038		[learning rate: 0.00059867]
		[batch 20/20] avg loss: 0.13702596134571254		[learning rate: 0.00059758]
	Learning Rate: 0.000597578
	LOSS [training: 0.14437203358460815 | validation: 0.10381440753106386]
	TIME [epoch: 8.19 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14577562756480342		[learning rate: 0.00059649]
		[batch 20/20] avg loss: 0.1655210200156151		[learning rate: 0.00059541]
	Learning Rate: 0.00059541
	LOSS [training: 0.15564832379020926 | validation: 0.2878120790092631]
	TIME [epoch: 8.21 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21881690335999773		[learning rate: 0.00059433]
		[batch 20/20] avg loss: 0.17361821441917508		[learning rate: 0.00059325]
	Learning Rate: 0.000593249
	LOSS [training: 0.1962175588895864 | validation: 0.12751606058802673]
	TIME [epoch: 8.19 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1652522485404043		[learning rate: 0.00059217]
		[batch 20/20] avg loss: 0.1365872870449114		[learning rate: 0.0005911]
	Learning Rate: 0.000591096
	LOSS [training: 0.15091976779265784 | validation: 0.11364011075681961]
	TIME [epoch: 8.19 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17448483996834435		[learning rate: 0.00059002]
		[batch 20/20] avg loss: 0.1230464574288798		[learning rate: 0.00058895]
	Learning Rate: 0.000588951
	LOSS [training: 0.14876564869861209 | validation: 0.2899764322862485]
	TIME [epoch: 8.19 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.153494611301696		[learning rate: 0.00058788]
		[batch 20/20] avg loss: 0.20707091852553322		[learning rate: 0.00058681]
	Learning Rate: 0.000586813
	LOSS [training: 0.18028276491361458 | validation: 0.11147563117843108]
	TIME [epoch: 8.22 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16403913191948313		[learning rate: 0.00058575]
		[batch 20/20] avg loss: 0.2773827020402445		[learning rate: 0.00058468]
	Learning Rate: 0.000584684
	LOSS [training: 0.22071091697986384 | validation: 0.16605233729739519]
	TIME [epoch: 8.19 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14847527193404947		[learning rate: 0.00058362]
		[batch 20/20] avg loss: 0.24164208016942737		[learning rate: 0.00058256]
	Learning Rate: 0.000582562
	LOSS [training: 0.1950586760517384 | validation: 0.17956298934902962]
	TIME [epoch: 8.19 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3834413594492146		[learning rate: 0.0005815]
		[batch 20/20] avg loss: 0.15695289420075836		[learning rate: 0.00058045]
	Learning Rate: 0.000580448
	LOSS [training: 0.27019712682498653 | validation: 0.10964573445468073]
	TIME [epoch: 8.19 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17740176084221707		[learning rate: 0.00057939]
		[batch 20/20] avg loss: 0.15150568198850087		[learning rate: 0.00057834]
	Learning Rate: 0.000578341
	LOSS [training: 0.16445372141535897 | validation: 0.10670450941340519]
	TIME [epoch: 8.21 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1875131202732364		[learning rate: 0.00057729]
		[batch 20/20] avg loss: 0.14242140108680001		[learning rate: 0.00057624]
	Learning Rate: 0.000576243
	LOSS [training: 0.1649672606800182 | validation: 0.09340421824805584]
	TIME [epoch: 8.19 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17757199605871926		[learning rate: 0.0005752]
		[batch 20/20] avg loss: 0.1608735988700689		[learning rate: 0.00057415]
	Learning Rate: 0.000574151
	LOSS [training: 0.16922279746439411 | validation: 0.18777359007501138]
	TIME [epoch: 8.19 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15092503750668207		[learning rate: 0.00057311]
		[batch 20/20] avg loss: 0.1492815236873783		[learning rate: 0.00057207]
	Learning Rate: 0.000572068
	LOSS [training: 0.15010328059703018 | validation: 0.11189989827971383]
	TIME [epoch: 8.19 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15000300245346468		[learning rate: 0.00057103]
		[batch 20/20] avg loss: 0.18894414909001564		[learning rate: 0.00056999]
	Learning Rate: 0.000569992
	LOSS [training: 0.16947357577174013 | validation: 0.18682885246861908]
	TIME [epoch: 8.21 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13150161074730304		[learning rate: 0.00056896]
		[batch 20/20] avg loss: 0.17340425775627627		[learning rate: 0.00056792]
	Learning Rate: 0.000567923
	LOSS [training: 0.15245293425178966 | validation: 0.1388747466581203]
	TIME [epoch: 8.19 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15070524514102734		[learning rate: 0.00056689]
		[batch 20/20] avg loss: 0.12515903127184183		[learning rate: 0.00056586]
	Learning Rate: 0.000565862
	LOSS [training: 0.13793213820643457 | validation: 0.1138820577801817]
	TIME [epoch: 8.19 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14384301727496715		[learning rate: 0.00056483]
		[batch 20/20] avg loss: 0.13662616334672434		[learning rate: 0.00056381]
	Learning Rate: 0.000563808
	LOSS [training: 0.14023459031084579 | validation: 0.1289027591775696]
	TIME [epoch: 8.19 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12724244937344298		[learning rate: 0.00056278]
		[batch 20/20] avg loss: 0.14219899847293546		[learning rate: 0.00056176]
	Learning Rate: 0.000561762
	LOSS [training: 0.13472072392318926 | validation: 0.10089868575541641]
	TIME [epoch: 8.21 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13818471939326377		[learning rate: 0.00056074]
		[batch 20/20] avg loss: 0.1400799537118988		[learning rate: 0.00055972]
	Learning Rate: 0.000559724
	LOSS [training: 0.1391323365525813 | validation: 0.17710798475377645]
	TIME [epoch: 8.19 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14528568650890114		[learning rate: 0.00055871]
		[batch 20/20] avg loss: 0.16707669961311558		[learning rate: 0.00055769]
	Learning Rate: 0.000557692
	LOSS [training: 0.1561811930610084 | validation: 0.11072500524336215]
	TIME [epoch: 8.19 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15024496960036032		[learning rate: 0.00055668]
		[batch 20/20] avg loss: 0.11316234496196795		[learning rate: 0.00055567]
	Learning Rate: 0.000555669
	LOSS [training: 0.13170365728116412 | validation: 0.12244092154123828]
	TIME [epoch: 8.19 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16679789937374673		[learning rate: 0.00055466]
		[batch 20/20] avg loss: 0.1722163179463146		[learning rate: 0.00055365]
	Learning Rate: 0.000553652
	LOSS [training: 0.1695071086600307 | validation: 0.13999479875797424]
	TIME [epoch: 8.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18085020999540266		[learning rate: 0.00055265]
		[batch 20/20] avg loss: 0.17670650911719077		[learning rate: 0.00055164]
	Learning Rate: 0.000551643
	LOSS [training: 0.1787783595562967 | validation: 0.11198881315607209]
	TIME [epoch: 8.19 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15074829069540926		[learning rate: 0.00055064]
		[batch 20/20] avg loss: 0.21145011149497153		[learning rate: 0.00054964]
	Learning Rate: 0.000549641
	LOSS [training: 0.18109920109519037 | validation: 0.14068800192890518]
	TIME [epoch: 8.19 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.140469520492681		[learning rate: 0.00054864]
		[batch 20/20] avg loss: 0.1623871663028342		[learning rate: 0.00054765]
	Learning Rate: 0.000547646
	LOSS [training: 0.15142834339775763 | validation: 0.16891976007771714]
	TIME [epoch: 8.19 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24687507537267916		[learning rate: 0.00054665]
		[batch 20/20] avg loss: 0.1838258443940267		[learning rate: 0.00054566]
	Learning Rate: 0.000545659
	LOSS [training: 0.21535045988335294 | validation: 0.08157816137691795]
	TIME [epoch: 8.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1462360438444064		[learning rate: 0.00054467]
		[batch 20/20] avg loss: 0.21756068541513746		[learning rate: 0.00054368]
	Learning Rate: 0.000543678
	LOSS [training: 0.1818983646297719 | validation: 0.1872271787914468]
	TIME [epoch: 8.19 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15087659717877772		[learning rate: 0.00054269]
		[batch 20/20] avg loss: 0.13214116189930605		[learning rate: 0.00054171]
	Learning Rate: 0.000541705
	LOSS [training: 0.14150887953904193 | validation: 0.10511082875055572]
	TIME [epoch: 8.19 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16888462950781885		[learning rate: 0.00054072]
		[batch 20/20] avg loss: 0.14355080114449525		[learning rate: 0.00053974]
	Learning Rate: 0.00053974
	LOSS [training: 0.15621771532615705 | validation: 0.11458697419452847]
	TIME [epoch: 8.19 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21271690812475766		[learning rate: 0.00053876]
		[batch 20/20] avg loss: 0.33000964739876737		[learning rate: 0.00053778]
	Learning Rate: 0.000537781
	LOSS [training: 0.27136327776176256 | validation: 0.3621911610849299]
	TIME [epoch: 8.21 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8595993963800217		[learning rate: 0.0005368]
		[batch 20/20] avg loss: 0.35029380845079944		[learning rate: 0.00053583]
	Learning Rate: 0.000535829
	LOSS [training: 0.6049466024154105 | validation: 0.26493036175077517]
	TIME [epoch: 8.19 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1687236132367282		[learning rate: 0.00053486]
		[batch 20/20] avg loss: 0.14206298461092765		[learning rate: 0.00053388]
	Learning Rate: 0.000533885
	LOSS [training: 0.15539329892382794 | validation: 0.22308895726910585]
	TIME [epoch: 8.19 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18411484143398996		[learning rate: 0.00053291]
		[batch 20/20] avg loss: 0.5002571895779228		[learning rate: 0.00053195]
	Learning Rate: 0.000531947
	LOSS [training: 0.34218601550595634 | validation: 0.2789343815545184]
	TIME [epoch: 8.19 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2655892165101972		[learning rate: 0.00053098]
		[batch 20/20] avg loss: 4.0507144343091275		[learning rate: 0.00053002]
	Learning Rate: 0.000530017
	LOSS [training: 2.658151825409662 | validation: 3.544295469284438]
	TIME [epoch: 8.21 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.947731518448225		[learning rate: 0.00052905]
		[batch 20/20] avg loss: 3.444010798540571		[learning rate: 0.00052809]
	Learning Rate: 0.000528093
	LOSS [training: 3.695871158494397 | validation: 2.3519906718404533]
	TIME [epoch: 8.2 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.652134953756118		[learning rate: 0.00052713]
		[batch 20/20] avg loss: 1.2635301105378092		[learning rate: 0.00052618]
	Learning Rate: 0.000526177
	LOSS [training: 1.957832532146964 | validation: 1.386241156425672]
	TIME [epoch: 8.19 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1141793773007977		[learning rate: 0.00052522]
		[batch 20/20] avg loss: 3.6571211835773587		[learning rate: 0.00052427]
	Learning Rate: 0.000524267
	LOSS [training: 2.8856502804390782 | validation: 2.6112628280935333]
	TIME [epoch: 8.19 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.430419003365761		[learning rate: 0.00052332]
		[batch 20/20] avg loss: 3.154940961198387		[learning rate: 0.00052236]
	Learning Rate: 0.000522365
	LOSS [training: 2.7926799822820736 | validation: 3.9357981542935168]
	TIME [epoch: 8.21 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.035779891939169		[learning rate: 0.00052142]
		[batch 20/20] avg loss: 5.459422120814934		[learning rate: 0.00052047]
	Learning Rate: 0.000520469
	LOSS [training: 4.747601006377052 | validation: 4.765066041957442]
	TIME [epoch: 8.19 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.570930268908078		[learning rate: 0.00051952]
		[batch 20/20] avg loss: 3.8710286806199496		[learning rate: 0.00051858]
	Learning Rate: 0.00051858
	LOSS [training: 4.720979474764014 | validation: 2.9901931902627803]
	TIME [epoch: 8.18 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.318644411629488		[learning rate: 0.00051764]
		[batch 20/20] avg loss: 1.943599125600333		[learning rate: 0.0005167]
	Learning Rate: 0.000516698
	LOSS [training: 2.131121768614911 | validation: 1.6211863523066103]
	TIME [epoch: 8.19 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7767483155995858		[learning rate: 0.00051576]
		[batch 20/20] avg loss: 1.9435305350797663		[learning rate: 0.00051482]
	Learning Rate: 0.000514823
	LOSS [training: 1.8601394253396761 | validation: 1.9141705497522126]
	TIME [epoch: 8.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5419869580692356		[learning rate: 0.00051389]
		[batch 20/20] avg loss: 1.2567190552575727		[learning rate: 0.00051295]
	Learning Rate: 0.000512955
	LOSS [training: 1.3993530066634043 | validation: 0.6067886784068909]
	TIME [epoch: 8.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4127402498837		[learning rate: 0.00051202]
		[batch 20/20] avg loss: 0.24689898521182702		[learning rate: 0.00051109]
	Learning Rate: 0.000511093
	LOSS [training: 0.32981961754776357 | validation: 0.13341239223144102]
	TIME [epoch: 8.19 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22010340877025464		[learning rate: 0.00051016]
		[batch 20/20] avg loss: 0.21595799677246666		[learning rate: 0.00050924]
	Learning Rate: 0.000509238
	LOSS [training: 0.2180307027713606 | validation: 0.16054175139636095]
	TIME [epoch: 8.19 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23854941565983281		[learning rate: 0.00050831]
		[batch 20/20] avg loss: 0.8079121743323524		[learning rate: 0.00050739]
	Learning Rate: 0.00050739
	LOSS [training: 0.5232307949960926 | validation: 0.6151722440024637]
	TIME [epoch: 8.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3560315403913052		[learning rate: 0.00050647]
		[batch 20/20] avg loss: 0.8393347683391378		[learning rate: 0.00050555]
	Learning Rate: 0.000505549
	LOSS [training: 0.5976831543652215 | validation: 0.6169425781154168]
	TIME [epoch: 8.2 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5762453205139373		[learning rate: 0.00050463]
		[batch 20/20] avg loss: 0.44570347753190215		[learning rate: 0.00050371]
	Learning Rate: 0.000503714
	LOSS [training: 0.5109743990229199 | validation: 0.504866191786561]
	TIME [epoch: 8.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43284700596181114		[learning rate: 0.0005028]
		[batch 20/20] avg loss: 0.3499984429645726		[learning rate: 0.00050189]
	Learning Rate: 0.000501886
	LOSS [training: 0.3914227244631919 | validation: 0.17190923599643718]
	TIME [epoch: 8.19 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3008398094908908		[learning rate: 0.00050097]
		[batch 20/20] avg loss: 0.2847445238251137		[learning rate: 0.00050006]
	Learning Rate: 0.000500065
	LOSS [training: 0.2927921666580023 | validation: 0.1405650801094415]
	TIME [epoch: 8.2 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1835040312786588		[learning rate: 0.00049916]
		[batch 20/20] avg loss: 0.4600780677274775		[learning rate: 0.00049825]
	Learning Rate: 0.00049825
	LOSS [training: 0.32179104950306814 | validation: 0.4212101085804793]
	TIME [epoch: 8.21 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37125488136850615		[learning rate: 0.00049735]
		[batch 20/20] avg loss: 0.23652198352957177		[learning rate: 0.00049644]
	Learning Rate: 0.000496442
	LOSS [training: 0.30388843244903885 | validation: 0.3433642857875997]
	TIME [epoch: 8.19 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2982174576388493		[learning rate: 0.00049554]
		[batch 20/20] avg loss: 0.17873877013130143		[learning rate: 0.00049464]
	Learning Rate: 0.00049464
	LOSS [training: 0.23847811388507534 | validation: 0.17296955406458936]
	TIME [epoch: 8.18 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27140823059512387		[learning rate: 0.00049374]
		[batch 20/20] avg loss: 0.2168557978270398		[learning rate: 0.00049285]
	Learning Rate: 0.000492845
	LOSS [training: 0.24413201421108183 | validation: 0.2195952731688777]
	TIME [epoch: 8.19 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18163173032866717		[learning rate: 0.00049195]
		[batch 20/20] avg loss: 0.20437177525494715		[learning rate: 0.00049106]
	Learning Rate: 0.000491057
	LOSS [training: 0.19300175279180715 | validation: 0.15760927506364508]
	TIME [epoch: 8.22 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16678485885768315		[learning rate: 0.00049016]
		[batch 20/20] avg loss: 0.21380982553221495		[learning rate: 0.00048927]
	Learning Rate: 0.000489275
	LOSS [training: 0.19029734219494904 | validation: 0.19955746446186493]
	TIME [epoch: 8.19 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24832811293880902		[learning rate: 0.00048839]
		[batch 20/20] avg loss: 0.17757877608501574		[learning rate: 0.0004875]
	Learning Rate: 0.000487499
	LOSS [training: 0.21295344451191234 | validation: 0.13053504125500176]
	TIME [epoch: 8.19 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14191809432244787		[learning rate: 0.00048661]
		[batch 20/20] avg loss: 0.18838916871022943		[learning rate: 0.00048573]
	Learning Rate: 0.00048573
	LOSS [training: 0.16515363151633863 | validation: 0.1530089700521926]
	TIME [epoch: 8.19 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1744886594027478		[learning rate: 0.00048485]
		[batch 20/20] avg loss: 0.14863150499222805		[learning rate: 0.00048397]
	Learning Rate: 0.000483967
	LOSS [training: 0.16156008219748794 | validation: 0.15199374011032007]
	TIME [epoch: 8.21 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18636668433789635		[learning rate: 0.00048309]
		[batch 20/20] avg loss: 0.1406112105745154		[learning rate: 0.00048221]
	Learning Rate: 0.000482211
	LOSS [training: 0.16348894745620585 | validation: 0.125430322183375]
	TIME [epoch: 8.19 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1811078102109974		[learning rate: 0.00048133]
		[batch 20/20] avg loss: 0.3922470340480593		[learning rate: 0.00048046]
	Learning Rate: 0.000480461
	LOSS [training: 0.2866774221295284 | validation: 0.8095160509355501]
	TIME [epoch: 8.19 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9450177078512949		[learning rate: 0.00047959]
		[batch 20/20] avg loss: 0.3002041446419817		[learning rate: 0.00047872]
	Learning Rate: 0.000478717
	LOSS [training: 0.6226109262466383 | validation: 0.1782152659508612]
	TIME [epoch: 8.19 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1856423048897497		[learning rate: 0.00047785]
		[batch 20/20] avg loss: 0.13522657838275845		[learning rate: 0.00047698]
	Learning Rate: 0.00047698
	LOSS [training: 0.16043444163625406 | validation: 0.17496754493463]
	TIME [epoch: 8.22 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15979948675326025		[learning rate: 0.00047611]
		[batch 20/20] avg loss: 0.169693744357998		[learning rate: 0.00047525]
	Learning Rate: 0.000475249
	LOSS [training: 0.16474661555562917 | validation: 0.12043773367367994]
	TIME [epoch: 8.19 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29759795670600864		[learning rate: 0.00047439]
		[batch 20/20] avg loss: 0.18710633115401715		[learning rate: 0.00047352]
	Learning Rate: 0.000473524
	LOSS [training: 0.24235214393001286 | validation: 0.15036832632653813]
	TIME [epoch: 8.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1567777991105876		[learning rate: 0.00047266]
		[batch 20/20] avg loss: 0.15676633011922309		[learning rate: 0.00047181]
	Learning Rate: 0.000471806
	LOSS [training: 0.1567720646149053 | validation: 0.10590611680459491]
	TIME [epoch: 8.19 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3128795293940373		[learning rate: 0.00047095]
		[batch 20/20] avg loss: 0.14782237603752063		[learning rate: 0.00047009]
	Learning Rate: 0.000470093
	LOSS [training: 0.230350952715779 | validation: 0.17404402145071562]
	TIME [epoch: 8.22 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1668920756578944		[learning rate: 0.00046924]
		[batch 20/20] avg loss: 0.17729908379185014		[learning rate: 0.00046839]
	Learning Rate: 0.000468388
	LOSS [training: 0.17209557972487227 | validation: 0.0872238820372916]
	TIME [epoch: 8.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16931105784332484		[learning rate: 0.00046754]
		[batch 20/20] avg loss: 0.13522333261733113		[learning rate: 0.00046669]
	Learning Rate: 0.000466688
	LOSS [training: 0.152267195230328 | validation: 0.133720885253166]
	TIME [epoch: 8.19 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20305281206323741		[learning rate: 0.00046584]
		[batch 20/20] avg loss: 0.18101637159196016		[learning rate: 0.00046499]
	Learning Rate: 0.000464994
	LOSS [training: 0.1920345918275988 | validation: 0.10670570186486564]
	TIME [epoch: 8.19 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13619883061150695		[learning rate: 0.00046415]
		[batch 20/20] avg loss: 0.16019242280814358		[learning rate: 0.00046331]
	Learning Rate: 0.000463307
	LOSS [training: 0.14819562670982525 | validation: 0.12408975241003767]
	TIME [epoch: 8.21 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14150024120244506		[learning rate: 0.00046247]
		[batch 20/20] avg loss: 0.198420813179575		[learning rate: 0.00046163]
	Learning Rate: 0.000461625
	LOSS [training: 0.16996052719101004 | validation: 0.12655343831004426]
	TIME [epoch: 8.19 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1519135271458753		[learning rate: 0.00046079]
		[batch 20/20] avg loss: 0.1470044430289353		[learning rate: 0.00045995]
	Learning Rate: 0.00045995
	LOSS [training: 0.14945898508740532 | validation: 0.15575057539915452]
	TIME [epoch: 8.19 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13124393437633505		[learning rate: 0.00045911]
		[batch 20/20] avg loss: 0.15901073771001978		[learning rate: 0.00045828]
	Learning Rate: 0.000458281
	LOSS [training: 0.14512733604317743 | validation: 0.10805850072338832]
	TIME [epoch: 8.18 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1598360978800208		[learning rate: 0.00045745]
		[batch 20/20] avg loss: 0.26990397878393224		[learning rate: 0.00045662]
	Learning Rate: 0.000456618
	LOSS [training: 0.21487003833197652 | validation: 0.14430265204647857]
	TIME [epoch: 8.21 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21200539686156988		[learning rate: 0.00045579]
		[batch 20/20] avg loss: 0.386932540640261		[learning rate: 0.00045496]
	Learning Rate: 0.000454961
	LOSS [training: 0.29946896875091544 | validation: 0.271551461253552]
	TIME [epoch: 8.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.185313344263827		[learning rate: 0.00045413]
		[batch 20/20] avg loss: 0.15913872515838143		[learning rate: 0.00045331]
	Learning Rate: 0.000453309
	LOSS [training: 0.17222603471110415 | validation: 0.1257833175808905]
	TIME [epoch: 8.19 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13794192423555224		[learning rate: 0.00045249]
		[batch 20/20] avg loss: 0.18668510973085098		[learning rate: 0.00045166]
	Learning Rate: 0.000451664
	LOSS [training: 0.16231351698320162 | validation: 0.09609576936252864]
	TIME [epoch: 8.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14476045990362466		[learning rate: 0.00045084]
		[batch 20/20] avg loss: 0.1764184160179588		[learning rate: 0.00045003]
	Learning Rate: 0.000450025
	LOSS [training: 0.16058943796079175 | validation: 0.1056003695482252]
	TIME [epoch: 8.21 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18605852176868984		[learning rate: 0.00044921]
		[batch 20/20] avg loss: 0.1733515423964038		[learning rate: 0.00044839]
	Learning Rate: 0.000448392
	LOSS [training: 0.17970503208254682 | validation: 0.10818735357705758]
	TIME [epoch: 8.19 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3201687943869005		[learning rate: 0.00044758]
		[batch 20/20] avg loss: 0.2827391013476124		[learning rate: 0.00044676]
	Learning Rate: 0.000446765
	LOSS [training: 0.3014539478672564 | validation: 0.11010220787400199]
	TIME [epoch: 8.19 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16143328082031225		[learning rate: 0.00044595]
		[batch 20/20] avg loss: 0.3200970446347247		[learning rate: 0.00044514]
	Learning Rate: 0.000445143
	LOSS [training: 0.24076516272751852 | validation: 0.2148470957178706]
	TIME [epoch: 8.19 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24090901644960683		[learning rate: 0.00044434]
		[batch 20/20] avg loss: 0.6897096191547821		[learning rate: 0.00044353]
	Learning Rate: 0.000443528
	LOSS [training: 0.4653093178021946 | validation: 0.2682515479256255]
	TIME [epoch: 8.22 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2720278976441968		[learning rate: 0.00044272]
		[batch 20/20] avg loss: 0.30335530146168893		[learning rate: 0.00044192]
	Learning Rate: 0.000441918
	LOSS [training: 0.2876915995529429 | validation: 0.14732990350894765]
	TIME [epoch: 8.19 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20552350011755127		[learning rate: 0.00044112]
		[batch 20/20] avg loss: 0.17715071594236426		[learning rate: 0.00044031]
	Learning Rate: 0.000440315
	LOSS [training: 0.1913371080299578 | validation: 0.11717593193142915]
	TIME [epoch: 8.19 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13579475615916187		[learning rate: 0.00043952]
		[batch 20/20] avg loss: 0.34963318865827614		[learning rate: 0.00043872]
	Learning Rate: 0.000438717
	LOSS [training: 0.242713972408719 | validation: 0.42644910826677906]
	TIME [epoch: 8.19 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36121256781909844		[learning rate: 0.00043792]
		[batch 20/20] avg loss: 0.1800207597650235		[learning rate: 0.00043712]
	Learning Rate: 0.000437125
	LOSS [training: 0.27061666379206095 | validation: 0.21534304639103433]
	TIME [epoch: 8.21 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26637450736464496		[learning rate: 0.00043633]
		[batch 20/20] avg loss: 0.28448737890562653		[learning rate: 0.00043554]
	Learning Rate: 0.000435538
	LOSS [training: 0.2754309431351357 | validation: 0.12803032419131705]
	TIME [epoch: 8.19 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28743841701397344		[learning rate: 0.00043475]
		[batch 20/20] avg loss: 0.21221785758772257		[learning rate: 0.00043396]
	Learning Rate: 0.000433958
	LOSS [training: 0.24982813730084805 | validation: 0.1245009322595054]
	TIME [epoch: 8.18 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15315381259055116		[learning rate: 0.00043317]
		[batch 20/20] avg loss: 0.31668097986363974		[learning rate: 0.00043238]
	Learning Rate: 0.000432383
	LOSS [training: 0.23491739622709548 | validation: 0.25138300805906383]
	TIME [epoch: 8.19 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20900194158006208		[learning rate: 0.0004316]
		[batch 20/20] avg loss: 0.16051872005229703		[learning rate: 0.00043081]
	Learning Rate: 0.000430814
	LOSS [training: 0.18476033081617957 | validation: 0.09967539930096912]
	TIME [epoch: 8.21 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18623968330893473		[learning rate: 0.00043003]
		[batch 20/20] avg loss: 0.16873285342434932		[learning rate: 0.00042925]
	Learning Rate: 0.00042925
	LOSS [training: 0.17748626836664202 | validation: 0.12392705833729405]
	TIME [epoch: 8.19 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1771071252315227		[learning rate: 0.00042847]
		[batch 20/20] avg loss: 0.17163460115755497		[learning rate: 0.00042769]
	Learning Rate: 0.000427692
	LOSS [training: 0.17437086319453882 | validation: 0.12814859494443942]
	TIME [epoch: 8.19 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1747332383286663		[learning rate: 0.00042692]
		[batch 20/20] avg loss: 0.19413250536150484		[learning rate: 0.00042614]
	Learning Rate: 0.00042614
	LOSS [training: 0.1844328718450856 | validation: 0.12981952214946174]
	TIME [epoch: 8.19 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2725344139262968		[learning rate: 0.00042537]
		[batch 20/20] avg loss: 0.33313635214037185		[learning rate: 0.00042459]
	Learning Rate: 0.000424594
	LOSS [training: 0.3028353830333343 | validation: 0.13845906555261922]
	TIME [epoch: 8.21 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18735769413554418		[learning rate: 0.00042382]
		[batch 20/20] avg loss: 0.3176419341505273		[learning rate: 0.00042305]
	Learning Rate: 0.000423053
	LOSS [training: 0.25249981414303574 | validation: 0.18574837706275182]
	TIME [epoch: 8.19 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2005205486296072		[learning rate: 0.00042228]
		[batch 20/20] avg loss: 0.30612900743868143		[learning rate: 0.00042152]
	Learning Rate: 0.000421518
	LOSS [training: 0.2533247780341443 | validation: 0.9276473076874359]
	TIME [epoch: 8.18 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.691182526662367		[learning rate: 0.00042075]
		[batch 20/20] avg loss: 0.19049690079742418		[learning rate: 0.00041999]
	Learning Rate: 0.000419988
	LOSS [training: 0.4408397137298956 | validation: 0.12163056936985442]
	TIME [epoch: 8.19 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19158907317024718		[learning rate: 0.00041923]
		[batch 20/20] avg loss: 0.2281306245752074		[learning rate: 0.00041846]
	Learning Rate: 0.000418464
	LOSS [training: 0.20985984887272732 | validation: 0.15873673850962294]
	TIME [epoch: 8.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17897095173946714		[learning rate: 0.0004177]
		[batch 20/20] avg loss: 0.20564328633085718		[learning rate: 0.00041695]
	Learning Rate: 0.000416945
	LOSS [training: 0.19230711903516212 | validation: 0.13742440005623918]
	TIME [epoch: 8.19 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18970557044689582		[learning rate: 0.00041619]
		[batch 20/20] avg loss: 0.19458459085234198		[learning rate: 0.00041543]
	Learning Rate: 0.000415432
	LOSS [training: 0.1921450806496189 | validation: 0.16015213552626348]
	TIME [epoch: 8.19 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20204297462795034		[learning rate: 0.00041468]
		[batch 20/20] avg loss: 0.15333948225785962		[learning rate: 0.00041392]
	Learning Rate: 0.000413924
	LOSS [training: 0.177691228442905 | validation: 0.10253000250388021]
	TIME [epoch: 8.19 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19492205178577215		[learning rate: 0.00041317]
		[batch 20/20] avg loss: 0.1975347972790612		[learning rate: 0.00041242]
	Learning Rate: 0.000412422
	LOSS [training: 0.19622842453241668 | validation: 0.13441932708786214]
	TIME [epoch: 8.21 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19345656072149967		[learning rate: 0.00041167]
		[batch 20/20] avg loss: 0.17826598075142852		[learning rate: 0.00041093]
	Learning Rate: 0.000410926
	LOSS [training: 0.18586127073646405 | validation: 0.10324477265327392]
	TIME [epoch: 8.19 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1897846611567703		[learning rate: 0.00041018]
		[batch 20/20] avg loss: 1.3775173347956724		[learning rate: 0.00040943]
	Learning Rate: 0.000409434
	LOSS [training: 0.7836509979762214 | validation: 2.5521835431240096]
	TIME [epoch: 8.19 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3900788169553542		[learning rate: 0.00040869]
		[batch 20/20] avg loss: 0.20831618938569782		[learning rate: 0.00040795]
	Learning Rate: 0.000407948
	LOSS [training: 0.7991975031705258 | validation: 0.14697342768095514]
	TIME [epoch: 8.19 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17612341370750506		[learning rate: 0.00040721]
		[batch 20/20] avg loss: 0.1721086180825931		[learning rate: 0.00040647]
	Learning Rate: 0.000406468
	LOSS [training: 0.17411601589504902 | validation: 0.11437175067743434]
	TIME [epoch: 8.21 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14734122472038974		[learning rate: 0.00040573]
		[batch 20/20] avg loss: 0.20565453178350418		[learning rate: 0.00040499]
	Learning Rate: 0.000404993
	LOSS [training: 0.17649787825194696 | validation: 0.1149058326779559]
	TIME [epoch: 8.19 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24075862004137036		[learning rate: 0.00040426]
		[batch 20/20] avg loss: 0.18619823903817018		[learning rate: 0.00040352]
	Learning Rate: 0.000403523
	LOSS [training: 0.2134784295397702 | validation: 0.10981916988410302]
	TIME [epoch: 8.19 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1513212740606633		[learning rate: 0.00040279]
		[batch 20/20] avg loss: 0.1377226135579669		[learning rate: 0.00040206]
	Learning Rate: 0.000402059
	LOSS [training: 0.1445219438093151 | validation: 0.11028851471720982]
	TIME [epoch: 8.19 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24672839067516078		[learning rate: 0.00040133]
		[batch 20/20] avg loss: 0.12954089634149488		[learning rate: 0.0004006]
	Learning Rate: 0.0004006
	LOSS [training: 0.18813464350832781 | validation: 0.083157021336239]
	TIME [epoch: 8.21 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1293165979713288		[learning rate: 0.00039987]
		[batch 20/20] avg loss: 0.18605948143812226		[learning rate: 0.00039915]
	Learning Rate: 0.000399146
	LOSS [training: 0.1576880397047255 | validation: 0.1161049033647091]
	TIME [epoch: 8.19 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2948716307827417		[learning rate: 0.00039842]
		[batch 20/20] avg loss: 2.993211833201247		[learning rate: 0.0003977]
	Learning Rate: 0.000397697
	LOSS [training: 2.1440417319919947 | validation: 3.7583646492168885]
	TIME [epoch: 8.19 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.70570458827155		[learning rate: 0.00039698]
		[batch 20/20] avg loss: 0.6117163965891679		[learning rate: 0.00039625]
	Learning Rate: 0.000396254
	LOSS [training: 1.1587104924303588 | validation: 1.7343176010443355]
	TIME [epoch: 8.18 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6207063363908083		[learning rate: 0.00039553]
		[batch 20/20] avg loss: 2.4657148042483734		[learning rate: 0.00039482]
	Learning Rate: 0.000394816
	LOSS [training: 2.543210570319591 | validation: 3.2006985427981953]
	TIME [epoch: 8.21 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.997021151991907		[learning rate: 0.0003941]
		[batch 20/20] avg loss: 0.5241663829610584		[learning rate: 0.00039338]
	Learning Rate: 0.000393383
	LOSS [training: 0.7605937674764828 | validation: 0.5571400855686562]
	TIME [epoch: 8.19 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42458355019996785		[learning rate: 0.00039267]
		[batch 20/20] avg loss: 0.18474088733398958		[learning rate: 0.00039196]
	Learning Rate: 0.000391956
	LOSS [training: 0.30466221876697874 | validation: 0.13079263072049915]
	TIME [epoch: 8.19 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15865738026731627		[learning rate: 0.00039124]
		[batch 20/20] avg loss: 0.19424217194371998		[learning rate: 0.00039053]
	Learning Rate: 0.000390533
	LOSS [training: 0.1764497761055181 | validation: 0.25845879458586657]
	TIME [epoch: 8.18 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18643041711149103		[learning rate: 0.00038982]
		[batch 20/20] avg loss: 0.17476531070872073		[learning rate: 0.00038912]
	Learning Rate: 0.000389116
	LOSS [training: 0.18059786391010585 | validation: 0.10460017723123587]
	TIME [epoch: 8.21 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16652324744765187		[learning rate: 0.00038841]
		[batch 20/20] avg loss: 0.17033868096804602		[learning rate: 0.0003877]
	Learning Rate: 0.000387704
	LOSS [training: 0.16843096420784898 | validation: 0.12294900492329308]
	TIME [epoch: 8.19 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1421207711442258		[learning rate: 0.000387]
		[batch 20/20] avg loss: 0.16055613412051029		[learning rate: 0.0003863]
	Learning Rate: 0.000386297
	LOSS [training: 0.15133845263236806 | validation: 0.10391265678661829]
	TIME [epoch: 8.18 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1980744920765849		[learning rate: 0.0003856]
		[batch 20/20] avg loss: 0.16949508601158822		[learning rate: 0.00038489]
	Learning Rate: 0.000384895
	LOSS [training: 0.18378478904408657 | validation: 0.13918359898709973]
	TIME [epoch: 8.18 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1467837049797929		[learning rate: 0.0003842]
		[batch 20/20] avg loss: 1.257337243364578		[learning rate: 0.0003835]
	Learning Rate: 0.000383498
	LOSS [training: 0.7020604741721854 | validation: 2.061723631301428]
	TIME [epoch: 8.21 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1627636561715586		[learning rate: 0.0003828]
		[batch 20/20] avg loss: 1.6738003705567466		[learning rate: 0.00038211]
	Learning Rate: 0.000382106
	LOSS [training: 1.4182820133641527 | validation: 2.931243870457717]
	TIME [epoch: 8.19 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8243293866412031		[learning rate: 0.00038141]
		[batch 20/20] avg loss: 3.7423326506620476		[learning rate: 0.00038072]
	Learning Rate: 0.00038072
	LOSS [training: 2.7833310186516256 | validation: 5.086697526267332]
	TIME [epoch: 8.19 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2567815381920084		[learning rate: 0.00038003]
		[batch 20/20] avg loss: 0.711078257983047		[learning rate: 0.00037934]
	Learning Rate: 0.000379338
	LOSS [training: 1.9839298980875282 | validation: 0.24477622886425549]
	TIME [epoch: 8.18 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29939300052000023		[learning rate: 0.00037865]
		[batch 20/20] avg loss: 0.16546191030166518		[learning rate: 0.00037796]
	Learning Rate: 0.000377961
	LOSS [training: 0.2324274554108327 | validation: 0.14803484064792263]
	TIME [epoch: 8.2 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22454149955855693		[learning rate: 0.00037727]
		[batch 20/20] avg loss: 0.25499712115942036		[learning rate: 0.00037659]
	Learning Rate: 0.00037659
	LOSS [training: 0.23976931035898869 | validation: 0.2817393967937928]
	TIME [epoch: 8.19 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1786677040057972		[learning rate: 0.00037591]
		[batch 20/20] avg loss: 0.14283931074890627		[learning rate: 0.00037522]
	Learning Rate: 0.000375223
	LOSS [training: 0.16075350737735172 | validation: 0.11805169615019503]
	TIME [epoch: 8.19 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13294527656782912		[learning rate: 0.00037454]
		[batch 20/20] avg loss: 0.23032034220530728		[learning rate: 0.00037386]
	Learning Rate: 0.000373861
	LOSS [training: 0.18163280938656817 | validation: 1.3639462937590616]
	TIME [epoch: 8.18 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2298883179095563		[learning rate: 0.00037318]
		[batch 20/20] avg loss: 2.4423347433349942		[learning rate: 0.0003725]
	Learning Rate: 0.000372505
	LOSS [training: 2.8361115306222753 | validation: 1.8717803493480316]
	TIME [epoch: 8.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6175535528602911		[learning rate: 0.00037183]
		[batch 20/20] avg loss: 0.906684353342537		[learning rate: 0.00037115]
	Learning Rate: 0.000371153
	LOSS [training: 1.2621189531014139 | validation: 1.9262447393524302]
	TIME [epoch: 8.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0884865211156531		[learning rate: 0.00037048]
		[batch 20/20] avg loss: 0.4423899574817785		[learning rate: 0.00036981]
	Learning Rate: 0.000369806
	LOSS [training: 0.7654382392987159 | validation: 0.6173986621224611]
	TIME [epoch: 8.19 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37367883267999497		[learning rate: 0.00036913]
		[batch 20/20] avg loss: 0.6269647452139411		[learning rate: 0.00036846]
	Learning Rate: 0.000368464
	LOSS [training: 0.500321788946968 | validation: 2.7775932702880097]
	TIME [epoch: 8.19 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8898348785918213		[learning rate: 0.00036779]
		[batch 20/20] avg loss: 0.23007674257744268		[learning rate: 0.00036713]
	Learning Rate: 0.000367127
	LOSS [training: 0.559955810584632 | validation: 0.26385746865759757]
	TIME [epoch: 8.19 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20914784813265547		[learning rate: 0.00036646]
		[batch 20/20] avg loss: 0.17668609510573557		[learning rate: 0.00036579]
	Learning Rate: 0.000365794
	LOSS [training: 0.19291697161919552 | validation: 0.11244864754237202]
	TIME [epoch: 8.21 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42414614756051733		[learning rate: 0.00036513]
		[batch 20/20] avg loss: 0.5078014396892885		[learning rate: 0.00036447]
	Learning Rate: 0.000364467
	LOSS [training: 0.46597379362490293 | validation: 0.1679675135345718]
	TIME [epoch: 8.18 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19768955012132203		[learning rate: 0.0003638]
		[batch 20/20] avg loss: 0.21062616287800268		[learning rate: 0.00036314]
	Learning Rate: 0.000363144
	LOSS [training: 0.2041578564996623 | validation: 0.13136279236623877]
	TIME [epoch: 8.19 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8667869522535139		[learning rate: 0.00036248]
		[batch 20/20] avg loss: 0.23371637010610108		[learning rate: 0.00036183]
	Learning Rate: 0.000361826
	LOSS [training: 0.5502516611798074 | validation: 0.12267051426711659]
	TIME [epoch: 8.19 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1566244499300168		[learning rate: 0.00036117]
		[batch 20/20] avg loss: 0.8352902452603409		[learning rate: 0.00036051]
	Learning Rate: 0.000360513
	LOSS [training: 0.4959573475951788 | validation: 0.45326341451448665]
	TIME [epoch: 8.21 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21942520161614207		[learning rate: 0.00035986]
		[batch 20/20] avg loss: 0.16325171064967833		[learning rate: 0.0003592]
	Learning Rate: 0.000359205
	LOSS [training: 0.1913384561329102 | validation: 0.12274768483978621]
	TIME [epoch: 8.18 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19869178505078292		[learning rate: 0.00035855]
		[batch 20/20] avg loss: 0.16024054345161548		[learning rate: 0.0003579]
	Learning Rate: 0.000357901
	LOSS [training: 0.17946616425119927 | validation: 0.18648632778825336]
	TIME [epoch: 8.18 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19772826924443498		[learning rate: 0.00035725]
		[batch 20/20] avg loss: 0.18324835001354065		[learning rate: 0.0003566]
	Learning Rate: 0.000356602
	LOSS [training: 0.19048830962898783 | validation: 0.1621703454219574]
	TIME [epoch: 8.19 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1672277150879972		[learning rate: 0.00035595]
		[batch 20/20] avg loss: 0.19424552694224367		[learning rate: 0.00035531]
	Learning Rate: 0.000355308
	LOSS [training: 0.18073662101512042 | validation: 0.12232145929347422]
	TIME [epoch: 8.21 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20596679608711055		[learning rate: 0.00035466]
		[batch 20/20] avg loss: 0.1918771443612105		[learning rate: 0.00035402]
	Learning Rate: 0.000354019
	LOSS [training: 0.1989219702241605 | validation: 0.14537171448097105]
	TIME [epoch: 8.19 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15588541786249227		[learning rate: 0.00035338]
		[batch 20/20] avg loss: 0.18547890034102296		[learning rate: 0.00035273]
	Learning Rate: 0.000352734
	LOSS [training: 0.1706821591017576 | validation: 0.12067672239765155]
	TIME [epoch: 8.18 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17312745644645153		[learning rate: 0.00035209]
		[batch 20/20] avg loss: 0.178035464940061		[learning rate: 0.00035145]
	Learning Rate: 0.000351454
	LOSS [training: 0.17558146069325628 | validation: 0.12201049743587347]
	TIME [epoch: 8.19 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16037066068532807		[learning rate: 0.00035082]
		[batch 20/20] avg loss: 0.18262642168450754		[learning rate: 0.00035018]
	Learning Rate: 0.000350179
	LOSS [training: 0.17149854118491778 | validation: 0.1315052067815185]
	TIME [epoch: 8.21 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18722732643212664		[learning rate: 0.00034954]
		[batch 20/20] avg loss: 0.1533040779319688		[learning rate: 0.00034891]
	Learning Rate: 0.000348908
	LOSS [training: 0.1702657021820477 | validation: 0.14011879432568103]
	TIME [epoch: 8.19 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18138528061475034		[learning rate: 0.00034827]
		[batch 20/20] avg loss: 0.1626515558116494		[learning rate: 0.00034764]
	Learning Rate: 0.000347641
	LOSS [training: 0.17201841821319985 | validation: 0.11551787924187298]
	TIME [epoch: 8.18 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14358775136968502		[learning rate: 0.00034701]
		[batch 20/20] avg loss: 0.15929737389296078		[learning rate: 0.00034638]
	Learning Rate: 0.00034638
	LOSS [training: 0.1514425626313229 | validation: 0.11994290201496216]
	TIME [epoch: 8.19 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19996089004019318		[learning rate: 0.00034575]
		[batch 20/20] avg loss: 0.1761801767432408		[learning rate: 0.00034512]
	Learning Rate: 0.000345123
	LOSS [training: 0.188070533391717 | validation: 0.20119046272028412]
	TIME [epoch: 8.21 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19864248609802904		[learning rate: 0.0003445]
		[batch 20/20] avg loss: 0.17966954765104967		[learning rate: 0.00034387]
	Learning Rate: 0.00034387
	LOSS [training: 0.18915601687453937 | validation: 0.16141969750062063]
	TIME [epoch: 8.19 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21690952749023867		[learning rate: 0.00034325]
		[batch 20/20] avg loss: 0.20033173590975864		[learning rate: 0.00034262]
	Learning Rate: 0.000342622
	LOSS [training: 0.20862063169999864 | validation: 0.17455292159063623]
	TIME [epoch: 8.18 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1966230022352534		[learning rate: 0.000342]
		[batch 20/20] avg loss: 0.170573877293454		[learning rate: 0.00034138]
	Learning Rate: 0.000341379
	LOSS [training: 0.1835984397643537 | validation: 0.12308940706237036]
	TIME [epoch: 8.19 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18589734081787693		[learning rate: 0.00034076]
		[batch 20/20] avg loss: 0.15559093005596938		[learning rate: 0.00034014]
	Learning Rate: 0.00034014
	LOSS [training: 0.1707441354369231 | validation: 0.12484726299949853]
	TIME [epoch: 8.21 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14773640900179		[learning rate: 0.00033952]
		[batch 20/20] avg loss: 0.18929257774129737		[learning rate: 0.00033891]
	Learning Rate: 0.000338906
	LOSS [training: 0.16851449337154367 | validation: 0.14002551676966596]
	TIME [epoch: 8.19 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18469867090538652		[learning rate: 0.00033829]
		[batch 20/20] avg loss: 0.17446771570009417		[learning rate: 0.00033768]
	Learning Rate: 0.000337676
	LOSS [training: 0.17958319330274033 | validation: 0.12886316321609687]
	TIME [epoch: 8.19 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1656597247211127		[learning rate: 0.00033706]
		[batch 20/20] avg loss: 0.17393365842642325		[learning rate: 0.00033645]
	Learning Rate: 0.00033645
	LOSS [training: 0.16979669157376798 | validation: 0.1561735974712506]
	TIME [epoch: 8.19 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18375528795618704		[learning rate: 0.00033584]
		[batch 20/20] avg loss: 0.18104014564267784		[learning rate: 0.00033523]
	Learning Rate: 0.000335229
	LOSS [training: 0.18239771679943245 | validation: 0.1243406511617373]
	TIME [epoch: 8.21 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18453287097339718		[learning rate: 0.00033462]
		[batch 20/20] avg loss: 0.22099846313876786		[learning rate: 0.00033401]
	Learning Rate: 0.000334013
	LOSS [training: 0.20276566705608254 | validation: 0.1483924732291674]
	TIME [epoch: 8.19 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19133050099398904		[learning rate: 0.00033341]
		[batch 20/20] avg loss: 0.16015803983416416		[learning rate: 0.0003328]
	Learning Rate: 0.000332801
	LOSS [training: 0.1757442704140766 | validation: 0.1694576877222506]
	TIME [epoch: 8.19 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1933495393344135		[learning rate: 0.0003322]
		[batch 20/20] avg loss: 0.19715516733869948		[learning rate: 0.00033159]
	Learning Rate: 0.000331593
	LOSS [training: 0.19525235333655652 | validation: 0.11868713339230688]
	TIME [epoch: 8.18 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16939660308000665		[learning rate: 0.00033099]
		[batch 20/20] avg loss: 0.16526385432181778		[learning rate: 0.00033039]
	Learning Rate: 0.00033039
	LOSS [training: 0.1673302287009122 | validation: 0.1194270782331375]
	TIME [epoch: 8.21 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16972579640681118		[learning rate: 0.00032979]
		[batch 20/20] avg loss: 0.1740993195304797		[learning rate: 0.00032919]
	Learning Rate: 0.000329191
	LOSS [training: 0.1719125579686454 | validation: 0.14359753671857345]
	TIME [epoch: 8.19 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1936264778573389		[learning rate: 0.00032859]
		[batch 20/20] avg loss: 0.18647189856851049		[learning rate: 0.000328]
	Learning Rate: 0.000327996
	LOSS [training: 0.19004918821292469 | validation: 0.13418736548746252]
	TIME [epoch: 8.18 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19103429672075348		[learning rate: 0.0003274]
		[batch 20/20] avg loss: 0.17905543818566783		[learning rate: 0.00032681]
	Learning Rate: 0.000326806
	LOSS [training: 0.18504486745321067 | validation: 0.19893335015354588]
	TIME [epoch: 8.19 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18193636143744887		[learning rate: 0.00032621]
		[batch 20/20] avg loss: 0.18992212043357024		[learning rate: 0.00032562]
	Learning Rate: 0.00032562
	LOSS [training: 0.18592924093550953 | validation: 0.1218913771504736]
	TIME [epoch: 8.21 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15102484632117136		[learning rate: 0.00032503]
		[batch 20/20] avg loss: 0.1698824267774391		[learning rate: 0.00032444]
	Learning Rate: 0.000324438
	LOSS [training: 0.1604536365493052 | validation: 0.10899090005437444]
	TIME [epoch: 8.19 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15730443296435612		[learning rate: 0.00032385]
		[batch 20/20] avg loss: 0.1614709934616076		[learning rate: 0.00032326]
	Learning Rate: 0.00032326
	LOSS [training: 0.15938771321298187 | validation: 0.11630842064743699]
	TIME [epoch: 8.18 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1521187540036168		[learning rate: 0.00032267]
		[batch 20/20] avg loss: 0.16029944876115512		[learning rate: 0.00032209]
	Learning Rate: 0.000322087
	LOSS [training: 0.15620910138238595 | validation: 0.14046643311307835]
	TIME [epoch: 8.18 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16313477270537322		[learning rate: 0.0003215]
		[batch 20/20] avg loss: 0.171658130040601		[learning rate: 0.00032092]
	Learning Rate: 0.000320918
	LOSS [training: 0.16739645137298712 | validation: 0.1176307144418735]
	TIME [epoch: 8.2 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18921739461981538		[learning rate: 0.00032034]
		[batch 20/20] avg loss: 0.17532994649417627		[learning rate: 0.00031975]
	Learning Rate: 0.000319754
	LOSS [training: 0.18227367055699586 | validation: 0.1490011073169291]
	TIME [epoch: 8.19 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19391206180697126		[learning rate: 0.00031917]
		[batch 20/20] avg loss: 0.16205252158923544		[learning rate: 0.00031859]
	Learning Rate: 0.000318593
	LOSS [training: 0.17798229169810337 | validation: 0.1265624337838278]
	TIME [epoch: 8.19 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1767946902495075		[learning rate: 0.00031801]
		[batch 20/20] avg loss: 0.1807696954958567		[learning rate: 0.00031744]
	Learning Rate: 0.000317437
	LOSS [training: 0.1787821928726821 | validation: 0.14156078844411418]
	TIME [epoch: 8.18 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1727511646668084		[learning rate: 0.00031686]
		[batch 20/20] avg loss: 0.1748464096696876		[learning rate: 0.00031629]
	Learning Rate: 0.000316285
	LOSS [training: 0.17379878716824798 | validation: 0.1074797370243421]
	TIME [epoch: 8.21 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16135296976797578		[learning rate: 0.00031571]
		[batch 20/20] avg loss: 0.18546276654582836		[learning rate: 0.00031514]
	Learning Rate: 0.000315137
	LOSS [training: 0.17340786815690207 | validation: 0.10448411609968718]
	TIME [epoch: 8.19 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16245954310451524		[learning rate: 0.00031457]
		[batch 20/20] avg loss: 0.15566047622933504		[learning rate: 0.00031399]
	Learning Rate: 0.000313994
	LOSS [training: 0.15906000966692516 | validation: 0.1553974560585044]
	TIME [epoch: 8.19 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.170902633868311		[learning rate: 0.00031342]
		[batch 20/20] avg loss: 0.21740000188578526		[learning rate: 0.00031285]
	Learning Rate: 0.000312854
	LOSS [training: 0.19415131787704815 | validation: 0.1618691996631976]
	TIME [epoch: 8.18 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17630958755601783		[learning rate: 0.00031229]
		[batch 20/20] avg loss: 0.1698315127439442		[learning rate: 0.00031172]
	Learning Rate: 0.000311719
	LOSS [training: 0.17307055014998102 | validation: 0.1365195375991985]
	TIME [epoch: 8.21 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1732985185869215		[learning rate: 0.00031115]
		[batch 20/20] avg loss: 0.20866593638985767		[learning rate: 0.00031059]
	Learning Rate: 0.000310588
	LOSS [training: 0.19098222748838958 | validation: 0.13348736013966567]
	TIME [epoch: 8.19 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16920546918583593		[learning rate: 0.00031002]
		[batch 20/20] avg loss: 0.15923521885844202		[learning rate: 0.00030946]
	Learning Rate: 0.000309461
	LOSS [training: 0.164220344022139 | validation: 0.13796219322513803]
	TIME [epoch: 8.19 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16355355958216095		[learning rate: 0.0003089]
		[batch 20/20] avg loss: 0.1632159386341332		[learning rate: 0.00030834]
	Learning Rate: 0.000308338
	LOSS [training: 0.16338474910814707 | validation: 0.11766902373767082]
	TIME [epoch: 8.18 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14894942859075677		[learning rate: 0.00030778]
		[batch 20/20] avg loss: 0.1668553749029382		[learning rate: 0.00030722]
	Learning Rate: 0.000307219
	LOSS [training: 0.1579024017468475 | validation: 0.12495559276296256]
	TIME [epoch: 8.21 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18285333026544176		[learning rate: 0.00030666]
		[batch 20/20] avg loss: 0.17879580584655688		[learning rate: 0.0003061]
	Learning Rate: 0.000306104
	LOSS [training: 0.18082456805599934 | validation: 0.13392025010288006]
	TIME [epoch: 8.19 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1865935562244083		[learning rate: 0.00030555]
		[batch 20/20] avg loss: 0.20606868939604453		[learning rate: 0.00030499]
	Learning Rate: 0.000304993
	LOSS [training: 0.19633112281022644 | validation: 0.15188958135317554]
	TIME [epoch: 8.19 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18514403115592737		[learning rate: 0.00030444]
		[batch 20/20] avg loss: 0.18098978371722485		[learning rate: 0.00030389]
	Learning Rate: 0.000303886
	LOSS [training: 0.1830669074365761 | validation: 0.1359174177085619]
	TIME [epoch: 8.19 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1833315048096304		[learning rate: 0.00030333]
		[batch 20/20] avg loss: 0.17035260917448652		[learning rate: 0.00030278]
	Learning Rate: 0.000302783
	LOSS [training: 0.17684205699205846 | validation: 0.13887338983530614]
	TIME [epoch: 8.22 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15240418078026036		[learning rate: 0.00030223]
		[batch 20/20] avg loss: 0.18896727510270894		[learning rate: 0.00030168]
	Learning Rate: 0.000301684
	LOSS [training: 0.17068572794148465 | validation: 0.1354511152788754]
	TIME [epoch: 8.19 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15944022082892728		[learning rate: 0.00030114]
		[batch 20/20] avg loss: 0.17507050427543785		[learning rate: 0.00030059]
	Learning Rate: 0.000300589
	LOSS [training: 0.16725536255218257 | validation: 0.10711805641176195]
	TIME [epoch: 8.19 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14964790900006697		[learning rate: 0.00030004]
		[batch 20/20] avg loss: 0.1425660658604697		[learning rate: 0.0002995]
	Learning Rate: 0.000299499
	LOSS [training: 0.14610698743026831 | validation: 0.09773172396046381]
	TIME [epoch: 8.19 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14964996007359188		[learning rate: 0.00029895]
		[batch 20/20] avg loss: 0.12549291119336314		[learning rate: 0.00029841]
	Learning Rate: 0.000298412
	LOSS [training: 0.13757143563347754 | validation: 0.08924505302250138]
	TIME [epoch: 8.2 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1532015036390828		[learning rate: 0.00029787]
		[batch 20/20] avg loss: 0.1469184390687783		[learning rate: 0.00029733]
	Learning Rate: 0.000297329
	LOSS [training: 0.15005997135393057 | validation: 0.08365120447118315]
	TIME [epoch: 8.19 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.131469011499649		[learning rate: 0.00029679]
		[batch 20/20] avg loss: 0.1409594473617834		[learning rate: 0.00029625]
	Learning Rate: 0.00029625
	LOSS [training: 0.1362142294307162 | validation: 0.09191411798415874]
	TIME [epoch: 8.18 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12357238261125833		[learning rate: 0.00029571]
		[batch 20/20] avg loss: 0.13503633097769524		[learning rate: 0.00029517]
	Learning Rate: 0.000295175
	LOSS [training: 0.12930435679447677 | validation: 0.10229308772774767]
	TIME [epoch: 8.19 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13484498366399733		[learning rate: 0.00029464]
		[batch 20/20] avg loss: 0.1548850588238627		[learning rate: 0.0002941]
	Learning Rate: 0.000294103
	LOSS [training: 0.14486502124393005 | validation: 0.1023001221428947]
	TIME [epoch: 8.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21080996922996734		[learning rate: 0.00029357]
		[batch 20/20] avg loss: 0.3041120321886211		[learning rate: 0.00029304]
	Learning Rate: 0.000293036
	LOSS [training: 0.25746100070929423 | validation: 0.08844789460666416]
	TIME [epoch: 8.2 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13576344725546446		[learning rate: 0.0002925]
		[batch 20/20] avg loss: 0.09850221253303539		[learning rate: 0.00029197]
	Learning Rate: 0.000291973
	LOSS [training: 0.11713282989424989 | validation: 0.09412775521079189]
	TIME [epoch: 8.18 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12320328647021647		[learning rate: 0.00029144]
		[batch 20/20] avg loss: 0.1205509468071686		[learning rate: 0.00029091]
	Learning Rate: 0.000290913
	LOSS [training: 0.12187711663869256 | validation: 0.0727921389788037]
	TIME [epoch: 8.19 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11712194847661508		[learning rate: 0.00029038]
		[batch 20/20] avg loss: 0.155872590111709		[learning rate: 0.00028986]
	Learning Rate: 0.000289857
	LOSS [training: 0.136497269294162 | validation: 0.3088517651668589]
	TIME [epoch: 8.21 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4204832908195524		[learning rate: 0.00028933]
		[batch 20/20] avg loss: 0.7206291839002759		[learning rate: 0.00028881]
	Learning Rate: 0.000288805
	LOSS [training: 0.5705562373599141 | validation: 0.7785946295911274]
	TIME [epoch: 8.19 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35365278622493923		[learning rate: 0.00028828]
		[batch 20/20] avg loss: 1.111108162538899		[learning rate: 0.00028776]
	Learning Rate: 0.000287757
	LOSS [training: 0.7323804743819193 | validation: 2.008343010914187]
	TIME [epoch: 8.19 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2920980786087413		[learning rate: 0.00028723]
		[batch 20/20] avg loss: 0.2953460596102386		[learning rate: 0.00028671]
	Learning Rate: 0.000286713
	LOSS [training: 0.7937220691094898 | validation: 0.47891436617379946]
	TIME [epoch: 8.2 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9161949309799757		[learning rate: 0.00028619]
		[batch 20/20] avg loss: 1.0816482462121282		[learning rate: 0.00028567]
	Learning Rate: 0.000285672
	LOSS [training: 0.9989215885960518 | validation: 1.7131271309263496]
	TIME [epoch: 8.21 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0349876361765649		[learning rate: 0.00028515]
		[batch 20/20] avg loss: 1.5660683601651297		[learning rate: 0.00028464]
	Learning Rate: 0.000284636
	LOSS [training: 1.3005279981708473 | validation: 1.9917945619051471]
	TIME [epoch: 8.2 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.252852847794293		[learning rate: 0.00028412]
		[batch 20/20] avg loss: 1.4324700653795825		[learning rate: 0.0002836]
	Learning Rate: 0.000283603
	LOSS [training: 1.342661456586938 | validation: 2.2236268976076654]
	TIME [epoch: 8.18 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0797896468376917		[learning rate: 0.00028309]
		[batch 20/20] avg loss: 3.063832688155938		[learning rate: 0.00028257]
	Learning Rate: 0.000282574
	LOSS [training: 2.571811167496815 | validation: 3.709173208443643]
	TIME [epoch: 8.19 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.239715862891531		[learning rate: 0.00028206]
		[batch 20/20] avg loss: 3.539452593140626		[learning rate: 0.00028155]
	Learning Rate: 0.000281548
	LOSS [training: 3.8895842280160777 | validation: 3.9430638414489496]
	TIME [epoch: 8.21 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.936558408719219		[learning rate: 0.00028104]
		[batch 20/20] avg loss: 2.978651560325008		[learning rate: 0.00028053]
	Learning Rate: 0.000280526
	LOSS [training: 2.957604984522113 | validation: 4.264280492709875]
	TIME [epoch: 8.21 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8531411184224345		[learning rate: 0.00028002]
		[batch 20/20] avg loss: 3.236166651308062		[learning rate: 0.00027951]
	Learning Rate: 0.000279508
	LOSS [training: 3.044653884865249 | validation: 4.912547529307927]
	TIME [epoch: 8.18 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.251575220376148		[learning rate: 0.000279]
		[batch 20/20] avg loss: 3.798532104402392		[learning rate: 0.00027849]
	Learning Rate: 0.000278494
	LOSS [training: 4.02505366238927 | validation: 4.86534117597326]
	TIME [epoch: 8.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.223936559091078		[learning rate: 0.00027799]
		[batch 20/20] avg loss: 3.9529552794635885		[learning rate: 0.00027748]
	Learning Rate: 0.000277483
	LOSS [training: 4.0884459192773335 | validation: 4.7039073670467575]
	TIME [epoch: 8.21 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8278312034287496		[learning rate: 0.00027698]
		[batch 20/20] avg loss: 3.889033013910621		[learning rate: 0.00027648]
	Learning Rate: 0.000276476
	LOSS [training: 3.8584321086696853 | validation: 4.67719432308326]
	TIME [epoch: 8.19 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.0768651202254675		[learning rate: 0.00027597]
		[batch 20/20] avg loss: 3.6590989794924256		[learning rate: 0.00027547]
	Learning Rate: 0.000275473
	LOSS [training: 3.8679820498589463 | validation: 4.21027545097456]
	TIME [epoch: 8.18 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.164140488390962		[learning rate: 0.00027497]
		[batch 20/20] avg loss: 2.3924569781401788		[learning rate: 0.00027447]
	Learning Rate: 0.000274473
	LOSS [training: 2.7782987332655695 | validation: 2.794737421857731]
	TIME [epoch: 8.18 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2200450686903674		[learning rate: 0.00027397]
		[batch 20/20] avg loss: 2.4937222313707585		[learning rate: 0.00027348]
	Learning Rate: 0.000273477
	LOSS [training: 2.3568836500305634 | validation: 3.3457355152831205]
	TIME [epoch: 8.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7667385731085559		[learning rate: 0.00027298]
		[batch 20/20] avg loss: 0.8010387990073704		[learning rate: 0.00027248]
	Learning Rate: 0.000272485
	LOSS [training: 1.2838886860579632 | validation: 1.2961860694547882]
	TIME [epoch: 8.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0467902287929503		[learning rate: 0.00027199]
		[batch 20/20] avg loss: 0.33220336536312367		[learning rate: 0.0002715]
	Learning Rate: 0.000271496
	LOSS [training: 0.6894967970780368 | validation: 0.225745766725743]
	TIME [epoch: 8.19 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18696206921506708		[learning rate: 0.000271]
		[batch 20/20] avg loss: 0.29963360985053933		[learning rate: 0.00027051]
	Learning Rate: 0.000270511
	LOSS [training: 0.2432978395328032 | validation: 0.2810564940342236]
	TIME [epoch: 8.19 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40087784826221773		[learning rate: 0.00027002]
		[batch 20/20] avg loss: 0.5474970663246774		[learning rate: 0.00026953]
	Learning Rate: 0.000269529
	LOSS [training: 0.4741874572934476 | validation: 0.37505476106811897]
	TIME [epoch: 8.19 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23471339517268022		[learning rate: 0.00026904]
		[batch 20/20] avg loss: 0.19484115898021426		[learning rate: 0.00026855]
	Learning Rate: 0.000268551
	LOSS [training: 0.21477727707644725 | validation: 0.15956106053064967]
	TIME [epoch: 8.21 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15442802281424112		[learning rate: 0.00026806]
		[batch 20/20] avg loss: 0.14882593135902794		[learning rate: 0.00026758]
	Learning Rate: 0.000267576
	LOSS [training: 0.1516269770866345 | validation: 0.08564812156746686]
	TIME [epoch: 8.19 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12396000006202859		[learning rate: 0.00026709]
		[batch 20/20] avg loss: 0.14458751661705468		[learning rate: 0.00026661]
	Learning Rate: 0.000266605
	LOSS [training: 0.13427375833954164 | validation: 0.1952588305074346]
	TIME [epoch: 8.18 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19689694667162927		[learning rate: 0.00026612]
		[batch 20/20] avg loss: 0.17033046734370488		[learning rate: 0.00026564]
	Learning Rate: 0.000265638
	LOSS [training: 0.1836137070076671 | validation: 0.14233349986502422]
	TIME [epoch: 8.19 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2185632225002266		[learning rate: 0.00026516]
		[batch 20/20] avg loss: 0.5214822345005347		[learning rate: 0.00026467]
	Learning Rate: 0.000264674
	LOSS [training: 0.37002272850038065 | validation: 0.45536143577562366]
	TIME [epoch: 8.21 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2089614314948883		[learning rate: 0.00026419]
		[batch 20/20] avg loss: 0.2392602258185597		[learning rate: 0.00026371]
	Learning Rate: 0.000263713
	LOSS [training: 0.22411082865672402 | validation: 0.13826214429054895]
	TIME [epoch: 8.19 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14517783181584637		[learning rate: 0.00026323]
		[batch 20/20] avg loss: 0.16994510070727994		[learning rate: 0.00026276]
	Learning Rate: 0.000262756
	LOSS [training: 0.1575614662615632 | validation: 0.17136380273909785]
	TIME [epoch: 8.18 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1359738147881969		[learning rate: 0.00026228]
		[batch 20/20] avg loss: 0.21903095928529207		[learning rate: 0.0002618]
	Learning Rate: 0.000261802
	LOSS [training: 0.17750238703674448 | validation: 0.6061990912042106]
	TIME [epoch: 8.19 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46222229977248147		[learning rate: 0.00026133]
		[batch 20/20] avg loss: 0.7043553957422406		[learning rate: 0.00026085]
	Learning Rate: 0.000260852
	LOSS [training: 0.583288847757361 | validation: 0.6733450929411754]
	TIME [epoch: 8.2 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8068177968009611		[learning rate: 0.00026038]
		[batch 20/20] avg loss: 1.6434877116296516		[learning rate: 0.00025991]
	Learning Rate: 0.000259906
	LOSS [training: 1.2251527542153062 | validation: 3.3944740623776024]
	TIME [epoch: 8.19 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8668133505043145		[learning rate: 0.00025943]
		[batch 20/20] avg loss: 1.5909269796079295		[learning rate: 0.00025896]
	Learning Rate: 0.000258962
	LOSS [training: 1.7288701650561218 | validation: 2.420231767173962]
	TIME [epoch: 8.19 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4232428826847914		[learning rate: 0.00025849]
		[batch 20/20] avg loss: 1.5440597086006533		[learning rate: 0.00025802]
	Learning Rate: 0.000258023
	LOSS [training: 1.4836512956427224 | validation: 2.1549943637560456]
	TIME [epoch: 8.18 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4338313245057024		[learning rate: 0.00025755]
		[batch 20/20] avg loss: 2.679572356263652		[learning rate: 0.00025709]
	Learning Rate: 0.000257086
	LOSS [training: 3.0567018403846777 | validation: 2.8163987984075254]
	TIME [epoch: 8.21 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7969906094628052		[learning rate: 0.00025662]
		[batch 20/20] avg loss: 0.47556823188452474		[learning rate: 0.00025615]
	Learning Rate: 0.000256153
	LOSS [training: 1.1362794206736648 | validation: 0.19931117698819018]
	TIME [epoch: 8.18 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8697865004963485		[learning rate: 0.00025569]
		[batch 20/20] avg loss: 2.498360681091323		[learning rate: 0.00025522]
	Learning Rate: 0.000255224
	LOSS [training: 1.684073590793836 | validation: 3.1672002015249916]
	TIME [epoch: 8.19 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.701395200171041		[learning rate: 0.00025476]
		[batch 20/20] avg loss: 3.9420234402595065		[learning rate: 0.0002543]
	Learning Rate: 0.000254298
	LOSS [training: 3.821709320215274 | validation: 4.705243615979223]
	TIME [epoch: 8.19 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.639135529916176		[learning rate: 0.00025384]
		[batch 20/20] avg loss: 3.244427025347602		[learning rate: 0.00025337]
	Learning Rate: 0.000253375
	LOSS [training: 3.4417812776318883 | validation: 3.9340397398962184]
	TIME [epoch: 8.21 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.079276034654073		[learning rate: 0.00025291]
		[batch 20/20] avg loss: 3.122989298634818		[learning rate: 0.00025246]
	Learning Rate: 0.000252455
	LOSS [training: 3.1011326666444456 | validation: 2.8281050407903727]
	TIME [epoch: 8.19 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4364993828215775		[learning rate: 0.000252]
		[batch 20/20] avg loss: 1.3113983592231995		[learning rate: 0.00025154]
	Learning Rate: 0.000251539
	LOSS [training: 1.8739488710223884 | validation: 0.9206979730455326]
	TIME [epoch: 8.19 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3591060633664277		[learning rate: 0.00025108]
		[batch 20/20] avg loss: 0.20867220111671614		[learning rate: 0.00025063]
	Learning Rate: 0.000250626
	LOSS [training: 0.2838891322415718 | validation: 0.16224032741424757]
	TIME [epoch: 8.19 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15211813753668713		[learning rate: 0.00025017]
		[batch 20/20] avg loss: 0.15057165189879998		[learning rate: 0.00024972]
	Learning Rate: 0.000249717
	LOSS [training: 0.15134489471774354 | validation: 0.11667032875439586]
	TIME [epoch: 8.21 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15075029390304684		[learning rate: 0.00024926]
		[batch 20/20] avg loss: 0.13208315923629224		[learning rate: 0.00024881]
	Learning Rate: 0.00024881
	LOSS [training: 0.14141672656966953 | validation: 0.11252020740430886]
	TIME [epoch: 8.19 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12302007576456546		[learning rate: 0.00024836]
		[batch 20/20] avg loss: 0.16183100827317776		[learning rate: 0.00024791]
	Learning Rate: 0.000247907
	LOSS [training: 0.1424255420188716 | validation: 0.12104395266368326]
	TIME [epoch: 8.19 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20739921446254267		[learning rate: 0.00024746]
		[batch 20/20] avg loss: 0.23458358156973946		[learning rate: 0.00024701]
	Learning Rate: 0.000247008
	LOSS [training: 0.22099139801614104 | validation: 0.11903581668522609]
	TIME [epoch: 8.19 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15302296688071343		[learning rate: 0.00024656]
		[batch 20/20] avg loss: 0.14135499042318111		[learning rate: 0.00024611]
	Learning Rate: 0.000246111
	LOSS [training: 0.14718897865194727 | validation: 0.21482750780057583]
	TIME [epoch: 8.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14669872818147783		[learning rate: 0.00024566]
		[batch 20/20] avg loss: 0.12166310558996449		[learning rate: 0.00024522]
	Learning Rate: 0.000245218
	LOSS [training: 0.13418091688572115 | validation: 0.16367951431008387]
	TIME [epoch: 8.19 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13610254948493383		[learning rate: 0.00024477]
		[batch 20/20] avg loss: 0.15446179805640503		[learning rate: 0.00024433]
	Learning Rate: 0.000244328
	LOSS [training: 0.14528217377066943 | validation: 0.09139403610674775]
	TIME [epoch: 8.19 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1266799436955598		[learning rate: 0.00024388]
		[batch 20/20] avg loss: 0.11986909739655463		[learning rate: 0.00024344]
	Learning Rate: 0.000243442
	LOSS [training: 0.12327452054605723 | validation: 0.08055263801870952]
	TIME [epoch: 8.19 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1285085460200542		[learning rate: 0.000243]
		[batch 20/20] avg loss: 0.12093042359608741		[learning rate: 0.00024256]
	Learning Rate: 0.000242558
	LOSS [training: 0.12471948480807082 | validation: 0.1041018050976822]
	TIME [epoch: 8.21 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13144094771640843		[learning rate: 0.00024212]
		[batch 20/20] avg loss: 0.14588192392485616		[learning rate: 0.00024168]
	Learning Rate: 0.000241678
	LOSS [training: 0.1386614358206323 | validation: 0.11205363255157048]
	TIME [epoch: 8.19 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13454152381766943		[learning rate: 0.00024124]
		[batch 20/20] avg loss: 0.14694806946079053		[learning rate: 0.0002408]
	Learning Rate: 0.000240801
	LOSS [training: 0.14074479663923 | validation: 0.0774789130485891]
	TIME [epoch: 8.19 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10774534878332934		[learning rate: 0.00024036]
		[batch 20/20] avg loss: 0.12206115888822859		[learning rate: 0.00023993]
	Learning Rate: 0.000239927
	LOSS [training: 0.11490325383577896 | validation: 0.10639521336234761]
	TIME [epoch: 8.19 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14446379373680282		[learning rate: 0.00023949]
		[batch 20/20] avg loss: 0.14789056483136762		[learning rate: 0.00023906]
	Learning Rate: 0.000239056
	LOSS [training: 0.14617717928408525 | validation: 0.18035624062051925]
	TIME [epoch: 8.21 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14434257766597017		[learning rate: 0.00023862]
		[batch 20/20] avg loss: 0.12582250168434134		[learning rate: 0.00023819]
	Learning Rate: 0.000238189
	LOSS [training: 0.13508253967515577 | validation: 0.07702410361504682]
	TIME [epoch: 8.19 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1397280993739255		[learning rate: 0.00023776]
		[batch 20/20] avg loss: 0.1467873450975889		[learning rate: 0.00023732]
	Learning Rate: 0.000237324
	LOSS [training: 0.14325772223575722 | validation: 0.08134440591799079]
	TIME [epoch: 8.19 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12190639542215535		[learning rate: 0.00023689]
		[batch 20/20] avg loss: 0.12489160346768872		[learning rate: 0.00023646]
	Learning Rate: 0.000236463
	LOSS [training: 0.12339899944492205 | validation: 0.10080766338140987]
	TIME [epoch: 8.19 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12277127127174567		[learning rate: 0.00023603]
		[batch 20/20] avg loss: 0.13632721235525339		[learning rate: 0.0002356]
	Learning Rate: 0.000235605
	LOSS [training: 0.12954924181349953 | validation: 0.08030425611411299]
	TIME [epoch: 8.21 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12111325165456595		[learning rate: 0.00023518]
		[batch 20/20] avg loss: 0.13466122505307215		[learning rate: 0.00023475]
	Learning Rate: 0.00023475
	LOSS [training: 0.12788723835381904 | validation: 0.09031082885811974]
	TIME [epoch: 8.19 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13312247626163903		[learning rate: 0.00023432]
		[batch 20/20] avg loss: 0.13026308446545093		[learning rate: 0.0002339]
	Learning Rate: 0.000233898
	LOSS [training: 0.13169278036354498 | validation: 0.10033010176654003]
	TIME [epoch: 8.19 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13068518871310864		[learning rate: 0.00023347]
		[batch 20/20] avg loss: 0.12126294021055828		[learning rate: 0.00023305]
	Learning Rate: 0.000233049
	LOSS [training: 0.12597406446183348 | validation: 0.08071792330898549]
	TIME [epoch: 8.19 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12889241298679863		[learning rate: 0.00023263]
		[batch 20/20] avg loss: 0.17393158096032263		[learning rate: 0.0002322]
	Learning Rate: 0.000232203
	LOSS [training: 0.15141199697356064 | validation: 0.09882055558424714]
	TIME [epoch: 8.21 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13336793705956768		[learning rate: 0.00023178]
		[batch 20/20] avg loss: 0.11446457680575098		[learning rate: 0.00023136]
	Learning Rate: 0.000231361
	LOSS [training: 0.12391625693265933 | validation: 0.09494960130167499]
	TIME [epoch: 8.19 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13886599089311047		[learning rate: 0.00023094]
		[batch 20/20] avg loss: 0.16510008449043767		[learning rate: 0.00023052]
	Learning Rate: 0.000230521
	LOSS [training: 0.15198303769177407 | validation: 0.10853740562138531]
	TIME [epoch: 8.19 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14221626219394776		[learning rate: 0.0002301]
		[batch 20/20] avg loss: 0.14182640630848287		[learning rate: 0.00022968]
	Learning Rate: 0.000229685
	LOSS [training: 0.14202133425121533 | validation: 0.0854072208360866]
	TIME [epoch: 8.19 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.132995935672826		[learning rate: 0.00022927]
		[batch 20/20] avg loss: 0.13040112424860714		[learning rate: 0.00022885]
	Learning Rate: 0.000228851
	LOSS [training: 0.13169852996071657 | validation: 0.08416201758141086]
	TIME [epoch: 8.21 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1314666528379732		[learning rate: 0.00022844]
		[batch 20/20] avg loss: 0.11089854604636282		[learning rate: 0.00022802]
	Learning Rate: 0.00022802
	LOSS [training: 0.12118259944216803 | validation: 0.10422248571246223]
	TIME [epoch: 8.19 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13732597199148897		[learning rate: 0.00022761]
		[batch 20/20] avg loss: 0.12132149123632543		[learning rate: 0.00022719]
	Learning Rate: 0.000227193
	LOSS [training: 0.12932373161390723 | validation: 0.076119024248948]
	TIME [epoch: 8.19 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11898874243284893		[learning rate: 0.00022678]
		[batch 20/20] avg loss: 0.1367485591485422		[learning rate: 0.00022637]
	Learning Rate: 0.000226368
	LOSS [training: 0.1278686507906956 | validation: 0.0976315197253112]
	TIME [epoch: 8.19 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1641794583672216		[learning rate: 0.00022596]
		[batch 20/20] avg loss: 0.12626044328237598		[learning rate: 0.00022555]
	Learning Rate: 0.000225547
	LOSS [training: 0.14521995082479874 | validation: 0.08542985092360017]
	TIME [epoch: 8.21 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10751369380644002		[learning rate: 0.00022514]
		[batch 20/20] avg loss: 0.16458247718444685		[learning rate: 0.00022473]
	Learning Rate: 0.000224728
	LOSS [training: 0.13604808549544342 | validation: 0.07880284833965212]
	TIME [epoch: 8.19 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14524212233272973		[learning rate: 0.00022432]
		[batch 20/20] avg loss: 0.2040370504505237		[learning rate: 0.00022391]
	Learning Rate: 0.000223913
	LOSS [training: 0.17463958639162677 | validation: 0.2074192039329878]
	TIME [epoch: 8.19 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2708715200090108		[learning rate: 0.00022351]
		[batch 20/20] avg loss: 0.11567140396961358		[learning rate: 0.0002231]
	Learning Rate: 0.0002231
	LOSS [training: 0.1932714619893122 | validation: 0.09023813977853584]
	TIME [epoch: 8.18 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13398215556946433		[learning rate: 0.0002227]
		[batch 20/20] avg loss: 0.16729211765330637		[learning rate: 0.00022229]
	Learning Rate: 0.000222291
	LOSS [training: 0.15063713661138534 | validation: 0.16946917351520988]
	TIME [epoch: 8.21 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13636685938848073		[learning rate: 0.00022189]
		[batch 20/20] avg loss: 0.14281189965692426		[learning rate: 0.00022148]
	Learning Rate: 0.000221484
	LOSS [training: 0.1395893795227025 | validation: 0.23073435641798995]
	TIME [epoch: 8.19 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15442864858218558		[learning rate: 0.00022108]
		[batch 20/20] avg loss: 0.1855610196589073		[learning rate: 0.00022068]
	Learning Rate: 0.00022068
	LOSS [training: 0.16999483412054645 | validation: 0.44025455424869153]
	TIME [epoch: 8.19 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17356930491158967		[learning rate: 0.00022028]
		[batch 20/20] avg loss: 0.16681573882901274		[learning rate: 0.00021988]
	Learning Rate: 0.000219879
	LOSS [training: 0.1701925218703012 | validation: 0.10168779694495547]
	TIME [epoch: 8.19 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13635458084593502		[learning rate: 0.00021948]
		[batch 20/20] avg loss: 0.1282029656617551		[learning rate: 0.00021908]
	Learning Rate: 0.000219081
	LOSS [training: 0.13227877325384502 | validation: 0.10630290786843014]
	TIME [epoch: 8.2 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11738736228463251		[learning rate: 0.00021868]
		[batch 20/20] avg loss: 0.13002307223607965		[learning rate: 0.00021829]
	Learning Rate: 0.000218286
	LOSS [training: 0.12370521726035608 | validation: 0.09957044811650592]
	TIME [epoch: 8.19 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12495099764261415		[learning rate: 0.00021789]
		[batch 20/20] avg loss: 0.16389286904685585		[learning rate: 0.00021749]
	Learning Rate: 0.000217494
	LOSS [training: 0.14442193334473494 | validation: 0.12008531264553632]
	TIME [epoch: 8.18 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13528800142362044		[learning rate: 0.0002171]
		[batch 20/20] avg loss: 0.12340983654658069		[learning rate: 0.0002167]
	Learning Rate: 0.000216705
	LOSS [training: 0.12934891898510054 | validation: 0.1593121580141032]
	TIME [epoch: 8.18 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16079514673832246		[learning rate: 0.00021631]
		[batch 20/20] avg loss: 0.12065132341501932		[learning rate: 0.00021592]
	Learning Rate: 0.000215918
	LOSS [training: 0.14072323507667084 | validation: 0.07069899635811297]
	TIME [epoch: 8.2 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14247936820953505		[learning rate: 0.00021553]
		[batch 20/20] avg loss: 0.10960111517048995		[learning rate: 0.00021513]
	Learning Rate: 0.000215135
	LOSS [training: 0.12604024169001252 | validation: 0.10830333006930243]
	TIME [epoch: 8.19 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1518605400150297		[learning rate: 0.00021474]
		[batch 20/20] avg loss: 0.11912590769928644		[learning rate: 0.00021435]
	Learning Rate: 0.000214354
	LOSS [training: 0.13549322385715806 | validation: 0.12199141426323404]
	TIME [epoch: 8.18 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16553105331302506		[learning rate: 0.00021396]
		[batch 20/20] avg loss: 0.12899767530854994		[learning rate: 0.00021358]
	Learning Rate: 0.000213576
	LOSS [training: 0.1472643643107875 | validation: 0.08899839961609095]
	TIME [epoch: 8.18 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15341411035716832		[learning rate: 0.00021319]
		[batch 20/20] avg loss: 0.1236280571440024		[learning rate: 0.0002128]
	Learning Rate: 0.000212801
	LOSS [training: 0.13852108375058533 | validation: 0.08665801023967873]
	TIME [epoch: 8.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1412287174121059		[learning rate: 0.00021241]
		[batch 20/20] avg loss: 0.1313515154960208		[learning rate: 0.00021203]
	Learning Rate: 0.000212029
	LOSS [training: 0.13629011645406336 | validation: 0.07920143575005956]
	TIME [epoch: 8.19 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1265568310639098		[learning rate: 0.00021164]
		[batch 20/20] avg loss: 0.7225111269220713		[learning rate: 0.00021126]
	Learning Rate: 0.000211259
	LOSS [training: 0.4245339789929906 | validation: 1.44331931028871]
	TIME [epoch: 8.18 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.248110985370312		[learning rate: 0.00021088]
		[batch 20/20] avg loss: 1.527035254834499		[learning rate: 0.00021049]
	Learning Rate: 0.000210493
	LOSS [training: 1.3875731201024055 | validation: 2.7006337038374353]
	TIME [epoch: 8.19 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.061010630718145		[learning rate: 0.00021011]
		[batch 20/20] avg loss: 5.5566003077589965		[learning rate: 0.00020973]
	Learning Rate: 0.000209729
	LOSS [training: 4.808805469238571 | validation: 5.809153994822901]
	TIME [epoch: 8.21 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.549994457537876		[learning rate: 0.00020935]
		[batch 20/20] avg loss: 7.418270276433193		[learning rate: 0.00020897]
	Learning Rate: 0.000208968
	LOSS [training: 6.984132366985536 | validation: 6.8365990024178025]
	TIME [epoch: 8.19 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.0586204242101		[learning rate: 0.00020859]
		[batch 20/20] avg loss: 6.7378184539757395		[learning rate: 0.00020821]
	Learning Rate: 0.000208209
	LOSS [training: 6.898219439092921 | validation: 6.19893377629391]
	TIME [epoch: 8.18 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.220207813720051		[learning rate: 0.00020783]
		[batch 20/20] avg loss: 5.797616578425975		[learning rate: 0.00020745]
	Learning Rate: 0.000207454
	LOSS [training: 6.008912196073011 | validation: 5.38758217864504]
	TIME [epoch: 8.18 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.614216882928357		[learning rate: 0.00020708]
		[batch 20/20] avg loss: 5.339394085344699		[learning rate: 0.0002067]
	Learning Rate: 0.000206701
	LOSS [training: 5.476805484136527 | validation: 4.852882018530942]
	TIME [epoch: 8.21 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.50346193369092		[learning rate: 0.00020633]
		[batch 20/20] avg loss: 4.94671451935195		[learning rate: 0.00020595]
	Learning Rate: 0.000205951
	LOSS [training: 4.725088226521435 | validation: 5.218858881450149]
	TIME [epoch: 8.19 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.738487356423019		[learning rate: 0.00020558]
		[batch 20/20] avg loss: 4.269403903320981		[learning rate: 0.0002052]
	Learning Rate: 0.000205203
	LOSS [training: 4.503945629872001 | validation: 4.671916212937604]
	TIME [epoch: 8.19 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.26926554722986		[learning rate: 0.00020483]
		[batch 20/20] avg loss: 4.025395800280419		[learning rate: 0.00020446]
	Learning Rate: 0.000204459
	LOSS [training: 4.64733067375514 | validation: 4.04316868534573]
	TIME [epoch: 8.19 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.175160056710451		[learning rate: 0.00020409]
		[batch 20/20] avg loss: 5.588833658598534		[learning rate: 0.00020372]
	Learning Rate: 0.000203717
	LOSS [training: 4.881996857654493 | validation: 5.587115976924159]
	TIME [epoch: 8.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.4449168035149915		[learning rate: 0.00020335]
		[batch 20/20] avg loss: 5.65264059180118		[learning rate: 0.00020298]
	Learning Rate: 0.000202977
	LOSS [training: 5.548778697658086 | validation: 6.096414342804915]
	TIME [epoch: 8.19 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.582960876813885		[learning rate: 0.00020261]
		[batch 20/20] avg loss: 6.809537926711661		[learning rate: 0.00020224]
	Learning Rate: 0.000202241
	LOSS [training: 6.696249401762772 | validation: 6.560033433508496]
	TIME [epoch: 8.19 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.950714400742844		[learning rate: 0.00020187]
		[batch 20/20] avg loss: 6.7702550454989865		[learning rate: 0.00020151]
	Learning Rate: 0.000201507
	LOSS [training: 6.860484723120915 | validation: 6.539715971681927]
	TIME [epoch: 8.19 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.747272289458391		[learning rate: 0.00020114]
		[batch 20/20] avg loss: 7.340445990596578		[learning rate: 0.00020078]
	Learning Rate: 0.000200775
	LOSS [training: 7.043859140027484 | validation: 6.820824135090805]
	TIME [epoch: 8.2 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.065576156962388		[learning rate: 0.00020041]
		[batch 20/20] avg loss: 6.1793084620058405		[learning rate: 0.00020005]
	Learning Rate: 0.000200047
	LOSS [training: 6.622442309484114 | validation: 5.609272778734146]
	TIME [epoch: 8.2 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.361494117872302		[learning rate: 0.00019968]
		[batch 20/20] avg loss: 5.3954107137125		[learning rate: 0.00019932]
	Learning Rate: 0.000199321
	LOSS [training: 5.378452415792402 | validation: 5.547567167753732]
	TIME [epoch: 8.19 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.154618455601695		[learning rate: 0.00019896]
		[batch 20/20] avg loss: 5.427605766575813		[learning rate: 0.0001986]
	Learning Rate: 0.000198597
	LOSS [training: 5.791112111088755 | validation: 5.662868945384059]
	TIME [epoch: 8.19 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.5549148524234075		[learning rate: 0.00019824]
		[batch 20/20] avg loss: 5.930408957321843		[learning rate: 0.00019788]
	Learning Rate: 0.000197877
	LOSS [training: 5.742661904872624 | validation: 6.166470338667454]
	TIME [epoch: 8.2 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.401438155798897		[learning rate: 0.00019752]
		[batch 20/20] avg loss: 6.577530322199232		[learning rate: 0.00019716]
	Learning Rate: 0.000197159
	LOSS [training: 6.489484238999064 | validation: 6.610334148558167]
	TIME [epoch: 8.2 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.603803862651853		[learning rate: 0.0001968]
		[batch 20/20] avg loss: 7.151985288811264		[learning rate: 0.00019644]
	Learning Rate: 0.000196443
	LOSS [training: 6.877894575731558 | validation: 6.9451905708882915]
	TIME [epoch: 8.19 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.376371789917499		[learning rate: 0.00019609]
		[batch 20/20] avg loss: 7.337040420573028		[learning rate: 0.00019573]
	Learning Rate: 0.00019573
	LOSS [training: 7.356706105245264 | validation: 6.87644261344625]
	TIME [epoch: 8.18 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.236521038950504		[learning rate: 0.00019537]
		[batch 20/20] avg loss: 6.356956624017722		[learning rate: 0.00019502]
	Learning Rate: 0.00019502
	LOSS [training: 6.796738831484113 | validation: 5.833955460975698]
	TIME [epoch: 8.19 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.855600597341332		[learning rate: 0.00019467]
		[batch 20/20] avg loss: 5.626246862038814		[learning rate: 0.00019431]
	Learning Rate: 0.000194312
	LOSS [training: 5.740923729690072 | validation: 5.549070963786114]
	TIME [epoch: 8.2 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.356656066838008		[learning rate: 0.00019396]
		[batch 20/20] avg loss: 5.893414487187293		[learning rate: 0.00019361]
	Learning Rate: 0.000193607
	LOSS [training: 5.625035277012651 | validation: 5.860368418063486]
	TIME [epoch: 8.2 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.401288814370792		[learning rate: 0.00019326]
		[batch 20/20] avg loss: 4.327330216814958		[learning rate: 0.0001929]
	Learning Rate: 0.000192904
	LOSS [training: 4.8643095155928755 | validation: 3.2446414276828355]
	TIME [epoch: 8.19 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3051579087057616		[learning rate: 0.00019255]
		[batch 20/20] avg loss: 2.551105419614257		[learning rate: 0.0001922]
	Learning Rate: 0.000192204
	LOSS [training: 2.428131664160009 | validation: 2.500273104819113]
	TIME [epoch: 8.2 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0323644026485397		[learning rate: 0.00019186]
		[batch 20/20] avg loss: 0.48192110363398066		[learning rate: 0.00019151]
	Learning Rate: 0.000191507
	LOSS [training: 1.25714275314126 | validation: 0.2263369221216124]
	TIME [epoch: 8.21 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19881393453913423		[learning rate: 0.00019116]
		[batch 20/20] avg loss: 0.12176270898067627		[learning rate: 0.00019081]
	Learning Rate: 0.000190812
	LOSS [training: 0.16028832175990526 | validation: 0.08318620452479755]
	TIME [epoch: 8.19 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13298793104058212		[learning rate: 0.00019047]
		[batch 20/20] avg loss: 0.12681641139391925		[learning rate: 0.00019012]
	Learning Rate: 0.000190119
	LOSS [training: 0.1299021712172507 | validation: 0.09099795286938843]
	TIME [epoch: 8.19 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11641418558497207		[learning rate: 0.00018977]
		[batch 20/20] avg loss: 0.10655353261579852		[learning rate: 0.00018943]
	Learning Rate: 0.000189429
	LOSS [training: 0.1114838591003853 | validation: 0.07067547746939706]
	TIME [epoch: 8.19 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10645395671311986		[learning rate: 0.00018909]
		[batch 20/20] avg loss: 0.10267050415648749		[learning rate: 0.00018874]
	Learning Rate: 0.000188742
	LOSS [training: 0.10456223043480364 | validation: 0.06567133113866702]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1192.pth
	Model improved!!!
EPOCH 1193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10930276479144432		[learning rate: 0.0001884]
		[batch 20/20] avg loss: 0.1090657333513263		[learning rate: 0.00018806]
	Learning Rate: 0.000188057
	LOSS [training: 0.10918424907138531 | validation: 0.07464183219081723]
	TIME [epoch: 8.2 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24536448333509653		[learning rate: 0.00018772]
		[batch 20/20] avg loss: 0.14519322261677264		[learning rate: 0.00018737]
	Learning Rate: 0.000187375
	LOSS [training: 0.19527885297593456 | validation: 0.06362600766832617]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1194.pth
	Model improved!!!
EPOCH 1195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1289123312104033		[learning rate: 0.00018703]
		[batch 20/20] avg loss: 0.10496364106451614		[learning rate: 0.00018669]
	Learning Rate: 0.000186695
	LOSS [training: 0.11693798613745973 | validation: 0.07198249191364302]
	TIME [epoch: 8.2 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12437743992836274		[learning rate: 0.00018636]
		[batch 20/20] avg loss: 0.12318015321364903		[learning rate: 0.00018602]
	Learning Rate: 0.000186017
	LOSS [training: 0.12377879657100588 | validation: 0.10083471735845945]
	TIME [epoch: 8.21 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11881055761459396		[learning rate: 0.00018568]
		[batch 20/20] avg loss: 0.10902523360770049		[learning rate: 0.00018534]
	Learning Rate: 0.000185342
	LOSS [training: 0.11391789561114722 | validation: 0.08290561594012608]
	TIME [epoch: 8.18 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11493670077585068		[learning rate: 0.00018501]
		[batch 20/20] avg loss: 0.10402285818950943		[learning rate: 0.00018467]
	Learning Rate: 0.000184669
	LOSS [training: 0.10947977948268006 | validation: 0.07816787763492583]
	TIME [epoch: 8.19 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10661767481134002		[learning rate: 0.00018433]
		[batch 20/20] avg loss: 0.10147733116609617		[learning rate: 0.000184]
	Learning Rate: 0.000183999
	LOSS [training: 0.10404750298871808 | validation: 0.07785034215633183]
	TIME [epoch: 8.19 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10928209417670494		[learning rate: 0.00018367]
		[batch 20/20] avg loss: 0.11970692191164442		[learning rate: 0.00018333]
	Learning Rate: 0.000183331
	LOSS [training: 0.11449450804417469 | validation: 0.10287128939002896]
	TIME [epoch: 8.2 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11299206502193408		[learning rate: 0.000183]
		[batch 20/20] avg loss: 0.10961876802908181		[learning rate: 0.00018267]
	Learning Rate: 0.000182666
	LOSS [training: 0.11130541652550793 | validation: 0.09453026827284412]
	TIME [epoch: 8.19 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1282491721006654		[learning rate: 0.00018233]
		[batch 20/20] avg loss: 0.1428520493754255		[learning rate: 0.000182]
	Learning Rate: 0.000182003
	LOSS [training: 0.13555061073804547 | validation: 0.07127737853673917]
	TIME [epoch: 8.19 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10194126981541571		[learning rate: 0.00018167]
		[batch 20/20] avg loss: 0.10414646363404065		[learning rate: 0.00018134]
	Learning Rate: 0.000181343
	LOSS [training: 0.1030438667247282 | validation: 0.07027036879810468]
	TIME [epoch: 8.18 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10694627721179133		[learning rate: 0.00018101]
		[batch 20/20] avg loss: 0.11755195821373861		[learning rate: 0.00018068]
	Learning Rate: 0.000180685
	LOSS [training: 0.11224911771276495 | validation: 0.07872788510437898]
	TIME [epoch: 8.21 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10677843350349545		[learning rate: 0.00018036]
		[batch 20/20] avg loss: 0.095984804970889		[learning rate: 0.00018003]
	Learning Rate: 0.000180029
	LOSS [training: 0.10138161923719222 | validation: 0.06980237258876959]
	TIME [epoch: 8.18 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10432442956236603		[learning rate: 0.0001797]
		[batch 20/20] avg loss: 0.10788816285664041		[learning rate: 0.00017938]
	Learning Rate: 0.000179376
	LOSS [training: 0.10610629620950321 | validation: 0.07174777825176487]
	TIME [epoch: 8.18 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1190194951899963		[learning rate: 0.00017905]
		[batch 20/20] avg loss: 0.12213738188628231		[learning rate: 0.00017872]
	Learning Rate: 0.000178725
	LOSS [training: 0.12057843853813932 | validation: 0.0848417560053247]
	TIME [epoch: 8.2 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11263893678151851		[learning rate: 0.0001784]
		[batch 20/20] avg loss: 0.0919225207636056		[learning rate: 0.00017808]
	Learning Rate: 0.000178076
	LOSS [training: 0.10228072877256204 | validation: 0.06583678432919568]
	TIME [epoch: 8.21 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11017486240394896		[learning rate: 0.00017775]
		[batch 20/20] avg loss: 0.16421988673697913		[learning rate: 0.00017743]
	Learning Rate: 0.00017743
	LOSS [training: 0.13719737457046405 | validation: 0.1517506910423408]
	TIME [epoch: 8.18 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16255562281394415		[learning rate: 0.00017711]
		[batch 20/20] avg loss: 0.12636873241197408		[learning rate: 0.00017679]
	Learning Rate: 0.000176786
	LOSS [training: 0.14446217761295913 | validation: 0.06843915200724429]
	TIME [epoch: 8.19 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10441568711163937		[learning rate: 0.00017646]
		[batch 20/20] avg loss: 0.13537859989680098		[learning rate: 0.00017614]
	Learning Rate: 0.000176144
	LOSS [training: 0.11989714350422016 | validation: 0.12858302088751744]
	TIME [epoch: 8.19 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13379361835278747		[learning rate: 0.00017582]
		[batch 20/20] avg loss: 0.15323758612875435		[learning rate: 0.0001755]
	Learning Rate: 0.000175505
	LOSS [training: 0.14351560224077092 | validation: 0.07995591942787596]
	TIME [epoch: 8.21 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11637802182227937		[learning rate: 0.00017519]
		[batch 20/20] avg loss: 0.12901940680309432		[learning rate: 0.00017487]
	Learning Rate: 0.000174868
	LOSS [training: 0.12269871431268686 | validation: 0.086493241258285]
	TIME [epoch: 8.19 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1681558320199455		[learning rate: 0.00017455]
		[batch 20/20] avg loss: 0.11635721156946237		[learning rate: 0.00017423]
	Learning Rate: 0.000174233
	LOSS [training: 0.14225652179470397 | validation: 0.1040810849387013]
	TIME [epoch: 8.19 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12306815932889477		[learning rate: 0.00017392]
		[batch 20/20] avg loss: 0.09338975722451845		[learning rate: 0.0001736]
	Learning Rate: 0.000173601
	LOSS [training: 0.10822895827670662 | validation: 0.07195207892842168]
	TIME [epoch: 8.19 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1218183302851548		[learning rate: 0.00017329]
		[batch 20/20] avg loss: 0.11750354976914142		[learning rate: 0.00017297]
	Learning Rate: 0.000172971
	LOSS [training: 0.11966094002714811 | validation: 0.08100835207852844]
	TIME [epoch: 8.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11850257959673058		[learning rate: 0.00017266]
		[batch 20/20] avg loss: 0.12506583965435505		[learning rate: 0.00017234]
	Learning Rate: 0.000172343
	LOSS [training: 0.12178420962554284 | validation: 0.08539829184367369]
	TIME [epoch: 8.19 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11443622882203983		[learning rate: 0.00017203]
		[batch 20/20] avg loss: 0.13050216961492825		[learning rate: 0.00017172]
	Learning Rate: 0.000171718
	LOSS [training: 0.12246919921848402 | validation: 0.07908615246731644]
	TIME [epoch: 8.19 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16657134565141418		[learning rate: 0.00017141]
		[batch 20/20] avg loss: 0.12445719640135985		[learning rate: 0.00017109]
	Learning Rate: 0.000171095
	LOSS [training: 0.14551427102638703 | validation: 0.07340591517379469]
	TIME [epoch: 8.19 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12594671621327067		[learning rate: 0.00017078]
		[batch 20/20] avg loss: 0.11703116436228492		[learning rate: 0.00017047]
	Learning Rate: 0.000170474
	LOSS [training: 0.12148894028777779 | validation: 0.08825029709094974]
	TIME [epoch: 8.21 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11222860935329874		[learning rate: 0.00017016]
		[batch 20/20] avg loss: 0.11802214877966452		[learning rate: 0.00016986]
	Learning Rate: 0.000169855
	LOSS [training: 0.11512537906648164 | validation: 0.0891538847955266]
	TIME [epoch: 8.19 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15605532697454		[learning rate: 0.00016955]
		[batch 20/20] avg loss: 0.15518825335832775		[learning rate: 0.00016924]
	Learning Rate: 0.000169239
	LOSS [training: 0.15562179016643388 | validation: 0.12051291778513816]
	TIME [epoch: 8.19 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13260275471455157		[learning rate: 0.00016893]
		[batch 20/20] avg loss: 0.11657825638937017		[learning rate: 0.00016862]
	Learning Rate: 0.000168625
	LOSS [training: 0.1245905055519609 | validation: 0.09364852819708738]
	TIME [epoch: 8.18 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1386325412358552		[learning rate: 0.00016832]
		[batch 20/20] avg loss: 0.14374336893292589		[learning rate: 0.00016801]
	Learning Rate: 0.000168013
	LOSS [training: 0.14118795508439055 | validation: 0.08494162102673569]
	TIME [epoch: 8.21 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1249226885321721		[learning rate: 0.00016771]
		[batch 20/20] avg loss: 0.1291977881588698		[learning rate: 0.0001674]
	Learning Rate: 0.000167403
	LOSS [training: 0.12706023834552094 | validation: 0.08848944211011664]
	TIME [epoch: 8.19 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12676429567767808		[learning rate: 0.0001671]
		[batch 20/20] avg loss: 0.13483336883793884		[learning rate: 0.0001668]
	Learning Rate: 0.000166795
	LOSS [training: 0.13079883225780847 | validation: 0.09381481409588235]
	TIME [epoch: 8.19 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13506563804675312		[learning rate: 0.00016649]
		[batch 20/20] avg loss: 0.12440421456365472		[learning rate: 0.00016619]
	Learning Rate: 0.00016619
	LOSS [training: 0.12973492630520392 | validation: 0.08880919310485855]
	TIME [epoch: 8.18 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11731607250716185		[learning rate: 0.00016589]
		[batch 20/20] avg loss: 0.12200972517736683		[learning rate: 0.00016559]
	Learning Rate: 0.000165587
	LOSS [training: 0.11966289884226433 | validation: 0.0683667610655502]
	TIME [epoch: 8.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12192837166170127		[learning rate: 0.00016529]
		[batch 20/20] avg loss: 0.12280849052867944		[learning rate: 0.00016499]
	Learning Rate: 0.000164986
	LOSS [training: 0.12236843109519038 | validation: 0.07713536089914493]
	TIME [epoch: 8.19 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.114490726280197		[learning rate: 0.00016469]
		[batch 20/20] avg loss: 0.13860444130624291		[learning rate: 0.00016439]
	Learning Rate: 0.000164387
	LOSS [training: 0.12654758379322 | validation: 0.09357052501167881]
	TIME [epoch: 8.18 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15066171548071164		[learning rate: 0.00016409]
		[batch 20/20] avg loss: 0.13461893645545492		[learning rate: 0.00016379]
	Learning Rate: 0.000163791
	LOSS [training: 0.1426403259680833 | validation: 0.09814480422199653]
	TIME [epoch: 8.18 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13045717539471235		[learning rate: 0.00016349]
		[batch 20/20] avg loss: 0.12207139237274425		[learning rate: 0.0001632]
	Learning Rate: 0.000163196
	LOSS [training: 0.1262642838837283 | validation: 0.0898989005269599]
	TIME [epoch: 8.21 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12683975157335858		[learning rate: 0.0001629]
		[batch 20/20] avg loss: 0.11964787789213624		[learning rate: 0.0001626]
	Learning Rate: 0.000162604
	LOSS [training: 0.12324381473274744 | validation: 0.0913865914334302]
	TIME [epoch: 8.2 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13032569041370473		[learning rate: 0.00016231]
		[batch 20/20] avg loss: 0.16787655602297238		[learning rate: 0.00016201]
	Learning Rate: 0.000162014
	LOSS [training: 0.14910112321833857 | validation: 0.11284506103769251]
	TIME [epoch: 8.19 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13154980187669355		[learning rate: 0.00016172]
		[batch 20/20] avg loss: 0.1381392395496306		[learning rate: 0.00016143]
	Learning Rate: 0.000161426
	LOSS [training: 0.13484452071316205 | validation: 0.09372337637639402]
	TIME [epoch: 8.18 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1270721742338916		[learning rate: 0.00016113]
		[batch 20/20] avg loss: 0.14273427486587068		[learning rate: 0.00016084]
	Learning Rate: 0.00016084
	LOSS [training: 0.13490322454988116 | validation: 0.11332289910561968]
	TIME [epoch: 8.21 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14061493191548566		[learning rate: 0.00016055]
		[batch 20/20] avg loss: 0.14969629336256057		[learning rate: 0.00016026]
	Learning Rate: 0.000160257
	LOSS [training: 0.14515561263902316 | validation: 0.10798182431508005]
	TIME [epoch: 8.19 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14848140573680146		[learning rate: 0.00015997]
		[batch 20/20] avg loss: 0.14044554984991153		[learning rate: 0.00015967]
	Learning Rate: 0.000159675
	LOSS [training: 0.1444634777933565 | validation: 0.09830719008134926]
	TIME [epoch: 8.19 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14684765407462325		[learning rate: 0.00015938]
		[batch 20/20] avg loss: 0.15163371160468558		[learning rate: 0.0001591]
	Learning Rate: 0.000159096
	LOSS [training: 0.14924068283965447 | validation: 0.10235000673453629]
	TIME [epoch: 8.19 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13591606423107422		[learning rate: 0.00015881]
		[batch 20/20] avg loss: 0.20988704204294698		[learning rate: 0.00015852]
	Learning Rate: 0.000158518
	LOSS [training: 0.1729015531370106 | validation: 0.19951181607349386]
	TIME [epoch: 8.2 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1928849299579159		[learning rate: 0.00015823]
		[batch 20/20] avg loss: 0.16639068752300717		[learning rate: 0.00015794]
	Learning Rate: 0.000157943
	LOSS [training: 0.17963780874046148 | validation: 0.12281963063935389]
	TIME [epoch: 8.19 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15322249273386906		[learning rate: 0.00015766]
		[batch 20/20] avg loss: 0.17606828215133366		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.16464538744260135 | validation: 0.1705294309617439]
	TIME [epoch: 8.18 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1935919560102364		[learning rate: 0.00015708]
		[batch 20/20] avg loss: 0.16627574936151696		[learning rate: 0.0001568]
	Learning Rate: 0.000156799
	LOSS [training: 0.17993385268587667 | validation: 0.12343769868433357]
	TIME [epoch: 8.18 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16388115233948441		[learning rate: 0.00015651]
		[batch 20/20] avg loss: 0.17817698114279862		[learning rate: 0.00015623]
	Learning Rate: 0.00015623
	LOSS [training: 0.17102906674114152 | validation: 0.15106779461508393]
	TIME [epoch: 8.2 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1765203964244863		[learning rate: 0.00015595]
		[batch 20/20] avg loss: 0.1890433196230456		[learning rate: 0.00015566]
	Learning Rate: 0.000155663
	LOSS [training: 0.18278185802376595 | validation: 0.11560514106058106]
	TIME [epoch: 8.19 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1829044352725643		[learning rate: 0.00015538]
		[batch 20/20] avg loss: 0.28331468378153674		[learning rate: 0.0001551]
	Learning Rate: 0.000155098
	LOSS [training: 0.23310955952705054 | validation: 0.17331798484824093]
	TIME [epoch: 8.19 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16642556279150528		[learning rate: 0.00015482]
		[batch 20/20] avg loss: 0.1600683287582388		[learning rate: 0.00015453]
	Learning Rate: 0.000154535
	LOSS [training: 0.16324694577487203 | validation: 0.1489184422128774]
	TIME [epoch: 8.18 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17576567420620365		[learning rate: 0.00015425]
		[batch 20/20] avg loss: 0.14284075287692538		[learning rate: 0.00015397]
	Learning Rate: 0.000153974
	LOSS [training: 0.15930321354156454 | validation: 0.12230522450244935]
	TIME [epoch: 8.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1858996543194135		[learning rate: 0.00015369]
		[batch 20/20] avg loss: 0.1520371125558695		[learning rate: 0.00015342]
	Learning Rate: 0.000153415
	LOSS [training: 0.16896838343764148 | validation: 0.12306129793536386]
	TIME [epoch: 8.19 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23089145370263808		[learning rate: 0.00015314]
		[batch 20/20] avg loss: 0.16602604310010813		[learning rate: 0.00015286]
	Learning Rate: 0.000152858
	LOSS [training: 0.1984587484013731 | validation: 0.13256970051528347]
	TIME [epoch: 8.18 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19811931550879383		[learning rate: 0.00015258]
		[batch 20/20] avg loss: 0.28805237945105755		[learning rate: 0.0001523]
	Learning Rate: 0.000152304
	LOSS [training: 0.24308584747992565 | validation: 0.34553778821044917]
	TIME [epoch: 8.19 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25844724329470925		[learning rate: 0.00015203]
		[batch 20/20] avg loss: 0.21301645023127352		[learning rate: 0.00015175]
	Learning Rate: 0.000151751
	LOSS [training: 0.23573184676299141 | validation: 0.23424363101704163]
	TIME [epoch: 8.21 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21732088621127205		[learning rate: 0.00015148]
		[batch 20/20] avg loss: 0.21504698860652813		[learning rate: 0.0001512]
	Learning Rate: 0.0001512
	LOSS [training: 0.21618393740890013 | validation: 0.15368817538340446]
	TIME [epoch: 8.19 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17287802350395373		[learning rate: 0.00015093]
		[batch 20/20] avg loss: 0.21661795610230733		[learning rate: 0.00015065]
	Learning Rate: 0.000150652
	LOSS [training: 0.19474798980313052 | validation: 0.11627266390665966]
	TIME [epoch: 8.18 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15929442765104967		[learning rate: 0.00015038]
		[batch 20/20] avg loss: 0.23496167075624724		[learning rate: 0.0001501]
	Learning Rate: 0.000150105
	LOSS [training: 0.1971280492036485 | validation: 0.164489117051283]
	TIME [epoch: 8.19 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20764626821848794		[learning rate: 0.00014983]
		[batch 20/20] avg loss: 0.2922872649556005		[learning rate: 0.00014956]
	Learning Rate: 0.00014956
	LOSS [training: 0.2499667665870442 | validation: 0.17789151363470893]
	TIME [epoch: 8.2 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2107057766272002		[learning rate: 0.00014929]
		[batch 20/20] avg loss: 0.20549890892222242		[learning rate: 0.00014902]
	Learning Rate: 0.000149017
	LOSS [training: 0.2081023427747113 | validation: 0.28816477468525625]
	TIME [epoch: 8.19 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2680793775937923		[learning rate: 0.00014875]
		[batch 20/20] avg loss: 0.34347564016824195		[learning rate: 0.00014848]
	Learning Rate: 0.000148477
	LOSS [training: 0.3057775088810172 | validation: 0.3919998408095151]
	TIME [epoch: 8.19 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5026312334507766		[learning rate: 0.00014821]
		[batch 20/20] avg loss: 0.40002543946671765		[learning rate: 0.00014794]
	Learning Rate: 0.000147938
	LOSS [training: 0.451328336458747 | validation: 0.37675698825023896]
	TIME [epoch: 8.18 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31159949996093333		[learning rate: 0.00014767]
		[batch 20/20] avg loss: 0.2055051143580855		[learning rate: 0.0001474]
	Learning Rate: 0.000147401
	LOSS [training: 0.2585523071595094 | validation: 0.15258699734987358]
	TIME [epoch: 8.2 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1711671098713854		[learning rate: 0.00014713]
		[batch 20/20] avg loss: 0.20399478707039917		[learning rate: 0.00014687]
	Learning Rate: 0.000146866
	LOSS [training: 0.1875809484708923 | validation: 0.14470630324734116]
	TIME [epoch: 8.18 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18363042900036458		[learning rate: 0.0001466]
		[batch 20/20] avg loss: 0.19203836860622164		[learning rate: 0.00014633]
	Learning Rate: 0.000146333
	LOSS [training: 0.18783439880329308 | validation: 0.13906209159306193]
	TIME [epoch: 8.19 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1765460443019538		[learning rate: 0.00014607]
		[batch 20/20] avg loss: 0.18894714806285115		[learning rate: 0.0001458]
	Learning Rate: 0.000145802
	LOSS [training: 0.18274659618240247 | validation: 0.12070779882852489]
	TIME [epoch: 8.18 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15914468947028687		[learning rate: 0.00014554]
		[batch 20/20] avg loss: 0.16302616868843675		[learning rate: 0.00014527]
	Learning Rate: 0.000145273
	LOSS [training: 0.1610854290793618 | validation: 0.1259195510917596]
	TIME [epoch: 8.2 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1937600906588312		[learning rate: 0.00014501]
		[batch 20/20] avg loss: 0.15939281274226672		[learning rate: 0.00014475]
	Learning Rate: 0.000144746
	LOSS [training: 0.17657645170054895 | validation: 0.10365866176834232]
	TIME [epoch: 8.18 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14211702059308376		[learning rate: 0.00014448]
		[batch 20/20] avg loss: 0.15567290457967864		[learning rate: 0.00014422]
	Learning Rate: 0.00014422
	LOSS [training: 0.14889496258638119 | validation: 0.14364755837878912]
	TIME [epoch: 8.18 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1552006836921455		[learning rate: 0.00014396]
		[batch 20/20] avg loss: 0.17660890267334567		[learning rate: 0.0001437]
	Learning Rate: 0.000143697
	LOSS [training: 0.1659047931827456 | validation: 0.11495133182091075]
	TIME [epoch: 8.18 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1648018437418477		[learning rate: 0.00014344]
		[batch 20/20] avg loss: 0.1371042351095111		[learning rate: 0.00014318]
	Learning Rate: 0.000143175
	LOSS [training: 0.15095303942567936 | validation: 0.12031482132564578]
	TIME [epoch: 8.2 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1654303106633505		[learning rate: 0.00014292]
		[batch 20/20] avg loss: 0.14238164105555348		[learning rate: 0.00014266]
	Learning Rate: 0.000142656
	LOSS [training: 0.153905975859452 | validation: 0.15582085760884168]
	TIME [epoch: 8.19 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15390358613006175		[learning rate: 0.0001424]
		[batch 20/20] avg loss: 0.1583517908619593		[learning rate: 0.00014214]
	Learning Rate: 0.000142138
	LOSS [training: 0.1561276884960105 | validation: 0.12168952898481308]
	TIME [epoch: 8.18 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1596499184249311		[learning rate: 0.00014188]
		[batch 20/20] avg loss: 0.15659361293283933		[learning rate: 0.00014162]
	Learning Rate: 0.000141622
	LOSS [training: 0.1581217656788852 | validation: 0.11529404463013188]
	TIME [epoch: 8.18 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16537777492373523		[learning rate: 0.00014137]
		[batch 20/20] avg loss: 0.15299515630280106		[learning rate: 0.00014111]
	Learning Rate: 0.000141108
	LOSS [training: 0.15918646561326816 | validation: 0.10288924989265316]
	TIME [epoch: 8.2 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15554857210017048		[learning rate: 0.00014085]
		[batch 20/20] avg loss: 0.14423022905132224		[learning rate: 0.0001406]
	Learning Rate: 0.000140596
	LOSS [training: 0.14988940057574637 | validation: 0.10723196284597633]
	TIME [epoch: 8.19 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15975174800846006		[learning rate: 0.00014034]
		[batch 20/20] avg loss: 0.1434314797752291		[learning rate: 0.00014009]
	Learning Rate: 0.000140086
	LOSS [training: 0.15159161389184456 | validation: 0.10751999923572736]
	TIME [epoch: 8.18 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16043449556486078		[learning rate: 0.00013983]
		[batch 20/20] avg loss: 0.1379371233049819		[learning rate: 0.00013958]
	Learning Rate: 0.000139578
	LOSS [training: 0.14918580943492132 | validation: 0.13039207462338676]
	TIME [epoch: 8.19 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17869690780768574		[learning rate: 0.00013932]
		[batch 20/20] avg loss: 0.16193782998039072		[learning rate: 0.00013907]
	Learning Rate: 0.000139071
	LOSS [training: 0.17031736889403826 | validation: 0.12073422109427827]
	TIME [epoch: 8.19 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.166924329377029		[learning rate: 0.00013882]
		[batch 20/20] avg loss: 0.14760089087652858		[learning rate: 0.00013857]
	Learning Rate: 0.000138566
	LOSS [training: 0.15726261012677883 | validation: 0.10411050600941296]
	TIME [epoch: 8.19 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1323584881043079		[learning rate: 0.00013831]
		[batch 20/20] avg loss: 0.16108696653538543		[learning rate: 0.00013806]
	Learning Rate: 0.000138064
	LOSS [training: 0.14672272731984665 | validation: 0.10485845030760957]
	TIME [epoch: 8.18 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1767795314904016		[learning rate: 0.00013781]
		[batch 20/20] avg loss: 0.16343214425565414		[learning rate: 0.00013756]
	Learning Rate: 0.000137562
	LOSS [training: 0.17010583787302783 | validation: 0.10862926230641673]
	TIME [epoch: 8.18 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14898109286515415		[learning rate: 0.00013731]
		[batch 20/20] avg loss: 0.16091960062846386		[learning rate: 0.00013706]
	Learning Rate: 0.000137063
	LOSS [training: 0.15495034674680902 | validation: 0.12330500643960325]
	TIME [epoch: 8.19 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1545953398504154		[learning rate: 0.00013681]
		[batch 20/20] avg loss: 0.14955108219920646		[learning rate: 0.00013657]
	Learning Rate: 0.000136566
	LOSS [training: 0.15207321102481094 | validation: 0.1137627010161111]
	TIME [epoch: 8.19 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15516814033367074		[learning rate: 0.00013632]
		[batch 20/20] avg loss: 0.1495695678650628		[learning rate: 0.00013607]
	Learning Rate: 0.00013607
	LOSS [training: 0.15236885409936676 | validation: 0.1139099350925837]
	TIME [epoch: 8.19 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17509226342117473		[learning rate: 0.00013582]
		[batch 20/20] avg loss: 0.15498060286025955		[learning rate: 0.00013558]
	Learning Rate: 0.000135576
	LOSS [training: 0.16503643314071714 | validation: 0.13937478548958868]
	TIME [epoch: 8.19 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15828306911473705		[learning rate: 0.00013533]
		[batch 20/20] avg loss: 0.17285399548063324		[learning rate: 0.00013508]
	Learning Rate: 0.000135084
	LOSS [training: 0.16556853229768512 | validation: 0.1057121846248493]
	TIME [epoch: 8.18 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14774855872471268		[learning rate: 0.00013484]
		[batch 20/20] avg loss: 0.16845527463554272		[learning rate: 0.00013459]
	Learning Rate: 0.000134594
	LOSS [training: 0.1581019166801277 | validation: 0.11445813236897678]
	TIME [epoch: 8.21 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15535660835817963		[learning rate: 0.00013435]
		[batch 20/20] avg loss: 0.16573120759574617		[learning rate: 0.00013411]
	Learning Rate: 0.000134106
	LOSS [training: 0.1605439079769629 | validation: 0.12166424321282288]
	TIME [epoch: 8.18 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13678838843626978		[learning rate: 0.00013386]
		[batch 20/20] avg loss: 0.15110901109507263		[learning rate: 0.00013362]
	Learning Rate: 0.000133619
	LOSS [training: 0.14394869976567118 | validation: 0.10089166833290333]
	TIME [epoch: 8.19 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14263186130444258		[learning rate: 0.00013338]
		[batch 20/20] avg loss: 0.13708376368592362		[learning rate: 0.00013313]
	Learning Rate: 0.000133134
	LOSS [training: 0.1398578124951831 | validation: 0.09558579190572909]
	TIME [epoch: 8.18 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15203375807298		[learning rate: 0.00013289]
		[batch 20/20] avg loss: 0.16791562834857407		[learning rate: 0.00013265]
	Learning Rate: 0.000132651
	LOSS [training: 0.15997469321077704 | validation: 0.10045351678153996]
	TIME [epoch: 8.2 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12928662445082922		[learning rate: 0.00013241]
		[batch 20/20] avg loss: 0.1441005321844917		[learning rate: 0.00013217]
	Learning Rate: 0.00013217
	LOSS [training: 0.13669357831766044 | validation: 0.10459085605693812]
	TIME [epoch: 8.18 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1794399615629613		[learning rate: 0.00013193]
		[batch 20/20] avg loss: 0.17323726501460116		[learning rate: 0.00013169]
	Learning Rate: 0.00013169
	LOSS [training: 0.17633861328878125 | validation: 0.13443668328696784]
	TIME [epoch: 8.19 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19623246735407524		[learning rate: 0.00013145]
		[batch 20/20] avg loss: 0.17421989015941658		[learning rate: 0.00013121]
	Learning Rate: 0.000131212
	LOSS [training: 0.1852261787567459 | validation: 0.12928934462654132]
	TIME [epoch: 8.18 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1614748691086089		[learning rate: 0.00013097]
		[batch 20/20] avg loss: 0.1393457253745381		[learning rate: 0.00013074]
	Learning Rate: 0.000130736
	LOSS [training: 0.1504102972415735 | validation: 0.10361093961615252]
	TIME [epoch: 8.2 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14303530267548908		[learning rate: 0.0001305]
		[batch 20/20] avg loss: 0.1438151448931128		[learning rate: 0.00013026]
	Learning Rate: 0.000130261
	LOSS [training: 0.14342522378430095 | validation: 0.11275169768429039]
	TIME [epoch: 8.19 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1513890131509682		[learning rate: 0.00013002]
		[batch 20/20] avg loss: 0.1421086184303265		[learning rate: 0.00012979]
	Learning Rate: 0.000129789
	LOSS [training: 0.14674881579064736 | validation: 0.11265350766286694]
	TIME [epoch: 8.18 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14229894355920372		[learning rate: 0.00012955]
		[batch 20/20] avg loss: 0.13013080685552963		[learning rate: 0.00012932]
	Learning Rate: 0.000129318
	LOSS [training: 0.13621487520736666 | validation: 0.12181291827740875]
	TIME [epoch: 8.19 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14051385452603649		[learning rate: 0.00012908]
		[batch 20/20] avg loss: 0.13742580286477624		[learning rate: 0.00012885]
	Learning Rate: 0.000128848
	LOSS [training: 0.13896982869540636 | validation: 0.09787873321902475]
	TIME [epoch: 8.2 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14823405020488534		[learning rate: 0.00012861]
		[batch 20/20] avg loss: 0.170441653826843		[learning rate: 0.00012838]
	Learning Rate: 0.000128381
	LOSS [training: 0.15933785201586415 | validation: 0.19153081539761624]
	TIME [epoch: 8.18 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18848433591573324		[learning rate: 0.00012815]
		[batch 20/20] avg loss: 0.15972456905449492		[learning rate: 0.00012791]
	Learning Rate: 0.000127915
	LOSS [training: 0.17410445248511408 | validation: 0.10858247899846438]
	TIME [epoch: 8.18 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17737842405422616		[learning rate: 0.00012768]
		[batch 20/20] avg loss: 0.15364000665500332		[learning rate: 0.00012745]
	Learning Rate: 0.000127451
	LOSS [training: 0.1655092153546147 | validation: 0.12063303045634632]
	TIME [epoch: 8.19 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13950905657212923		[learning rate: 0.00012722]
		[batch 20/20] avg loss: 0.1607445024352981		[learning rate: 0.00012699]
	Learning Rate: 0.000126988
	LOSS [training: 0.15012677950371361 | validation: 0.1054665130482971]
	TIME [epoch: 8.2 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14768583752965267		[learning rate: 0.00012676]
		[batch 20/20] avg loss: 0.1386646077190195		[learning rate: 0.00012653]
	Learning Rate: 0.000126527
	LOSS [training: 0.1431752226243361 | validation: 0.11176063653974214]
	TIME [epoch: 8.18 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14633116352231834		[learning rate: 0.0001263]
		[batch 20/20] avg loss: 0.14384757739010268		[learning rate: 0.00012607]
	Learning Rate: 0.000126068
	LOSS [training: 0.1450893704562105 | validation: 0.11645753058054589]
	TIME [epoch: 8.18 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15534976002874445		[learning rate: 0.00012584]
		[batch 20/20] avg loss: 0.13133869413727645		[learning rate: 0.00012561]
	Learning Rate: 0.000125611
	LOSS [training: 0.14334422708301045 | validation: 0.09778198724181864]
	TIME [epoch: 8.18 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1527685328443817		[learning rate: 0.00012538]
		[batch 20/20] avg loss: 0.15352784777237333		[learning rate: 0.00012515]
	Learning Rate: 0.000125155
	LOSS [training: 0.1531481903083775 | validation: 0.09472542368555684]
	TIME [epoch: 8.21 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13240820041265197		[learning rate: 0.00012493]
		[batch 20/20] avg loss: 0.14024728361086528		[learning rate: 0.0001247]
	Learning Rate: 0.000124701
	LOSS [training: 0.1363277420117586 | validation: 0.10024595293501147]
	TIME [epoch: 8.18 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15322393799906994		[learning rate: 0.00012447]
		[batch 20/20] avg loss: 0.16648258072510755		[learning rate: 0.00012425]
	Learning Rate: 0.000124248
	LOSS [training: 0.15985325936208875 | validation: 0.14896514475124373]
	TIME [epoch: 8.18 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1640423014401972		[learning rate: 0.00012402]
		[batch 20/20] avg loss: 0.1565919759730877		[learning rate: 0.0001238]
	Learning Rate: 0.000123797
	LOSS [training: 0.16031713870664246 | validation: 0.11092938993218114]
	TIME [epoch: 8.18 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1493568904648392		[learning rate: 0.00012357]
		[batch 20/20] avg loss: 0.14490807592422533		[learning rate: 0.00012335]
	Learning Rate: 0.000123348
	LOSS [training: 0.14713248319453226 | validation: 0.09262962339801904]
	TIME [epoch: 8.2 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14753455194189086		[learning rate: 0.00012312]
		[batch 20/20] avg loss: 0.1432192456625669		[learning rate: 0.0001229]
	Learning Rate: 0.0001229
	LOSS [training: 0.14537689880222887 | validation: 0.10887697762576262]
	TIME [epoch: 8.19 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19868326431750066		[learning rate: 0.00012268]
		[batch 20/20] avg loss: 0.35007481297199317		[learning rate: 0.00012245]
	Learning Rate: 0.000122454
	LOSS [training: 0.2743790386447469 | validation: 0.4169450328763317]
	TIME [epoch: 8.19 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36551622181211196		[learning rate: 0.00012223]
		[batch 20/20] avg loss: 0.7116360112489445		[learning rate: 0.00012201]
	Learning Rate: 0.00012201
	LOSS [training: 0.5385761165305283 | validation: 0.638479964707729]
	TIME [epoch: 8.18 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8124218163066266		[learning rate: 0.00012179]
		[batch 20/20] avg loss: 0.7638759220791924		[learning rate: 0.00012157]
	Learning Rate: 0.000121567
	LOSS [training: 0.7881488691929096 | validation: 0.730317770532628]
	TIME [epoch: 8.21 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6500632340347677		[learning rate: 0.00012135]
		[batch 20/20] avg loss: 0.797573394959237		[learning rate: 0.00012113]
	Learning Rate: 0.000121126
	LOSS [training: 0.7238183144970024 | validation: 0.655234862731964]
	TIME [epoch: 8.19 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5350821563224983		[learning rate: 0.00012091]
		[batch 20/20] avg loss: 0.4444435301362818		[learning rate: 0.00012069]
	Learning Rate: 0.000120686
	LOSS [training: 0.48976284322939023 | validation: 0.4662007138158624]
	TIME [epoch: 8.18 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3937445200048546		[learning rate: 0.00012047]
		[batch 20/20] avg loss: 0.3057494340837677		[learning rate: 0.00012025]
	Learning Rate: 0.000120248
	LOSS [training: 0.3497469770443112 | validation: 0.3241966060998439]
	TIME [epoch: 8.19 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24965504328292715		[learning rate: 0.00012003]
		[batch 20/20] avg loss: 0.18078207239522423		[learning rate: 0.00011981]
	Learning Rate: 0.000119812
	LOSS [training: 0.21521855783907573 | validation: 0.10972607011545829]
	TIME [epoch: 8.21 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16467033905677023		[learning rate: 0.00011959]
		[batch 20/20] avg loss: 0.2511027954033249		[learning rate: 0.00011938]
	Learning Rate: 0.000119377
	LOSS [training: 0.20788656723004756 | validation: 0.4097981695517238]
	TIME [epoch: 8.2 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.366645797738674		[learning rate: 0.00011916]
		[batch 20/20] avg loss: 0.20494614448661452		[learning rate: 0.00011894]
	Learning Rate: 0.000118944
	LOSS [training: 0.28579597111264426 | validation: 0.14856627742981005]
	TIME [epoch: 8.19 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21553736271869858		[learning rate: 0.00011873]
		[batch 20/20] avg loss: 0.22515699713482712		[learning rate: 0.00011851]
	Learning Rate: 0.000118512
	LOSS [training: 0.22034717992676284 | validation: 0.11754347872255341]
	TIME [epoch: 8.19 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13524552409225307		[learning rate: 0.0001183]
		[batch 20/20] avg loss: 0.2313018230790737		[learning rate: 0.00011808]
	Learning Rate: 0.000118082
	LOSS [training: 0.18327367358566343 | validation: 0.1548315286882469]
	TIME [epoch: 8.21 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22540620662848135		[learning rate: 0.00011787]
		[batch 20/20] avg loss: 0.16177434286367814		[learning rate: 0.00011765]
	Learning Rate: 0.000117654
	LOSS [training: 0.19359027474607973 | validation: 0.110022861306813]
	TIME [epoch: 8.19 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15651016942828258		[learning rate: 0.00011744]
		[batch 20/20] avg loss: 0.13676291365896678		[learning rate: 0.00011723]
	Learning Rate: 0.000117227
	LOSS [training: 0.14663654154362468 | validation: 0.10884839677508282]
	TIME [epoch: 8.19 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14057348147127216		[learning rate: 0.00011701]
		[batch 20/20] avg loss: 0.14970599792469894		[learning rate: 0.0001168]
	Learning Rate: 0.000116801
	LOSS [training: 0.1451397396979855 | validation: 0.12582529392159805]
	TIME [epoch: 8.19 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14896337733269577		[learning rate: 0.00011659]
		[batch 20/20] avg loss: 0.15526915359285481		[learning rate: 0.00011638]
	Learning Rate: 0.000116377
	LOSS [training: 0.1521162654627753 | validation: 0.10732708045521244]
	TIME [epoch: 8.2 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1468748754936376		[learning rate: 0.00011617]
		[batch 20/20] avg loss: 0.1705875131558296		[learning rate: 0.00011595]
	Learning Rate: 0.000115955
	LOSS [training: 0.15873119432473357 | validation: 0.11794070779784516]
	TIME [epoch: 8.19 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18108167604956543		[learning rate: 0.00011574]
		[batch 20/20] avg loss: 0.1568290454996917		[learning rate: 0.00011553]
	Learning Rate: 0.000115534
	LOSS [training: 0.16895536077462853 | validation: 0.10875791989391925]
	TIME [epoch: 8.18 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15847891624657634		[learning rate: 0.00011532]
		[batch 20/20] avg loss: 0.13145246641458347		[learning rate: 0.00011511]
	Learning Rate: 0.000115115
	LOSS [training: 0.14496569133057993 | validation: 0.11478956326508982]
	TIME [epoch: 8.19 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13689943283163292		[learning rate: 0.00011491]
		[batch 20/20] avg loss: 0.13799936291381296		[learning rate: 0.0001147]
	Learning Rate: 0.000114697
	LOSS [training: 0.13744939787272295 | validation: 0.09009251296226979]
	TIME [epoch: 8.21 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14389480271284744		[learning rate: 0.00011449]
		[batch 20/20] avg loss: 0.18106475941333805		[learning rate: 0.00011428]
	Learning Rate: 0.000114281
	LOSS [training: 0.16247978106309277 | validation: 0.11558769441735348]
	TIME [epoch: 8.19 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14078206350663197		[learning rate: 0.00011407]
		[batch 20/20] avg loss: 0.19955174619765598		[learning rate: 0.00011387]
	Learning Rate: 0.000113866
	LOSS [training: 0.17016690485214397 | validation: 0.15083805882305967]
	TIME [epoch: 8.19 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20441156116583326		[learning rate: 0.00011366]
		[batch 20/20] avg loss: 0.19700793464285069		[learning rate: 0.00011345]
	Learning Rate: 0.000113453
	LOSS [training: 0.20070974790434196 | validation: 0.12525049141747874]
	TIME [epoch: 8.19 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1773422671579768		[learning rate: 0.00011325]
		[batch 20/20] avg loss: 0.19282791226179402		[learning rate: 0.00011304]
	Learning Rate: 0.000113041
	LOSS [training: 0.18508508970988544 | validation: 0.12140210845094118]
	TIME [epoch: 8.2 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20030158557553657		[learning rate: 0.00011284]
		[batch 20/20] avg loss: 0.2126931338542085		[learning rate: 0.00011263]
	Learning Rate: 0.000112631
	LOSS [training: 0.20649735971487254 | validation: 0.12199331402479893]
	TIME [epoch: 8.19 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2043376492026673		[learning rate: 0.00011243]
		[batch 20/20] avg loss: 0.16319416912078832		[learning rate: 0.00011222]
	Learning Rate: 0.000112222
	LOSS [training: 0.1837659091617278 | validation: 0.14355135803696706]
	TIME [epoch: 8.18 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15577269309219255		[learning rate: 0.00011202]
		[batch 20/20] avg loss: 0.21321377499675057		[learning rate: 0.00011181]
	Learning Rate: 0.000111815
	LOSS [training: 0.18449323404447154 | validation: 0.14832971950529789]
	TIME [epoch: 8.18 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16278449080803148		[learning rate: 0.00011161]
		[batch 20/20] avg loss: 0.17720102173215085		[learning rate: 0.00011141]
	Learning Rate: 0.000111409
	LOSS [training: 0.16999275627009117 | validation: 0.1342719279998297]
	TIME [epoch: 8.2 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18147159344208855		[learning rate: 0.00011121]
		[batch 20/20] avg loss: 0.1469424174546529		[learning rate: 0.000111]
	Learning Rate: 0.000111005
	LOSS [training: 0.1642070054483707 | validation: 0.15044003447391294]
	TIME [epoch: 8.18 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20962825068804208		[learning rate: 0.0001108]
		[batch 20/20] avg loss: 0.1803535292165783		[learning rate: 0.0001106]
	Learning Rate: 0.000110602
	LOSS [training: 0.19499088995231018 | validation: 0.1869442368037655]
	TIME [epoch: 8.18 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18041470124582507		[learning rate: 0.0001104]
		[batch 20/20] avg loss: 0.15405860356769815		[learning rate: 0.0001102]
	Learning Rate: 0.000110201
	LOSS [training: 0.16723665240676164 | validation: 0.1146000388141157]
	TIME [epoch: 8.18 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1717779116598207		[learning rate: 0.00011]
		[batch 20/20] avg loss: 0.16917514143059686		[learning rate: 0.0001098]
	Learning Rate: 0.000109801
	LOSS [training: 0.17047652654520878 | validation: 0.12897102107015554]
	TIME [epoch: 8.21 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1483875343005348		[learning rate: 0.0001096]
		[batch 20/20] avg loss: 0.14632927121533795		[learning rate: 0.0001094]
	Learning Rate: 0.000109402
	LOSS [training: 0.14735840275793638 | validation: 0.09242915823692205]
	TIME [epoch: 8.19 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14025807024387513		[learning rate: 0.0001092]
		[batch 20/20] avg loss: 0.15740465343767457		[learning rate: 0.00010901]
	Learning Rate: 0.000109005
	LOSS [training: 0.1488313618407749 | validation: 0.12229493388349177]
	TIME [epoch: 8.19 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14882901819358788		[learning rate: 0.00010881]
		[batch 20/20] avg loss: 0.12551721398665866		[learning rate: 0.00010861]
	Learning Rate: 0.00010861
	LOSS [training: 0.13717311609012323 | validation: 0.11474607400581223]
	TIME [epoch: 8.18 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13957774473075046		[learning rate: 0.00010841]
		[batch 20/20] avg loss: 0.16528042368125856		[learning rate: 0.00010822]
	Learning Rate: 0.000108215
	LOSS [training: 0.15242908420600454 | validation: 0.1179739503934277]
	TIME [epoch: 8.2 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14389655981877722		[learning rate: 0.00010802]
		[batch 20/20] avg loss: 0.14986158105870712		[learning rate: 0.00010782]
	Learning Rate: 0.000107823
	LOSS [training: 0.14687907043874213 | validation: 0.10873848671020894]
	TIME [epoch: 8.18 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1531662056752276		[learning rate: 0.00010763]
		[batch 20/20] avg loss: 0.1301513316428291		[learning rate: 0.00010743]
	Learning Rate: 0.000107432
	LOSS [training: 0.14165876865902832 | validation: 0.10849404949578642]
	TIME [epoch: 8.18 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14133128725191282		[learning rate: 0.00010724]
		[batch 20/20] avg loss: 0.13357965512339118		[learning rate: 0.00010704]
	Learning Rate: 0.000107042
	LOSS [training: 0.137455471187652 | validation: 0.07811836701573698]
	TIME [epoch: 8.18 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1199960430877983		[learning rate: 0.00010685]
		[batch 20/20] avg loss: 0.1533071687269696		[learning rate: 0.00010665]
	Learning Rate: 0.000106653
	LOSS [training: 0.13665160590738396 | validation: 0.10628087184994717]
	TIME [epoch: 8.21 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12985906522353535		[learning rate: 0.00010646]
		[batch 20/20] avg loss: 0.13282310899939223		[learning rate: 0.00010627]
	Learning Rate: 0.000106266
	LOSS [training: 0.13134108711146378 | validation: 0.0922444250015501]
	TIME [epoch: 8.18 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13371001257566137		[learning rate: 0.00010607]
		[batch 20/20] avg loss: 0.1231968663600995		[learning rate: 0.00010588]
	Learning Rate: 0.00010588
	LOSS [training: 0.12845343946788046 | validation: 0.1048787807083731]
	TIME [epoch: 8.18 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1414517136648649		[learning rate: 0.00010569]
		[batch 20/20] avg loss: 0.13218841614940682		[learning rate: 0.0001055]
	Learning Rate: 0.000105496
	LOSS [training: 0.13682006490713586 | validation: 0.09131886844950395]
	TIME [epoch: 8.19 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1287693173951663		[learning rate: 0.0001053]
		[batch 20/20] avg loss: 0.13675062025814078		[learning rate: 0.00010511]
	Learning Rate: 0.000105113
	LOSS [training: 0.13275996882665356 | validation: 0.09646873868967802]
	TIME [epoch: 8.21 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13221789039604767		[learning rate: 0.00010492]
		[batch 20/20] avg loss: 0.12318348010010398		[learning rate: 0.00010473]
	Learning Rate: 0.000104732
	LOSS [training: 0.12770068524807585 | validation: 0.09727259275382921]
	TIME [epoch: 8.19 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11067921310126083		[learning rate: 0.00010454]
		[batch 20/20] avg loss: 0.13499937471980644		[learning rate: 0.00010435]
	Learning Rate: 0.000104352
	LOSS [training: 0.12283929391053361 | validation: 0.07875611410776853]
	TIME [epoch: 8.18 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13917686014778652		[learning rate: 0.00010416]
		[batch 20/20] avg loss: 0.1613641427678611		[learning rate: 0.00010397]
	Learning Rate: 0.000103973
	LOSS [training: 0.1502705014578238 | validation: 0.09291383477059459]
	TIME [epoch: 8.18 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12242313728809526		[learning rate: 0.00010378]
		[batch 20/20] avg loss: 0.11504476015810346		[learning rate: 0.0001036]
	Learning Rate: 0.000103596
	LOSS [training: 0.11873394872309935 | validation: 0.08046450249971768]
	TIME [epoch: 8.21 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12117932910819174		[learning rate: 0.00010341]
		[batch 20/20] avg loss: 0.114050334671747		[learning rate: 0.00010322]
	Learning Rate: 0.00010322
	LOSS [training: 0.11761483188996938 | validation: 0.07289348608028155]
	TIME [epoch: 8.19 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1271312951072257		[learning rate: 0.00010303]
		[batch 20/20] avg loss: 0.11551197225277861		[learning rate: 0.00010285]
	Learning Rate: 0.000102845
	LOSS [training: 0.12132163368000215 | validation: 0.06948992500515475]
	TIME [epoch: 8.18 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11806812630416344		[learning rate: 0.00010266]
		[batch 20/20] avg loss: 0.1319805990843322		[learning rate: 0.00010247]
	Learning Rate: 0.000102472
	LOSS [training: 0.1250243626942478 | validation: 0.08125461900164599]
	TIME [epoch: 8.18 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11749647115487698		[learning rate: 0.00010229]
		[batch 20/20] avg loss: 0.13190899881909018		[learning rate: 0.0001021]
	Learning Rate: 0.0001021
	LOSS [training: 0.12470273498698362 | validation: 0.08602011634935484]
	TIME [epoch: 8.2 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13456467706275504		[learning rate: 0.00010191]
		[batch 20/20] avg loss: 0.11640489889657162		[learning rate: 0.00010173]
	Learning Rate: 0.00010173
	LOSS [training: 0.12548478797966334 | validation: 0.08904656178773489]
	TIME [epoch: 8.18 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10206223052490629		[learning rate: 0.00010154]
		[batch 20/20] avg loss: 0.127821333826275		[learning rate: 0.00010136]
	Learning Rate: 0.00010136
	LOSS [training: 0.11494178217559066 | validation: 0.08341541601447346]
	TIME [epoch: 8.18 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11110971028398561		[learning rate: 0.00010118]
		[batch 20/20] avg loss: 0.12068410389461706		[learning rate: 0.00010099]
	Learning Rate: 0.000100993
	LOSS [training: 0.11589690708930134 | validation: 0.0822351766878257]
	TIME [epoch: 8.19 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1346271098213408		[learning rate: 0.00010081]
		[batch 20/20] avg loss: 0.11198315204212513		[learning rate: 0.00010063]
	Learning Rate: 0.000100626
	LOSS [training: 0.12330513093173297 | validation: 0.07674149760074254]
	TIME [epoch: 8.19 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12579251544171913		[learning rate: 0.00010044]
		[batch 20/20] avg loss: 0.13508031397007877		[learning rate: 0.00010026]
	Learning Rate: 0.000100261
	LOSS [training: 0.13043641470589898 | validation: 0.0751976777062281]
	TIME [epoch: 8.19 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11085530544831299		[learning rate: 0.00010008]
		[batch 20/20] avg loss: 0.11567553044135614		[learning rate: 9.9897e-05]
	Learning Rate: 9.98971e-05
	LOSS [training: 0.11326541794483458 | validation: 0.0770091855476279]
	TIME [epoch: 8.18 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13513836098810816		[learning rate: 9.9716e-05]
		[batch 20/20] avg loss: 0.1337479100908814		[learning rate: 9.9535e-05]
	Learning Rate: 9.95345e-05
	LOSS [training: 0.1344431355394948 | validation: 0.06820473375783215]
	TIME [epoch: 8.18 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10505145374265087		[learning rate: 9.9354e-05]
		[batch 20/20] avg loss: 0.1113771202804715		[learning rate: 9.9173e-05]
	Learning Rate: 9.91733e-05
	LOSS [training: 0.10821428701156117 | validation: 0.0641752239973077]
	TIME [epoch: 8.2 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11886174122369102		[learning rate: 9.8993e-05]
		[batch 20/20] avg loss: 0.120688687993285		[learning rate: 9.8813e-05]
	Learning Rate: 9.88134e-05
	LOSS [training: 0.11977521460848801 | validation: 0.09612142679112945]
	TIME [epoch: 8.19 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1309825997726764		[learning rate: 9.8634e-05]
		[batch 20/20] avg loss: 0.10825630412708828		[learning rate: 9.8455e-05]
	Learning Rate: 9.84548e-05
	LOSS [training: 0.11961945194988235 | validation: 0.07647292168570569]
	TIME [epoch: 8.19 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10317176617368143		[learning rate: 9.8276e-05]
		[batch 20/20] avg loss: 0.10029038537990567		[learning rate: 9.8098e-05]
	Learning Rate: 9.80975e-05
	LOSS [training: 0.10173107577679355 | validation: 0.08389696628306899]
	TIME [epoch: 8.18 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10470531696797943		[learning rate: 9.7919e-05]
		[batch 20/20] avg loss: 0.12489462241739169		[learning rate: 9.7742e-05]
	Learning Rate: 9.77415e-05
	LOSS [training: 0.11479996969268558 | validation: 0.09007638289069243]
	TIME [epoch: 8.18 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16881290451633238		[learning rate: 9.7564e-05]
		[batch 20/20] avg loss: 0.13482917525883883		[learning rate: 9.7387e-05]
	Learning Rate: 9.73868e-05
	LOSS [training: 0.1518210398875856 | validation: 0.07469980647479865]
	TIME [epoch: 8.21 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11047699643304305		[learning rate: 9.721e-05]
		[batch 20/20] avg loss: 0.11728412636382465		[learning rate: 9.7033e-05]
	Learning Rate: 9.70334e-05
	LOSS [training: 0.11388056139843386 | validation: 0.07882072062737516]
	TIME [epoch: 8.18 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1024922344257555		[learning rate: 9.6857e-05]
		[batch 20/20] avg loss: 0.13226048014226785		[learning rate: 9.6681e-05]
	Learning Rate: 9.66812e-05
	LOSS [training: 0.11737635728401168 | validation: 0.08693742276938103]
	TIME [epoch: 8.18 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11667069459737836		[learning rate: 9.6506e-05]
		[batch 20/20] avg loss: 0.11118882085515817		[learning rate: 9.633e-05]
	Learning Rate: 9.63304e-05
	LOSS [training: 0.1139297577262683 | validation: 0.08695120907965612]
	TIME [epoch: 8.18 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1225882689156856		[learning rate: 9.6155e-05]
		[batch 20/20] avg loss: 0.11505859075487432		[learning rate: 9.5981e-05]
	Learning Rate: 9.59808e-05
	LOSS [training: 0.11882342983527996 | validation: 0.08260263410745722]
	TIME [epoch: 8.21 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09388093338747798		[learning rate: 9.5806e-05]
		[batch 20/20] avg loss: 0.11117535326500101		[learning rate: 9.5632e-05]
	Learning Rate: 9.56324e-05
	LOSS [training: 0.1025281433262395 | validation: 0.07952221884101]
	TIME [epoch: 8.18 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10492043471439791		[learning rate: 9.5459e-05]
		[batch 20/20] avg loss: 0.11022239855709295		[learning rate: 9.5285e-05]
	Learning Rate: 9.52854e-05
	LOSS [training: 0.10757141663574543 | validation: 0.07725182608041133]
	TIME [epoch: 8.19 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10172292517274188		[learning rate: 9.5112e-05]
		[batch 20/20] avg loss: 0.11482974316090826		[learning rate: 9.494e-05]
	Learning Rate: 9.49396e-05
	LOSS [training: 0.1082763341668251 | validation: 0.09116941512842601]
	TIME [epoch: 8.18 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1339029169048181		[learning rate: 9.4767e-05]
		[batch 20/20] avg loss: 0.17263085912573897		[learning rate: 9.4595e-05]
	Learning Rate: 9.45951e-05
	LOSS [training: 0.15326688801527855 | validation: 0.20830791113265584]
	TIME [epoch: 8.21 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19267237412759242		[learning rate: 9.4423e-05]
		[batch 20/20] avg loss: 0.13658137436996637		[learning rate: 9.4252e-05]
	Learning Rate: 9.42518e-05
	LOSS [training: 0.16462687424877936 | validation: 0.11956293093170442]
	TIME [epoch: 8.18 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1486040949250756		[learning rate: 9.4081e-05]
		[batch 20/20] avg loss: 0.11157176951988376		[learning rate: 9.391e-05]
	Learning Rate: 9.39097e-05
	LOSS [training: 0.13008793222247966 | validation: 0.12277721169954636]
	TIME [epoch: 8.19 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15566213560818096		[learning rate: 9.3739e-05]
		[batch 20/20] avg loss: 0.12551281230121986		[learning rate: 9.3569e-05]
	Learning Rate: 9.35689e-05
	LOSS [training: 0.1405874739547004 | validation: 0.14612380699083416]
	TIME [epoch: 8.19 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23773967849243008		[learning rate: 9.3399e-05]
		[batch 20/20] avg loss: 0.1855311653051733		[learning rate: 9.3229e-05]
	Learning Rate: 9.32294e-05
	LOSS [training: 0.2116354218988017 | validation: 0.11524292160773517]
	TIME [epoch: 8.21 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12052792981907738		[learning rate: 9.306e-05]
		[batch 20/20] avg loss: 0.1292844019337465		[learning rate: 9.2891e-05]
	Learning Rate: 9.2891e-05
	LOSS [training: 0.12490616587641194 | validation: 0.08388843210859448]
	TIME [epoch: 8.19 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11871192820986913		[learning rate: 9.2722e-05]
		[batch 20/20] avg loss: 0.11181689217742312		[learning rate: 9.2554e-05]
	Learning Rate: 9.25539e-05
	LOSS [training: 0.11526441019364612 | validation: 0.10232187909667995]
	TIME [epoch: 8.18 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13932808993178292		[learning rate: 9.2386e-05]
		[batch 20/20] avg loss: 0.13776273936009886		[learning rate: 9.2218e-05]
	Learning Rate: 9.2218e-05
	LOSS [training: 0.13854541464594092 | validation: 0.09627107831676582]
	TIME [epoch: 8.18 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12327132335391752		[learning rate: 9.2051e-05]
		[batch 20/20] avg loss: 0.1573912495273851		[learning rate: 9.1883e-05]
	Learning Rate: 9.18834e-05
	LOSS [training: 0.14033128644065135 | validation: 0.15057954428238693]
	TIME [epoch: 8.21 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27716802408216756		[learning rate: 9.1716e-05]
		[batch 20/20] avg loss: 0.3808158561946856		[learning rate: 9.155e-05]
	Learning Rate: 9.15499e-05
	LOSS [training: 0.3289919401384266 | validation: 0.31424519355748065]
	TIME [epoch: 8.18 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20934573207146828		[learning rate: 9.1384e-05]
		[batch 20/20] avg loss: 0.2094564098602391		[learning rate: 9.1218e-05]
	Learning Rate: 9.12177e-05
	LOSS [training: 0.20940107096585367 | validation: 0.14182436346944313]
	TIME [epoch: 8.18 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1226517882112695		[learning rate: 9.1052e-05]
		[batch 20/20] avg loss: 0.12693844859468545		[learning rate: 9.0887e-05]
	Learning Rate: 9.08866e-05
	LOSS [training: 0.12479511840297748 | validation: 0.11461909553427335]
	TIME [epoch: 8.21 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1247346455808239		[learning rate: 9.0722e-05]
		[batch 20/20] avg loss: 0.12265560339468588		[learning rate: 9.0557e-05]
	Learning Rate: 9.05568e-05
	LOSS [training: 0.12369512448775488 | validation: 0.0886756891311404]
	TIME [epoch: 8.2 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14517806812274467		[learning rate: 9.0392e-05]
		[batch 20/20] avg loss: 0.1222574489681918		[learning rate: 9.0228e-05]
	Learning Rate: 9.02281e-05
	LOSS [training: 0.13371775854546822 | validation: 0.08932489692192301]
	TIME [epoch: 8.19 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11430353697173201		[learning rate: 9.0064e-05]
		[batch 20/20] avg loss: 0.1391039187580589		[learning rate: 8.9901e-05]
	Learning Rate: 8.99007e-05
	LOSS [training: 0.1267037278648955 | validation: 0.2663127995857891]
	TIME [epoch: 8.18 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34426950660913624		[learning rate: 8.9737e-05]
		[batch 20/20] avg loss: 0.5749085951114747		[learning rate: 8.9574e-05]
	Learning Rate: 8.95745e-05
	LOSS [training: 0.45958905086030555 | validation: 0.8266917815617577]
	TIME [epoch: 8.18 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9303795343012693		[learning rate: 8.9412e-05]
		[batch 20/20] avg loss: 1.0088287685152297		[learning rate: 8.9249e-05]
	Learning Rate: 8.92494e-05
	LOSS [training: 0.9696041514082495 | validation: 1.3486040453960464]
	TIME [epoch: 8.2 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4407700532779237		[learning rate: 8.9087e-05]
		[batch 20/20] avg loss: 2.5048718675268393		[learning rate: 8.8926e-05]
	Learning Rate: 8.89255e-05
	LOSS [training: 1.9728209604023814 | validation: 3.010615381211502]
	TIME [epoch: 8.19 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3369752167718687		[learning rate: 8.8764e-05]
		[batch 20/20] avg loss: 3.2400028967706844		[learning rate: 8.8603e-05]
	Learning Rate: 8.86028e-05
	LOSS [training: 3.288489056771277 | validation: 3.7692248094916097]
	TIME [epoch: 8.18 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.092905536754292		[learning rate: 8.8442e-05]
		[batch 20/20] avg loss: 4.214324418473709		[learning rate: 8.8281e-05]
	Learning Rate: 8.82813e-05
	LOSS [training: 4.153614977614 | validation: 4.0314441357116655]
	TIME [epoch: 8.18 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.420602597413693		[learning rate: 8.8121e-05]
		[batch 20/20] avg loss: 2.7103834288764683		[learning rate: 8.7961e-05]
	Learning Rate: 8.79609e-05
	LOSS [training: 3.06549301314508 | validation: 3.32288293645573]
	TIME [epoch: 8.2 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1922377555494856		[learning rate: 8.7801e-05]
		[batch 20/20] avg loss: 3.2501644834268526		[learning rate: 8.7642e-05]
	Learning Rate: 8.76417e-05
	LOSS [training: 3.221201119488169 | validation: 3.6432024420579086]
	TIME [epoch: 8.18 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.5263913474617454		[learning rate: 8.7482e-05]
		[batch 20/20] avg loss: 3.147772980965836		[learning rate: 8.7324e-05]
	Learning Rate: 8.73236e-05
	LOSS [training: 3.3370821642137907 | validation: 3.488168939371637]
	TIME [epoch: 8.19 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6882988411652375		[learning rate: 8.7165e-05]
		[batch 20/20] avg loss: 2.5310034903155945		[learning rate: 8.7007e-05]
	Learning Rate: 8.70067e-05
	LOSS [training: 3.1096511657404156 | validation: 2.8680571195100937]
	TIME [epoch: 8.18 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.53724575947909		[learning rate: 8.6849e-05]
		[batch 20/20] avg loss: 2.99008543059116		[learning rate: 8.6691e-05]
	Learning Rate: 8.66909e-05
	LOSS [training: 2.7636655950351248 | validation: 3.4514522956497333]
	TIME [epoch: 8.2 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0116732868098275		[learning rate: 8.6533e-05]
		[batch 20/20] avg loss: 1.7534659902864842		[learning rate: 8.6376e-05]
	Learning Rate: 8.63763e-05
	LOSS [training: 2.382569638548156 | validation: 1.6531417230128194]
	TIME [epoch: 8.19 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9027082619590197		[learning rate: 8.6219e-05]
		[batch 20/20] avg loss: 2.281523805087354		[learning rate: 8.6063e-05]
	Learning Rate: 8.60629e-05
	LOSS [training: 2.0921160335231868 | validation: 2.3890356269433632]
	TIME [epoch: 8.18 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.343511858331034		[learning rate: 8.5907e-05]
		[batch 20/20] avg loss: 2.4121318477128897		[learning rate: 8.5751e-05]
	Learning Rate: 8.57505e-05
	LOSS [training: 2.3778218530219624 | validation: 2.6611117253723076]
	TIME [epoch: 8.18 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.522667518415002		[learning rate: 8.5595e-05]
		[batch 20/20] avg loss: 2.2497883805268604		[learning rate: 8.5439e-05]
	Learning Rate: 8.54394e-05
	LOSS [training: 2.386227949470931 | validation: 2.4812621287403305]
	TIME [epoch: 8.2 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2121810334063556		[learning rate: 8.5284e-05]
		[batch 20/20] avg loss: 2.1846180747718345		[learning rate: 8.5129e-05]
	Learning Rate: 8.51293e-05
	LOSS [training: 2.198399554089095 | validation: 2.258431834983422]
	TIME [epoch: 8.19 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9140907282127404		[learning rate: 8.4975e-05]
		[batch 20/20] avg loss: 1.494815357509204		[learning rate: 8.482e-05]
	Learning Rate: 8.48204e-05
	LOSS [training: 1.704453042860972 | validation: 1.7250189518810877]
	TIME [epoch: 8.18 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6980156638773387		[learning rate: 8.4666e-05]
		[batch 20/20] avg loss: 1.8348097337336526		[learning rate: 8.4513e-05]
	Learning Rate: 8.45125e-05
	LOSS [training: 1.7664126988054953 | validation: 1.6828191047695868]
	TIME [epoch: 8.18 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8418276047156497		[learning rate: 8.4359e-05]
		[batch 20/20] avg loss: 2.732674791683662		[learning rate: 8.4206e-05]
	Learning Rate: 8.42058e-05
	LOSS [training: 2.287251198199656 | validation: 3.389936897790516]
	TIME [epoch: 8.21 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.2818635152255133		[learning rate: 8.4053e-05]
		[batch 20/20] avg loss: 3.3889461922083344		[learning rate: 8.39e-05]
	Learning Rate: 8.39002e-05
	LOSS [training: 3.3354048537169234 | validation: 3.4673026782693572]
	TIME [epoch: 8.18 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.269715113960152		[learning rate: 8.3748e-05]
		[batch 20/20] avg loss: 2.9292744653344163		[learning rate: 8.3596e-05]
	Learning Rate: 8.35958e-05
	LOSS [training: 3.0994947896472835 | validation: 3.1744200513998906]
	TIME [epoch: 8.18 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.926323260625408		[learning rate: 8.3444e-05]
		[batch 20/20] avg loss: 2.2233428756156624		[learning rate: 8.3292e-05]
	Learning Rate: 8.32924e-05
	LOSS [training: 2.5748330681205345 | validation: 2.155816620618487]
	TIME [epoch: 8.18 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8583102939772314		[learning rate: 8.3141e-05]
		[batch 20/20] avg loss: 1.667676465259854		[learning rate: 8.299e-05]
	Learning Rate: 8.29901e-05
	LOSS [training: 1.7629933796185426 | validation: 1.7880135349515467]
	TIME [epoch: 8.2 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.3582036445515042		[learning rate: 8.2839e-05]
		[batch 20/20] avg loss: 1.2872332192147127		[learning rate: 8.2689e-05]
	Learning Rate: 8.26889e-05
	LOSS [training: 1.3227184318831084 | validation: 1.8814312579473067]
	TIME [epoch: 8.18 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4961541268287106		[learning rate: 8.2539e-05]
		[batch 20/20] avg loss: 1.1958781788536765		[learning rate: 8.2389e-05]
	Learning Rate: 8.23889e-05
	LOSS [training: 1.3460161528411936 | validation: 1.059067661219678]
	TIME [epoch: 8.18 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9409492026304596		[learning rate: 8.2239e-05]
		[batch 20/20] avg loss: 0.8568022970712977		[learning rate: 8.209e-05]
	Learning Rate: 8.20899e-05
	LOSS [training: 0.8988757498508784 | validation: 0.8185676657352042]
	TIME [epoch: 8.18 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.568519968239234		[learning rate: 8.1941e-05]
		[batch 20/20] avg loss: 0.4572598711553965		[learning rate: 8.1792e-05]
	Learning Rate: 8.17919e-05
	LOSS [training: 0.5128899196973153 | validation: 0.5457435846221679]
	TIME [epoch: 8.2 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5144669513818594		[learning rate: 8.1643e-05]
		[batch 20/20] avg loss: 0.5940068167596011		[learning rate: 8.1495e-05]
	Learning Rate: 8.14951e-05
	LOSS [training: 0.5542368840707304 | validation: 0.46878921784949035]
	TIME [epoch: 8.18 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29784583122684827		[learning rate: 8.1347e-05]
		[batch 20/20] avg loss: 0.2047355543382387		[learning rate: 8.1199e-05]
	Learning Rate: 8.11994e-05
	LOSS [training: 0.25129069278254346 | validation: 0.30134491303772526]
	TIME [epoch: 8.18 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21158113073581913		[learning rate: 8.1052e-05]
		[batch 20/20] avg loss: 0.18746727645572708		[learning rate: 8.0905e-05]
	Learning Rate: 8.09047e-05
	LOSS [training: 0.1995242035957731 | validation: 0.2709062819877932]
	TIME [epoch: 8.18 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28458237243585993		[learning rate: 8.0758e-05]
		[batch 20/20] avg loss: 0.3947068030321537		[learning rate: 8.0611e-05]
	Learning Rate: 8.06111e-05
	LOSS [training: 0.33964458773400674 | validation: 0.6255359667770053]
	TIME [epoch: 8.21 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5711916820227856		[learning rate: 8.0465e-05]
		[batch 20/20] avg loss: 0.6721716092680035		[learning rate: 8.0319e-05]
	Learning Rate: 8.03186e-05
	LOSS [training: 0.6216816456453944 | validation: 0.8050072600092449]
	TIME [epoch: 8.18 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.564439555954042		[learning rate: 8.0173e-05]
		[batch 20/20] avg loss: 0.35379109119721525		[learning rate: 8.0027e-05]
	Learning Rate: 8.00271e-05
	LOSS [training: 0.4591153235756287 | validation: 0.35840756793786827]
	TIME [epoch: 8.18 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24169476676127788		[learning rate: 7.9882e-05]
		[batch 20/20] avg loss: 0.1713062221326019		[learning rate: 7.9737e-05]
	Learning Rate: 7.97367e-05
	LOSS [training: 0.2065004944469399 | validation: 0.22702527409945772]
	TIME [epoch: 8.19 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18355741391681102		[learning rate: 7.9592e-05]
		[batch 20/20] avg loss: 0.21797548334054162		[learning rate: 7.9447e-05]
	Learning Rate: 7.94473e-05
	LOSS [training: 0.2007664486286763 | validation: 0.23454939372973557]
	TIME [epoch: 8.2 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.15831224291102716		[learning rate: 7.9303e-05]
		[batch 20/20] avg loss: 0.14665602719666757		[learning rate: 7.9159e-05]
	Learning Rate: 7.91589e-05
	LOSS [training: 0.15248413505384734 | validation: 0.11172692356260744]
	TIME [epoch: 8.18 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14846075365909406		[learning rate: 7.9015e-05]
		[batch 20/20] avg loss: 0.13866597821887952		[learning rate: 7.8872e-05]
	Learning Rate: 7.88717e-05
	LOSS [training: 0.14356336593898683 | validation: 0.1471270417718947]
	TIME [epoch: 8.18 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13180667277401314		[learning rate: 7.8728e-05]
		[batch 20/20] avg loss: 0.1441143720793463		[learning rate: 7.8585e-05]
	Learning Rate: 7.85854e-05
	LOSS [training: 0.13796052242667975 | validation: 0.1985077630997676]
	TIME [epoch: 8.19 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.18150285783024941		[learning rate: 7.8443e-05]
		[batch 20/20] avg loss: 0.2580710830371094		[learning rate: 7.83e-05]
	Learning Rate: 7.83003e-05
	LOSS [training: 0.2197869704336795 | validation: 0.360809435475336]
	TIME [epoch: 8.2 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4017468614197548		[learning rate: 7.8158e-05]
		[batch 20/20] avg loss: 0.4764695217499086		[learning rate: 7.8016e-05]
	Learning Rate: 7.80161e-05
	LOSS [training: 0.4391081915848317 | validation: 0.48429091212549624]
	TIME [epoch: 8.18 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26997021930915527		[learning rate: 7.7874e-05]
		[batch 20/20] avg loss: 0.17865666378662534		[learning rate: 7.7733e-05]
	Learning Rate: 7.7733e-05
	LOSS [training: 0.22431344154789032 | validation: 0.2009119395603685]
	TIME [epoch: 8.18 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17761798591389585		[learning rate: 7.7592e-05]
		[batch 20/20] avg loss: 0.1426236272504939		[learning rate: 7.7451e-05]
	Learning Rate: 7.74509e-05
	LOSS [training: 0.1601208065821949 | validation: 0.14433610120326495]
	TIME [epoch: 8.18 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14877984039624356		[learning rate: 7.731e-05]
		[batch 20/20] avg loss: 0.15339703523057793		[learning rate: 7.717e-05]
	Learning Rate: 7.71698e-05
	LOSS [training: 0.15108843781341075 | validation: 0.22978255018807758]
	TIME [epoch: 8.2 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1993306040872361		[learning rate: 7.703e-05]
		[batch 20/20] avg loss: 0.1648136837538745		[learning rate: 7.689e-05]
	Learning Rate: 7.68897e-05
	LOSS [training: 0.18207214392055532 | validation: 0.19373170566281278]
	TIME [epoch: 8.18 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1539523921553163		[learning rate: 7.675e-05]
		[batch 20/20] avg loss: 0.15949307686328176		[learning rate: 7.6611e-05]
	Learning Rate: 7.66107e-05
	LOSS [training: 0.156722734509299 | validation: 0.26550283877038333]
	TIME [epoch: 8.18 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26744013147757595		[learning rate: 7.6472e-05]
		[batch 20/20] avg loss: 0.2757812640869669		[learning rate: 7.6333e-05]
	Learning Rate: 7.63327e-05
	LOSS [training: 0.27161069778227137 | validation: 0.26573509249202226]
	TIME [epoch: 8.18 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29467912136987656		[learning rate: 7.6194e-05]
		[batch 20/20] avg loss: 0.3949629604213616		[learning rate: 7.6056e-05]
	Learning Rate: 7.60557e-05
	LOSS [training: 0.34482104089561905 | validation: 0.41020800356533627]
	TIME [epoch: 8.2 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2567154214076325		[learning rate: 7.5918e-05]
		[batch 20/20] avg loss: 0.17834700373181891		[learning rate: 7.578e-05]
	Learning Rate: 7.57797e-05
	LOSS [training: 0.21753121256972568 | validation: 0.14790539793432272]
	TIME [epoch: 8.19 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1648346980604642		[learning rate: 7.5642e-05]
		[batch 20/20] avg loss: 0.15964086119642573		[learning rate: 7.5505e-05]
	Learning Rate: 7.55047e-05
	LOSS [training: 0.16223777962844493 | validation: 0.13445934777023835]
	TIME [epoch: 8.18 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.16156018873794642		[learning rate: 7.5368e-05]
		[batch 20/20] avg loss: 0.181596226367814		[learning rate: 7.5231e-05]
	Learning Rate: 7.52307e-05
	LOSS [training: 0.1715782075528802 | validation: 0.13779042391352309]
	TIME [epoch: 8.18 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14322813735877377		[learning rate: 7.5094e-05]
		[batch 20/20] avg loss: 0.1483578522070927		[learning rate: 7.4958e-05]
	Learning Rate: 7.49576e-05
	LOSS [training: 0.14579299478293326 | validation: 0.11746660019957267]
	TIME [epoch: 8.21 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1336040550716527		[learning rate: 7.4822e-05]
		[batch 20/20] avg loss: 0.12798628977167484		[learning rate: 7.4686e-05]
	Learning Rate: 7.46856e-05
	LOSS [training: 0.13079517242166377 | validation: 0.09855457925897934]
	TIME [epoch: 8.18 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14415940603668492		[learning rate: 7.455e-05]
		[batch 20/20] avg loss: 0.12811420929800035		[learning rate: 7.4415e-05]
	Learning Rate: 7.44146e-05
	LOSS [training: 0.13613680766734265 | validation: 0.09535685540857058]
	TIME [epoch: 8.18 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13077632413657286		[learning rate: 7.4279e-05]
		[batch 20/20] avg loss: 0.13937921782605844		[learning rate: 7.4144e-05]
	Learning Rate: 7.41445e-05
	LOSS [training: 0.13507777098131568 | validation: 0.10348079438161756]
	TIME [epoch: 8.18 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14021222803315428		[learning rate: 7.401e-05]
		[batch 20/20] avg loss: 0.15250669363390196		[learning rate: 7.3875e-05]
	Learning Rate: 7.38754e-05
	LOSS [training: 0.1463594608335281 | validation: 0.08422632667093426]
	TIME [epoch: 8.2 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11590678987532417		[learning rate: 7.3741e-05]
		[batch 20/20] avg loss: 0.10427252614695252		[learning rate: 7.3607e-05]
	Learning Rate: 7.36073e-05
	LOSS [training: 0.11008965801113835 | validation: 0.09766501658215752]
	TIME [epoch: 8.18 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11327348105230284		[learning rate: 7.3474e-05]
		[batch 20/20] avg loss: 0.10490445041668166		[learning rate: 7.334e-05]
	Learning Rate: 7.33402e-05
	LOSS [training: 0.10908896573449225 | validation: 0.08182278233759925]
	TIME [epoch: 8.18 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1077020731975769		[learning rate: 7.3207e-05]
		[batch 20/20] avg loss: 0.11242695798089657		[learning rate: 7.3074e-05]
	Learning Rate: 7.30741e-05
	LOSS [training: 0.11006451558923676 | validation: 0.09298067849714192]
	TIME [epoch: 8.18 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12379125459371668		[learning rate: 7.2941e-05]
		[batch 20/20] avg loss: 0.15885722956734075		[learning rate: 7.2809e-05]
	Learning Rate: 7.28089e-05
	LOSS [training: 0.14132424208052874 | validation: 0.10596855304335723]
	TIME [epoch: 8.19 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11226947037047999		[learning rate: 7.2677e-05]
		[batch 20/20] avg loss: 0.14104492031109125		[learning rate: 7.2545e-05]
	Learning Rate: 7.25447e-05
	LOSS [training: 0.12665719534078562 | validation: 0.18097215660859844]
	TIME [epoch: 8.19 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12884260594398506		[learning rate: 7.2413e-05]
		[batch 20/20] avg loss: 0.0972021833542372		[learning rate: 7.2281e-05]
	Learning Rate: 7.22814e-05
	LOSS [training: 0.11302239464911112 | validation: 0.08472924914541233]
	TIME [epoch: 8.17 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10254614466573113		[learning rate: 7.215e-05]
		[batch 20/20] avg loss: 0.09623694176091488		[learning rate: 7.2019e-05]
	Learning Rate: 7.2019e-05
	LOSS [training: 0.09939154321332302 | validation: 0.08499244657867204]
	TIME [epoch: 8.18 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1168711352748247		[learning rate: 7.1888e-05]
		[batch 20/20] avg loss: 0.09957654865573956		[learning rate: 7.1758e-05]
	Learning Rate: 7.17577e-05
	LOSS [training: 0.10822384196528212 | validation: 0.09696325080135862]
	TIME [epoch: 8.19 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1419879797417808		[learning rate: 7.1627e-05]
		[batch 20/20] avg loss: 0.1213224426541811		[learning rate: 7.1497e-05]
	Learning Rate: 7.14973e-05
	LOSS [training: 0.1316552111979809 | validation: 0.07507275274331596]
	TIME [epoch: 8.19 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09313911129730805		[learning rate: 7.1367e-05]
		[batch 20/20] avg loss: 0.0966609032896727		[learning rate: 7.1238e-05]
	Learning Rate: 7.12378e-05
	LOSS [training: 0.09490000729349038 | validation: 0.08061875401987537]
	TIME [epoch: 8.18 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09990481492902609		[learning rate: 7.1108e-05]
		[batch 20/20] avg loss: 0.1323731583015728		[learning rate: 7.0979e-05]
	Learning Rate: 7.09793e-05
	LOSS [training: 0.11613898661529945 | validation: 0.08698651644663902]
	TIME [epoch: 8.18 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1084662902825261		[learning rate: 7.085e-05]
		[batch 20/20] avg loss: 0.13414106608593454		[learning rate: 7.0722e-05]
	Learning Rate: 7.07217e-05
	LOSS [training: 0.12130367818423034 | validation: 0.10822480797273765]
	TIME [epoch: 8.18 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10583556969634773		[learning rate: 7.0593e-05]
		[batch 20/20] avg loss: 0.10755519040315656		[learning rate: 7.0465e-05]
	Learning Rate: 7.0465e-05
	LOSS [training: 0.1066953800497521 | validation: 0.0774201624041223]
	TIME [epoch: 8.2 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11530656978824341		[learning rate: 7.0337e-05]
		[batch 20/20] avg loss: 0.10881775279863126		[learning rate: 7.0209e-05]
	Learning Rate: 7.02093e-05
	LOSS [training: 0.11206216129343734 | validation: 0.09318593044855766]
	TIME [epoch: 8.18 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12070601386305488		[learning rate: 7.0082e-05]
		[batch 20/20] avg loss: 0.11531667330253077		[learning rate: 6.9955e-05]
	Learning Rate: 6.99545e-05
	LOSS [training: 0.11801134358279282 | validation: 0.07824140477801608]
	TIME [epoch: 8.18 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11167372740086781		[learning rate: 6.9827e-05]
		[batch 20/20] avg loss: 0.09975953603404966		[learning rate: 6.9701e-05]
	Learning Rate: 6.97006e-05
	LOSS [training: 0.10571663171745871 | validation: 0.07380925107559907]
	TIME [epoch: 8.18 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10464200061156918		[learning rate: 6.9574e-05]
		[batch 20/20] avg loss: 0.09997211280515238		[learning rate: 6.9448e-05]
	Learning Rate: 6.94477e-05
	LOSS [training: 0.10230705670836078 | validation: 0.08059808060949476]
	TIME [epoch: 8.2 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09768216518668364		[learning rate: 6.9322e-05]
		[batch 20/20] avg loss: 0.1048549801696436		[learning rate: 6.9196e-05]
	Learning Rate: 6.91957e-05
	LOSS [training: 0.10126857267816361 | validation: 0.08541265832426184]
	TIME [epoch: 8.17 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1183156831649537		[learning rate: 6.907e-05]
		[batch 20/20] avg loss: 0.11358426490547728		[learning rate: 6.8945e-05]
	Learning Rate: 6.89446e-05
	LOSS [training: 0.1159499740352155 | validation: 0.11336069993933905]
	TIME [epoch: 8.17 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1093303233970804		[learning rate: 6.8819e-05]
		[batch 20/20] avg loss: 0.12038935304093912		[learning rate: 6.8694e-05]
	Learning Rate: 6.86944e-05
	LOSS [training: 0.11485983821900976 | validation: 0.1298939750368981]
	TIME [epoch: 8.18 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17310691389798577		[learning rate: 6.857e-05]
		[batch 20/20] avg loss: 0.2553570512804816		[learning rate: 6.8445e-05]
	Learning Rate: 6.84451e-05
	LOSS [training: 0.21423198258923365 | validation: 0.16239428446690987]
	TIME [epoch: 8.2 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.14953955655336648		[learning rate: 6.8321e-05]
		[batch 20/20] avg loss: 0.14356088897340974		[learning rate: 6.8197e-05]
	Learning Rate: 6.81967e-05
	LOSS [training: 0.14655022276338808 | validation: 0.16546772923462297]
	TIME [epoch: 8.18 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17064323840264173		[learning rate: 6.8073e-05]
		[batch 20/20] avg loss: 0.11905169152820079		[learning rate: 6.7949e-05]
	Learning Rate: 6.79492e-05
	LOSS [training: 0.14484746496542128 | validation: 0.13412393157331792]
	TIME [epoch: 8.18 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17294269310549634		[learning rate: 6.7826e-05]
		[batch 20/20] avg loss: 0.12218841748281825		[learning rate: 6.7703e-05]
	Learning Rate: 6.77026e-05
	LOSS [training: 0.1475655552941573 | validation: 0.10449779329150655]
	TIME [epoch: 8.18 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11501258594803143		[learning rate: 6.758e-05]
		[batch 20/20] avg loss: 0.10796935322128232		[learning rate: 6.7457e-05]
	Learning Rate: 6.74569e-05
	LOSS [training: 0.11149096958465687 | validation: 0.091596451957902]
	TIME [epoch: 8.2 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1032985470134119		[learning rate: 6.7334e-05]
		[batch 20/20] avg loss: 0.11513725913836532		[learning rate: 6.7212e-05]
	Learning Rate: 6.72121e-05
	LOSS [training: 0.1092179030758886 | validation: 0.11273897128587128]
	TIME [epoch: 8.17 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.13388221566208325		[learning rate: 6.709e-05]
		[batch 20/20] avg loss: 0.13471385015366502		[learning rate: 6.6968e-05]
	Learning Rate: 6.69682e-05
	LOSS [training: 0.13429803290787415 | validation: 0.15000002258268658]
	TIME [epoch: 8.19 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12444126942494424		[learning rate: 6.6847e-05]
		[batch 20/20] avg loss: 0.13425307486763696		[learning rate: 6.6725e-05]
	Learning Rate: 6.67251e-05
	LOSS [training: 0.1293471721462906 | validation: 0.12199256076618631]
	TIME [epoch: 8.18 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11374405908422473		[learning rate: 6.6604e-05]
		[batch 20/20] avg loss: 0.11638115171428871		[learning rate: 6.6483e-05]
	Learning Rate: 6.6483e-05
	LOSS [training: 0.11506260539925672 | validation: 0.08607081109128098]
	TIME [epoch: 8.2 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1167418420104952		[learning rate: 6.6362e-05]
		[batch 20/20] avg loss: 0.1063160168460902		[learning rate: 6.6242e-05]
	Learning Rate: 6.62417e-05
	LOSS [training: 0.1115289294282927 | validation: 0.09551937566137646]
	TIME [epoch: 8.18 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12275703171164501		[learning rate: 6.6121e-05]
		[batch 20/20] avg loss: 0.1391366607576054		[learning rate: 6.6001e-05]
	Learning Rate: 6.60013e-05
	LOSS [training: 0.13094684623462519 | validation: 0.07435807750375636]
	TIME [epoch: 8.18 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11129997339272199		[learning rate: 6.5881e-05]
		[batch 20/20] avg loss: 0.10996886445384912		[learning rate: 6.5762e-05]
	Learning Rate: 6.57618e-05
	LOSS [training: 0.11063441892328556 | validation: 0.07863238740913567]
	TIME [epoch: 8.18 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10844688157967845		[learning rate: 6.5642e-05]
		[batch 20/20] avg loss: 0.10396615558488817		[learning rate: 6.5523e-05]
	Learning Rate: 6.55232e-05
	LOSS [training: 0.10620651858228333 | validation: 0.08333867946715716]
	TIME [epoch: 8.2 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11612917723651128		[learning rate: 6.5404e-05]
		[batch 20/20] avg loss: 0.11475508384781688		[learning rate: 6.5285e-05]
	Learning Rate: 6.52854e-05
	LOSS [training: 0.1154421305421641 | validation: 0.07271957622823097]
	TIME [epoch: 8.18 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10591314236042575		[learning rate: 6.5167e-05]
		[batch 20/20] avg loss: 0.11204301906405467		[learning rate: 6.5048e-05]
	Learning Rate: 6.50484e-05
	LOSS [training: 0.10897808071224022 | validation: 0.0696422997159008]
	TIME [epoch: 8.18 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10462264756839108		[learning rate: 6.493e-05]
		[batch 20/20] avg loss: 0.12621848137266908		[learning rate: 6.4812e-05]
	Learning Rate: 6.48124e-05
	LOSS [training: 0.11542056447053009 | validation: 0.09062172003346129]
	TIME [epoch: 8.18 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09710179832380318		[learning rate: 6.4695e-05]
		[batch 20/20] avg loss: 0.113052782099169		[learning rate: 6.4577e-05]
	Learning Rate: 6.45772e-05
	LOSS [training: 0.10507729021148611 | validation: 0.08399813860341289]
	TIME [epoch: 8.19 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09840098775850231		[learning rate: 6.446e-05]
		[batch 20/20] avg loss: 0.09833767650270202		[learning rate: 6.4343e-05]
	Learning Rate: 6.43428e-05
	LOSS [training: 0.09836933213060214 | validation: 0.07686262071700778]
	TIME [epoch: 8.18 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09635650778919705		[learning rate: 6.4226e-05]
		[batch 20/20] avg loss: 0.1065793479371168		[learning rate: 6.4109e-05]
	Learning Rate: 6.41093e-05
	LOSS [training: 0.10146792786315692 | validation: 0.07024358657372333]
	TIME [epoch: 8.18 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09921425057331971		[learning rate: 6.3993e-05]
		[batch 20/20] avg loss: 0.09692081432127411		[learning rate: 6.3877e-05]
	Learning Rate: 6.38767e-05
	LOSS [training: 0.09806753244729692 | validation: 0.06962561138280814]
	TIME [epoch: 8.18 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09473751211573671		[learning rate: 6.3761e-05]
		[batch 20/20] avg loss: 0.10891574574518623		[learning rate: 6.3645e-05]
	Learning Rate: 6.36449e-05
	LOSS [training: 0.1018266289304615 | validation: 0.08537583455066422]
	TIME [epoch: 8.2 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11189537230487254		[learning rate: 6.3529e-05]
		[batch 20/20] avg loss: 0.09898179797613803		[learning rate: 6.3414e-05]
	Learning Rate: 6.34139e-05
	LOSS [training: 0.10543858514050526 | validation: 0.07572446819569074]
	TIME [epoch: 8.18 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09869859126353989		[learning rate: 6.3299e-05]
		[batch 20/20] avg loss: 0.11656188248504526		[learning rate: 6.3184e-05]
	Learning Rate: 6.31837e-05
	LOSS [training: 0.10763023687429256 | validation: 0.09173131544529954]
	TIME [epoch: 8.18 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10782385476179242		[learning rate: 6.3069e-05]
		[batch 20/20] avg loss: 0.0915926994141866		[learning rate: 6.2954e-05]
	Learning Rate: 6.29544e-05
	LOSS [training: 0.09970827708798952 | validation: 0.08058245465729648]
	TIME [epoch: 8.18 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10089195817752125		[learning rate: 6.284e-05]
		[batch 20/20] avg loss: 0.09139922364217311		[learning rate: 6.2726e-05]
	Learning Rate: 6.2726e-05
	LOSS [training: 0.09614559090984716 | validation: 0.07363584179641618]
	TIME [epoch: 8.2 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10250834571764111		[learning rate: 6.2612e-05]
		[batch 20/20] avg loss: 0.09174917676272835		[learning rate: 6.2498e-05]
	Learning Rate: 6.24983e-05
	LOSS [training: 0.09712876124018474 | validation: 0.07439271839625441]
	TIME [epoch: 8.18 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08305839005808272		[learning rate: 6.2385e-05]
		[batch 20/20] avg loss: 0.10820981338026596		[learning rate: 6.2272e-05]
	Learning Rate: 6.22715e-05
	LOSS [training: 0.09563410171917434 | validation: 0.0836227302446922]
	TIME [epoch: 8.18 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11508183818580672		[learning rate: 6.2158e-05]
		[batch 20/20] avg loss: 0.10425541556526349		[learning rate: 6.2046e-05]
	Learning Rate: 6.20455e-05
	LOSS [training: 0.10966862687553511 | validation: 0.07593267252288213]
	TIME [epoch: 8.18 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09538526878724761		[learning rate: 6.1933e-05]
		[batch 20/20] avg loss: 0.09039447570704898		[learning rate: 6.182e-05]
	Learning Rate: 6.18204e-05
	LOSS [training: 0.09288987224714829 | validation: 0.06857526945013956]
	TIME [epoch: 8.2 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10136683137763078		[learning rate: 6.1708e-05]
		[batch 20/20] avg loss: 0.09290274288128895		[learning rate: 6.1596e-05]
	Learning Rate: 6.1596e-05
	LOSS [training: 0.09713478712945986 | validation: 0.06443430238121216]
	TIME [epoch: 8.19 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08567052165938951		[learning rate: 6.1484e-05]
		[batch 20/20] avg loss: 0.1027043234566464		[learning rate: 6.1372e-05]
	Learning Rate: 6.13725e-05
	LOSS [training: 0.09418742255801793 | validation: 0.07226476443844985]
	TIME [epoch: 8.18 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11466931506559544		[learning rate: 6.1261e-05]
		[batch 20/20] avg loss: 0.09608995672501214		[learning rate: 6.115e-05]
	Learning Rate: 6.11498e-05
	LOSS [training: 0.10537963589530377 | validation: 0.0888256050919699]
	TIME [epoch: 8.18 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.12295243220344798		[learning rate: 6.1039e-05]
		[batch 20/20] avg loss: 0.09494165308525915		[learning rate: 6.0928e-05]
	Learning Rate: 6.09278e-05
	LOSS [training: 0.10894704264435358 | validation: 0.08839977967267357]
	TIME [epoch: 8.2 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10472243058874944		[learning rate: 6.0817e-05]
		[batch 20/20] avg loss: 0.0849000481853451		[learning rate: 6.0707e-05]
	Learning Rate: 6.07067e-05
	LOSS [training: 0.09481123938704729 | validation: 0.07659040389499186]
	TIME [epoch: 8.18 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10611139048262279		[learning rate: 6.0596e-05]
		[batch 20/20] avg loss: 0.08807017952031007		[learning rate: 6.0486e-05]
	Learning Rate: 6.04864e-05
	LOSS [training: 0.09709078500146642 | validation: 0.0674245346663766]
	TIME [epoch: 8.18 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1118027941996012		[learning rate: 6.0377e-05]
		[batch 20/20] avg loss: 0.09503766166139868		[learning rate: 6.0267e-05]
	Learning Rate: 6.02669e-05
	LOSS [training: 0.10342022793049994 | validation: 0.08784931253443515]
	TIME [epoch: 8.18 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10192442622891462		[learning rate: 6.0157e-05]
		[batch 20/20] avg loss: 0.09156071492268616		[learning rate: 6.0048e-05]
	Learning Rate: 6.00482e-05
	LOSS [training: 0.09674257057580038 | validation: 0.06701056365208202]
	TIME [epoch: 8.2 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09553475992944085		[learning rate: 5.9939e-05]
		[batch 20/20] avg loss: 0.0983689423698956		[learning rate: 5.983e-05]
	Learning Rate: 5.98303e-05
	LOSS [training: 0.09695185114966823 | validation: 0.0714740501980431]
	TIME [epoch: 8.19 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09592680397002724		[learning rate: 5.9722e-05]
		[batch 20/20] avg loss: 0.08764309799522858		[learning rate: 5.9613e-05]
	Learning Rate: 5.96132e-05
	LOSS [training: 0.09178495098262791 | validation: 0.07280770140470204]
	TIME [epoch: 8.18 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10144355940235576		[learning rate: 5.9505e-05]
		[batch 20/20] avg loss: 0.09558250943103815		[learning rate: 5.9397e-05]
	Learning Rate: 5.93968e-05
	LOSS [training: 0.09851303441669697 | validation: 0.07328904924999091]
	TIME [epoch: 8.18 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09755173187146426		[learning rate: 5.9289e-05]
		[batch 20/20] avg loss: 0.1088824013615177		[learning rate: 5.9181e-05]
	Learning Rate: 5.91813e-05
	LOSS [training: 0.103217066616491 | validation: 0.07759236417543217]
	TIME [epoch: 8.2 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09669062554399435		[learning rate: 5.9074e-05]
		[batch 20/20] avg loss: 0.09430999813899267		[learning rate: 5.8966e-05]
	Learning Rate: 5.89665e-05
	LOSS [training: 0.09550031184149352 | validation: 0.06995677535140189]
	TIME [epoch: 8.19 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09750133693475427		[learning rate: 5.8859e-05]
		[batch 20/20] avg loss: 0.10882908078421036		[learning rate: 5.8752e-05]
	Learning Rate: 5.87525e-05
	LOSS [training: 0.10316520885948233 | validation: 0.09451944060678677]
	TIME [epoch: 8.18 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10655762069841535		[learning rate: 5.8646e-05]
		[batch 20/20] avg loss: 0.08808133823717347		[learning rate: 5.8539e-05]
	Learning Rate: 5.85393e-05
	LOSS [training: 0.0973194794677944 | validation: 0.07072417225240771]
	TIME [epoch: 8.18 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10412479071940857		[learning rate: 5.8433e-05]
		[batch 20/20] avg loss: 0.10045280562915988		[learning rate: 5.8327e-05]
	Learning Rate: 5.83268e-05
	LOSS [training: 0.10228879817428423 | validation: 0.0686332867428773]
	TIME [epoch: 8.2 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10116240975949642		[learning rate: 5.8221e-05]
		[batch 20/20] avg loss: 0.09487076942148823		[learning rate: 5.8115e-05]
	Learning Rate: 5.81152e-05
	LOSS [training: 0.09801658959049234 | validation: 0.06560815985608036]
	TIME [epoch: 8.18 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09719639528509046		[learning rate: 5.801e-05]
		[batch 20/20] avg loss: 0.09464368625461905		[learning rate: 5.7904e-05]
	Learning Rate: 5.79043e-05
	LOSS [training: 0.09592004076985475 | validation: 0.06289942196657164]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1517.pth
	Model improved!!!
EPOCH 1518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09597892980966091		[learning rate: 5.7799e-05]
		[batch 20/20] avg loss: 0.10553462362319514		[learning rate: 5.7694e-05]
	Learning Rate: 5.76941e-05
	LOSS [training: 0.10075677671642802 | validation: 0.07605071273992636]
	TIME [epoch: 8.19 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09725105682182202		[learning rate: 5.7589e-05]
		[batch 20/20] avg loss: 0.09141987659148139		[learning rate: 5.7485e-05]
	Learning Rate: 5.74847e-05
	LOSS [training: 0.09433546670665169 | validation: 0.06837794216213038]
	TIME [epoch: 8.21 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08228349873106412		[learning rate: 5.738e-05]
		[batch 20/20] avg loss: 0.10619811035995626		[learning rate: 5.7276e-05]
	Learning Rate: 5.72761e-05
	LOSS [training: 0.09424080454551018 | validation: 0.06282338980977642]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1520.pth
	Model improved!!!
EPOCH 1521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08821165742654423		[learning rate: 5.7172e-05]
		[batch 20/20] avg loss: 0.09770005409381712		[learning rate: 5.7068e-05]
	Learning Rate: 5.70683e-05
	LOSS [training: 0.09295585576018067 | validation: 0.06371196905382248]
	TIME [epoch: 8.2 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11305885347868569		[learning rate: 5.6965e-05]
		[batch 20/20] avg loss: 0.10012021238072581		[learning rate: 5.6861e-05]
	Learning Rate: 5.68612e-05
	LOSS [training: 0.10658953292970577 | validation: 0.07251376335325674]
	TIME [epoch: 8.18 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09727709762165727		[learning rate: 5.6758e-05]
		[batch 20/20] avg loss: 0.0896772130482675		[learning rate: 5.6655e-05]
	Learning Rate: 5.66548e-05
	LOSS [training: 0.0934771553349624 | validation: 0.06641625542921656]
	TIME [epoch: 8.21 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08760821844477361		[learning rate: 5.6552e-05]
		[batch 20/20] avg loss: 0.10331258965935508		[learning rate: 5.6449e-05]
	Learning Rate: 5.64492e-05
	LOSS [training: 0.09546040405206437 | validation: 0.06962269282328473]
	TIME [epoch: 8.19 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10627115635279061		[learning rate: 5.6347e-05]
		[batch 20/20] avg loss: 0.08356537859143164		[learning rate: 5.6244e-05]
	Learning Rate: 5.62444e-05
	LOSS [training: 0.09491826747211113 | validation: 0.06662663053067014]
	TIME [epoch: 8.18 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09799274171007213		[learning rate: 5.6142e-05]
		[batch 20/20] avg loss: 0.09207414772581768		[learning rate: 5.604e-05]
	Learning Rate: 5.60403e-05
	LOSS [training: 0.09503344471794489 | validation: 0.07555749039052331]
	TIME [epoch: 8.18 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09379438661534056		[learning rate: 5.5938e-05]
		[batch 20/20] avg loss: 0.10192860003251376		[learning rate: 5.5837e-05]
	Learning Rate: 5.58369e-05
	LOSS [training: 0.09786149332392716 | validation: 0.0704745603447047]
	TIME [epoch: 8.2 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09893699744730754		[learning rate: 5.5735e-05]
		[batch 20/20] avg loss: 0.10244614039447406		[learning rate: 5.5634e-05]
	Learning Rate: 5.56342e-05
	LOSS [training: 0.10069156892089082 | validation: 0.07118848944952573]
	TIME [epoch: 8.19 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08411331552705556		[learning rate: 5.5533e-05]
		[batch 20/20] avg loss: 0.10274550370596203		[learning rate: 5.5432e-05]
	Learning Rate: 5.54323e-05
	LOSS [training: 0.09342940961650881 | validation: 0.06239789367361169]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1529.pth
	Model improved!!!
EPOCH 1530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09051987675860954		[learning rate: 5.5332e-05]
		[batch 20/20] avg loss: 0.09744671111705638		[learning rate: 5.5231e-05]
	Learning Rate: 5.52312e-05
	LOSS [training: 0.09398329393783296 | validation: 0.06601953592567951]
	TIME [epoch: 8.19 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10309309608421054		[learning rate: 5.5131e-05]
		[batch 20/20] avg loss: 0.08395886140318248		[learning rate: 5.5031e-05]
	Learning Rate: 5.50307e-05
	LOSS [training: 0.0935259787436965 | validation: 0.07392769159006884]
	TIME [epoch: 8.21 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09126547097745641		[learning rate: 5.4931e-05]
		[batch 20/20] avg loss: 0.0971905202611576		[learning rate: 5.4831e-05]
	Learning Rate: 5.4831e-05
	LOSS [training: 0.09422799561930702 | validation: 0.07354939997498608]
	TIME [epoch: 8.18 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09148466073136867		[learning rate: 5.4731e-05]
		[batch 20/20] avg loss: 0.10552334159572452		[learning rate: 5.4632e-05]
	Learning Rate: 5.4632e-05
	LOSS [training: 0.0985040011635466 | validation: 0.06502862525976066]
	TIME [epoch: 8.19 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10280571294784917		[learning rate: 5.4533e-05]
		[batch 20/20] avg loss: 0.11570643452056302		[learning rate: 5.4434e-05]
	Learning Rate: 5.44338e-05
	LOSS [training: 0.1092560737342061 | validation: 0.0703722930592211]
	TIME [epoch: 8.18 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09992471160170102		[learning rate: 5.4335e-05]
		[batch 20/20] avg loss: 0.10988054501587523		[learning rate: 5.4236e-05]
	Learning Rate: 5.42362e-05
	LOSS [training: 0.10490262830878812 | validation: 0.072207723970161]
	TIME [epoch: 8.21 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10533591117419605		[learning rate: 5.4138e-05]
		[batch 20/20] avg loss: 0.09491188795197447		[learning rate: 5.4039e-05]
	Learning Rate: 5.40394e-05
	LOSS [training: 0.10012389956308528 | validation: 0.06219225442697136]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1536.pth
	Model improved!!!
EPOCH 1537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11152260445093458		[learning rate: 5.3941e-05]
		[batch 20/20] avg loss: 0.11938600883509005		[learning rate: 5.3843e-05]
	Learning Rate: 5.38433e-05
	LOSS [training: 0.1154543066430123 | validation: 0.08322790136664115]
	TIME [epoch: 8.2 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10335155631907011		[learning rate: 5.3746e-05]
		[batch 20/20] avg loss: 0.0953328221314015		[learning rate: 5.3648e-05]
	Learning Rate: 5.36479e-05
	LOSS [training: 0.0993421892252358 | validation: 0.07052288698441352]
	TIME [epoch: 8.19 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11655380738645386		[learning rate: 5.355e-05]
		[batch 20/20] avg loss: 0.10994250361242217		[learning rate: 5.3453e-05]
	Learning Rate: 5.34532e-05
	LOSS [training: 0.11324815549943804 | validation: 0.06045083804492316]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1539.pth
	Model improved!!!
EPOCH 1540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09839232035234091		[learning rate: 5.3356e-05]
		[batch 20/20] avg loss: 0.09796333544773284		[learning rate: 5.3259e-05]
	Learning Rate: 5.32592e-05
	LOSS [training: 0.0981778279000369 | validation: 0.06991107087584275]
	TIME [epoch: 8.2 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10107449480946151		[learning rate: 5.3162e-05]
		[batch 20/20] avg loss: 0.09486222649918893		[learning rate: 5.3066e-05]
	Learning Rate: 5.30659e-05
	LOSS [training: 0.09796836065432521 | validation: 0.0786715522794934]
	TIME [epoch: 8.2 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0984744038686647		[learning rate: 5.297e-05]
		[batch 20/20] avg loss: 0.11117508161811218		[learning rate: 5.2873e-05]
	Learning Rate: 5.28734e-05
	LOSS [training: 0.10482474274338842 | validation: 0.07759707981890596]
	TIME [epoch: 8.18 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09809890890766257		[learning rate: 5.2777e-05]
		[batch 20/20] avg loss: 0.10356319389093807		[learning rate: 5.2681e-05]
	Learning Rate: 5.26815e-05
	LOSS [training: 0.1008310513993003 | validation: 0.07467526980879616]
	TIME [epoch: 8.21 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09088341064266478		[learning rate: 5.2586e-05]
		[batch 20/20] avg loss: 0.10350477684416588		[learning rate: 5.249e-05]
	Learning Rate: 5.24903e-05
	LOSS [training: 0.09719409374341534 | validation: 0.07091217049285166]
	TIME [epoch: 8.19 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09531572245513298		[learning rate: 5.2395e-05]
		[batch 20/20] avg loss: 0.09620020846445657		[learning rate: 5.23e-05]
	Learning Rate: 5.22998e-05
	LOSS [training: 0.09575796545979473 | validation: 0.060182588275379256]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1545.pth
	Model improved!!!
EPOCH 1546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09375992005068251		[learning rate: 5.2205e-05]
		[batch 20/20] avg loss: 0.10048444320479114		[learning rate: 5.211e-05]
	Learning Rate: 5.211e-05
	LOSS [training: 0.09712218162773681 | validation: 0.07348571699590364]
	TIME [epoch: 8.19 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11287739499205238		[learning rate: 5.2015e-05]
		[batch 20/20] avg loss: 0.1017638975853484		[learning rate: 5.1921e-05]
	Learning Rate: 5.19209e-05
	LOSS [training: 0.10732064628870039 | validation: 0.057466392541563216]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1547.pth
	Model improved!!!
EPOCH 1548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09516518260401143		[learning rate: 5.1827e-05]
		[batch 20/20] avg loss: 0.09026592025595784		[learning rate: 5.1732e-05]
	Learning Rate: 5.17325e-05
	LOSS [training: 0.0927155514299846 | validation: 0.06912657622719116]
	TIME [epoch: 8.2 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10201704105539315		[learning rate: 5.1639e-05]
		[batch 20/20] avg loss: 0.11519810697748485		[learning rate: 5.1545e-05]
	Learning Rate: 5.15447e-05
	LOSS [training: 0.10860757401643902 | validation: 0.08567673234265732]
	TIME [epoch: 8.18 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10162252187876711		[learning rate: 5.1451e-05]
		[batch 20/20] avg loss: 0.10003714367491948		[learning rate: 5.1358e-05]
	Learning Rate: 5.13577e-05
	LOSS [training: 0.10082983277684329 | validation: 0.07956188324183985]
	TIME [epoch: 8.19 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10229445968054882		[learning rate: 5.1264e-05]
		[batch 20/20] avg loss: 0.10019171878922226		[learning rate: 5.1171e-05]
	Learning Rate: 5.11713e-05
	LOSS [training: 0.10124308923488554 | validation: 0.06129611016001475]
	TIME [epoch: 8.21 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0976705323221393		[learning rate: 5.1078e-05]
		[batch 20/20] avg loss: 0.11065489760356302		[learning rate: 5.0986e-05]
	Learning Rate: 5.09856e-05
	LOSS [training: 0.10416271496285116 | validation: 0.06399146320022683]
	TIME [epoch: 8.19 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09971118284658695		[learning rate: 5.0893e-05]
		[batch 20/20] avg loss: 0.10436455456197954		[learning rate: 5.0801e-05]
	Learning Rate: 5.08006e-05
	LOSS [training: 0.10203786870428326 | validation: 0.06130057246357612]
	TIME [epoch: 8.2 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09683327951539214		[learning rate: 5.0708e-05]
		[batch 20/20] avg loss: 0.10584676012461558		[learning rate: 5.0616e-05]
	Learning Rate: 5.06162e-05
	LOSS [training: 0.10134001982000387 | validation: 0.06558188944709482]
	TIME [epoch: 8.19 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11383359637982024		[learning rate: 5.0524e-05]
		[batch 20/20] avg loss: 0.1177934908066413		[learning rate: 5.0433e-05]
	Learning Rate: 5.04325e-05
	LOSS [training: 0.11581354359323075 | validation: 0.06799196324442708]
	TIME [epoch: 8.22 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0987177453992179		[learning rate: 5.0341e-05]
		[batch 20/20] avg loss: 0.1029960097457624		[learning rate: 5.0249e-05]
	Learning Rate: 5.02495e-05
	LOSS [training: 0.10085687757249015 | validation: 0.0820452947600371]
	TIME [epoch: 8.2 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10123082504955658		[learning rate: 5.0158e-05]
		[batch 20/20] avg loss: 0.10401377345125051		[learning rate: 5.0067e-05]
	Learning Rate: 5.00671e-05
	LOSS [training: 0.10262229925040353 | validation: 0.07301604513959284]
	TIME [epoch: 8.19 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10591154773150646		[learning rate: 4.9976e-05]
		[batch 20/20] avg loss: 0.09117345229794921		[learning rate: 4.9885e-05]
	Learning Rate: 4.98854e-05
	LOSS [training: 0.09854250001472785 | validation: 0.06737537143449915]
	TIME [epoch: 8.18 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09655321109031634		[learning rate: 4.9795e-05]
		[batch 20/20] avg loss: 0.10301195886800871		[learning rate: 4.9704e-05]
	Learning Rate: 4.97044e-05
	LOSS [training: 0.09978258497916252 | validation: 0.06910146068509784]
	TIME [epoch: 8.21 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09356084608800685		[learning rate: 4.9614e-05]
		[batch 20/20] avg loss: 0.1087872085806871		[learning rate: 4.9524e-05]
	Learning Rate: 4.9524e-05
	LOSS [training: 0.10117402733434695 | validation: 0.06606855197087727]
	TIME [epoch: 8.2 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1052678314593852		[learning rate: 4.9434e-05]
		[batch 20/20] avg loss: 0.10561811707691908		[learning rate: 4.9344e-05]
	Learning Rate: 4.93443e-05
	LOSS [training: 0.10544297426815215 | validation: 0.07732288530864909]
	TIME [epoch: 8.19 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10847268750891581		[learning rate: 4.9255e-05]
		[batch 20/20] avg loss: 0.10333857350277462		[learning rate: 4.9165e-05]
	Learning Rate: 4.91652e-05
	LOSS [training: 0.10590563050584523 | validation: 0.07004040752046942]
	TIME [epoch: 8.19 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10281334895618774		[learning rate: 4.9076e-05]
		[batch 20/20] avg loss: 0.09750821300874915		[learning rate: 4.8987e-05]
	Learning Rate: 4.89868e-05
	LOSS [training: 0.10016078098246846 | validation: 0.06414845442824893]
	TIME [epoch: 8.2 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10958308026695227		[learning rate: 4.8898e-05]
		[batch 20/20] avg loss: 0.09532867702885998		[learning rate: 4.8809e-05]
	Learning Rate: 4.8809e-05
	LOSS [training: 0.1024558786479061 | validation: 0.06820616158329368]
	TIME [epoch: 8.2 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10015888202082704		[learning rate: 4.872e-05]
		[batch 20/20] avg loss: 0.11452200383649		[learning rate: 4.8632e-05]
	Learning Rate: 4.86319e-05
	LOSS [training: 0.10734044292865849 | validation: 0.06002737475955988]
	TIME [epoch: 8.18 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10024016745653583		[learning rate: 4.8544e-05]
		[batch 20/20] avg loss: 0.10577992820931219		[learning rate: 4.8455e-05]
	Learning Rate: 4.84554e-05
	LOSS [training: 0.10301004783292403 | validation: 0.07386599088010648]
	TIME [epoch: 8.19 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09410751546675292		[learning rate: 4.8367e-05]
		[batch 20/20] avg loss: 0.1083861248537971		[learning rate: 4.828e-05]
	Learning Rate: 4.82795e-05
	LOSS [training: 0.101246820160275 | validation: 0.06988081170019046]
	TIME [epoch: 8.2 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09772257770796498		[learning rate: 4.8192e-05]
		[batch 20/20] avg loss: 0.1147533391412979		[learning rate: 4.8104e-05]
	Learning Rate: 4.81043e-05
	LOSS [training: 0.10623795842463143 | validation: 0.06548051336911725]
	TIME [epoch: 8.19 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1088823987594029		[learning rate: 4.8017e-05]
		[batch 20/20] avg loss: 0.09930744126227911		[learning rate: 4.793e-05]
	Learning Rate: 4.79298e-05
	LOSS [training: 0.10409492001084102 | validation: 0.07388196224659563]
	TIME [epoch: 8.18 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10271922048849644		[learning rate: 4.7843e-05]
		[batch 20/20] avg loss: 0.0963621971358077		[learning rate: 4.7756e-05]
	Learning Rate: 4.77558e-05
	LOSS [training: 0.09954070881215206 | validation: 0.0676962233153987]
	TIME [epoch: 8.19 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10979725516769925		[learning rate: 4.7669e-05]
		[batch 20/20] avg loss: 0.09954513188151337		[learning rate: 4.7583e-05]
	Learning Rate: 4.75825e-05
	LOSS [training: 0.1046711935246063 | validation: 0.0714574387907781]
	TIME [epoch: 8.21 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09795897708636342		[learning rate: 4.7496e-05]
		[batch 20/20] avg loss: 0.1011266804068526		[learning rate: 4.741e-05]
	Learning Rate: 4.74098e-05
	LOSS [training: 0.09954282874660801 | validation: 0.06635491022982996]
	TIME [epoch: 8.19 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10832726871895426		[learning rate: 4.7324e-05]
		[batch 20/20] avg loss: 0.08912051230368365		[learning rate: 4.7238e-05]
	Learning Rate: 4.72378e-05
	LOSS [training: 0.098723890511319 | validation: 0.06367576640831593]
	TIME [epoch: 8.19 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09500263392092957		[learning rate: 4.7152e-05]
		[batch 20/20] avg loss: 0.10614103071380927		[learning rate: 4.7066e-05]
	Learning Rate: 4.70664e-05
	LOSS [training: 0.10057183231736941 | validation: 0.06791885134123185]
	TIME [epoch: 8.19 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11164365811875106		[learning rate: 4.6981e-05]
		[batch 20/20] avg loss: 0.0944157395152422		[learning rate: 4.6896e-05]
	Learning Rate: 4.68955e-05
	LOSS [training: 0.10302969881699664 | validation: 0.07189869531885967]
	TIME [epoch: 8.2 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09729671002514528		[learning rate: 4.681e-05]
		[batch 20/20] avg loss: 0.09603604542731843		[learning rate: 4.6725e-05]
	Learning Rate: 4.67254e-05
	LOSS [training: 0.09666637772623185 | validation: 0.06509912499422368]
	TIME [epoch: 8.19 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10185044227629447		[learning rate: 4.6641e-05]
		[batch 20/20] avg loss: 0.10551436248579668		[learning rate: 4.6556e-05]
	Learning Rate: 4.65558e-05
	LOSS [training: 0.10368240238104558 | validation: 0.06817525418392995]
	TIME [epoch: 8.18 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10418802472072974		[learning rate: 4.6471e-05]
		[batch 20/20] avg loss: 0.09383383005158229		[learning rate: 4.6387e-05]
	Learning Rate: 4.63868e-05
	LOSS [training: 0.09901092738615602 | validation: 0.06254054265020861]
	TIME [epoch: 8.18 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09575136565203556		[learning rate: 4.6303e-05]
		[batch 20/20] avg loss: 0.09186058687863279		[learning rate: 4.6219e-05]
	Learning Rate: 4.62185e-05
	LOSS [training: 0.09380597626533418 | validation: 0.06595952652630399]
	TIME [epoch: 8.21 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09623250445947577		[learning rate: 4.6135e-05]
		[batch 20/20] avg loss: 0.095811183578883		[learning rate: 4.6051e-05]
	Learning Rate: 4.60508e-05
	LOSS [training: 0.09602184401917938 | validation: 0.0686785589895563]
	TIME [epoch: 8.19 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11200919410927587		[learning rate: 4.5967e-05]
		[batch 20/20] avg loss: 0.09057102382554168		[learning rate: 4.5884e-05]
	Learning Rate: 4.58836e-05
	LOSS [training: 0.10129010896740875 | validation: 0.06395038465059187]
	TIME [epoch: 8.18 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1000753811195606		[learning rate: 4.58e-05]
		[batch 20/20] avg loss: 0.1091025900083584		[learning rate: 4.5717e-05]
	Learning Rate: 4.57171e-05
	LOSS [training: 0.1045889855639595 | validation: 0.06408205873262421]
	TIME [epoch: 8.18 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09861413116977327		[learning rate: 4.5634e-05]
		[batch 20/20] avg loss: 0.10169345573122986		[learning rate: 4.5551e-05]
	Learning Rate: 4.55512e-05
	LOSS [training: 0.10015379345050154 | validation: 0.06854730239909071]
	TIME [epoch: 8.2 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10171369230764249		[learning rate: 4.5468e-05]
		[batch 20/20] avg loss: 0.1079038737345929		[learning rate: 4.5386e-05]
	Learning Rate: 4.53859e-05
	LOSS [training: 0.10480878302111771 | validation: 0.07407511105709375]
	TIME [epoch: 8.18 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10505945882679735		[learning rate: 4.5303e-05]
		[batch 20/20] avg loss: 0.10042560409380415		[learning rate: 4.5221e-05]
	Learning Rate: 4.52212e-05
	LOSS [training: 0.10274253146030074 | validation: 0.06944201054835446]
	TIME [epoch: 8.18 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10307800992490775		[learning rate: 4.5139e-05]
		[batch 20/20] avg loss: 0.10438796086994348		[learning rate: 4.5057e-05]
	Learning Rate: 4.50571e-05
	LOSS [training: 0.10373298539742563 | validation: 0.07605207496758035]
	TIME [epoch: 8.19 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09225137934677728		[learning rate: 4.4975e-05]
		[batch 20/20] avg loss: 0.11736812120362301		[learning rate: 4.4894e-05]
	Learning Rate: 4.48936e-05
	LOSS [training: 0.10480975027520016 | validation: 0.06647657624512256]
	TIME [epoch: 8.19 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09893606187080115		[learning rate: 4.4812e-05]
		[batch 20/20] avg loss: 0.1074625193219317		[learning rate: 4.4731e-05]
	Learning Rate: 4.47307e-05
	LOSS [training: 0.10319929059636643 | validation: 0.0594405974341886]
	TIME [epoch: 8.19 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1053510566979657		[learning rate: 4.4649e-05]
		[batch 20/20] avg loss: 0.10507474364653824		[learning rate: 4.4568e-05]
	Learning Rate: 4.45683e-05
	LOSS [training: 0.10521290017225197 | validation: 0.06474757137191288]
	TIME [epoch: 8.19 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09449668469617961		[learning rate: 4.4487e-05]
		[batch 20/20] avg loss: 0.1059446918804606		[learning rate: 4.4407e-05]
	Learning Rate: 4.44066e-05
	LOSS [training: 0.10022068828832012 | validation: 0.07273542700149127]
	TIME [epoch: 8.18 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09824265036719536		[learning rate: 4.4326e-05]
		[batch 20/20] avg loss: 0.1073455520646863		[learning rate: 4.4245e-05]
	Learning Rate: 4.42454e-05
	LOSS [training: 0.10279410121594083 | validation: 0.07427441305763752]
	TIME [epoch: 8.2 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11736568564867311		[learning rate: 4.4165e-05]
		[batch 20/20] avg loss: 0.11148748376337414		[learning rate: 4.4085e-05]
	Learning Rate: 4.40849e-05
	LOSS [training: 0.11442658470602363 | validation: 0.06420608379634553]
	TIME [epoch: 8.2 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10389419694174687		[learning rate: 4.4005e-05]
		[batch 20/20] avg loss: 0.09728919371572706		[learning rate: 4.3925e-05]
	Learning Rate: 4.39249e-05
	LOSS [training: 0.10059169532873698 | validation: 0.06795756858195422]
	TIME [epoch: 8.19 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10813526686527342		[learning rate: 4.3845e-05]
		[batch 20/20] avg loss: 0.100037672857713		[learning rate: 4.3765e-05]
	Learning Rate: 4.37655e-05
	LOSS [training: 0.10408646986149321 | validation: 0.07043062765198194]
	TIME [epoch: 8.19 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10423472232730698		[learning rate: 4.3686e-05]
		[batch 20/20] avg loss: 0.10096940525188274		[learning rate: 4.3607e-05]
	Learning Rate: 4.36067e-05
	LOSS [training: 0.10260206378959484 | validation: 0.07330774570326173]
	TIME [epoch: 8.19 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10143946001107454		[learning rate: 4.3527e-05]
		[batch 20/20] avg loss: 0.09957844520359269		[learning rate: 4.3448e-05]
	Learning Rate: 4.34484e-05
	LOSS [training: 0.10050895260733358 | validation: 0.06844319893091269]
	TIME [epoch: 8.2 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09834305884726577		[learning rate: 4.3369e-05]
		[batch 20/20] avg loss: 0.09674283918225998		[learning rate: 4.3291e-05]
	Learning Rate: 4.32907e-05
	LOSS [training: 0.09754294901476288 | validation: 0.06149997772655756]
	TIME [epoch: 8.19 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09728887630683396		[learning rate: 4.3212e-05]
		[batch 20/20] avg loss: 0.09697009088907058		[learning rate: 4.3134e-05]
	Learning Rate: 4.31336e-05
	LOSS [training: 0.09712948359795225 | validation: 0.06953397594457728]
	TIME [epoch: 8.18 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10554498216219362		[learning rate: 4.3055e-05]
		[batch 20/20] avg loss: 0.09419516445861144		[learning rate: 4.2977e-05]
	Learning Rate: 4.29771e-05
	LOSS [training: 0.09987007331040254 | validation: 0.0626151179259663]
	TIME [epoch: 8.18 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1051364865132467		[learning rate: 4.2899e-05]
		[batch 20/20] avg loss: 0.09996652954273531		[learning rate: 4.2821e-05]
	Learning Rate: 4.28211e-05
	LOSS [training: 0.102551508027991 | validation: 0.07079076912346509]
	TIME [epoch: 8.21 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1014696988458553		[learning rate: 4.2743e-05]
		[batch 20/20] avg loss: 0.10366544754955376		[learning rate: 4.2666e-05]
	Learning Rate: 4.26657e-05
	LOSS [training: 0.10256757319770453 | validation: 0.06611773906020128]
	TIME [epoch: 8.19 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1038642645370665		[learning rate: 4.2588e-05]
		[batch 20/20] avg loss: 0.09585103237079746		[learning rate: 4.2511e-05]
	Learning Rate: 4.25109e-05
	LOSS [training: 0.09985764845393197 | validation: 0.06464051246217879]
	TIME [epoch: 8.18 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09638516904869825		[learning rate: 4.2434e-05]
		[batch 20/20] avg loss: 0.09803722316604062		[learning rate: 4.2357e-05]
	Learning Rate: 4.23566e-05
	LOSS [training: 0.09721119610736942 | validation: 0.05918851899640566]
	TIME [epoch: 8.18 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09386758595042381		[learning rate: 4.228e-05]
		[batch 20/20] avg loss: 0.0993090551823348		[learning rate: 4.2203e-05]
	Learning Rate: 4.22029e-05
	LOSS [training: 0.09658832056637928 | validation: 0.06439629148352363]
	TIME [epoch: 8.2 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09144419226013735		[learning rate: 4.2126e-05]
		[batch 20/20] avg loss: 0.09910446571163067		[learning rate: 4.205e-05]
	Learning Rate: 4.20497e-05
	LOSS [training: 0.095274328985884 | validation: 0.06626816299312294]
	TIME [epoch: 8.19 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10286917229353063		[learning rate: 4.1973e-05]
		[batch 20/20] avg loss: 0.10617684274697814		[learning rate: 4.1897e-05]
	Learning Rate: 4.18971e-05
	LOSS [training: 0.10452300752025438 | validation: 0.07971704084122487]
	TIME [epoch: 8.18 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09490061641592759		[learning rate: 4.1821e-05]
		[batch 20/20] avg loss: 0.10828411752371118		[learning rate: 4.1745e-05]
	Learning Rate: 4.17451e-05
	LOSS [training: 0.1015923669698194 | validation: 0.06408083837673774]
	TIME [epoch: 8.19 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10590742695125488		[learning rate: 4.1669e-05]
		[batch 20/20] avg loss: 0.08835482327718927		[learning rate: 4.1594e-05]
	Learning Rate: 4.15936e-05
	LOSS [training: 0.09713112511422209 | validation: 0.06183727329711161]
	TIME [epoch: 8.21 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10268875664633839		[learning rate: 4.1518e-05]
		[batch 20/20] avg loss: 0.08737255542876428		[learning rate: 4.1443e-05]
	Learning Rate: 4.14426e-05
	LOSS [training: 0.09503065603755131 | validation: 0.06392265451846028]
	TIME [epoch: 8.18 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09220494456648357		[learning rate: 4.1367e-05]
		[batch 20/20] avg loss: 0.10226935728293433		[learning rate: 4.1292e-05]
	Learning Rate: 4.12922e-05
	LOSS [training: 0.09723715092470896 | validation: 0.0704624629314901]
	TIME [epoch: 8.18 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08666827977681477		[learning rate: 4.1217e-05]
		[batch 20/20] avg loss: 0.09876308564746399		[learning rate: 4.1142e-05]
	Learning Rate: 4.11424e-05
	LOSS [training: 0.09271568271213938 | validation: 0.059496288260179664]
	TIME [epoch: 8.19 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08623540205328181		[learning rate: 4.1068e-05]
		[batch 20/20] avg loss: 0.10024709064114048		[learning rate: 4.0993e-05]
	Learning Rate: 4.09931e-05
	LOSS [training: 0.09324124634721115 | validation: 0.07498861601333737]
	TIME [epoch: 8.21 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10257691432577734		[learning rate: 4.0919e-05]
		[batch 20/20] avg loss: 0.09243849283631175		[learning rate: 4.0844e-05]
	Learning Rate: 4.08443e-05
	LOSS [training: 0.09750770358104455 | validation: 0.06082077411026181]
	TIME [epoch: 8.18 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10004476044719224		[learning rate: 4.077e-05]
		[batch 20/20] avg loss: 0.09168064077301522		[learning rate: 4.0696e-05]
	Learning Rate: 4.06961e-05
	LOSS [training: 0.09586270061010374 | validation: 0.05882460806642084]
	TIME [epoch: 8.19 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09407798286934595		[learning rate: 4.0622e-05]
		[batch 20/20] avg loss: 0.09630318917441194		[learning rate: 4.0548e-05]
	Learning Rate: 4.05484e-05
	LOSS [training: 0.09519058602187894 | validation: 0.06922156127953347]
	TIME [epoch: 8.18 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0968578768957962		[learning rate: 4.0475e-05]
		[batch 20/20] avg loss: 0.09275079257443498		[learning rate: 4.0401e-05]
	Learning Rate: 4.04012e-05
	LOSS [training: 0.0948043347351156 | validation: 0.06374970503689378]
	TIME [epoch: 8.2 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09799998465614199		[learning rate: 4.0328e-05]
		[batch 20/20] avg loss: 0.08880445649802823		[learning rate: 4.0255e-05]
	Learning Rate: 4.02546e-05
	LOSS [training: 0.09340222057708511 | validation: 0.06338515689660587]
	TIME [epoch: 8.18 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10168911241338982		[learning rate: 4.0182e-05]
		[batch 20/20] avg loss: 0.0860023880841571		[learning rate: 4.0109e-05]
	Learning Rate: 4.01085e-05
	LOSS [training: 0.09384575024877348 | validation: 0.06572885268099532]
	TIME [epoch: 8.19 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08760250123745077		[learning rate: 4.0036e-05]
		[batch 20/20] avg loss: 0.1023124259404431		[learning rate: 3.9963e-05]
	Learning Rate: 3.9963e-05
	LOSS [training: 0.09495746358894694 | validation: 0.05937278220388022]
	TIME [epoch: 8.18 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08688136701264745		[learning rate: 3.989e-05]
		[batch 20/20] avg loss: 0.09879966233001272		[learning rate: 3.9818e-05]
	Learning Rate: 3.9818e-05
	LOSS [training: 0.0928405146713301 | validation: 0.06704232812743761]
	TIME [epoch: 8.2 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08016975727747178		[learning rate: 3.9746e-05]
		[batch 20/20] avg loss: 0.10174747379553357		[learning rate: 3.9673e-05]
	Learning Rate: 3.96735e-05
	LOSS [training: 0.09095861553650267 | validation: 0.05465039490348847]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1621.pth
	Model improved!!!
EPOCH 1622/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09471103260456379		[learning rate: 3.9601e-05]
		[batch 20/20] avg loss: 0.0856102637663407		[learning rate: 3.9529e-05]
	Learning Rate: 3.95295e-05
	LOSS [training: 0.09016064818545223 | validation: 0.06592886542586081]
	TIME [epoch: 8.2 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09531190745245281		[learning rate: 3.9458e-05]
		[batch 20/20] avg loss: 0.09064831693593495		[learning rate: 3.9386e-05]
	Learning Rate: 3.9386e-05
	LOSS [training: 0.09298011219419387 | validation: 0.06989540416234871]
	TIME [epoch: 8.19 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09057979350415254		[learning rate: 3.9315e-05]
		[batch 20/20] avg loss: 0.09542010785979561		[learning rate: 3.9243e-05]
	Learning Rate: 3.92431e-05
	LOSS [training: 0.09299995068197406 | validation: 0.06359728792321305]
	TIME [epoch: 8.21 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08759211632976996		[learning rate: 3.9172e-05]
		[batch 20/20] avg loss: 0.09208135014869309		[learning rate: 3.9101e-05]
	Learning Rate: 3.91007e-05
	LOSS [training: 0.08983673323923151 | validation: 0.06689452210391633]
	TIME [epoch: 8.19 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08473298722534114		[learning rate: 3.903e-05]
		[batch 20/20] avg loss: 0.10000849409452824		[learning rate: 3.8959e-05]
	Learning Rate: 3.89588e-05
	LOSS [training: 0.0923707406599347 | validation: 0.08154754550964335]
	TIME [epoch: 8.19 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10687153848452073		[learning rate: 3.8888e-05]
		[batch 20/20] avg loss: 0.10520329999532821		[learning rate: 3.8817e-05]
	Learning Rate: 3.88174e-05
	LOSS [training: 0.10603741923992449 | validation: 0.07270852650055003]
	TIME [epoch: 8.19 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09390446549490428		[learning rate: 3.8747e-05]
		[batch 20/20] avg loss: 0.09297241074267079		[learning rate: 3.8677e-05]
	Learning Rate: 3.86765e-05
	LOSS [training: 0.09343843811878753 | validation: 0.06996760733678731]
	TIME [epoch: 8.21 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09512835995050507		[learning rate: 3.8606e-05]
		[batch 20/20] avg loss: 0.08792007433919398		[learning rate: 3.8536e-05]
	Learning Rate: 3.85362e-05
	LOSS [training: 0.09152421714484951 | validation: 0.07273895789668414]
	TIME [epoch: 8.19 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10003364106026043		[learning rate: 3.8466e-05]
		[batch 20/20] avg loss: 0.0932491890038517		[learning rate: 3.8396e-05]
	Learning Rate: 3.83963e-05
	LOSS [training: 0.09664141503205606 | validation: 0.05911459474444007]
	TIME [epoch: 8.19 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09204861715040578		[learning rate: 3.8327e-05]
		[batch 20/20] avg loss: 0.0878473011403453		[learning rate: 3.8257e-05]
	Learning Rate: 3.8257e-05
	LOSS [training: 0.08994795914537554 | validation: 0.0598250451443182]
	TIME [epoch: 8.19 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0876933408226124		[learning rate: 3.8187e-05]
		[batch 20/20] avg loss: 0.0951944884002333		[learning rate: 3.8118e-05]
	Learning Rate: 3.81181e-05
	LOSS [training: 0.09144391461142286 | validation: 0.07110847906892862]
	TIME [epoch: 8.21 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11057882508503158		[learning rate: 3.8049e-05]
		[batch 20/20] avg loss: 0.10568007502925374		[learning rate: 3.798e-05]
	Learning Rate: 3.79798e-05
	LOSS [training: 0.10812945005714267 | validation: 0.07334266510891173]
	TIME [epoch: 8.19 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09457036825027544		[learning rate: 3.7911e-05]
		[batch 20/20] avg loss: 0.09655272484030178		[learning rate: 3.7842e-05]
	Learning Rate: 3.7842e-05
	LOSS [training: 0.0955615465452886 | validation: 0.08000423948185713]
	TIME [epoch: 8.19 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09380950815319086		[learning rate: 3.7773e-05]
		[batch 20/20] avg loss: 0.08189043058885333		[learning rate: 3.7705e-05]
	Learning Rate: 3.77046e-05
	LOSS [training: 0.0878499693710221 | validation: 0.07078647812535979]
	TIME [epoch: 8.19 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0967977757307302		[learning rate: 3.7636e-05]
		[batch 20/20] avg loss: 0.08408193839208543		[learning rate: 3.7568e-05]
	Learning Rate: 3.75678e-05
	LOSS [training: 0.09043985706140782 | validation: 0.07586936492239126]
	TIME [epoch: 8.21 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10694923011896806		[learning rate: 3.75e-05]
		[batch 20/20] avg loss: 0.09334279010947281		[learning rate: 3.7431e-05]
	Learning Rate: 3.74315e-05
	LOSS [training: 0.10014601011422045 | validation: 0.07491634437533473]
	TIME [epoch: 8.19 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09807509520314982		[learning rate: 3.7363e-05]
		[batch 20/20] avg loss: 0.08614446806672944		[learning rate: 3.7296e-05]
	Learning Rate: 3.72956e-05
	LOSS [training: 0.09210978163493963 | validation: 0.06417414160755162]
	TIME [epoch: 8.2 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09644939943274199		[learning rate: 3.7228e-05]
		[batch 20/20] avg loss: 0.08406177731286622		[learning rate: 3.716e-05]
	Learning Rate: 3.71603e-05
	LOSS [training: 0.09025558837280412 | validation: 0.07055222397044728]
	TIME [epoch: 8.18 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08906643750806935		[learning rate: 3.7093e-05]
		[batch 20/20] avg loss: 0.09276715103309821		[learning rate: 3.7025e-05]
	Learning Rate: 3.70254e-05
	LOSS [training: 0.09091679427058376 | validation: 0.06339816038967276]
	TIME [epoch: 8.2 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09301661628455106		[learning rate: 3.6958e-05]
		[batch 20/20] avg loss: 0.09242136628790609		[learning rate: 3.6891e-05]
	Learning Rate: 3.68911e-05
	LOSS [training: 0.09271899128622858 | validation: 0.0582514496900825]
	TIME [epoch: 8.2 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09023813583105643		[learning rate: 3.6824e-05]
		[batch 20/20] avg loss: 0.09159211728442186		[learning rate: 3.6757e-05]
	Learning Rate: 3.67572e-05
	LOSS [training: 0.09091512655773915 | validation: 0.06727411669239727]
	TIME [epoch: 8.19 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10590477266380194		[learning rate: 3.669e-05]
		[batch 20/20] avg loss: 0.08788678824815406		[learning rate: 3.6624e-05]
	Learning Rate: 3.66238e-05
	LOSS [training: 0.09689578045597799 | validation: 0.06852404901559209]
	TIME [epoch: 8.18 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08853335563443805		[learning rate: 3.6557e-05]
		[batch 20/20] avg loss: 0.09598871377325999		[learning rate: 3.6491e-05]
	Learning Rate: 3.64909e-05
	LOSS [training: 0.09226103470384903 | validation: 0.06497550572059678]
	TIME [epoch: 8.21 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09180497281284067		[learning rate: 3.6425e-05]
		[batch 20/20] avg loss: 0.08946851719734784		[learning rate: 3.6358e-05]
	Learning Rate: 3.63584e-05
	LOSS [training: 0.09063674500509425 | validation: 0.06877118628312578]
	TIME [epoch: 8.19 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09509808422737563		[learning rate: 3.6292e-05]
		[batch 20/20] avg loss: 0.097554102063346		[learning rate: 3.6226e-05]
	Learning Rate: 3.62265e-05
	LOSS [training: 0.09632609314536081 | validation: 0.07732530754063177]
	TIME [epoch: 8.18 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08756201080012611		[learning rate: 3.6161e-05]
		[batch 20/20] avg loss: 0.09834759522469612		[learning rate: 3.6095e-05]
	Learning Rate: 3.6095e-05
	LOSS [training: 0.0929548030124111 | validation: 0.06087063095937348]
	TIME [epoch: 8.19 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0897738789726967		[learning rate: 3.6029e-05]
		[batch 20/20] avg loss: 0.08370476671895286		[learning rate: 3.5964e-05]
	Learning Rate: 3.5964e-05
	LOSS [training: 0.08673932284582478 | validation: 0.06596036985871523]
	TIME [epoch: 8.2 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09155744641719024		[learning rate: 3.5899e-05]
		[batch 20/20] avg loss: 0.08462230655702267		[learning rate: 3.5834e-05]
	Learning Rate: 3.58335e-05
	LOSS [training: 0.08808987648710645 | validation: 0.06421518282446186]
	TIME [epoch: 8.19 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09393113183127025		[learning rate: 3.5768e-05]
		[batch 20/20] avg loss: 0.08339404092423289		[learning rate: 3.5703e-05]
	Learning Rate: 3.57035e-05
	LOSS [training: 0.08866258637775157 | validation: 0.06274973236116485]
	TIME [epoch: 8.19 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09046290790030134		[learning rate: 3.5639e-05]
		[batch 20/20] avg loss: 0.08480551349597042		[learning rate: 3.5574e-05]
	Learning Rate: 3.55739e-05
	LOSS [training: 0.08763421069813589 | validation: 0.06510166258407385]
	TIME [epoch: 8.19 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09609466728263939		[learning rate: 3.5509e-05]
		[batch 20/20] avg loss: 0.08204661808767001		[learning rate: 3.5445e-05]
	Learning Rate: 3.54448e-05
	LOSS [training: 0.0890706426851547 | validation: 0.06839462073278088]
	TIME [epoch: 8.2 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08938099007971931		[learning rate: 3.538e-05]
		[batch 20/20] avg loss: 0.09160960697652569		[learning rate: 3.5316e-05]
	Learning Rate: 3.53162e-05
	LOSS [training: 0.09049529852812249 | validation: 0.06430832414540662]
	TIME [epoch: 8.19 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08548225040606708		[learning rate: 3.5252e-05]
		[batch 20/20] avg loss: 0.09422849276042841		[learning rate: 3.5188e-05]
	Learning Rate: 3.5188e-05
	LOSS [training: 0.08985537158324775 | validation: 0.07064975181188263]
	TIME [epoch: 8.18 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09103607927243346		[learning rate: 3.5124e-05]
		[batch 20/20] avg loss: 0.0895787586518709		[learning rate: 3.506e-05]
	Learning Rate: 3.50603e-05
	LOSS [training: 0.09030741896215218 | validation: 0.05653479188193912]
	TIME [epoch: 8.19 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0861846784822938		[learning rate: 3.4997e-05]
		[batch 20/20] avg loss: 0.0902071031193264		[learning rate: 3.4933e-05]
	Learning Rate: 3.49331e-05
	LOSS [training: 0.0881958908008101 | validation: 0.06008964037705042]
	TIME [epoch: 8.21 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08985503296267604		[learning rate: 3.487e-05]
		[batch 20/20] avg loss: 0.088212393369303		[learning rate: 3.4806e-05]
	Learning Rate: 3.48063e-05
	LOSS [training: 0.0890337131659895 | validation: 0.0708464794845659]
	TIME [epoch: 8.19 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09504045459791935		[learning rate: 3.4743e-05]
		[batch 20/20] avg loss: 0.08592610698838836		[learning rate: 3.468e-05]
	Learning Rate: 3.468e-05
	LOSS [training: 0.09048328079315383 | validation: 0.0618216299470139]
	TIME [epoch: 8.19 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08631290849365067		[learning rate: 3.4617e-05]
		[batch 20/20] avg loss: 0.09409342202524472		[learning rate: 3.4554e-05]
	Learning Rate: 3.45541e-05
	LOSS [training: 0.09020316525944772 | validation: 0.0757268982394512]
	TIME [epoch: 8.19 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08485911405912652		[learning rate: 3.4491e-05]
		[batch 20/20] avg loss: 0.09582474476315456		[learning rate: 3.4429e-05]
	Learning Rate: 3.44287e-05
	LOSS [training: 0.09034192941114053 | validation: 0.07128104779275367]
	TIME [epoch: 8.21 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08665963153223198		[learning rate: 3.4366e-05]
		[batch 20/20] avg loss: 0.10670010226209062		[learning rate: 3.4304e-05]
	Learning Rate: 3.43038e-05
	LOSS [training: 0.0966798668971613 | validation: 0.07824555747953246]
	TIME [epoch: 8.19 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.11667942795390499		[learning rate: 3.4241e-05]
		[batch 20/20] avg loss: 0.10376215849233791		[learning rate: 3.4179e-05]
	Learning Rate: 3.41793e-05
	LOSS [training: 0.11022079322312146 | validation: 0.07952647240520536]
	TIME [epoch: 8.18 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08843956008282369		[learning rate: 3.4117e-05]
		[batch 20/20] avg loss: 0.10018331297630141		[learning rate: 3.4055e-05]
	Learning Rate: 3.40553e-05
	LOSS [training: 0.09431143652956253 | validation: 0.06558042580960756]
	TIME [epoch: 8.18 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08852026099760014		[learning rate: 3.3993e-05]
		[batch 20/20] avg loss: 0.10064506125744448		[learning rate: 3.3932e-05]
	Learning Rate: 3.39317e-05
	LOSS [training: 0.09458266112752231 | validation: 0.06820874877764821]
	TIME [epoch: 8.21 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10665798521168741		[learning rate: 3.387e-05]
		[batch 20/20] avg loss: 0.08416471158062462		[learning rate: 3.3809e-05]
	Learning Rate: 3.38085e-05
	LOSS [training: 0.09541134839615602 | validation: 0.06604002128524813]
	TIME [epoch: 8.18 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08724580835624791		[learning rate: 3.3747e-05]
		[batch 20/20] avg loss: 0.09093387371496349		[learning rate: 3.3686e-05]
	Learning Rate: 3.36858e-05
	LOSS [training: 0.08908984103560572 | validation: 0.05800928867167421]
	TIME [epoch: 8.19 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09516343724573051		[learning rate: 3.3625e-05]
		[batch 20/20] avg loss: 0.08404589681256118		[learning rate: 3.3564e-05]
	Learning Rate: 3.35636e-05
	LOSS [training: 0.08960466702914587 | validation: 0.06429188584883569]
	TIME [epoch: 8.18 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09544126476053923		[learning rate: 3.3503e-05]
		[batch 20/20] avg loss: 0.08772963288911224		[learning rate: 3.3442e-05]
	Learning Rate: 3.34418e-05
	LOSS [training: 0.09158544882482575 | validation: 0.06920608124191854]
	TIME [epoch: 8.2 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08514335246176237		[learning rate: 3.3381e-05]
		[batch 20/20] avg loss: 0.08756260217190395		[learning rate: 3.332e-05]
	Learning Rate: 3.33204e-05
	LOSS [training: 0.08635297731683315 | validation: 0.06327222304238397]
	TIME [epoch: 8.18 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10141966198432537		[learning rate: 3.326e-05]
		[batch 20/20] avg loss: 0.08749825612434481		[learning rate: 3.32e-05]
	Learning Rate: 3.31995e-05
	LOSS [training: 0.09445895905433507 | validation: 0.07043624995441353]
	TIME [epoch: 8.19 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09073042850288685		[learning rate: 3.3139e-05]
		[batch 20/20] avg loss: 0.08633628208085167		[learning rate: 3.3079e-05]
	Learning Rate: 3.3079e-05
	LOSS [training: 0.08853335529186926 | validation: 0.06560152168262604]
	TIME [epoch: 8.18 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09295201076794053		[learning rate: 3.3019e-05]
		[batch 20/20] avg loss: 0.08970439616970065		[learning rate: 3.2959e-05]
	Learning Rate: 3.2959e-05
	LOSS [training: 0.09132820346882059 | validation: 0.0623943936898906]
	TIME [epoch: 8.21 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09229992595962269		[learning rate: 3.2899e-05]
		[batch 20/20] avg loss: 0.08939581987801336		[learning rate: 3.2839e-05]
	Learning Rate: 3.28394e-05
	LOSS [training: 0.09084787291881803 | validation: 0.06347019494912184]
	TIME [epoch: 8.19 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0853961384843281		[learning rate: 3.278e-05]
		[batch 20/20] avg loss: 0.08808579444322742		[learning rate: 3.272e-05]
	Learning Rate: 3.27202e-05
	LOSS [training: 0.08674096646377774 | validation: 0.06966537003497394]
	TIME [epoch: 8.18 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09991130825424478		[learning rate: 3.2661e-05]
		[batch 20/20] avg loss: 0.09166870259671026		[learning rate: 3.2601e-05]
	Learning Rate: 3.26014e-05
	LOSS [training: 0.09579000542547751 | validation: 0.06976358688770198]
	TIME [epoch: 8.17 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10096658980859836		[learning rate: 3.2542e-05]
		[batch 20/20] avg loss: 0.08254916634749478		[learning rate: 3.2483e-05]
	Learning Rate: 3.24831e-05
	LOSS [training: 0.09175787807804658 | validation: 0.061757180567568]
	TIME [epoch: 8.2 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08411381619162613		[learning rate: 3.2424e-05]
		[batch 20/20] avg loss: 0.09123237474798684		[learning rate: 3.2365e-05]
	Learning Rate: 3.23652e-05
	LOSS [training: 0.08767309546980648 | validation: 0.06388530861124733]
	TIME [epoch: 8.22 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07867142760940148		[learning rate: 3.2306e-05]
		[batch 20/20] avg loss: 0.0950919803764208		[learning rate: 3.2248e-05]
	Learning Rate: 3.22478e-05
	LOSS [training: 0.08688170399291113 | validation: 0.05841525807984075]
	TIME [epoch: 8.21 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09359700686079817		[learning rate: 3.2189e-05]
		[batch 20/20] avg loss: 0.08247297022706744		[learning rate: 3.2131e-05]
	Learning Rate: 3.21308e-05
	LOSS [training: 0.0880349885439328 | validation: 0.06553526862883823]
	TIME [epoch: 8.22 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08507338020968616		[learning rate: 3.2072e-05]
		[batch 20/20] avg loss: 0.09407526250244277		[learning rate: 3.2014e-05]
	Learning Rate: 3.20142e-05
	LOSS [training: 0.08957432135606448 | validation: 0.06147274283721273]
	TIME [epoch: 8.24 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08632948640803563		[learning rate: 3.1956e-05]
		[batch 20/20] avg loss: 0.08958229542337268		[learning rate: 3.1898e-05]
	Learning Rate: 3.1898e-05
	LOSS [training: 0.08795589091570417 | validation: 0.06027404945622787]
	TIME [epoch: 8.2 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08848323223955971		[learning rate: 3.184e-05]
		[batch 20/20] avg loss: 0.08976022085897165		[learning rate: 3.1782e-05]
	Learning Rate: 3.17822e-05
	LOSS [training: 0.08912172654926567 | validation: 0.061005201975141345]
	TIME [epoch: 8.21 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08399271550569778		[learning rate: 3.1725e-05]
		[batch 20/20] avg loss: 0.09077294138368316		[learning rate: 3.1667e-05]
	Learning Rate: 3.16669e-05
	LOSS [training: 0.08738282844469049 | validation: 0.07531892519707584]
	TIME [epoch: 8.22 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09124700427926184		[learning rate: 3.1609e-05]
		[batch 20/20] avg loss: 0.08338251969105208		[learning rate: 3.1552e-05]
	Learning Rate: 3.1552e-05
	LOSS [training: 0.08731476198515697 | validation: 0.06221404450524207]
	TIME [epoch: 8.24 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08489159108590479		[learning rate: 3.1495e-05]
		[batch 20/20] avg loss: 0.09588289332874493		[learning rate: 3.1437e-05]
	Learning Rate: 3.14375e-05
	LOSS [training: 0.09038724220732484 | validation: 0.06909053651775557]
	TIME [epoch: 8.22 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09673321199989673		[learning rate: 3.138e-05]
		[batch 20/20] avg loss: 0.08754670653593019		[learning rate: 3.1323e-05]
	Learning Rate: 3.13234e-05
	LOSS [training: 0.09213995926791345 | validation: 0.06535930186688994]
	TIME [epoch: 8.21 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0977067209079318		[learning rate: 3.1266e-05]
		[batch 20/20] avg loss: 0.08419161885287485		[learning rate: 3.121e-05]
	Learning Rate: 3.12097e-05
	LOSS [training: 0.09094916988040333 | validation: 0.06547197724030959]
	TIME [epoch: 8.22 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09022124122028188		[learning rate: 3.1153e-05]
		[batch 20/20] avg loss: 0.0813735257811625		[learning rate: 3.1096e-05]
	Learning Rate: 3.10964e-05
	LOSS [training: 0.0857973835007222 | validation: 0.06642419778986058]
	TIME [epoch: 8.24 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09052580880115817		[learning rate: 3.104e-05]
		[batch 20/20] avg loss: 0.09361982152625571		[learning rate: 3.0984e-05]
	Learning Rate: 3.09836e-05
	LOSS [training: 0.09207281516370694 | validation: 0.05835862864064596]
	TIME [epoch: 8.23 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08904663314032671		[learning rate: 3.0927e-05]
		[batch 20/20] avg loss: 0.08762353126624572		[learning rate: 3.0871e-05]
	Learning Rate: 3.08711e-05
	LOSS [training: 0.0883350822032862 | validation: 0.060107219678672275]
	TIME [epoch: 8.21 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08162372423697599		[learning rate: 3.0815e-05]
		[batch 20/20] avg loss: 0.08915485537317033		[learning rate: 3.0759e-05]
	Learning Rate: 3.07591e-05
	LOSS [training: 0.08538928980507318 | validation: 0.055300774197435926]
	TIME [epoch: 8.22 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08848933779434123		[learning rate: 3.0703e-05]
		[batch 20/20] avg loss: 0.09091576979469866		[learning rate: 3.0647e-05]
	Learning Rate: 3.06475e-05
	LOSS [training: 0.08970255379451994 | validation: 0.06276817429305684]
	TIME [epoch: 8.24 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08841101759151573		[learning rate: 3.0592e-05]
		[batch 20/20] avg loss: 0.09050696311064975		[learning rate: 3.0536e-05]
	Learning Rate: 3.05363e-05
	LOSS [training: 0.08945899035108272 | validation: 0.06822082309396194]
	TIME [epoch: 8.24 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0929824488294606		[learning rate: 3.0481e-05]
		[batch 20/20] avg loss: 0.0866845189122833		[learning rate: 3.0425e-05]
	Learning Rate: 3.04254e-05
	LOSS [training: 0.08983348387087195 | validation: 0.06396655219148985]
	TIME [epoch: 8.21 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09676888916292244		[learning rate: 3.037e-05]
		[batch 20/20] avg loss: 0.0789281521390785		[learning rate: 3.0315e-05]
	Learning Rate: 3.0315e-05
	LOSS [training: 0.08784852065100048 | validation: 0.06275490671976929]
	TIME [epoch: 8.19 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09248592519576675		[learning rate: 3.026e-05]
		[batch 20/20] avg loss: 0.08352708775601235		[learning rate: 3.0205e-05]
	Learning Rate: 3.0205e-05
	LOSS [training: 0.08800650647588955 | validation: 0.06470248979288391]
	TIME [epoch: 8.23 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09319418668883142		[learning rate: 3.015e-05]
		[batch 20/20] avg loss: 0.0878746547933652		[learning rate: 3.0095e-05]
	Learning Rate: 3.00954e-05
	LOSS [training: 0.0905344207410983 | validation: 0.06543813038367985]
	TIME [epoch: 8.23 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08941944585964252		[learning rate: 3.0041e-05]
		[batch 20/20] avg loss: 0.09110142816638814		[learning rate: 2.9986e-05]
	Learning Rate: 2.99862e-05
	LOSS [training: 0.09026043701301532 | validation: 0.07916738989803387]
	TIME [epoch: 8.22 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10598857112620388		[learning rate: 2.9932e-05]
		[batch 20/20] avg loss: 0.09239161137528598		[learning rate: 2.9877e-05]
	Learning Rate: 2.98774e-05
	LOSS [training: 0.09919009125074493 | validation: 0.06106113404318081]
	TIME [epoch: 8.23 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08817070718962698		[learning rate: 2.9823e-05]
		[batch 20/20] avg loss: 0.08738634753533178		[learning rate: 2.9769e-05]
	Learning Rate: 2.97689e-05
	LOSS [training: 0.0877785273624794 | validation: 0.06684835858906757]
	TIME [epoch: 8.22 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0900821800104552		[learning rate: 2.9715e-05]
		[batch 20/20] avg loss: 0.08527776251600848		[learning rate: 2.9661e-05]
	Learning Rate: 2.96609e-05
	LOSS [training: 0.08767997126323182 | validation: 0.06827130736740605]
	TIME [epoch: 8.23 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0943981175998502		[learning rate: 2.9607e-05]
		[batch 20/20] avg loss: 0.07941808315529537		[learning rate: 2.9553e-05]
	Learning Rate: 2.95533e-05
	LOSS [training: 0.0869081003775728 | validation: 0.058506323988701175]
	TIME [epoch: 8.22 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08641989961890836		[learning rate: 2.95e-05]
		[batch 20/20] avg loss: 0.09354547055606581		[learning rate: 2.9446e-05]
	Learning Rate: 2.9446e-05
	LOSS [training: 0.08998268508748711 | validation: 0.060143983408652964]
	TIME [epoch: 8.22 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08756572808182554		[learning rate: 2.9393e-05]
		[batch 20/20] avg loss: 0.09681040758786952		[learning rate: 2.9339e-05]
	Learning Rate: 2.93391e-05
	LOSS [training: 0.09218806783484754 | validation: 0.08105130090408395]
	TIME [epoch: 8.22 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10244057511532834		[learning rate: 2.9286e-05]
		[batch 20/20] avg loss: 0.08876641024236583		[learning rate: 2.9233e-05]
	Learning Rate: 2.92327e-05
	LOSS [training: 0.09560349267884707 | validation: 0.07216786905337383]
	TIME [epoch: 8.23 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09753683387815695		[learning rate: 2.918e-05]
		[batch 20/20] avg loss: 0.08902989873251996		[learning rate: 2.9127e-05]
	Learning Rate: 2.91266e-05
	LOSS [training: 0.09328336630533848 | validation: 0.07113602042639287]
	TIME [epoch: 8.22 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08778573880020556		[learning rate: 2.9074e-05]
		[batch 20/20] avg loss: 0.08928946079654822		[learning rate: 2.9021e-05]
	Learning Rate: 2.90209e-05
	LOSS [training: 0.0885375997983769 | validation: 0.06726026022326324]
	TIME [epoch: 8.22 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08485658060917269		[learning rate: 2.8968e-05]
		[batch 20/20] avg loss: 0.09589012844473269		[learning rate: 2.8916e-05]
	Learning Rate: 2.89156e-05
	LOSS [training: 0.09037335452695269 | validation: 0.06159607634040629]
	TIME [epoch: 8.22 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08347034562626597		[learning rate: 2.8863e-05]
		[batch 20/20] avg loss: 0.09511121425713856		[learning rate: 2.8811e-05]
	Learning Rate: 2.88106e-05
	LOSS [training: 0.08929077994170224 | validation: 0.0674470455377947]
	TIME [epoch: 8.23 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0802608824659253		[learning rate: 2.8758e-05]
		[batch 20/20] avg loss: 0.09168170207524856		[learning rate: 2.8706e-05]
	Learning Rate: 2.87061e-05
	LOSS [training: 0.08597129227058692 | validation: 0.07165704161391233]
	TIME [epoch: 8.22 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08384558405177038		[learning rate: 2.8654e-05]
		[batch 20/20] avg loss: 0.09552744078693562		[learning rate: 2.8602e-05]
	Learning Rate: 2.86019e-05
	LOSS [training: 0.08968651241935299 | validation: 0.0688351683565696]
	TIME [epoch: 8.24 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08461542232414905		[learning rate: 2.855e-05]
		[batch 20/20] avg loss: 0.08894719561196254		[learning rate: 2.8498e-05]
	Learning Rate: 2.84981e-05
	LOSS [training: 0.08678130896805579 | validation: 0.06198575308244422]
	TIME [epoch: 8.23 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0919323721358979		[learning rate: 2.8446e-05]
		[batch 20/20] avg loss: 0.08122528507679974		[learning rate: 2.8395e-05]
	Learning Rate: 2.83947e-05
	LOSS [training: 0.08657882860634881 | validation: 0.06135412182093465]
	TIME [epoch: 8.25 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07969948607283438		[learning rate: 2.8343e-05]
		[batch 20/20] avg loss: 0.08853831678195181		[learning rate: 2.8292e-05]
	Learning Rate: 2.82916e-05
	LOSS [training: 0.08411890142739309 | validation: 0.06159644499744642]
	TIME [epoch: 8.22 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0915946682729327		[learning rate: 2.824e-05]
		[batch 20/20] avg loss: 0.08231999920095277		[learning rate: 2.8189e-05]
	Learning Rate: 2.8189e-05
	LOSS [training: 0.08695733373694274 | validation: 0.064525053060013]
	TIME [epoch: 8.22 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08540869627082977		[learning rate: 2.8138e-05]
		[batch 20/20] avg loss: 0.08825822943491143		[learning rate: 2.8087e-05]
	Learning Rate: 2.80867e-05
	LOSS [training: 0.08683346285287058 | validation: 0.06270370901314531]
	TIME [epoch: 8.22 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09072399666899474		[learning rate: 2.8036e-05]
		[batch 20/20] avg loss: 0.0819845796432215		[learning rate: 2.7985e-05]
	Learning Rate: 2.79847e-05
	LOSS [training: 0.08635428815610811 | validation: 0.06533353824761978]
	TIME [epoch: 8.24 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09226691654813637		[learning rate: 2.7934e-05]
		[batch 20/20] avg loss: 0.08241227171867624		[learning rate: 2.7883e-05]
	Learning Rate: 2.78832e-05
	LOSS [training: 0.0873395941334063 | validation: 0.06457290370232785]
	TIME [epoch: 8.23 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09399962819245275		[learning rate: 2.7833e-05]
		[batch 20/20] avg loss: 0.07638823655869413		[learning rate: 2.7782e-05]
	Learning Rate: 2.7782e-05
	LOSS [training: 0.08519393237557342 | validation: 0.0718058560485239]
	TIME [epoch: 8.21 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08438275907547985		[learning rate: 2.7732e-05]
		[batch 20/20] avg loss: 0.09874636837680506		[learning rate: 2.7681e-05]
	Learning Rate: 2.76812e-05
	LOSS [training: 0.09156456372614245 | validation: 0.06623961942767445]
	TIME [epoch: 8.22 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09480314880423778		[learning rate: 2.7631e-05]
		[batch 20/20] avg loss: 0.10223461191593437		[learning rate: 2.7581e-05]
	Learning Rate: 2.75807e-05
	LOSS [training: 0.09851888036008608 | validation: 0.09093138077216767]
	TIME [epoch: 8.24 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.1091152872794475		[learning rate: 2.7531e-05]
		[batch 20/20] avg loss: 0.10722761877675632		[learning rate: 2.7481e-05]
	Learning Rate: 2.74806e-05
	LOSS [training: 0.1081714530281019 | validation: 0.0694641836760066]
	TIME [epoch: 8.22 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10103813681828333		[learning rate: 2.7431e-05]
		[batch 20/20] avg loss: 0.07765142558316603		[learning rate: 2.7381e-05]
	Learning Rate: 2.73809e-05
	LOSS [training: 0.08934478120072467 | validation: 0.06582993373856752]
	TIME [epoch: 8.21 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10159090079549168		[learning rate: 2.7331e-05]
		[batch 20/20] avg loss: 0.09347358994580932		[learning rate: 2.7282e-05]
	Learning Rate: 2.72815e-05
	LOSS [training: 0.09753224537065049 | validation: 0.06221787887686049]
	TIME [epoch: 8.22 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08959904433938916		[learning rate: 2.7232e-05]
		[batch 20/20] avg loss: 0.09168865949540157		[learning rate: 2.7183e-05]
	Learning Rate: 2.71825e-05
	LOSS [training: 0.09064385191739537 | validation: 0.06067576808584061]
	TIME [epoch: 8.24 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09430005851067974		[learning rate: 2.7133e-05]
		[batch 20/20] avg loss: 0.08011906203581583		[learning rate: 2.7084e-05]
	Learning Rate: 2.70839e-05
	LOSS [training: 0.08720956027324778 | validation: 0.061989982246671196]
	TIME [epoch: 8.23 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08575909095595624		[learning rate: 2.7035e-05]
		[batch 20/20] avg loss: 0.08284864294146979		[learning rate: 2.6986e-05]
	Learning Rate: 2.69856e-05
	LOSS [training: 0.08430386694871303 | validation: 0.0588212267960572]
	TIME [epoch: 8.18 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08452342682549249		[learning rate: 2.6937e-05]
		[batch 20/20] avg loss: 0.09554178293060961		[learning rate: 2.6888e-05]
	Learning Rate: 2.68876e-05
	LOSS [training: 0.09003260487805105 | validation: 0.08446147844568702]
	TIME [epoch: 8.18 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10423229194837261		[learning rate: 2.6839e-05]
		[batch 20/20] avg loss: 0.08349972429919604		[learning rate: 2.679e-05]
	Learning Rate: 2.67901e-05
	LOSS [training: 0.09386600812378432 | validation: 0.06319743592391225]
	TIME [epoch: 8.22 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07723151459780234		[learning rate: 2.6741e-05]
		[batch 20/20] avg loss: 0.09511339701037973		[learning rate: 2.6693e-05]
	Learning Rate: 2.66928e-05
	LOSS [training: 0.08617245580409102 | validation: 0.07183848852343423]
	TIME [epoch: 8.21 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08636533750311438		[learning rate: 2.6644e-05]
		[batch 20/20] avg loss: 0.08481601831230283		[learning rate: 2.6596e-05]
	Learning Rate: 2.6596e-05
	LOSS [training: 0.0855906779077086 | validation: 0.07001499397569975]
	TIME [epoch: 8.22 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08605435762491716		[learning rate: 2.6548e-05]
		[batch 20/20] avg loss: 0.08684824560259374		[learning rate: 2.6499e-05]
	Learning Rate: 2.64994e-05
	LOSS [training: 0.08645130161375544 | validation: 0.055200541070460137]
	TIME [epoch: 8.21 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0793159837017769		[learning rate: 2.6451e-05]
		[batch 20/20] avg loss: 0.08424315192102258		[learning rate: 2.6403e-05]
	Learning Rate: 2.64033e-05
	LOSS [training: 0.08177956781139974 | validation: 0.05796283819467002]
	TIME [epoch: 8.23 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08654084795435671		[learning rate: 2.6355e-05]
		[batch 20/20] avg loss: 0.0943246529344793		[learning rate: 2.6307e-05]
	Learning Rate: 2.63075e-05
	LOSS [training: 0.09043275044441801 | validation: 0.06281582643371982]
	TIME [epoch: 8.2 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08402075169951935		[learning rate: 2.626e-05]
		[batch 20/20] avg loss: 0.0932143979034327		[learning rate: 2.6212e-05]
	Learning Rate: 2.6212e-05
	LOSS [training: 0.08861757480147604 | validation: 0.06186448186349078]
	TIME [epoch: 8.18 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08824249048696273		[learning rate: 2.6164e-05]
		[batch 20/20] avg loss: 0.093401982891378		[learning rate: 2.6117e-05]
	Learning Rate: 2.61169e-05
	LOSS [training: 0.09082223668917036 | validation: 0.06723297691445224]
	TIME [epoch: 8.21 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08603977624479377		[learning rate: 2.6069e-05]
		[batch 20/20] avg loss: 0.0952982018042843		[learning rate: 2.6022e-05]
	Learning Rate: 2.60221e-05
	LOSS [training: 0.09066898902453904 | validation: 0.0684745527485569]
	TIME [epoch: 8.22 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08879507031912016		[learning rate: 2.5975e-05]
		[batch 20/20] avg loss: 0.08155069378441		[learning rate: 2.5928e-05]
	Learning Rate: 2.59277e-05
	LOSS [training: 0.08517288205176507 | validation: 0.057725839490119064]
	TIME [epoch: 8.19 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09905485854656201		[learning rate: 2.5881e-05]
		[batch 20/20] avg loss: 0.08496833174024532		[learning rate: 2.5834e-05]
	Learning Rate: 2.58336e-05
	LOSS [training: 0.09201159514340365 | validation: 0.06597113795933887]
	TIME [epoch: 8.18 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08879611722940499		[learning rate: 2.5787e-05]
		[batch 20/20] avg loss: 0.08625584751644413		[learning rate: 2.574e-05]
	Learning Rate: 2.57398e-05
	LOSS [training: 0.08752598237292455 | validation: 0.06079305789851926]
	TIME [epoch: 8.18 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09162332420638555		[learning rate: 2.5693e-05]
		[batch 20/20] avg loss: 0.08239554996786749		[learning rate: 2.5646e-05]
	Learning Rate: 2.56464e-05
	LOSS [training: 0.08700943708712651 | validation: 0.06133543015258314]
	TIME [epoch: 8.2 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08368355569281234		[learning rate: 2.56e-05]
		[batch 20/20] avg loss: 0.08996588482058764		[learning rate: 2.5553e-05]
	Learning Rate: 2.55533e-05
	LOSS [training: 0.08682472025669999 | validation: 0.06292035093180944]
	TIME [epoch: 8.19 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08605135205391506		[learning rate: 2.5507e-05]
		[batch 20/20] avg loss: 0.09910256100245937		[learning rate: 2.5461e-05]
	Learning Rate: 2.54606e-05
	LOSS [training: 0.09257695652818722 | validation: 0.07724224216573107]
	TIME [epoch: 8.18 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09635157300480454		[learning rate: 2.5414e-05]
		[batch 20/20] avg loss: 0.09107187010111922		[learning rate: 2.5368e-05]
	Learning Rate: 2.53682e-05
	LOSS [training: 0.09371172155296187 | validation: 0.0628872272394328]
	TIME [epoch: 8.18 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09420502475418394		[learning rate: 2.5322e-05]
		[batch 20/20] avg loss: 0.09588628595743082		[learning rate: 2.5276e-05]
	Learning Rate: 2.52761e-05
	LOSS [training: 0.09504565535580736 | validation: 0.06561144100819022]
	TIME [epoch: 8.2 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08197484270225097		[learning rate: 2.523e-05]
		[batch 20/20] avg loss: 0.08569377212808488		[learning rate: 2.5184e-05]
	Learning Rate: 2.51844e-05
	LOSS [training: 0.08383430741516792 | validation: 0.05220137121334409]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1746.pth
	Model improved!!!
EPOCH 1747/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09327557383416929		[learning rate: 2.5139e-05]
		[batch 20/20] avg loss: 0.07744533307442036		[learning rate: 2.5093e-05]
	Learning Rate: 2.5093e-05
	LOSS [training: 0.08536045345429483 | validation: 0.06064079207299283]
	TIME [epoch: 8.19 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0934513926412199		[learning rate: 2.5047e-05]
		[batch 20/20] avg loss: 0.08617526349328722		[learning rate: 2.5002e-05]
	Learning Rate: 2.50019e-05
	LOSS [training: 0.08981332806725355 | validation: 0.06145405677256631]
	TIME [epoch: 8.18 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07800177737717306		[learning rate: 2.4957e-05]
		[batch 20/20] avg loss: 0.09863672839856699		[learning rate: 2.4911e-05]
	Learning Rate: 2.49112e-05
	LOSS [training: 0.08831925288787004 | validation: 0.07257149327201581]
	TIME [epoch: 8.2 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08793378877650344		[learning rate: 2.4866e-05]
		[batch 20/20] avg loss: 0.08672541412149329		[learning rate: 2.4821e-05]
	Learning Rate: 2.48208e-05
	LOSS [training: 0.08732960144899837 | validation: 0.057659763352746134]
	TIME [epoch: 8.18 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08388837384436525		[learning rate: 2.4776e-05]
		[batch 20/20] avg loss: 0.08523465598631241		[learning rate: 2.4731e-05]
	Learning Rate: 2.47307e-05
	LOSS [training: 0.08456151491533884 | validation: 0.06174438928614269]
	TIME [epoch: 8.18 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07874630706621985		[learning rate: 2.4686e-05]
		[batch 20/20] avg loss: 0.08806744959966534		[learning rate: 2.4641e-05]
	Learning Rate: 2.4641e-05
	LOSS [training: 0.0834068783329426 | validation: 0.05654498700604417]
	TIME [epoch: 8.18 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0958458782841719		[learning rate: 2.4596e-05]
		[batch 20/20] avg loss: 0.08234410042108997		[learning rate: 2.4552e-05]
	Learning Rate: 2.45516e-05
	LOSS [training: 0.08909498935263094 | validation: 0.06444016301652389]
	TIME [epoch: 8.2 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09272258329708212		[learning rate: 2.4507e-05]
		[batch 20/20] avg loss: 0.08684095913154345		[learning rate: 2.4462e-05]
	Learning Rate: 2.44625e-05
	LOSS [training: 0.08978177121431277 | validation: 0.07039782619689071]
	TIME [epoch: 8.18 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0919525151682254		[learning rate: 2.4418e-05]
		[batch 20/20] avg loss: 0.08350568904633471		[learning rate: 2.4374e-05]
	Learning Rate: 2.43737e-05
	LOSS [training: 0.08772910210728006 | validation: 0.0630824161972169]
	TIME [epoch: 8.18 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08765037919870364		[learning rate: 2.4329e-05]
		[batch 20/20] avg loss: 0.09055426512720376		[learning rate: 2.4285e-05]
	Learning Rate: 2.42852e-05
	LOSS [training: 0.08910232216295369 | validation: 0.06520402198338485]
	TIME [epoch: 8.18 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07988200185391295		[learning rate: 2.4241e-05]
		[batch 20/20] avg loss: 0.09189567814603204		[learning rate: 2.4197e-05]
	Learning Rate: 2.41971e-05
	LOSS [training: 0.08588883999997249 | validation: 0.06998424512849072]
	TIME [epoch: 8.2 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08486484895286622		[learning rate: 2.4153e-05]
		[batch 20/20] avg loss: 0.08639821082600196		[learning rate: 2.4109e-05]
	Learning Rate: 2.41093e-05
	LOSS [training: 0.0856315298894341 | validation: 0.06295394795447253]
	TIME [epoch: 8.18 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08456539460858495		[learning rate: 2.4065e-05]
		[batch 20/20] avg loss: 0.08958763055038194		[learning rate: 2.4022e-05]
	Learning Rate: 2.40218e-05
	LOSS [training: 0.08707651257948343 | validation: 0.06161614582581005]
	TIME [epoch: 8.18 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08065276417200365		[learning rate: 2.3978e-05]
		[batch 20/20] avg loss: 0.09328375258792203		[learning rate: 2.3935e-05]
	Learning Rate: 2.39346e-05
	LOSS [training: 0.08696825837996286 | validation: 0.05978131479418834]
	TIME [epoch: 8.18 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08907263173763721		[learning rate: 2.3891e-05]
		[batch 20/20] avg loss: 0.09285957483848725		[learning rate: 2.3848e-05]
	Learning Rate: 2.38477e-05
	LOSS [training: 0.09096610328806223 | validation: 0.052499806056013995]
	TIME [epoch: 8.2 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09162903357570337		[learning rate: 2.3804e-05]
		[batch 20/20] avg loss: 0.09042225329796114		[learning rate: 2.3761e-05]
	Learning Rate: 2.37612e-05
	LOSS [training: 0.09102564343683224 | validation: 0.06302088214921735]
	TIME [epoch: 8.19 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09391956790886878		[learning rate: 2.3718e-05]
		[batch 20/20] avg loss: 0.08300187999763071		[learning rate: 2.3675e-05]
	Learning Rate: 2.3675e-05
	LOSS [training: 0.08846072395324975 | validation: 0.061918673421298795]
	TIME [epoch: 8.17 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08966590305218816		[learning rate: 2.3632e-05]
		[batch 20/20] avg loss: 0.0864171821107828		[learning rate: 2.3589e-05]
	Learning Rate: 2.35891e-05
	LOSS [training: 0.08804154258148547 | validation: 0.06097559684668865]
	TIME [epoch: 8.18 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0795116679006332		[learning rate: 2.3546e-05]
		[batch 20/20] avg loss: 0.087435185124663		[learning rate: 2.3503e-05]
	Learning Rate: 2.35034e-05
	LOSS [training: 0.08347342651264811 | validation: 0.05974323694072586]
	TIME [epoch: 8.21 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09390602226211457		[learning rate: 2.3461e-05]
		[batch 20/20] avg loss: 0.07760804525181221		[learning rate: 2.3418e-05]
	Learning Rate: 2.34182e-05
	LOSS [training: 0.08575703375696338 | validation: 0.06323992982369189]
	TIME [epoch: 8.19 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09282015522774248		[learning rate: 2.3376e-05]
		[batch 20/20] avg loss: 0.08984124295995359		[learning rate: 2.3333e-05]
	Learning Rate: 2.33332e-05
	LOSS [training: 0.09133069909384804 | validation: 0.06883138939504696]
	TIME [epoch: 8.18 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09949581447602861		[learning rate: 2.3291e-05]
		[batch 20/20] avg loss: 0.0826789904629409		[learning rate: 2.3248e-05]
	Learning Rate: 2.32485e-05
	LOSS [training: 0.09108740246948475 | validation: 0.06993710012688212]
	TIME [epoch: 8.19 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09649274628881004		[learning rate: 2.3206e-05]
		[batch 20/20] avg loss: 0.10504532280800076		[learning rate: 2.3164e-05]
	Learning Rate: 2.31641e-05
	LOSS [training: 0.1007690345484054 | validation: 0.07104181763615508]
	TIME [epoch: 8.2 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10293689073337789		[learning rate: 2.3122e-05]
		[batch 20/20] avg loss: 0.08003140157919648		[learning rate: 2.308e-05]
	Learning Rate: 2.30801e-05
	LOSS [training: 0.09148414615628718 | validation: 0.06601052518784555]
	TIME [epoch: 8.19 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09031839257593448		[learning rate: 2.3038e-05]
		[batch 20/20] avg loss: 0.08144587502948937		[learning rate: 2.2996e-05]
	Learning Rate: 2.29963e-05
	LOSS [training: 0.08588213380271194 | validation: 0.07068805486660405]
	TIME [epoch: 8.17 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08168562368828465		[learning rate: 2.2955e-05]
		[batch 20/20] avg loss: 0.08722177618065079		[learning rate: 2.2913e-05]
	Learning Rate: 2.29128e-05
	LOSS [training: 0.0844536999344677 | validation: 0.06729524789351347]
	TIME [epoch: 8.18 sec]
EPOCH 1773/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08222865527607212		[learning rate: 2.2871e-05]
		[batch 20/20] avg loss: 0.086165989240794		[learning rate: 2.283e-05]
	Learning Rate: 2.28297e-05
	LOSS [training: 0.08419732225843307 | validation: 0.05940092586653634]
	TIME [epoch: 8.2 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08537441490472661		[learning rate: 2.2788e-05]
		[batch 20/20] avg loss: 0.08706914767736046		[learning rate: 2.2747e-05]
	Learning Rate: 2.27468e-05
	LOSS [training: 0.08622178129104355 | validation: 0.07021844154131783]
	TIME [epoch: 8.19 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08774167884419329		[learning rate: 2.2706e-05]
		[batch 20/20] avg loss: 0.0875215321877202		[learning rate: 2.2664e-05]
	Learning Rate: 2.26643e-05
	LOSS [training: 0.08763160551595671 | validation: 0.0643155132225646]
	TIME [epoch: 8.18 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08442926839727194		[learning rate: 2.2623e-05]
		[batch 20/20] avg loss: 0.09261608361680776		[learning rate: 2.2582e-05]
	Learning Rate: 2.2582e-05
	LOSS [training: 0.08852267600703984 | validation: 0.06613895328131217]
	TIME [epoch: 8.18 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08321735867670972		[learning rate: 2.2541e-05]
		[batch 20/20] avg loss: 0.08937071354495656		[learning rate: 2.25e-05]
	Learning Rate: 2.25001e-05
	LOSS [training: 0.08629403611083314 | validation: 0.06766087810909202]
	TIME [epoch: 8.19 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.089015707542545		[learning rate: 2.2459e-05]
		[batch 20/20] avg loss: 0.08566407425151831		[learning rate: 2.2418e-05]
	Learning Rate: 2.24184e-05
	LOSS [training: 0.08733989089703167 | validation: 0.06704621737387423]
	TIME [epoch: 8.18 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09477033433473968		[learning rate: 2.2378e-05]
		[batch 20/20] avg loss: 0.07940326857246652		[learning rate: 2.2337e-05]
	Learning Rate: 2.23371e-05
	LOSS [training: 0.0870868014536031 | validation: 0.06860058848146966]
	TIME [epoch: 8.18 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09065490675959678		[learning rate: 2.2297e-05]
		[batch 20/20] avg loss: 0.0815363798270107		[learning rate: 2.2256e-05]
	Learning Rate: 2.2256e-05
	LOSS [training: 0.08609564329330374 | validation: 0.06574760963714538]
	TIME [epoch: 8.17 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08737356635451621		[learning rate: 2.2216e-05]
		[batch 20/20] avg loss: 0.08023961072537782		[learning rate: 2.2175e-05]
	Learning Rate: 2.21753e-05
	LOSS [training: 0.08380658853994702 | validation: 0.056529882809434495]
	TIME [epoch: 8.19 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0785059836611942		[learning rate: 2.2135e-05]
		[batch 20/20] avg loss: 0.0876539470456392		[learning rate: 2.2095e-05]
	Learning Rate: 2.20948e-05
	LOSS [training: 0.08307996535341669 | validation: 0.05861515321224656]
	TIME [epoch: 8.18 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08394891807618526		[learning rate: 2.2055e-05]
		[batch 20/20] avg loss: 0.08837698965158008		[learning rate: 2.2015e-05]
	Learning Rate: 2.20146e-05
	LOSS [training: 0.08616295386388267 | validation: 0.06686957261105594]
	TIME [epoch: 8.17 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08978877465560312		[learning rate: 2.1975e-05]
		[batch 20/20] avg loss: 0.0834973872541433		[learning rate: 2.1935e-05]
	Learning Rate: 2.19347e-05
	LOSS [training: 0.0866430809548732 | validation: 0.06534649587686674]
	TIME [epoch: 8.19 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08491802063250238		[learning rate: 2.1895e-05]
		[batch 20/20] avg loss: 0.08808066095444517		[learning rate: 2.1855e-05]
	Learning Rate: 2.18551e-05
	LOSS [training: 0.0864993407934738 | validation: 0.06590687042094572]
	TIME [epoch: 8.2 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09425155843039149		[learning rate: 2.1815e-05]
		[batch 20/20] avg loss: 0.07923452751718404		[learning rate: 2.1776e-05]
	Learning Rate: 2.17758e-05
	LOSS [training: 0.08674304297378777 | validation: 0.06692613828907636]
	TIME [epoch: 8.18 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08376311638732566		[learning rate: 2.1736e-05]
		[batch 20/20] avg loss: 0.08840345399102079		[learning rate: 2.1697e-05]
	Learning Rate: 2.16968e-05
	LOSS [training: 0.08608328518917321 | validation: 0.061566949208788524]
	TIME [epoch: 8.19 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08820673562940483		[learning rate: 2.1657e-05]
		[batch 20/20] avg loss: 0.08785587808550863		[learning rate: 2.1618e-05]
	Learning Rate: 2.1618e-05
	LOSS [training: 0.08803130685745672 | validation: 0.06263422068449863]
	TIME [epoch: 8.17 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08203985814494946		[learning rate: 2.1579e-05]
		[batch 20/20] avg loss: 0.09115583854037279		[learning rate: 2.154e-05]
	Learning Rate: 2.15396e-05
	LOSS [training: 0.08659784834266113 | validation: 0.062474878912962516]
	TIME [epoch: 8.2 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07748681223703563		[learning rate: 2.15e-05]
		[batch 20/20] avg loss: 0.10058818610461559		[learning rate: 2.1461e-05]
	Learning Rate: 2.14614e-05
	LOSS [training: 0.08903749917082562 | validation: 0.07555114857327128]
	TIME [epoch: 8.18 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08753261587565148		[learning rate: 2.1422e-05]
		[batch 20/20] avg loss: 0.09182106755237057		[learning rate: 2.1384e-05]
	Learning Rate: 2.13835e-05
	LOSS [training: 0.089676841714011 | validation: 0.06943686440441482]
	TIME [epoch: 8.18 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09044130993201671		[learning rate: 2.1345e-05]
		[batch 20/20] avg loss: 0.08376726228272185		[learning rate: 2.1306e-05]
	Learning Rate: 2.13059e-05
	LOSS [training: 0.08710428610736927 | validation: 0.06741335596874821]
	TIME [epoch: 8.17 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08395648431003737		[learning rate: 2.1267e-05]
		[batch 20/20] avg loss: 0.09033256350887149		[learning rate: 2.1229e-05]
	Learning Rate: 2.12286e-05
	LOSS [training: 0.0871445239094544 | validation: 0.06530463818062467]
	TIME [epoch: 8.2 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07554906729992224		[learning rate: 2.119e-05]
		[batch 20/20] avg loss: 0.09304290976926147		[learning rate: 2.1152e-05]
	Learning Rate: 2.11515e-05
	LOSS [training: 0.08429598853459186 | validation: 0.06187695137720416]
	TIME [epoch: 8.18 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09131583864405925		[learning rate: 2.1113e-05]
		[batch 20/20] avg loss: 0.0821248656270647		[learning rate: 2.1075e-05]
	Learning Rate: 2.10748e-05
	LOSS [training: 0.08672035213556195 | validation: 0.06055271456614351]
	TIME [epoch: 8.19 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08874050748237083		[learning rate: 2.1037e-05]
		[batch 20/20] avg loss: 0.08308159972881278		[learning rate: 2.0998e-05]
	Learning Rate: 2.09983e-05
	LOSS [training: 0.08591105360559181 | validation: 0.063365032874857]
	TIME [epoch: 8.17 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0859502624324679		[learning rate: 2.096e-05]
		[batch 20/20] avg loss: 0.08527413025438368		[learning rate: 2.0922e-05]
	Learning Rate: 2.09221e-05
	LOSS [training: 0.08561219634342579 | validation: 0.07779366291441785]
	TIME [epoch: 8.21 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09070876590721068		[learning rate: 2.0884e-05]
		[batch 20/20] avg loss: 0.08727990200821956		[learning rate: 2.0846e-05]
	Learning Rate: 2.08462e-05
	LOSS [training: 0.08899433395771512 | validation: 0.06268778833499894]
	TIME [epoch: 8.18 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08421236160683018		[learning rate: 2.0808e-05]
		[batch 20/20] avg loss: 0.09215854198016028		[learning rate: 2.0771e-05]
	Learning Rate: 2.07705e-05
	LOSS [training: 0.08818545179349523 | validation: 0.0628268416996884]
	TIME [epoch: 8.18 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09673312949976304		[learning rate: 2.0733e-05]
		[batch 20/20] avg loss: 0.08899839171192096		[learning rate: 2.0695e-05]
	Learning Rate: 2.06951e-05
	LOSS [training: 0.09286576060584202 | validation: 0.06365252776922942]
	TIME [epoch: 8.18 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10108673153720674		[learning rate: 2.0658e-05]
		[batch 20/20] avg loss: 0.08299889077377995		[learning rate: 2.062e-05]
	Learning Rate: 2.062e-05
	LOSS [training: 0.09204281115549334 | validation: 0.06293742273364959]
	TIME [epoch: 8.2 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08742084750396142		[learning rate: 2.0583e-05]
		[batch 20/20] avg loss: 0.09838416336222647		[learning rate: 2.0545e-05]
	Learning Rate: 2.05452e-05
	LOSS [training: 0.09290250543309395 | validation: 0.06852886971520536]
	TIME [epoch: 8.19 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08829828728224857		[learning rate: 2.0508e-05]
		[batch 20/20] avg loss: 0.0813022429371056		[learning rate: 2.0471e-05]
	Learning Rate: 2.04706e-05
	LOSS [training: 0.08480026510967709 | validation: 0.06333767828093963]
	TIME [epoch: 8.18 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08217831763481451		[learning rate: 2.0433e-05]
		[batch 20/20] avg loss: 0.0846843385583892		[learning rate: 2.0396e-05]
	Learning Rate: 2.03964e-05
	LOSS [training: 0.08343132809660185 | validation: 0.06917451741145485]
	TIME [epoch: 8.18 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09312189691474045		[learning rate: 2.0359e-05]
		[batch 20/20] avg loss: 0.09289221913603346		[learning rate: 2.0322e-05]
	Learning Rate: 2.03223e-05
	LOSS [training: 0.09300705802538695 | validation: 0.06490147919387325]
	TIME [epoch: 8.2 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09533522229788657		[learning rate: 2.0285e-05]
		[batch 20/20] avg loss: 0.08175419047187278		[learning rate: 2.0249e-05]
	Learning Rate: 2.02486e-05
	LOSS [training: 0.08854470638487967 | validation: 0.07062264968388943]
	TIME [epoch: 8.18 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08146540947321929		[learning rate: 2.0212e-05]
		[batch 20/20] avg loss: 0.09293100429327686		[learning rate: 2.0175e-05]
	Learning Rate: 2.01751e-05
	LOSS [training: 0.08719820688324806 | validation: 0.06354735692603096]
	TIME [epoch: 8.18 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08714336925055018		[learning rate: 2.0138e-05]
		[batch 20/20] avg loss: 0.08377276005060257		[learning rate: 2.0102e-05]
	Learning Rate: 2.01019e-05
	LOSS [training: 0.08545806465057637 | validation: 0.06039741201414194]
	TIME [epoch: 8.18 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08683519838519563		[learning rate: 2.0065e-05]
		[batch 20/20] avg loss: 0.08378876013766187		[learning rate: 2.0029e-05]
	Learning Rate: 2.00289e-05
	LOSS [training: 0.08531197926142875 | validation: 0.056872849792552874]
	TIME [epoch: 8.19 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09505601654531975		[learning rate: 1.9993e-05]
		[batch 20/20] avg loss: 0.08265080800436904		[learning rate: 1.9956e-05]
	Learning Rate: 1.99563e-05
	LOSS [training: 0.08885341227484442 | validation: 0.06306011187897198]
	TIME [epoch: 8.19 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07318189456559862		[learning rate: 1.992e-05]
		[batch 20/20] avg loss: 0.10242824518402094		[learning rate: 1.9884e-05]
	Learning Rate: 1.98838e-05
	LOSS [training: 0.0878050698748098 | validation: 0.06232467792669061]
	TIME [epoch: 8.18 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08110707678652085		[learning rate: 1.9848e-05]
		[batch 20/20] avg loss: 0.09201550488508349		[learning rate: 1.9812e-05]
	Learning Rate: 1.98117e-05
	LOSS [training: 0.08656129083580218 | validation: 0.06492609356060122]
	TIME [epoch: 8.18 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08489603444707876		[learning rate: 1.9776e-05]
		[batch 20/20] avg loss: 0.0830310456014806		[learning rate: 1.974e-05]
	Learning Rate: 1.97398e-05
	LOSS [training: 0.0839635400242797 | validation: 0.05444670938923493]
	TIME [epoch: 8.19 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08179804824575063		[learning rate: 1.9704e-05]
		[batch 20/20] avg loss: 0.08388942985616986		[learning rate: 1.9668e-05]
	Learning Rate: 1.96681e-05
	LOSS [training: 0.08284373905096025 | validation: 0.05916177023843009]
	TIME [epoch: 8.19 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09109138803772814		[learning rate: 1.9632e-05]
		[batch 20/20] avg loss: 0.08441673761329593		[learning rate: 1.9597e-05]
	Learning Rate: 1.95968e-05
	LOSS [training: 0.08775406282551204 | validation: 0.057926742289788]
	TIME [epoch: 8.18 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08048821482788913		[learning rate: 1.9561e-05]
		[batch 20/20] avg loss: 0.08901342286287664		[learning rate: 1.9526e-05]
	Learning Rate: 1.95256e-05
	LOSS [training: 0.08475081884538288 | validation: 0.06312170890133678]
	TIME [epoch: 8.18 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08473534916876445		[learning rate: 1.949e-05]
		[batch 20/20] avg loss: 0.0911916994599766		[learning rate: 1.9455e-05]
	Learning Rate: 1.94548e-05
	LOSS [training: 0.08796352431437052 | validation: 0.05917007452394237]
	TIME [epoch: 8.19 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09361754763325278		[learning rate: 1.9419e-05]
		[batch 20/20] avg loss: 0.08476083916100026		[learning rate: 1.9384e-05]
	Learning Rate: 1.93842e-05
	LOSS [training: 0.08918919339712653 | validation: 0.05884717823984034]
	TIME [epoch: 8.19 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08909917399855088		[learning rate: 1.9349e-05]
		[batch 20/20] avg loss: 0.08145428604016172		[learning rate: 1.9314e-05]
	Learning Rate: 1.93138e-05
	LOSS [training: 0.0852767300193563 | validation: 0.060334552619131905]
	TIME [epoch: 8.18 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08622456185759617		[learning rate: 1.9279e-05]
		[batch 20/20] avg loss: 0.09227278806740173		[learning rate: 1.9244e-05]
	Learning Rate: 1.92437e-05
	LOSS [training: 0.08924867496249897 | validation: 0.06627585938685361]
	TIME [epoch: 8.17 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09484862962066304		[learning rate: 1.9209e-05]
		[batch 20/20] avg loss: 0.07618327400299278		[learning rate: 1.9174e-05]
	Learning Rate: 1.91739e-05
	LOSS [training: 0.08551595181182789 | validation: 0.06177541476789668]
	TIME [epoch: 8.19 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08808255599792582		[learning rate: 1.9139e-05]
		[batch 20/20] avg loss: 0.07701368878062657		[learning rate: 1.9104e-05]
	Learning Rate: 1.91043e-05
	LOSS [training: 0.0825481223892762 | validation: 0.061929593358656075]
	TIME [epoch: 8.2 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07567246868898389		[learning rate: 1.907e-05]
		[batch 20/20] avg loss: 0.09218324456729925		[learning rate: 1.9035e-05]
	Learning Rate: 1.9035e-05
	LOSS [training: 0.08392785662814156 | validation: 0.056835438802237466]
	TIME [epoch: 8.19 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0857291016427004		[learning rate: 1.9e-05]
		[batch 20/20] avg loss: 0.083689216293823		[learning rate: 1.8966e-05]
	Learning Rate: 1.89659e-05
	LOSS [training: 0.0847091589682617 | validation: 0.05486313756774201]
	TIME [epoch: 8.18 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07993094695828387		[learning rate: 1.8931e-05]
		[batch 20/20] avg loss: 0.09053444484861653		[learning rate: 1.8897e-05]
	Learning Rate: 1.88971e-05
	LOSS [training: 0.0852326959034502 | validation: 0.056496275943509096]
	TIME [epoch: 8.18 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09109058612899017		[learning rate: 1.8863e-05]
		[batch 20/20] avg loss: 0.08126416597643885		[learning rate: 1.8829e-05]
	Learning Rate: 1.88285e-05
	LOSS [training: 0.0861773760527145 | validation: 0.06893405533339775]
	TIME [epoch: 8.21 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08981040283867879		[learning rate: 1.8794e-05]
		[batch 20/20] avg loss: 0.09257443847979972		[learning rate: 1.876e-05]
	Learning Rate: 1.87602e-05
	LOSS [training: 0.09119242065923926 | validation: 0.065434554645477]
	TIME [epoch: 8.18 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0910806458665975		[learning rate: 1.8726e-05]
		[batch 20/20] avg loss: 0.0844988549486449		[learning rate: 1.8692e-05]
	Learning Rate: 1.86921e-05
	LOSS [training: 0.0877897504076212 | validation: 0.06636385925258989]
	TIME [epoch: 8.18 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0962698318579928		[learning rate: 1.8658e-05]
		[batch 20/20] avg loss: 0.07900068326922116		[learning rate: 1.8624e-05]
	Learning Rate: 1.86243e-05
	LOSS [training: 0.08763525756360697 | validation: 0.06803665518400884]
	TIME [epoch: 8.18 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09102303669470771		[learning rate: 1.859e-05]
		[batch 20/20] avg loss: 0.07793937918177068		[learning rate: 1.8557e-05]
	Learning Rate: 1.85567e-05
	LOSS [training: 0.0844812079382392 | validation: 0.06452339221803198]
	TIME [epoch: 8.2 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08745183271557308		[learning rate: 1.8523e-05]
		[batch 20/20] avg loss: 0.08067921080638434		[learning rate: 1.8489e-05]
	Learning Rate: 1.84893e-05
	LOSS [training: 0.08406552176097873 | validation: 0.061752842265487703]
	TIME [epoch: 8.18 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09338784848374167		[learning rate: 1.8456e-05]
		[batch 20/20] avg loss: 0.0807560066188349		[learning rate: 1.8422e-05]
	Learning Rate: 1.84222e-05
	LOSS [training: 0.0870719275512883 | validation: 0.0601913522081064]
	TIME [epoch: 8.18 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08920243044196038		[learning rate: 1.8389e-05]
		[batch 20/20] avg loss: 0.08607170411208104		[learning rate: 1.8355e-05]
	Learning Rate: 1.83554e-05
	LOSS [training: 0.08763706727702072 | validation: 0.059933783896958887]
	TIME [epoch: 8.18 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08604947025038082		[learning rate: 1.8322e-05]
		[batch 20/20] avg loss: 0.07851275577008009		[learning rate: 1.8289e-05]
	Learning Rate: 1.82888e-05
	LOSS [training: 0.08228111301023043 | validation: 0.06347768107141696]
	TIME [epoch: 8.18 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08457158111160787		[learning rate: 1.8256e-05]
		[batch 20/20] avg loss: 0.08561875769318746		[learning rate: 1.8222e-05]
	Learning Rate: 1.82224e-05
	LOSS [training: 0.08509516940239767 | validation: 0.05325275241354205]
	TIME [epoch: 8.23 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07823523403624225		[learning rate: 1.8189e-05]
		[batch 20/20] avg loss: 0.0974443258321245		[learning rate: 1.8156e-05]
	Learning Rate: 1.81563e-05
	LOSS [training: 0.08783977993418338 | validation: 0.061893646163611524]
	TIME [epoch: 8.21 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08969529445925353		[learning rate: 1.8123e-05]
		[batch 20/20] avg loss: 0.10510179552660363		[learning rate: 1.809e-05]
	Learning Rate: 1.80904e-05
	LOSS [training: 0.09739854499292858 | validation: 0.07951369447558389]
	TIME [epoch: 8.21 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09791775589689412		[learning rate: 1.8058e-05]
		[batch 20/20] avg loss: 0.08862736986989969		[learning rate: 1.8025e-05]
	Learning Rate: 1.80247e-05
	LOSS [training: 0.09327256288339691 | validation: 0.06093636057182778]
	TIME [epoch: 8.22 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08535563181562834		[learning rate: 1.7992e-05]
		[batch 20/20] avg loss: 0.09157353122358272		[learning rate: 1.7959e-05]
	Learning Rate: 1.79593e-05
	LOSS [training: 0.08846458151960554 | validation: 0.06608851298906362]
	TIME [epoch: 8.2 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09115342712230652		[learning rate: 1.7927e-05]
		[batch 20/20] avg loss: 0.0891356368693264		[learning rate: 1.7894e-05]
	Learning Rate: 1.78941e-05
	LOSS [training: 0.09014453199581646 | validation: 0.060324845453015204]
	TIME [epoch: 8.19 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08731543027937957		[learning rate: 1.7862e-05]
		[batch 20/20] avg loss: 0.08870913866140669		[learning rate: 1.7829e-05]
	Learning Rate: 1.78292e-05
	LOSS [training: 0.08801228447039314 | validation: 0.06020908967680383]
	TIME [epoch: 8.21 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08168200948498926		[learning rate: 1.7797e-05]
		[batch 20/20] avg loss: 0.08937917872686547		[learning rate: 1.7764e-05]
	Learning Rate: 1.77645e-05
	LOSS [training: 0.08553059410592737 | validation: 0.05773840592723592]
	TIME [epoch: 8.21 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08205284461525991		[learning rate: 1.7732e-05]
		[batch 20/20] avg loss: 0.08583517744449932		[learning rate: 1.77e-05]
	Learning Rate: 1.77e-05
	LOSS [training: 0.08394401102987961 | validation: 0.06357943334688823]
	TIME [epoch: 8.18 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08930625886223142		[learning rate: 1.7668e-05]
		[batch 20/20] avg loss: 0.08858441371719081		[learning rate: 1.7636e-05]
	Learning Rate: 1.76358e-05
	LOSS [training: 0.08894533628971112 | validation: 0.05698286462269783]
	TIME [epoch: 8.21 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08701714450067569		[learning rate: 1.7604e-05]
		[batch 20/20] avg loss: 0.08064194413289752		[learning rate: 1.7572e-05]
	Learning Rate: 1.75718e-05
	LOSS [training: 0.0838295443167866 | validation: 0.06168087801123527]
	TIME [epoch: 8.2 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08980695348329328		[learning rate: 1.754e-05]
		[batch 20/20] avg loss: 0.09571770595246752		[learning rate: 1.7508e-05]
	Learning Rate: 1.7508e-05
	LOSS [training: 0.09276232971788043 | validation: 0.07305930518220201]
	TIME [epoch: 8.2 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.10382879923338387		[learning rate: 1.7476e-05]
		[batch 20/20] avg loss: 0.08561264521459441		[learning rate: 1.7444e-05]
	Learning Rate: 1.74445e-05
	LOSS [training: 0.09472072222398914 | validation: 0.056137075632508276]
	TIME [epoch: 8.18 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09546412981677167		[learning rate: 1.7413e-05]
		[batch 20/20] avg loss: 0.0830255884637063		[learning rate: 1.7381e-05]
	Learning Rate: 1.73812e-05
	LOSS [training: 0.08924485914023898 | validation: 0.05817580562801273]
	TIME [epoch: 8.17 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08387307803012123		[learning rate: 1.735e-05]
		[batch 20/20] avg loss: 0.10042799341496866		[learning rate: 1.7318e-05]
	Learning Rate: 1.73181e-05
	LOSS [training: 0.09215053572254493 | validation: 0.06744584411928002]
	TIME [epoch: 8.17 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08123528915500615		[learning rate: 1.7287e-05]
		[batch 20/20] avg loss: 0.09197407807136448		[learning rate: 1.7255e-05]
	Learning Rate: 1.72552e-05
	LOSS [training: 0.08660468361318531 | validation: 0.05811973517529791]
	TIME [epoch: 8.19 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08135008078970554		[learning rate: 1.7224e-05]
		[batch 20/20] avg loss: 0.09167314310647848		[learning rate: 1.7193e-05]
	Learning Rate: 1.71926e-05
	LOSS [training: 0.08651161194809201 | validation: 0.05256278513895437]
	TIME [epoch: 8.17 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09066438106370057		[learning rate: 1.7161e-05]
		[batch 20/20] avg loss: 0.09222613130865062		[learning rate: 1.713e-05]
	Learning Rate: 1.71302e-05
	LOSS [training: 0.0914452561861756 | validation: 0.07032536843079568]
	TIME [epoch: 8.16 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0890529399238578		[learning rate: 1.7099e-05]
		[batch 20/20] avg loss: 0.09120493938900696		[learning rate: 1.7068e-05]
	Learning Rate: 1.70681e-05
	LOSS [training: 0.09012893965643239 | validation: 0.06272656022283112]
	TIME [epoch: 8.17 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08301184096800773		[learning rate: 1.7037e-05]
		[batch 20/20] avg loss: 0.08728547235887202		[learning rate: 1.7006e-05]
	Learning Rate: 1.70061e-05
	LOSS [training: 0.08514865666343986 | validation: 0.05642707861283017]
	TIME [epoch: 8.19 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07820597593496961		[learning rate: 1.6975e-05]
		[batch 20/20] avg loss: 0.09636489287560028		[learning rate: 1.6944e-05]
	Learning Rate: 1.69444e-05
	LOSS [training: 0.08728543440528495 | validation: 0.06387882967349501]
	TIME [epoch: 8.17 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09567442894922189		[learning rate: 1.6914e-05]
		[batch 20/20] avg loss: 0.09137580283584956		[learning rate: 1.6883e-05]
	Learning Rate: 1.68829e-05
	LOSS [training: 0.09352511589253573 | validation: 0.06358703065509462]
	TIME [epoch: 8.17 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09259685788372321		[learning rate: 1.6852e-05]
		[batch 20/20] avg loss: 0.08488804551031748		[learning rate: 1.6822e-05]
	Learning Rate: 1.68216e-05
	LOSS [training: 0.08874245169702034 | validation: 0.06385643012964187]
	TIME [epoch: 8.17 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08630873432568147		[learning rate: 1.6791e-05]
		[batch 20/20] avg loss: 0.08922678351036621		[learning rate: 1.6761e-05]
	Learning Rate: 1.67606e-05
	LOSS [training: 0.08776775891802384 | validation: 0.06091185382209227]
	TIME [epoch: 8.18 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08274266729639762		[learning rate: 1.673e-05]
		[batch 20/20] avg loss: 0.09625202188654217		[learning rate: 1.67e-05]
	Learning Rate: 1.66998e-05
	LOSS [training: 0.08949734459146991 | validation: 0.05391000006657615]
	TIME [epoch: 8.18 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0896662173417884		[learning rate: 1.6669e-05]
		[batch 20/20] avg loss: 0.08240014305653656		[learning rate: 1.6639e-05]
	Learning Rate: 1.66392e-05
	LOSS [training: 0.08603318019916248 | validation: 0.06026834480253869]
	TIME [epoch: 8.17 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08346993255559451		[learning rate: 1.6609e-05]
		[batch 20/20] avg loss: 0.08718686268378302		[learning rate: 1.6579e-05]
	Learning Rate: 1.65788e-05
	LOSS [training: 0.08532839761968876 | validation: 0.06323547648221498]
	TIME [epoch: 8.16 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08217943574037916		[learning rate: 1.6549e-05]
		[batch 20/20] avg loss: 0.09250685029009138		[learning rate: 1.6519e-05]
	Learning Rate: 1.65186e-05
	LOSS [training: 0.08734314301523526 | validation: 0.05620148145466977]
	TIME [epoch: 8.18 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0835377555707824		[learning rate: 1.6489e-05]
		[batch 20/20] avg loss: 0.08367747634167069		[learning rate: 1.6459e-05]
	Learning Rate: 1.64587e-05
	LOSS [training: 0.08360761595622654 | validation: 0.06069096250026351]
	TIME [epoch: 8.18 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09142980548284534		[learning rate: 1.6429e-05]
		[batch 20/20] avg loss: 0.08663846945061054		[learning rate: 1.6399e-05]
	Learning Rate: 1.63989e-05
	LOSS [training: 0.08903413746672793 | validation: 0.057627189610934645]
	TIME [epoch: 8.16 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08343609717388506		[learning rate: 1.6369e-05]
		[batch 20/20] avg loss: 0.08390966115252367		[learning rate: 1.6339e-05]
	Learning Rate: 1.63394e-05
	LOSS [training: 0.08367287916320439 | validation: 0.05589314763566783]
	TIME [epoch: 8.17 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08071699290403815		[learning rate: 1.631e-05]
		[batch 20/20] avg loss: 0.09253237821212686		[learning rate: 1.628e-05]
	Learning Rate: 1.62801e-05
	LOSS [training: 0.0866246855580825 | validation: 0.056469539703183566]
	TIME [epoch: 8.2 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07619270042484366		[learning rate: 1.6251e-05]
		[batch 20/20] avg loss: 0.09334105212144575		[learning rate: 1.6221e-05]
	Learning Rate: 1.62211e-05
	LOSS [training: 0.08476687627314469 | validation: 0.06514379356063879]
	TIME [epoch: 8.17 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09063592185545648		[learning rate: 1.6192e-05]
		[batch 20/20] avg loss: 0.0778862222627927		[learning rate: 1.6162e-05]
	Learning Rate: 1.61622e-05
	LOSS [training: 0.0842610720591246 | validation: 0.060601733961869214]
	TIME [epoch: 8.17 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08940411200112222		[learning rate: 1.6133e-05]
		[batch 20/20] avg loss: 0.09189085703228517		[learning rate: 1.6104e-05]
	Learning Rate: 1.61035e-05
	LOSS [training: 0.09064748451670371 | validation: 0.05714088413137005]
	TIME [epoch: 8.16 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08816841698730218		[learning rate: 1.6074e-05]
		[batch 20/20] avg loss: 0.08076971731203536		[learning rate: 1.6045e-05]
	Learning Rate: 1.60451e-05
	LOSS [training: 0.08446906714966877 | validation: 0.05818749088352701]
	TIME [epoch: 8.19 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08931448583850532		[learning rate: 1.6016e-05]
		[batch 20/20] avg loss: 0.08123438604175351		[learning rate: 1.5987e-05]
	Learning Rate: 1.59869e-05
	LOSS [training: 0.08527443594012943 | validation: 0.06307246657689822]
	TIME [epoch: 8.17 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08107974245377583		[learning rate: 1.5958e-05]
		[batch 20/20] avg loss: 0.08467335021014205		[learning rate: 1.5929e-05]
	Learning Rate: 1.59288e-05
	LOSS [training: 0.08287654633195894 | validation: 0.05474054391450918]
	TIME [epoch: 8.18 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0805453629010223		[learning rate: 1.59e-05]
		[batch 20/20] avg loss: 0.08914681675313897		[learning rate: 1.5871e-05]
	Learning Rate: 1.5871e-05
	LOSS [training: 0.08484608982708063 | validation: 0.05996851042331939]
	TIME [epoch: 8.17 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08023957540745374		[learning rate: 1.5842e-05]
		[batch 20/20] avg loss: 0.09250810946882668		[learning rate: 1.5813e-05]
	Learning Rate: 1.58134e-05
	LOSS [training: 0.08637384243814022 | validation: 0.061073186409839206]
	TIME [epoch: 8.19 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08102157770260324		[learning rate: 1.5785e-05]
		[batch 20/20] avg loss: 0.08338922721591015		[learning rate: 1.5756e-05]
	Learning Rate: 1.57561e-05
	LOSS [training: 0.08220540245925669 | validation: 0.051946180578199753]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1875.pth
	Model improved!!!
EPOCH 1876/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09416960206567715		[learning rate: 1.5727e-05]
		[batch 20/20] avg loss: 0.07540369385845949		[learning rate: 1.5699e-05]
	Learning Rate: 1.56989e-05
	LOSS [training: 0.08478664796206833 | validation: 0.05511052599074505]
	TIME [epoch: 8.18 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.084492648834143		[learning rate: 1.567e-05]
		[batch 20/20] avg loss: 0.0820760070075731		[learning rate: 1.5642e-05]
	Learning Rate: 1.56419e-05
	LOSS [training: 0.08328432792085806 | validation: 0.0595446074972245]
	TIME [epoch: 8.17 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08815804642857723		[learning rate: 1.5613e-05]
		[batch 20/20] avg loss: 0.07880241349800525		[learning rate: 1.5585e-05]
	Learning Rate: 1.55851e-05
	LOSS [training: 0.08348022996329124 | validation: 0.06131076208463432]
	TIME [epoch: 8.19 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08260187184412328		[learning rate: 1.5557e-05]
		[batch 20/20] avg loss: 0.08903393181111252		[learning rate: 1.5529e-05]
	Learning Rate: 1.55286e-05
	LOSS [training: 0.08581790182761789 | validation: 0.05829318541012105]
	TIME [epoch: 8.17 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08767980600160684		[learning rate: 1.55e-05]
		[batch 20/20] avg loss: 0.07791825660961929		[learning rate: 1.5472e-05]
	Learning Rate: 1.54722e-05
	LOSS [training: 0.08279903130561307 | validation: 0.058133049323163176]
	TIME [epoch: 8.18 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08388335945987306		[learning rate: 1.5444e-05]
		[batch 20/20] avg loss: 0.08396802849212887		[learning rate: 1.5416e-05]
	Learning Rate: 1.54161e-05
	LOSS [training: 0.08392569397600096 | validation: 0.063186135967965]
	TIME [epoch: 8.17 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0854283859385644		[learning rate: 1.5388e-05]
		[batch 20/20] avg loss: 0.08837503822714479		[learning rate: 1.536e-05]
	Learning Rate: 1.53601e-05
	LOSS [training: 0.08690171208285459 | validation: 0.06054696487297312]
	TIME [epoch: 8.19 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0780397790454272		[learning rate: 1.5332e-05]
		[batch 20/20] avg loss: 0.08916360502799002		[learning rate: 1.5304e-05]
	Learning Rate: 1.53044e-05
	LOSS [training: 0.0836016920367086 | validation: 0.05477956707360836]
	TIME [epoch: 8.17 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08167131182287107		[learning rate: 1.5277e-05]
		[batch 20/20] avg loss: 0.08945091163841341		[learning rate: 1.5249e-05]
	Learning Rate: 1.52488e-05
	LOSS [training: 0.08556111173064225 | validation: 0.06441054123215288]
	TIME [epoch: 8.17 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07895773666841416		[learning rate: 1.5221e-05]
		[batch 20/20] avg loss: 0.09311043049560228		[learning rate: 1.5194e-05]
	Learning Rate: 1.51935e-05
	LOSS [training: 0.08603408358200823 | validation: 0.056625402952186904]
	TIME [epoch: 8.17 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09120175400757556		[learning rate: 1.5166e-05]
		[batch 20/20] avg loss: 0.08323275258678578		[learning rate: 1.5138e-05]
	Learning Rate: 1.51384e-05
	LOSS [training: 0.08721725329718066 | validation: 0.06302921719452811]
	TIME [epoch: 8.19 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08713388262757252		[learning rate: 1.5111e-05]
		[batch 20/20] avg loss: 0.08645751919656261		[learning rate: 1.5083e-05]
	Learning Rate: 1.50834e-05
	LOSS [training: 0.08679570091206758 | validation: 0.06193098233481299]
	TIME [epoch: 8.17 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0739030858454902		[learning rate: 1.5056e-05]
		[batch 20/20] avg loss: 0.08928093945022167		[learning rate: 1.5029e-05]
	Learning Rate: 1.50287e-05
	LOSS [training: 0.08159201264785594 | validation: 0.05705953626080275]
	TIME [epoch: 8.18 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07354508668678986		[learning rate: 1.5001e-05]
		[batch 20/20] avg loss: 0.09316165446813099		[learning rate: 1.4974e-05]
	Learning Rate: 1.49741e-05
	LOSS [training: 0.08335337057746042 | validation: 0.05929144363656423]
	TIME [epoch: 8.17 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08127158364389522		[learning rate: 1.4947e-05]
		[batch 20/20] avg loss: 0.08818915270418151		[learning rate: 1.492e-05]
	Learning Rate: 1.49198e-05
	LOSS [training: 0.08473036817403838 | validation: 0.06279250381845601]
	TIME [epoch: 8.2 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0841538357038906		[learning rate: 1.4893e-05]
		[batch 20/20] avg loss: 0.08435261082024492		[learning rate: 1.4866e-05]
	Learning Rate: 1.48657e-05
	LOSS [training: 0.08425322326206776 | validation: 0.06061400421340727]
	TIME [epoch: 8.17 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08724219162091935		[learning rate: 1.4839e-05]
		[batch 20/20] avg loss: 0.0842759876525401		[learning rate: 1.4812e-05]
	Learning Rate: 1.48117e-05
	LOSS [training: 0.08575908963672971 | validation: 0.06202792289105856]
	TIME [epoch: 8.17 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08877113858888311		[learning rate: 1.4785e-05]
		[batch 20/20] avg loss: 0.0877635247956046		[learning rate: 1.4758e-05]
	Learning Rate: 1.4758e-05
	LOSS [training: 0.08826733169224389 | validation: 0.06586941534575641]
	TIME [epoch: 8.17 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0931829653889216		[learning rate: 1.4731e-05]
		[batch 20/20] avg loss: 0.08792579932596463		[learning rate: 1.4704e-05]
	Learning Rate: 1.47044e-05
	LOSS [training: 0.0905543823574431 | validation: 0.06078795415415726]
	TIME [epoch: 8.19 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08914404874529248		[learning rate: 1.4678e-05]
		[batch 20/20] avg loss: 0.08376674766887764		[learning rate: 1.4651e-05]
	Learning Rate: 1.4651e-05
	LOSS [training: 0.08645539820708506 | validation: 0.06357540813856376]
	TIME [epoch: 8.17 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09236073486340463		[learning rate: 1.4624e-05]
		[batch 20/20] avg loss: 0.07776612180077805		[learning rate: 1.4598e-05]
	Learning Rate: 1.45979e-05
	LOSS [training: 0.08506342833209135 | validation: 0.0589213047059884]
	TIME [epoch: 8.17 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0803201612940165		[learning rate: 1.4571e-05]
		[batch 20/20] avg loss: 0.08493684010266342		[learning rate: 1.4545e-05]
	Learning Rate: 1.45449e-05
	LOSS [training: 0.08262850069833996 | validation: 0.06259252724491328]
	TIME [epoch: 8.17 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09050623331374205		[learning rate: 1.4518e-05]
		[batch 20/20] avg loss: 0.08450868768638492		[learning rate: 1.4492e-05]
	Learning Rate: 1.44921e-05
	LOSS [training: 0.0875074605000635 | validation: 0.056857691318640005]
	TIME [epoch: 8.19 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08203275570445291		[learning rate: 1.4466e-05]
		[batch 20/20] avg loss: 0.08668963189355021		[learning rate: 1.444e-05]
	Learning Rate: 1.44395e-05
	LOSS [training: 0.08436119379900156 | validation: 0.053676725359630514]
	TIME [epoch: 8.17 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07021157911770541		[learning rate: 1.4413e-05]
		[batch 20/20] avg loss: 0.09614798257575677		[learning rate: 1.4387e-05]
	Learning Rate: 1.43871e-05
	LOSS [training: 0.08317978084673108 | validation: 0.058391297014353985]
	TIME [epoch: 8.17 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08615006037903328		[learning rate: 1.4361e-05]
		[batch 20/20] avg loss: 0.08445987242262736		[learning rate: 1.4335e-05]
	Learning Rate: 1.43349e-05
	LOSS [training: 0.08530496640083032 | validation: 0.057644422293796876]
	TIME [epoch: 8.17 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07765658372457448		[learning rate: 1.4309e-05]
		[batch 20/20] avg loss: 0.09392825607933308		[learning rate: 1.4283e-05]
	Learning Rate: 1.42829e-05
	LOSS [training: 0.0857924199019538 | validation: 0.05753031270192774]
	TIME [epoch: 8.19 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08078554125014052		[learning rate: 1.4257e-05]
		[batch 20/20] avg loss: 0.08556997018866509		[learning rate: 1.4231e-05]
	Learning Rate: 1.4231e-05
	LOSS [training: 0.08317775571940281 | validation: 0.056810871383106906]
	TIME [epoch: 8.18 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07685210561606297		[learning rate: 1.4205e-05]
		[batch 20/20] avg loss: 0.09396089077074107		[learning rate: 1.4179e-05]
	Learning Rate: 1.41794e-05
	LOSS [training: 0.08540649819340203 | validation: 0.05860047053217274]
	TIME [epoch: 8.17 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07582946174901635		[learning rate: 1.4154e-05]
		[batch 20/20] avg loss: 0.09423673113768541		[learning rate: 1.4128e-05]
	Learning Rate: 1.41279e-05
	LOSS [training: 0.08503309644335087 | validation: 0.061338622546413576]
	TIME [epoch: 8.17 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08282993972700987		[learning rate: 1.4102e-05]
		[batch 20/20] avg loss: 0.08150362008228285		[learning rate: 1.4077e-05]
	Learning Rate: 1.40767e-05
	LOSS [training: 0.08216677990464635 | validation: 0.06621807219101764]
	TIME [epoch: 8.18 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0858001443990786		[learning rate: 1.4051e-05]
		[batch 20/20] avg loss: 0.0821793927321578		[learning rate: 1.4026e-05]
	Learning Rate: 1.40256e-05
	LOSS [training: 0.0839897685656182 | validation: 0.0523400176203382]
	TIME [epoch: 8.17 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07986597878304133		[learning rate: 1.4e-05]
		[batch 20/20] avg loss: 0.0873076260938687		[learning rate: 1.3975e-05]
	Learning Rate: 1.39747e-05
	LOSS [training: 0.08358680243845502 | validation: 0.04807973118303567]
	TIME [epoch: 8.16 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r3_20240219_193834/states/model_tr_study202_1908.pth
	Model improved!!!
EPOCH 1909/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08940565233443114		[learning rate: 1.3949e-05]
		[batch 20/20] avg loss: 0.08026985338081134		[learning rate: 1.3924e-05]
	Learning Rate: 1.3924e-05
	LOSS [training: 0.08483775285762123 | validation: 0.056764619772849495]
	TIME [epoch: 8.18 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08595867138580335		[learning rate: 1.3899e-05]
		[batch 20/20] avg loss: 0.08176927503453651		[learning rate: 1.3873e-05]
	Learning Rate: 1.38734e-05
	LOSS [training: 0.08386397321016993 | validation: 0.0489285134947659]
	TIME [epoch: 8.2 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08667632959988067		[learning rate: 1.3848e-05]
		[batch 20/20] avg loss: 0.08660973866835817		[learning rate: 1.3823e-05]
	Learning Rate: 1.38231e-05
	LOSS [training: 0.0866430341341194 | validation: 0.06356951746445504]
	TIME [epoch: 8.17 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08698532200783761		[learning rate: 1.3798e-05]
		[batch 20/20] avg loss: 0.0824227846113803		[learning rate: 1.3773e-05]
	Learning Rate: 1.37729e-05
	LOSS [training: 0.08470405330960894 | validation: 0.06023887157230995]
	TIME [epoch: 8.17 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08562235865935189		[learning rate: 1.3748e-05]
		[batch 20/20] avg loss: 0.08833876848494418		[learning rate: 1.3723e-05]
	Learning Rate: 1.37229e-05
	LOSS [training: 0.08698056357214803 | validation: 0.05881474557843576]
	TIME [epoch: 8.18 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08731598808873212		[learning rate: 1.3698e-05]
		[batch 20/20] avg loss: 0.07805121206597694		[learning rate: 1.3673e-05]
	Learning Rate: 1.36731e-05
	LOSS [training: 0.08268360007735452 | validation: 0.06318517728128929]
	TIME [epoch: 8.19 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0880788529811044		[learning rate: 1.3648e-05]
		[batch 20/20] avg loss: 0.09044444270608124		[learning rate: 1.3624e-05]
	Learning Rate: 1.36235e-05
	LOSS [training: 0.08926164784359285 | validation: 0.0663087445828711]
	TIME [epoch: 8.19 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08616765358953302		[learning rate: 1.3599e-05]
		[batch 20/20] avg loss: 0.09056651091256976		[learning rate: 1.3574e-05]
	Learning Rate: 1.35741e-05
	LOSS [training: 0.0883670822510514 | validation: 0.06421792286648]
	TIME [epoch: 8.18 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08791782778130051		[learning rate: 1.3549e-05]
		[batch 20/20] avg loss: 0.08702820947532366		[learning rate: 1.3525e-05]
	Learning Rate: 1.35248e-05
	LOSS [training: 0.0874730186283121 | validation: 0.05888297184803245]
	TIME [epoch: 8.17 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08430123813238839		[learning rate: 1.35e-05]
		[batch 20/20] avg loss: 0.08666906498917118		[learning rate: 1.3476e-05]
	Learning Rate: 1.34757e-05
	LOSS [training: 0.08548515156077978 | validation: 0.059864137701355534]
	TIME [epoch: 8.18 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0804079266754293		[learning rate: 1.3451e-05]
		[batch 20/20] avg loss: 0.08783410947998062		[learning rate: 1.3427e-05]
	Learning Rate: 1.34268e-05
	LOSS [training: 0.08412101807770497 | validation: 0.0650937821414514]
	TIME [epoch: 8.19 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.075625985511752		[learning rate: 1.3402e-05]
		[batch 20/20] avg loss: 0.09565742025805715		[learning rate: 1.3378e-05]
	Learning Rate: 1.33781e-05
	LOSS [training: 0.08564170288490457 | validation: 0.058949304034120485]
	TIME [epoch: 8.18 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07684247983536337		[learning rate: 1.3354e-05]
		[batch 20/20] avg loss: 0.09090293117042023		[learning rate: 1.333e-05]
	Learning Rate: 1.33296e-05
	LOSS [training: 0.08387270550289179 | validation: 0.058436627991257484]
	TIME [epoch: 8.18 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08320614122829084		[learning rate: 1.3305e-05]
		[batch 20/20] avg loss: 0.08693982418681732		[learning rate: 1.3281e-05]
	Learning Rate: 1.32812e-05
	LOSS [training: 0.08507298270755408 | validation: 0.06411440233978394]
	TIME [epoch: 8.17 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08164035332420451		[learning rate: 1.3257e-05]
		[batch 20/20] avg loss: 0.08662709790450594		[learning rate: 1.3233e-05]
	Learning Rate: 1.3233e-05
	LOSS [training: 0.08413372561435523 | validation: 0.06324664773382868]
	TIME [epoch: 8.2 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08039845191623282		[learning rate: 1.3209e-05]
		[batch 20/20] avg loss: 0.08378453689501984		[learning rate: 1.3185e-05]
	Learning Rate: 1.3185e-05
	LOSS [training: 0.08209149440562633 | validation: 0.05644356483622477]
	TIME [epoch: 8.17 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08644030294634761		[learning rate: 1.3161e-05]
		[batch 20/20] avg loss: 0.07789090633317318		[learning rate: 1.3137e-05]
	Learning Rate: 1.31371e-05
	LOSS [training: 0.08216560463976039 | validation: 0.054934708954338426]
	TIME [epoch: 8.17 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0891798526590247		[learning rate: 1.3113e-05]
		[batch 20/20] avg loss: 0.0769182565339108		[learning rate: 1.3089e-05]
	Learning Rate: 1.30894e-05
	LOSS [training: 0.08304905459646775 | validation: 0.05907681984407927]
	TIME [epoch: 8.18 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08521146774298075		[learning rate: 1.3066e-05]
		[batch 20/20] avg loss: 0.08721878412593415		[learning rate: 1.3042e-05]
	Learning Rate: 1.30419e-05
	LOSS [training: 0.08621512593445745 | validation: 0.059697847877127654]
	TIME [epoch: 8.2 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09031962037565287		[learning rate: 1.3018e-05]
		[batch 20/20] avg loss: 0.0822148452984121		[learning rate: 1.2995e-05]
	Learning Rate: 1.29946e-05
	LOSS [training: 0.08626723283703248 | validation: 0.06367277601768859]
	TIME [epoch: 8.17 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08006245485159864		[learning rate: 1.2971e-05]
		[batch 20/20] avg loss: 0.08798205920845344		[learning rate: 1.2947e-05]
	Learning Rate: 1.29475e-05
	LOSS [training: 0.08402225703002604 | validation: 0.06384154087623793]
	TIME [epoch: 8.18 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08342828527952686		[learning rate: 1.2924e-05]
		[batch 20/20] avg loss: 0.08606090941652617		[learning rate: 1.29e-05]
	Learning Rate: 1.29005e-05
	LOSS [training: 0.0847445973480265 | validation: 0.06929494392451374]
	TIME [epoch: 8.17 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07755480765938774		[learning rate: 1.2877e-05]
		[batch 20/20] avg loss: 0.08897096968310714		[learning rate: 1.2854e-05]
	Learning Rate: 1.28536e-05
	LOSS [training: 0.08326288867124745 | validation: 0.06130337284057899]
	TIME [epoch: 8.19 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08265934725520066		[learning rate: 1.283e-05]
		[batch 20/20] avg loss: 0.08114166918177115		[learning rate: 1.2807e-05]
	Learning Rate: 1.2807e-05
	LOSS [training: 0.0819005082184859 | validation: 0.06273653155109632]
	TIME [epoch: 8.18 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08178192234339611		[learning rate: 1.2784e-05]
		[batch 20/20] avg loss: 0.08226028487233152		[learning rate: 1.2761e-05]
	Learning Rate: 1.27605e-05
	LOSS [training: 0.08202110360786383 | validation: 0.06283981492670176]
	TIME [epoch: 8.17 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08869338753480437		[learning rate: 1.2737e-05]
		[batch 20/20] avg loss: 0.0829738451846839		[learning rate: 1.2714e-05]
	Learning Rate: 1.27142e-05
	LOSS [training: 0.08583361635974414 | validation: 0.06194923644735566]
	TIME [epoch: 8.17 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08954118548457879		[learning rate: 1.2691e-05]
		[batch 20/20] avg loss: 0.07714727584432314		[learning rate: 1.2668e-05]
	Learning Rate: 1.26681e-05
	LOSS [training: 0.08334423066445096 | validation: 0.058832513586357126]
	TIME [epoch: 8.19 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.080342427266869		[learning rate: 1.2645e-05]
		[batch 20/20] avg loss: 0.08848126577380434		[learning rate: 1.2622e-05]
	Learning Rate: 1.26221e-05
	LOSS [training: 0.08441184652033665 | validation: 0.0621422745340951]
	TIME [epoch: 8.17 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08882704675769233		[learning rate: 1.2599e-05]
		[batch 20/20] avg loss: 0.08263816281386198		[learning rate: 1.2576e-05]
	Learning Rate: 1.25763e-05
	LOSS [training: 0.08573260478577714 | validation: 0.060597752305890595]
	TIME [epoch: 8.18 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09046325678297899		[learning rate: 1.2553e-05]
		[batch 20/20] avg loss: 0.08176783766133601		[learning rate: 1.2531e-05]
	Learning Rate: 1.25307e-05
	LOSS [training: 0.08611554722215752 | validation: 0.056619743037984074]
	TIME [epoch: 8.17 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08210199130849008		[learning rate: 1.2508e-05]
		[batch 20/20] avg loss: 0.0883914521941642		[learning rate: 1.2485e-05]
	Learning Rate: 1.24852e-05
	LOSS [training: 0.08524672175132712 | validation: 0.06406352508102017]
	TIME [epoch: 8.19 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08748596436755317		[learning rate: 1.2463e-05]
		[batch 20/20] avg loss: 0.08406762074747283		[learning rate: 1.244e-05]
	Learning Rate: 1.24399e-05
	LOSS [training: 0.085776792557513 | validation: 0.059630673772958054]
	TIME [epoch: 8.17 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0892986062980518		[learning rate: 1.2417e-05]
		[batch 20/20] avg loss: 0.08122016093529122		[learning rate: 1.2395e-05]
	Learning Rate: 1.23947e-05
	LOSS [training: 0.0852593836166715 | validation: 0.05394217091900766]
	TIME [epoch: 8.17 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08515646774593191		[learning rate: 1.2372e-05]
		[batch 20/20] avg loss: 0.08840997470298242		[learning rate: 1.235e-05]
	Learning Rate: 1.23497e-05
	LOSS [training: 0.08678322122445717 | validation: 0.058018280074629815]
	TIME [epoch: 8.17 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07453521995663803		[learning rate: 1.2327e-05]
		[batch 20/20] avg loss: 0.08772111851624073		[learning rate: 1.2305e-05]
	Learning Rate: 1.23049e-05
	LOSS [training: 0.08112816923643937 | validation: 0.061956835092256926]
	TIME [epoch: 8.2 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08953405133860745		[learning rate: 1.2283e-05]
		[batch 20/20] avg loss: 0.0764629143657499		[learning rate: 1.226e-05]
	Learning Rate: 1.22603e-05
	LOSS [training: 0.08299848285217867 | validation: 0.06293082182581337]
	TIME [epoch: 8.17 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08426703113444164		[learning rate: 1.2238e-05]
		[batch 20/20] avg loss: 0.07950696908691335		[learning rate: 1.2216e-05]
	Learning Rate: 1.22158e-05
	LOSS [training: 0.08188700011067748 | validation: 0.05799757817599194]
	TIME [epoch: 8.18 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08757344111963788		[learning rate: 1.2194e-05]
		[batch 20/20] avg loss: 0.08775379870273303		[learning rate: 1.2171e-05]
	Learning Rate: 1.21714e-05
	LOSS [training: 0.08766361991118546 | validation: 0.06019939756908213]
	TIME [epoch: 8.17 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09540784019463351		[learning rate: 1.2149e-05]
		[batch 20/20] avg loss: 0.08113471384424249		[learning rate: 1.2127e-05]
	Learning Rate: 1.21273e-05
	LOSS [training: 0.088271277019438 | validation: 0.05863794662720765]
	TIME [epoch: 8.19 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08546777925271966		[learning rate: 1.2105e-05]
		[batch 20/20] avg loss: 0.0798273195372183		[learning rate: 1.2083e-05]
	Learning Rate: 1.20833e-05
	LOSS [training: 0.08264754939496896 | validation: 0.05844743720826594]
	TIME [epoch: 8.18 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08784716803414258		[learning rate: 1.2061e-05]
		[batch 20/20] avg loss: 0.07680722772406925		[learning rate: 1.2039e-05]
	Learning Rate: 1.20394e-05
	LOSS [training: 0.08232719787910593 | validation: 0.07091683244619655]
	TIME [epoch: 8.17 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07780291447957297		[learning rate: 1.2018e-05]
		[batch 20/20] avg loss: 0.09244049418649439		[learning rate: 1.1996e-05]
	Learning Rate: 1.19957e-05
	LOSS [training: 0.08512170433303368 | validation: 0.05946920065817468]
	TIME [epoch: 8.17 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0830063308584822		[learning rate: 1.1974e-05]
		[batch 20/20] avg loss: 0.09040532081182485		[learning rate: 1.1952e-05]
	Learning Rate: 1.19522e-05
	LOSS [training: 0.08670582583515353 | validation: 0.06650283965839181]
	TIME [epoch: 8.19 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08328808743391541		[learning rate: 1.193e-05]
		[batch 20/20] avg loss: 0.09005584656946305		[learning rate: 1.1909e-05]
	Learning Rate: 1.19088e-05
	LOSS [training: 0.08667196700168925 | validation: 0.0538854878118304]
	TIME [epoch: 8.18 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08667137149159572		[learning rate: 1.1887e-05]
		[batch 20/20] avg loss: 0.08586777501487855		[learning rate: 1.1866e-05]
	Learning Rate: 1.18656e-05
	LOSS [training: 0.08626957325323713 | validation: 0.05698644860277104]
	TIME [epoch: 8.17 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0831935966880188		[learning rate: 1.1844e-05]
		[batch 20/20] avg loss: 0.08834262866126663		[learning rate: 1.1823e-05]
	Learning Rate: 1.18225e-05
	LOSS [training: 0.08576811267464271 | validation: 0.06071965987614425]
	TIME [epoch: 8.17 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0865239507600095		[learning rate: 1.1801e-05]
		[batch 20/20] avg loss: 0.08575022028857664		[learning rate: 1.178e-05]
	Learning Rate: 1.17796e-05
	LOSS [training: 0.08613708552429307 | validation: 0.06348230848042175]
	TIME [epoch: 8.19 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08553980917053024		[learning rate: 1.1758e-05]
		[batch 20/20] avg loss: 0.08642564806944879		[learning rate: 1.1737e-05]
	Learning Rate: 1.17369e-05
	LOSS [training: 0.0859827286199895 | validation: 0.06182798284017611]
	TIME [epoch: 8.17 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08322229895165315		[learning rate: 1.1716e-05]
		[batch 20/20] avg loss: 0.08253854704178137		[learning rate: 1.1694e-05]
	Learning Rate: 1.16943e-05
	LOSS [training: 0.08288042299671729 | validation: 0.05974612886715344]
	TIME [epoch: 8.17 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0843036801074154		[learning rate: 1.1673e-05]
		[batch 20/20] avg loss: 0.08894280777272082		[learning rate: 1.1652e-05]
	Learning Rate: 1.16518e-05
	LOSS [training: 0.08662324394006811 | validation: 0.06527260277210004]
	TIME [epoch: 8.17 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0882993180797835		[learning rate: 1.1631e-05]
		[batch 20/20] avg loss: 0.07920461136780779		[learning rate: 1.161e-05]
	Learning Rate: 1.16096e-05
	LOSS [training: 0.08375196472379566 | validation: 0.06106192154417388]
	TIME [epoch: 8.19 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09130918826417019		[learning rate: 1.1588e-05]
		[batch 20/20] avg loss: 0.08198477908348047		[learning rate: 1.1567e-05]
	Learning Rate: 1.15674e-05
	LOSS [training: 0.08664698367382531 | validation: 0.06009755001067636]
	TIME [epoch: 8.18 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08693552818648285		[learning rate: 1.1546e-05]
		[batch 20/20] avg loss: 0.08718179855975788		[learning rate: 1.1525e-05]
	Learning Rate: 1.15255e-05
	LOSS [training: 0.08705866337312036 | validation: 0.05901885008380216]
	TIME [epoch: 8.17 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09628602706748415		[learning rate: 1.1505e-05]
		[batch 20/20] avg loss: 0.08029156431038413		[learning rate: 1.1484e-05]
	Learning Rate: 1.14836e-05
	LOSS [training: 0.08828879568893415 | validation: 0.05631571160238424]
	TIME [epoch: 8.18 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09194588421672234		[learning rate: 1.1463e-05]
		[batch 20/20] avg loss: 0.0808770810453245		[learning rate: 1.1442e-05]
	Learning Rate: 1.1442e-05
	LOSS [training: 0.08641148263102341 | validation: 0.05503967172114055]
	TIME [epoch: 8.2 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09369208860961967		[learning rate: 1.1421e-05]
		[batch 20/20] avg loss: 0.09122618877558222		[learning rate: 1.14e-05]
	Learning Rate: 1.14004e-05
	LOSS [training: 0.09245913869260095 | validation: 0.05587029828858148]
	TIME [epoch: 8.18 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0763653733662306		[learning rate: 1.138e-05]
		[batch 20/20] avg loss: 0.09901521148732038		[learning rate: 1.1359e-05]
	Learning Rate: 1.13591e-05
	LOSS [training: 0.0876902924267755 | validation: 0.06306611796748986]
	TIME [epoch: 8.17 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08735762986387698		[learning rate: 1.1338e-05]
		[batch 20/20] avg loss: 0.08695770616810727		[learning rate: 1.1318e-05]
	Learning Rate: 1.13178e-05
	LOSS [training: 0.08715766801599213 | validation: 0.066393192799202]
	TIME [epoch: 8.17 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08493772822813048		[learning rate: 1.1297e-05]
		[batch 20/20] avg loss: 0.09478601326828962		[learning rate: 1.1277e-05]
	Learning Rate: 1.12768e-05
	LOSS [training: 0.08986187074821006 | validation: 0.06830355374275643]
	TIME [epoch: 8.2 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09179278875507586		[learning rate: 1.1256e-05]
		[batch 20/20] avg loss: 0.09264416895227064		[learning rate: 1.1236e-05]
	Learning Rate: 1.12358e-05
	LOSS [training: 0.09221847885367325 | validation: 0.06671109998080811]
	TIME [epoch: 8.18 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0840473647119985		[learning rate: 1.1215e-05]
		[batch 20/20] avg loss: 0.09067794217051597		[learning rate: 1.1195e-05]
	Learning Rate: 1.11951e-05
	LOSS [training: 0.08736265344125724 | validation: 0.057786480919578456]
	TIME [epoch: 8.17 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08186410999534628		[learning rate: 1.1175e-05]
		[batch 20/20] avg loss: 0.08500923420573332		[learning rate: 1.1154e-05]
	Learning Rate: 1.11544e-05
	LOSS [training: 0.0834366721005398 | validation: 0.058697739321504065]
	TIME [epoch: 8.18 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09028897197914901		[learning rate: 1.1134e-05]
		[batch 20/20] avg loss: 0.0791178071946905		[learning rate: 1.1114e-05]
	Learning Rate: 1.1114e-05
	LOSS [training: 0.08470338958691974 | validation: 0.0596211815104892]
	TIME [epoch: 8.19 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.082659740751062		[learning rate: 1.1094e-05]
		[batch 20/20] avg loss: 0.08117565291221443		[learning rate: 1.1074e-05]
	Learning Rate: 1.10736e-05
	LOSS [training: 0.08191769683163821 | validation: 0.062461517883387774]
	TIME [epoch: 8.19 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.0826554478174629		[learning rate: 1.1054e-05]
		[batch 20/20] avg loss: 0.08162717392298732		[learning rate: 1.1033e-05]
	Learning Rate: 1.10334e-05
	LOSS [training: 0.0821413108702251 | validation: 0.06353158671815276]
	TIME [epoch: 8.17 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08548016623761681		[learning rate: 1.1013e-05]
		[batch 20/20] avg loss: 0.07782698350991149		[learning rate: 1.0993e-05]
	Learning Rate: 1.09934e-05
	LOSS [training: 0.08165357487376415 | validation: 0.0657105274430097]
	TIME [epoch: 8.18 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08935266244935677		[learning rate: 1.0973e-05]
		[batch 20/20] avg loss: 0.0748258330766812		[learning rate: 1.0953e-05]
	Learning Rate: 1.09535e-05
	LOSS [training: 0.08208924776301899 | validation: 0.06570525598957913]
	TIME [epoch: 8.19 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08286473367810196		[learning rate: 1.0934e-05]
		[batch 20/20] avg loss: 0.08337261385540146		[learning rate: 1.0914e-05]
	Learning Rate: 1.09137e-05
	LOSS [training: 0.08311867376675171 | validation: 0.062371195320711295]
	TIME [epoch: 8.18 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07891248372191864		[learning rate: 1.0894e-05]
		[batch 20/20] avg loss: 0.09074613993254493		[learning rate: 1.0874e-05]
	Learning Rate: 1.08741e-05
	LOSS [training: 0.08482931182723176 | validation: 0.06093422540321585]
	TIME [epoch: 8.18 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08843413755052774		[learning rate: 1.0854e-05]
		[batch 20/20] avg loss: 0.0847788434794236		[learning rate: 1.0835e-05]
	Learning Rate: 1.08347e-05
	LOSS [training: 0.08660649051497567 | validation: 0.06032568393787756]
	TIME [epoch: 8.17 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08679184567810115		[learning rate: 1.0815e-05]
		[batch 20/20] avg loss: 0.08155397118507221		[learning rate: 1.0795e-05]
	Learning Rate: 1.07954e-05
	LOSS [training: 0.08417290843158669 | validation: 0.05819851135101825]
	TIME [epoch: 8.21 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09212347338340339		[learning rate: 1.0776e-05]
		[batch 20/20] avg loss: 0.0797896136443744		[learning rate: 1.0756e-05]
	Learning Rate: 1.07562e-05
	LOSS [training: 0.0859565435138889 | validation: 0.05851387911761831]
	TIME [epoch: 8.18 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08804267956162987		[learning rate: 1.0737e-05]
		[batch 20/20] avg loss: 0.08934191106160132		[learning rate: 1.0717e-05]
	Learning Rate: 1.07171e-05
	LOSS [training: 0.0886922953116156 | validation: 0.06326977521599586]
	TIME [epoch: 8.18 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09163051273920012		[learning rate: 1.0698e-05]
		[batch 20/20] avg loss: 0.07859340156273667		[learning rate: 1.0678e-05]
	Learning Rate: 1.06782e-05
	LOSS [training: 0.08511195715096839 | validation: 0.061947632559472166]
	TIME [epoch: 8.18 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08873594938511745		[learning rate: 1.0659e-05]
		[batch 20/20] avg loss: 0.08003545411981747		[learning rate: 1.0639e-05]
	Learning Rate: 1.06395e-05
	LOSS [training: 0.08438570175246744 | validation: 0.06387179758848016]
	TIME [epoch: 8.2 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09230568109698489		[learning rate: 1.062e-05]
		[batch 20/20] avg loss: 0.07782785884994996		[learning rate: 1.0601e-05]
	Learning Rate: 1.06009e-05
	LOSS [training: 0.08506676997346743 | validation: 0.05792781413915324]
	TIME [epoch: 8.17 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08335683316843799		[learning rate: 1.0582e-05]
		[batch 20/20] avg loss: 0.08232780924334124		[learning rate: 1.0562e-05]
	Learning Rate: 1.05624e-05
	LOSS [training: 0.08284232120588962 | validation: 0.059844656592696215]
	TIME [epoch: 8.18 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08121047811420529		[learning rate: 1.0543e-05]
		[batch 20/20] avg loss: 0.08826985331372209		[learning rate: 1.0524e-05]
	Learning Rate: 1.05241e-05
	LOSS [training: 0.08474016571396367 | validation: 0.06157815130548938]
	TIME [epoch: 8.17 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08306611370974443		[learning rate: 1.0505e-05]
		[batch 20/20] avg loss: 0.08499909668042885		[learning rate: 1.0486e-05]
	Learning Rate: 1.04859e-05
	LOSS [training: 0.08403260519508664 | validation: 0.05914952578241481]
	TIME [epoch: 8.2 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08599098785369884		[learning rate: 1.0467e-05]
		[batch 20/20] avg loss: 0.08562266168136727		[learning rate: 1.0448e-05]
	Learning Rate: 1.04478e-05
	LOSS [training: 0.08580682476753307 | validation: 0.0527651871433724]
	TIME [epoch: 8.17 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07817758025250651		[learning rate: 1.0429e-05]
		[batch 20/20] avg loss: 0.08636285540380391		[learning rate: 1.041e-05]
	Learning Rate: 1.04099e-05
	LOSS [training: 0.08227021782815522 | validation: 0.061073946213017496]
	TIME [epoch: 8.17 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07696162490853711		[learning rate: 1.0391e-05]
		[batch 20/20] avg loss: 0.09766576003332697		[learning rate: 1.0372e-05]
	Learning Rate: 1.03721e-05
	LOSS [training: 0.08731369247093206 | validation: 0.05795416651720469]
	TIME [epoch: 8.18 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08611520260408363		[learning rate: 1.0353e-05]
		[batch 20/20] avg loss: 0.08422807260676486		[learning rate: 1.0335e-05]
	Learning Rate: 1.03345e-05
	LOSS [training: 0.08517163760542425 | validation: 0.0692349184517823]
	TIME [epoch: 8.2 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08608963483899097		[learning rate: 1.0316e-05]
		[batch 20/20] avg loss: 0.08808479748810485		[learning rate: 1.0297e-05]
	Learning Rate: 1.0297e-05
	LOSS [training: 0.08708721616354789 | validation: 0.07073625859926108]
	TIME [epoch: 8.18 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07496572773571843		[learning rate: 1.0278e-05]
		[batch 20/20] avg loss: 0.0958254825055498		[learning rate: 1.026e-05]
	Learning Rate: 1.02596e-05
	LOSS [training: 0.08539560512063411 | validation: 0.06583161758556542]
	TIME [epoch: 8.18 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07910840923562026		[learning rate: 1.0241e-05]
		[batch 20/20] avg loss: 0.09587387790711542		[learning rate: 1.0222e-05]
	Learning Rate: 1.02224e-05
	LOSS [training: 0.08749114357136784 | validation: 0.06442606285348149]
	TIME [epoch: 8.18 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08709540918950529		[learning rate: 1.0204e-05]
		[batch 20/20] avg loss: 0.08883672809336833		[learning rate: 1.0185e-05]
	Learning Rate: 1.01853e-05
	LOSS [training: 0.08796606864143681 | validation: 0.057380488769414484]
	TIME [epoch: 8.19 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08314199281942461		[learning rate: 1.0167e-05]
		[batch 20/20] avg loss: 0.08721384664718097		[learning rate: 1.0148e-05]
	Learning Rate: 1.01483e-05
	LOSS [training: 0.0851779197333028 | validation: 0.05923122948642674]
	TIME [epoch: 8.18 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.09435199506509237		[learning rate: 1.013e-05]
		[batch 20/20] avg loss: 0.07042815136363809		[learning rate: 1.0112e-05]
	Learning Rate: 1.01115e-05
	LOSS [training: 0.08239007321436523 | validation: 0.05499831220320353]
	TIME [epoch: 8.18 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08910964710887553		[learning rate: 1.0093e-05]
		[batch 20/20] avg loss: 0.0802801235932962		[learning rate: 1.0075e-05]
	Learning Rate: 1.00748e-05
	LOSS [training: 0.08469488535108587 | validation: 0.06005492430477373]
	TIME [epoch: 8.17 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.07505127109462459		[learning rate: 1.0057e-05]
		[batch 20/20] avg loss: 0.09099049872243588		[learning rate: 1.0038e-05]
	Learning Rate: 1.00382e-05
	LOSS [training: 0.0830208849085302 | validation: 0.05309652654177554]
	TIME [epoch: 8.2 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.08431015430860096		[learning rate: 1.002e-05]
		[batch 20/20] avg loss: 0.07832917008440313		[learning rate: 1.0002e-05]
	Learning Rate: 1.00018e-05
	LOSS [training: 0.08131966219650205 | validation: 0.05752107758926574]
	TIME [epoch: 8.17 sec]
Finished training in 16513.185 seconds.
