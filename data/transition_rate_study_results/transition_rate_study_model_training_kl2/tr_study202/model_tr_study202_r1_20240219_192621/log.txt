Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r1', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1045015155

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.32955987807445		[learning rate: 0.01]
		[batch 20/20] avg loss: 9.718280377146668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.523920127610559 | validation: 6.574927146504171]
	TIME [epoch: 52.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.687083828650192		[learning rate: 0.01]
		[batch 20/20] avg loss: 6.496590755977709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.59183729231395 | validation: 6.155234156039159]
	TIME [epoch: 8.23 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.201225039882565		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.099005676574589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.150115358228577 | validation: 6.062872550451556]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.875395377604084		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.333867569120627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.604631473362356 | validation: 5.2645801825410015]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.9870494289546325		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.623789212270475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.805419320612553 | validation: 4.2977971144622265]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.138114247923329		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.01515830005586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.076636273989594 | validation: 4.315396041176907]
	TIME [epoch: 8.2 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.953099979643871		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.768180338067742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.860640158855806 | validation: 4.590118402216596]
	TIME [epoch: 8.17 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.464275392176821		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.42591958433979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.445097488258306 | validation: 5.456585900567606]
	TIME [epoch: 8.16 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.401298367887911		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.371490686253246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3863945270705775 | validation: 4.27385222458016]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.312045949260587		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.241270442122848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.276658195691717 | validation: 3.9760982877439686]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.8585937830240553		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.173434842954476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.016014312989265 | validation: 3.447971372230819]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7261362720430347		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.495182882654761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6106595773488985 | validation: 2.6463827426896542]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.0927699710900507		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.5291981126069913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3109840418485206 | validation: 2.826738309183595]
	TIME [epoch: 8.21 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7542270398291264		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.46093241894766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.607579729388393 | validation: 2.482136942518167]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.727755706575158		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3338908293835283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5308232679793434 | validation: 1.7430609137610173]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2605900245734016		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.340346213302964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3004681189381833 | validation: 2.3119264644510746]
	TIME [epoch: 8.19 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0694177538201646		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9512135957143026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0103156747672335 | validation: 1.8933555489029552]
	TIME [epoch: 8.21 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9098242477904102		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.221636325854772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0657302868225913 | validation: 2.014342394220876]
	TIME [epoch: 8.18 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0790581749018466		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8458532732655868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.962455724083717 | validation: 1.3768708889075387]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7594102027049368		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2034538018860355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9814320022954859 | validation: 3.1037317658259562]
	TIME [epoch: 8.2 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0738126973130298		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7648503862337386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9193315417733845 | validation: 1.8942809524165867]
	TIME [epoch: 8.2 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5819019624144288		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.750806330110187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.666354146262308 | validation: 1.422809355004885]
	TIME [epoch: 8.18 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6604610603503538		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.596186114539603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.628323587444978 | validation: 1.700233360886421]
	TIME [epoch: 8.18 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4353190584487263		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7794570907935303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6073880746211284 | validation: 1.842161286876268]
	TIME [epoch: 8.18 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.572949793383692		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.398555992503054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4857528929433728 | validation: 1.1722669303831055]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6019981993511305		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8222892502337775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.712143724792454 | validation: 1.1451270287899944]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4659763798141336		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2935016189918775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3797389994030056 | validation: 1.2516182199325527]
	TIME [epoch: 8.19 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2124141108181992		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6246442215403323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4185291661792658 | validation: 2.685596961100395]
	TIME [epoch: 8.21 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.55994943670601		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3872872489906778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.473618342848344 | validation: 1.877944996929363]
	TIME [epoch: 8.18 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.27715927964885		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3103895947216682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.293774437185259 | validation: 0.9785727493853619]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1928673204108415		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1904717542875596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1916695373492003 | validation: 0.9251224361975553]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1838921655958492		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.3233402216241472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.253616193609998 | validation: 0.8046585324759338]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.708049332851553		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5499194609360885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6289843968938207 | validation: 1.0418975064112836]
	TIME [epoch: 8.21 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2294325508235768		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.1988031332551121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2141178420393446 | validation: 1.4431948751820918]
	TIME [epoch: 8.18 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1454842543258408		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0883578034715615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1169210288987013 | validation: 1.0043619506404435]
	TIME [epoch: 8.2 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.075137334858038		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.5620266264533156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3185819806556769 | validation: 0.9093118768034595]
	TIME [epoch: 8.2 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1599412600492063		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.147181763856476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.153561511952841 | validation: 1.0514908647988654]
	TIME [epoch: 8.22 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0180437387370267		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2193136185264621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1186786786317442 | validation: 0.8331085001394886]
	TIME [epoch: 8.2 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0166867809448834		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0617551310972806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.039220956021082 | validation: 1.2702945138674007]
	TIME [epoch: 8.19 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0237163074902391		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.0408261599515467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0322712337208926 | validation: 0.7943473770624347]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9198487972586642		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9887953673479684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9543220823033163 | validation: 0.9785066037800911]
	TIME [epoch: 8.21 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8959355650541069		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9254137377341307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9106746513941187 | validation: 0.989791904368868]
	TIME [epoch: 8.2 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8968984976940249		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8772905304582042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8870945140761144 | validation: 0.7830845223467124]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8508388841605354		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.914857675176418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8828482796684767 | validation: 0.7833293201741466]
	TIME [epoch: 8.18 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9393771418220386		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8500374907130187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8947073162675286 | validation: 0.7641025430796824]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0084992255193852		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.066427492025598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0374633587724915 | validation: 0.5853765536516495]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1017875273122475		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8257350904520315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9637613088821395 | validation: 0.9727927823257153]
	TIME [epoch: 8.19 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9412337763030998		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8554721910637856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983529836834426 | validation: 0.5538947153216671]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8125378465522666		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.260104173333018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0363210099426423 | validation: 1.8649687225679954]
	TIME [epoch: 8.18 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.139371287495221		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9613772183732987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0503742529342603 | validation: 1.2750371366506448]
	TIME [epoch: 8.19 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8529669179016486		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.159441083708931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.00620400080529 | validation: 1.549204067355026]
	TIME [epoch: 8.19 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8715318250633676		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7802790920140827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8259054585387251 | validation: 1.0778986442303322]
	TIME [epoch: 8.18 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.036671187340026		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7665883077462873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9016297475431566 | validation: 0.6738883963358715]
	TIME [epoch: 8.18 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7717334671208195		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.732015998788555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518747329546873 | validation: 0.7102567279337917]
	TIME [epoch: 8.18 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.711741450575704		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6960924936870884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.703916972131396 | validation: 1.1013769204504265]
	TIME [epoch: 8.2 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1799101304520927		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9220273021759688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0509687163140309 | validation: 0.8008177116640991]
	TIME [epoch: 8.18 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9756560636174335		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7891523512132483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8824042074153408 | validation: 0.6423700759036567]
	TIME [epoch: 8.19 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7543999088701266		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7429738026881034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486868557791149 | validation: 1.1360053374814276]
	TIME [epoch: 8.18 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.9369269988189106		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6615595535294492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7992432761741799 | validation: 0.6812558805118355]
	TIME [epoch: 8.18 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7639829860110174		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6413044733117814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7026437296613994 | validation: 0.5558785593513329]
	TIME [epoch: 8.21 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.632192072546527		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6783985497140866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6552953111303068 | validation: 0.7190232470932791]
	TIME [epoch: 8.19 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6554109744188497		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6427047700898988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6490578722543743 | validation: 0.8527969792407112]
	TIME [epoch: 8.18 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6662179061746414		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.728413024802386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6973154654885136 | validation: 1.3170748169565103]
	TIME [epoch: 8.18 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6879190377024772		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.2830126290837438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9854658333931106 | validation: 0.4426250981004553]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6671521707999231		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7112703489636366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.68921125988178 | validation: 0.7465716603845417]
	TIME [epoch: 8.19 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6544058011529996		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6528813247761158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6536435629645576 | validation: 1.160273855794058]
	TIME [epoch: 8.18 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6665010113443308		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8662174397659594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7663592255551452 | validation: 1.5410479127590402]
	TIME [epoch: 8.18 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8994245851560725		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5720432944882315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357339398221521 | validation: 0.48015657508820137]
	TIME [epoch: 8.18 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8214387726292693		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5784930636418774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999659181355733 | validation: 0.34609845212086654]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0130936858309272		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7215689743628749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.867331330096901 | validation: 0.6154217477464021]
	TIME [epoch: 8.18 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8412427552054217		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5178754027882371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6795590789968293 | validation: 0.575805403913077]
	TIME [epoch: 8.18 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.707007032678183		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5791896204534384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6430983265658107 | validation: 0.5438389998893095]
	TIME [epoch: 8.18 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7885869912087695		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6473146093567025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179508002827361 | validation: 0.6443077158637209]
	TIME [epoch: 8.18 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7127228318151345		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.9566776856161218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.834700258715628 | validation: 0.47517939494871875]
	TIME [epoch: 8.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.639861956707221		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6062679722023087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.623064964454765 | validation: 0.3620146867197255]
	TIME [epoch: 8.18 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5958753131406006		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6632108198046177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6295430664726093 | validation: 0.5005548002229335]
	TIME [epoch: 8.18 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.740016007669336		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6102286429424688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6751223253059023 | validation: 0.4209229522834993]
	TIME [epoch: 8.18 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6755308317448009		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.622331585006657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.648931208375729 | validation: 0.3238270839260192]
	TIME [epoch: 8.2 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8338412059624023		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5193718010701058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6766065035162541 | validation: 0.6570896853723046]
	TIME [epoch: 8.2 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.859882712422829		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5505831174741008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052329149484649 | validation: 1.9700883634437056]
	TIME [epoch: 8.2 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8558479521817327		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6319408088273779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7438943805045554 | validation: 0.6080204387559041]
	TIME [epoch: 8.2 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6027429136239932		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6112940270774331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6070184703507131 | validation: 1.2774937374295552]
	TIME [epoch: 8.19 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7433824660538281		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5956705468057983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6695265064298133 | validation: 0.7142850086569364]
	TIME [epoch: 8.22 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6795418178647834		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5877727778546106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.633657297859697 | validation: 0.3681831376532232]
	TIME [epoch: 8.2 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5341529739882392		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7650614153179979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6496071946531184 | validation: 0.9909728783072551]
	TIME [epoch: 8.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8044461440646717		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6789587770935845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7417024605791278 | validation: 1.2160527909096965]
	TIME [epoch: 8.2 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7223193103414948		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5497529226385893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360361164900421 | validation: 0.31165394117157497]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6038996873885116		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.55817018153924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5810349344638759 | validation: 0.351177769329326]
	TIME [epoch: 8.21 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.619622787406078		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6866687910060009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6531457892060394 | validation: 0.46605623793217543]
	TIME [epoch: 8.19 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5544978132456609		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.6183708464204758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5864343298330683 | validation: 0.35298780058619017]
	TIME [epoch: 8.19 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5493066688406679		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5509644789208357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5501355738807516 | validation: 0.49550454972162455]
	TIME [epoch: 8.19 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5227038504621857		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4614965614027569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4921002059324713 | validation: 0.6217181939925056]
	TIME [epoch: 8.22 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6362182553049627		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5163326788230853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5762754670640239 | validation: 0.3847186335957939]
	TIME [epoch: 8.2 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47804388139400195		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.5186965935496642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.498370237471833 | validation: 0.3297587130104081]
	TIME [epoch: 8.19 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5334649996929065		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.4890005649783585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5112327823356324 | validation: 0.8431461047260869]
	TIME [epoch: 8.19 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5282979822643753		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.7978779516688941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630879669666345 | validation: 0.4687984744680591]
	TIME [epoch: 8.2 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5170531056155037		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.8765337637373625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696793434676433 | validation: 0.8755938195640662]
	TIME [epoch: 8.22 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.559865329631055		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45854057165185014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5092029506414526 | validation: 0.4128384401862417]
	TIME [epoch: 8.19 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5298398674509389		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.42775224826452724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4787960578577331 | validation: 0.2949887148829733]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7722994893877916		[learning rate: 0.01]
		[batch 20/20] avg loss: 0.45335947382973635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6128294816087639 | validation: 0.31575549206793024]
	TIME [epoch: 8.2 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44925254938636205		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 0.5755394776133054		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 0.5123960134998338 | validation: 0.35777319480624753]
	TIME [epoch: 8.22 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4855829775973066		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 0.4644923130160028		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 0.4750376453066547 | validation: 0.46934332257764383]
	TIME [epoch: 8.2 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.410286858177986		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 0.4906637470665996		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 0.4504753026222928 | validation: 1.0466461255925457]
	TIME [epoch: 8.19 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5452744179671681		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 0.4719268320587263		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 0.5086006250129471 | validation: 1.4879941655126807]
	TIME [epoch: 8.19 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6279257705980553		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 0.6622298157169826		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 0.6450777931575189 | validation: 1.2863267720586737]
	TIME [epoch: 8.19 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6140024359701834		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 0.5945704999345559		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 0.6042864679523697 | validation: 0.9897379229312444]
	TIME [epoch: 8.22 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5130492755666645		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 0.4313050203637581		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 0.4721771479652113 | validation: 0.7030050892291685]
	TIME [epoch: 8.19 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4475456834312852		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 0.41004899780362825		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 0.4287973406174568 | validation: 0.6774919649307648]
	TIME [epoch: 8.19 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4315801666786041		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 0.45842766415746594		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 0.4450039154180351 | validation: 0.19832427436401964]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4523356753702645		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 0.5096622285306027		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 0.4809989519504335 | validation: 0.34417397749208156]
	TIME [epoch: 8.19 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4096341700847441		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 0.5140895417088045		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 0.4618618558967743 | validation: 0.23937387509994143]
	TIME [epoch: 8.22 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42367689649074086		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 0.462978843822325		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 0.4433278701565329 | validation: 0.525305703122096]
	TIME [epoch: 8.19 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.503809856236129		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 0.47219051864971817		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 0.48800018744292367 | validation: 0.5938980525430995]
	TIME [epoch: 8.19 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41717340417930987		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 0.539876287293098		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 0.4785248457362039 | validation: 0.39238365097673245]
	TIME [epoch: 8.19 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46028986794638804		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 0.45414930076008575		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 0.45721958435323684 | validation: 0.37299551564818806]
	TIME [epoch: 8.21 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4457086847143755		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 0.5728405506845597		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 0.5092746176994675 | validation: 0.39623606912092246]
	TIME [epoch: 8.19 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4543957523436801		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 0.6172379605976576		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 0.5358168564706689 | validation: 0.36606330207542903]
	TIME [epoch: 8.19 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45117235617520846		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 0.3946671901146179		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 0.42291977314491314 | validation: 0.21904691626806866]
	TIME [epoch: 8.19 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6101504270614387		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 0.5641626292150866		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 0.5871565281382628 | validation: 0.9706216237069523]
	TIME [epoch: 8.18 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.591984044666968		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 0.39609121142057097		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 0.4940376280437695 | validation: 0.20303580932133378]
	TIME [epoch: 8.21 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3509457335320644		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 0.3655042676524648		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 0.3582250005922646 | validation: 0.3690424214013985]
	TIME [epoch: 8.19 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3242497008598234		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 0.4259148698626767		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 0.37508228536125 | validation: 0.21760555227499012]
	TIME [epoch: 8.18 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3431155139780281		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 0.5671460429573927		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 0.45513077846771044 | validation: 0.5195409246290144]
	TIME [epoch: 8.18 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4577358892635027		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 0.3756249937715169		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 0.4166804415175099 | validation: 0.260247244013284]
	TIME [epoch: 8.19 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3578271075120914		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 0.5866280236284617		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 0.47222756557027656 | validation: 0.42346243416446727]
	TIME [epoch: 8.21 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4188856922909559		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 0.4138832166164036		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 0.4163844544536798 | validation: 0.3976868450864664]
	TIME [epoch: 8.18 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5924470643058507		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 0.49109977021050766		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 0.5417734172581793 | validation: 0.22052136558365756]
	TIME [epoch: 8.18 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3955374540741799		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 0.35479802661557247		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 0.37516774034487615 | validation: 0.36371509775167127]
	TIME [epoch: 8.18 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4404494617420799		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 0.37279768699876464		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 0.40662357437042235 | validation: 0.1574415175786884]
	TIME [epoch: 8.21 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5146447173013875		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 0.4855775838176177		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 0.5001111505595027 | validation: 0.36653593989348277]
	TIME [epoch: 8.19 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4125964458005688		[learning rate: 0.008952]
		[batch 20/20] avg loss: 0.5515643116111535		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 0.4820803787058612 | validation: 0.2765169895359878]
	TIME [epoch: 8.18 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3681302604021385		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 0.4806812605096268		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 0.42440576045588274 | validation: 0.2790797616734103]
	TIME [epoch: 8.19 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3668252702219014		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 0.49993707462928294		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 0.43338117242559215 | validation: 0.32000818102432405]
	TIME [epoch: 8.18 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4397389542103676		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 0.40412205580546523		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 0.4219305050079164 | validation: 0.5541759334616909]
	TIME [epoch: 8.21 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4641986346569418		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 0.3607844230285451		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 0.4124915288427434 | validation: 0.28206902173757475]
	TIME [epoch: 8.18 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44636190851126345		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 0.45758656329284103		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 0.45197423590205227 | validation: 0.4672275661261726]
	TIME [epoch: 8.19 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43597887600889945		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 0.6377261571048694		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 0.5368525165568845 | validation: 0.36041536477343855]
	TIME [epoch: 8.19 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5124692970102382		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 0.43321805660623836		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 0.4728436768082383 | validation: 0.32527398352174314]
	TIME [epoch: 8.19 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3748719515929172		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 0.38205620172727983		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 0.3784640766600985 | validation: 0.3213557721048702]
	TIME [epoch: 8.21 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3669144602849195		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 0.5551456708889362		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 0.46103006558692783 | validation: 0.3428347615364166]
	TIME [epoch: 8.18 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5379998796843719		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 0.3464303637137244		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 0.4422151216990481 | validation: 0.5623538695822851]
	TIME [epoch: 8.19 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5055202223837172		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 0.43681191844014106		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 0.4711660704119291 | validation: 0.8158402810796551]
	TIME [epoch: 8.19 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47221824389242045		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 0.48001464147172823		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 0.4761164426820745 | validation: 0.896401362928667]
	TIME [epoch: 8.21 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5756740808457745		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 0.7234607307370894		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 0.6495674057914318 | validation: 0.3574186760878255]
	TIME [epoch: 8.19 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38030828639494185		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 0.4546480383995507		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 0.41747816239724633 | validation: 0.7382864017459265]
	TIME [epoch: 8.18 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42066838559028286		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 0.43024950819340235		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 0.4254589468918426 | validation: 0.5326107016915413]
	TIME [epoch: 8.18 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4513619290975819		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 0.36772926825220054		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 0.4095455986748912 | validation: 0.5230007874646728]
	TIME [epoch: 8.18 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5354091755987276		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 0.41702651389629974		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 0.47621784474751366 | validation: 0.4237300594189227]
	TIME [epoch: 8.21 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.49946578187706836		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 0.6368192177544587		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 0.5681424998157636 | validation: 0.3549155375110844]
	TIME [epoch: 8.19 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5120005542794919		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 0.37933125133270584		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 0.44566590280609886 | validation: 0.7645850324076847]
	TIME [epoch: 8.18 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44358029374641		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 0.4366046355035751		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 0.44009246462499246 | validation: 0.30087286220071097]
	TIME [epoch: 8.18 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38825949169552354		[learning rate: 0.008294]
		[batch 20/20] avg loss: 0.42927632535620497		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 0.40876790852586425 | validation: 0.4062085262476595]
	TIME [epoch: 8.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4764906811302387		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 0.5540609006001311		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 0.515275790865185 | validation: 0.7613319035891556]
	TIME [epoch: 8.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42080143664008673		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 0.4333440985317812		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 0.427072767585934 | validation: 0.6859940434596052]
	TIME [epoch: 8.18 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39196187096529467		[learning rate: 0.008204]
		[batch 20/20] avg loss: 0.49727865759691453		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 0.4446202642811047 | validation: 0.28809723687587424]
	TIME [epoch: 8.18 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41338040914582475		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 0.3970314230254652		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 0.405205916085645 | validation: 1.0525030248241432]
	TIME [epoch: 8.19 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4834522442385355		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 0.5565249935046495		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 0.5199886188715925 | validation: 0.3522984346667192]
	TIME [epoch: 8.21 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5351524841708662		[learning rate: 0.008115]
		[batch 20/20] avg loss: 0.4854417431242859		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 0.510297113647576 | validation: 0.4104944778366299]
	TIME [epoch: 8.19 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4668976714701178		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 0.47846112851113054		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 0.4726793999906242 | validation: 0.8722532766839739]
	TIME [epoch: 8.19 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5681965085692834		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 0.5136791448154344		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 0.5409378266923589 | validation: 0.30252749816606467]
	TIME [epoch: 8.18 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45432988330509094		[learning rate: 0.008027]
		[batch 20/20] avg loss: 0.4535523185452751		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 0.45394110092518297 | validation: 0.26344628597704006]
	TIME [epoch: 8.19 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46052357905988883		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 0.4603171275364346		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 0.4604203532981618 | validation: 0.5553831056283008]
	TIME [epoch: 8.21 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38464747258482535		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 0.4264808467550286		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 0.4055641596699269 | validation: 0.365877431381334]
	TIME [epoch: 8.19 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42484070947926045		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 0.3996460226518378		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 0.412243366065549 | validation: 0.490423895534425]
	TIME [epoch: 8.19 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4848969171439451		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 0.5247643984704476		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 0.5048306578071965 | validation: 0.29158815155155693]
	TIME [epoch: 8.19 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40896172010721576		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 0.47466575051965015		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 0.44181373531343293 | validation: 0.23545665403637375]
	TIME [epoch: 8.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41078884412620387		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 0.606097094595949		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 0.5084429693610765 | validation: 0.4617235935633891]
	TIME [epoch: 8.2 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42783322245321076		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 0.36546445594592414		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 0.39664883919956745 | validation: 1.7151779461473264]
	TIME [epoch: 8.19 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6747871405751511		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 0.5422568509536082		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 0.6085219957643797 | validation: 0.46614879634298423]
	TIME [epoch: 8.19 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47667168699021556		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 0.5536136927886931		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 0.5151426898894542 | validation: 0.3191715467797555]
	TIME [epoch: 8.19 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38106075055839994		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 0.4326394393801425		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 0.4068500949692712 | validation: 0.3507492371306349]
	TIME [epoch: 8.22 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7271895055356882		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 0.4242159055886874		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 0.5757027055621877 | validation: 0.28081899488446804]
	TIME [epoch: 8.2 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3973383202991597		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 0.4449648871688052		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 0.4211516037339825 | validation: 0.5609889837525041]
	TIME [epoch: 8.19 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4917520463627364		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 0.4539933967161895		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 0.472872721539463 | validation: 0.7696249034630758]
	TIME [epoch: 8.19 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4484287721492703		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 0.5171917921934044		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 0.48281028217133726 | validation: 0.6167199113938726]
	TIME [epoch: 8.19 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5714463360006516		[learning rate: 0.007601]
		[batch 20/20] avg loss: 0.4597562763118165		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 0.5156013061562341 | validation: 0.27693783576873887]
	TIME [epoch: 8.22 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38534627358166645		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 0.40731649084704563		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 0.39633138221435604 | validation: 0.1948757343108254]
	TIME [epoch: 8.19 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4149471760287913		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 0.4543823510581685		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 0.4346647635434798 | validation: 0.4523563651009432]
	TIME [epoch: 8.19 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41891009030142456		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 0.5045453768422357		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 0.46172773357183017 | validation: 0.47254855793438016]
	TIME [epoch: 8.19 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4430539207764646		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 0.598317415484065		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 0.520685668130265 | validation: 0.27563913230107234]
	TIME [epoch: 8.21 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.43255938285994383		[learning rate: 0.007464]
		[batch 20/20] avg loss: 0.500385932285887		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 0.4664726575729154 | validation: 1.0131459779536098]
	TIME [epoch: 8.21 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5228079103616301		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 0.4722247646755802		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 0.49751633751860525 | validation: 0.4017007076169795]
	TIME [epoch: 8.19 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4963113711387742		[learning rate: 0.00741]
		[batch 20/20] avg loss: 0.46141204784214096		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 0.4788617094904578 | validation: 0.5033516792068826]
	TIME [epoch: 8.2 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.44716656684641143		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 0.45718984415024055		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 0.452178205498326 | validation: 0.5559168732599393]
	TIME [epoch: 8.19 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5687179448257409		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 0.6331376554844159		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 0.6009278001550784 | validation: 0.2913325033213638]
	TIME [epoch: 8.22 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47357515034517644		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 0.42844004914720524		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 0.4510075997461909 | validation: 1.5149873684674011]
	TIME [epoch: 8.19 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.625380452027209		[learning rate: 0.007303]
		[batch 20/20] avg loss: 0.7286988557009478		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 0.6770396538640785 | validation: 0.2623368444380161]
	TIME [epoch: 8.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5405821983770569		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 0.8566457635836497		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 0.6986139809803534 | validation: 0.4426649738128617]
	TIME [epoch: 8.19 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.506365754126402		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 0.5224916688470727		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 0.5144287114867373 | validation: 0.6005384183565586]
	TIME [epoch: 8.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.574648713527714		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 0.574086426448263		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 0.5743675699879884 | validation: 0.5108308787968489]
	TIME [epoch: 8.22 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45679297025208776		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 0.5825134420557923		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 0.51965320615394 | validation: 0.6361570662204012]
	TIME [epoch: 8.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4152158911448721		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 0.52562318980033		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 0.4704195404726009 | validation: 0.2766792827226926]
	TIME [epoch: 8.2 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6698649110206765		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 0.4192904192609623		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 0.5445776651408194 | validation: 0.4302138738491453]
	TIME [epoch: 8.19 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39333509159663343		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 0.5087124711362608		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 0.45102378136644716 | validation: 0.26648567764486253]
	TIME [epoch: 8.22 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3538788197963825		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 0.4283303409864659		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 0.3911045803914242 | validation: 0.38971750714237974]
	TIME [epoch: 8.2 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3853872611808426		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 0.47055420746323334		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 0.427970734322038 | validation: 0.31515697695922196]
	TIME [epoch: 8.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39228646987780147		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 0.5396407305991356		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 0.4659636002384685 | validation: 0.3191428662658294]
	TIME [epoch: 8.2 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36408167490053694		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 0.3881425911408475		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 0.37611213302069213 | validation: 0.35804824264581064]
	TIME [epoch: 8.19 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40834344302545916		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 0.46515511176672125		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 0.43674927739609026 | validation: 0.38303618372385806]
	TIME [epoch: 8.22 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.401106340844684		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 0.4417764876340352		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 0.4214414142393597 | validation: 0.5687214520881322]
	TIME [epoch: 8.2 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3913248790444933		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 0.5076300647985775		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 0.44947747192153537 | validation: 0.4680682147543727]
	TIME [epoch: 8.2 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38780765614527757		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 0.4560677159117126		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 0.42193768602849513 | validation: 0.38761188272385294]
	TIME [epoch: 8.19 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3356720251115707		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 0.4785115410268343		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 0.4070917830692026 | validation: 0.2797200412825877]
	TIME [epoch: 8.19 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.617336474771889		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 0.5061012805091956		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 0.5617188776405423 | validation: 0.6182379661271754]
	TIME [epoch: 8.22 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7215222945633697		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 0.5954536689106767		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 0.6584879817370232 | validation: 0.29677083660870224]
	TIME [epoch: 8.2 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38011790406567203		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 0.4317538519586437		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 0.4059358780121579 | validation: 0.3847173447681145]
	TIME [epoch: 8.19 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3846368542693603		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 0.42175305012241415		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 0.40319495219588725 | validation: 0.41121182720208793]
	TIME [epoch: 8.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.483456016750379		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 0.5148369428663271		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 0.499146479808353 | validation: 0.3884039074306931]
	TIME [epoch: 8.22 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37610163139023983		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 0.3434985070906941		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 0.3598000692404669 | validation: 0.4105555636003977]
	TIME [epoch: 8.2 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.39468625231321214		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 0.386125247760442		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 0.3904057500368271 | validation: 0.1897073014132712]
	TIME [epoch: 8.19 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4701156540552664		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 0.42691411658853273		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 0.44851488532189954 | validation: 0.2268490644332713]
	TIME [epoch: 8.2 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3132921895704535		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 0.3464622025704948		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 0.32987719607047417 | validation: 0.35593920785140837]
	TIME [epoch: 8.2 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34691673862547534		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 4.75215644232361		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 2.549536590474543 | validation: 6.484878002261594]
	TIME [epoch: 8.22 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.977503208590406		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 6.537726888811103		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 6.257615048700754 | validation: 5.149701723062499]
	TIME [epoch: 8.2 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.1078379101103675		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 5.093178201780803		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 4.6005080559455855 | validation: 5.508120860497078]
	TIME [epoch: 8.2 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.353126543799158		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 1.65089027127175		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 3.502008407535454 | validation: 0.6669182249933908]
	TIME [epoch: 8.2 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5377212951528296		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 0.5900130244936639		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 0.5638671598232468 | validation: 0.3719325141996609]
	TIME [epoch: 8.2 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4491702159367602		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 0.6042958470117122		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 0.5267330314742362 | validation: 0.4434390995062793]
	TIME [epoch: 8.23 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0331461254314394		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 0.4292471845203333		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 0.7311966549758864 | validation: 0.8535487781245201]
	TIME [epoch: 8.2 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.47120806622154376		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 0.6530601973524581		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 0.5621341317870009 | validation: 0.5872425906023968]
	TIME [epoch: 8.2 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6284051434279398		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 0.46879143215019636		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 0.548598287789068 | validation: 0.34201554495092784]
	TIME [epoch: 8.2 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.48919272035702627		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 0.39656523777222424		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 0.44287897906462526 | validation: 0.4895264576205958]
	TIME [epoch: 8.22 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.528059707635818		[learning rate: 0.006407]
		[batch 20/20] avg loss: 1.1046271692255258		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 0.816343438430672 | validation: 0.3464524467766256]
	TIME [epoch: 8.2 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4804718199894701		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 0.47852884232277626		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 0.4795003311561231 | validation: 0.3288539567204055]
	TIME [epoch: 8.2 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40691187672942314		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 0.4658125681993489		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 0.43636222246438605 | validation: 0.36278185341869995]
	TIME [epoch: 8.2 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3421953306881977		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 0.5046850499282681		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 0.4234401903082329 | validation: 0.2402130008384204]
	TIME [epoch: 8.2 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3566053777441356		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 0.667814824447825		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 0.5122101010959803 | validation: 0.4374756876007712]
	TIME [epoch: 8.23 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3179461261979046		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 0.4832557456832324		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 0.40060093594056845 | validation: 0.2409973977771456]
	TIME [epoch: 8.2 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42015946445709196		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 1.1722327617616073		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 0.7961961131093496 | validation: 0.2772221324557734]
	TIME [epoch: 8.2 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6501411972842858		[learning rate: 0.006246]
		[batch 20/20] avg loss: 0.3661594121652071		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 0.5081503047247465 | validation: 0.23482966990528267]
	TIME [epoch: 8.2 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3594013526060795		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 0.5923719494744233		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 0.47588665104025135 | validation: 0.348404754593441]
	TIME [epoch: 8.2 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40185367795215365		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 0.5957986363377064		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 0.49882615714493 | validation: 0.5068199174765138]
	TIME [epoch: 8.22 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3767369834547466		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 0.420444572532531		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 0.39859077799363873 | validation: 0.7402532705861655]
	TIME [epoch: 8.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40476704511573763		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 0.4337359534275662		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 0.41925149927165195 | validation: 0.4175857032211679]
	TIME [epoch: 8.2 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2941303499481379		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 0.5585172092623596		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 0.4263237796052487 | validation: 0.3075351759925612]
	TIME [epoch: 8.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3259194172318433		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 0.39083204099981417		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 0.3583757291158287 | validation: 0.35106563528183543]
	TIME [epoch: 8.21 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3914909742680739		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 0.820793767026182		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 0.6061423706471281 | validation: 1.9598932714724104]
	TIME [epoch: 8.19 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6398000886630071		[learning rate: 0.006067]
		[batch 20/20] avg loss: 0.34920789817875175		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 0.49450399342087936 | validation: 1.916193495921903]
	TIME [epoch: 8.19 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4858040222549563		[learning rate: 0.006045]
		[batch 20/20] avg loss: 0.45996955758110836		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 0.47288678991803235 | validation: 0.3656213602034305]
	TIME [epoch: 8.19 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3618816677503321		[learning rate: 0.006023]
		[batch 20/20] avg loss: 0.38733759492742686		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 0.3746096313388794 | validation: 0.3087929238888274]
	TIME [epoch: 8.19 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3414734513721426		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 3.5971523824796385		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 1.9693129169258898 | validation: 7.396847785937255]
	TIME [epoch: 8.21 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.923902666700068		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 1.2481859902977739		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 4.08604432849892 | validation: 0.5849644140163639]
	TIME [epoch: 8.19 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4017018668692581		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 0.34458845104833796		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 0.3731451589587979 | validation: 0.542704210085368]
	TIME [epoch: 8.19 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3751363690402898		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 0.366127024947019		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 0.3706316969936544 | validation: 0.3894564055489924]
	TIME [epoch: 8.19 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3006197434615969		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 0.3446446392303647		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 0.3226321913459808 | validation: 0.5002762405152674]
	TIME [epoch: 8.19 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4092388009578459		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 0.36956974559893163		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 0.3894042732783887 | validation: 0.23007340336185692]
	TIME [epoch: 8.22 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3111330414244327		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 0.3597747570344091		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 0.33545389922942087 | validation: 0.2778681246364433]
	TIME [epoch: 8.19 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29408599107277017		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 0.36384600291445796		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 0.32896599699361406 | validation: 0.2635382568205028]
	TIME [epoch: 8.19 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5277853825831959		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 0.2720586193917714		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 0.39992200098748365 | validation: 0.5187374115535145]
	TIME [epoch: 8.21 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4095283171182901		[learning rate: 0.005808]
		[batch 20/20] avg loss: 0.32237199961210616		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 0.3659501583651981 | validation: 0.26336420795603416]
	TIME [epoch: 8.24 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.405401081266049		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 0.28091096058158804		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 0.3431560209238185 | validation: 0.2440456719745005]
	TIME [epoch: 8.21 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.956108550752038		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 4.104438777599998		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 3.5302736641760184 | validation: 0.33851803197429775]
	TIME [epoch: 8.21 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5265078146702005		[learning rate: 0.005745]
		[batch 20/20] avg loss: 0.4382596497056032		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 0.48238373218790187 | validation: 0.6778930134606497]
	TIME [epoch: 8.21 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4168765075322833		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 1.3401174768399797		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 0.8784969921861314 | validation: 0.37904579458532695]
	TIME [epoch: 8.21 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4095327772091063		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 0.48245001657017406		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 0.44599139688964023 | validation: 0.5281872414202909]
	TIME [epoch: 8.23 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3519610858539625		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 0.5365644322555913		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 0.44426275905477686 | validation: 0.3168698120742466]
	TIME [epoch: 8.22 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3539052455318112		[learning rate: 0.005662]
		[batch 20/20] avg loss: 0.33658339415317495		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 0.345244319842493 | validation: 0.24467706605529693]
	TIME [epoch: 8.21 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3670783365111158		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 0.3076606550303479		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 0.3373694957707318 | validation: 0.3221545529732916]
	TIME [epoch: 8.21 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.40659167129479573		[learning rate: 0.005621]
		[batch 20/20] avg loss: 0.3470335924723963		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 0.37681263188359604 | validation: 0.4396848132294431]
	TIME [epoch: 8.22 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41248757984717743		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 0.30650155301480975		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 0.35949456643099353 | validation: 0.26060082109616284]
	TIME [epoch: 8.23 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.431968416928711		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 0.403540843347134		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 0.4177546301379224 | validation: 0.46733502781261993]
	TIME [epoch: 8.21 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37294016582624145		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 0.3829719854428036		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 0.37795607563452255 | validation: 0.40953123203435415]
	TIME [epoch: 8.18 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3466230975164241		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 0.4335723480950911		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 0.3900977228057576 | validation: 0.7534480092933955]
	TIME [epoch: 8.18 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46424913066935425		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 4.705328847229188		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 2.584788988949271 | validation: 7.829239872129605]
	TIME [epoch: 8.2 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.581602672234527		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 7.649512782733472		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 7.615557727484 | validation: 7.468449172522773]
	TIME [epoch: 8.22 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.0245512699004236		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 6.241711381102771		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 6.633131325501597 | validation: 3.2070184200556677]
	TIME [epoch: 8.23 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.0416395450491076		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 0.43175787472733196		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 0.7366987098882198 | validation: 0.3183088496533553]
	TIME [epoch: 8.22 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4506427898592141		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 0.48532488130129475		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 0.4679838355802544 | validation: 0.33276319413433536]
	TIME [epoch: 8.22 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3725529727956924		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 0.3250164202971103		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 0.34878469654640143 | validation: 0.3379799708461233]
	TIME [epoch: 8.23 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3383215267271109		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 0.29102689826209643		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 0.31467421249460364 | validation: 0.28671090281966305]
	TIME [epoch: 8.2 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3181382980459279		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 0.3601700787336247		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 0.3391541883897764 | validation: 0.320678695223428]
	TIME [epoch: 8.2 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3389688105929545		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 0.31637841362175423		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 0.32767361210735435 | validation: 0.6978243760554516]
	TIME [epoch: 8.21 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41097575497442823		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 0.3119742235735904		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 0.36147498927400934 | validation: 0.37570832198534837]
	TIME [epoch: 8.22 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34002559150106615		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 0.3423308906182413		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 0.34117824105965366 | validation: 0.19582928301126185]
	TIME [epoch: 8.2 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3231848354641034		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 0.3058262608137897		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 0.3145055481389466 | validation: 0.3478192084330933]
	TIME [epoch: 8.19 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2996368647775303		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 0.43250437717842116		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 0.36607062097797577 | validation: 0.4090853573108266]
	TIME [epoch: 8.19 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3505158533883631		[learning rate: 0.005265]
		[batch 20/20] avg loss: 0.30112432660639993		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 0.32582008999738155 | validation: 0.20744601166802124]
	TIME [epoch: 8.19 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2348997326753502		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 0.3319099893875494		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 0.28340486103144985 | validation: 0.30498826209061247]
	TIME [epoch: 8.21 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27806838507889925		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 0.25608401881077236		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 0.26707620194483583 | validation: 0.3305758745562403]
	TIME [epoch: 8.19 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.364051020766191		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 0.2444531113488086		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 0.30425206605749977 | validation: 0.21595515110969102]
	TIME [epoch: 8.19 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33164865580025527		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 0.48823160444882907		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 0.40994013012454217 | validation: 0.2133828015692472]
	TIME [epoch: 8.19 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25308867256451306		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 0.3907572582171839		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 0.32192296539084847 | validation: 0.2154456721709355]
	TIME [epoch: 8.19 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25450320719093467		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 0.33354401376356163		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 0.2940236104772481 | validation: 0.33551693326859655]
	TIME [epoch: 8.21 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3049600515257719		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 0.3272876841327824		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 0.31612386782927715 | validation: 0.5672037951003406]
	TIME [epoch: 8.19 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3450493335508147		[learning rate: 0.005114]
		[batch 20/20] avg loss: 0.3092294747102167		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 0.3271394041305157 | validation: 0.2121754653310544]
	TIME [epoch: 8.18 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31891276670663976		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 0.3269640997433246		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 0.32293843322498217 | validation: 0.4156415711099517]
	TIME [epoch: 8.19 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24194159303263557		[learning rate: 0.005077]
		[batch 20/20] avg loss: 0.34688908255717427		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 0.2944153377949049 | validation: 0.2951876488988748]
	TIME [epoch: 8.2 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28429480597149237		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 0.28074163623214615		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 0.28251822110181923 | validation: 0.2890943292799573]
	TIME [epoch: 8.2 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3638679783033489		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 0.3065289620541076		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 0.3351984701787282 | validation: 0.35483155241073394]
	TIME [epoch: 8.19 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32672116195471373		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 0.27135539632153305		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 0.2990382791381233 | validation: 0.18575253533395172]
	TIME [epoch: 8.19 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3212812292051462		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 0.26505221452167127		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 0.2931667218634087 | validation: 0.19828737418792494]
	TIME [epoch: 8.19 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2592018169688368		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 0.28179733686841263		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 0.2704995769186248 | validation: 0.26033658416619015]
	TIME [epoch: 8.21 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2545682847070598		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 0.28813816008705123		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 0.2713532223970555 | validation: 0.1612200158429831]
	TIME [epoch: 8.19 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4923856775141685		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 0.4064418612770681		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 0.44941376939561845 | validation: 1.6060999972462167]
	TIME [epoch: 8.19 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.41815648130626953		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 0.23360488401457052		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 0.3258806826604199 | validation: 0.37185052856429457]
	TIME [epoch: 8.19 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35951359055841525		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 0.2461322790222913		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 0.3028229347903533 | validation: 0.16655429782632647]
	TIME [epoch: 8.19 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2736963989119676		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 0.2937176399741646		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 0.2837070194430661 | validation: 0.5108879758971705]
	TIME [epoch: 8.21 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.37115720524610246		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 0.30372533459933737		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 0.3374412699227199 | validation: 0.2742210216129031]
	TIME [epoch: 8.19 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2823933805842872		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 0.2808688063506887		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 0.28163109346748794 | validation: 0.23879757026774634]
	TIME [epoch: 8.19 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3748482370881282		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 1.3520836459484769		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 0.8634659415183025 | validation: 0.29273940667610543]
	TIME [epoch: 8.18 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.42552444313454796		[learning rate: 0.004825]
		[batch 20/20] avg loss: 0.2767018410951407		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 0.3511131421148443 | validation: 0.2098335038828825]
	TIME [epoch: 8.2 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29109009653112394		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 0.31494651415187574		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 0.3030183053414998 | validation: 0.17124424443154218]
	TIME [epoch: 8.2 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.32740131966062974		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 0.3760028105814673		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 0.3517020651210486 | validation: 0.5877163847458999]
	TIME [epoch: 8.18 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38846598470293586		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 0.24224482791170843		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 0.3153554063073221 | validation: 0.23409223119557243]
	TIME [epoch: 8.19 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24921129637973247		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 0.2530555525364291		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 0.2511334244580808 | validation: 0.30509990087693817]
	TIME [epoch: 8.19 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23831346658540992		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 0.2799454850942953		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 0.25912947583985263 | validation: 0.24740231995110396]
	TIME [epoch: 8.21 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27373379072269816		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 0.2890733527601102		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 0.2814035717414042 | validation: 0.15530609110815588]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26878725694211225		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 0.2411988236247508		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 0.2549930402834315 | validation: 0.184357444888305]
	TIME [epoch: 8.18 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30037146597052405		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 0.25766464641949227		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 0.27901805619500814 | validation: 0.28114996240034235]
	TIME [epoch: 8.18 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2580063994494497		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 0.27803798327228046		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 0.26802219136086514 | validation: 0.2703209747349038]
	TIME [epoch: 8.18 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27431038910729494		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 0.32792877561045397		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 0.3011195823588744 | validation: 0.245089776599093]
	TIME [epoch: 8.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25049217135837315		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 0.32722646586871085		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 0.28885931861354197 | validation: 0.2047337626983762]
	TIME [epoch: 8.18 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2517368153774365		[learning rate: 0.004619]
		[batch 20/20] avg loss: 0.2265047625488666		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 0.23912078896315153 | validation: 0.24284530659781173]
	TIME [epoch: 8.18 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28419970698597813		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 0.26773417221404305		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 0.2759669396000107 | validation: 0.1458883388988962]
	TIME [epoch: 8.18 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23820985612331053		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 0.27488152513926895		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 0.25654569063128974 | validation: 0.5289568612760547]
	TIME [epoch: 8.2 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3044228211121285		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 0.22825584811616903		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 0.2663393346141488 | validation: 0.3079071884841076]
	TIME [epoch: 8.18 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2541735098700674		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 0.27872490853999377		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 0.26644920920503057 | validation: 0.19353583011750686]
	TIME [epoch: 8.17 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2481482794348465		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 0.24042183904101022		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 0.24428505923792837 | validation: 0.27903958360893955]
	TIME [epoch: 8.17 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27552611288006473		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 0.2744555939044327		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 0.2749908533922487 | validation: 0.6677421999404187]
	TIME [epoch: 8.17 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7096532501685094		[learning rate: 0.004503]
		[batch 20/20] avg loss: 0.24137633657041566		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 0.47551479336946256 | validation: 0.13916708945436315]
	TIME [epoch: 8.19 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26609063461727206		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 0.46797501059034297		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 0.3670328226038075 | validation: 0.1548939050857838]
	TIME [epoch: 8.18 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3037736775128891		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 0.2798958923978613		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 0.2918347849553752 | validation: 0.14485534854821064]
	TIME [epoch: 8.17 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27679191163791533		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 0.2774183113313974		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 0.2771051114846564 | validation: 0.24800971204803907]
	TIME [epoch: 8.17 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26575861665440054		[learning rate: 0.004438]
		[batch 20/20] avg loss: 0.2202996319997334		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 0.24302912432706694 | validation: 0.2089627055248541]
	TIME [epoch: 8.17 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25043700462103524		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 0.2759418801732806		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 0.2631894423971579 | validation: 0.3744912537319258]
	TIME [epoch: 8.19 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.35101740185467023		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 0.25563693813967553		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 0.30332716999717285 | validation: 0.15010858925383494]
	TIME [epoch: 8.17 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2603766471899685		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 0.2637100006239286		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 0.2620433239069485 | validation: 0.13541287737452246]
	TIME [epoch: 8.17 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240219_192621/states/model_tr_study202_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34478456313551165		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 0.21799747399353434		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 0.281391018564523 | validation: 0.23334436897809413]
	TIME [epoch: 8.18 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3090969909786586		[learning rate: 0.004358]
		[batch 20/20] avg loss: 0.27752369699843693		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 0.2933103439885477 | validation: 0.20843626327066134]
	TIME [epoch: 8.19 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2747998685926654		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 0.2552867329677677		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 0.26504330078021654 | validation: 0.24051727604033452]
	TIME [epoch: 8.18 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3007685533792381		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 0.30279062714499483		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 0.3017795902621165 | validation: 0.30419282914831935]
	TIME [epoch: 8.17 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23387730582872415		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 0.3396539031771989		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 0.2867656045029615 | validation: 0.22751735693681657]
	TIME [epoch: 8.17 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22095051927866863		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 0.186867909730841		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 0.20390921450475483 | validation: 0.2575030857105036]
	TIME [epoch: 8.17 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2143244202163157		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 0.5668377404307628		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 0.39058108032353933 | validation: 0.42565313888944234]
	TIME [epoch: 8.2 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5711405261724279		[learning rate: 0.004264]
		[batch 20/20] avg loss: 0.2923004398403434		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 0.43172048300638577 | validation: 0.2917097612457346]
	TIME [epoch: 8.18 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.342143800033932		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 0.2579425205159397		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 0.30004316027493594 | validation: 0.2637013527048759]
	TIME [epoch: 8.17 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.6040473217355762		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 0.5068022705707205		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 0.5554247961531482 | validation: 0.2033889242839238]
	TIME [epoch: 8.17 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7581590570927437		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 0.3276887139308367		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 0.5429238855117902 | validation: 0.2805815391235451]
	TIME [epoch: 8.18 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.320107288242795		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 0.2464963122897314		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 0.2833018002662632 | validation: 0.2646994004126792]
	TIME [epoch: 8.18 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2933939948833423		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 1.1219986283796513		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 0.7076963116314967 | validation: 1.0391594028168742]
	TIME [epoch: 8.17 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.05187279408858		[learning rate: 0.004172]
		[batch 20/20] avg loss: 0.6990382394106663		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 1.3754555167496236 | validation: 0.39121420577219823]
	TIME [epoch: 8.17 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3268505629493116		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 0.5508678529146286		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 0.4388592079319701 | validation: 0.24170183005839696]
	TIME [epoch: 8.18 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8031910211505041		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 0.8402083316213922		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 0.8216996763859481 | validation: 0.5671031341289509]
	TIME [epoch: 8.2 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4558354072927995		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 2.072583046893875		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 1.2642092270933374 | validation: 5.292972329544139]
	TIME [epoch: 8.18 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.1121952790107712		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 0.3090131380633847		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 0.710604208537078 | validation: 0.2503173734023526]
	TIME [epoch: 8.17 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26746947516969793		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 0.9510477457956196		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 0.6092586104826587 | validation: 5.493731973615323]
	TIME [epoch: 8.17 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.766317467460023		[learning rate: 0.004082]
		[batch 20/20] avg loss: 0.8459366050244752		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 1.806127036242249 | validation: 0.34056283668918513]
	TIME [epoch: 8.19 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.33643909741306677		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 0.3623346153726719		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 0.3493868563928693 | validation: 0.263056026353104]
	TIME [epoch: 8.18 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.276561689431947		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 0.2492732342417356		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 0.2629174618368413 | validation: 0.3561320824115805]
	TIME [epoch: 8.17 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2766992047577658		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 0.48230097119772886		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 0.37950008797774737 | validation: 0.24916905756742386]
	TIME [epoch: 8.17 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.010672640649416		[learning rate: 0.004023]
		[batch 20/20] avg loss: 0.7066860363349787		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 2.3586793384921974 | validation: 2.56894713254552]
	TIME [epoch: 8.17 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.789426834337753		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 2.453774287020382		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 4.621600560679068 | validation: 1.5162335560818951]
	TIME [epoch: 8.19 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.527594951532921		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 0.25137152989711004		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 0.8894832407150156 | validation: 0.2108545656483569]
	TIME [epoch: 8.17 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2749634290482308		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 0.2814107252973199		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 0.27818707717277535 | validation: 0.29782460333544736]
	TIME [epoch: 8.18 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2666709558989718		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 0.21175817530311192		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 0.23921456560104182 | validation: 0.1868712752417941]
	TIME [epoch: 8.17 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.437612146251541		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 2.394157354251464		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 1.915884750251503 | validation: 0.24232097270769026]
	TIME [epoch: 8.19 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26410764455263924		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 0.3138871129718662		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 0.2889973787622528 | validation: 0.28316872503597157]
	TIME [epoch: 8.18 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2754363484968918		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 0.3311555267766795		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 0.30329593763678564 | validation: 0.24584723843272427]
	TIME [epoch: 8.17 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2528074072238313		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 1.2452656579305825		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 0.749036532577207 | validation: 0.4988204054137022]
	TIME [epoch: 8.17 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.27678982937390323		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 0.2943784661744338		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 0.28558414777416846 | validation: 0.259461502210232]
	TIME [epoch: 8.18 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3257129063167395		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 0.42338421599748965		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 0.37454856115711455 | validation: 0.1955867417500251]
	TIME [epoch: 8.2 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3016955116387425		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 0.31902537275597576		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 0.3103604421973591 | validation: 0.19530634019330687]
	TIME [epoch: 8.17 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21979311249504835		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 1.6131585527328454		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 0.9164758326139468 | validation: 1.479090792946]
	TIME [epoch: 8.17 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.191606516517104		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 7.413602422210221		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 5.802604469363662 | validation: 8.007120144850843]
	TIME [epoch: 8.17 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.688623821019855		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 7.9780049039949485		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 7.833314362507403 | validation: 8.018834425505366]
	TIME [epoch: 8.2 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.868823394039369		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 5.074478925292768		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 6.471651159666068 | validation: 0.35742690674941974]
	TIME [epoch: 8.18 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2941307253659312		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 0.2716803588127113		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 0.2829055420893213 | validation: 0.26903117742335747]
	TIME [epoch: 8.17 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.46779472044254966		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 0.747640157892915		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 0.6077174391677322 | validation: 0.9924527194211887]
	TIME [epoch: 8.17 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4315996640669829		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 0.625971051153759		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 0.5287853576103709 | validation: 2.9192733776752453]
	TIME [epoch: 8.18 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.605644306250443		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 6.549678610952592		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 5.577661458601517 | validation: 8.03265168851305]
	TIME [epoch: 8.19 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.660596856857197		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 7.831330595526725		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 7.745963726191962 | validation: 8.028246799946436]
	TIME [epoch: 8.17 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.878064347330022		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 7.8573062091654124		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 7.867685278247718 | validation: 8.089678605332166]
	TIME [epoch: 8.18 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.566551823091897		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 9.69773198469804		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 9.132141903894968 | validation: 9.121758459805173]
	TIME [epoch: 8.17 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.526747000933733		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 9.559562399468955		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 9.543154700201347 | validation: 8.765576171593224]
	TIME [epoch: 8.2 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.170672736673904		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 10.097470721114647		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 10.134071728894275 | validation: 10.077557215083186]
	TIME [epoch: 8.18 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.857804887152458		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 11.152447310890008		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 11.005126099021233 | validation: 11.05447050251702]
	TIME [epoch: 8.17 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.098069596253158		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 9.822986457233975		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 10.460528026743566 | validation: 8.78836418972586]
	TIME [epoch: 8.17 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.998872530215781		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 9.988196370148762		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 9.99353445018227 | validation: 11.007289526669451]
	TIME [epoch: 8.2 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.514109082443074		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 11.046239896335099		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 10.780174489389086 | validation: 11.330223152474945]
	TIME [epoch: 8.18 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.183035275313383		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 10.876257739797136		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 11.029646507555261 | validation: 10.750445451318528]
	TIME [epoch: 8.18 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.790862205288716		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 10.629008371296614		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 10.709935288292666 | validation: 10.397462159423938]
	TIME [epoch: 8.18 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.015737300188503		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 11.220459319767238		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 11.11809830997787 | validation: 11.564990409210038]
	TIME [epoch: 8.18 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.286338529702045		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 10.948773646101696		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 11.117556087901871 | validation: 11.225316571914496]
	TIME [epoch: 8.2 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.684818653942		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 9.868990453230433		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 10.276904553586217 | validation: 9.365428570763116]
	TIME [epoch: 8.18 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.510891448578626		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 10.915053424947661		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 10.712972436763144 | validation: 10.592046820209205]
	TIME [epoch: 8.17 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.624263109564948		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 10.087856063022127		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 10.35605958629354 | validation: 9.323893979374368]
	TIME [epoch: 8.18 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.737999202349345		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 9.602117308395123		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 9.670058255372233 | validation: 9.07048604188179]
	TIME [epoch: 8.19 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.472087907513062		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 10.430593356724419		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 10.45134063211874 | validation: 9.466111424688028]
	TIME [epoch: 8.18 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.042125113704113		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 10.397997723656202		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 10.220061418680157 | validation: 9.6958714948725]
	TIME [epoch: 8.17 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.565512213224542		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 9.775902495561027		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 9.670707354392784 | validation: 9.572031189168356]
	TIME [epoch: 8.17 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.635941797405918		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 9.517776515648771		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 9.576859156527345 | validation: 9.257035055778227]
	TIME [epoch: 8.18 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.417815019152364		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 9.456980457006265		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 9.437397738079316 | validation: 9.332254562484852]
	TIME [epoch: 8.2 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.430184088544136		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 9.574228127562469		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 9.502206108053302 | validation: 9.034999234737255]
	TIME [epoch: 8.17 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.242485427069973		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 9.674204619945524		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 9.45834502350775 | validation: 9.278884185202866]
	TIME [epoch: 8.17 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.623179218948668		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 9.949212514270354		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 9.786195866609509 | validation: 10.072474094435574]
	TIME [epoch: 8.17 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.980390477119407		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 9.776016374180031		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 9.878203425649717 | validation: 10.078908453911236]
	TIME [epoch: 8.2 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.643239369803032		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 9.583861412774926		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 9.61355039128898 | validation: 9.971032950052592]
	TIME [epoch: 8.18 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.755369647223212		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 10.149205688250735		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 9.952287667736975 | validation: 9.99683823998079]
	TIME [epoch: 8.18 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.131707396290839		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 10.138640836474991		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 10.135174116382917 | validation: 10.332292904230787]
	TIME [epoch: 8.17 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.216748639318393		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 10.035751145231638		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 10.126249892275016 | validation: 9.97147883233098]
	TIME [epoch: 8.18 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.79293530474759		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 9.460227499133302		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 9.626581401940447 | validation: 9.702144765500691]
	TIME [epoch: 8.18 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.306678480164855		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 9.305050023397708		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 9.305864251781282 | validation: 9.643390087918732]
	TIME [epoch: 8.18 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.469503291386342		[learning rate: 0.00333]
		[batch 20/20] avg loss: 8.685979882639117		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 9.077741587012728 | validation: 8.68129269141862]
	TIME [epoch: 8.17 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.072618110011986		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 9.302891367436088		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 9.187754738724035 | validation: 9.719137833691118]
	TIME [epoch: 8.18 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.617668058580467		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 9.232921051625297		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 9.425294555102882 | validation: 9.70506056498992]
	TIME [epoch: 8.2 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.54281247187036		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 9.781437778740827		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 9.662125125305591 | validation: 9.643819500833075]
	TIME [epoch: 8.18 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.697577466848708		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 9.660941987628105		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 9.679259727238406 | validation: 9.863152215985384]
	TIME [epoch: 8.18 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.876075659068453		[learning rate: 0.00327]
		[batch 20/20] avg loss: 9.76199473080303		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 9.81903519493574 | validation: 9.68172237684614]
	TIME [epoch: 8.17 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.457761474327821		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 9.81766469874297		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 9.637713086535395 | validation: 9.742923403232222]
	TIME [epoch: 8.2 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.926038848133178		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 8.654411799456687		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 9.290225323794932 | validation: 7.795945883965874]
	TIME [epoch: 8.18 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.001285170557461		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 5.295406618037257		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 6.148345894297358 | validation: 6.156891628206404]
	TIME [epoch: 8.18 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.6508621215433035		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 7.849705117652107		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 7.750283619597704 | validation: 8.507155366710938]
	TIME [epoch: 8.17 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.726765342581432		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 5.5038958964025095		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 7.115330619491971 | validation: 4.165222453289108]
	TIME [epoch: 8.17 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.56188982212956		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 4.949629631689431		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 4.7557597269094956 | validation: 4.821252394879591]
	TIME [epoch: 8.19 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.43048489873332		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 5.706162442718176		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 5.568323670725748 | validation: 5.255445717933337]
	TIME [epoch: 8.18 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.1857914356172925		[learning rate: 0.0031763]
		[batch 20/20] avg loss: 4.671774152172782		[learning rate: 0.0031705]
	Learning Rate: 0.00317053
	LOSS [training: 4.928782793895037 | validation: 4.272575318766144]
	TIME [epoch: 8.17 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.574409872837037		[learning rate: 0.0031648]
		[batch 20/20] avg loss: 4.752112512511066		[learning rate: 0.003159]
	Learning Rate: 0.00315902
	LOSS [training: 4.663261192674051 | validation: 4.979263270646911]
	TIME [epoch: 8.17 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.370754082647898		[learning rate: 0.0031533]
		[batch 20/20] avg loss: 5.643069023601734		[learning rate: 0.0031476]
	Learning Rate: 0.00314756
	LOSS [training: 5.506911553124815 | validation: 5.547087710666649]
	TIME [epoch: 8.2 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.262619265576459		[learning rate: 0.0031418]
		[batch 20/20] avg loss: 4.737239628452395		[learning rate: 0.0031361]
	Learning Rate: 0.00313613
	LOSS [training: 4.999929447014428 | validation: 4.571219619300752]
	TIME [epoch: 8.18 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.812374719887101		[learning rate: 0.0031304]
		[batch 20/20] avg loss: 5.459569809347531		[learning rate: 0.0031248]
	Learning Rate: 0.00312475
	LOSS [training: 5.135972264617315 | validation: 5.935432395092303]
	TIME [epoch: 8.17 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.503016629784058		[learning rate: 0.0031191]
		[batch 20/20] avg loss: 5.217151318134773		[learning rate: 0.0031134]
	Learning Rate: 0.00311341
	LOSS [training: 5.360083973959417 | validation: 4.940666205216811]
	TIME [epoch: 8.17 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.846527468706666		[learning rate: 0.0031078]
		[batch 20/20] avg loss: 4.733624473173686		[learning rate: 0.0031021]
	Learning Rate: 0.00310212
	LOSS [training: 4.790075970940179 | validation: 4.365942712814885]
	TIME [epoch: 8.18 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.4924753048077495		[learning rate: 0.0030965]
		[batch 20/20] avg loss: 4.260682912102619		[learning rate: 0.0030909]
	Learning Rate: 0.00309086
	LOSS [training: 4.376579108455184 | validation: 3.5203634965435158]
	TIME [epoch: 8.2 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.304803399247114		[learning rate: 0.0030852]
		[batch 20/20] avg loss: 3.2171099789459334		[learning rate: 0.0030796]
	Learning Rate: 0.00307964
	LOSS [training: 3.260956689096523 | validation: 3.4966524242903647]
	TIME [epoch: 8.17 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.6464627573586545		[learning rate: 0.003074]
		[batch 20/20] avg loss: 6.53406021601368		[learning rate: 0.0030685]
	Learning Rate: 0.00306846
	LOSS [training: 5.590261486686168 | validation: 6.742334294818251]
	TIME [epoch: 8.17 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.028909210480094		[learning rate: 0.0030629]
		[batch 20/20] avg loss: 5.769462968071322		[learning rate: 0.0030573]
	Learning Rate: 0.00305733
	LOSS [training: 6.399186089275709 | validation: 7.27273924911837]
	TIME [epoch: 8.17 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.461879349255087		[learning rate: 0.0030518]
		[batch 20/20] avg loss: 4.01663733998749		[learning rate: 0.0030462]
	Learning Rate: 0.00304623
	LOSS [training: 5.239258344621288 | validation: 3.4727147723917606]
	TIME [epoch: 8.19 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4301099127735015		[learning rate: 0.0030407]
		[batch 20/20] avg loss: 3.347627105635911		[learning rate: 0.0030352]
	Learning Rate: 0.00303518
	LOSS [training: 3.3888685092047064 | validation: 6.0475037064893495]
	TIME [epoch: 8.17 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.254792021663718		[learning rate: 0.0030297]
		[batch 20/20] avg loss: 7.234047457854743		[learning rate: 0.0030242]
	Learning Rate: 0.00302416
	LOSS [training: 6.7444197397592305 | validation: 8.422697864238202]
	TIME [epoch: 8.18 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.189980241832385		[learning rate: 0.0030187]
		[batch 20/20] avg loss: 9.638195398063692		[learning rate: 0.0030132]
	Learning Rate: 0.00301319
	LOSS [training: 9.414087819948039 | validation: 9.653908920733587]
	TIME [epoch: 8.17 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.741655579690034		[learning rate: 0.0030077]
		[batch 20/20] avg loss: 9.439647709860918		[learning rate: 0.0030023]
	Learning Rate: 0.00300225
	LOSS [training: 9.590651644775477 | validation: 9.442233342568315]
	TIME [epoch: 8.18 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.758011909800882		[learning rate: 0.0029968]
		[batch 20/20] avg loss: 9.27716308382389		[learning rate: 0.0029914]
	Learning Rate: 0.00299136
	LOSS [training: 9.517587496812386 | validation: 9.123184800440452]
	TIME [epoch: 8.2 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.361768013935002		[learning rate: 0.0029859]
		[batch 20/20] avg loss: 6.149336891403834		[learning rate: 0.0029805]
	Learning Rate: 0.0029805
	LOSS [training: 7.755552452669418 | validation: 4.877972568644564]
	TIME [epoch: 8.19 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.546450441104772		[learning rate: 0.0029751]
		[batch 20/20] avg loss: 5.978772752121874		[learning rate: 0.0029697]
	Learning Rate: 0.00296969
	LOSS [training: 5.262611596613323 | validation: 4.178220075097479]
	TIME [epoch: 8.18 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.02183553850837		[learning rate: 0.0029643]
		[batch 20/20] avg loss: 2.912207858624775		[learning rate: 0.0029589]
	Learning Rate: 0.00295891
	LOSS [training: 2.9670216985665725 | validation: 2.77377080209994]
	TIME [epoch: 8.18 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.321530291229446		[learning rate: 0.0029535]
		[batch 20/20] avg loss: 3.075755452413616		[learning rate: 0.0029482]
	Learning Rate: 0.00294817
	LOSS [training: 3.198642871821531 | validation: 2.7565718507980383]
	TIME [epoch: 8.21 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.4612607450314092		[learning rate: 0.0029428]
		[batch 20/20] avg loss: 3.4301088406854445		[learning rate: 0.0029375]
	Learning Rate: 0.00293747
	LOSS [training: 3.445684792858427 | validation: 3.3186334321944506]
	TIME [epoch: 8.17 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.79266193428084		[learning rate: 0.0029321]
		[batch 20/20] avg loss: 3.8957860901913515		[learning rate: 0.0029268]
	Learning Rate: 0.00292681
	LOSS [training: 3.844224012236096 | validation: 4.360457640323274]
	TIME [epoch: 8.17 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.575474167323836		[learning rate: 0.0029215]
		[batch 20/20] avg loss: 6.958605044029919		[learning rate: 0.0029162]
	Learning Rate: 0.00291619
	LOSS [training: 6.267039605676877 | validation: 6.696631336292434]
	TIME [epoch: 8.17 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.095311985871824		[learning rate: 0.0029109]
		[batch 20/20] avg loss: 6.928019161862906		[learning rate: 0.0029056]
	Learning Rate: 0.00290561
	LOSS [training: 7.011665573867366 | validation: 6.365271383076317]
	TIME [epoch: 8.19 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.158732020670726		[learning rate: 0.0029003]
		[batch 20/20] avg loss: 7.285409524159256		[learning rate: 0.0028951]
	Learning Rate: 0.00289506
	LOSS [training: 7.222070772414992 | validation: 7.1222756582002305]
	TIME [epoch: 8.18 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.813600791160016		[learning rate: 0.0028898]
		[batch 20/20] avg loss: 7.720372837655086		[learning rate: 0.0028846]
	Learning Rate: 0.00288456
	LOSS [training: 7.7669868144075505 | validation: 7.648074837187651]
	TIME [epoch: 8.18 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.8027010340456515		[learning rate: 0.0028793]
		[batch 20/20] avg loss: 7.696992527747431		[learning rate: 0.0028741]
	Learning Rate: 0.00287409
	LOSS [training: 7.749846780896543 | validation: 7.7817573649212255]
	TIME [epoch: 8.18 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.586770454608072		[learning rate: 0.0028689]
		[batch 20/20] avg loss: 8.201992557308746		[learning rate: 0.0028637]
	Learning Rate: 0.00286366
	LOSS [training: 7.894381505958409 | validation: 8.109710994696895]
	TIME [epoch: 8.17 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.618096929822851		[learning rate: 0.0028585]
		[batch 20/20] avg loss: 7.760127770058096		[learning rate: 0.0028533]
	Learning Rate: 0.00285326
	LOSS [training: 8.189112349940475 | validation: 7.572294727857717]
	TIME [epoch: 8.2 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.272164879535845		[learning rate: 0.0028481]
		[batch 20/20] avg loss: 6.748735056582677		[learning rate: 0.0028429]
	Learning Rate: 0.00284291
	LOSS [training: 7.01044996805926 | validation: 5.422814849160468]
	TIME [epoch: 8.18 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.116809866159935		[learning rate: 0.0028377]
		[batch 20/20] avg loss: 6.930722296520297		[learning rate: 0.0028326]
	Learning Rate: 0.00283259
	LOSS [training: 6.523766081340116 | validation: 7.259932531270792]
	TIME [epoch: 8.18 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.116607799490453		[learning rate: 0.0028274]
		[batch 20/20] avg loss: 7.050323000535542		[learning rate: 0.0028223]
	Learning Rate: 0.00282231
	LOSS [training: 7.083465400012997 | validation: 5.920263057125492]
	TIME [epoch: 8.18 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.448058292250193		[learning rate: 0.0028172]
		[batch 20/20] avg loss: 7.437017119444411		[learning rate: 0.0028121]
	Learning Rate: 0.00281207
	LOSS [training: 7.442537705847303 | validation: 5.931812182005899]
	TIME [epoch: 8.2 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.540597549642293		[learning rate: 0.002807]
		[batch 20/20] avg loss: 7.796007455108523		[learning rate: 0.0028019]
	Learning Rate: 0.00280187
	LOSS [training: 7.168302502375409 | validation: 7.846657219090994]
	TIME [epoch: 8.18 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.615824415536457		[learning rate: 0.0027968]
		[batch 20/20] avg loss: 5.883552117099308		[learning rate: 0.0027917]
	Learning Rate: 0.0027917
	LOSS [training: 6.749688266317884 | validation: 5.767051497509444]
	TIME [epoch: 8.17 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.273549662209963		[learning rate: 0.0027866]
		[batch 20/20] avg loss: 6.678746586305095		[learning rate: 0.0027816]
	Learning Rate: 0.00278157
	LOSS [training: 6.476148124257529 | validation: 6.119415516252349]
	TIME [epoch: 8.17 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.868111723634243		[learning rate: 0.0027765]
		[batch 20/20] avg loss: 7.068984818169183		[learning rate: 0.0027715]
	Learning Rate: 0.00277147
	LOSS [training: 6.968548270901712 | validation: 6.759262073753198]
	TIME [epoch: 8.18 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.356923723581522		[learning rate: 0.0027664]
		[batch 20/20] avg loss: 7.215500221883022		[learning rate: 0.0027614]
	Learning Rate: 0.00276141
	LOSS [training: 7.286211972732272 | validation: 7.300541299612487]
	TIME [epoch: 8.19 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.19600068430068		[learning rate: 0.0027564]
		[batch 20/20] avg loss: 7.444686236256663		[learning rate: 0.0027514]
	Learning Rate: 0.00275139
	LOSS [training: 7.32034346027867 | validation: 6.070753611310719]
	TIME [epoch: 8.17 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.1359362357361915		[learning rate: 0.0027464]
		[batch 20/20] avg loss: 7.469126537762669		[learning rate: 0.0027414]
	Learning Rate: 0.00274141
	LOSS [training: 7.302531386749431 | validation: 6.817846586450919]
	TIME [epoch: 8.17 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.686733258379213		[learning rate: 0.0027364]
		[batch 20/20] avg loss: 6.574598412862019		[learning rate: 0.0027315]
	Learning Rate: 0.00273146
	LOSS [training: 7.130665835620617 | validation: 6.151111827267898]
	TIME [epoch: 8.17 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.443445868710514		[learning rate: 0.0027265]
		[batch 20/20] avg loss: 5.275105351536533		[learning rate: 0.0027215]
	Learning Rate: 0.00272155
	LOSS [training: 5.8592756101235235 | validation: 4.96788447035312]
	TIME [epoch: 8.19 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.863213324615998		[learning rate: 0.0027166]
		[batch 20/20] avg loss: 5.172316899779035		[learning rate: 0.0027117]
	Learning Rate: 0.00271167
	LOSS [training: 5.017765112197515 | validation: 3.613619044920913]
	TIME [epoch: 8.17 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.187933795948463		[learning rate: 0.0027067]
		[batch 20/20] avg loss: 2.329230650789971		[learning rate: 0.0027018]
	Learning Rate: 0.00270183
	LOSS [training: 2.758582223369217 | validation: 2.462700415360227]
	TIME [epoch: 8.17 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.656705667547239		[learning rate: 0.0026969]
		[batch 20/20] avg loss: 8.527222538598469		[learning rate: 0.002692]
	Learning Rate: 0.00269202
	LOSS [training: 7.091964103072854 | validation: 8.186022451321522]
	TIME [epoch: 8.18 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.835304604889938		[learning rate: 0.0026871]
		[batch 20/20] avg loss: 9.978048499450885		[learning rate: 0.0026823]
	Learning Rate: 0.00268225
	LOSS [training: 9.406676552170412 | validation: 9.801863802473015]
	TIME [epoch: 8.17 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.413552625621618		[learning rate: 0.0026774]
		[batch 20/20] avg loss: 9.29621022024269		[learning rate: 0.0026725]
	Learning Rate: 0.00267252
	LOSS [training: 9.854881422932156 | validation: 9.476186849043476]
	TIME [epoch: 8.21 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.842466133491582		[learning rate: 0.0026677]
		[batch 20/20] avg loss: 9.393235163120663		[learning rate: 0.0026628]
	Learning Rate: 0.00266282
	LOSS [training: 9.617850648306122 | validation: 7.206089566937881]
	TIME [epoch: 8.18 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.33497521763103		[learning rate: 0.002658]
		[batch 20/20] avg loss: 8.577325693340587		[learning rate: 0.0026532]
	Learning Rate: 0.00265316
	LOSS [training: 8.456150455485806 | validation: 8.070118931261936]
	TIME [epoch: 8.17 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.190227975490183		[learning rate: 0.0026483]
		[batch 20/20] avg loss: 8.456449856737654		[learning rate: 0.0026435]
	Learning Rate: 0.00264353
	LOSS [training: 8.323338916113919 | validation: 8.962139900163573]
	TIME [epoch: 8.16 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.51138806893665		[learning rate: 0.0026387]
		[batch 20/20] avg loss: 9.859655731534406		[learning rate: 0.0026339]
	Learning Rate: 0.00263394
	LOSS [training: 9.68552190023553 | validation: 9.599217245306232]
	TIME [epoch: 8.2 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.945473952269019		[learning rate: 0.0026292]
		[batch 20/20] avg loss: 9.869731875616845		[learning rate: 0.0026244]
	Learning Rate: 0.00262438
	LOSS [training: 9.907602913942933 | validation: 9.296510520663507]
	TIME [epoch: 8.17 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 10/20] avg loss: 8.787525001378881		[learning rate: 0.0026196]
		[batch 20/20] avg loss: 7.061493948083421		[learning rate: 0.0026149]
	Learning Rate: 0.00261485
	LOSS [training: 7.924509474731151 | validation: 8.18452202137031]
	TIME [epoch: 8.17 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 10/20] avg loss: 6.340540623642377		[learning rate: 0.0026101]
		[batch 20/20] avg loss: 8.492120502802216		[learning rate: 0.0026054]
	Learning Rate: 0.00260536
	LOSS [training: 7.416330563222298 | validation: 9.496507574572613]
	TIME [epoch: 8.17 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 10/20] avg loss: 9.022474084715197		[learning rate: 0.0026006]
		[batch 20/20] avg loss: 8.843432992933746		[learning rate: 0.0025959]
	Learning Rate: 0.00259591
	LOSS [training: 8.932953538824473 | validation: 5.778939909813181]
	TIME [epoch: 8.18 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.265933152599892		[learning rate: 0.0025912]
		[batch 20/20] avg loss: 5.444052655551059		[learning rate: 0.0025865]
	Learning Rate: 0.00258649
	LOSS [training: 5.354992904075476 | validation: 6.742838085504486]
	TIME [epoch: 8.19 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 10/20] avg loss: 5.227382435213873		[learning rate: 0.0025818]
		[batch 20/20] avg loss: 1.8399657346423275		[learning rate: 0.0025771]
	Learning Rate: 0.0025771
	LOSS [training: 3.5336740849281005 | validation: 1.5134733838589347]
	TIME [epoch: 8.16 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.2769340369041513		[learning rate: 0.0025724]
		[batch 20/20] avg loss: 1.0413532528550586		[learning rate: 0.0025677]
	Learning Rate: 0.00256775
	LOSS [training: 1.1591436448796049 | validation: 0.9332355473740372]
	TIME [epoch: 8.17 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.8439670501702267		[learning rate: 0.0025631]
		[batch 20/20] avg loss: 0.8352539201463314		[learning rate: 0.0025584]
	Learning Rate: 0.00255843
	LOSS [training: 0.8396104851582791 | validation: 0.8167346244694832]
	TIME [epoch: 8.17 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.7144194468663329		[learning rate: 0.0025538]
		[batch 20/20] avg loss: 0.6792446045574511		[learning rate: 0.0025491]
	Learning Rate: 0.00254915
	LOSS [training: 0.6968320257118918 | validation: 0.7858664680739509]
	TIME [epoch: 8.19 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5842317065814967		[learning rate: 0.0025445]
		[batch 20/20] avg loss: 0.6276165519792195		[learning rate: 0.0025399]
	Learning Rate: 0.0025399
	LOSS [training: 0.6059241292803581 | validation: 0.5127185822902647]
	TIME [epoch: 8.18 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.626498079518748		[learning rate: 0.0025353]
		[batch 20/20] avg loss: 0.48627322041123805		[learning rate: 0.0025307]
	Learning Rate: 0.00253068
	LOSS [training: 0.5563856499649928 | validation: 0.5774485575428996]
	TIME [epoch: 8.18 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4780444399802796		[learning rate: 0.0025261]
		[batch 20/20] avg loss: 0.4986300689317174		[learning rate: 0.0025215]
	Learning Rate: 0.00252149
	LOSS [training: 0.4883372544559986 | validation: 0.6380517746841192]
	TIME [epoch: 8.17 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.5191513923659467		[learning rate: 0.0025169]
		[batch 20/20] avg loss: 0.42820231097275274		[learning rate: 0.0025123]
	Learning Rate: 0.00251234
	LOSS [training: 0.47367685166934975 | validation: 0.5198722956663568]
	TIME [epoch: 8.19 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.38387514275149986		[learning rate: 0.0025078]
		[batch 20/20] avg loss: 0.36538801120211273		[learning rate: 0.0025032]
	Learning Rate: 0.00250323
	LOSS [training: 0.3746315769768063 | validation: 0.38796666267177327]
	TIME [epoch: 8.18 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3538637425594445		[learning rate: 0.0024987]
		[batch 20/20] avg loss: 0.368454305550881		[learning rate: 0.0024941]
	Learning Rate: 0.00249414
	LOSS [training: 0.36115902405516287 | validation: 0.39580563803120006]
	TIME [epoch: 8.17 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.34958476966847896		[learning rate: 0.0024896]
		[batch 20/20] avg loss: 0.3705120775537043		[learning rate: 0.0024851]
	Learning Rate: 0.00248509
	LOSS [training: 0.3600484236110916 | validation: 0.49157627452485]
	TIME [epoch: 8.17 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3429371111604795		[learning rate: 0.0024806]
		[batch 20/20] avg loss: 0.37586837494277414		[learning rate: 0.0024761]
	Learning Rate: 0.00247607
	LOSS [training: 0.3594027430516267 | validation: 0.3673049364297149]
	TIME [epoch: 8.18 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3560344409762257		[learning rate: 0.0024716]
		[batch 20/20] avg loss: 0.33137027287863996		[learning rate: 0.0024671]
	Learning Rate: 0.00246709
	LOSS [training: 0.3437023569274328 | validation: 0.7079386777699861]
	TIME [epoch: 8.19 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.4296114636957908		[learning rate: 0.0024626]
		[batch 20/20] avg loss: 0.3691962305369466		[learning rate: 0.0024581]
	Learning Rate: 0.00245813
	LOSS [training: 0.3994038471163687 | validation: 0.3279574923613264]
	TIME [epoch: 8.17 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29453557695375443		[learning rate: 0.0024537]
		[batch 20/20] avg loss: 0.2828173935473312		[learning rate: 0.0024492]
	Learning Rate: 0.00244921
	LOSS [training: 0.28867648525054285 | validation: 0.4232090381071464]
	TIME [epoch: 8.18 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3748657866631381		[learning rate: 0.0024448]
		[batch 20/20] avg loss: 0.32423829079225247		[learning rate: 0.0024403]
	Learning Rate: 0.00244032
	LOSS [training: 0.34955203872769525 | validation: 0.5828562342055014]
	TIME [epoch: 8.17 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.45412403858467165		[learning rate: 0.0024359]
		[batch 20/20] avg loss: 0.2833045353779883		[learning rate: 0.0024315]
	Learning Rate: 0.00243147
	LOSS [training: 0.36871428698132996 | validation: 0.26933756233255274]
	TIME [epoch: 8.2 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28474245810003107		[learning rate: 0.0024271]
		[batch 20/20] avg loss: 0.3126657053665319		[learning rate: 0.0024226]
	Learning Rate: 0.00242264
	LOSS [training: 0.29870408173328145 | validation: 0.32707879119860533]
	TIME [epoch: 8.17 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2959236183395612		[learning rate: 0.0024182]
		[batch 20/20] avg loss: 0.3102426702619335		[learning rate: 0.0024139]
	Learning Rate: 0.00241385
	LOSS [training: 0.3030831443007473 | validation: 0.3485353322740406]
	TIME [epoch: 8.17 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.31316260282209274		[learning rate: 0.0024095]
		[batch 20/20] avg loss: 0.2888435829076408		[learning rate: 0.0024051]
	Learning Rate: 0.00240509
	LOSS [training: 0.30100309286486676 | validation: 0.23760868464851778]
	TIME [epoch: 8.17 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.301063620111995		[learning rate: 0.0024007]
		[batch 20/20] avg loss: 0.317737122666383		[learning rate: 0.0023964]
	Learning Rate: 0.00239636
	LOSS [training: 0.3094003713891889 | validation: 0.30671198848254766]
	TIME [epoch: 8.16 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29581293903322103		[learning rate: 0.002392]
		[batch 20/20] avg loss: 0.33907368405990057		[learning rate: 0.0023877]
	Learning Rate: 0.00238767
	LOSS [training: 0.31744331154656086 | validation: 0.2340270890999157]
	TIME [epoch: 8.2 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29157012383390535		[learning rate: 0.0023833]
		[batch 20/20] avg loss: 0.3059871052100979		[learning rate: 0.002379]
	Learning Rate: 0.002379
	LOSS [training: 0.2987786145220016 | validation: 0.4276161596651]
	TIME [epoch: 8.17 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2781283312340399		[learning rate: 0.0023747]
		[batch 20/20] avg loss: 0.2941544264483755		[learning rate: 0.0023704]
	Learning Rate: 0.00237037
	LOSS [training: 0.2861413788412076 | validation: 0.39400417934082266]
	TIME [epoch: 8.17 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.36080932613651007		[learning rate: 0.0023661]
		[batch 20/20] avg loss: 0.31229531307310443		[learning rate: 0.0023618]
	Learning Rate: 0.00236177
	LOSS [training: 0.33655231960480725 | validation: 0.30643973548941505]
	TIME [epoch: 8.17 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25349336478083173		[learning rate: 0.0023575]
		[batch 20/20] avg loss: 0.287740569957777		[learning rate: 0.0023532]
	Learning Rate: 0.00235319
	LOSS [training: 0.2706169673693043 | validation: 0.7735075721636342]
	TIME [epoch: 8.19 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3123937240915809		[learning rate: 0.0023489]
		[batch 20/20] avg loss: 0.25648981389791686		[learning rate: 0.0023447]
	Learning Rate: 0.00234465
	LOSS [training: 0.28444176899474893 | validation: 0.3114092433580261]
	TIME [epoch: 8.17 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25615026780839234		[learning rate: 0.0023404]
		[batch 20/20] avg loss: 0.27967873856908215		[learning rate: 0.0023361]
	Learning Rate: 0.00233615
	LOSS [training: 0.2679145031887372 | validation: 0.5274727444260382]
	TIME [epoch: 8.16 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29605379525597947		[learning rate: 0.0023319]
		[batch 20/20] avg loss: 0.2440296785300121		[learning rate: 0.0023277]
	Learning Rate: 0.00232767
	LOSS [training: 0.27004173689299577 | validation: 0.24629738005771523]
	TIME [epoch: 8.16 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.30450725882173596		[learning rate: 0.0023234]
		[batch 20/20] avg loss: 0.3036980777383035		[learning rate: 0.0023192]
	Learning Rate: 0.00231922
	LOSS [training: 0.3041026682800197 | validation: 0.29258869885480576]
	TIME [epoch: 8.17 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25000458177714785		[learning rate: 0.002315]
		[batch 20/20] avg loss: 0.28787636708340536		[learning rate: 0.0023108]
	Learning Rate: 0.0023108
	LOSS [training: 0.26894047443027663 | validation: 0.31181812582212864]
	TIME [epoch: 8.18 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2999381046099655		[learning rate: 0.0023066]
		[batch 20/20] avg loss: 0.27775023373582014		[learning rate: 0.0023024]
	Learning Rate: 0.00230242
	LOSS [training: 0.2888441691728928 | validation: 0.30613167744542386]
	TIME [epoch: 8.17 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3396560306478435		[learning rate: 0.0022982]
		[batch 20/20] avg loss: 0.27999531394635435		[learning rate: 0.0022941]
	Learning Rate: 0.00229406
	LOSS [training: 0.30982567229709895 | validation: 0.2995717474570856]
	TIME [epoch: 8.16 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2500617077671671		[learning rate: 0.0022899]
		[batch 20/20] avg loss: 0.28682082823461463		[learning rate: 0.0022857]
	Learning Rate: 0.00228574
	LOSS [training: 0.26844126800089085 | validation: 0.2400963294273935]
	TIME [epoch: 8.16 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2414966353784413		[learning rate: 0.0022816]
		[batch 20/20] avg loss: 0.2650606516865744		[learning rate: 0.0022774]
	Learning Rate: 0.00227744
	LOSS [training: 0.2532786435325079 | validation: 0.3046731149665255]
	TIME [epoch: 8.2 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29473884977459225		[learning rate: 0.0022733]
		[batch 20/20] avg loss: 0.23040416757103496		[learning rate: 0.0022692]
	Learning Rate: 0.00226918
	LOSS [training: 0.2625715086728136 | validation: 0.4574032527171029]
	TIME [epoch: 8.17 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29101713832122217		[learning rate: 0.0022651]
		[batch 20/20] avg loss: 0.24056195128704366		[learning rate: 0.0022609]
	Learning Rate: 0.00226094
	LOSS [training: 0.26578954480413286 | validation: 0.271459321112962]
	TIME [epoch: 8.16 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2908653552527998		[learning rate: 0.0022568]
		[batch 20/20] avg loss: 0.23493115643912515		[learning rate: 0.0022527]
	Learning Rate: 0.00225274
	LOSS [training: 0.2628982558459624 | validation: 0.26873279878731005]
	TIME [epoch: 8.17 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25154463575990116		[learning rate: 0.0022486]
		[batch 20/20] avg loss: 0.24025647002098474		[learning rate: 0.0022446]
	Learning Rate: 0.00224456
	LOSS [training: 0.2459005528904429 | validation: 0.32900534861777997]
	TIME [epoch: 8.19 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2430948744274753		[learning rate: 0.0022405]
		[batch 20/20] avg loss: 0.23109967824641506		[learning rate: 0.0022364]
	Learning Rate: 0.00223642
	LOSS [training: 0.23709727633694516 | validation: 0.25143079921042844]
	TIME [epoch: 8.16 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.25594193020139133		[learning rate: 0.0022324]
		[batch 20/20] avg loss: 0.24224033048096344		[learning rate: 0.0022283]
	Learning Rate: 0.0022283
	LOSS [training: 0.2490911303411774 | validation: 0.2956281338155257]
	TIME [epoch: 8.17 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2500354341958008		[learning rate: 0.0022243]
		[batch 20/20] avg loss: 0.25356350453778215		[learning rate: 0.0022202]
	Learning Rate: 0.00222021
	LOSS [training: 0.2517994693667915 | validation: 0.3113064498338963]
	TIME [epoch: 8.17 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.3687447346671712		[learning rate: 0.0022162]
		[batch 20/20] avg loss: 0.2801636709949719		[learning rate: 0.0022122]
	Learning Rate: 0.00221216
	LOSS [training: 0.32445420283107157 | validation: 0.28914837994569126]
	TIME [epoch: 8.18 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23550949437494867		[learning rate: 0.0022081]
		[batch 20/20] avg loss: 0.2860529993336923		[learning rate: 0.0022041]
	Learning Rate: 0.00220413
	LOSS [training: 0.2607812468543205 | validation: 0.30433787093938697]
	TIME [epoch: 8.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22850038343903809		[learning rate: 0.0022001]
		[batch 20/20] avg loss: 0.25633200221685526		[learning rate: 0.0021961]
	Learning Rate: 0.00219613
	LOSS [training: 0.24241619282794663 | validation: 0.3001177702091652]
	TIME [epoch: 8.18 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23524187465402982		[learning rate: 0.0021921]
		[batch 20/20] avg loss: 0.3476726272832375		[learning rate: 0.0021882]
	Learning Rate: 0.00218816
	LOSS [training: 0.2914572509686337 | validation: 0.391888642508221]
	TIME [epoch: 8.17 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.340631319540309		[learning rate: 0.0021842]
		[batch 20/20] avg loss: 0.2836802813207396		[learning rate: 0.0021802]
	Learning Rate: 0.00218022
	LOSS [training: 0.31215580043052427 | validation: 0.4468060874107689]
	TIME [epoch: 8.17 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2727084152453274		[learning rate: 0.0021763]
		[batch 20/20] avg loss: 0.27076449972573646		[learning rate: 0.0021723]
	Learning Rate: 0.00217231
	LOSS [training: 0.27173645748553193 | validation: 0.33090652046147917]
	TIME [epoch: 8.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26615311623592686		[learning rate: 0.0021684]
		[batch 20/20] avg loss: 0.24228991418810172		[learning rate: 0.0021644]
	Learning Rate: 0.00216442
	LOSS [training: 0.2542215152120143 | validation: 0.29260275614306464]
	TIME [epoch: 8.17 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2804884206369369		[learning rate: 0.0021605]
		[batch 20/20] avg loss: 0.2840688369303677		[learning rate: 0.0021566]
	Learning Rate: 0.00215657
	LOSS [training: 0.28227862878365234 | validation: 0.38706268212590367]
	TIME [epoch: 8.17 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29819301925632574		[learning rate: 0.0021527]
		[batch 20/20] avg loss: 0.2775513678685558		[learning rate: 0.0021487]
	Learning Rate: 0.00214874
	LOSS [training: 0.2878721935624407 | validation: 0.38194655164371644]
	TIME [epoch: 8.18 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.29057172416889177		[learning rate: 0.0021448]
		[batch 20/20] avg loss: 0.2930958795205175		[learning rate: 0.0021409]
	Learning Rate: 0.00214094
	LOSS [training: 0.29183380184470464 | validation: 0.3236438433542914]
	TIME [epoch: 8.16 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26996143800575084		[learning rate: 0.0021371]
		[batch 20/20] avg loss: 0.23534391710361366		[learning rate: 0.0021332]
	Learning Rate: 0.00213317
	LOSS [training: 0.2526526775546823 | validation: 0.27549694005900455]
	TIME [epoch: 8.19 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2753559311213898		[learning rate: 0.0021293]
		[batch 20/20] avg loss: 0.20623878040625887		[learning rate: 0.0021254]
	Learning Rate: 0.00212543
	LOSS [training: 0.24079735576382433 | validation: 0.2939030249977834]
	TIME [epoch: 8.18 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22289490280426208		[learning rate: 0.0021216]
		[batch 20/20] avg loss: 0.2519414235709242		[learning rate: 0.0021177]
	Learning Rate: 0.00211772
	LOSS [training: 0.23741816318759307 | validation: 0.32705239773784545]
	TIME [epoch: 8.18 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.26611347110034056		[learning rate: 0.0021139]
		[batch 20/20] avg loss: 0.25426780419125017		[learning rate: 0.00211]
	Learning Rate: 0.00211003
	LOSS [training: 0.26019063764579536 | validation: 0.3751849635465092]
	TIME [epoch: 8.17 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2685424804213512		[learning rate: 0.0021062]
		[batch 20/20] avg loss: 0.22317438117648317		[learning rate: 0.0021024]
	Learning Rate: 0.00210238
	LOSS [training: 0.2458584307989172 | validation: 0.29289518604502895]
	TIME [epoch: 8.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23925537248114392		[learning rate: 0.0020986]
		[batch 20/20] avg loss: 0.3671359089730934		[learning rate: 0.0020947]
	Learning Rate: 0.00209475
	LOSS [training: 0.3031956407271187 | validation: 0.2995044242690785]
	TIME [epoch: 8.17 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23055172687026698		[learning rate: 0.0020909]
		[batch 20/20] avg loss: 0.22191902762441043		[learning rate: 0.0020871]
	Learning Rate: 0.00208714
	LOSS [training: 0.2262353772473388 | validation: 0.226221102445065]
	TIME [epoch: 8.18 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24232955631970698		[learning rate: 0.0020834]
		[batch 20/20] avg loss: 0.228862152881181		[learning rate: 0.0020796]
	Learning Rate: 0.00207957
	LOSS [training: 0.23559585460044397 | validation: 0.2646037214513118]
	TIME [epoch: 8.17 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23194889152482706		[learning rate: 0.0020758]
		[batch 20/20] avg loss: 0.2322479163065445		[learning rate: 0.002072]
	Learning Rate: 0.00207202
	LOSS [training: 0.23209840391568579 | validation: 0.26370963702244055]
	TIME [epoch: 8.19 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2296746066941628		[learning rate: 0.0020683]
		[batch 20/20] avg loss: 0.23506026484214043		[learning rate: 0.0020645]
	Learning Rate: 0.0020645
	LOSS [training: 0.23236743576815164 | validation: 0.23114899826470317]
	TIME [epoch: 8.17 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22329357271976505		[learning rate: 0.0020608]
		[batch 20/20] avg loss: 0.27874672484764		[learning rate: 0.002057]
	Learning Rate: 0.00205701
	LOSS [training: 0.25102014878370255 | validation: 0.2843529606115642]
	TIME [epoch: 8.16 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22400831607841293		[learning rate: 0.0020533]
		[batch 20/20] avg loss: 0.24337623439254044		[learning rate: 0.0020495]
	Learning Rate: 0.00204955
	LOSS [training: 0.23369227523547664 | validation: 0.2783427916534938]
	TIME [epoch: 8.16 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23125172887297443		[learning rate: 0.0020458]
		[batch 20/20] avg loss: 0.21951408742996142		[learning rate: 0.0020421]
	Learning Rate: 0.00204211
	LOSS [training: 0.22538290815146794 | validation: 0.35681856792456534]
	TIME [epoch: 8.16 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2598818369725295		[learning rate: 0.0020384]
		[batch 20/20] avg loss: 0.22680126325558908		[learning rate: 0.0020347]
	Learning Rate: 0.0020347
	LOSS [training: 0.24334155011405928 | validation: 0.23067350803353387]
	TIME [epoch: 8.18 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.20538298956678638		[learning rate: 0.002031]
		[batch 20/20] avg loss: 0.21910162281235848		[learning rate: 0.0020273]
	Learning Rate: 0.00202731
	LOSS [training: 0.2122423061895724 | validation: 0.2377274939771572]
	TIME [epoch: 8.17 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.17965437182836305		[learning rate: 0.0020236]
		[batch 20/20] avg loss: 0.2424399757372758		[learning rate: 0.00202]
	Learning Rate: 0.00201996
	LOSS [training: 0.2110471737828194 | validation: 0.2752013097346458]
	TIME [epoch: 8.16 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2607164512819004		[learning rate: 0.0020163]
		[batch 20/20] avg loss: 0.24733193513744442		[learning rate: 0.0020126]
	Learning Rate: 0.00201263
	LOSS [training: 0.25402419320967246 | validation: 0.7450465331857328]
	TIME [epoch: 8.16 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.28755300049706334		[learning rate: 0.002009]
		[batch 20/20] avg loss: 0.23611231615633907		[learning rate: 0.0020053]
	Learning Rate: 0.00200532
	LOSS [training: 0.26183265832670116 | validation: 0.27560971292822384]
	TIME [epoch: 8.19 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.21233769522174928		[learning rate: 0.0020017]
		[batch 20/20] avg loss: 0.22873379815469774		[learning rate: 0.001998]
	Learning Rate: 0.00199805
	LOSS [training: 0.22053574668822348 | validation: 0.22916809987701553]
	TIME [epoch: 8.17 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.19915780280651818		[learning rate: 0.0019944]
		[batch 20/20] avg loss: 0.2866449893426136		[learning rate: 0.0019908]
	Learning Rate: 0.00199079
	LOSS [training: 0.2429013960745659 | validation: 0.1994262431599957]
	TIME [epoch: 8.17 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2091914316960223		[learning rate: 0.0019872]
		[batch 20/20] avg loss: 0.19406326145414504		[learning rate: 0.0019836]
	Learning Rate: 0.00198357
	LOSS [training: 0.2016273465750837 | validation: 0.25438319509150215]
	TIME [epoch: 8.17 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2848722916304366		[learning rate: 0.00198]
		[batch 20/20] avg loss: 0.19145353807094648		[learning rate: 0.0019764]
	Learning Rate: 0.00197637
	LOSS [training: 0.23816291485069155 | validation: 0.2351651404936188]
	TIME [epoch: 8.16 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.22770433007761123		[learning rate: 0.0019728]
		[batch 20/20] avg loss: 0.21335731884362788		[learning rate: 0.0019692]
	Learning Rate: 0.0019692
	LOSS [training: 0.22053082446061958 | validation: 0.2532429777156517]
	TIME [epoch: 8.19 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.2623733100552075		[learning rate: 0.0019656]
		[batch 20/20] avg loss: 0.19052471934562104		[learning rate: 0.0019621]
	Learning Rate: 0.00196205
	LOSS [training: 0.22644901470041426 | validation: 0.7109178893750341]
	TIME [epoch: 8.17 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.23951406343192408		[learning rate: 0.0019585]
		[batch 20/20] avg loss: 0.2371348446418386		[learning rate: 0.0019549]
	Learning Rate: 0.00195493
	LOSS [training: 0.23832445403688132 | validation: 0.24334722471706396]
	TIME [epoch: 8.16 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 10/20] avg loss: 0.24552859531404084		[learning rate: 0.0019514]
		[batch 20/20] avg loss: 1.2492760180827085		[learning rate: 0.0019478]
	Learning Rate: 0.00194784
	LOSS [training: 0.7474023066983746 | validation: 2.722614751630309]
	TIME [epoch: 8.16 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.1410724905802185		[learning rate: 0.0019443]
		[batch 20/20] avg loss: 3.845412997885435		[learning rate: 0.0019408]
	Learning Rate: 0.00194077
	LOSS [training: 3.4932427442328278 | validation: 2.155276426348141]
	TIME [epoch: 8.2 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9370435773494787		[learning rate: 0.0019372]
		[batch 20/20] avg loss: 2.4557708706107793		[learning rate: 0.0019337]
	Learning Rate: 0.00193373
	LOSS [training: 2.1964072239801293 | validation: 2.716415525351311]
	TIME [epoch: 8.18 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.068810658733769		[learning rate: 0.0019302]
		[batch 20/20] avg loss: 3.1270813165721263		[learning rate: 0.0019267]
	Learning Rate: 0.00192671
	LOSS [training: 3.0979459876529467 | validation: 3.114534979825425]
	TIME [epoch: 8.18 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.7161005263983036		[learning rate: 0.0019232]
		[batch 20/20] avg loss: 3.854655483591465		[learning rate: 0.0019197]
	Learning Rate: 0.00191972
	LOSS [training: 3.7853780049948837 | validation: 3.9810279284192744]
	TIME [epoch: 8.17 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.031972928967588		[learning rate: 0.0019162]
		[batch 20/20] avg loss: 2.4753620952164965		[learning rate: 0.0019127]
	Learning Rate: 0.00191275
	LOSS [training: 3.253667512092042 | validation: 3.1950686627101543]
	TIME [epoch: 8.17 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.618545377248386		[learning rate: 0.0019093]
		[batch 20/20] avg loss: 2.4404223574643487		[learning rate: 0.0019058]
	Learning Rate: 0.00190581
	LOSS [training: 2.529483867356367 | validation: 3.049155726363878]
	TIME [epoch: 8.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3809745546481875		[learning rate: 0.0019023]
		[batch 20/20] avg loss: 2.2772580778224842		[learning rate: 0.0018989]
	Learning Rate: 0.00189889
	LOSS [training: 2.329116316235336 | validation: 2.9380496361849553]
	TIME [epoch: 8.16 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.270421471456009		[learning rate: 0.0018954]
		[batch 20/20] avg loss: 2.276762141357727		[learning rate: 0.001892]
	Learning Rate: 0.001892
	LOSS [training: 2.273591806406868 | validation: 2.9139308065683394]
	TIME [epoch: 8.16 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3325845968823513		[learning rate: 0.0018886]
		[batch 20/20] avg loss: 2.1916555577224517		[learning rate: 0.0018851]
	Learning Rate: 0.00188513
	LOSS [training: 2.2621200773024017 | validation: 3.004432355463343]
	TIME [epoch: 8.16 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2254323881232927		[learning rate: 0.0018817]
		[batch 20/20] avg loss: 2.3724147256179022		[learning rate: 0.0018783]
	Learning Rate: 0.00187829
	LOSS [training: 2.298923556870597 | validation: 2.950103827814033]
	TIME [epoch: 8.2 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.250085082561461		[learning rate: 0.0018749]
		[batch 20/20] avg loss: 2.170675950433705		[learning rate: 0.0018715]
	Learning Rate: 0.00187148
	LOSS [training: 2.2103805164975823 | validation: 2.931279411595877]
	TIME [epoch: 8.17 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.138117951328609		[learning rate: 0.0018681]
		[batch 20/20] avg loss: 2.443009602264361		[learning rate: 0.0018647]
	Learning Rate: 0.00186468
	LOSS [training: 2.2905637767964855 | validation: 2.9831333815594365]
	TIME [epoch: 8.17 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2948559162415405		[learning rate: 0.0018613]
		[batch 20/20] avg loss: 2.9249358630630584		[learning rate: 0.0018579]
	Learning Rate: 0.00185792
	LOSS [training: 2.6098958896522992 | validation: 3.296896592787828]
	TIME [epoch: 8.16 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5403018709727236		[learning rate: 0.0018545]
		[batch 20/20] avg loss: 2.6084102872834354		[learning rate: 0.0018512]
	Learning Rate: 0.00185117
	LOSS [training: 2.574356079128079 | validation: 3.0149288046074867]
	TIME [epoch: 8.16 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.266109475999843		[learning rate: 0.0018478]
		[batch 20/20] avg loss: 2.356771236142902		[learning rate: 0.0018445]
	Learning Rate: 0.00184446
	LOSS [training: 2.311440356071372 | validation: 3.1628178664462703]
	TIME [epoch: 8.18 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.217016903066418		[learning rate: 0.0018411]
		[batch 20/20] avg loss: 2.443646807112156		[learning rate: 0.0018378]
	Learning Rate: 0.00183776
	LOSS [training: 2.330331855089287 | validation: 3.1380408307092953]
	TIME [epoch: 8.16 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.17814101806549		[learning rate: 0.0018344]
		[batch 20/20] avg loss: 2.2155903853345307		[learning rate: 0.0018311]
	Learning Rate: 0.00183109
	LOSS [training: 2.19686570170001 | validation: 3.07393858063371]
	TIME [epoch: 8.16 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.139973457129888		[learning rate: 0.0018278]
		[batch 20/20] avg loss: 2.4103185873360027		[learning rate: 0.0018244]
	Learning Rate: 0.00182445
	LOSS [training: 2.275146022232945 | validation: 3.3783042922884343]
	TIME [epoch: 8.17 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.186959215003503		[learning rate: 0.0018211]
		[batch 20/20] avg loss: 2.8288129933974377		[learning rate: 0.0018178]
	Learning Rate: 0.00181783
	LOSS [training: 3.0078861042004705 | validation: 3.1192384451565918]
	TIME [epoch: 8.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4426420482677984		[learning rate: 0.0018145]
		[batch 20/20] avg loss: 2.017769937963208		[learning rate: 0.0018112]
	Learning Rate: 0.00181123
	LOSS [training: 2.2302059931155034 | validation: 2.932490422273609]
	TIME [epoch: 8.16 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2899785370079875		[learning rate: 0.0018079]
		[batch 20/20] avg loss: 2.408154125139255		[learning rate: 0.0018047]
	Learning Rate: 0.00180466
	LOSS [training: 2.3490663310736215 | validation: 3.3360778144945247]
	TIME [epoch: 8.17 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5809315744932655		[learning rate: 0.0018014]
		[batch 20/20] avg loss: 2.231200823110225		[learning rate: 0.0017981]
	Learning Rate: 0.00179811
	LOSS [training: 2.406066198801745 | validation: 2.8979441188513277]
	TIME [epoch: 8.17 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3363590583294185		[learning rate: 0.0017948]
		[batch 20/20] avg loss: 2.2725753188724944		[learning rate: 0.0017916]
	Learning Rate: 0.00179158
	LOSS [training: 2.3044671886009565 | validation: 2.9571402830049474]
	TIME [epoch: 8.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2495927585828195		[learning rate: 0.0017883]
		[batch 20/20] avg loss: 2.2038406527412793		[learning rate: 0.0017851]
	Learning Rate: 0.00178508
	LOSS [training: 2.2267167056620494 | validation: 2.903415365472039]
	TIME [epoch: 8.18 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0769082204418834		[learning rate: 0.0017818]
		[batch 20/20] avg loss: 2.357828234745745		[learning rate: 0.0017786]
	Learning Rate: 0.0017786
	LOSS [training: 2.2173682275938145 | validation: 2.938650897709351]
	TIME [epoch: 8.17 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0073979626431546		[learning rate: 0.0017754]
		[batch 20/20] avg loss: 2.2865942611477936		[learning rate: 0.0017721]
	Learning Rate: 0.00177215
	LOSS [training: 2.146996111895474 | validation: 2.977744910778249]
	TIME [epoch: 8.17 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3392797312430167		[learning rate: 0.0017689]
		[batch 20/20] avg loss: 2.0521412379947774		[learning rate: 0.0017657]
	Learning Rate: 0.00176572
	LOSS [training: 2.195710484618897 | validation: 2.895495349290151]
	TIME [epoch: 8.17 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.061897916780134		[learning rate: 0.0017625]
		[batch 20/20] avg loss: 2.23244583549322		[learning rate: 0.0017593]
	Learning Rate: 0.00175931
	LOSS [training: 2.147171876136677 | validation: 2.963135638354105]
	TIME [epoch: 8.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.146936226336914		[learning rate: 0.0017561]
		[batch 20/20] avg loss: 2.1179753252259985		[learning rate: 0.0017529]
	Learning Rate: 0.00175292
	LOSS [training: 2.132455775781456 | validation: 2.959011433557007]
	TIME [epoch: 8.18 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.184822838220925		[learning rate: 0.0017497]
		[batch 20/20] avg loss: 2.079786545369298		[learning rate: 0.0017466]
	Learning Rate: 0.00174656
	LOSS [training: 2.132304691795111 | validation: 2.9152295942480455]
	TIME [epoch: 8.17 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0450556353799554		[learning rate: 0.0017434]
		[batch 20/20] avg loss: 2.198370818465759		[learning rate: 0.0017402]
	Learning Rate: 0.00174022
	LOSS [training: 2.1217132269228567 | validation: 2.911162519157429]
	TIME [epoch: 8.18 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0721238175023156		[learning rate: 0.0017371]
		[batch 20/20] avg loss: 2.145624345557034		[learning rate: 0.0017339]
	Learning Rate: 0.00173391
	LOSS [training: 2.1088740815296747 | validation: 2.871201552921458]
	TIME [epoch: 8.19 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9588112718277677		[learning rate: 0.0017308]
		[batch 20/20] avg loss: 2.270672488015781		[learning rate: 0.0017276]
	Learning Rate: 0.00172762
	LOSS [training: 2.114741879921774 | validation: 2.929093508097272]
	TIME [epoch: 8.16 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.137122119579444		[learning rate: 0.0017245]
		[batch 20/20] avg loss: 2.0935894167113926		[learning rate: 0.0017213]
	Learning Rate: 0.00172135
	LOSS [training: 2.115355768145419 | validation: 2.910271420902517]
	TIME [epoch: 8.16 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1230290951702453		[learning rate: 0.0017182]
		[batch 20/20] avg loss: 2.0677576601471133		[learning rate: 0.0017151]
	Learning Rate: 0.0017151
	LOSS [training: 2.0953933776586795 | validation: 2.915075197101043]
	TIME [epoch: 8.16 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0700110925520803		[learning rate: 0.001712]
		[batch 20/20] avg loss: 2.17158165045763		[learning rate: 0.0017089]
	Learning Rate: 0.00170888
	LOSS [training: 2.1207963715048557 | validation: 2.9244130398538437]
	TIME [epoch: 8.17 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3143459344567243		[learning rate: 0.0017058]
		[batch 20/20] avg loss: 1.9326453685653011		[learning rate: 0.0017027]
	Learning Rate: 0.00170267
	LOSS [training: 2.1234956515110124 | validation: 2.993567049352598]
	TIME [epoch: 8.19 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2501977704788914		[learning rate: 0.0016996]
		[batch 20/20] avg loss: 2.034606540207043		[learning rate: 0.0016965]
	Learning Rate: 0.0016965
	LOSS [training: 2.142402155342967 | validation: 2.933902272623772]
	TIME [epoch: 8.16 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0668122281175085		[learning rate: 0.0016934]
		[batch 20/20] avg loss: 2.150022201018585		[learning rate: 0.0016903]
	Learning Rate: 0.00169034
	LOSS [training: 2.1084172145680466 | validation: 2.919669954644347]
	TIME [epoch: 8.18 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0482012933160183		[learning rate: 0.0016873]
		[batch 20/20] avg loss: 2.1455932635918993		[learning rate: 0.0016842]
	Learning Rate: 0.0016842
	LOSS [training: 2.0968972784539583 | validation: 2.861229370636164]
	TIME [epoch: 8.17 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0060020471946256		[learning rate: 0.0016811]
		[batch 20/20] avg loss: 2.1435538464110637		[learning rate: 0.0016781]
	Learning Rate: 0.00167809
	LOSS [training: 2.0747779468028447 | validation: 2.8860567467634115]
	TIME [epoch: 8.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.225479552940604		[learning rate: 0.001675]
		[batch 20/20] avg loss: 1.9819609366976887		[learning rate: 0.001672]
	Learning Rate: 0.001672
	LOSS [training: 2.103720244819146 | validation: 2.9053516568683166]
	TIME [epoch: 8.18 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1439873285983877		[learning rate: 0.001669]
		[batch 20/20] avg loss: 2.062391819329497		[learning rate: 0.0016659]
	Learning Rate: 0.00166593
	LOSS [training: 2.1031895739639426 | validation: 2.9171291114619695]
	TIME [epoch: 8.18 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.116361283316956		[learning rate: 0.0016629]
		[batch 20/20] avg loss: 2.0653763685599715		[learning rate: 0.0016599]
	Learning Rate: 0.00165989
	LOSS [training: 2.090868825938464 | validation: 2.951653695294151]
	TIME [epoch: 8.17 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.077187194325648		[learning rate: 0.0016569]
		[batch 20/20] avg loss: 2.0938435307095054		[learning rate: 0.0016539]
	Learning Rate: 0.00165387
	LOSS [training: 2.085515362517577 | validation: 2.886028281241469]
	TIME [epoch: 8.17 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.257070927685744		[learning rate: 0.0016509]
		[batch 20/20] avg loss: 1.939052440282095		[learning rate: 0.0016479]
	Learning Rate: 0.00164786
	LOSS [training: 2.098061683983919 | validation: 2.8770740618704727]
	TIME [epoch: 8.19 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9804255827971204		[learning rate: 0.0016449]
		[batch 20/20] avg loss: 2.223263805024535		[learning rate: 0.0016419]
	Learning Rate: 0.00164188
	LOSS [training: 2.1018446939108277 | validation: 2.879065836226822]
	TIME [epoch: 8.16 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.039901807855903		[learning rate: 0.0016389]
		[batch 20/20] avg loss: 2.130995609303924		[learning rate: 0.0016359]
	Learning Rate: 0.00163592
	LOSS [training: 2.085448708579914 | validation: 2.871582311482114]
	TIME [epoch: 8.17 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.108227203235475		[learning rate: 0.001633]
		[batch 20/20] avg loss: 2.091714164103524		[learning rate: 0.00163]
	Learning Rate: 0.00162999
	LOSS [training: 2.0999706836694996 | validation: 2.8964967003087585]
	TIME [epoch: 8.16 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.071366610137871		[learning rate: 0.001627]
		[batch 20/20] avg loss: 2.1218790711179722		[learning rate: 0.0016241]
	Learning Rate: 0.00162407
	LOSS [training: 2.096622840627922 | validation: 2.8710464632366812]
	TIME [epoch: 8.19 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1441870039197934		[learning rate: 0.0016211]
		[batch 20/20] avg loss: 2.046629152759622		[learning rate: 0.0016182]
	Learning Rate: 0.00161818
	LOSS [training: 2.0954080783397075 | validation: 2.861037970386335]
	TIME [epoch: 8.16 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2291177991752087		[learning rate: 0.0016152]
		[batch 20/20] avg loss: 2.0528237071882858		[learning rate: 0.0016123]
	Learning Rate: 0.00161231
	LOSS [training: 2.140970753181747 | validation: 2.996502730276717]
	TIME [epoch: 8.17 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.320209162548847		[learning rate: 0.0016094]
		[batch 20/20] avg loss: 2.450844460987209		[learning rate: 0.0016065]
	Learning Rate: 0.00160645
	LOSS [training: 2.385526811768028 | validation: 2.9521418568951274]
	TIME [epoch: 8.17 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1445869938142503		[learning rate: 0.0016035]
		[batch 20/20] avg loss: 2.080639263925589		[learning rate: 0.0016006]
	Learning Rate: 0.00160062
	LOSS [training: 2.1126131288699197 | validation: 2.8875441744203973]
	TIME [epoch: 8.18 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.05237887472746		[learning rate: 0.0015977]
		[batch 20/20] avg loss: 2.094771922233415		[learning rate: 0.0015948]
	Learning Rate: 0.00159482
	LOSS [training: 2.073575398480437 | validation: 2.867668370618436]
	TIME [epoch: 8.18 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1432737843778176		[learning rate: 0.0015919]
		[batch 20/20] avg loss: 2.244234111576783		[learning rate: 0.001589]
	Learning Rate: 0.00158903
	LOSS [training: 2.1937539479773 | validation: 2.948199889119776]
	TIME [epoch: 8.17 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.17037482384855		[learning rate: 0.0015861]
		[batch 20/20] avg loss: 2.291497438758637		[learning rate: 0.0015833]
	Learning Rate: 0.00158326
	LOSS [training: 2.2309361313035936 | validation: 3.325021233202262]
	TIME [epoch: 8.17 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4048139856997306		[learning rate: 0.0015804]
		[batch 20/20] avg loss: 2.147005222405881		[learning rate: 0.0015775]
	Learning Rate: 0.00157752
	LOSS [training: 2.275909604052806 | validation: 2.9178704226876397]
	TIME [epoch: 8.17 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.075286121963262		[learning rate: 0.0015747]
		[batch 20/20] avg loss: 2.1714028908842016		[learning rate: 0.0015718]
	Learning Rate: 0.00157179
	LOSS [training: 2.1233445064237317 | validation: 2.9665594743686325]
	TIME [epoch: 8.19 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.132770724114986		[learning rate: 0.0015689]
		[batch 20/20] avg loss: 2.1282290139124584		[learning rate: 0.0015661]
	Learning Rate: 0.00156609
	LOSS [training: 2.130499869013722 | validation: 2.9222946701875325]
	TIME [epoch: 8.17 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0317720414149694		[learning rate: 0.0015632]
		[batch 20/20] avg loss: 2.167783051957139		[learning rate: 0.0015604]
	Learning Rate: 0.0015604
	LOSS [training: 2.0997775466860547 | validation: 2.870439948530745]
	TIME [epoch: 8.17 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0434052562757428		[learning rate: 0.0015576]
		[batch 20/20] avg loss: 2.395541771852804		[learning rate: 0.0015547]
	Learning Rate: 0.00155474
	LOSS [training: 2.2194735140642736 | validation: 3.121824243055732]
	TIME [epoch: 8.16 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4796388668375187		[learning rate: 0.0015519]
		[batch 20/20] avg loss: 2.1409848451660447		[learning rate: 0.0015491]
	Learning Rate: 0.0015491
	LOSS [training: 2.310311856001781 | validation: 3.058863865129064]
	TIME [epoch: 8.19 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.639737436557709		[learning rate: 0.0015463]
		[batch 20/20] avg loss: 2.811390920726282		[learning rate: 0.0015435]
ERROR:
nan encountered in epoch 613 (validation loss).
