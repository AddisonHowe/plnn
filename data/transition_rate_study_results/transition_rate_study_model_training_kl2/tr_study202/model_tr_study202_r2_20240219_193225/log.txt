Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r2', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r2', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=200, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=100, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1643028050

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 10/20] avg loss: 11.646751473149163		[learning rate: 0.01]
		[batch 20/20] avg loss: 10.914394821488859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.280573147319014 | validation: 12.017515855402507]
	TIME [epoch: 52.3 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 10/20] avg loss: 10.255669941809128		[learning rate: 0.01]
		[batch 20/20] avg loss: 10.260867361895547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.258268651852342 | validation: 7.208288853862247]
	TIME [epoch: 8.41 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 10/20] avg loss: 7.331178665302991		[learning rate: 0.01]
		[batch 20/20] avg loss: 5.314062923431898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.322620794367444 | validation: 5.791712000380811]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.795162883067452		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.4780059458677925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.636584414467622 | validation: 5.978484494537721]
	TIME [epoch: 8.35 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 10/20] avg loss: 4.348114727613656		[learning rate: 0.01]
		[batch 20/20] avg loss: 4.243891862415795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.296003295014726 | validation: 4.983854605998794]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.807728967476419		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.7939042603585293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.800816613917474 | validation: 4.707322515795978]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.6384049894070154		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.346393516997069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4923992532020423 | validation: 4.504048502633143]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.3709847669259907		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.125824090922805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.248404428924398 | validation: 3.917214195636089]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 10/20] avg loss: 3.179452197861452		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.000255461628293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0898538297448725 | validation: 4.656236900875347]
	TIME [epoch: 8.39 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.9532935512389042		[learning rate: 0.01]
		[batch 20/20] avg loss: 3.0332890380577995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.993291294648351 | validation: 3.931919816545843]
	TIME [epoch: 8.37 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.993954401977981		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8837884748955243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.938871438436753 | validation: 3.7584834909424]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8790553754544015		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8885213446192335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8837883600368177 | validation: 3.763005737753331]
	TIME [epoch: 8.37 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.87914662451171		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.9078470436003565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.893496834056033 | validation: 4.095018731314419]
	TIME [epoch: 8.38 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8287339762197448		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8749304150439574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.851832195631851 | validation: 4.076716477977428]
	TIME [epoch: 8.35 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.875005763316546		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.8616690798229207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.868337421569733 | validation: 3.686799257952017]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.791786474015029		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.741334385461294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7665604297381616 | validation: 3.6554711293497286]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.8429860464228764		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.77304448054253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8080152634827034 | validation: 3.6350910509007797]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7165500472622672		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.818211485924711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.767380766593489 | validation: 3.5356474196133902]
	TIME [epoch: 8.34 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.69485854276357		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.7811934225927915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7380259826781805 | validation: 3.90499009311495]
	TIME [epoch: 8.35 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.7512922764148375		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.6301931261173737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.690742701266105 | validation: 3.5212772674247006]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6138065379926987		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.572232384258439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.593019461125569 | validation: 3.6420284674755012]
	TIME [epoch: 8.36 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6871556363002003		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.537475655968204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6123156461342028 | validation: 3.8425205610534054]
	TIME [epoch: 8.34 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6856831410488295		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4031502286588027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.544416684853816 | validation: 3.431875788177676]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.6342991259966797		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3592968704123174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4967979982044985 | validation: 4.0591042453333195]
	TIME [epoch: 8.38 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.581560886720006		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.489886771931668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.535723829325837 | validation: 3.5136152511183645]
	TIME [epoch: 8.34 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4275580500345337		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.504029309082883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4657936795587077 | validation: 3.743341448964047]
	TIME [epoch: 8.34 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5717031796995027		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4369503710268225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5043267753631633 | validation: 3.282916270762087]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3608095118461185		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4273122720206852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.394060891933402 | validation: 3.644692736256363]
	TIME [epoch: 8.38 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.431838671764715		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.32137194824211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3766053100034124 | validation: 3.2002598399962343]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5200766393524003		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.418592500967885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.469334570160142 | validation: 3.5813354304671274]
	TIME [epoch: 8.34 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.443032322201525		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2850485692763365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3640404457389312 | validation: 3.3143890603986157]
	TIME [epoch: 8.36 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.4304974122269085		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.306976162305137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3687367872660228 | validation: 3.142882152038241]
	TIME [epoch: 8.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3527366095320916		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2573971873254055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3050668984287492 | validation: 3.7318287142198416]
	TIME [epoch: 8.36 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.5455755757390826		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1471563008311763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.346365938285129 | validation: 3.154683869388947]
	TIME [epoch: 8.36 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2143618615355765		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.398771040297361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.306566450916468 | validation: 3.2753485087423746]
	TIME [epoch: 8.39 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.290570001501231		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.26959353129058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.280081766395905 | validation: 3.054181730734645]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0667446923778656		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.4539062170859793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2603254547319223 | validation: 3.22570342599092]
	TIME [epoch: 8.35 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3658238062172336		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3373653119863405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3515945591017866 | validation: 3.179175625512425]
	TIME [epoch: 8.35 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.327009277275982		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.2965547125670307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.311781994921507 | validation: 3.0919904947425993]
	TIME [epoch: 8.38 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2535258113001095		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.3300861811810463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2918059962405777 | validation: 3.114051605864569]
	TIME [epoch: 8.37 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2035706492113882		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.313152250710753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.258361449961071 | validation: 3.095410007847212]
	TIME [epoch: 8.35 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.290194203995918		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0671843352862025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.17868926964106 | validation: 3.111254947048427]
	TIME [epoch: 8.36 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.214048993032533		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1674398020026246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.190744397517579 | validation: 3.0233235474119073]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.43446731050037		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.040851882940817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2376595967205937 | validation: 3.241199746912149]
	TIME [epoch: 8.37 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.2685351908696756		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.065050684785724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1667929378277 | validation: 3.3869617224479547]
	TIME [epoch: 8.33 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1403178899741535		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0096223473489867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0749701186615703 | validation: 3.2667157888803073]
	TIME [epoch: 8.36 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0773625957879296		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.108219957136648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0927912764622887 | validation: 3.0064524044703953]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.3064261449632917		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9768358247978814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.141630984880586 | validation: 2.9820418459523164]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0791835890720467		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0303595175171014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0547715532945743 | validation: 3.1312583042187647]
	TIME [epoch: 8.34 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.104987982999206		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.092116229370383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.098552106184795 | validation: 3.029882122694181]
	TIME [epoch: 8.37 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1467836267647096		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0059908165245415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0763872216446257 | validation: 2.8186179940408342]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9252997529620182		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1558950487503914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.040597400856205 | validation: 2.879344147174649]
	TIME [epoch: 8.34 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.088742867327516		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9598051941570602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0242740307422884 | validation: 3.081319574889346]
	TIME [epoch: 8.34 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9121171864705697		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.1128195211327316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0124683538016503 | validation: 2.8497377201435894]
	TIME [epoch: 8.38 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0616102424375446		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.084543572338233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0730769073878887 | validation: 2.8829811914772687]
	TIME [epoch: 8.38 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0556565568707397		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.017401866430015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.036529211650377 | validation: 2.99185522681322]
	TIME [epoch: 8.35 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0723983781213535		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0667962860137585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0695973320675556 | validation: 3.3648313422408016]
	TIME [epoch: 8.35 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1488845367528047		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.791537285134411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.970210910943608 | validation: 2.812750244777546]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9480725563970307		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0984591995753017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0232658779861663 | validation: 3.3547774796959096]
	TIME [epoch: 8.37 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.01674473557545		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9671962045416884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.991970470058569 | validation: 3.0092746177701764]
	TIME [epoch: 8.34 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9043565342461437		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9405245993669626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9224405668065532 | validation: 3.151563322932274]
	TIME [epoch: 8.35 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.00753087284063		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9041147816845103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9558228272625695 | validation: 2.7953807938897897]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.895355692727205		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9118412506288993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9035984716780519 | validation: 2.790749150758085]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.98744694348615		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9630815059254587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9752642247058045 | validation: 2.7661367080150834]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1182266222095056		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8352562074028487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9767414148061775 | validation: 2.799836517734584]
	TIME [epoch: 8.38 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9845519327832044		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9100380591884467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9472949959858254 | validation: 3.269967932343073]
	TIME [epoch: 8.38 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8039587355334696		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9017434647963831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8528511001649264 | validation: 2.706761640738786]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9229077769699177		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9352712121226692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9290894945462935 | validation: 3.3750444551003174]
	TIME [epoch: 8.37 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8940021033329		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9174915019822794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9057468026575897 | validation: 3.009990241286446]
	TIME [epoch: 8.39 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.872272534679259		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9624557562802931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.917364145479776 | validation: 2.864331060028131]
	TIME [epoch: 8.37 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7934472707947329		[learning rate: 0.01]
		[batch 20/20] avg loss: 2.0619265445339514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9276869076643426 | validation: 2.841660859379895]
	TIME [epoch: 8.39 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0119776296509086		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6873900262623664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8496838279566379 | validation: 2.802514633093227]
	TIME [epoch: 8.36 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.849466729334976		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8563047849094343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8528857571222055 | validation: 2.6412467253661265]
	TIME [epoch: 8.4 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9429561700714906		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9691178651821126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9560370176268016 | validation: 3.05451261669927]
	TIME [epoch: 8.36 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9693457046655372		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.987805807417041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9785757560412893 | validation: 2.752956813263922]
	TIME [epoch: 8.38 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.815842308363515		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9244375545576304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8701399314605731 | validation: 2.774633488301075]
	TIME [epoch: 8.36 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8588115394858236		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8640688322139205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.861440185849872 | validation: 2.7090202010897007]
	TIME [epoch: 8.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9150390445882757		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7485733223313809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8318061834598283 | validation: 2.906322449972923]
	TIME [epoch: 8.36 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9595872417210756		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8835250759045667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.921556158812821 | validation: 2.839818670107379]
	TIME [epoch: 8.38 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.874590940410553		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7484212595001274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.81150609995534 | validation: 2.968076698480443]
	TIME [epoch: 8.35 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9417137412348537		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8649275665887566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9033206539118055 | validation: 2.9235959340938362]
	TIME [epoch: 8.38 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8910654781480942		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.804301915958391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8476836970532429 | validation: 3.0610854640947665]
	TIME [epoch: 8.36 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.818651308871792		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.932307258661412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8754792837666017 | validation: 2.6789337483491655]
	TIME [epoch: 8.38 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.394443003287857		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8003631264746491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.097403064881253 | validation: 3.1111840917459097]
	TIME [epoch: 8.35 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9939785064930873		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7882744227057508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8911264645994188 | validation: 3.256080243205991]
	TIME [epoch: 8.39 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8038233655837874		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8375839733573016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8207036694705447 | validation: 2.8614925717720876]
	TIME [epoch: 8.35 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9097477379228078		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6638827868989368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7868152624108724 | validation: 2.7795819912683677]
	TIME [epoch: 8.38 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.763580360565847		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8881903750398834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.825885367802865 | validation: 2.791329388859604]
	TIME [epoch: 8.35 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7991315597479915		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8429032243248293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8210173920364106 | validation: 2.5847795442022963]
	TIME [epoch: 8.39 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8606329597730717		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7986633975133395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8296481786432057 | validation: 2.5820239227556194]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7949007478982029		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.9145469740756482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8547238609869254 | validation: 3.1355311127409284]
	TIME [epoch: 8.37 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7793540530441567		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.760358347854097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.769856200449127 | validation: 2.761554399342648]
	TIME [epoch: 8.36 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7785154236317655		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8752022053558506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8268588144938083 | validation: 2.6953667231643417]
	TIME [epoch: 8.35 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8986383793373662		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7187117759615163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.808675077649441 | validation: 2.757029369103894]
	TIME [epoch: 8.34 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7638522329971345		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.712841062813655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7383466479053946 | validation: 2.803832888250909]
	TIME [epoch: 8.36 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8510062739095623		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.7840249419153829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8175156079124721 | validation: 2.838902495270733]
	TIME [epoch: 8.37 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.706237633863001		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8815896753626178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7939136546128094 | validation: 2.7765956758824917]
	TIME [epoch: 8.36 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8099346764471975		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6963893701885264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7531620233178615 | validation: 2.977988169366585]
	TIME [epoch: 8.34 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9155031234885935		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.6797319770834906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.797617550286042 | validation: 2.7992168147531364]
	TIME [epoch: 8.36 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7528638867565647		[learning rate: 0.01]
		[batch 20/20] avg loss: 1.8820248857780122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.817444386267288 | validation: 3.1081735992956983]
	TIME [epoch: 8.35 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7242158243479273		[learning rate: 0.0099837]
		[batch 20/20] avg loss: 1.7884688727443117		[learning rate: 0.0099655]
	Learning Rate: 0.00996552
	LOSS [training: 1.7563423485461194 | validation: 2.8315867228011538]
	TIME [epoch: 8.36 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8934376927678602		[learning rate: 0.0099474]
		[batch 20/20] avg loss: 1.709122993494495		[learning rate: 0.0099294]
	Learning Rate: 0.00992935
	LOSS [training: 1.8012803431311775 | validation: 2.632558223290157]
	TIME [epoch: 8.34 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7144264096734028		[learning rate: 0.0099113]
		[batch 20/20] avg loss: 1.9667826576498146		[learning rate: 0.0098933]
	Learning Rate: 0.00989332
	LOSS [training: 1.8406045336616088 | validation: 2.795460322051009]
	TIME [epoch: 8.36 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7155794305879533		[learning rate: 0.0098754]
		[batch 20/20] avg loss: 1.7626059634294413		[learning rate: 0.0098574]
	Learning Rate: 0.00985742
	LOSS [training: 1.7390926970086973 | validation: 2.910769266148242]
	TIME [epoch: 8.37 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7585023772332868		[learning rate: 0.0098395]
		[batch 20/20] avg loss: 1.745313354317723		[learning rate: 0.0098216]
	Learning Rate: 0.00982164
	LOSS [training: 1.751907865775505 | validation: 2.8163194729437806]
	TIME [epoch: 8.36 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9297436821952787		[learning rate: 0.0098038]
		[batch 20/20] avg loss: 1.6661321030885212		[learning rate: 0.009786]
	Learning Rate: 0.009786
	LOSS [training: 1.7979378926419003 | validation: 3.0745464982264332]
	TIME [epoch: 8.34 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6712126672474148		[learning rate: 0.0097682]
		[batch 20/20] avg loss: 1.7777764490635846		[learning rate: 0.0097505]
	Learning Rate: 0.00975049
	LOSS [training: 1.7244945581554998 | validation: 2.687954611247556]
	TIME [epoch: 8.36 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8594129223688856		[learning rate: 0.0097328]
		[batch 20/20] avg loss: 1.5117888456547761		[learning rate: 0.0097151]
	Learning Rate: 0.0097151
	LOSS [training: 1.6856008840118306 | validation: 2.7172585523433295]
	TIME [epoch: 8.37 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9910139743349284		[learning rate: 0.0096975]
		[batch 20/20] avg loss: 1.7796091676033878		[learning rate: 0.0096798]
	Learning Rate: 0.00967984
	LOSS [training: 1.885311570969158 | validation: 2.6137427635141988]
	TIME [epoch: 8.34 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.746782786458033		[learning rate: 0.0096623]
		[batch 20/20] avg loss: 1.76257600240466		[learning rate: 0.0096447]
	Learning Rate: 0.00964472
	LOSS [training: 1.7546793944313464 | validation: 2.884744422814281]
	TIME [epoch: 8.35 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7334369612184477		[learning rate: 0.0096272]
		[batch 20/20] avg loss: 1.790923162627394		[learning rate: 0.0096097]
	Learning Rate: 0.00960972
	LOSS [training: 1.7621800619229213 | validation: 2.57538592124308]
	TIME [epoch: 8.36 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7004337839562638		[learning rate: 0.0095923]
		[batch 20/20] avg loss: 1.7986306220789487		[learning rate: 0.0095748]
	Learning Rate: 0.00957484
	LOSS [training: 1.7495322030176066 | validation: 2.5965233848303506]
	TIME [epoch: 8.38 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.778217502313506		[learning rate: 0.0095575]
		[batch 20/20] avg loss: 1.759272509231846		[learning rate: 0.0095401]
	Learning Rate: 0.00954009
	LOSS [training: 1.7687450057726761 | validation: 2.7724469100220754]
	TIME [epoch: 8.34 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6267008214882153		[learning rate: 0.0095228]
		[batch 20/20] avg loss: 1.9132080937504106		[learning rate: 0.0095055]
	Learning Rate: 0.00950547
	LOSS [training: 1.7699544576193134 | validation: 2.8213125758716253]
	TIME [epoch: 8.36 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8322853634879617		[learning rate: 0.0094882]
		[batch 20/20] avg loss: 1.5811141021981694		[learning rate: 0.009471]
	Learning Rate: 0.00947098
	LOSS [training: 1.706699732843066 | validation: 2.650691606415537]
	TIME [epoch: 8.35 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.983671788557524		[learning rate: 0.0094538]
		[batch 20/20] avg loss: 1.6961582446422683		[learning rate: 0.0094366]
	Learning Rate: 0.0094366
	LOSS [training: 1.839915016599896 | validation: 2.561256844494462]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6945216329454111		[learning rate: 0.0094195]
		[batch 20/20] avg loss: 1.795334676609557		[learning rate: 0.0094024]
	Learning Rate: 0.00940236
	LOSS [training: 1.7449281547774842 | validation: 2.793086750909154]
	TIME [epoch: 8.33 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9138520657547697		[learning rate: 0.0093853]
		[batch 20/20] avg loss: 1.7164843787313846		[learning rate: 0.0093682]
	Learning Rate: 0.00936824
	LOSS [training: 1.8151682222430767 | validation: 2.6127233501124496]
	TIME [epoch: 8.36 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6138533692190815		[learning rate: 0.0093512]
		[batch 20/20] avg loss: 1.775472647782244		[learning rate: 0.0093342]
	Learning Rate: 0.00933424
	LOSS [training: 1.6946630085006629 | validation: 2.6099119359912883]
	TIME [epoch: 8.35 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7620775095899883		[learning rate: 0.0093173]
		[batch 20/20] avg loss: 1.6966101230389374		[learning rate: 0.0093004]
	Learning Rate: 0.00930036
	LOSS [training: 1.7293438163144628 | validation: 2.7518165081722]
	TIME [epoch: 8.35 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6715339171964174		[learning rate: 0.0092835]
		[batch 20/20] avg loss: 1.6948414400088179		[learning rate: 0.0092666]
	Learning Rate: 0.00926661
	LOSS [training: 1.6831876786026179 | validation: 2.5961355846476923]
	TIME [epoch: 8.32 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.687506087528081		[learning rate: 0.0092498]
		[batch 20/20] avg loss: 1.820604179994405		[learning rate: 0.009233]
	Learning Rate: 0.00923298
	LOSS [training: 1.7540551337612431 | validation: 3.2325712237495536]
	TIME [epoch: 8.35 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7119239726073316		[learning rate: 0.0092162]
		[batch 20/20] avg loss: 1.807590158529996		[learning rate: 0.0091995]
	Learning Rate: 0.00919948
	LOSS [training: 1.7597570655686638 | validation: 2.719353363897815]
	TIME [epoch: 8.35 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6993241849883733		[learning rate: 0.0091828]
		[batch 20/20] avg loss: 1.7439134424944704		[learning rate: 0.0091661]
	Learning Rate: 0.00916609
	LOSS [training: 1.7216188137414221 | validation: 2.5563021721230204]
	TIME [epoch: 8.35 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7739347708539763		[learning rate: 0.0091494]
		[batch 20/20] avg loss: 1.6327908044434873		[learning rate: 0.0091328]
	Learning Rate: 0.00913283
	LOSS [training: 1.7033627876487318 | validation: 2.7273514819576397]
	TIME [epoch: 8.34 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.59680867984485		[learning rate: 0.0091162]
		[batch 20/20] avg loss: 1.8310126929885275		[learning rate: 0.0090997]
	Learning Rate: 0.00909968
	LOSS [training: 1.7139106864166886 | validation: 2.8988537453712326]
	TIME [epoch: 8.36 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7315477949393965		[learning rate: 0.0090832]
		[batch 20/20] avg loss: 1.661046205318213		[learning rate: 0.0090667]
	Learning Rate: 0.00906666
	LOSS [training: 1.696297000128805 | validation: 2.5189325525518855]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5416739555135348		[learning rate: 0.0090502]
		[batch 20/20] avg loss: 1.866185332318696		[learning rate: 0.0090338]
	Learning Rate: 0.00903376
	LOSS [training: 1.7039296439161156 | validation: 2.7392901430922523]
	TIME [epoch: 8.35 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7009632684300247		[learning rate: 0.0090174]
		[batch 20/20] avg loss: 1.7954093572374017		[learning rate: 0.009001]
	Learning Rate: 0.00900097
	LOSS [training: 1.7481863128337132 | validation: 2.572329870911202]
	TIME [epoch: 8.34 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6521855105764476		[learning rate: 0.0089846]
		[batch 20/20] avg loss: 1.7264303311033184		[learning rate: 0.0089683]
	Learning Rate: 0.00896831
	LOSS [training: 1.6893079208398827 | validation: 2.5684564061990884]
	TIME [epoch: 8.36 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7592267602227996		[learning rate: 0.008952]
		[batch 20/20] avg loss: 1.8121264326488038		[learning rate: 0.0089358]
	Learning Rate: 0.00893576
	LOSS [training: 1.7856765964358015 | validation: 2.5895894399239303]
	TIME [epoch: 8.37 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7560022539320617		[learning rate: 0.0089195]
		[batch 20/20] avg loss: 1.5728817149265386		[learning rate: 0.0089033]
	Learning Rate: 0.00890333
	LOSS [training: 1.6644419844293004 | validation: 2.780697843078215]
	TIME [epoch: 8.34 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6661479196453108		[learning rate: 0.0088872]
		[batch 20/20] avg loss: 1.7836989882310916		[learning rate: 0.008871]
	Learning Rate: 0.00887102
	LOSS [training: 1.7249234539382012 | validation: 2.883001123486353]
	TIME [epoch: 8.34 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7460574649588119		[learning rate: 0.0088549]
		[batch 20/20] avg loss: 1.7584003745447778		[learning rate: 0.0088388]
	Learning Rate: 0.00883883
	LOSS [training: 1.7522289197517948 | validation: 2.9025139838296483]
	TIME [epoch: 8.35 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7219562323465212		[learning rate: 0.0088228]
		[batch 20/20] avg loss: 1.7229157024685726		[learning rate: 0.0088068]
	Learning Rate: 0.00880675
	LOSS [training: 1.722435967407547 | validation: 2.6487022337031303]
	TIME [epoch: 8.39 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9883405083884622		[learning rate: 0.0087908]
		[batch 20/20] avg loss: 1.7094029317131987		[learning rate: 0.0087748]
	Learning Rate: 0.00877479
	LOSS [training: 1.848871720050831 | validation: 2.6793975517927766]
	TIME [epoch: 8.34 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6695091907929522		[learning rate: 0.0087589]
		[batch 20/20] avg loss: 1.6457358290216622		[learning rate: 0.0087429]
	Learning Rate: 0.00874295
	LOSS [training: 1.657622509907307 | validation: 2.7330833690507834]
	TIME [epoch: 8.34 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6630101306529401		[learning rate: 0.0087271]
		[batch 20/20] avg loss: 1.8475552315168784		[learning rate: 0.0087112]
	Learning Rate: 0.00871122
	LOSS [training: 1.7552826810849094 | validation: 2.787048979571762]
	TIME [epoch: 8.36 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.564555808783266		[learning rate: 0.0086954]
		[batch 20/20] avg loss: 1.7699789799336234		[learning rate: 0.0086796]
	Learning Rate: 0.00867961
	LOSS [training: 1.6672673943584446 | validation: 2.610543136638743]
	TIME [epoch: 8.38 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6878063636712564		[learning rate: 0.0086638]
		[batch 20/20] avg loss: 1.6431768969502205		[learning rate: 0.0086481]
	Learning Rate: 0.00864811
	LOSS [training: 1.6654916303107388 | validation: 2.611953026820915]
	TIME [epoch: 8.33 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7091794049465459		[learning rate: 0.0086324]
		[batch 20/20] avg loss: 1.641990125585983		[learning rate: 0.0086167]
	Learning Rate: 0.00861672
	LOSS [training: 1.6755847652662648 | validation: 2.682763070309281]
	TIME [epoch: 8.34 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6716971684042274		[learning rate: 0.0086011]
		[batch 20/20] avg loss: 1.680619309897958		[learning rate: 0.0085855]
	Learning Rate: 0.00858545
	LOSS [training: 1.6761582391510927 | validation: 2.6779283740692983]
	TIME [epoch: 8.38 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8046520030413808		[learning rate: 0.0085699]
		[batch 20/20] avg loss: 1.5828986909404006		[learning rate: 0.0085543]
	Learning Rate: 0.00855429
	LOSS [training: 1.693775346990891 | validation: 2.6387585602361208]
	TIME [epoch: 8.36 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.616219400012185		[learning rate: 0.0085388]
		[batch 20/20] avg loss: 1.75312814239843		[learning rate: 0.0085232]
	Learning Rate: 0.00852325
	LOSS [training: 1.6846737712053077 | validation: 2.596045557260231]
	TIME [epoch: 8.33 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7469555484789236		[learning rate: 0.0085078]
		[batch 20/20] avg loss: 2.1461468575208777		[learning rate: 0.0084923]
	Learning Rate: 0.00849232
	LOSS [training: 1.9465512029999004 | validation: 2.6694519652795234]
	TIME [epoch: 8.33 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6646151289459308		[learning rate: 0.0084769]
		[batch 20/20] avg loss: 1.6442698859132243		[learning rate: 0.0084615]
	Learning Rate: 0.0084615
	LOSS [training: 1.6544425074295777 | validation: 2.7208544302343993]
	TIME [epoch: 8.38 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7291704839116728		[learning rate: 0.0084461]
		[batch 20/20] avg loss: 1.6594248429771834		[learning rate: 0.0084308]
	Learning Rate: 0.00843079
	LOSS [training: 1.6942976634444282 | validation: 2.6238749934886214]
	TIME [epoch: 8.36 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7431249467875147		[learning rate: 0.0084155]
		[batch 20/20] avg loss: 1.6412041252531986		[learning rate: 0.0084002]
	Learning Rate: 0.0084002
	LOSS [training: 1.6921645360203565 | validation: 2.7042302146097743]
	TIME [epoch: 8.34 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6909076450285982		[learning rate: 0.0083849]
		[batch 20/20] avg loss: 1.64990956566632		[learning rate: 0.0083697]
	Learning Rate: 0.00836971
	LOSS [training: 1.670408605347459 | validation: 2.633089673752921]
	TIME [epoch: 8.33 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7730746120145002		[learning rate: 0.0083545]
		[batch 20/20] avg loss: 1.4824266817848808		[learning rate: 0.0083393]
	Learning Rate: 0.00833934
	LOSS [training: 1.6277506468996905 | validation: 2.6803260348145264]
	TIME [epoch: 8.4 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6534594061485186		[learning rate: 0.0083242]
		[batch 20/20] avg loss: 1.569384201954605		[learning rate: 0.0083091]
	Learning Rate: 0.00830907
	LOSS [training: 1.6114218040515618 | validation: 3.017865261005926]
	TIME [epoch: 8.35 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7461526496836175		[learning rate: 0.008294]
		[batch 20/20] avg loss: 1.651052822543224		[learning rate: 0.0082789]
	Learning Rate: 0.00827892
	LOSS [training: 1.6986027361134206 | validation: 2.637289223205794]
	TIME [epoch: 8.33 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.714852591450502		[learning rate: 0.0082639]
		[batch 20/20] avg loss: 1.665946145535192		[learning rate: 0.0082489]
	Learning Rate: 0.00824887
	LOSS [training: 1.6903993684928467 | validation: 2.609006498676589]
	TIME [epoch: 8.33 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8877308103394927		[learning rate: 0.0082339]
		[batch 20/20] avg loss: 1.447860979252298		[learning rate: 0.0082189]
	Learning Rate: 0.00821894
	LOSS [training: 1.6677958947958955 | validation: 2.758518124370792]
	TIME [epoch: 8.4 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7540967631264734		[learning rate: 0.008204]
		[batch 20/20] avg loss: 1.5467449545671885		[learning rate: 0.0081891]
	Learning Rate: 0.00818911
	LOSS [training: 1.650420858846831 | validation: 2.6686879488235444]
	TIME [epoch: 8.35 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8649403926169037		[learning rate: 0.0081742]
		[batch 20/20] avg loss: 1.7514637973450875		[learning rate: 0.0081594]
	Learning Rate: 0.00815939
	LOSS [training: 1.8082020949809956 | validation: 2.682148843698778]
	TIME [epoch: 8.33 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.807372292830982		[learning rate: 0.0081446]
		[batch 20/20] avg loss: 1.6123638032950196		[learning rate: 0.0081298]
	Learning Rate: 0.00812978
	LOSS [training: 1.7098680480630009 | validation: 2.656833582023294]
	TIME [epoch: 8.33 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6587353318576104		[learning rate: 0.008115]
		[batch 20/20] avg loss: 1.6090136552902954		[learning rate: 0.0081003]
	Learning Rate: 0.00810028
	LOSS [training: 1.633874493573953 | validation: 2.559168306993611]
	TIME [epoch: 8.4 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5874242295577836		[learning rate: 0.0080856]
		[batch 20/20] avg loss: 1.8009402091185922		[learning rate: 0.0080709]
	Learning Rate: 0.00807088
	LOSS [training: 1.6941822193381881 | validation: 3.0526355062937722]
	TIME [epoch: 8.35 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7911060303029132		[learning rate: 0.0080562]
		[batch 20/20] avg loss: 1.6457084898678933		[learning rate: 0.0080416]
	Learning Rate: 0.00804159
	LOSS [training: 1.7184072600854035 | validation: 2.7001541474541737]
	TIME [epoch: 8.34 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6802431153441553		[learning rate: 0.008027]
		[batch 20/20] avg loss: 1.5543918167985928		[learning rate: 0.0080124]
	Learning Rate: 0.00801241
	LOSS [training: 1.6173174660713745 | validation: 2.62677029887665]
	TIME [epoch: 8.34 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7368769725325315		[learning rate: 0.0079979]
		[batch 20/20] avg loss: 1.549749042343159		[learning rate: 0.0079833]
	Learning Rate: 0.00798333
	LOSS [training: 1.6433130074378455 | validation: 2.6056758807826093]
	TIME [epoch: 8.41 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8017954014150626		[learning rate: 0.0079688]
		[batch 20/20] avg loss: 1.565712812203284		[learning rate: 0.0079544]
	Learning Rate: 0.00795436
	LOSS [training: 1.683754106809173 | validation: 2.8656948468986654]
	TIME [epoch: 8.36 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6923755846807744		[learning rate: 0.0079399]
		[batch 20/20] avg loss: 1.6912205027543408		[learning rate: 0.0079255]
	Learning Rate: 0.00792549
	LOSS [training: 1.691798043717558 | validation: 2.7256224560222555]
	TIME [epoch: 8.34 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.591293035619664		[learning rate: 0.0079111]
		[batch 20/20] avg loss: 1.6764768240197483		[learning rate: 0.0078967]
	Learning Rate: 0.00789673
	LOSS [training: 1.6338849298197065 | validation: 2.815531227360832]
	TIME [epoch: 8.36 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6412488335009479		[learning rate: 0.0078824]
		[batch 20/20] avg loss: 1.7697963341731906		[learning rate: 0.0078681]
	Learning Rate: 0.00786807
	LOSS [training: 1.705522583837069 | validation: 2.7660842742023304]
	TIME [epoch: 8.4 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7670201364517915		[learning rate: 0.0078538]
		[batch 20/20] avg loss: 1.6891527738333678		[learning rate: 0.0078395]
	Learning Rate: 0.00783952
	LOSS [training: 1.7280864551425796 | validation: 2.874200284846418]
	TIME [epoch: 8.35 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5847770848162157		[learning rate: 0.0078253]
		[batch 20/20] avg loss: 1.8456265891530925		[learning rate: 0.0078111]
	Learning Rate: 0.00781107
	LOSS [training: 1.7152018369846544 | validation: 2.990546900516082]
	TIME [epoch: 8.34 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.675184652418827		[learning rate: 0.0077969]
		[batch 20/20] avg loss: 1.6989967794658891		[learning rate: 0.0077827]
	Learning Rate: 0.00778272
	LOSS [training: 1.6870907159423585 | validation: 2.706781158730704]
	TIME [epoch: 8.37 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7550791352821915		[learning rate: 0.0077686]
		[batch 20/20] avg loss: 1.6592219614374994		[learning rate: 0.0077545]
	Learning Rate: 0.00775448
	LOSS [training: 1.7071505483598457 | validation: 2.579327120145642]
	TIME [epoch: 8.39 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8320511493888638		[learning rate: 0.0077404]
		[batch 20/20] avg loss: 1.6850753234777163		[learning rate: 0.0077263]
	Learning Rate: 0.00772634
	LOSS [training: 1.7585632364332902 | validation: 2.8613747567905357]
	TIME [epoch: 8.35 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7052520704718468		[learning rate: 0.0077123]
		[batch 20/20] avg loss: 1.81049329861832		[learning rate: 0.0076983]
	Learning Rate: 0.0076983
	LOSS [training: 1.7578726845450834 | validation: 2.6711977565851193]
	TIME [epoch: 8.34 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7386368417734281		[learning rate: 0.0076843]
		[batch 20/20] avg loss: 1.8446211971134936		[learning rate: 0.0076704]
	Learning Rate: 0.00767036
	LOSS [training: 1.7916290194434612 | validation: 2.7825834884886556]
	TIME [epoch: 8.35 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5876527957958508		[learning rate: 0.0076564]
		[batch 20/20] avg loss: 1.9532418167932775		[learning rate: 0.0076425]
	Learning Rate: 0.00764252
	LOSS [training: 1.7704473062945643 | validation: 2.832994166516215]
	TIME [epoch: 8.39 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8331209535707331		[learning rate: 0.0076286]
		[batch 20/20] avg loss: 1.6919562851170447		[learning rate: 0.0076148]
	Learning Rate: 0.00761479
	LOSS [training: 1.7625386193438888 | validation: 2.6237796225798893]
	TIME [epoch: 8.34 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7501375370048298		[learning rate: 0.007601]
		[batch 20/20] avg loss: 1.700587607494117		[learning rate: 0.0075872]
	Learning Rate: 0.00758715
	LOSS [training: 1.7253625722494732 | validation: 2.783690402138564]
	TIME [epoch: 8.35 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7435345592575338		[learning rate: 0.0075734]
		[batch 20/20] avg loss: 1.71037480340019		[learning rate: 0.0075596]
	Learning Rate: 0.00755962
	LOSS [training: 1.7269546813288617 | validation: 2.8980265003538066]
	TIME [epoch: 8.37 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.659944570387824		[learning rate: 0.0075459]
		[batch 20/20] avg loss: 1.8623551372985836		[learning rate: 0.0075322]
	Learning Rate: 0.00753219
	LOSS [training: 1.7611498538432033 | validation: 2.618868034574686]
	TIME [epoch: 8.39 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8779247001766568		[learning rate: 0.0075185]
		[batch 20/20] avg loss: 1.5022106106388913		[learning rate: 0.0075049]
	Learning Rate: 0.00750485
	LOSS [training: 1.690067655407774 | validation: 2.7011732334139307]
	TIME [epoch: 8.35 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9193359296949786		[learning rate: 0.0074912]
		[batch 20/20] avg loss: 1.804237107095635		[learning rate: 0.0074776]
	Learning Rate: 0.00747762
	LOSS [training: 1.861786518395307 | validation: 2.5709526343826687]
	TIME [epoch: 8.36 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9241985504747432		[learning rate: 0.007464]
		[batch 20/20] avg loss: 1.717259632081309		[learning rate: 0.0074505]
	Learning Rate: 0.00745048
	LOSS [training: 1.8207290912780256 | validation: 2.6239637724082696]
	TIME [epoch: 8.38 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.626116654048341		[learning rate: 0.0074369]
		[batch 20/20] avg loss: 1.9110488003009034		[learning rate: 0.0074234]
	Learning Rate: 0.00742344
	LOSS [training: 1.768582727174622 | validation: 2.787772857956411]
	TIME [epoch: 8.38 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.082912622479216		[learning rate: 0.00741]
		[batch 20/20] avg loss: 1.904750179578493		[learning rate: 0.0073965]
	Learning Rate: 0.0073965
	LOSS [training: 1.993831401028854 | validation: 3.0406758820832893]
	TIME [epoch: 8.35 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.979402326980821		[learning rate: 0.0073831]
		[batch 20/20] avg loss: 1.8112235823512601		[learning rate: 0.0073697]
	Learning Rate: 0.00736966
	LOSS [training: 1.8953129546660406 | validation: 2.9777968854578347]
	TIME [epoch: 8.35 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.904709473247125		[learning rate: 0.0073563]
		[batch 20/20] avg loss: 1.6938409604275892		[learning rate: 0.0073429]
	Learning Rate: 0.00734291
	LOSS [training: 1.7992752168373574 | validation: 2.9143948667560577]
	TIME [epoch: 8.38 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.881895613028768		[learning rate: 0.0073296]
		[batch 20/20] avg loss: 1.7661978324492966		[learning rate: 0.0073163]
	Learning Rate: 0.00731627
	LOSS [training: 1.8240467227390322 | validation: 2.802316069483154]
	TIME [epoch: 8.38 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.586255168415454		[learning rate: 0.007303]
		[batch 20/20] avg loss: 2.0730454788483215		[learning rate: 0.0072897]
	Learning Rate: 0.00728971
	LOSS [training: 1.8296503236318877 | validation: 3.2302958277038023]
	TIME [epoch: 8.37 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.694463545553673		[learning rate: 0.0072765]
		[batch 20/20] avg loss: 1.9614608728682943		[learning rate: 0.0072633]
	Learning Rate: 0.00726326
	LOSS [training: 1.8279622092109835 | validation: 3.056024710891807]
	TIME [epoch: 8.35 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9533787961141278		[learning rate: 0.0072501]
		[batch 20/20] avg loss: 1.8140577317754076		[learning rate: 0.0072369]
	Learning Rate: 0.0072369
	LOSS [training: 1.8837182639447678 | validation: 3.0072890822398683]
	TIME [epoch: 8.39 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9093626468801161		[learning rate: 0.0072238]
		[batch 20/20] avg loss: 1.8464270154267268		[learning rate: 0.0072106]
	Learning Rate: 0.00721064
	LOSS [training: 1.8778948311534216 | validation: 2.8548693889220202]
	TIME [epoch: 8.38 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8961565138575005		[learning rate: 0.0071975]
		[batch 20/20] avg loss: 1.863517057715347		[learning rate: 0.0071845]
	Learning Rate: 0.00718447
	LOSS [training: 1.879836785786424 | validation: 2.8797843910336987]
	TIME [epoch: 8.36 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7250351910485022		[learning rate: 0.0071714]
		[batch 20/20] avg loss: 1.944455361923303		[learning rate: 0.0071584]
	Learning Rate: 0.0071584
	LOSS [training: 1.834745276485903 | validation: 2.921009115789319]
	TIME [epoch: 8.35 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7922801945466187		[learning rate: 0.0071454]
		[batch 20/20] avg loss: 1.8852199551748465		[learning rate: 0.0071324]
	Learning Rate: 0.00713242
	LOSS [training: 1.8387500748607324 | validation: 2.9698488289540945]
	TIME [epoch: 8.38 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8908935307902184		[learning rate: 0.0071195]
		[batch 20/20] avg loss: 1.8081637292681403		[learning rate: 0.0071065]
	Learning Rate: 0.00710653
	LOSS [training: 1.849528630029179 | validation: 3.1743098327256227]
	TIME [epoch: 8.38 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8307571434623493		[learning rate: 0.0070936]
		[batch 20/20] avg loss: 1.9092624387268704		[learning rate: 0.0070807]
	Learning Rate: 0.00708074
	LOSS [training: 1.8700097910946094 | validation: 2.821813217092716]
	TIME [epoch: 8.36 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.729887540951666		[learning rate: 0.0070679]
		[batch 20/20] avg loss: 1.840538873097427		[learning rate: 0.007055]
	Learning Rate: 0.00705505
	LOSS [training: 1.7852132070245463 | validation: 2.9657883696453986]
	TIME [epoch: 8.38 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8237208527762605		[learning rate: 0.0070422]
		[batch 20/20] avg loss: 1.8361046825286718		[learning rate: 0.0070294]
	Learning Rate: 0.00702945
	LOSS [training: 1.8299127676524662 | validation: 2.7399622923893583]
	TIME [epoch: 8.38 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.665286640050842		[learning rate: 0.0070167]
		[batch 20/20] avg loss: 1.8043557937982957		[learning rate: 0.0070039]
	Learning Rate: 0.00700394
	LOSS [training: 1.7348212169245687 | validation: 2.7926753864529097]
	TIME [epoch: 8.38 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5880283814776288		[learning rate: 0.0069912]
		[batch 20/20] avg loss: 1.8133228250332079		[learning rate: 0.0069785]
	Learning Rate: 0.00697852
	LOSS [training: 1.7006756032554187 | validation: 2.801515239197804]
	TIME [epoch: 8.36 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6425161495596576		[learning rate: 0.0069658]
		[batch 20/20] avg loss: 1.6728632520341695		[learning rate: 0.0069532]
	Learning Rate: 0.00695319
	LOSS [training: 1.6576897007969138 | validation: 2.600587086493618]
	TIME [epoch: 8.37 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5569690036463246		[learning rate: 0.0069406]
		[batch 20/20] avg loss: 1.7442501568164155		[learning rate: 0.006928]
	Learning Rate: 0.00692796
	LOSS [training: 1.6506095802313703 | validation: 2.6603056616707867]
	TIME [epoch: 8.38 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5396040262781434		[learning rate: 0.0069154]
		[batch 20/20] avg loss: 2.0505610882020733		[learning rate: 0.0069028]
	Learning Rate: 0.00690282
	LOSS [training: 1.7950825572401083 | validation: 2.8085376244134426]
	TIME [epoch: 8.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7215911850659005		[learning rate: 0.0068903]
		[batch 20/20] avg loss: 2.218429362804701		[learning rate: 0.0068778]
	Learning Rate: 0.00687777
	LOSS [training: 1.9700102739353007 | validation: 3.3093432813050683]
	TIME [epoch: 8.36 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.995386316662895		[learning rate: 0.0068653]
		[batch 20/20] avg loss: 2.1141415202953238		[learning rate: 0.0068528]
	Learning Rate: 0.00685281
	LOSS [training: 2.054763918479109 | validation: 3.06478811190102]
	TIME [epoch: 8.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.047838860941688		[learning rate: 0.0068404]
		[batch 20/20] avg loss: 1.9810301867529385		[learning rate: 0.0068279]
	Learning Rate: 0.00682794
	LOSS [training: 2.0144345238473136 | validation: 3.1875600852282173]
	TIME [epoch: 8.37 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9412708563407761		[learning rate: 0.0068155]
		[batch 20/20] avg loss: 1.7494510942249242		[learning rate: 0.0068032]
	Learning Rate: 0.00680316
	LOSS [training: 1.84536097528285 | validation: 2.781215629967268]
	TIME [epoch: 8.38 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.763694504048167		[learning rate: 0.0067908]
		[batch 20/20] avg loss: 1.8512709536005896		[learning rate: 0.0067785]
	Learning Rate: 0.00677847
	LOSS [training: 1.8074827288243784 | validation: 2.930100950169492]
	TIME [epoch: 8.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6580813185486023		[learning rate: 0.0067662]
		[batch 20/20] avg loss: 2.0003388338561745		[learning rate: 0.0067539]
	Learning Rate: 0.00675387
	LOSS [training: 1.8292100762023882 | validation: 2.731482657651247]
	TIME [epoch: 8.38 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8547670189500507		[learning rate: 0.0067416]
		[batch 20/20] avg loss: 1.8890506727852774		[learning rate: 0.0067294]
	Learning Rate: 0.00672936
	LOSS [training: 1.871908845867664 | validation: 2.7457970610385396]
	TIME [epoch: 8.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8530421422915222		[learning rate: 0.0067171]
		[batch 20/20] avg loss: 1.7896364052569922		[learning rate: 0.0067049]
	Learning Rate: 0.00670494
	LOSS [training: 1.8213392737742577 | validation: 2.6485268305007033]
	TIME [epoch: 8.38 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9046226451674027		[learning rate: 0.0066928]
		[batch 20/20] avg loss: 2.207319204127144		[learning rate: 0.0066806]
	Learning Rate: 0.0066806
	LOSS [training: 2.0559709246472737 | validation: 3.0773197461767277]
	TIME [epoch: 8.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7919345000730584		[learning rate: 0.0066685]
		[batch 20/20] avg loss: 1.9067989596026016		[learning rate: 0.0066564]
	Learning Rate: 0.00665636
	LOSS [training: 1.8493667298378305 | validation: 2.786721313272008]
	TIME [epoch: 8.38 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8022674183034835		[learning rate: 0.0066443]
		[batch 20/20] avg loss: 1.8184793598874829		[learning rate: 0.0066322]
	Learning Rate: 0.0066322
	LOSS [training: 1.8103733890954827 | validation: 2.7231412033961186]
	TIME [epoch: 8.34 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.844606187131408		[learning rate: 0.0066202]
		[batch 20/20] avg loss: 1.82030182695619		[learning rate: 0.0066081]
	Learning Rate: 0.00660814
	LOSS [training: 1.8324540070437987 | validation: 2.696356497683002]
	TIME [epoch: 8.38 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7706586199081726		[learning rate: 0.0065961]
		[batch 20/20] avg loss: 1.8893676442288878		[learning rate: 0.0065842]
	Learning Rate: 0.00658415
	LOSS [training: 1.8300131320685302 | validation: 2.628922523088426]
	TIME [epoch: 8.37 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7859868432358135		[learning rate: 0.0065722]
		[batch 20/20] avg loss: 1.8268753783976444		[learning rate: 0.0065603]
	Learning Rate: 0.00656026
	LOSS [training: 1.8064311108167288 | validation: 2.619659929044137]
	TIME [epoch: 8.37 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6624892274886247		[learning rate: 0.0065483]
		[batch 20/20] avg loss: 1.8688800081053625		[learning rate: 0.0065365]
	Learning Rate: 0.00653645
	LOSS [training: 1.7656846177969938 | validation: 2.629092850105634]
	TIME [epoch: 8.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.879414369488833		[learning rate: 0.0065246]
		[batch 20/20] avg loss: 1.906946791275883		[learning rate: 0.0065127]
	Learning Rate: 0.00651273
	LOSS [training: 1.8931805803823583 | validation: 2.789671866692514]
	TIME [epoch: 8.38 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9863447165629602		[learning rate: 0.0065009]
		[batch 20/20] avg loss: 1.9421381955108565		[learning rate: 0.0064891]
	Learning Rate: 0.0064891
	LOSS [training: 1.9642414560369084 | validation: 2.6915604503294936]
	TIME [epoch: 8.37 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.699947967530801		[learning rate: 0.0064773]
		[batch 20/20] avg loss: 1.7868865016485778		[learning rate: 0.0064655]
	Learning Rate: 0.00646555
	LOSS [training: 1.7434172345896894 | validation: 2.9374367244610444]
	TIME [epoch: 8.37 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8396485387874777		[learning rate: 0.0064538]
		[batch 20/20] avg loss: 1.63782872725607		[learning rate: 0.0064421]
	Learning Rate: 0.00644208
	LOSS [training: 1.738738633021774 | validation: 2.672421033074888]
	TIME [epoch: 8.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7320873913691295		[learning rate: 0.0064304]
		[batch 20/20] avg loss: 1.7512553827576238		[learning rate: 0.0064187]
	Learning Rate: 0.0064187
	LOSS [training: 1.7416713870633767 | validation: 2.6008517625125447]
	TIME [epoch: 8.38 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7742212619700308		[learning rate: 0.006407]
		[batch 20/20] avg loss: 1.5754042384703018		[learning rate: 0.0063954]
	Learning Rate: 0.00639541
	LOSS [training: 1.6748127502201666 | validation: 2.6798622095850364]
	TIME [epoch: 8.38 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.545193509443476		[learning rate: 0.0063838]
		[batch 20/20] avg loss: 1.9044148218364583		[learning rate: 0.0063722]
	Learning Rate: 0.0063722
	LOSS [training: 1.724804165639967 | validation: 2.747593294772603]
	TIME [epoch: 8.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.785226518672022		[learning rate: 0.0063606]
		[batch 20/20] avg loss: 1.768057338855637		[learning rate: 0.0063491]
	Learning Rate: 0.00634908
	LOSS [training: 1.7766419287638293 | validation: 2.6673996034119]
	TIME [epoch: 8.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7030496290466406		[learning rate: 0.0063375]
		[batch 20/20] avg loss: 1.8013362339635788		[learning rate: 0.006326]
	Learning Rate: 0.00632603
	LOSS [training: 1.7521929315051097 | validation: 2.7389801345990943]
	TIME [epoch: 8.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6464951435531994		[learning rate: 0.0063145]
		[batch 20/20] avg loss: 1.733305725378154		[learning rate: 0.0063031]
	Learning Rate: 0.00630308
	LOSS [training: 1.6899004344656767 | validation: 2.558962906139513]
	TIME [epoch: 8.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8559709142304495		[learning rate: 0.0062916]
		[batch 20/20] avg loss: 1.458189506142162		[learning rate: 0.0062802]
	Learning Rate: 0.0062802
	LOSS [training: 1.6570802101863058 | validation: 2.604248978807216]
	TIME [epoch: 8.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.798083764459033		[learning rate: 0.0062688]
		[batch 20/20] avg loss: 1.6921305650631904		[learning rate: 0.0062574]
	Learning Rate: 0.00625741
	LOSS [training: 1.7451071647611116 | validation: 2.786122253487299]
	TIME [epoch: 8.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.916549735652215		[learning rate: 0.006246]
		[batch 20/20] avg loss: 1.7659118434516068		[learning rate: 0.0062347]
	Learning Rate: 0.0062347
	LOSS [training: 1.8412307895519107 | validation: 2.892423449928859]
	TIME [epoch: 8.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6664433239150607		[learning rate: 0.0062234]
		[batch 20/20] avg loss: 1.753188565801191		[learning rate: 0.0062121]
	Learning Rate: 0.00621208
	LOSS [training: 1.7098159448581256 | validation: 2.7229226919741314]
	TIME [epoch: 8.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7731581713344826		[learning rate: 0.0062008]
		[batch 20/20] avg loss: 1.5699227740174493		[learning rate: 0.0061895]
	Learning Rate: 0.00618953
	LOSS [training: 1.6715404726759657 | validation: 2.6358737695647982]
	TIME [epoch: 8.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6174207411856394		[learning rate: 0.0061783]
		[batch 20/20] avg loss: 1.7128476991634305		[learning rate: 0.0061671]
	Learning Rate: 0.00616707
	LOSS [training: 1.6651342201745352 | validation: 2.8665542725447084]
	TIME [epoch: 8.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6700615939305248		[learning rate: 0.0061559]
		[batch 20/20] avg loss: 1.6897729378187116		[learning rate: 0.0061447]
	Learning Rate: 0.00614469
	LOSS [training: 1.679917265874618 | validation: 2.6057075599737978]
	TIME [epoch: 8.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6830748471948371		[learning rate: 0.0061335]
		[batch 20/20] avg loss: 1.6678601585147892		[learning rate: 0.0061224]
	Learning Rate: 0.00612239
	LOSS [training: 1.6754675028548127 | validation: 2.7670026825279344]
	TIME [epoch: 8.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.75836547283255		[learning rate: 0.0061113]
		[batch 20/20] avg loss: 1.5983731648924198		[learning rate: 0.0061002]
	Learning Rate: 0.00610017
	LOSS [training: 1.6783693188624849 | validation: 2.558140982639634]
	TIME [epoch: 8.34 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7453349565853533		[learning rate: 0.0060891]
		[batch 20/20] avg loss: 1.9296712409091072		[learning rate: 0.006078]
	Learning Rate: 0.00607803
	LOSS [training: 1.83750309874723 | validation: 2.6248711464273766]
	TIME [epoch: 8.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7131906895699678		[learning rate: 0.006067]
		[batch 20/20] avg loss: 1.837641978917085		[learning rate: 0.006056]
	Learning Rate: 0.00605598
	LOSS [training: 1.7754163342435263 | validation: 2.826121993603369]
	TIME [epoch: 8.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.084299715923549		[learning rate: 0.006045]
		[batch 20/20] avg loss: 1.6754671758801059		[learning rate: 0.006034]
	Learning Rate: 0.006034
	LOSS [training: 1.8798834459018277 | validation: 2.595149049329366]
	TIME [epoch: 8.35 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6805804743087265		[learning rate: 0.006023]
		[batch 20/20] avg loss: 1.6263867757473416		[learning rate: 0.0060121]
	Learning Rate: 0.0060121
	LOSS [training: 1.6534836250280347 | validation: 2.629203588088394]
	TIME [epoch: 8.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5658288975317247		[learning rate: 0.0060012]
		[batch 20/20] avg loss: 1.8216455760830452		[learning rate: 0.0059903]
	Learning Rate: 0.00599028
	LOSS [training: 1.6937372368073853 | validation: 2.660349360653875]
	TIME [epoch: 8.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8111547449507313		[learning rate: 0.0059794]
		[batch 20/20] avg loss: 1.6124274566379913		[learning rate: 0.0059685]
	Learning Rate: 0.00596854
	LOSS [training: 1.7117911007943611 | validation: 2.5758498330137303]
	TIME [epoch: 8.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7373435902028411		[learning rate: 0.0059577]
		[batch 20/20] avg loss: 1.6508010613140485		[learning rate: 0.0059469]
	Learning Rate: 0.00594688
	LOSS [training: 1.6940723257584445 | validation: 2.5865135822500753]
	TIME [epoch: 8.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.713578633440563		[learning rate: 0.0059361]
		[batch 20/20] avg loss: 1.6792943108823426		[learning rate: 0.0059253]
	Learning Rate: 0.0059253
	LOSS [training: 1.6964364721614527 | validation: 2.6008884935537857]
	TIME [epoch: 8.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6820268884753173		[learning rate: 0.0059145]
		[batch 20/20] avg loss: 1.751263366337739		[learning rate: 0.0059038]
	Learning Rate: 0.0059038
	LOSS [training: 1.7166451274065284 | validation: 2.5854499092064724]
	TIME [epoch: 8.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7861125598220156		[learning rate: 0.0058931]
		[batch 20/20] avg loss: 1.615711610857936		[learning rate: 0.0058824]
	Learning Rate: 0.00588237
	LOSS [training: 1.7009120853399757 | validation: 2.6164961194172216]
	TIME [epoch: 8.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7621244765491029		[learning rate: 0.0058717]
		[batch 20/20] avg loss: 1.6449875901692366		[learning rate: 0.005861]
	Learning Rate: 0.00586103
	LOSS [training: 1.7035560333591697 | validation: 2.578239949950861]
	TIME [epoch: 8.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6229756688831949		[learning rate: 0.0058504]
		[batch 20/20] avg loss: 1.7801643691663322		[learning rate: 0.0058398]
	Learning Rate: 0.00583976
	LOSS [training: 1.7015700190247631 | validation: 2.6571824771302013]
	TIME [epoch: 8.34 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8861093549343166		[learning rate: 0.0058291]
		[batch 20/20] avg loss: 1.5900717340142116		[learning rate: 0.0058186]
	Learning Rate: 0.00581856
	LOSS [training: 1.738090544474264 | validation: 2.6098551201494633]
	TIME [epoch: 8.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8608517315898905		[learning rate: 0.005808]
		[batch 20/20] avg loss: 1.7932157501706694		[learning rate: 0.0057974]
	Learning Rate: 0.00579745
	LOSS [training: 1.8270337408802804 | validation: 2.6264589049028952]
	TIME [epoch: 8.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.666304479061371		[learning rate: 0.0057869]
		[batch 20/20] avg loss: 1.71110135987822		[learning rate: 0.0057764]
	Learning Rate: 0.00577641
	LOSS [training: 1.6887029194697956 | validation: 2.6196948828123583]
	TIME [epoch: 8.35 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7057418044326105		[learning rate: 0.0057659]
		[batch 20/20] avg loss: 1.609963827729181		[learning rate: 0.0057554]
	Learning Rate: 0.00575545
	LOSS [training: 1.6578528160808959 | validation: 2.6911708800022334]
	TIME [epoch: 8.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6642516643629879		[learning rate: 0.005745]
		[batch 20/20] avg loss: 2.039128823024407		[learning rate: 0.0057346]
	Learning Rate: 0.00573456
	LOSS [training: 1.8516902436936977 | validation: 3.212436352932571]
	TIME [epoch: 8.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7721626862370332		[learning rate: 0.0057241]
		[batch 20/20] avg loss: 1.5946870645899305		[learning rate: 0.0057137]
	Learning Rate: 0.00571375
	LOSS [training: 1.683424875413482 | validation: 2.5655450513183493]
	TIME [epoch: 8.35 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8534931857097354		[learning rate: 0.0057034]
		[batch 20/20] avg loss: 1.6712127020815324		[learning rate: 0.005693]
	Learning Rate: 0.00569301
	LOSS [training: 1.762352943895634 | validation: 2.550554730654903]
	TIME [epoch: 8.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6146631348367984		[learning rate: 0.0056827]
		[batch 20/20] avg loss: 1.7745882613781538		[learning rate: 0.0056724]
	Learning Rate: 0.00567235
	LOSS [training: 1.6946256981074757 | validation: 2.7353237061647806]
	TIME [epoch: 8.37 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6632395445318302		[learning rate: 0.005662]
		[batch 20/20] avg loss: 1.6197410182323246		[learning rate: 0.0056518]
	Learning Rate: 0.00565177
	LOSS [training: 1.6414902813820778 | validation: 2.6142293639998857]
	TIME [epoch: 8.37 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.828359941025937		[learning rate: 0.0056415]
		[batch 20/20] avg loss: 1.7269548797246217		[learning rate: 0.0056313]
	Learning Rate: 0.00563126
	LOSS [training: 1.7776574103752796 | validation: 2.663071656140633]
	TIME [epoch: 8.35 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7342361586590829		[learning rate: 0.005621]
		[batch 20/20] avg loss: 1.7517293470638542		[learning rate: 0.0056108]
	Learning Rate: 0.00561082
	LOSS [training: 1.7429827528614688 | validation: 2.7370322789698482]
	TIME [epoch: 8.34 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8127156637760353		[learning rate: 0.0056006]
		[batch 20/20] avg loss: 1.6841055174930883		[learning rate: 0.0055905]
	Learning Rate: 0.00559046
	LOSS [training: 1.748410590634562 | validation: 2.6997794166302946]
	TIME [epoch: 8.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7164983537764016		[learning rate: 0.0055803]
		[batch 20/20] avg loss: 1.7378763949061966		[learning rate: 0.0055702]
	Learning Rate: 0.00557017
	LOSS [training: 1.7271873743412989 | validation: 2.876542020096541]
	TIME [epoch: 8.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7144801185137453		[learning rate: 0.0055601]
		[batch 20/20] avg loss: 1.6457084242955191		[learning rate: 0.00555]
	Learning Rate: 0.00554996
	LOSS [training: 1.680094271404632 | validation: 2.6873967770454503]
	TIME [epoch: 8.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.683211083373271		[learning rate: 0.0055399]
		[batch 20/20] avg loss: 1.7448692552504057		[learning rate: 0.0055298]
	Learning Rate: 0.00552981
	LOSS [training: 1.7140401693118381 | validation: 2.8166972774919623]
	TIME [epoch: 8.34 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5226997392963868		[learning rate: 0.0055198]
		[batch 20/20] avg loss: 1.8078874836236047		[learning rate: 0.0055097]
	Learning Rate: 0.00550975
	LOSS [training: 1.6652936114599957 | validation: 2.619041653673232]
	TIME [epoch: 8.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8818413581302476		[learning rate: 0.0054997]
		[batch 20/20] avg loss: 1.7099779276377955		[learning rate: 0.0054898]
	Learning Rate: 0.00548975
	LOSS [training: 1.7959096428840216 | validation: 2.6246057618284997]
	TIME [epoch: 8.37 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.762845576515332		[learning rate: 0.0054798]
		[batch 20/20] avg loss: 1.7475709492574267		[learning rate: 0.0054698]
	Learning Rate: 0.00546983
	LOSS [training: 1.755208262886379 | validation: 2.547006398944064]
	TIME [epoch: 8.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7598002218937654		[learning rate: 0.0054599]
		[batch 20/20] avg loss: 1.8594590093564023		[learning rate: 0.00545]
	Learning Rate: 0.00544998
	LOSS [training: 1.809629615625084 | validation: 2.7452303884985745]
	TIME [epoch: 8.32 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8361290525322782		[learning rate: 0.0054401]
		[batch 20/20] avg loss: 1.6068295757359485		[learning rate: 0.0054302]
	Learning Rate: 0.0054302
	LOSS [training: 1.7214793141341134 | validation: 2.7799000397118814]
	TIME [epoch: 8.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6815278743995592		[learning rate: 0.0054203]
		[batch 20/20] avg loss: 1.7925602733766965		[learning rate: 0.0054105]
	Learning Rate: 0.00541049
	LOSS [training: 1.737044073888128 | validation: 2.942317274493061]
	TIME [epoch: 8.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7222194713736207		[learning rate: 0.0054007]
		[batch 20/20] avg loss: 1.7107972726696672		[learning rate: 0.0053909]
	Learning Rate: 0.00539086
	LOSS [training: 1.716508372021644 | validation: 2.850619317255083]
	TIME [epoch: 8.34 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6676495109329807		[learning rate: 0.0053811]
		[batch 20/20] avg loss: 1.790793573068973		[learning rate: 0.0053713]
	Learning Rate: 0.00537129
	LOSS [training: 1.7292215420009769 | validation: 2.9789508774543783]
	TIME [epoch: 8.34 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6882168912537925		[learning rate: 0.0053615]
		[batch 20/20] avg loss: 1.7618629841029452		[learning rate: 0.0053518]
	Learning Rate: 0.0053518
	LOSS [training: 1.7250399376783687 | validation: 2.8238575364250793]
	TIME [epoch: 8.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6603855837431212		[learning rate: 0.0053421]
		[batch 20/20] avg loss: 1.9690003056941765		[learning rate: 0.0053324]
	Learning Rate: 0.00533238
	LOSS [training: 1.8146929447186486 | validation: 2.648155483007935]
	TIME [epoch: 8.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.623275782453548		[learning rate: 0.0053227]
		[batch 20/20] avg loss: 1.867211357239853		[learning rate: 0.005313]
	Learning Rate: 0.00531303
	LOSS [training: 1.7452435698467006 | validation: 3.0712656401952385]
	TIME [epoch: 8.35 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8319057283566356		[learning rate: 0.0053034]
		[batch 20/20] avg loss: 1.8094364621032937		[learning rate: 0.0052937]
	Learning Rate: 0.00529375
	LOSS [training: 1.8206710952299652 | validation: 2.742869690909425]
	TIME [epoch: 8.35 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.614248914174894		[learning rate: 0.0052841]
		[batch 20/20] avg loss: 1.6709840422240738		[learning rate: 0.0052745]
	Learning Rate: 0.00527454
	LOSS [training: 1.6426164781994839 | validation: 2.570589433511495]
	TIME [epoch: 8.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8207750461328864		[learning rate: 0.005265]
		[batch 20/20] avg loss: 1.8518461296483166		[learning rate: 0.0052554]
	Learning Rate: 0.00525539
	LOSS [training: 1.8363105878906016 | validation: 2.7041233938610727]
	TIME [epoch: 8.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7656247010893318		[learning rate: 0.0052458]
		[batch 20/20] avg loss: 1.7382506678322496		[learning rate: 0.0052363]
	Learning Rate: 0.00523632
	LOSS [training: 1.7519376844607908 | validation: 2.6390166924969907]
	TIME [epoch: 8.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7411581999936128		[learning rate: 0.0052268]
		[batch 20/20] avg loss: 1.7691551322411665		[learning rate: 0.0052173]
	Learning Rate: 0.00521732
	LOSS [training: 1.7551566661173896 | validation: 2.6177410994064902]
	TIME [epoch: 8.34 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.725923168236704		[learning rate: 0.0052078]
		[batch 20/20] avg loss: 1.7541115441408135		[learning rate: 0.0051984]
	Learning Rate: 0.00519839
	LOSS [training: 1.7400173561887584 | validation: 2.6019155512375716]
	TIME [epoch: 8.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7522947986569448		[learning rate: 0.0051889]
		[batch 20/20] avg loss: 1.6391528915863784		[learning rate: 0.0051795]
	Learning Rate: 0.00517952
	LOSS [training: 1.695723845121662 | validation: 2.5969723371920583]
	TIME [epoch: 8.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7400995071027743		[learning rate: 0.0051701]
		[batch 20/20] avg loss: 1.7235609928848752		[learning rate: 0.0051607]
	Learning Rate: 0.00516072
	LOSS [training: 1.7318302499938247 | validation: 2.6418957391868356]
	TIME [epoch: 8.34 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5163436172351623		[learning rate: 0.0051513]
		[batch 20/20] avg loss: 1.927064200484801		[learning rate: 0.005142]
	Learning Rate: 0.00514199
	LOSS [training: 1.7217039088599815 | validation: 2.91477245008625]
	TIME [epoch: 8.34 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7896228271704857		[learning rate: 0.0051327]
		[batch 20/20] avg loss: 1.5832630861707906		[learning rate: 0.0051233]
	Learning Rate: 0.00512333
	LOSS [training: 1.686442956670638 | validation: 2.660512250060751]
	TIME [epoch: 8.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5857030789158202		[learning rate: 0.005114]
		[batch 20/20] avg loss: 1.7661126989458218		[learning rate: 0.0051047]
	Learning Rate: 0.00510474
	LOSS [training: 1.675907888930821 | validation: 2.6634851885217294]
	TIME [epoch: 8.35 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7427422949336147		[learning rate: 0.0050955]
		[batch 20/20] avg loss: 1.6716349550781158		[learning rate: 0.0050862]
	Learning Rate: 0.00508622
	LOSS [training: 1.707188625005865 | validation: 2.616882696931261]
	TIME [epoch: 8.34 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7118313119445823		[learning rate: 0.005077]
		[batch 20/20] avg loss: 1.5889756312006285		[learning rate: 0.0050678]
	Learning Rate: 0.00506776
	LOSS [training: 1.6504034715726053 | validation: 2.602184683028347]
	TIME [epoch: 8.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4960594203246869		[learning rate: 0.0050586]
		[batch 20/20] avg loss: 1.8625837479947294		[learning rate: 0.0050494]
	Learning Rate: 0.00504937
	LOSS [training: 1.6793215841597082 | validation: 2.5877625363897048]
	TIME [epoch: 8.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6259336664740627		[learning rate: 0.0050402]
		[batch 20/20] avg loss: 1.7529448642883068		[learning rate: 0.005031]
	Learning Rate: 0.00503104
	LOSS [training: 1.689439265381185 | validation: 2.7402329693884386]
	TIME [epoch: 8.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6538511309944526		[learning rate: 0.0050219]
		[batch 20/20] avg loss: 1.6534653977613136		[learning rate: 0.0050128]
	Learning Rate: 0.00501278
	LOSS [training: 1.6536582643778828 | validation: 2.6594963083349676]
	TIME [epoch: 8.34 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.699284993734865		[learning rate: 0.0050037]
		[batch 20/20] avg loss: 1.6246021458112103		[learning rate: 0.0049946]
	Learning Rate: 0.00499459
	LOSS [training: 1.6619435697730374 | validation: 2.687908046816858]
	TIME [epoch: 8.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6626432108809546		[learning rate: 0.0049855]
		[batch 20/20] avg loss: 1.6366748982410588		[learning rate: 0.0049765]
	Learning Rate: 0.00497647
	LOSS [training: 1.6496590545610066 | validation: 2.557507794916687]
	TIME [epoch: 8.35 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5805519554701255		[learning rate: 0.0049674]
		[batch 20/20] avg loss: 1.6289740809136177		[learning rate: 0.0049584]
	Learning Rate: 0.00495841
	LOSS [training: 1.6047630181918713 | validation: 2.539914827562616]
	TIME [epoch: 8.37 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7384216665653067		[learning rate: 0.0049494]
		[batch 20/20] avg loss: 1.530783900459743		[learning rate: 0.0049404]
	Learning Rate: 0.00494041
	LOSS [training: 1.6346027835125252 | validation: 2.5851248467233625]
	TIME [epoch: 8.34 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6144190355814707		[learning rate: 0.0049314]
		[batch 20/20] avg loss: 1.7070881481068383		[learning rate: 0.0049225]
	Learning Rate: 0.00492248
	LOSS [training: 1.6607535918441545 | validation: 2.6064167433278875]
	TIME [epoch: 8.37 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6821611605876972		[learning rate: 0.0049135]
		[batch 20/20] avg loss: 1.6704987474665884		[learning rate: 0.0049046]
	Learning Rate: 0.00490462
	LOSS [training: 1.6763299540271426 | validation: 2.588637816636882]
	TIME [epoch: 8.33 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7549763541979981		[learning rate: 0.0048957]
		[batch 20/20] avg loss: 1.5475346211761158		[learning rate: 0.0048868]
	Learning Rate: 0.00488682
	LOSS [training: 1.6512554876870567 | validation: 2.546360983817382]
	TIME [epoch: 8.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6124246436798475		[learning rate: 0.0048779]
		[batch 20/20] avg loss: 1.6752388958041315		[learning rate: 0.0048691]
	Learning Rate: 0.00486909
	LOSS [training: 1.6438317697419893 | validation: 2.814083550892986]
	TIME [epoch: 8.34 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.587120631126011		[learning rate: 0.0048602]
		[batch 20/20] avg loss: 1.8671244379338294		[learning rate: 0.0048514]
	Learning Rate: 0.00485141
	LOSS [training: 1.72712253452992 | validation: 2.515357398408417]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.576422230934321		[learning rate: 0.0048426]
		[batch 20/20] avg loss: 1.690844412617897		[learning rate: 0.0048338]
	Learning Rate: 0.00483381
	LOSS [training: 1.633633321776109 | validation: 2.6691150745567698]
	TIME [epoch: 8.33 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4627222639992592		[learning rate: 0.004825]
		[batch 20/20] avg loss: 1.8208045357225735		[learning rate: 0.0048163]
	Learning Rate: 0.00481627
	LOSS [training: 1.6417633998609165 | validation: 2.552604685690519]
	TIME [epoch: 8.37 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.450791006809968		[learning rate: 0.0048075]
		[batch 20/20] avg loss: 1.6778086398544463		[learning rate: 0.0047988]
	Learning Rate: 0.00479879
	LOSS [training: 1.564299823332207 | validation: 2.5753183519554907]
	TIME [epoch: 8.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.69717986722112		[learning rate: 0.0047901]
		[batch 20/20] avg loss: 1.5126789433046108		[learning rate: 0.0047814]
	Learning Rate: 0.00478137
	LOSS [training: 1.6049294052628653 | validation: 2.65860761133941]
	TIME [epoch: 8.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6461953956747017		[learning rate: 0.0047727]
		[batch 20/20] avg loss: 1.5699489664680701		[learning rate: 0.004764]
	Learning Rate: 0.00476402
	LOSS [training: 1.6080721810713858 | validation: 2.577168387891399]
	TIME [epoch: 8.33 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.604582394310145		[learning rate: 0.0047554]
		[batch 20/20] avg loss: 1.6510513495852615		[learning rate: 0.0047467]
	Learning Rate: 0.00474673
	LOSS [training: 1.6278168719477033 | validation: 2.740267689618566]
	TIME [epoch: 8.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6066375321781308		[learning rate: 0.0047381]
		[batch 20/20] avg loss: 1.6178464823364038		[learning rate: 0.0047295]
	Learning Rate: 0.00472951
	LOSS [training: 1.6122420072572674 | validation: 2.5437075292625413]
	TIME [epoch: 8.35 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6060493636851028		[learning rate: 0.0047209]
		[batch 20/20] avg loss: 1.754676731757007		[learning rate: 0.0047123]
	Learning Rate: 0.00471234
	LOSS [training: 1.680363047721055 | validation: 2.538651838251468]
	TIME [epoch: 8.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5170440782208474		[learning rate: 0.0047038]
		[batch 20/20] avg loss: 1.7153739530428276		[learning rate: 0.0046952]
	Learning Rate: 0.00469524
	LOSS [training: 1.6162090156318374 | validation: 2.507201747228429]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7983227298340452		[learning rate: 0.0046867]
		[batch 20/20] avg loss: 1.4266884782199079		[learning rate: 0.0046782]
	Learning Rate: 0.0046782
	LOSS [training: 1.612505604026976 | validation: 2.569228348115068]
	TIME [epoch: 8.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6803507681254874		[learning rate: 0.0046697]
		[batch 20/20] avg loss: 1.5443960565211863		[learning rate: 0.0046612]
	Learning Rate: 0.00466122
	LOSS [training: 1.6123734123233369 | validation: 2.58326155356043]
	TIME [epoch: 8.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5933842220914571		[learning rate: 0.0046528]
		[batch 20/20] avg loss: 1.6331996209540591		[learning rate: 0.0046443]
	Learning Rate: 0.00464431
	LOSS [training: 1.6132919215227581 | validation: 2.504919756993841]
	TIME [epoch: 8.37 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.60479428832097		[learning rate: 0.0046359]
		[batch 20/20] avg loss: 1.6366808846856884		[learning rate: 0.0046275]
	Learning Rate: 0.00462745
	LOSS [training: 1.620737586503329 | validation: 2.5512754184981077]
	TIME [epoch: 8.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6112784987700401		[learning rate: 0.004619]
		[batch 20/20] avg loss: 1.6162701308747167		[learning rate: 0.0046107]
	Learning Rate: 0.00461066
	LOSS [training: 1.6137743148223787 | validation: 2.6090487351694627]
	TIME [epoch: 8.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6078978150134577		[learning rate: 0.0046023]
		[batch 20/20] avg loss: 1.674035527487811		[learning rate: 0.0045939]
	Learning Rate: 0.00459393
	LOSS [training: 1.6409666712506343 | validation: 2.6763464517991746]
	TIME [epoch: 8.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.566264803632622		[learning rate: 0.0045856]
		[batch 20/20] avg loss: 1.6531238765825207		[learning rate: 0.0045773]
	Learning Rate: 0.00457726
	LOSS [training: 1.6096943401075712 | validation: 2.5686648685854205]
	TIME [epoch: 8.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.608009575885319		[learning rate: 0.0045689]
		[batch 20/20] avg loss: 1.712317053336235		[learning rate: 0.0045606]
	Learning Rate: 0.00456065
	LOSS [training: 1.6601633146107768 | validation: 2.552573989140027]
	TIME [epoch: 8.35 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6100731503448742		[learning rate: 0.0045524]
		[batch 20/20] avg loss: 1.6169465148219302		[learning rate: 0.0045441]
	Learning Rate: 0.00454409
	LOSS [training: 1.6135098325834023 | validation: 2.5437422961434275]
	TIME [epoch: 8.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6546967906816352		[learning rate: 0.0045358]
		[batch 20/20] avg loss: 1.7659191511737344		[learning rate: 0.0045276]
	Learning Rate: 0.0045276
	LOSS [training: 1.7103079709276845 | validation: 2.744108694037181]
	TIME [epoch: 8.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.665248753197794		[learning rate: 0.0045194]
		[batch 20/20] avg loss: 1.7270266028942658		[learning rate: 0.0045112]
	Learning Rate: 0.00451117
	LOSS [training: 1.6961376780460298 | validation: 2.5287661592937347]
	TIME [epoch: 8.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4985531571044222		[learning rate: 0.004503]
		[batch 20/20] avg loss: 1.7537592915573037		[learning rate: 0.0044948]
	Learning Rate: 0.0044948
	LOSS [training: 1.6261562243308627 | validation: 2.535110875806364]
	TIME [epoch: 8.35 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6066083918103975		[learning rate: 0.0044866]
		[batch 20/20] avg loss: 1.5985944651813397		[learning rate: 0.0044785]
	Learning Rate: 0.00447849
	LOSS [training: 1.6026014284958687 | validation: 2.5264379645716417]
	TIME [epoch: 8.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.644707216535116		[learning rate: 0.0044704]
		[batch 20/20] avg loss: 1.5250175399920398		[learning rate: 0.0044622]
	Learning Rate: 0.00446224
	LOSS [training: 1.5848623782635776 | validation: 2.5024463283811853]
	TIME [epoch: 8.38 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.654718997251845		[learning rate: 0.0044541]
		[batch 20/20] avg loss: 1.6156318358076518		[learning rate: 0.004446]
	Learning Rate: 0.00444604
	LOSS [training: 1.6351754165297485 | validation: 2.5024562033821605]
	TIME [epoch: 8.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6295116778776861		[learning rate: 0.004438]
		[batch 20/20] avg loss: 1.6105713774424935		[learning rate: 0.0044299]
	Learning Rate: 0.00442991
	LOSS [training: 1.62004152766009 | validation: 2.5402418303735335]
	TIME [epoch: 8.35 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7066781169245342		[learning rate: 0.0044219]
		[batch 20/20] avg loss: 1.6547969457260507		[learning rate: 0.0044138]
	Learning Rate: 0.00441383
	LOSS [training: 1.6807375313252924 | validation: 2.748302142836536]
	TIME [epoch: 8.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5325814562537223		[learning rate: 0.0044058]
		[batch 20/20] avg loss: 1.6757186795037442		[learning rate: 0.0043978]
	Learning Rate: 0.00439781
	LOSS [training: 1.6041500678787333 | validation: 2.557828723364742]
	TIME [epoch: 8.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6682388001178794		[learning rate: 0.0043898]
		[batch 20/20] avg loss: 1.5679853737708755		[learning rate: 0.0043819]
	Learning Rate: 0.00438185
	LOSS [training: 1.6181120869443775 | validation: 2.83712976097117]
	TIME [epoch: 8.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.584760411094792		[learning rate: 0.0043739]
		[batch 20/20] avg loss: 1.6962740485518526		[learning rate: 0.004366]
	Learning Rate: 0.00436595
	LOSS [training: 1.6405172298233226 | validation: 2.5950633127585245]
	TIME [epoch: 8.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.643323870302104		[learning rate: 0.004358]
		[batch 20/20] avg loss: 1.5627356760340516		[learning rate: 0.0043501]
	Learning Rate: 0.00435011
	LOSS [training: 1.6030297731680778 | validation: 2.70537628084105]
	TIME [epoch: 8.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6152194244714901		[learning rate: 0.0043422]
		[batch 20/20] avg loss: 1.5982040715126897		[learning rate: 0.0043343]
	Learning Rate: 0.00433432
	LOSS [training: 1.6067117479920903 | validation: 2.6275184365515187]
	TIME [epoch: 8.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5895095310541765		[learning rate: 0.0043264]
		[batch 20/20] avg loss: 1.6779469714184387		[learning rate: 0.0043186]
	Learning Rate: 0.00431859
	LOSS [training: 1.6337282512363078 | validation: 2.507843995690905]
	TIME [epoch: 8.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.4772733266298945		[learning rate: 0.0043107]
		[batch 20/20] avg loss: 1.7490948190552777		[learning rate: 0.0043029]
	Learning Rate: 0.00430292
	LOSS [training: 1.6131840728425864 | validation: 2.609444203425066]
	TIME [epoch: 8.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5347023934699366		[learning rate: 0.0042951]
		[batch 20/20] avg loss: 1.7449737801204155		[learning rate: 0.0042873]
	Learning Rate: 0.0042873
	LOSS [training: 1.6398380867951758 | validation: 2.610274425823163]
	TIME [epoch: 8.42 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.608490541363766		[learning rate: 0.0042795]
		[batch 20/20] avg loss: 1.6188678317085476		[learning rate: 0.0042717]
	Learning Rate: 0.00427174
	LOSS [training: 1.6136791865361566 | validation: 2.76223991517067]
	TIME [epoch: 8.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.741384444903343		[learning rate: 0.004264]
		[batch 20/20] avg loss: 1.5309323866088826		[learning rate: 0.0042562]
	Learning Rate: 0.00425624
	LOSS [training: 1.6361584157561129 | validation: 2.7526393905671847]
	TIME [epoch: 8.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6819236265425104		[learning rate: 0.0042485]
		[batch 20/20] avg loss: 1.5910023213447215		[learning rate: 0.0042408]
	Learning Rate: 0.0042408
	LOSS [training: 1.636462973943616 | validation: 2.6387401712271923]
	TIME [epoch: 8.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6288162771308812		[learning rate: 0.0042331]
		[batch 20/20] avg loss: 1.666969879438689		[learning rate: 0.0042254]
	Learning Rate: 0.00422541
	LOSS [training: 1.6478930782847852 | validation: 2.6289853895137427]
	TIME [epoch: 8.42 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5282000341723287		[learning rate: 0.0042177]
		[batch 20/20] avg loss: 1.6974624630004058		[learning rate: 0.0042101]
	Learning Rate: 0.00421007
	LOSS [training: 1.6128312485863678 | validation: 2.5323414587191673]
	TIME [epoch: 8.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7162170097845482		[learning rate: 0.0042024]
		[batch 20/20] avg loss: 1.5832139197037924		[learning rate: 0.0041948]
	Learning Rate: 0.00419479
	LOSS [training: 1.64971546474417 | validation: 2.5234175679156263]
	TIME [epoch: 8.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6196191976167924		[learning rate: 0.0041872]
		[batch 20/20] avg loss: 1.8592703118803033		[learning rate: 0.0041796]
	Learning Rate: 0.00417957
	LOSS [training: 1.739444754748548 | validation: 2.50982049807364]
	TIME [epoch: 8.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7886945344161234		[learning rate: 0.004172]
		[batch 20/20] avg loss: 1.8454087289647283		[learning rate: 0.0041644]
	Learning Rate: 0.0041644
	LOSS [training: 1.8170516316904255 | validation: 2.768023113127808]
	TIME [epoch: 8.42 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5543514266885354		[learning rate: 0.0041568]
		[batch 20/20] avg loss: 1.8618225090085532		[learning rate: 0.0041493]
	Learning Rate: 0.00414929
	LOSS [training: 1.7080869678485446 | validation: 3.2434248899388725]
	TIME [epoch: 8.35 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7033892846874508		[learning rate: 0.0041418]
		[batch 20/20] avg loss: 1.8358294867888465		[learning rate: 0.0041342]
	Learning Rate: 0.00413423
	LOSS [training: 1.7696093857381485 | validation: 2.583689524040015]
	TIME [epoch: 8.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8345356282069667		[learning rate: 0.0041267]
		[batch 20/20] avg loss: 1.6760710264029464		[learning rate: 0.0041192]
	Learning Rate: 0.00411923
	LOSS [training: 1.755303327304956 | validation: 2.729081764008356]
	TIME [epoch: 8.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8823511473184134		[learning rate: 0.0041117]
		[batch 20/20] avg loss: 1.556253708768007		[learning rate: 0.0041043]
	Learning Rate: 0.00410428
	LOSS [training: 1.7193024280432105 | validation: 2.544655077420672]
	TIME [epoch: 8.42 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7534136346366829		[learning rate: 0.0040968]
		[batch 20/20] avg loss: 1.6280275501271557		[learning rate: 0.0040894]
	Learning Rate: 0.00408938
	LOSS [training: 1.6907205923819197 | validation: 2.8220511305430778]
	TIME [epoch: 8.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.614150739336493		[learning rate: 0.004082]
		[batch 20/20] avg loss: 1.7352140088175023		[learning rate: 0.0040745]
	Learning Rate: 0.00407454
	LOSS [training: 1.6746823740769976 | validation: 2.767289578851374]
	TIME [epoch: 8.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.834132846897838		[learning rate: 0.0040671]
		[batch 20/20] avg loss: 1.6304007824845628		[learning rate: 0.0040598]
	Learning Rate: 0.00405976
	LOSS [training: 1.7322668146911997 | validation: 2.6623067612357354]
	TIME [epoch: 8.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6770044761868348		[learning rate: 0.0040524]
		[batch 20/20] avg loss: 1.7044922770432964		[learning rate: 0.004045]
	Learning Rate: 0.00404502
	LOSS [training: 1.690748376615066 | validation: 2.592429407908978]
	TIME [epoch: 8.4 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.783968836063666		[learning rate: 0.0040377]
		[batch 20/20] avg loss: 1.5737808129374806		[learning rate: 0.0040303]
	Learning Rate: 0.00403034
	LOSS [training: 1.6788748245005736 | validation: 2.6144175523359845]
	TIME [epoch: 8.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.615478620742013		[learning rate: 0.004023]
		[batch 20/20] avg loss: 1.7672675088169316		[learning rate: 0.0040157]
	Learning Rate: 0.00401572
	LOSS [training: 1.6913730647794722 | validation: 3.0012160321132466]
	TIME [epoch: 8.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6532577934624808		[learning rate: 0.0040084]
		[batch 20/20] avg loss: 1.749158737396406		[learning rate: 0.0040011]
	Learning Rate: 0.00400114
	LOSS [training: 1.7012082654294436 | validation: 2.822225662221173]
	TIME [epoch: 8.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7102684250067555		[learning rate: 0.0039939]
		[batch 20/20] avg loss: 1.705456332376858		[learning rate: 0.0039866]
	Learning Rate: 0.00398662
	LOSS [training: 1.7078623786918066 | validation: 2.8550953763019713]
	TIME [epoch: 8.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.727035488815762		[learning rate: 0.0039794]
		[batch 20/20] avg loss: 1.6058831777757085		[learning rate: 0.0039722]
	Learning Rate: 0.00397216
	LOSS [training: 1.6664593332957356 | validation: 3.046888624156542]
	TIME [epoch: 8.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8105777040664854		[learning rate: 0.0039649]
		[batch 20/20] avg loss: 1.5989207320220404		[learning rate: 0.0039577]
	Learning Rate: 0.00395774
	LOSS [training: 1.7047492180442627 | validation: 2.76447939457469]
	TIME [epoch: 8.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7712154960751552		[learning rate: 0.0039506]
		[batch 20/20] avg loss: 1.6064969466324528		[learning rate: 0.0039434]
	Learning Rate: 0.00394338
	LOSS [training: 1.688856221353804 | validation: 2.783134490266347]
	TIME [epoch: 8.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.664580096817741		[learning rate: 0.0039362]
		[batch 20/20] avg loss: 1.7262055320951741		[learning rate: 0.0039291]
	Learning Rate: 0.00392907
	LOSS [training: 1.6953928144564578 | validation: 2.65253385775558]
	TIME [epoch: 8.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8448955917651086		[learning rate: 0.0039219]
		[batch 20/20] avg loss: 1.48931766643924		[learning rate: 0.0039148]
	Learning Rate: 0.00391481
	LOSS [training: 1.667106629102174 | validation: 2.7317634739933405]
	TIME [epoch: 8.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.642831385572358		[learning rate: 0.0039077]
		[batch 20/20] avg loss: 1.745435672554882		[learning rate: 0.0039006]
	Learning Rate: 0.0039006
	LOSS [training: 1.69413352906362 | validation: 2.5654163385007696]
	TIME [epoch: 8.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.774116989434388		[learning rate: 0.0038935]
		[batch 20/20] avg loss: 1.8781885631264932		[learning rate: 0.0038864]
	Learning Rate: 0.00388645
	LOSS [training: 1.8261527762804406 | validation: 2.5846643682922337]
	TIME [epoch: 8.39 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6433871363455819		[learning rate: 0.0038794]
		[batch 20/20] avg loss: 1.648168786955615		[learning rate: 0.0038723]
	Learning Rate: 0.00387234
	LOSS [training: 1.645777961650598 | validation: 2.595335427295188]
	TIME [epoch: 8.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8043388970710115		[learning rate: 0.0038653]
		[batch 20/20] avg loss: 1.601061621003923		[learning rate: 0.0038583]
	Learning Rate: 0.00385829
	LOSS [training: 1.7027002590374676 | validation: 2.683067589125061]
	TIME [epoch: 8.35 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6913980356194558		[learning rate: 0.0038513]
		[batch 20/20] avg loss: 1.6066162459241995		[learning rate: 0.0038443]
	Learning Rate: 0.00384429
	LOSS [training: 1.6490071407718276 | validation: 2.7320017206585088]
	TIME [epoch: 8.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6272728538821064		[learning rate: 0.0038373]
		[batch 20/20] avg loss: 1.738505413951028		[learning rate: 0.0038303]
	Learning Rate: 0.00383034
	LOSS [training: 1.6828891339165672 | validation: 2.875105914290958]
	TIME [epoch: 8.4 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5434290248117617		[learning rate: 0.0038234]
		[batch 20/20] avg loss: 1.798015784887348		[learning rate: 0.0038164]
	Learning Rate: 0.00381644
	LOSS [training: 1.670722404849555 | validation: 2.602574148759143]
	TIME [epoch: 8.37 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9179385855463287		[learning rate: 0.0038095]
		[batch 20/20] avg loss: 1.7014633646479473		[learning rate: 0.0038026]
	Learning Rate: 0.00380258
	LOSS [training: 1.809700975097138 | validation: 2.5938742144708957]
	TIME [epoch: 8.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5905953748637525		[learning rate: 0.0037957]
		[batch 20/20] avg loss: 1.6809618367635832		[learning rate: 0.0037888]
	Learning Rate: 0.00378879
	LOSS [training: 1.6357786058136679 | validation: 2.722037466998515]
	TIME [epoch: 8.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.573187933440168		[learning rate: 0.0037819]
		[batch 20/20] avg loss: 1.7322201114837015		[learning rate: 0.003775]
	Learning Rate: 0.00377504
	LOSS [training: 1.6527040224619345 | validation: 2.573234704220847]
	TIME [epoch: 8.4 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7610534372061246		[learning rate: 0.0037682]
		[batch 20/20] avg loss: 1.4895552888533303		[learning rate: 0.0037613]
	Learning Rate: 0.00376134
	LOSS [training: 1.6253043630297275 | validation: 2.6570226716724186]
	TIME [epoch: 8.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6476624696606212		[learning rate: 0.0037545]
		[batch 20/20] avg loss: 1.6279541572545664		[learning rate: 0.0037477]
	Learning Rate: 0.00374769
	LOSS [training: 1.6378083134575938 | validation: 2.6117603213611926]
	TIME [epoch: 8.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6206133212980163		[learning rate: 0.0037409]
		[batch 20/20] avg loss: 1.643994835802085		[learning rate: 0.0037341]
	Learning Rate: 0.00373408
	LOSS [training: 1.6323040785500509 | validation: 2.815754240984367]
	TIME [epoch: 8.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.66629007790286		[learning rate: 0.0037273]
		[batch 20/20] avg loss: 1.6233802623112925		[learning rate: 0.0037205]
	Learning Rate: 0.00372053
	LOSS [training: 1.644835170107076 | validation: 2.6740714182188983]
	TIME [epoch: 8.4 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7505582087271416		[learning rate: 0.0037138]
		[batch 20/20] avg loss: 1.7367347550126666		[learning rate: 0.003707]
	Learning Rate: 0.00370703
	LOSS [training: 1.743646481869904 | validation: 2.6304398248598297]
	TIME [epoch: 8.37 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.669462738427113		[learning rate: 0.0037003]
		[batch 20/20] avg loss: 1.6105129678967516		[learning rate: 0.0036936]
	Learning Rate: 0.00369358
	LOSS [training: 1.6399878531619323 | validation: 2.635989034681923]
	TIME [epoch: 8.35 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7387565436865717		[learning rate: 0.0036869]
		[batch 20/20] avg loss: 1.596632460124947		[learning rate: 0.0036802]
	Learning Rate: 0.00368017
	LOSS [training: 1.6676945019057599 | validation: 2.729807589035733]
	TIME [epoch: 8.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6874503597476636		[learning rate: 0.0036735]
		[batch 20/20] avg loss: 1.6045212511668734		[learning rate: 0.0036668]
	Learning Rate: 0.00366682
	LOSS [training: 1.6459858054572685 | validation: 2.788456545523042]
	TIME [epoch: 8.4 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6089772665306399		[learning rate: 0.0036602]
		[batch 20/20] avg loss: 1.9855150280295777		[learning rate: 0.0036535]
	Learning Rate: 0.00365351
	LOSS [training: 1.7972461472801087 | validation: 2.839024222286156]
	TIME [epoch: 8.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.64693294240671		[learning rate: 0.0036469]
		[batch 20/20] avg loss: 1.646962765009798		[learning rate: 0.0036403]
	Learning Rate: 0.00364025
	LOSS [training: 1.6469478537082538 | validation: 2.7281718985021257]
	TIME [epoch: 8.35 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7352617617620822		[learning rate: 0.0036336]
		[batch 20/20] avg loss: 1.7018950504907249		[learning rate: 0.003627]
	Learning Rate: 0.00362704
	LOSS [training: 1.7185784061264033 | validation: 2.648324240161238]
	TIME [epoch: 8.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5355514393692196		[learning rate: 0.0036205]
		[batch 20/20] avg loss: 1.7421939785034481		[learning rate: 0.0036139]
	Learning Rate: 0.00361388
	LOSS [training: 1.6388727089363342 | validation: 2.7940067966084383]
	TIME [epoch: 8.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8162334767518131		[learning rate: 0.0036073]
		[batch 20/20] avg loss: 1.6111078684878954		[learning rate: 0.0036008]
	Learning Rate: 0.00360076
	LOSS [training: 1.713670672619854 | validation: 2.5467774061950283]
	TIME [epoch: 8.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5905015863957506		[learning rate: 0.0035942]
		[batch 20/20] avg loss: 1.720104725136767		[learning rate: 0.0035877]
	Learning Rate: 0.0035877
	LOSS [training: 1.6553031557662585 | validation: 2.5377226894802374]
	TIME [epoch: 8.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6968527643651707		[learning rate: 0.0035812]
		[batch 20/20] avg loss: 1.648632595594303		[learning rate: 0.0035747]
	Learning Rate: 0.00357468
	LOSS [training: 1.6727426799797371 | validation: 2.5912957914931516]
	TIME [epoch: 8.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6021192071029993		[learning rate: 0.0035682]
		[batch 20/20] avg loss: 1.710803686196527		[learning rate: 0.0035617]
	Learning Rate: 0.0035617
	LOSS [training: 1.6564614466497634 | validation: 2.5070536399699135]
	TIME [epoch: 8.39 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7311011340508986		[learning rate: 0.0035552]
		[batch 20/20] avg loss: 1.6154151486180726		[learning rate: 0.0035488]
	Learning Rate: 0.00354878
	LOSS [training: 1.673258141334486 | validation: 2.528347080351996]
	TIME [epoch: 8.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7175277375010893		[learning rate: 0.0035423]
		[batch 20/20] avg loss: 1.5537186445541329		[learning rate: 0.0035359]
	Learning Rate: 0.0035359
	LOSS [training: 1.635623191027611 | validation: 2.512077252476245]
	TIME [epoch: 8.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.677359344901996		[learning rate: 0.0035295]
		[batch 20/20] avg loss: 1.8319551735732422		[learning rate: 0.0035231]
	Learning Rate: 0.00352307
	LOSS [training: 1.754657259237619 | validation: 2.67754757202198]
	TIME [epoch: 8.37 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7215694298644997		[learning rate: 0.0035167]
		[batch 20/20] avg loss: 1.5314699980391788		[learning rate: 0.0035103]
	Learning Rate: 0.00351028
	LOSS [training: 1.6265197139518393 | validation: 2.7187496127564628]
	TIME [epoch: 8.38 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6026408740899492		[learning rate: 0.0035039]
		[batch 20/20] avg loss: 1.6676149960918853		[learning rate: 0.0034975]
	Learning Rate: 0.00349754
	LOSS [training: 1.635127935090917 | validation: 2.6364394114239698]
	TIME [epoch: 8.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6098309197162681		[learning rate: 0.0034912]
		[batch 20/20] avg loss: 1.6118382706074883		[learning rate: 0.0034849]
	Learning Rate: 0.00348485
	LOSS [training: 1.610834595161878 | validation: 2.5545167161298887]
	TIME [epoch: 8.34 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5674339881135038		[learning rate: 0.0034785]
		[batch 20/20] avg loss: 1.870968617643316		[learning rate: 0.0034722]
	Learning Rate: 0.0034722
	LOSS [training: 1.7192013028784099 | validation: 4.054658238684305]
	TIME [epoch: 8.37 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.312374626940366		[learning rate: 0.0034659]
		[batch 20/20] avg loss: 2.0034161687202827		[learning rate: 0.0034596]
	Learning Rate: 0.0034596
	LOSS [training: 2.157895397830324 | validation: 2.919793901934632]
	TIME [epoch: 8.37 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8749754902034312		[learning rate: 0.0034533]
		[batch 20/20] avg loss: 1.6890188034493796		[learning rate: 0.003447]
	Learning Rate: 0.00344705
	LOSS [training: 1.7819971468264053 | validation: 2.5711341541827837]
	TIME [epoch: 8.34 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5978373418060738		[learning rate: 0.0034408]
		[batch 20/20] avg loss: 1.8425244988729133		[learning rate: 0.0034345]
	Learning Rate: 0.00343454
	LOSS [training: 1.7201809203394933 | validation: 2.635500095756409]
	TIME [epoch: 8.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6548695126331967		[learning rate: 0.0034283]
		[batch 20/20] avg loss: 1.649782055412445		[learning rate: 0.0034221]
	Learning Rate: 0.00342207
	LOSS [training: 1.6523257840228205 | validation: 3.4817131293341257]
	TIME [epoch: 8.37 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1733063343179677		[learning rate: 0.0034159]
		[batch 20/20] avg loss: 1.6973961287852326		[learning rate: 0.0034097]
	Learning Rate: 0.00340966
	LOSS [training: 1.9353512315516 | validation: 2.7837077877794627]
	TIME [epoch: 8.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7260576578254017		[learning rate: 0.0034035]
		[batch 20/20] avg loss: 1.5861444849989375		[learning rate: 0.0033973]
	Learning Rate: 0.00339728
	LOSS [training: 1.6561010714121693 | validation: 2.6540693113453897]
	TIME [epoch: 8.34 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5821814537104806		[learning rate: 0.0033911]
		[batch 20/20] avg loss: 1.837192862059559		[learning rate: 0.003385]
	Learning Rate: 0.00338495
	LOSS [training: 1.70968715788502 | validation: 2.62923004753758]
	TIME [epoch: 8.34 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6176976028569505		[learning rate: 0.0033788]
		[batch 20/20] avg loss: 1.7237735846755626		[learning rate: 0.0033727]
	Learning Rate: 0.00337267
	LOSS [training: 1.6707355937662567 | validation: 2.875510007072368]
	TIME [epoch: 8.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6585471289613163		[learning rate: 0.0033665]
		[batch 20/20] avg loss: 1.7108570473976326		[learning rate: 0.0033604]
	Learning Rate: 0.00336043
	LOSS [training: 1.684702088179474 | validation: 2.5685591136977948]
	TIME [epoch: 8.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7020185674934383		[learning rate: 0.0033543]
		[batch 20/20] avg loss: 1.9209524924387487		[learning rate: 0.0033482]
	Learning Rate: 0.00334823
	LOSS [training: 1.8114855299660935 | validation: 2.4919477601387796]
	TIME [epoch: 8.33 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r2_20240219_193225/states/model_tr_study202_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.7701339370087976		[learning rate: 0.0033422]
		[batch 20/20] avg loss: 1.3553452711443696		[learning rate: 0.0033361]
	Learning Rate: 0.00333608
	LOSS [training: 1.5627396040765835 | validation: 2.6366681732950825]
	TIME [epoch: 8.35 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.5846831431373478		[learning rate: 0.00333]
		[batch 20/20] avg loss: 1.566527096259529		[learning rate: 0.003324]
	Learning Rate: 0.00332398
	LOSS [training: 1.5756051196984382 | validation: 2.4999570180605355]
	TIME [epoch: 8.35 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.6214746724682942		[learning rate: 0.0033179]
		[batch 20/20] avg loss: 1.6057631458746229		[learning rate: 0.0033119]
	Learning Rate: 0.00331191
	LOSS [training: 1.6136189091714588 | validation: 2.5571097860330756]
	TIME [epoch: 8.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9619444719308319		[learning rate: 0.0033059]
		[batch 20/20] avg loss: 2.2057410732765175		[learning rate: 0.0032999]
	Learning Rate: 0.00329989
	LOSS [training: 2.0838427726036746 | validation: 2.646783034887978]
	TIME [epoch: 8.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.0503624878128535		[learning rate: 0.0032939]
		[batch 20/20] avg loss: 1.7822808144060005		[learning rate: 0.0032879]
	Learning Rate: 0.00328792
	LOSS [training: 1.9163216511094265 | validation: 2.6900556522837133]
	TIME [epoch: 8.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9145261026226006		[learning rate: 0.0032819]
		[batch 20/20] avg loss: 1.9179459042009892		[learning rate: 0.003276]
	Learning Rate: 0.00327599
	LOSS [training: 1.9162360034117945 | validation: 2.671863192222113]
	TIME [epoch: 8.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8832701361162638		[learning rate: 0.00327]
		[batch 20/20] avg loss: 1.9128417287766282		[learning rate: 0.0032641]
	Learning Rate: 0.0032641
	LOSS [training: 1.8980559324464459 | validation: 2.717065378101011]
	TIME [epoch: 8.35 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8623816171892738		[learning rate: 0.0032582]
		[batch 20/20] avg loss: 1.8883292649486223		[learning rate: 0.0032523]
	Learning Rate: 0.00325225
	LOSS [training: 1.875355441068948 | validation: 2.622086153466964]
	TIME [epoch: 8.34 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8915623100061514		[learning rate: 0.0032463]
		[batch 20/20] avg loss: 1.9208737877456428		[learning rate: 0.0032404]
	Learning Rate: 0.00324045
	LOSS [training: 1.9062180488758969 | validation: 2.5812925095599475]
	TIME [epoch: 8.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9259099541118991		[learning rate: 0.0032346]
		[batch 20/20] avg loss: 1.8614044403074033		[learning rate: 0.0032287]
	Learning Rate: 0.00322869
	LOSS [training: 1.8936571972096516 | validation: 2.6017063878200166]
	TIME [epoch: 8.34 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 10/20] avg loss: 2.1173028084411936		[learning rate: 0.0032228]
		[batch 20/20] avg loss: 1.846200349020015		[learning rate: 0.003217]
	Learning Rate: 0.00321697
	LOSS [training: 1.981751578730604 | validation: 2.69434927117646]
	TIME [epoch: 8.35 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8389450009651196		[learning rate: 0.0032111]
		[batch 20/20] avg loss: 1.8089310760561497		[learning rate: 0.0032053]
	Learning Rate: 0.0032053
	LOSS [training: 1.8239380385106347 | validation: 2.6203107121967624]
	TIME [epoch: 8.34 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9317917808008285		[learning rate: 0.0031995]
		[batch 20/20] avg loss: 1.6743229665592831		[learning rate: 0.0031937]
	Learning Rate: 0.00319367
	LOSS [training: 1.803057373680056 | validation: 2.6115892044076126]
	TIME [epoch: 8.37 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.8866595546563496		[learning rate: 0.0031879]
		[batch 20/20] avg loss: 1.7408409864982624		[learning rate: 0.0031821]
	Learning Rate: 0.00318208
	LOSS [training: 1.8137502705773059 | validation: 2.6582364690943834]
	TIME [epoch: 8.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 10/20] avg loss: 1.9396811140228838		[learning rate: 0.0031763]
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.05
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.025
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.0125
Encountered nan in loss. Reverting update and performing model surgery.
	New model confinement_factor: 0.00625
ERROR:
Encountered nan in loss and reached the maximum number of model alterations: 4.
