Args:
Namespace(name='model_tr_study202', outdir='out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1', training_data='data/transition_rate_studies/tr_study202/tr_study202_training/r1', validation_data='data/transition_rate_studies/tr_study202/tr_study202_validation/r1', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, batch_size=250, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.5, nan_max_attempts=4, ndims=2, nparams=2, nsigs=2, ncells=500, dt=0.1, signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.075, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.01], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.01], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='kl', optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 317897628

Training model...

Saving initial model state to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.603930279955915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.603930279955915 | validation: 11.834300148722333]
	TIME [epoch: 93.6 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 11.79686566871767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 11.79686566871767 | validation: 11.433299560718783]
	TIME [epoch: 5.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 10.260304131452767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 10.260304131452767 | validation: 10.614188196725552]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.8089441808618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.8089441808618 | validation: 10.94947457933727]
	TIME [epoch: 5.74 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.454012739919568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.454012739919568 | validation: 10.7383718312777]
	TIME [epoch: 5.71 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 9.694771922985506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 9.694771922985506 | validation: 9.439457592933882]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 8.11162198761543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 8.11162198761543 | validation: 8.988375874254055]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 7.664888516078165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 7.664888516078165 | validation: 6.820653789329599]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 6.0129920845047256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.0129920845047256 | validation: 6.53370497551691]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.71581640256349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.71581640256349 | validation: 5.993518825264024]
	TIME [epoch: 5.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.1902495993815965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1902495993815965 | validation: 5.972753174008659]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.503862165122731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.503862165122731 | validation: 5.631332625497503]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.532614845635339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.532614845635339 | validation: 5.707413664515225]
	TIME [epoch: 5.7 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.188226509920775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.188226509920775 | validation: 5.551597099875976]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.4930583980474825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4930583980474825 | validation: 5.612653607823285]
	TIME [epoch: 5.7 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.032871445375006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.032871445375006 | validation: 5.400391380435394]
	TIME [epoch: 5.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.366789588399908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.366789588399908 | validation: 5.6846300879580625]
	TIME [epoch: 5.71 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.320253480128467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.320253480128467 | validation: 5.339769124648164]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.059381728103613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.059381728103613 | validation: 5.9593105661739445]
	TIME [epoch: 5.7 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.1153562896862494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1153562896862494 | validation: 5.436244497137332]
	TIME [epoch: 5.71 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.725293148550011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.725293148550011 | validation: 5.150412742305362]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.628529817922129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.628529817922129 | validation: 5.173465393701536]
	TIME [epoch: 5.74 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.641015345851737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.641015345851737 | validation: 8.33137581783376]
	TIME [epoch: 5.72 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 5.312745390470163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.312745390470163 | validation: 6.082558158790785]
	TIME [epoch: 5.7 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.245290826793298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.245290826793298 | validation: 5.462948123365761]
	TIME [epoch: 5.71 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6825743965792035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6825743965792035 | validation: 5.255793095867744]
	TIME [epoch: 5.71 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5545701730266166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5545701730266166 | validation: 5.1087419135092835]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5596491527134533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5596491527134533 | validation: 5.278346405735704]
	TIME [epoch: 5.73 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.379384017611581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.379384017611581 | validation: 5.429986087712753]
	TIME [epoch: 5.71 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8306734142973022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8306734142973022 | validation: 5.727522960763731]
	TIME [epoch: 5.7 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.861986274935817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.861986274935817 | validation: 5.2949103357341185]
	TIME [epoch: 5.69 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.674898880689506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.674898880689506 | validation: 5.402668016082949]
	TIME [epoch: 5.7 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6917143685253597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6917143685253597 | validation: 5.112206468872065]
	TIME [epoch: 5.69 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5095534633507324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5095534633507324 | validation: 5.983438526276837]
	TIME [epoch: 5.71 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.8555056340473666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8555056340473666 | validation: 5.371499918436877]
	TIME [epoch: 5.72 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.635346661306148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.635346661306148 | validation: 5.020777884331026]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7562141838253744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7562141838253744 | validation: 5.192924018398539]
	TIME [epoch: 5.7 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4581316448465853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4581316448465853 | validation: 5.618524764605502]
	TIME [epoch: 5.69 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.7294653203526487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7294653203526487 | validation: 5.210725475639099]
	TIME [epoch: 5.69 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.537453072134579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.537453072134579 | validation: 5.4488433191410754]
	TIME [epoch: 5.7 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.506186512224616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.506186512224616 | validation: 4.9652577234202475]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.677613592758476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.677613592758476 | validation: 5.2202094522689455]
	TIME [epoch: 5.7 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6049178795900554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6049178795900554 | validation: 5.5927246824411805]
	TIME [epoch: 5.7 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.844275025509419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.844275025509419 | validation: 5.012499776083102]
	TIME [epoch: 5.69 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2303629668766893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2303629668766893 | validation: 4.811709826128493]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.6266123853539787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6266123853539787 | validation: 4.9247389070150165]
	TIME [epoch: 5.7 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3974451986126466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3974451986126466 | validation: 4.893386076224078]
	TIME [epoch: 5.73 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4901648165255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4901648165255 | validation: 4.938383837697633]
	TIME [epoch: 5.7 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.362942987603673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.362942987603673 | validation: 5.003875960060165]
	TIME [epoch: 5.7 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3400203210609156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3400203210609156 | validation: 4.894924873926474]
	TIME [epoch: 5.69 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3833499958159767		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 3.3833499958159767 | validation: 4.780706684717004]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.366329746991719		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 3.366329746991719 | validation: 4.725772174280442]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.31094228571557		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 3.31094228571557 | validation: 5.081109296641558]
	TIME [epoch: 5.73 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3264419324298586		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 3.3264419324298586 | validation: 5.613490688616209]
	TIME [epoch: 5.74 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3769059289306784		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 3.3769059289306784 | validation: 4.780277648347908]
	TIME [epoch: 5.72 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3099482467165533		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 3.3099482467165533 | validation: 4.874929765575291]
	TIME [epoch: 5.72 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3426938533919786		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 3.3426938533919786 | validation: 4.775651770342403]
	TIME [epoch: 5.72 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2612860195458455		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 3.2612860195458455 | validation: 4.810802328770708]
	TIME [epoch: 5.72 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3312589291018764		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 3.3312589291018764 | validation: 4.6746818439937465]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.3424073347871373		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 3.3424073347871373 | validation: 4.94174650924866]
	TIME [epoch: 5.74 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1838213131309265		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 3.1838213131309265 | validation: 5.200142087110361]
	TIME [epoch: 5.72 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.287339543844861		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 3.287339543844861 | validation: 4.819744356526132]
	TIME [epoch: 5.71 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.196980803390005		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 3.196980803390005 | validation: 5.048899671467105]
	TIME [epoch: 5.71 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1658707800432113		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 3.1658707800432113 | validation: 4.818508009691156]
	TIME [epoch: 5.71 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2864633170246		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 3.2864633170246 | validation: 4.668227069269929]
	TIME [epoch: 5.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1417169791742476		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 3.1417169791742476 | validation: 4.610360838721923]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.248849150104777		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 3.248849150104777 | validation: 4.721223747788589]
	TIME [epoch: 5.72 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.197617745411971		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 3.197617745411971 | validation: 4.692924745857085]
	TIME [epoch: 5.71 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1506320911694905		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 3.1506320911694905 | validation: 4.658304873419307]
	TIME [epoch: 5.71 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1391718175874646		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 3.1391718175874646 | validation: 4.844655486319699]
	TIME [epoch: 5.71 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1788484661587244		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 3.1788484661587244 | validation: 4.582612248807893]
	TIME [epoch: 5.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1613112563527777		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 3.1613112563527777 | validation: 4.9524407354792155]
	TIME [epoch: 5.73 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9927562819870266		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 2.9927562819870266 | validation: 5.050057174818195]
	TIME [epoch: 5.71 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0927457062286634		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 3.0927457062286634 | validation: 5.009232396587357]
	TIME [epoch: 5.71 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.070289640005865		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 3.070289640005865 | validation: 4.680954115658345]
	TIME [epoch: 5.71 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.202092271861459		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 3.202092271861459 | validation: 4.719519134195744]
	TIME [epoch: 5.71 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5166570079767285		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 3.5166570079767285 | validation: 5.590032784873746]
	TIME [epoch: 5.75 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.530916962672167		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 3.530916962672167 | validation: 4.663546144279701]
	TIME [epoch: 5.72 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1265188373760076		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 3.1265188373760076 | validation: 4.898911574496596]
	TIME [epoch: 5.71 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1598520220411026		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 3.1598520220411026 | validation: 4.670543280462793]
	TIME [epoch: 5.71 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9403243678743833		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 2.9403243678743833 | validation: 4.759726414909767]
	TIME [epoch: 5.71 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.109500519176926		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 3.109500519176926 | validation: 4.624116530336025]
	TIME [epoch: 5.71 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9219404793934878		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 2.9219404793934878 | validation: 5.651606452579288]
	TIME [epoch: 5.74 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.272714563198357		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 3.272714563198357 | validation: 4.617312883197494]
	TIME [epoch: 5.73 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1078032237656688		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 3.1078032237656688 | validation: 4.702140950586447]
	TIME [epoch: 5.71 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1099616928902027		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 3.1099616928902027 | validation: 5.564443646955865]
	TIME [epoch: 5.71 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.2570119725845967		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 3.2570119725845967 | validation: 4.724877268166451]
	TIME [epoch: 5.71 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.087306503126497		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 3.087306503126497 | validation: 4.888853107490278]
	TIME [epoch: 5.71 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.152463778361006		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 3.152463778361006 | validation: 4.782077931308353]
	TIME [epoch: 5.74 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.172990080535493		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 3.172990080535493 | validation: 6.043624590616077]
	TIME [epoch: 5.73 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.4876897459576672		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 3.4876897459576672 | validation: 4.636514668012815]
	TIME [epoch: 5.71 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.000890427815969		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 3.000890427815969 | validation: 4.6375669119059015]
	TIME [epoch: 5.71 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.92172589023553		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 2.92172589023553 | validation: 4.635599972033049]
	TIME [epoch: 5.71 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1038869908937623		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 3.1038869908937623 | validation: 4.574104722202791]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.976311099974861		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 2.976311099974861 | validation: 4.610669971342585]
	TIME [epoch: 5.74 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.956645591199398		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 2.956645591199398 | validation: 4.578121869739249]
	TIME [epoch: 5.73 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.914035158508472		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 2.914035158508472 | validation: 4.745341347407908]
	TIME [epoch: 5.72 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1026964453980694		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 3.1026964453980694 | validation: 4.697106241379879]
	TIME [epoch: 5.71 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.956974594786465		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 2.956974594786465 | validation: 4.586760990613427]
	TIME [epoch: 5.72 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0536555880417966		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 3.0536555880417966 | validation: 4.618214937697749]
	TIME [epoch: 5.7 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.995452512665195		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 2.995452512665195 | validation: 4.758990852598031]
	TIME [epoch: 5.74 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8947375402466147		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 2.8947375402466147 | validation: 4.9431953638461845]
	TIME [epoch: 5.72 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9823278824089794		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 2.9823278824089794 | validation: 4.644252772943609]
	TIME [epoch: 5.71 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1197498595571		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 3.1197498595571 | validation: 4.6810382348902975]
	TIME [epoch: 5.71 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.904546767391056		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 2.904546767391056 | validation: 4.636012129406259]
	TIME [epoch: 5.71 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8708998966389334		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 2.8708998966389334 | validation: 4.517749115722372]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7737422874846933		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 2.7737422874846933 | validation: 5.028995495539168]
	TIME [epoch: 5.75 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.042035421048453		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 3.042035421048453 | validation: 4.577962684165762]
	TIME [epoch: 5.72 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9387661892407912		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 2.9387661892407912 | validation: 4.48695138044746]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.920900357534225		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 2.920900357534225 | validation: 4.610128881382951]
	TIME [epoch: 5.7 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8098036157794635		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 2.8098036157794635 | validation: 4.776983507997616]
	TIME [epoch: 5.7 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9087060462278034		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 2.9087060462278034 | validation: 4.668931917266882]
	TIME [epoch: 5.71 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9263730165327493		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 2.9263730165327493 | validation: 4.6044964735837945]
	TIME [epoch: 5.74 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.867720247531691		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 2.867720247531691 | validation: 4.495905778034308]
	TIME [epoch: 5.72 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.774560557050207		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 2.774560557050207 | validation: 4.515319986041906]
	TIME [epoch: 5.71 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.84331253970152		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 2.84331253970152 | validation: 4.706115324694473]
	TIME [epoch: 5.71 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.888508159045905		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 2.888508159045905 | validation: 4.580236480706008]
	TIME [epoch: 5.71 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.769327023750781		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 2.769327023750781 | validation: 5.2294497279713505]
	TIME [epoch: 5.71 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.019894829189784		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 3.019894829189784 | validation: 4.7755970466768085]
	TIME [epoch: 5.75 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.953704509807256		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 2.953704509807256 | validation: 4.489479547361049]
	TIME [epoch: 5.72 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7264441368826478		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 2.7264441368826478 | validation: 4.711776917812567]
	TIME [epoch: 5.71 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.899974572279336		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 2.899974572279336 | validation: 4.573370481267816]
	TIME [epoch: 5.71 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8368946320810373		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 2.8368946320810373 | validation: 4.663325513484909]
	TIME [epoch: 5.71 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.9087054392414995		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 2.9087054392414995 | validation: 4.494511965195581]
	TIME [epoch: 5.71 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.859264769866014		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 2.859264769866014 | validation: 4.6094265282277975]
	TIME [epoch: 5.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7992167965467885		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 2.7992167965467885 | validation: 4.752391847069239]
	TIME [epoch: 5.73 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.854277620825946		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 2.854277620825946 | validation: 4.568469501356552]
	TIME [epoch: 5.71 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.932197189013064		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 2.932197189013064 | validation: 4.498771904898941]
	TIME [epoch: 5.71 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.681908313981114		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 2.681908313981114 | validation: 4.607177251078794]
	TIME [epoch: 5.7 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7707766096489426		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 2.7707766096489426 | validation: 4.599558544896762]
	TIME [epoch: 5.71 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.901025782636958		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 2.901025782636958 | validation: 4.5260367272359705]
	TIME [epoch: 5.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8054448696925274		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 2.8054448696925274 | validation: 4.413696546616596]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.783880048185976		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 2.783880048185976 | validation: 4.486574624329712]
	TIME [epoch: 5.71 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7529645611919724		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 2.7529645611919724 | validation: 4.459765018987552]
	TIME [epoch: 5.7 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6386375110319964		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 2.6386375110319964 | validation: 4.661653366257788]
	TIME [epoch: 5.7 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8917872458484606		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 2.8917872458484606 | validation: 4.517543505486958]
	TIME [epoch: 5.7 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8164383604335193		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 2.8164383604335193 | validation: 4.475321856953739]
	TIME [epoch: 5.75 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6832264778308272		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 2.6832264778308272 | validation: 4.739932614851055]
	TIME [epoch: 5.71 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.844872406289557		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 2.844872406289557 | validation: 4.53327630214944]
	TIME [epoch: 5.7 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6859540183021076		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 2.6859540183021076 | validation: 4.508355514638754]
	TIME [epoch: 5.71 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.76452106969401		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 2.76452106969401 | validation: 4.452841234618531]
	TIME [epoch: 5.7 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7202911492955444		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 2.7202911492955444 | validation: 4.599876809769174]
	TIME [epoch: 5.71 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8072297761364284		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 2.8072297761364284 | validation: 4.470197548814975]
	TIME [epoch: 5.74 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.64478627173146		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 2.64478627173146 | validation: 4.495638360371987]
	TIME [epoch: 5.73 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.722076392583619		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 2.722076392583619 | validation: 4.643079269383097]
	TIME [epoch: 5.71 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.773183002144063		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 2.773183002144063 | validation: 4.440087919714875]
	TIME [epoch: 5.7 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6368727996624948		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 2.6368727996624948 | validation: 4.490800870563988]
	TIME [epoch: 5.71 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7767450563369955		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 2.7767450563369955 | validation: 4.447784990597141]
	TIME [epoch: 5.72 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6230638244500444		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 2.6230638244500444 | validation: 4.769992445500779]
	TIME [epoch: 5.74 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7631711647027224		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 2.7631711647027224 | validation: 4.489111456466494]
	TIME [epoch: 5.72 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6535449533234647		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 2.6535449533234647 | validation: 4.546846486387574]
	TIME [epoch: 5.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.656855668089395		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 2.656855668089395 | validation: 4.49388837625599]
	TIME [epoch: 5.7 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6197376814683304		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 2.6197376814683304 | validation: 4.659868089545355]
	TIME [epoch: 5.71 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.748518502909625		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 2.748518502909625 | validation: 4.36391034520419]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7087710467470556		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 2.7087710467470556 | validation: 4.775734072318113]
	TIME [epoch: 5.76 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.860843459346089		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 2.860843459346089 | validation: 4.836580264887904]
	TIME [epoch: 5.73 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.775969908275937		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 2.775969908275937 | validation: 4.606807501284764]
	TIME [epoch: 5.72 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7204588810898187		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 2.7204588810898187 | validation: 4.548065170082014]
	TIME [epoch: 5.7 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6323524186271317		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 2.6323524186271317 | validation: 4.339531235970415]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.61415913786329		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 2.61415913786329 | validation: 4.428109098272223]
	TIME [epoch: 5.7 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6182837871907707		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 2.6182837871907707 | validation: 4.361273654266167]
	TIME [epoch: 5.74 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.560121873996987		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 2.560121873996987 | validation: 4.591499700337168]
	TIME [epoch: 5.71 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6600866679388844		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 2.6600866679388844 | validation: 4.375684836717109]
	TIME [epoch: 5.7 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.727938155801982		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 2.727938155801982 | validation: 4.943645403577509]
	TIME [epoch: 5.7 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8504518275810042		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 2.8504518275810042 | validation: 4.321832228347781]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6517989292304462		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 2.6517989292304462 | validation: 4.366285102445286]
	TIME [epoch: 5.7 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5831871868979173		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 2.5831871868979173 | validation: 4.4436320288022015]
	TIME [epoch: 5.74 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5553419515203215		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 2.5553419515203215 | validation: 4.413299246559885]
	TIME [epoch: 5.71 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.637005542677545		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 2.637005542677545 | validation: 4.258574960502765]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.627913179227188		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 2.627913179227188 | validation: 4.415048901757274]
	TIME [epoch: 5.7 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.662450076513158		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 2.662450076513158 | validation: 4.282232538179875]
	TIME [epoch: 5.7 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6114775156506123		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 2.6114775156506123 | validation: 4.552208870064196]
	TIME [epoch: 5.71 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6588343089824518		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 2.6588343089824518 | validation: 4.463654856624813]
	TIME [epoch: 5.75 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.589678742293543		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 2.589678742293543 | validation: 4.4019260888719645]
	TIME [epoch: 5.72 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6149469858289094		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 2.6149469858289094 | validation: 4.269278240911545]
	TIME [epoch: 5.71 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7836222941845463		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 2.7836222941845463 | validation: 4.487774008456796]
	TIME [epoch: 5.71 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.620172281414744		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 2.620172281414744 | validation: 4.328793666563046]
	TIME [epoch: 5.71 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4681126476930615		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 2.4681126476930615 | validation: 4.624200552215659]
	TIME [epoch: 5.71 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5865165224683135		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 2.5865165224683135 | validation: 4.432745399201367]
	TIME [epoch: 5.75 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5399531030616322		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 2.5399531030616322 | validation: 4.285385653559509]
	TIME [epoch: 5.72 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5373789754789198		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 2.5373789754789198 | validation: 4.266047566415319]
	TIME [epoch: 5.71 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5196642304745858		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 2.5196642304745858 | validation: 4.618914573367122]
	TIME [epoch: 5.71 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.7104417938842778		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 2.7104417938842778 | validation: 4.38702583405174]
	TIME [epoch: 5.71 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.556694255427999		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 2.556694255427999 | validation: 4.256257941182412]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5779608856123684		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 2.5779608856123684 | validation: 4.301623395837454]
	TIME [epoch: 5.75 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.504938684647012		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 2.504938684647012 | validation: 4.452353170947514]
	TIME [epoch: 5.73 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5242621014205837		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 2.5242621014205837 | validation: 4.284841585815539]
	TIME [epoch: 5.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4921735061827683		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 2.4921735061827683 | validation: 4.308710548517085]
	TIME [epoch: 5.71 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.53198027005061		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 2.53198027005061 | validation: 4.313005050049331]
	TIME [epoch: 5.72 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4784187041540204		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 2.4784187041540204 | validation: 4.283079269920627]
	TIME [epoch: 5.71 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6904279380701124		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 2.6904279380701124 | validation: 5.149224864221386]
	TIME [epoch: 5.75 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.897971142936963		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 2.897971142936963 | validation: 4.364169327036265]
	TIME [epoch: 5.71 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5172797729442165		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 2.5172797729442165 | validation: 4.276884705583135]
	TIME [epoch: 5.7 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.486705834665531		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 2.486705834665531 | validation: 4.3462348573379534]
	TIME [epoch: 5.7 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4841019674541416		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 2.4841019674541416 | validation: 4.37612291353512]
	TIME [epoch: 5.7 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6143989670073897		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 2.6143989670073897 | validation: 4.261414210667451]
	TIME [epoch: 5.7 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.539124427195101		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 2.539124427195101 | validation: 4.261057217161996]
	TIME [epoch: 5.74 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.491317861714849		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 2.491317861714849 | validation: 4.430578623928493]
	TIME [epoch: 5.71 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5414717394341673		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 2.5414717394341673 | validation: 4.240963046522327]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.507925682498677		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 2.507925682498677 | validation: 4.263127887417569]
	TIME [epoch: 5.7 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5135737400514633		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 2.5135737400514633 | validation: 4.293764184313945]
	TIME [epoch: 5.71 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.476948182027906		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 2.476948182027906 | validation: 4.299603119922351]
	TIME [epoch: 5.72 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5772923245489867		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 2.5772923245489867 | validation: 4.283598339456947]
	TIME [epoch: 5.76 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5516848979767732		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 2.5516848979767732 | validation: 4.243516008747435]
	TIME [epoch: 5.73 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.416755409947671		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 2.416755409947671 | validation: 4.50667661925408]
	TIME [epoch: 5.7 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.496450259390401		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 2.496450259390401 | validation: 4.291472152319667]
	TIME [epoch: 5.72 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4699617232899977		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 2.4699617232899977 | validation: 4.238500100309336]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4933751237855786		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 2.4933751237855786 | validation: 4.271389877067945]
	TIME [epoch: 5.71 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6879096125218185		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 2.6879096125218185 | validation: 4.538805425083807]
	TIME [epoch: 5.75 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.523526868312772		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 2.523526868312772 | validation: 4.311687508974219]
	TIME [epoch: 5.7 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5527284905253325		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 2.5527284905253325 | validation: 4.243018095246656]
	TIME [epoch: 5.7 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4185904628120563		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 2.4185904628120563 | validation: 4.300273814749012]
	TIME [epoch: 5.7 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4050824403792657		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 2.4050824403792657 | validation: 4.230771356975997]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4639589195731673		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 2.4639589195731673 | validation: 4.443105974559503]
	TIME [epoch: 5.71 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5125374082155076		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 2.5125374082155076 | validation: 4.282226685873701]
	TIME [epoch: 5.75 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5962828558637785		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 2.5962828558637785 | validation: 4.260601644150636]
	TIME [epoch: 5.72 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4102469642703035		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 2.4102469642703035 | validation: 4.274900579322219]
	TIME [epoch: 5.7 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5334000733995445		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 2.5334000733995445 | validation: 4.290318733246383]
	TIME [epoch: 5.7 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.511604008515259		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 2.511604008515259 | validation: 4.338807430247536]
	TIME [epoch: 5.7 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.459942500284237		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 2.459942500284237 | validation: 4.208889812642155]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.391308406527816		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 2.391308406527816 | validation: 4.345611685197065]
	TIME [epoch: 5.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.40100240464533		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 2.40100240464533 | validation: 4.272764322317996]
	TIME [epoch: 5.71 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.485750960503538		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 2.485750960503538 | validation: 4.403988459071639]
	TIME [epoch: 5.7 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4717611865017126		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 2.4717611865017126 | validation: 4.353900572707376]
	TIME [epoch: 5.71 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4468646895380504		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 2.4468646895380504 | validation: 4.232150196035623]
	TIME [epoch: 5.7 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.40723968878784		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 2.40723968878784 | validation: 4.403407848105874]
	TIME [epoch: 5.71 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.057765822960679		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 3.057765822960679 | validation: 4.297131789816539]
	TIME [epoch: 5.75 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.460099914924671		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 2.460099914924671 | validation: 4.242116655771141]
	TIME [epoch: 5.7 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.361604010017823		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 2.361604010017823 | validation: 4.236652751835966]
	TIME [epoch: 5.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.65888971922412		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 2.65888971922412 | validation: 4.478882898168968]
	TIME [epoch: 5.71 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.42219083821076		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 2.42219083821076 | validation: 4.210045831795254]
	TIME [epoch: 5.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.371402708346004		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 2.371402708346004 | validation: 4.557156614291386]
	TIME [epoch: 5.71 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5653420945014025		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 2.5653420945014025 | validation: 4.231003028691765]
	TIME [epoch: 5.74 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.401484369922532		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 2.401484369922532 | validation: 4.258126581163063]
	TIME [epoch: 5.71 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.374310968130435		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 2.374310968130435 | validation: 4.209657807661241]
	TIME [epoch: 5.7 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3205779468676173		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 2.3205779468676173 | validation: 4.164857292298079]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3312640746804774		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 2.3312640746804774 | validation: 4.246359103586643]
	TIME [epoch: 5.7 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4013714355733193		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 2.4013714355733193 | validation: 4.375558892033148]
	TIME [epoch: 5.7 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3868629128228114		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 2.3868629128228114 | validation: 4.380433899559744]
	TIME [epoch: 5.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.407222005868516		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 2.407222005868516 | validation: 4.2384665999308995]
	TIME [epoch: 5.7 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.360868904530241		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 2.360868904530241 | validation: 4.230356846544789]
	TIME [epoch: 5.71 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4817900130290873		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 2.4817900130290873 | validation: 4.248042459796003]
	TIME [epoch: 5.71 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.661377125060651		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 2.661377125060651 | validation: 4.210032918099464]
	TIME [epoch: 5.71 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.35955181424934		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 2.35955181424934 | validation: 4.196701290315016]
	TIME [epoch: 5.7 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3030866449849148		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 2.3030866449849148 | validation: 4.183984731814655]
	TIME [epoch: 5.74 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3743099209110725		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 2.3743099209110725 | validation: 4.225791738313383]
	TIME [epoch: 5.72 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.387450844443813		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 2.387450844443813 | validation: 4.202064165134441]
	TIME [epoch: 5.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.448086810432887		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 2.448086810432887 | validation: 4.199272028055446]
	TIME [epoch: 5.7 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.546164891199501		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 2.546164891199501 | validation: 4.256956833770525]
	TIME [epoch: 5.7 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.342609384409112		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 2.342609384409112 | validation: 4.2486830630842825]
	TIME [epoch: 5.71 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.370218152861187		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 2.370218152861187 | validation: 4.1913141011328285]
	TIME [epoch: 5.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2875656612117816		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 2.2875656612117816 | validation: 4.21522348291451]
	TIME [epoch: 5.72 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.281196068459563		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 2.281196068459563 | validation: 4.234411111514441]
	TIME [epoch: 5.7 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4776909096897466		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 2.4776909096897466 | validation: 4.336340520379765]
	TIME [epoch: 5.71 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.319715996869639		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 2.319715996869639 | validation: 4.223720743137028]
	TIME [epoch: 5.7 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3785501354623997		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 2.3785501354623997 | validation: 4.192473807829628]
	TIME [epoch: 5.7 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2896638947094026		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 2.2896638947094026 | validation: 4.31733684038945]
	TIME [epoch: 5.75 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.311865446717367		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 2.311865446717367 | validation: 4.158815755037392]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3715738674748565		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 2.3715738674748565 | validation: 4.1751010988464445]
	TIME [epoch: 5.7 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3272161249795014		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 2.3272161249795014 | validation: 4.207170834693597]
	TIME [epoch: 5.7 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.314588604295048		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 2.314588604295048 | validation: 4.165133578901564]
	TIME [epoch: 5.7 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.27066403169437		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 2.27066403169437 | validation: 4.4627022521304545]
	TIME [epoch: 5.7 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.447934542817948		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 2.447934542817948 | validation: 4.306273046953635]
	TIME [epoch: 5.75 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3537158719626285		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 2.3537158719626285 | validation: 4.1812712468748146]
	TIME [epoch: 5.71 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3320539066355708		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 2.3320539066355708 | validation: 4.224541264094745]
	TIME [epoch: 5.71 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.294718857873738		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 2.294718857873738 | validation: 4.207886808907985]
	TIME [epoch: 5.7 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2948922771681204		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 2.2948922771681204 | validation: 4.329831923532282]
	TIME [epoch: 5.7 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.398755139660218		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 2.398755139660218 | validation: 4.195805558207667]
	TIME [epoch: 5.7 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.391630608648705		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 2.391630608648705 | validation: 4.222868021016069]
	TIME [epoch: 5.74 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.310958616502143		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 2.310958616502143 | validation: 4.353362877522356]
	TIME [epoch: 5.71 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3783095365295552		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 2.3783095365295552 | validation: 4.162226396058627]
	TIME [epoch: 5.7 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2429260553256896		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 2.2429260553256896 | validation: 4.266395250506832]
	TIME [epoch: 5.7 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.537275321420779		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 2.537275321420779 | validation: 4.235994242776879]
	TIME [epoch: 5.7 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5182388832778897		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 2.5182388832778897 | validation: 4.225361502091245]
	TIME [epoch: 5.7 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2857858873443337		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 2.2857858873443337 | validation: 4.21797020844392]
	TIME [epoch: 5.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.288949207480216		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 2.288949207480216 | validation: 4.16747063832923]
	TIME [epoch: 5.71 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2840736455653987		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 2.2840736455653987 | validation: 4.156801425827312]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2424615912326775		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 2.2424615912326775 | validation: 4.225846454387581]
	TIME [epoch: 5.7 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.321466641102216		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 2.321466641102216 | validation: 4.157096015396992]
	TIME [epoch: 5.7 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2665970974142207		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 2.2665970974142207 | validation: 4.134028646143482]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.290675449019132		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 2.290675449019132 | validation: 4.37091160831048]
	TIME [epoch: 5.75 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3601580501906474		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 2.3601580501906474 | validation: 4.159286997883022]
	TIME [epoch: 5.72 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.241805216726097		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 2.241805216726097 | validation: 4.428104311809274]
	TIME [epoch: 5.71 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.334525611078998		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 2.334525611078998 | validation: 4.370699624534013]
	TIME [epoch: 5.71 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.318548522820109		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 2.318548522820109 | validation: 4.307344348143418]
	TIME [epoch: 5.71 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.296083541770871		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 2.296083541770871 | validation: 4.146541694714301]
	TIME [epoch: 5.72 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.253072331082931		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 2.253072331082931 | validation: 4.26439375441853]
	TIME [epoch: 5.75 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4320132512031636		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 2.4320132512031636 | validation: 4.3801923946598595]
	TIME [epoch: 5.72 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.300027444159906		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 2.300027444159906 | validation: 4.227309319541663]
	TIME [epoch: 5.72 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2755950092043555		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 2.2755950092043555 | validation: 4.2415925125081575]
	TIME [epoch: 5.71 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3839892777761555		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 2.3839892777761555 | validation: 4.269634992547983]
	TIME [epoch: 5.71 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3034376730021857		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 2.3034376730021857 | validation: 4.199080003105755]
	TIME [epoch: 5.72 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2725744066858256		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 2.2725744066858256 | validation: 4.231138557524498]
	TIME [epoch: 5.75 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2656507766783265		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 2.2656507766783265 | validation: 4.201188801460815]
	TIME [epoch: 5.72 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.300906581747255		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 2.300906581747255 | validation: 4.288119888024514]
	TIME [epoch: 5.71 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3364792711534297		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 2.3364792711534297 | validation: 4.17653269812084]
	TIME [epoch: 5.71 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2695929883731387		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 2.2695929883731387 | validation: 4.274534894614518]
	TIME [epoch: 5.71 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.29697948502277		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 2.29697948502277 | validation: 4.190638237120456]
	TIME [epoch: 5.72 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.280338293952766		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 2.280338293952766 | validation: 4.187289970581844]
	TIME [epoch: 5.75 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.233904362218562		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 2.233904362218562 | validation: 4.197659894584983]
	TIME [epoch: 5.72 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2567739956828836		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 2.2567739956828836 | validation: 4.188958147060196]
	TIME [epoch: 5.71 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2985650006662524		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 2.2985650006662524 | validation: 4.194613494924937]
	TIME [epoch: 5.71 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2628242479684255		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 2.2628242479684255 | validation: 4.2155162712290455]
	TIME [epoch: 5.71 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5127071422413536		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 2.5127071422413536 | validation: 4.283010066888663]
	TIME [epoch: 5.72 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2766484622296934		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 2.2766484622296934 | validation: 4.179912103169689]
	TIME [epoch: 5.75 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.234546464887657		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 2.234546464887657 | validation: 4.194464045072021]
	TIME [epoch: 5.72 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.356345719457172		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 2.356345719457172 | validation: 4.279422428297774]
	TIME [epoch: 5.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2996860301944597		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 2.2996860301944597 | validation: 4.170003858213372]
	TIME [epoch: 5.71 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.229382971735647		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 2.229382971735647 | validation: 4.128226601615892]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2468765944307147		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 2.2468765944307147 | validation: 4.439609788400643]
	TIME [epoch: 5.72 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4045174504620888		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 2.4045174504620888 | validation: 4.166239885485552]
	TIME [epoch: 5.75 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.221978113355942		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 2.221978113355942 | validation: 4.1778898136025715]
	TIME [epoch: 5.72 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2162902219610987		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 2.2162902219610987 | validation: 4.132203791379855]
	TIME [epoch: 5.71 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1956781995047665		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 2.1956781995047665 | validation: 4.232436663358546]
	TIME [epoch: 5.71 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.338480812165511		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 2.338480812165511 | validation: 4.146681365766487]
	TIME [epoch: 5.71 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2003023801654034		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 2.2003023801654034 | validation: 4.143180348915339]
	TIME [epoch: 5.71 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2525601996956612		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 2.2525601996956612 | validation: 4.098629359593625]
	TIME [epoch: 5.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_317.pth
	Model improved!!!
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.186941806965888		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 2.186941806965888 | validation: 4.546064034467516]
	TIME [epoch: 5.72 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.378120083349236		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 2.378120083349236 | validation: 4.230038035592906]
	TIME [epoch: 5.71 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.203178415614946		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 2.203178415614946 | validation: 4.092930416778422]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1735214749019796		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 2.1735214749019796 | validation: 4.119078865384738]
	TIME [epoch: 5.71 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2536649827623476		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 2.2536649827623476 | validation: 4.527562501498891]
	TIME [epoch: 5.71 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.4661417478687637		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 2.4661417478687637 | validation: 4.183702045746539]
	TIME [epoch: 5.75 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2500369855970113		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 2.2500369855970113 | validation: 4.05482633884225]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.244062294768977		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 2.244062294768977 | validation: 4.065865817887204]
	TIME [epoch: 5.72 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2100505869428346		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 2.2100505869428346 | validation: 4.144504748005508]
	TIME [epoch: 5.71 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2274939886737806		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 2.2274939886737806 | validation: 4.194516007989885]
	TIME [epoch: 5.71 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3499302843863425		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 2.3499302843863425 | validation: 4.074824117623839]
	TIME [epoch: 5.72 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1790550729464435		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 2.1790550729464435 | validation: 4.052817216772293]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1662239655388613		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 2.1662239655388613 | validation: 4.1215582001536335]
	TIME [epoch: 5.71 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1698955835491343		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 2.1698955835491343 | validation: 4.153226668189176]
	TIME [epoch: 5.7 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2335051625143527		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 2.2335051625143527 | validation: 4.217610830599995]
	TIME [epoch: 5.69 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2534475468197313		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 2.2534475468197313 | validation: 4.080486188639285]
	TIME [epoch: 5.71 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21422175325033		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 2.21422175325033 | validation: 4.10803722028507]
	TIME [epoch: 5.71 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2137816796240113		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 2.2137816796240113 | validation: 4.156888494530092]
	TIME [epoch: 5.73 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2241289953774133		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 2.2241289953774133 | validation: 4.1417826476214525]
	TIME [epoch: 5.7 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2078994790335704		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 2.2078994790335704 | validation: 4.076545257275248]
	TIME [epoch: 5.7 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.176927238452354		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 2.176927238452354 | validation: 4.078584814304361]
	TIME [epoch: 5.71 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.19988915349536		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 2.19988915349536 | validation: 4.054463447352295]
	TIME [epoch: 5.7 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.204162566380679		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 2.204162566380679 | validation: 4.095656127619369]
	TIME [epoch: 5.72 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.190638766830703		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 2.190638766830703 | validation: 4.069839438602622]
	TIME [epoch: 5.73 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2345380320421215		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 2.2345380320421215 | validation: 4.118556437266044]
	TIME [epoch: 5.7 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1987595770762955		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 2.1987595770762955 | validation: 4.075386274415884]
	TIME [epoch: 5.7 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.232235442460503		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 2.232235442460503 | validation: 4.16949033067513]
	TIME [epoch: 5.69 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.213741492418066		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 2.213741492418066 | validation: 4.147161242907828]
	TIME [epoch: 5.7 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.154929875207576		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 2.154929875207576 | validation: 4.068059613614593]
	TIME [epoch: 5.71 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1377198760661402		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 2.1377198760661402 | validation: 4.063302398703629]
	TIME [epoch: 5.73 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.3421098025321885		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 2.3421098025321885 | validation: 4.080383083537489]
	TIME [epoch: 5.71 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.135303844833183		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 2.135303844833183 | validation: 4.085784259990937]
	TIME [epoch: 5.7 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.121705249652218		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 2.121705249652218 | validation: 4.05729849734305]
	TIME [epoch: 5.71 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1179234722366376		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 2.1179234722366376 | validation: 4.045166465391492]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.158616609856307		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 2.158616609856307 | validation: 4.107356845043579]
	TIME [epoch: 5.71 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1382378559325566		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 2.1382378559325566 | validation: 4.19918437692094]
	TIME [epoch: 5.73 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1914642260530304		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 2.1914642260530304 | validation: 4.169144786967878]
	TIME [epoch: 5.7 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.164333641814005		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 2.164333641814005 | validation: 4.059508878490034]
	TIME [epoch: 5.7 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1574095109861435		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 2.1574095109861435 | validation: 4.04328931595375]
	TIME [epoch: 5.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.126558417141543		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 2.126558417141543 | validation: 4.100285127277668]
	TIME [epoch: 5.72 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.169551916151381		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 2.169551916151381 | validation: 4.075372549884129]
	TIME [epoch: 5.73 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.293611417570183		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 2.293611417570183 | validation: 4.07925406705611]
	TIME [epoch: 5.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1888228795784075		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 2.1888228795784075 | validation: 4.1400417011956865]
	TIME [epoch: 5.72 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.21279055839721		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 2.21279055839721 | validation: 4.103966198428344]
	TIME [epoch: 5.7 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.256236323035558		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 2.256236323035558 | validation: 4.3057541018507335]
	TIME [epoch: 5.72 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2140201568927065		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 2.2140201568927065 | validation: 4.0568940827290305]
	TIME [epoch: 5.71 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1542143723994682		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 2.1542143723994682 | validation: 4.097007911788344]
	TIME [epoch: 5.71 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1436734492165184		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 2.1436734492165184 | validation: 4.070554219231508]
	TIME [epoch: 5.75 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.130721652099835		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 2.130721652099835 | validation: 4.03130040796311]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.254997144380817		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 2.254997144380817 | validation: 4.316800519886051]
	TIME [epoch: 5.7 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.178645203645991		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 2.178645203645991 | validation: 4.0258023021544815]
	TIME [epoch: 5.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1034849722011524		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 2.1034849722011524 | validation: 4.095716554907206]
	TIME [epoch: 5.71 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1267357009140864		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 2.1267357009140864 | validation: 4.141375156558947]
	TIME [epoch: 5.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.113811216513749		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 2.113811216513749 | validation: 4.152075629930259]
	TIME [epoch: 5.97 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1837685663414144		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 2.1837685663414144 | validation: 4.04873538767591]
	TIME [epoch: 5.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2406030215100627		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 2.2406030215100627 | validation: 4.157240668817752]
	TIME [epoch: 5.7 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.141811931145329		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 2.141811931145329 | validation: 4.078813084183158]
	TIME [epoch: 5.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1097289573279143		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 2.1097289573279143 | validation: 4.0279170077149855]
	TIME [epoch: 5.7 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.211581276463472		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 2.211581276463472 | validation: 4.03859403726848]
	TIME [epoch: 5.73 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1388050178594833		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 2.1388050178594833 | validation: 4.140378612281153]
	TIME [epoch: 5.71 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1717065482389266		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 2.1717065482389266 | validation: 4.209668121328841]
	TIME [epoch: 5.7 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.202203420487232		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 2.202203420487232 | validation: 4.130662248821996]
	TIME [epoch: 5.7 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1311161392610973		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 2.1311161392610973 | validation: 4.071159495880076]
	TIME [epoch: 5.7 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.148341940343716		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 2.148341940343716 | validation: 4.012318018881777]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1152006290484295		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 2.1152006290484295 | validation: 3.952818445060915]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.08049098895494		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 2.08049098895494 | validation: 4.090827469742171]
	TIME [epoch: 5.71 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.202194326458525		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 2.202194326458525 | validation: 4.051156757007841]
	TIME [epoch: 5.7 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.151366292763698		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 2.151366292763698 | validation: 4.073990617245204]
	TIME [epoch: 5.7 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.180688209595101		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 2.180688209595101 | validation: 4.131880794160596]
	TIME [epoch: 5.7 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1427809984010215		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 2.1427809984010215 | validation: 4.152256384944215]
	TIME [epoch: 5.7 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2198120465306506		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 2.2198120465306506 | validation: 4.0125375448148555]
	TIME [epoch: 5.73 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1406223084422766		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 2.1406223084422766 | validation: 3.985308162089176]
	TIME [epoch: 5.71 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.101891580616415		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 2.101891580616415 | validation: 4.08173389786219]
	TIME [epoch: 5.7 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.23196352301604		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 2.23196352301604 | validation: 4.0273957127957605]
	TIME [epoch: 5.7 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1431055115099165		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 2.1431055115099165 | validation: 4.056634397831789]
	TIME [epoch: 5.7 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1562285633898477		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 2.1562285633898477 | validation: 4.040730173578386]
	TIME [epoch: 5.7 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2193665053051417		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 2.2193665053051417 | validation: 4.029556584659856]
	TIME [epoch: 5.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1072863519493237		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 2.1072863519493237 | validation: 3.9951866288881503]
	TIME [epoch: 5.72 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.104437988099573		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 2.104437988099573 | validation: 4.050296991829968]
	TIME [epoch: 5.7 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0992930359347923		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 2.0992930359347923 | validation: 4.004051635812582]
	TIME [epoch: 5.7 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1338221048767814		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 2.1338221048767814 | validation: 4.059221381607976]
	TIME [epoch: 5.7 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1644061345233174		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 2.1644061345233174 | validation: 4.081630522220014]
	TIME [epoch: 5.7 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.157879457546856		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 2.157879457546856 | validation: 4.05792955014851]
	TIME [epoch: 5.73 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1113768205501753		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 2.1113768205501753 | validation: 4.00311388253597]
	TIME [epoch: 5.72 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.098234574499708		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 2.098234574499708 | validation: 4.06464841615074]
	TIME [epoch: 5.7 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1669726072625757		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 2.1669726072625757 | validation: 3.966439536996136]
	TIME [epoch: 5.7 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1402567960112915		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 2.1402567960112915 | validation: 3.9671579248123363]
	TIME [epoch: 5.7 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1484658526284455		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 2.1484658526284455 | validation: 3.9620592029128665]
	TIME [epoch: 5.7 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.05536139428919		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 2.05536139428919 | validation: 3.9850470041452573]
	TIME [epoch: 5.73 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0711742143387784		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 2.0711742143387784 | validation: 3.998134719642732]
	TIME [epoch: 5.71 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.165651274549589		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 2.165651274549589 | validation: 3.966671339033841]
	TIME [epoch: 5.7 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0915238819955286		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 2.0915238819955286 | validation: 3.9757935190159506]
	TIME [epoch: 5.7 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0746630403077226		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 2.0746630403077226 | validation: 4.157421921105144]
	TIME [epoch: 5.7 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.181993910850058		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 2.181993910850058 | validation: 4.17813386304199]
	TIME [epoch: 5.7 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1778137491814205		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 2.1778137491814205 | validation: 4.028087313387833]
	TIME [epoch: 5.71 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1355103370842254		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 2.1355103370842254 | validation: 4.010556436797678]
	TIME [epoch: 5.72 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0848866816611134		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 2.0848866816611134 | validation: 4.076818099296893]
	TIME [epoch: 5.7 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.188411062284691		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 2.188411062284691 | validation: 4.04576178304989]
	TIME [epoch: 5.7 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.138574912797809		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 2.138574912797809 | validation: 4.0262947528854065]
	TIME [epoch: 5.7 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0844296700793805		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 2.0844296700793805 | validation: 4.079689031411262]
	TIME [epoch: 5.7 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0982497676412004		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 2.0982497676412004 | validation: 4.102833512640646]
	TIME [epoch: 5.71 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.217542244142259		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 2.217542244142259 | validation: 4.007497933114319]
	TIME [epoch: 5.72 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.074540963406224		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 2.074540963406224 | validation: 4.012739270691955]
	TIME [epoch: 5.7 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.084981355854999		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 2.084981355854999 | validation: 4.247819711746884]
	TIME [epoch: 5.7 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1575052453147934		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 2.1575052453147934 | validation: 4.047095521421753]
	TIME [epoch: 5.7 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.084639106030304		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 2.084639106030304 | validation: 4.02947265874906]
	TIME [epoch: 5.7 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1036352276581614		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 2.1036352276581614 | validation: 4.042588274581093]
	TIME [epoch: 5.71 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.149265964375901		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 2.149265964375901 | validation: 4.186183903211979]
	TIME [epoch: 5.72 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.113068705089084		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 2.113068705089084 | validation: 4.079266576464774]
	TIME [epoch: 5.7 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.073911457882049		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 2.073911457882049 | validation: 4.007460349925006]
	TIME [epoch: 5.7 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1173933958859745		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 2.1173933958859745 | validation: 4.063607069537576]
	TIME [epoch: 5.7 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.07930353085813		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 2.07930353085813 | validation: 4.063401311488186]
	TIME [epoch: 5.7 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.163504770373687		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 2.163504770373687 | validation: 4.033281948461227]
	TIME [epoch: 5.71 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.11182068979379		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 2.11182068979379 | validation: 4.00093437614812]
	TIME [epoch: 5.72 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0447290350029		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 2.0447290350029 | validation: 4.064884123254742]
	TIME [epoch: 5.7 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0800419176600693		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 2.0800419176600693 | validation: 4.2884112467392566]
	TIME [epoch: 5.7 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.199997036122146		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 2.199997036122146 | validation: 3.9930238759177463]
	TIME [epoch: 5.7 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1043052422246804		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 2.1043052422246804 | validation: 4.022630282147328]
	TIME [epoch: 5.7 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.133792815610506		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 2.133792815610506 | validation: 4.0784672435569735]
	TIME [epoch: 5.71 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1190976074926624		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 2.1190976074926624 | validation: 4.147762302878262]
	TIME [epoch: 5.72 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.162673714306495		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 2.162673714306495 | validation: 4.009006695836835]
	TIME [epoch: 5.7 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0936091810029356		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 2.0936091810029356 | validation: 4.024289173704549]
	TIME [epoch: 5.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1250920754673266		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 2.1250920754673266 | validation: 4.019813641016966]
	TIME [epoch: 5.7 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.064330336593394		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 2.064330336593394 | validation: 4.3465539581176165]
	TIME [epoch: 5.7 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.176589383100225		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 2.176589383100225 | validation: 4.032076775316901]
	TIME [epoch: 5.71 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0674950201929163		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 2.0674950201929163 | validation: 3.9950975481030673]
	TIME [epoch: 5.72 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.107679031424629		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 2.107679031424629 | validation: 4.016282553719002]
	TIME [epoch: 5.7 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0835768338457674		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 2.0835768338457674 | validation: 3.987225167211132]
	TIME [epoch: 5.7 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0614859303927955		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 2.0614859303927955 | validation: 3.9949889805628254]
	TIME [epoch: 5.7 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.089904512727946		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 2.089904512727946 | validation: 4.010563559796593]
	TIME [epoch: 5.69 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0729792294886713		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 2.0729792294886713 | validation: 4.0631105842176325]
	TIME [epoch: 5.7 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0959921060888638		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 2.0959921060888638 | validation: 3.9811663257024548]
	TIME [epoch: 5.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0785915016407426		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 2.0785915016407426 | validation: 3.9937336826428145]
	TIME [epoch: 5.7 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0543444057118156		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 2.0543444057118156 | validation: 4.022409898662652]
	TIME [epoch: 5.7 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0715204716877924		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 2.0715204716877924 | validation: 4.04495671321128]
	TIME [epoch: 5.69 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.12082315646187		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 2.12082315646187 | validation: 4.201867103954543]
	TIME [epoch: 5.69 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.136054404513886		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 2.136054404513886 | validation: 4.017152531219006]
	TIME [epoch: 5.7 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.079012376766175		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 2.079012376766175 | validation: 4.085490259976564]
	TIME [epoch: 5.73 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0911612816576928		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 2.0911612816576928 | validation: 4.0206850017108]
	TIME [epoch: 5.7 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.038923055614572		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 2.038923055614572 | validation: 4.017488039400998]
	TIME [epoch: 5.7 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.073526808624093		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 2.073526808624093 | validation: 4.0638231654170935]
	TIME [epoch: 5.7 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0603133909535436		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 2.0603133909535436 | validation: 4.044446778214214]
	TIME [epoch: 5.69 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0958020974703624		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 2.0958020974703624 | validation: 4.008864809250223]
	TIME [epoch: 5.7 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0636560863208198		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 2.0636560863208198 | validation: 4.031595255207416]
	TIME [epoch: 5.73 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0839854873786674		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 2.0839854873786674 | validation: 4.1210487565840275]
	TIME [epoch: 5.7 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.170280792628031		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 2.170280792628031 | validation: 4.017174984598772]
	TIME [epoch: 5.7 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0633036655944306		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 2.0633036655944306 | validation: 3.949024724140735]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0345848084666693		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 2.0345848084666693 | validation: 4.170964543988929]
	TIME [epoch: 5.7 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1331398098604715		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 2.1331398098604715 | validation: 4.064878641840278]
	TIME [epoch: 5.7 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.072189324845578		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 2.072189324845578 | validation: 4.03067228878758]
	TIME [epoch: 5.73 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.047072709927268		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 2.047072709927268 | validation: 3.94454774856389]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.072522773499915		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 2.072522773499915 | validation: 4.006716395425625]
	TIME [epoch: 5.7 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0306500264940532		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 2.0306500264940532 | validation: 4.103358071828475]
	TIME [epoch: 5.69 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2248917577087264		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 2.2248917577087264 | validation: 3.9337131011687165]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.08060732752486		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 2.08060732752486 | validation: 4.075892124219418]
	TIME [epoch: 5.71 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0983810183053277		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 2.0983810183053277 | validation: 3.967473837694504]
	TIME [epoch: 5.73 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0610176693825393		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 2.0610176693825393 | validation: 3.9393848960924904]
	TIME [epoch: 5.71 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.02582760067515		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 2.02582760067515 | validation: 3.9468707182089178]
	TIME [epoch: 5.7 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0478765463729305		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 2.0478765463729305 | validation: 3.9023796541836493]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.046381350239737		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 2.046381350239737 | validation: 3.9221639976956677]
	TIME [epoch: 5.71 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0699373889596235		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 2.0699373889596235 | validation: 4.06310960224721]
	TIME [epoch: 5.71 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.045300937104869		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 2.045300937104869 | validation: 3.9501966467834038]
	TIME [epoch: 5.73 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1139937565126408		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 2.1139937565126408 | validation: 4.01598571587838]
	TIME [epoch: 5.69 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.050522980367407		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 2.050522980367407 | validation: 3.9126398909581037]
	TIME [epoch: 5.7 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.011327717896343		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 2.011327717896343 | validation: 3.9501833813722405]
	TIME [epoch: 5.71 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0378358801434313		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 2.0378358801434313 | validation: 3.951026380674726]
	TIME [epoch: 5.7 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0257645548985694		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 2.0257645548985694 | validation: 4.035269607065683]
	TIME [epoch: 5.73 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.120638732311437		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 2.120638732311437 | validation: 3.9012018039253538]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0204424185456324		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 2.0204424185456324 | validation: 3.962131426436954]
	TIME [epoch: 5.72 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0188292667679324		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 2.0188292667679324 | validation: 3.933846433492257]
	TIME [epoch: 5.71 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0141366395332834		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 2.0141366395332834 | validation: 3.8949185703680285]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.012364676145719		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 2.012364676145719 | validation: 3.920513783764773]
	TIME [epoch: 5.7 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.027182371921733		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 2.027182371921733 | validation: 3.879612796078035]
	TIME [epoch: 5.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.008294263487034		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 2.008294263487034 | validation: 3.8863392457323744]
	TIME [epoch: 5.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.996652589207924		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 1.996652589207924 | validation: 4.0164286239515645]
	TIME [epoch: 5.7 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.02263716144695		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 2.02263716144695 | validation: 4.0470508377863]
	TIME [epoch: 5.7 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0833518555967183		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 2.0833518555967183 | validation: 3.9344387716330864]
	TIME [epoch: 5.71 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0212701033081815		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 2.0212701033081815 | validation: 3.883870559897294]
	TIME [epoch: 5.71 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.998995733442095		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 1.998995733442095 | validation: 3.928646069656061]
	TIME [epoch: 5.74 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9973053487407824		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 1.9973053487407824 | validation: 4.109090829461805]
	TIME [epoch: 5.71 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1019460568568262		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 2.1019460568568262 | validation: 3.914007965673245]
	TIME [epoch: 5.71 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9961247305891439		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 1.9961247305891439 | validation: 3.884709295659452]
	TIME [epoch: 5.7 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0111742594966664		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 2.0111742594966664 | validation: 3.9415948904427798]
	TIME [epoch: 5.71 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.040443800992917		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 2.040443800992917 | validation: 3.894891385958357]
	TIME [epoch: 5.7 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9955281765712525		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 1.9955281765712525 | validation: 3.9081806920809794]
	TIME [epoch: 5.74 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0076877457166753		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 2.0076877457166753 | validation: 3.9069756977700325]
	TIME [epoch: 5.71 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9903938037877458		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 1.9903938037877458 | validation: 3.935790889720954]
	TIME [epoch: 5.71 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9955295548199525		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 1.9955295548199525 | validation: 4.002090427407595]
	TIME [epoch: 5.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0608569684191		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 2.0608569684191 | validation: 3.9093076975356476]
	TIME [epoch: 5.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9948934376592837		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 1.9948934376592837 | validation: 3.9285141353618793]
	TIME [epoch: 5.71 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0108058901510892		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 2.0108058901510892 | validation: 3.9558646524528984]
	TIME [epoch: 5.73 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.049448693102621		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 2.049448693102621 | validation: 4.114101487853844]
	TIME [epoch: 5.71 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1207377393829026		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 2.1207377393829026 | validation: 3.8568462385741964]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.022340650365278		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 2.022340650365278 | validation: 3.8850679134668837]
	TIME [epoch: 5.71 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.982541607135152		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 1.982541607135152 | validation: 3.879630936554211]
	TIME [epoch: 5.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.023510173863702		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 2.023510173863702 | validation: 3.9028569860051467]
	TIME [epoch: 5.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.994703616865401		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 1.994703616865401 | validation: 3.860097457265406]
	TIME [epoch: 5.74 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9711330532707678		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 1.9711330532707678 | validation: 3.892990046661302]
	TIME [epoch: 5.73 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0317108069974488		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 2.0317108069974488 | validation: 3.9340227659380553]
	TIME [epoch: 5.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0085322482210612		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 2.0085322482210612 | validation: 3.879330470681424]
	TIME [epoch: 5.69 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9832538312848065		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 1.9832538312848065 | validation: 3.8962433865070665]
	TIME [epoch: 5.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9728664115947296		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 1.9728664115947296 | validation: 3.9333617515423165]
	TIME [epoch: 5.71 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0068629644245366		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 2.0068629644245366 | validation: 3.9143075760162755]
	TIME [epoch: 5.73 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9997367930251662		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 1.9997367930251662 | validation: 3.8705651267160714]
	TIME [epoch: 5.71 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9808418785033124		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 1.9808418785033124 | validation: 3.8667062480428966]
	TIME [epoch: 5.71 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9791101628507688		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 1.9791101628507688 | validation: 4.018655958310726]
	TIME [epoch: 5.69 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0339987801962955		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 2.0339987801962955 | validation: 3.8655496714836612]
	TIME [epoch: 5.69 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.003852329519081		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 2.003852329519081 | validation: 3.8892748919680202]
	TIME [epoch: 5.69 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9841847973860478		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 1.9841847973860478 | validation: 3.885168089800598]
	TIME [epoch: 5.72 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9803105123212663		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 1.9803105123212663 | validation: 3.844215647490874]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_527.pth
	Model improved!!!
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9645149881500803		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 1.9645149881500803 | validation: 3.8870387049680466]
	TIME [epoch: 5.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0150912700475856		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 2.0150912700475856 | validation: 3.9876693065171707]
	TIME [epoch: 5.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0106064801751504		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 2.0106064801751504 | validation: 3.8894155309541896]
	TIME [epoch: 5.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0415985333973863		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 2.0415985333973863 | validation: 4.042371861750118]
	TIME [epoch: 5.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0786861165284227		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 2.0786861165284227 | validation: 3.9277882454046638]
	TIME [epoch: 5.73 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.028373266432461		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 2.028373266432461 | validation: 3.8689868113001444]
	TIME [epoch: 5.71 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9788439370761073		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 1.9788439370761073 | validation: 3.9053212588008037]
	TIME [epoch: 5.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.00187889024056		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 2.00187889024056 | validation: 3.9560041695302197]
	TIME [epoch: 5.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9986551405435695		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 1.9986551405435695 | validation: 3.8811684850972075]
	TIME [epoch: 5.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9984124791491156		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 1.9984124791491156 | validation: 3.883197541827483]
	TIME [epoch: 5.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.972971302142723		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 1.972971302142723 | validation: 3.873516538069058]
	TIME [epoch: 5.73 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0209821467682203		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 2.0209821467682203 | validation: 3.9387032185199007]
	TIME [epoch: 5.72 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9987165819476749		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 1.9987165819476749 | validation: 3.8564861911475368]
	TIME [epoch: 5.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9743473771295963		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 1.9743473771295963 | validation: 3.936556307406902]
	TIME [epoch: 5.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9933100621173994		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 1.9933100621173994 | validation: 3.876506072110917]
	TIME [epoch: 5.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.026140009830853		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 2.026140009830853 | validation: 3.9290265020628086]
	TIME [epoch: 5.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9857135459473187		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 1.9857135459473187 | validation: 3.8633291397175857]
	TIME [epoch: 5.73 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9569043009950855		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 1.9569043009950855 | validation: 3.85005000150392]
	TIME [epoch: 5.72 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9566214541827862		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 1.9566214541827862 | validation: 3.844478070261435]
	TIME [epoch: 5.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9503200485123071		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 1.9503200485123071 | validation: 3.842644533925155]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.011410483327814		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 2.011410483327814 | validation: 3.8816049593850854]
	TIME [epoch: 5.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9683949345777052		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 1.9683949345777052 | validation: 3.8997753605540098]
	TIME [epoch: 5.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9969838818118748		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 1.9969838818118748 | validation: 3.8638095733304243]
	TIME [epoch: 5.72 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.006146164966145		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 2.006146164966145 | validation: 4.037607567001649]
	TIME [epoch: 5.71 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.034508827662835		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 2.034508827662835 | validation: 3.8573350987220985]
	TIME [epoch: 5.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9938594829280323		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 1.9938594829280323 | validation: 3.907390798705492]
	TIME [epoch: 5.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.013958112322793		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 2.013958112322793 | validation: 3.8669246272926814]
	TIME [epoch: 5.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.968175576170537		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 1.968175576170537 | validation: 3.845276147482554]
	TIME [epoch: 5.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.016917264501788		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 2.016917264501788 | validation: 3.915231022947566]
	TIME [epoch: 5.72 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.984145704444003		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 1.984145704444003 | validation: 3.8616438590735753]
	TIME [epoch: 5.71 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9754342489832233		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 1.9754342489832233 | validation: 3.8421791239958383]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9654586558448588		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 1.9654586558448588 | validation: 3.839249996434566]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9934928462308394		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 1.9934928462308394 | validation: 3.927393638154608]
	TIME [epoch: 5.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9876127583754373		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 1.9876127583754373 | validation: 3.8665030349971845]
	TIME [epoch: 5.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0095373960418024		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 2.0095373960418024 | validation: 3.8571717231453824]
	TIME [epoch: 5.73 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9489987478030515		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 1.9489987478030515 | validation: 3.8498646514606816]
	TIME [epoch: 5.71 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9615178534288353		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 1.9615178534288353 | validation: 3.8715201300737694]
	TIME [epoch: 5.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.987629601356085		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 1.987629601356085 | validation: 3.846964893401116]
	TIME [epoch: 5.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0169460202623317		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 2.0169460202623317 | validation: 3.9036170640111907]
	TIME [epoch: 5.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0559188336960825		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 2.0559188336960825 | validation: 3.8549865082440045]
	TIME [epoch: 5.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.976041102728725		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 1.976041102728725 | validation: 3.8620769080222512]
	TIME [epoch: 5.73 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9594160803268557		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 1.9594160803268557 | validation: 3.8580383546324915]
	TIME [epoch: 5.71 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9473613154529523		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 1.9473613154529523 | validation: 3.850150607184959]
	TIME [epoch: 5.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9312851538356162		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 1.9312851538356162 | validation: 3.849165758654507]
	TIME [epoch: 5.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9561874346773442		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 1.9561874346773442 | validation: 3.889950509547087]
	TIME [epoch: 5.7 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.950624018238042		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 1.950624018238042 | validation: 3.848049016586908]
	TIME [epoch: 5.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9848769421739165		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 1.9848769421739165 | validation: 3.871520810235387]
	TIME [epoch: 5.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9659866088513396		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 1.9659866088513396 | validation: 3.9092536702607106]
	TIME [epoch: 5.71 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9511362080481036		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 1.9511362080481036 | validation: 3.8487358295034686]
	TIME [epoch: 5.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.078772215211834		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 2.078772215211834 | validation: 3.8546300303983565]
	TIME [epoch: 5.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9721617668523295		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 1.9721617668523295 | validation: 3.9188749793341247]
	TIME [epoch: 5.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.006249693959766		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 2.006249693959766 | validation: 3.8591805534242787]
	TIME [epoch: 5.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9734143239358874		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 1.9734143239358874 | validation: 3.8637381651629723]
	TIME [epoch: 5.73 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9586611873893234		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 1.9586611873893234 | validation: 3.850360843030238]
	TIME [epoch: 5.71 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9369844347399896		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 1.9369844347399896 | validation: 3.8298238197466867]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9346570171827058		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 1.9346570171827058 | validation: 3.875726443578204]
	TIME [epoch: 5.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.949145430660166		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 1.949145430660166 | validation: 3.8476580075112925]
	TIME [epoch: 5.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9679615159167922		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 1.9679615159167922 | validation: 3.877900311895568]
	TIME [epoch: 5.71 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.962291308852199		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 1.962291308852199 | validation: 3.841323966480564]
	TIME [epoch: 5.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9496683642706107		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 1.9496683642706107 | validation: 3.865925719383666]
	TIME [epoch: 5.71 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.010142651746401		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 2.010142651746401 | validation: 3.9674482921486094]
	TIME [epoch: 5.71 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0094479086630868		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 2.0094479086630868 | validation: 3.8957083062394116]
	TIME [epoch: 5.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9654264037289626		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 1.9654264037289626 | validation: 3.8307629357453483]
	TIME [epoch: 5.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9268267814133673		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 1.9268267814133673 | validation: 3.848041883604312]
	TIME [epoch: 5.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9518309814745656		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 1.9518309814745656 | validation: 3.8801775924517425]
	TIME [epoch: 5.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9373040035031426		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 1.9373040035031426 | validation: 3.836170906165433]
	TIME [epoch: 5.71 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9365756893478951		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 1.9365756893478951 | validation: 3.828433620626236]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9742610669148122		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 1.9742610669148122 | validation: 3.82994281672746]
	TIME [epoch: 5.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.96099023970659		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 1.96099023970659 | validation: 3.8947231874814805]
	TIME [epoch: 5.71 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9607639463325908		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 1.9607639463325908 | validation: 3.844575244045209]
	TIME [epoch: 5.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9399362967155487		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 1.9399362967155487 | validation: 3.90390754557503]
	TIME [epoch: 5.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.982987508979861		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 1.982987508979861 | validation: 3.885583019448922]
	TIME [epoch: 5.71 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.948492867560555		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 1.948492867560555 | validation: 3.8296369908099237]
	TIME [epoch: 5.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.93102663235376		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 1.93102663235376 | validation: 3.8186635861575566]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_601.pth
	Model improved!!!
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.947907919009284		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 1.947907919009284 | validation: 3.8853695170149014]
	TIME [epoch: 5.7 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9828552311856051		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 1.9828552311856051 | validation: 3.855344852829415]
	TIME [epoch: 5.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9875719652311754		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 1.9875719652311754 | validation: 3.894457436925521]
	TIME [epoch: 5.73 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9461787588017456		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 1.9461787588017456 | validation: 3.844222917181726]
	TIME [epoch: 5.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9326396409681998		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 1.9326396409681998 | validation: 3.8473283799176263]
	TIME [epoch: 5.69 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9535828786413414		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 1.9535828786413414 | validation: 3.8559864500437118]
	TIME [epoch: 5.69 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9774971813027318		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 1.9774971813027318 | validation: 3.8721047039666905]
	TIME [epoch: 5.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.002040383257504		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 2.002040383257504 | validation: 3.845843836102239]
	TIME [epoch: 5.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9429084400191723		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 1.9429084400191723 | validation: 3.8576328394045762]
	TIME [epoch: 5.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.95333771049346		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 1.95333771049346 | validation: 3.8353827648808867]
	TIME [epoch: 5.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9669997257927285		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 1.9669997257927285 | validation: 3.8762755323112614]
	TIME [epoch: 5.69 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9429319081339336		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 1.9429319081339336 | validation: 3.846521364433336]
	TIME [epoch: 5.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9397255445752377		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 1.9397255445752377 | validation: 3.8363747687620453]
	TIME [epoch: 5.69 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9363504503344844		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 1.9363504503344844 | validation: 3.82746318602575]
	TIME [epoch: 5.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.954655061270452		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 1.954655061270452 | validation: 3.8587443752981914]
	TIME [epoch: 5.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.963194089062448		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 1.963194089062448 | validation: 3.8540510788973927]
	TIME [epoch: 5.72 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9653721581017198		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 1.9653721581017198 | validation: 3.847639429779357]
	TIME [epoch: 5.71 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.967435287903021		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 1.967435287903021 | validation: 3.8309443699445502]
	TIME [epoch: 5.71 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9711307190921068		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 1.9711307190921068 | validation: 3.8335785289044906]
	TIME [epoch: 5.71 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9447407631394085		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 1.9447407631394085 | validation: 3.9211819614959382]
	TIME [epoch: 5.71 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9604974450274852		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 1.9604974450274852 | validation: 3.936507247869712]
	TIME [epoch: 5.73 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9845519791321178		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 1.9845519791321178 | validation: 3.835433885504196]
	TIME [epoch: 5.71 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9306698311742958		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 1.9306698311742958 | validation: 3.85543828786184]
	TIME [epoch: 5.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9536388809127454		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 1.9536388809127454 | validation: 3.882900493997066]
	TIME [epoch: 5.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.178682013278866		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 2.178682013278866 | validation: 3.8578487409655504]
	TIME [epoch: 5.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9689532847952638		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 1.9689532847952638 | validation: 3.8187661802569313]
	TIME [epoch: 5.71 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9177566590151618		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 1.9177566590151618 | validation: 3.8108132858916544]
	TIME [epoch: 5.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_628.pth
	Model improved!!!
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9387791931703895		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 1.9387791931703895 | validation: 3.8352229221289083]
	TIME [epoch: 5.73 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.961216991177888		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 1.961216991177888 | validation: 3.805693588630668]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9200777690814934		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 1.9200777690814934 | validation: 3.8260064768113398]
	TIME [epoch: 5.71 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9633778555376742		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 1.9633778555376742 | validation: 3.8176343762934812]
	TIME [epoch: 5.71 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9275760436535276		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 1.9275760436535276 | validation: 3.8388063812398485]
	TIME [epoch: 5.71 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9395718991028545		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 1.9395718991028545 | validation: 3.852960677645656]
	TIME [epoch: 5.75 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9449074829867046		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 1.9449074829867046 | validation: 3.8492484846019726]
	TIME [epoch: 5.71 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9560404140296717		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 1.9560404140296717 | validation: 3.875491955425635]
	TIME [epoch: 5.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9496096568404195		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 1.9496096568404195 | validation: 3.8532184902712765]
	TIME [epoch: 5.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9388814510863566		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 1.9388814510863566 | validation: 3.8202842031913766]
	TIME [epoch: 5.71 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9580941789563295		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 1.9580941789563295 | validation: 3.8103449073780316]
	TIME [epoch: 5.71 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.940675072219474		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 1.940675072219474 | validation: 3.9042633846004198]
	TIME [epoch: 5.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.971158640383679		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 1.971158640383679 | validation: 3.8157649782729517]
	TIME [epoch: 5.72 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9796232781616394		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 1.9796232781616394 | validation: 3.840893209698409]
	TIME [epoch: 5.71 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.937603730815751		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 1.937603730815751 | validation: 3.8119041043312745]
	TIME [epoch: 5.71 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.934780089878388		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 1.934780089878388 | validation: 3.82039505796939]
	TIME [epoch: 5.71 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9715573806046935		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 1.9715573806046935 | validation: 3.8364270528607474]
	TIME [epoch: 5.71 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.922080701691677		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 1.922080701691677 | validation: 3.8143428540655497]
	TIME [epoch: 5.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9469643110684522		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 1.9469643110684522 | validation: 3.836259433160384]
	TIME [epoch: 5.69 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9413347793468267		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 1.9413347793468267 | validation: 3.956010964565363]
	TIME [epoch: 5.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.978305405922447		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 1.978305405922447 | validation: 3.825906763155312]
	TIME [epoch: 5.71 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9393442823595617		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 1.9393442823595617 | validation: 3.8174990660033536]
	TIME [epoch: 5.71 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9292633290465877		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 1.9292633290465877 | validation: 3.842222145318126]
	TIME [epoch: 5.71 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9807151410528354		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 1.9807151410528354 | validation: 3.840968677495837]
	TIME [epoch: 5.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9210153573960962		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 1.9210153573960962 | validation: 3.823960122072928]
	TIME [epoch: 5.72 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9198607034354485		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 1.9198607034354485 | validation: 3.8313740079860383]
	TIME [epoch: 5.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.941846169401352		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 1.941846169401352 | validation: 3.8471447949679103]
	TIME [epoch: 5.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.936478590963027		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 1.936478590963027 | validation: 3.809457486170152]
	TIME [epoch: 5.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.952191629436942		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 1.952191629436942 | validation: 3.795977775469259]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9422375601554878		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 1.9422375601554878 | validation: 3.8780849489941533]
	TIME [epoch: 5.73 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9803378023090972		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 1.9803378023090972 | validation: 3.820857952654068]
	TIME [epoch: 5.71 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9106370639574592		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 1.9106370639574592 | validation: 3.8150610191578394]
	TIME [epoch: 5.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9188142810871633		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 1.9188142810871633 | validation: 3.855399253859697]
	TIME [epoch: 5.71 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9366619403907732		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 1.9366619403907732 | validation: 3.8238767118491603]
	TIME [epoch: 5.71 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9177274069848553		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 1.9177274069848553 | validation: 3.8742904701553904]
	TIME [epoch: 5.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9611022864481868		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 1.9611022864481868 | validation: 3.8179331778153984]
	TIME [epoch: 5.75 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9160038404550452		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 1.9160038404550452 | validation: 3.8268231737178007]
	TIME [epoch: 5.71 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9586686637185335		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 1.9586686637185335 | validation: 3.8678654601072937]
	TIME [epoch: 5.71 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.1050416649713455		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 2.1050416649713455 | validation: 3.8349945686374367]
	TIME [epoch: 5.7 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9595287199226612		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 1.9595287199226612 | validation: 3.807755117274993]
	TIME [epoch: 5.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.912455466976303		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 1.912455466976303 | validation: 3.8317673578348685]
	TIME [epoch: 5.71 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9461145225368588		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 1.9461145225368588 | validation: 3.7911730905648153]
	TIME [epoch: 5.75 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_670.pth
	Model improved!!!
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9426598005185842		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 1.9426598005185842 | validation: 3.8205050182679976]
	TIME [epoch: 5.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9371310748417239		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 1.9371310748417239 | validation: 3.832059639875191]
	TIME [epoch: 5.71 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9371003225574368		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 1.9371003225574368 | validation: 3.8030762475141193]
	TIME [epoch: 5.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.912466802771921		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 1.912466802771921 | validation: 3.8040504653277982]
	TIME [epoch: 5.71 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9648623799074416		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 1.9648623799074416 | validation: 3.851887511257844]
	TIME [epoch: 5.71 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9305199806331736		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 1.9305199806331736 | validation: 3.8142265181268615]
	TIME [epoch: 5.73 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9333530983208211		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 1.9333530983208211 | validation: 3.7966870298048536]
	TIME [epoch: 5.7 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9197007708837837		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 1.9197007708837837 | validation: 3.8070719996425977]
	TIME [epoch: 5.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.922251188179634		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 1.922251188179634 | validation: 3.8525713919765447]
	TIME [epoch: 5.69 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9205439061841376		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 1.9205439061841376 | validation: 3.8284809036186447]
	TIME [epoch: 5.69 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9079071922925328		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 1.9079071922925328 | validation: 3.8371440542390736]
	TIME [epoch: 5.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9460324376357998		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 1.9460324376357998 | validation: 3.9251638387859726]
	TIME [epoch: 5.73 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9436847549969012		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 1.9436847549969012 | validation: 3.8127969991873365]
	TIME [epoch: 5.71 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9203240581840402		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 1.9203240581840402 | validation: 3.8058500506807853]
	TIME [epoch: 5.7 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9382752768899538		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 1.9382752768899538 | validation: 3.822202430168187]
	TIME [epoch: 5.69 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9164119104295514		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 1.9164119104295514 | validation: 3.8236176506747372]
	TIME [epoch: 5.69 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9437233026769625		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 1.9437233026769625 | validation: 3.806157658879464]
	TIME [epoch: 5.69 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.957331971921366		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 1.957331971921366 | validation: 3.8203962273059258]
	TIME [epoch: 5.72 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9194082753223094		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 1.9194082753223094 | validation: 3.8130833217914644]
	TIME [epoch: 5.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9357496137550405		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 1.9357496137550405 | validation: 3.8071198264587833]
	TIME [epoch: 5.69 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9101015893570104		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 1.9101015893570104 | validation: 3.8129833990654247]
	TIME [epoch: 5.71 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9149935670165412		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 1.9149935670165412 | validation: 3.8245399737247023]
	TIME [epoch: 5.69 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9097121757560382		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 1.9097121757560382 | validation: 3.8212793010229547]
	TIME [epoch: 5.71 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9090705389072185		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 1.9090705389072185 | validation: 3.82447863416662]
	TIME [epoch: 5.74 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9467377456590325		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 1.9467377456590325 | validation: 3.7993642109023757]
	TIME [epoch: 5.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9440646792819551		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 1.9440646792819551 | validation: 3.8179423930648753]
	TIME [epoch: 5.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9354690481659795		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 1.9354690481659795 | validation: 3.7935935364127795]
	TIME [epoch: 5.71 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.921287152618783		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 1.921287152618783 | validation: 3.8047641605439617]
	TIME [epoch: 5.69 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8943432585729563		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 1.8943432585729563 | validation: 3.8144415817152857]
	TIME [epoch: 5.71 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.941015128220005		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 1.941015128220005 | validation: 3.7982345982485737]
	TIME [epoch: 5.74 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.915101882651212		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 1.915101882651212 | validation: 3.835318658062032]
	TIME [epoch: 5.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9284996423732181		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 1.9284996423732181 | validation: 3.8023352742785]
	TIME [epoch: 5.71 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.91807948228946		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 1.91807948228946 | validation: 3.8139080122708195]
	TIME [epoch: 5.96 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9202948220696392		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 1.9202948220696392 | validation: 3.8003226908467727]
	TIME [epoch: 5.7 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.913749330620461		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 1.913749330620461 | validation: 3.7977234962352266]
	TIME [epoch: 5.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9041765681341323		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 1.9041765681341323 | validation: 3.80998904685855]
	TIME [epoch: 5.73 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.918489906514345		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 1.918489906514345 | validation: 3.850961545691488]
	TIME [epoch: 5.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9454772383162924		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 1.9454772383162924 | validation: 3.807296703010202]
	TIME [epoch: 5.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9431706062773024		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 1.9431706062773024 | validation: 3.9177112562002447]
	TIME [epoch: 5.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9555063480264394		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 1.9555063480264394 | validation: 3.820451742394786]
	TIME [epoch: 5.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.912311355022832		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 1.912311355022832 | validation: 3.810965315524486]
	TIME [epoch: 5.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9281706548823525		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 1.9281706548823525 | validation: 3.796603539712397]
	TIME [epoch: 5.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9174200051436288		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 1.9174200051436288 | validation: 3.8215075366772173]
	TIME [epoch: 5.72 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9340051782483378		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 1.9340051782483378 | validation: 3.827552733768231]
	TIME [epoch: 5.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9366324729250928		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 1.9366324729250928 | validation: 3.855321804998143]
	TIME [epoch: 5.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9279106369102865		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 1.9279106369102865 | validation: 3.790890417719485]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9043284719884987		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 1.9043284719884987 | validation: 3.8147063015275613]
	TIME [epoch: 5.7 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9126035215643216		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 1.9126035215643216 | validation: 3.8340125808293335]
	TIME [epoch: 5.73 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9299498317150001		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 1.9299498317150001 | validation: 3.7989072383927898]
	TIME [epoch: 5.72 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9096182459066449		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 1.9096182459066449 | validation: 3.786434349741699]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_720.pth
	Model improved!!!
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9291218357614408		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 1.9291218357614408 | validation: 3.8159441822748215]
	TIME [epoch: 5.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9812677149645572		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 1.9812677149645572 | validation: 3.802906097215906]
	TIME [epoch: 5.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.921597667717444		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 1.921597667717444 | validation: 3.888562982652344]
	TIME [epoch: 5.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9890939925475095		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 1.9890939925475095 | validation: 3.8202890419605033]
	TIME [epoch: 5.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9096843057349084		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 1.9096843057349084 | validation: 3.8221157959256016]
	TIME [epoch: 5.71 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9452978234686096		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 1.9452978234686096 | validation: 3.8063243411420222]
	TIME [epoch: 5.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9161172800529074		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 1.9161172800529074 | validation: 3.794127447114504]
	TIME [epoch: 5.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.908059996112093		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 1.908059996112093 | validation: 3.811296479977968]
	TIME [epoch: 5.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9077266943836246		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 1.9077266943836246 | validation: 3.8097063724586757]
	TIME [epoch: 5.7 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.920811995888282		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 1.920811995888282 | validation: 3.8264886953727375]
	TIME [epoch: 5.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9384388803832653		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 1.9384388803832653 | validation: 3.822068234172493]
	TIME [epoch: 5.71 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.921491673153662		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 1.921491673153662 | validation: 3.793655788645224]
	TIME [epoch: 5.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.905262441319131		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 1.905262441319131 | validation: 3.811413267008625]
	TIME [epoch: 5.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9100737614859193		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 1.9100737614859193 | validation: 3.8222924347778715]
	TIME [epoch: 5.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.944248473900333		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 1.944248473900333 | validation: 3.809263586042303]
	TIME [epoch: 5.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9149482092128458		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 1.9149482092128458 | validation: 3.796208111190465]
	TIME [epoch: 5.73 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9251268919291735		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 1.9251268919291735 | validation: 3.819994512922942]
	TIME [epoch: 5.71 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9104410039564477		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 1.9104410039564477 | validation: 3.8015501873365776]
	TIME [epoch: 5.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8995233071215867		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 1.8995233071215867 | validation: 3.805616166149404]
	TIME [epoch: 5.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9206121932435671		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 1.9206121932435671 | validation: 3.81519241168799]
	TIME [epoch: 5.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9132335247963976		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 1.9132335247963976 | validation: 3.8178085953148697]
	TIME [epoch: 5.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9463128755149195		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 1.9463128755149195 | validation: 3.791981099392159]
	TIME [epoch: 5.73 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9043780658523415		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 1.9043780658523415 | validation: 3.7743051389601363]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8983271553210932		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 1.8983271553210932 | validation: 3.7981525228201236]
	TIME [epoch: 5.71 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9018090263381247		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 1.9018090263381247 | validation: 3.793693816609224]
	TIME [epoch: 5.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9017298047896856		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 1.9017298047896856 | validation: 3.7839653014296584]
	TIME [epoch: 5.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9081161010571803		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 1.9081161010571803 | validation: 3.8043198598000747]
	TIME [epoch: 5.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9029986330181776		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 1.9029986330181776 | validation: 3.797450665780626]
	TIME [epoch: 5.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9009952472965455		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 1.9009952472965455 | validation: 3.7934640378798172]
	TIME [epoch: 5.71 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.892808016490603		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 1.892808016490603 | validation: 3.793456482815054]
	TIME [epoch: 5.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.904831443509448		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 1.904831443509448 | validation: 3.873504692812712]
	TIME [epoch: 5.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9452402805051576		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 1.9452402805051576 | validation: 3.7796927877724533]
	TIME [epoch: 5.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.887030768928069		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 1.887030768928069 | validation: 3.8355858326662706]
	TIME [epoch: 5.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9320296738956546		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 1.9320296738956546 | validation: 3.797288823339709]
	TIME [epoch: 5.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8925258615553		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 1.8925258615553 | validation: 3.7935734066616886]
	TIME [epoch: 5.71 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9014389334536717		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 1.9014389334536717 | validation: 3.822615033712028]
	TIME [epoch: 5.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9147168820632827		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 1.9147168820632827 | validation: 3.782026818188291]
	TIME [epoch: 5.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.905613116842011		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 1.905613116842011 | validation: 3.781471350323088]
	TIME [epoch: 5.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8972659458545489		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 1.8972659458545489 | validation: 3.8263385419599714]
	TIME [epoch: 5.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9266626837188605		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 1.9266626837188605 | validation: 3.7835994492853957]
	TIME [epoch: 5.73 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8991139339265666		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 1.8991139339265666 | validation: 3.8285144592727325]
	TIME [epoch: 5.71 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.915026596709911		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 1.915026596709911 | validation: 3.834932227662839]
	TIME [epoch: 5.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.910967569995784		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 1.910967569995784 | validation: 3.8182826994138646]
	TIME [epoch: 5.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.906577467078102		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 1.906577467078102 | validation: 3.783404561936198]
	TIME [epoch: 5.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.91665128530955		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 1.91665128530955 | validation: 3.7851042659068233]
	TIME [epoch: 5.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9129291145872613		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 1.9129291145872613 | validation: 3.7984544381790455]
	TIME [epoch: 5.73 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9017268724761307		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 1.9017268724761307 | validation: 3.810288902358109]
	TIME [epoch: 5.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9155538861741352		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 1.9155538861741352 | validation: 3.7942616694393507]
	TIME [epoch: 5.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9522657092056113		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 1.9522657092056113 | validation: 3.8248026235300996]
	TIME [epoch: 5.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9185329933043933		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 1.9185329933043933 | validation: 3.811050073719804]
	TIME [epoch: 5.71 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8992597379752525		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 1.8992597379752525 | validation: 3.806417222127031]
	TIME [epoch: 5.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8998042267046533		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 1.8998042267046533 | validation: 3.790042289301831]
	TIME [epoch: 5.73 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8953770560717136		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 1.8953770560717136 | validation: 3.802871050450124]
	TIME [epoch: 5.72 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.916788193797863		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 1.916788193797863 | validation: 3.780409606336669]
	TIME [epoch: 5.71 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8920929934955348		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 1.8920929934955348 | validation: 3.8021887392505835]
	TIME [epoch: 5.7 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9067858599825933		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 1.9067858599825933 | validation: 3.8361633086332323]
	TIME [epoch: 5.7 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9068319524642414		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 1.9068319524642414 | validation: 3.805861725316942]
	TIME [epoch: 5.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.899075449145926		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 1.899075449145926 | validation: 3.7980450789963176]
	TIME [epoch: 5.73 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.89001067142178		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 1.89001067142178 | validation: 3.8033406351558607]
	TIME [epoch: 5.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9496727376465706		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 1.9496727376465706 | validation: 3.789638740786673]
	TIME [epoch: 5.71 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9221305235624235		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 1.9221305235624235 | validation: 3.8593170295666086]
	TIME [epoch: 5.71 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.932280067255668		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 1.932280067255668 | validation: 3.784867268888578]
	TIME [epoch: 5.71 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.89902190356883		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 1.89902190356883 | validation: 3.776652805457336]
	TIME [epoch: 5.71 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8939511198933556		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 1.8939511198933556 | validation: 3.7838425389472343]
	TIME [epoch: 5.73 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9054565941666781		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 1.9054565941666781 | validation: 3.8615660571802004]
	TIME [epoch: 5.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9269641898085854		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 1.9269641898085854 | validation: 3.7916602425426413]
	TIME [epoch: 5.71 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9136692473766805		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 1.9136692473766805 | validation: 3.821175877353223]
	TIME [epoch: 5.71 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8996580723562206		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 1.8996580723562206 | validation: 3.7883454538644163]
	TIME [epoch: 5.71 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9029035477714533		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 1.9029035477714533 | validation: 3.785468865817413]
	TIME [epoch: 5.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8940360225880732		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 1.8940360225880732 | validation: 3.789881578302785]
	TIME [epoch: 5.73 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9007134859758696		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 1.9007134859758696 | validation: 3.8497066636467547]
	TIME [epoch: 5.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.926949951106348		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 1.926949951106348 | validation: 3.775668036493598]
	TIME [epoch: 5.71 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8912807657968016		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 1.8912807657968016 | validation: 3.7882755959738708]
	TIME [epoch: 5.71 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9175125931741266		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 1.9175125931741266 | validation: 3.9898310548409506]
	TIME [epoch: 5.7 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.0024843373921213		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 2.0024843373921213 | validation: 3.7747995906906686]
	TIME [epoch: 5.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8988046636317817		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 1.8988046636317817 | validation: 3.8603377888696704]
	TIME [epoch: 5.73 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9278809481780015		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 1.9278809481780015 | validation: 3.7996056140690717]
	TIME [epoch: 5.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9175469653098904		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 1.9175469653098904 | validation: 3.7923146117308764]
	TIME [epoch: 5.71 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9012924885433726		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 1.9012924885433726 | validation: 3.801820531611365]
	TIME [epoch: 5.71 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9050059920647184		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 1.9050059920647184 | validation: 3.8354425702186234]
	TIME [epoch: 5.71 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.909866861262099		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 1.909866861262099 | validation: 3.787675296169215]
	TIME [epoch: 5.71 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9018717443600888		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 1.9018717443600888 | validation: 3.783479365831974]
	TIME [epoch: 5.73 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.889971363763008		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 1.889971363763008 | validation: 3.775099914725914]
	TIME [epoch: 5.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8894050197607117		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 1.8894050197607117 | validation: 3.790359106685563]
	TIME [epoch: 5.71 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9048986154237522		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 1.9048986154237522 | validation: 3.8298000120408733]
	TIME [epoch: 5.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.926913972619043		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 1.926913972619043 | validation: 3.7893244656878595]
	TIME [epoch: 5.71 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9137481744814309		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 1.9137481744814309 | validation: 3.8238424858143607]
	TIME [epoch: 5.71 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9010696683303236		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 1.9010696683303236 | validation: 3.7805858084049397]
	TIME [epoch: 5.72 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9210812274766533		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 1.9210812274766533 | validation: 3.780496297434256]
	TIME [epoch: 5.73 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8933223504010195		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 1.8933223504010195 | validation: 3.7793666033490507]
	TIME [epoch: 5.71 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8945434111504271		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 1.8945434111504271 | validation: 3.8258340995788105]
	TIME [epoch: 5.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9057281420436922		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 1.9057281420436922 | validation: 3.788402491640611]
	TIME [epoch: 5.71 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.888658041391499		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 1.888658041391499 | validation: 3.780082234587252]
	TIME [epoch: 5.71 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8933580630365805		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 1.8933580630365805 | validation: 3.793926461121832]
	TIME [epoch: 5.72 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8991785186767374		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 1.8991785186767374 | validation: 3.7749406967316155]
	TIME [epoch: 5.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9144873089091416		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 1.9144873089091416 | validation: 3.779881273055273]
	TIME [epoch: 5.71 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.902850632417394		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 1.902850632417394 | validation: 3.7866550105705086]
	TIME [epoch: 5.71 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9132143890206375		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 1.9132143890206375 | validation: 3.81087350604117]
	TIME [epoch: 5.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8886566879925755		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 1.8886566879925755 | validation: 3.774811359519172]
	TIME [epoch: 5.71 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8941059613896978		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 1.8941059613896978 | validation: 3.789232420356068]
	TIME [epoch: 5.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9125456071346265		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 1.9125456071346265 | validation: 3.780392780354471]
	TIME [epoch: 5.73 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8919511085607488		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 1.8919511085607488 | validation: 3.82118080684103]
	TIME [epoch: 5.71 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9298050012193777		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 1.9298050012193777 | validation: 3.79193618134714]
	TIME [epoch: 5.71 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8992001034036707		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 1.8992001034036707 | validation: 3.794739261218108]
	TIME [epoch: 5.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.891891582198743		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 1.891891582198743 | validation: 3.8043637997822275]
	TIME [epoch: 5.71 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.898419190925753		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 1.898419190925753 | validation: 3.7725938266121597]
	TIME [epoch: 5.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8811535806708322		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 1.8811535806708322 | validation: 3.7744048258751874]
	TIME [epoch: 5.73 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9122133596180588		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 1.9122133596180588 | validation: 3.8740451698136833]
	TIME [epoch: 5.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9312799596690593		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 1.9312799596690593 | validation: 3.843201331549]
	TIME [epoch: 5.71 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9452083159679936		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 1.9452083159679936 | validation: 3.771428621899693]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_830.pth
	Model improved!!!
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8909829229108452		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 1.8909829229108452 | validation: 3.767329511896753]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.884010788531501		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 1.884010788531501 | validation: 3.7911202975505387]
	TIME [epoch: 5.73 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8909092170599586		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 1.8909092170599586 | validation: 3.787949965520169]
	TIME [epoch: 5.71 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8937528878824161		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 1.8937528878824161 | validation: 3.7700948212652605]
	TIME [epoch: 5.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8853538186813623		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 1.8853538186813623 | validation: 3.770910246037417]
	TIME [epoch: 5.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9131043634012976		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 1.9131043634012976 | validation: 3.8045839881645414]
	TIME [epoch: 5.69 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9328908010248496		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 1.9328908010248496 | validation: 3.8204386528591248]
	TIME [epoch: 5.69 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.916272225668587		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 1.916272225668587 | validation: 3.783621041587527]
	TIME [epoch: 5.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9112420598507998		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 1.9112420598507998 | validation: 3.8106540658990036]
	TIME [epoch: 5.71 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9012843659882683		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 1.9012843659882683 | validation: 3.7833250689043796]
	TIME [epoch: 5.71 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.89414616264345		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 1.89414616264345 | validation: 3.792320860087281]
	TIME [epoch: 5.71 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8982343727973077		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 1.8982343727973077 | validation: 3.7774371481945286]
	TIME [epoch: 5.71 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8948730606208752		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 1.8948730606208752 | validation: 3.784462478778913]
	TIME [epoch: 5.71 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8840390740453896		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 1.8840390740453896 | validation: 3.7764082638529395]
	TIME [epoch: 5.72 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8952118206299176		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 1.8952118206299176 | validation: 3.78418069360164]
	TIME [epoch: 5.7 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9183002716298705		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 1.9183002716298705 | validation: 3.771609058965705]
	TIME [epoch: 5.69 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9108247068604123		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 1.9108247068604123 | validation: 3.8267750235939957]
	TIME [epoch: 5.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9073240972693344		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 1.9073240972693344 | validation: 3.7749858570473145]
	TIME [epoch: 5.69 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9032843529897119		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 1.9032843529897119 | validation: 3.8185935582308304]
	TIME [epoch: 5.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9558101026735275		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 1.9558101026735275 | validation: 3.7910134678362533]
	TIME [epoch: 5.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.901749623102143		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 1.901749623102143 | validation: 3.7732466714097774]
	TIME [epoch: 5.71 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9091650203057418		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 1.9091650203057418 | validation: 3.7726168244011435]
	TIME [epoch: 5.69 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9078164180567803		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 1.9078164180567803 | validation: 3.7753800330280014]
	TIME [epoch: 5.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8953843217294217		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 1.8953843217294217 | validation: 3.770584178097428]
	TIME [epoch: 5.69 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8869959508805216		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 1.8869959508805216 | validation: 3.768996024658402]
	TIME [epoch: 5.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8909392651584631		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 1.8909392651584631 | validation: 3.7890715354563067]
	TIME [epoch: 5.72 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.906760318946105		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 1.906760318946105 | validation: 3.8022176454170675]
	TIME [epoch: 5.72 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8925010487758902		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 1.8925010487758902 | validation: 3.7861636443867224]
	TIME [epoch: 5.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9164783174973878		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 1.9164783174973878 | validation: 3.7965067410198663]
	TIME [epoch: 5.71 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.897688416790415		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 1.897688416790415 | validation: 3.783009193939012]
	TIME [epoch: 5.7 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8908085997462176		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 1.8908085997462176 | validation: 3.7776763276383902]
	TIME [epoch: 5.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.901797463844542		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 1.901797463844542 | validation: 3.7826674791047483]
	TIME [epoch: 5.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.889907945814174		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 1.889907945814174 | validation: 3.7794861601695415]
	TIME [epoch: 5.71 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.883827036173691		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 1.883827036173691 | validation: 3.7736917572112634]
	TIME [epoch: 5.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8838071922257806		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 1.8838071922257806 | validation: 3.788031561998053]
	TIME [epoch: 5.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8937149581723152		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 1.8937149581723152 | validation: 3.771022335957581]
	TIME [epoch: 5.69 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9034325173347886		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 1.9034325173347886 | validation: 3.7683588793524327]
	TIME [epoch: 5.71 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8866217038031887		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 1.8866217038031887 | validation: 3.7819611440732395]
	TIME [epoch: 5.73 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8872197898136025		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 1.8872197898136025 | validation: 3.8310651364130512]
	TIME [epoch: 5.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8999576444251383		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 1.8999576444251383 | validation: 3.7709987060006327]
	TIME [epoch: 5.72 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.88240241326382		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 1.88240241326382 | validation: 3.772555205847476]
	TIME [epoch: 5.71 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8855602131256581		[learning rate: 0.00054421]
	Learning Rate: 0.000544213
	LOSS [training: 1.8855602131256581 | validation: 3.7687342406717232]
	TIME [epoch: 5.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8815310771781661		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 1.8815310771781661 | validation: 3.7676817181651554]
	TIME [epoch: 5.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.882631569774761		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 1.882631569774761 | validation: 3.796759453264407]
	TIME [epoch: 5.73 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8848070379212605		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 1.8848070379212605 | validation: 3.7775098234046256]
	TIME [epoch: 5.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.906623500255941		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 1.906623500255941 | validation: 3.7767044440343778]
	TIME [epoch: 5.72 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8861972195134866		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 1.8861972195134866 | validation: 3.798822178851109]
	TIME [epoch: 5.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.894325142969015		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 1.894325142969015 | validation: 3.773724121121839]
	TIME [epoch: 5.71 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8950261895252296		[learning rate: 0.00053088]
	Learning Rate: 0.000530884
	LOSS [training: 1.8950261895252296 | validation: 3.7765292464192486]
	TIME [epoch: 5.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8841909689881158		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 1.8841909689881158 | validation: 3.8037678982144745]
	TIME [epoch: 5.73 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8877758893656682		[learning rate: 0.00052714]
	Learning Rate: 0.000527136
	LOSS [training: 1.8877758893656682 | validation: 3.766344434443722]
	TIME [epoch: 5.73 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9001592165290089		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 1.9001592165290089 | validation: 3.82009929330314]
	TIME [epoch: 5.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9076923655706133		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 1.9076923655706133 | validation: 3.8047001759852]
	TIME [epoch: 5.69 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8876787590327477		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 1.8876787590327477 | validation: 3.814784389913236]
	TIME [epoch: 5.69 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.892434070056048		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 1.892434070056048 | validation: 3.766560591809736]
	TIME [epoch: 5.69 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8815258866154145		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 1.8815258866154145 | validation: 3.7778761217787906]
	TIME [epoch: 5.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8891807812731156		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 1.8891807812731156 | validation: 3.794546783503047]
	TIME [epoch: 5.73 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.892264885445099		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 1.892264885445099 | validation: 3.774750346087543]
	TIME [epoch: 5.71 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8924210630551093		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 1.8924210630551093 | validation: 3.776066777108117]
	TIME [epoch: 5.71 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8849103118213302		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 1.8849103118213302 | validation: 3.7912529136361535]
	TIME [epoch: 5.71 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.895808784386968		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 1.895808784386968 | validation: 3.806169756481403]
	TIME [epoch: 5.69 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.888185021588852		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 1.888185021588852 | validation: 3.780447043014373]
	TIME [epoch: 5.7 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8774629972323411		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 1.8774629972323411 | validation: 3.7662267784194157]
	TIME [epoch: 5.74 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8825698573361263		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 1.8825698573361263 | validation: 3.764950154006076]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_894.pth
	Model improved!!!
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.874788306628761		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 1.874788306628761 | validation: 3.773767693844692]
	TIME [epoch: 5.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.88840927339495		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 1.88840927339495 | validation: 3.760524003357826]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_896.pth
	Model improved!!!
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8821360271087717		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 1.8821360271087717 | validation: 3.7658285529214526]
	TIME [epoch: 5.7 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8856627642825097		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 1.8856627642825097 | validation: 3.766900167109672]
	TIME [epoch: 5.75 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8751596470498653		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 1.8751596470498653 | validation: 3.780646650392073]
	TIME [epoch: 5.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8925891451934138		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 1.8925891451934138 | validation: 3.774425527702017]
	TIME [epoch: 5.71 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8811220276420249		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 1.8811220276420249 | validation: 3.777957156651756]
	TIME [epoch: 5.69 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9120885147477975		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 1.9120885147477975 | validation: 3.782246079113783]
	TIME [epoch: 5.71 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8957482713722726		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 1.8957482713722726 | validation: 3.847087469104332]
	TIME [epoch: 5.71 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.918262822777929		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 1.918262822777929 | validation: 3.7778164970714987]
	TIME [epoch: 5.75 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8839546619342142		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 1.8839546619342142 | validation: 3.7666547633495835]
	TIME [epoch: 5.72 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.876257686640984		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 1.876257686640984 | validation: 3.7758782488640463]
	TIME [epoch: 5.71 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8888328943326644		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 1.8888328943326644 | validation: 3.7771567644466972]
	TIME [epoch: 5.71 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.881772166126436		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 1.881772166126436 | validation: 3.7710189447108156]
	TIME [epoch: 5.71 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8773391108234654		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 1.8773391108234654 | validation: 3.763096661389885]
	TIME [epoch: 5.71 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8724813646922054		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 1.8724813646922054 | validation: 3.7634828949821344]
	TIME [epoch: 5.74 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9080497547789759		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 1.9080497547789759 | validation: 3.801619631837675]
	TIME [epoch: 5.72 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8922040140121592		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 1.8922040140121592 | validation: 3.7810215975241155]
	TIME [epoch: 5.71 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8952938837377469		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 1.8952938837377469 | validation: 3.770435645735046]
	TIME [epoch: 5.7 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.886249906497549		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 1.886249906497549 | validation: 3.7657509650051693]
	TIME [epoch: 5.71 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.87766683919363		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 1.87766683919363 | validation: 3.7634297651238677]
	TIME [epoch: 5.71 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8765728654815632		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 1.8765728654815632 | validation: 3.7935889010055632]
	TIME [epoch: 5.74 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8912800395500007		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 1.8912800395500007 | validation: 3.7707342656060803]
	TIME [epoch: 5.72 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8905131929341654		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 1.8905131929341654 | validation: 3.772306423838242]
	TIME [epoch: 5.71 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8923911092812147		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 1.8923911092812147 | validation: 3.785789039758596]
	TIME [epoch: 5.71 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8948377851473728		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 1.8948377851473728 | validation: 3.7859883081158414]
	TIME [epoch: 5.71 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.885442731895388		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 1.885442731895388 | validation: 3.7650754884001527]
	TIME [epoch: 5.71 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8855241046652658		[learning rate: 0.00045588]
	Learning Rate: 0.000455875
	LOSS [training: 1.8855241046652658 | validation: 3.7752613661844965]
	TIME [epoch: 5.74 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.885430020631126		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 1.885430020631126 | validation: 3.751040745178481]
	TIME [epoch: 5.72 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8689433388053045		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 1.8689433388053045 | validation: 3.7672387457899696]
	TIME [epoch: 5.7 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8893125650382263		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 1.8893125650382263 | validation: 3.7714325990276802]
	TIME [epoch: 5.7 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8875131985299913		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 1.8875131985299913 | validation: 3.7576931550553208]
	TIME [epoch: 5.7 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8823649600438275		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 1.8823649600438275 | validation: 3.786052114924179]
	TIME [epoch: 5.69 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8935500157647809		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 1.8935500157647809 | validation: 3.7781568470592073]
	TIME [epoch: 5.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.898460337652816		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 1.898460337652816 | validation: 3.767612744239818]
	TIME [epoch: 5.7 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8837185807823724		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 1.8837185807823724 | validation: 3.7661409351958386]
	TIME [epoch: 5.69 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.878428630981828		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 1.878428630981828 | validation: 3.762868290203881]
	TIME [epoch: 5.7 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9079414278500777		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 1.9079414278500777 | validation: 3.7777950515433587]
	TIME [epoch: 5.7 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8939749889549633		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 1.8939749889549633 | validation: 3.770105439648751]
	TIME [epoch: 5.7 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.887240669648172		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 1.887240669648172 | validation: 3.755016298338441]
	TIME [epoch: 5.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8869236961194542		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 1.8869236961194542 | validation: 3.7679710004473095]
	TIME [epoch: 5.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8831014869190967		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 1.8831014869190967 | validation: 3.76197969754899]
	TIME [epoch: 5.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.884529239908834		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 1.884529239908834 | validation: 3.765929566626104]
	TIME [epoch: 5.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8769316983508704		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 1.8769316983508704 | validation: 3.7616386175857883]
	TIME [epoch: 5.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8761384488711332		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 1.8761384488711332 | validation: 3.7691173667549256]
	TIME [epoch: 5.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8827098129188873		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 1.8827098129188873 | validation: 3.7651119864149383]
	TIME [epoch: 5.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8759474858144582		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 1.8759474858144582 | validation: 3.773602194350001]
	TIME [epoch: 5.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8995636878258608		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 1.8995636878258608 | validation: 3.778228487004923]
	TIME [epoch: 5.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8785540405130443		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 1.8785540405130443 | validation: 3.76041896009491]
	TIME [epoch: 5.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8864509739048578		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 1.8864509739048578 | validation: 3.773570399251066]
	TIME [epoch: 5.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8835734093631717		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 1.8835734093631717 | validation: 3.770931637455218]
	TIME [epoch: 5.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8955678992824592		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 1.8955678992824592 | validation: 3.75631902788272]
	TIME [epoch: 5.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8801460673595018		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 1.8801460673595018 | validation: 3.76036051466883]
	TIME [epoch: 5.71 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8765074379089224		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 1.8765074379089224 | validation: 3.8029772593084057]
	TIME [epoch: 5.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9415967879630716		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 1.9415967879630716 | validation: 3.796647128769003]
	TIME [epoch: 5.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.887253084341451		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 1.887253084341451 | validation: 3.756586719668693]
	TIME [epoch: 5.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8787163811536558		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 1.8787163811536558 | validation: 3.7699689107415466]
	TIME [epoch: 5.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8801282022508468		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 1.8801282022508468 | validation: 3.755625370579868]
	TIME [epoch: 5.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8749077073007523		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 1.8749077073007523 | validation: 3.7639501933580757]
	TIME [epoch: 5.71 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8769530453187528		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 1.8769530453187528 | validation: 3.7945213758443663]
	TIME [epoch: 5.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.890812008811902		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 1.890812008811902 | validation: 3.7611994929545816]
	TIME [epoch: 5.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8914466251578457		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 1.8914466251578457 | validation: 3.7676872599507214]
	TIME [epoch: 5.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.873647603236255		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 1.873647603236255 | validation: 3.7591537474862657]
	TIME [epoch: 5.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8716322136095076		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 1.8716322136095076 | validation: 3.7653606714455363]
	TIME [epoch: 5.73 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.884272197704836		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 1.884272197704836 | validation: 3.76522959478675]
	TIME [epoch: 5.71 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8749420356505666		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 1.8749420356505666 | validation: 3.759423046508524]
	TIME [epoch: 5.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8874581956383405		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 1.8874581956383405 | validation: 3.795322999447526]
	TIME [epoch: 5.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.882970537244874		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 1.882970537244874 | validation: 3.761642203013831]
	TIME [epoch: 5.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8881710745014153		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 1.8881710745014153 | validation: 3.786016388858809]
	TIME [epoch: 5.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8873602193723613		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 1.8873602193723613 | validation: 3.7636482400484095]
	TIME [epoch: 5.73 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.889605724464272		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 1.889605724464272 | validation: 3.7631369676688156]
	TIME [epoch: 5.71 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8741535323683203		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 1.8741535323683203 | validation: 3.7735525048348575]
	TIME [epoch: 5.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8741690857578446		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 1.8741690857578446 | validation: 3.7610387999393424]
	TIME [epoch: 5.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.88095869697698		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 1.88095869697698 | validation: 3.7635226235953256]
	TIME [epoch: 5.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.875430142881973		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 1.875430142881973 | validation: 3.7593599408933493]
	TIME [epoch: 5.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8784582671578585		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 1.8784582671578585 | validation: 3.7691684677821677]
	TIME [epoch: 5.73 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8848922908326653		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 1.8848922908326653 | validation: 3.761587371367838]
	TIME [epoch: 5.71 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8801570347693044		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 1.8801570347693044 | validation: 3.7698356980913177]
	TIME [epoch: 5.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8937182407233553		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 1.8937182407233553 | validation: 3.7801421803320046]
	TIME [epoch: 5.7 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8843364528715614		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 1.8843364528715614 | validation: 3.778124272486812]
	TIME [epoch: 5.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.870768122496995		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 1.870768122496995 | validation: 3.7574564312771828]
	TIME [epoch: 5.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8738575232894044		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 1.8738575232894044 | validation: 3.778194558085328]
	TIME [epoch: 5.73 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.881282276111015		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 1.881282276111015 | validation: 3.750233243425655]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8688687644830715		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 1.8688687644830715 | validation: 3.7550003146948905]
	TIME [epoch: 5.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8832982063272565		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 1.8832982063272565 | validation: 3.768502060386178]
	TIME [epoch: 5.69 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8839041165456618		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 1.8839041165456618 | validation: 3.758877982331885]
	TIME [epoch: 5.69 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.879827009124787		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 1.879827009124787 | validation: 3.7648164066280447]
	TIME [epoch: 5.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.884380772175149		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 1.884380772175149 | validation: 3.7502731198182135]
	TIME [epoch: 5.72 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.875811683213536		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 1.875811683213536 | validation: 3.752392815379114]
	TIME [epoch: 5.71 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8714660962315306		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 1.8714660962315306 | validation: 3.7693861552830743]
	TIME [epoch: 5.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8697905262671686		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 1.8697905262671686 | validation: 3.756993531546225]
	TIME [epoch: 5.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8789814512399676		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 1.8789814512399676 | validation: 3.762316922479528]
	TIME [epoch: 5.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.877953314927759		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 1.877953314927759 | validation: 3.7576840657275263]
	TIME [epoch: 5.69 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8716195589269122		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 1.8716195589269122 | validation: 3.75604468441543]
	TIME [epoch: 5.72 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.881048429153024		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 1.881048429153024 | validation: 3.77416274990347]
	TIME [epoch: 5.71 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8722611423891573		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 1.8722611423891573 | validation: 3.778130694055373]
	TIME [epoch: 5.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9177966650899598		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 1.9177966650899598 | validation: 3.764823613779684]
	TIME [epoch: 5.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8771772019246071		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 1.8771772019246071 | validation: 3.7632841824424976]
	TIME [epoch: 5.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8950662781568073		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 1.8950662781568073 | validation: 3.7641584212447676]
	TIME [epoch: 5.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8750654471723012		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 1.8750654471723012 | validation: 3.762957372201052]
	TIME [epoch: 5.73 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8808169867714675		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 1.8808169867714675 | validation: 3.7581683712521623]
	TIME [epoch: 5.71 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.866978228215706		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 1.866978228215706 | validation: 3.758001263834611]
	TIME [epoch: 5.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.88415822491734		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 1.88415822491734 | validation: 3.7762412681105446]
	TIME [epoch: 5.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.879717986911591		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 1.879717986911591 | validation: 3.763805477834945]
	TIME [epoch: 5.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8736786460162842		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 1.8736786460162842 | validation: 3.7575464020621623]
	TIME [epoch: 5.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8721025191374896		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 1.8721025191374896 | validation: 3.763870275204506]
	TIME [epoch: 5.72 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.882668141257046		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 1.882668141257046 | validation: 3.7949861670224507]
	TIME [epoch: 5.71 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8784629475046632		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 1.8784629475046632 | validation: 3.7483263929749127]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8743705771734136		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 1.8743705771734136 | validation: 3.7682079394495163]
	TIME [epoch: 5.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8837861526937887		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 1.8837861526937887 | validation: 3.7508132137892973]
	TIME [epoch: 5.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8743083881870448		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 1.8743083881870448 | validation: 3.8163494238530906]
	TIME [epoch: 5.69 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9061029712900686		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 1.9061029712900686 | validation: 3.7804819847615385]
	TIME [epoch: 5.73 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.892098348045221		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 1.892098348045221 | validation: 3.7545832986516503]
	TIME [epoch: 5.72 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8758493922417028		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 1.8758493922417028 | validation: 3.7780692622370955]
	TIME [epoch: 5.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8943454592652116		[learning rate: 0.00033497]
	Learning Rate: 0.000334965
	LOSS [training: 1.8943454592652116 | validation: 3.7685371475586114]
	TIME [epoch: 5.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8829090399096233		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 1.8829090399096233 | validation: 3.7888174029024113]
	TIME [epoch: 5.69 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8849680796215296		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 1.8849680796215296 | validation: 3.7565817891952293]
	TIME [epoch: 5.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8742731537331987		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 1.8742731537331987 | validation: 3.768978961071602]
	TIME [epoch: 5.73 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8901130194774505		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 1.8901130194774505 | validation: 3.7613463968415557]
	TIME [epoch: 5.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.872296082154247		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 1.872296082154247 | validation: 3.75749784379922]
	TIME [epoch: 5.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.870470366527565		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 1.870470366527565 | validation: 3.757601627923242]
	TIME [epoch: 5.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8747492522349014		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 1.8747492522349014 | validation: 3.764620084917812]
	TIME [epoch: 5.71 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8742071692825601		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 1.8742071692825601 | validation: 3.7604976228717324]
	TIME [epoch: 5.69 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.872679654024916		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 1.872679654024916 | validation: 3.769337994146896]
	TIME [epoch: 5.74 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8849707471253698		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 1.8849707471253698 | validation: 3.753335348191118]
	TIME [epoch: 5.72 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.872941499304233		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 1.872941499304233 | validation: 3.761703258546953]
	TIME [epoch: 5.71 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8680674124132255		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 1.8680674124132255 | validation: 3.76583883354652]
	TIME [epoch: 5.69 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8829175501508784		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 1.8829175501508784 | validation: 3.748795624014338]
	TIME [epoch: 5.69 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8709896795798269		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 1.8709896795798269 | validation: 3.7638389245883914]
	TIME [epoch: 5.69 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8782986876785719		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 1.8782986876785719 | validation: 3.788138502447458]
	TIME [epoch: 5.72 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.90158463125999		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 1.90158463125999 | validation: 3.76873467931644]
	TIME [epoch: 5.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8827596890001737		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 1.8827596890001737 | validation: 3.7629821216530877]
	TIME [epoch: 5.69 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.871463582032057		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 1.871463582032057 | validation: 3.7548880958879547]
	TIME [epoch: 5.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.874543944895111		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 1.874543944895111 | validation: 3.7436510611716667]
	TIME [epoch: 5.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1028.pth
	Model improved!!!
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8843481128186355		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 1.8843481128186355 | validation: 3.770564249567041]
	TIME [epoch: 5.69 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8713549813868844		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 1.8713549813868844 | validation: 3.7534379175331924]
	TIME [epoch: 5.72 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9022059051703084		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 1.9022059051703084 | validation: 3.7856551196799444]
	TIME [epoch: 5.72 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8927962851669422		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 1.8927962851669422 | validation: 3.7559724039994022]
	TIME [epoch: 5.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8717445131259816		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 1.8717445131259816 | validation: 3.7744497041109333]
	TIME [epoch: 5.71 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.89490571116983		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 1.89490571116983 | validation: 3.7546427242677294]
	TIME [epoch: 5.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8783609827010248		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 1.8783609827010248 | validation: 3.780499171273059]
	TIME [epoch: 5.71 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.880198147837492		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 1.880198147837492 | validation: 3.750886417582127]
	TIME [epoch: 5.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8750822182202034		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 1.8750822182202034 | validation: 3.7551528276103414]
	TIME [epoch: 5.72 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8731234928783573		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 1.8731234928783573 | validation: 3.769693297818411]
	TIME [epoch: 5.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.869058274185579		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 1.869058274185579 | validation: 3.7589370070173063]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8684239814500299		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 1.8684239814500299 | validation: 3.7626738735312415]
	TIME [epoch: 5.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8746673128366875		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 1.8746673128366875 | validation: 3.782923746069965]
	TIME [epoch: 5.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8895886091497538		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 1.8895886091497538 | validation: 3.7478884589132564]
	TIME [epoch: 5.73 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.87061166581793		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 1.87061166581793 | validation: 3.7522894652063368]
	TIME [epoch: 5.71 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8828915011109468		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 1.8828915011109468 | validation: 3.765951363943476]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8807272943766413		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 1.8807272943766413 | validation: 3.763396413468692]
	TIME [epoch: 5.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8793763560576007		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 1.8793763560576007 | validation: 3.7593495052044883]
	TIME [epoch: 5.71 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8708866148865622		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 1.8708866148865622 | validation: 3.755392065738775]
	TIME [epoch: 5.71 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.866256527781247		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 1.866256527781247 | validation: 3.748192152360496]
	TIME [epoch: 5.74 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.868604593202855		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 1.868604593202855 | validation: 3.7506096391920223]
	TIME [epoch: 5.73 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8715516436655781		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 1.8715516436655781 | validation: 3.747784293741869]
	TIME [epoch: 5.71 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8664162052714408		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 1.8664162052714408 | validation: 3.7424719850783674]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1051.pth
	Model improved!!!
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8673718142153852		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 1.8673718142153852 | validation: 3.7497430371036002]
	TIME [epoch: 5.71 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8821289515832018		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 1.8821289515832018 | validation: 3.766605236542097]
	TIME [epoch: 5.71 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.9011555843351111		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 1.9011555843351111 | validation: 3.7887867946188143]
	TIME [epoch: 5.72 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8918401192493128		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 1.8918401192493128 | validation: 3.751461051322583]
	TIME [epoch: 5.72 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8650503176957067		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 1.8650503176957067 | validation: 3.7526735626106946]
	TIME [epoch: 5.69 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8698820898021329		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 1.8698820898021329 | validation: 3.7591720072318493]
	TIME [epoch: 5.71 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.870839569394048		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 1.870839569394048 | validation: 3.7633926365734323]
	TIME [epoch: 5.69 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8782398859216185		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 1.8782398859216185 | validation: 3.751924292632302]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8691729197097908		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 1.8691729197097908 | validation: 3.7657629311261775]
	TIME [epoch: 5.74 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8710326560584523		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 1.8710326560584523 | validation: 3.7659835374545616]
	TIME [epoch: 5.72 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8816807543587954		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 1.8816807543587954 | validation: 3.7504596408015773]
	TIME [epoch: 5.71 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8668523914389816		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 1.8668523914389816 | validation: 3.765136386390092]
	TIME [epoch: 5.71 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8696610347013891		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 1.8696610347013891 | validation: 3.7604071434342705]
	TIME [epoch: 5.71 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8663650303946846		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 1.8663650303946846 | validation: 3.7488387093746103]
	TIME [epoch: 5.69 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8854141228514227		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 1.8854141228514227 | validation: 3.7790228087310243]
	TIME [epoch: 5.74 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8736998598158947		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 1.8736998598158947 | validation: 3.747850640224657]
	TIME [epoch: 5.72 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8748334482248583		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 1.8748334482248583 | validation: 3.7586213510571094]
	TIME [epoch: 5.71 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8795530495312507		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 1.8795530495312507 | validation: 3.769849545891478]
	TIME [epoch: 5.69 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8806144717557474		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 1.8806144717557474 | validation: 3.765208091078921]
	TIME [epoch: 5.71 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8676602493186243		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 1.8676602493186243 | validation: 3.7586598277418135]
	TIME [epoch: 5.69 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.875083621084915		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 1.875083621084915 | validation: 3.761384322301049]
	TIME [epoch: 5.74 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8734657934253838		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 1.8734657934253838 | validation: 3.7676802186072003]
	TIME [epoch: 5.71 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862997474046444		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 1.862997474046444 | validation: 3.749417580260622]
	TIME [epoch: 5.71 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8704635346316911		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 1.8704635346316911 | validation: 3.757465843714557]
	TIME [epoch: 5.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8649081111766757		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 1.8649081111766757 | validation: 3.762039079561696]
	TIME [epoch: 5.69 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8692697971740437		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 1.8692697971740437 | validation: 3.757222950078116]
	TIME [epoch: 5.71 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.87930346530108		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 1.87930346530108 | validation: 3.752987935313663]
	TIME [epoch: 5.74 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.873118224298957		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 1.873118224298957 | validation: 3.7657674301130544]
	TIME [epoch: 5.72 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8805426462546644		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 1.8805426462546644 | validation: 3.7563122253138386]
	TIME [epoch: 5.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.863954752623731		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 1.863954752623731 | validation: 3.749678908436919]
	TIME [epoch: 5.69 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8699237676953977		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 1.8699237676953977 | validation: 3.75707635586152]
	TIME [epoch: 5.69 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8671062866475405		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 1.8671062866475405 | validation: 3.756596181570718]
	TIME [epoch: 5.69 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.87845870432924		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 1.87845870432924 | validation: 3.7537520249428526]
	TIME [epoch: 5.72 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8663295767855763		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 1.8663295767855763 | validation: 3.758478319028891]
	TIME [epoch: 5.72 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8703376978038944		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 1.8703376978038944 | validation: 3.749445085739566]
	TIME [epoch: 5.71 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8658801667918743		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 1.8658801667918743 | validation: 3.7558513071664232]
	TIME [epoch: 5.71 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.867390111191277		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 1.867390111191277 | validation: 3.7471037891141123]
	TIME [epoch: 5.71 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.867891371110953		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 1.867891371110953 | validation: 3.7492247019027842]
	TIME [epoch: 5.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8602580021232304		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 1.8602580021232304 | validation: 3.752739648572789]
	TIME [epoch: 5.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8677962687603167		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 1.8677962687603167 | validation: 3.7531975550864902]
	TIME [epoch: 5.71 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8701427735808518		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 1.8701427735808518 | validation: 3.7624901302114218]
	TIME [epoch: 5.69 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8687620091829136		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 1.8687620091829136 | validation: 3.7535527797157227]
	TIME [epoch: 5.69 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8671042357884144		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 1.8671042357884144 | validation: 3.7505000139380713]
	TIME [epoch: 5.69 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.868469327706952		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 1.868469327706952 | validation: 3.7712119691846304]
	TIME [epoch: 5.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8737148043116025		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 1.8737148043116025 | validation: 3.751431933750063]
	TIME [epoch: 5.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8716011848586498		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 1.8716011848586498 | validation: 3.7702445449402355]
	TIME [epoch: 5.72 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8694312923194631		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 1.8694312923194631 | validation: 3.755250757116624]
	TIME [epoch: 5.69 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8634392088616274		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 1.8634392088616274 | validation: 3.7559794405872116]
	TIME [epoch: 5.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8725598114004782		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 1.8725598114004782 | validation: 3.7550799966229675]
	TIME [epoch: 5.69 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8677301150848507		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 1.8677301150848507 | validation: 3.7379407363840165]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1101.pth
	Model improved!!!
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8665095512816183		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 1.8665095512816183 | validation: 3.752013227008849]
	TIME [epoch: 5.72 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.873505890725055		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 1.873505890725055 | validation: 3.7499192954580116]
	TIME [epoch: 5.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8700540076769727		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 1.8700540076769727 | validation: 3.767758612065277]
	TIME [epoch: 5.69 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8713796715636937		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 1.8713796715636937 | validation: 3.7453153861733224]
	TIME [epoch: 5.69 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8633438825304107		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 1.8633438825304107 | validation: 3.7576649044146153]
	TIME [epoch: 5.69 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.881257938062924		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 1.881257938062924 | validation: 3.7475719809698957]
	TIME [epoch: 5.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865809901180458		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 1.865809901180458 | validation: 3.7620974298066114]
	TIME [epoch: 5.72 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8756816551412663		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 1.8756816551412663 | validation: 3.7516657696272198]
	TIME [epoch: 5.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8683638523362465		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 1.8683638523362465 | validation: 3.7429282642617103]
	TIME [epoch: 5.69 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.86019880571827		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 1.86019880571827 | validation: 3.7458883258785454]
	TIME [epoch: 5.71 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.864741427491598		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 1.864741427491598 | validation: 3.747648712305062]
	TIME [epoch: 5.71 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.86437697993407		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 1.86437697993407 | validation: 3.7528144287742737]
	TIME [epoch: 5.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.869578101513393		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 1.869578101513393 | validation: 3.751701293719528]
	TIME [epoch: 5.72 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8618020295647164		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 1.8618020295647164 | validation: 3.7479694381247444]
	TIME [epoch: 5.72 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8702068855340428		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 1.8702068855340428 | validation: 3.754903748521396]
	TIME [epoch: 5.69 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8680892522063544		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 1.8680892522063544 | validation: 3.7611331615601142]
	TIME [epoch: 5.69 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.870933167635146		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 1.870933167635146 | validation: 3.7470495409086424]
	TIME [epoch: 5.69 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.867814209730607		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 1.867814209730607 | validation: 3.741649668754817]
	TIME [epoch: 5.69 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8686827842089375		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 1.8686827842089375 | validation: 3.763280445771424]
	TIME [epoch: 5.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8887012684359303		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 1.8887012684359303 | validation: 3.773538383059522]
	TIME [epoch: 5.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8803603266529623		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 1.8803603266529623 | validation: 3.7548993755923084]
	TIME [epoch: 5.69 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8648281784385965		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 1.8648281784385965 | validation: 3.7577167882265683]
	TIME [epoch: 5.69 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8694190469279834		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 1.8694190469279834 | validation: 3.7529786226670545]
	TIME [epoch: 5.71 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8665317423827332		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 1.8665317423827332 | validation: 3.7891892539698917]
	TIME [epoch: 5.71 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.907893254631818		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 1.907893254631818 | validation: 3.773243924421503]
	TIME [epoch: 5.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8706694731510694		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 1.8706694731510694 | validation: 3.758181975679057]
	TIME [epoch: 5.72 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8679916519862831		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 1.8679916519862831 | validation: 3.765092403240408]
	TIME [epoch: 5.69 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8731091791382886		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 1.8731091791382886 | validation: 3.7661665332768077]
	TIME [epoch: 5.69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8672522216674514		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 1.8672522216674514 | validation: 3.7523592529455043]
	TIME [epoch: 5.69 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.873212253368469		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 1.873212253368469 | validation: 3.746950183872034]
	TIME [epoch: 5.71 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8809778819205436		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 1.8809778819205436 | validation: 3.7450676890961994]
	TIME [epoch: 5.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8633127688872158		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 1.8633127688872158 | validation: 3.7492624952774976]
	TIME [epoch: 5.72 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8638941872215782		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 1.8638941872215782 | validation: 3.7515863218432037]
	TIME [epoch: 5.69 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8678669127282939		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 1.8678669127282939 | validation: 3.763072003709972]
	TIME [epoch: 5.69 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.868347675479265		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 1.868347675479265 | validation: 3.752068096728458]
	TIME [epoch: 5.69 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8661375265095579		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 1.8661375265095579 | validation: 3.7487413589808334]
	TIME [epoch: 5.69 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8705940457453927		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 1.8705940457453927 | validation: 3.7681313443901314]
	TIME [epoch: 5.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.884242225028286		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 1.884242225028286 | validation: 3.7525729343467953]
	TIME [epoch: 5.72 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8690652064087687		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 1.8690652064087687 | validation: 3.7474624248970803]
	TIME [epoch: 5.69 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.879754124160014		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 1.879754124160014 | validation: 3.755378003734206]
	TIME [epoch: 5.71 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8739466487349867		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 1.8739466487349867 | validation: 3.747683391731251]
	TIME [epoch: 5.69 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8680954277426496		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 1.8680954277426496 | validation: 3.745219118633808]
	TIME [epoch: 5.69 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.878133297425524		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 1.878133297425524 | validation: 3.7625434271329836]
	TIME [epoch: 5.72 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8930114935406066		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 1.8930114935406066 | validation: 3.7520226226192235]
	TIME [epoch: 5.73 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8809154261763623		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 1.8809154261763623 | validation: 3.7477106843696437]
	TIME [epoch: 5.71 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8647517117380594		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 1.8647517117380594 | validation: 3.752405913027836]
	TIME [epoch: 5.71 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8641310074612134		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 1.8641310074612134 | validation: 3.753398047035327]
	TIME [epoch: 5.69 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8684926814769		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 1.8684926814769 | validation: 3.747349157572849]
	TIME [epoch: 5.71 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8652449401517224		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 1.8652449401517224 | validation: 3.7448201453628758]
	TIME [epoch: 5.72 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8644804589784019		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 1.8644804589784019 | validation: 3.744413115643174]
	TIME [epoch: 5.74 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861015625421479		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 1.861015625421479 | validation: 3.7479739108845713]
	TIME [epoch: 5.69 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862108008326278		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 1.862108008326278 | validation: 3.7393045179863833]
	TIME [epoch: 5.69 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8640555885283985		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 1.8640555885283985 | validation: 3.7406602400576014]
	TIME [epoch: 5.69 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865608814810352		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 1.865608814810352 | validation: 3.7431052605860864]
	TIME [epoch: 5.69 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8623566679180208		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 1.8623566679180208 | validation: 3.758748497553412]
	TIME [epoch: 5.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8789512266398123		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 1.8789512266398123 | validation: 3.761097838006348]
	TIME [epoch: 5.73 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8641709749353939		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 1.8641709749353939 | validation: 3.7410780655088196]
	TIME [epoch: 5.71 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8629595361395213		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 1.8629595361395213 | validation: 3.7571363567693643]
	TIME [epoch: 5.71 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8777918059774734		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 1.8777918059774734 | validation: 3.746306920252659]
	TIME [epoch: 5.71 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8602754356833708		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 1.8602754356833708 | validation: 3.751183584831442]
	TIME [epoch: 5.69 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8618866987147622		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 1.8618866987147622 | validation: 3.755643566259705]
	TIME [epoch: 5.72 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.875812895926441		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 1.875812895926441 | validation: 3.7468983443371506]
	TIME [epoch: 5.74 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8722499292774424		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 1.8722499292774424 | validation: 3.7678251716354954]
	TIME [epoch: 5.71 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8674095776551136		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 1.8674095776551136 | validation: 3.7446971902418165]
	TIME [epoch: 5.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8656838334038328		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 1.8656838334038328 | validation: 3.7484895667167457]
	TIME [epoch: 5.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.866892399705447		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 1.866892399705447 | validation: 3.742640667126459]
	TIME [epoch: 5.71 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8707416096892087		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 1.8707416096892087 | validation: 3.756114063765306]
	TIME [epoch: 5.7 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8779843789145405		[learning rate: 0.00019004]
	Learning Rate: 0.00019004
	LOSS [training: 1.8779843789145405 | validation: 3.755811592561156]
	TIME [epoch: 5.72 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865620612501001		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 1.865620612501001 | validation: 3.7358358315940237]
	TIME [epoch: 5.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1170.pth
	Model improved!!!
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8614364465071287		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 1.8614364465071287 | validation: 3.7430060338035203]
	TIME [epoch: 5.71 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8674053457078155		[learning rate: 0.00018803]
	Learning Rate: 0.000188032
	LOSS [training: 1.8674053457078155 | validation: 3.755820806067685]
	TIME [epoch: 5.71 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8777334080336803		[learning rate: 0.00018737]
	Learning Rate: 0.000187367
	LOSS [training: 1.8777334080336803 | validation: 3.7542388581989954]
	TIME [epoch: 5.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8707867698138623		[learning rate: 0.0001867]
	Learning Rate: 0.000186704
	LOSS [training: 1.8707867698138623 | validation: 3.751089260241739]
	TIME [epoch: 5.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8621445329051585		[learning rate: 0.00018604]
	Learning Rate: 0.000186044
	LOSS [training: 1.8621445329051585 | validation: 3.746545042089189]
	TIME [epoch: 5.72 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8570872408827122		[learning rate: 0.00018539]
	Learning Rate: 0.000185386
	LOSS [training: 1.8570872408827122 | validation: 3.7480104774971723]
	TIME [epoch: 5.72 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8694504963524023		[learning rate: 0.00018473]
	Learning Rate: 0.00018473
	LOSS [training: 1.8694504963524023 | validation: 3.7671603641680806]
	TIME [epoch: 5.71 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8684511239931865		[learning rate: 0.00018408]
	Learning Rate: 0.000184077
	LOSS [training: 1.8684511239931865 | validation: 3.7407133720195227]
	TIME [epoch: 5.71 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8659644258722363		[learning rate: 0.00018343]
	Learning Rate: 0.000183426
	LOSS [training: 1.8659644258722363 | validation: 3.754689846386396]
	TIME [epoch: 5.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8724611250681567		[learning rate: 0.00018278]
	Learning Rate: 0.000182778
	LOSS [training: 1.8724611250681567 | validation: 3.766512235679788]
	TIME [epoch: 5.72 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8748784478551506		[learning rate: 0.00018213]
	Learning Rate: 0.000182131
	LOSS [training: 1.8748784478551506 | validation: 3.74566930148309]
	TIME [epoch: 5.74 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8641271390509855		[learning rate: 0.00018149]
	Learning Rate: 0.000181487
	LOSS [training: 1.8641271390509855 | validation: 3.7465505086901265]
	TIME [epoch: 5.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8621892469066612		[learning rate: 0.00018085]
	Learning Rate: 0.000180846
	LOSS [training: 1.8621892469066612 | validation: 3.746749981996946]
	TIME [epoch: 5.69 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8622151994138356		[learning rate: 0.00018021]
	Learning Rate: 0.000180206
	LOSS [training: 1.8622151994138356 | validation: 3.7478800404289148]
	TIME [epoch: 5.69 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8631606636862155		[learning rate: 0.00017957]
	Learning Rate: 0.000179569
	LOSS [training: 1.8631606636862155 | validation: 3.750637391904524]
	TIME [epoch: 5.69 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8634487547335108		[learning rate: 0.00017893]
	Learning Rate: 0.000178934
	LOSS [training: 1.8634487547335108 | validation: 3.750963667972935]
	TIME [epoch: 5.7 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8687960484370665		[learning rate: 0.0001783]
	Learning Rate: 0.000178301
	LOSS [training: 1.8687960484370665 | validation: 3.7632558046546727]
	TIME [epoch: 5.73 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8644660697743203		[learning rate: 0.00017767]
	Learning Rate: 0.000177671
	LOSS [training: 1.8644660697743203 | validation: 3.752893015102024]
	TIME [epoch: 5.69 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8712785318969012		[learning rate: 0.00017704]
	Learning Rate: 0.000177042
	LOSS [training: 1.8712785318969012 | validation: 3.753483662560372]
	TIME [epoch: 5.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654841538025755		[learning rate: 0.00017642]
	Learning Rate: 0.000176416
	LOSS [training: 1.8654841538025755 | validation: 3.752634416512775]
	TIME [epoch: 5.69 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861193868112747		[learning rate: 0.00017579]
	Learning Rate: 0.000175792
	LOSS [training: 1.861193868112747 | validation: 3.7523013585109117]
	TIME [epoch: 5.69 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8666574855193412		[learning rate: 0.00017517]
	Learning Rate: 0.000175171
	LOSS [training: 1.8666574855193412 | validation: 3.7617150239872683]
	TIME [epoch: 5.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8673505568350763		[learning rate: 0.00017455]
	Learning Rate: 0.000174551
	LOSS [training: 1.8673505568350763 | validation: 3.754876298403035]
	TIME [epoch: 5.72 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8675939152452883		[learning rate: 0.00017393]
	Learning Rate: 0.000173934
	LOSS [training: 1.8675939152452883 | validation: 3.749814214302525]
	TIME [epoch: 5.71 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8632075156129098		[learning rate: 0.00017332]
	Learning Rate: 0.000173319
	LOSS [training: 1.8632075156129098 | validation: 3.7481837724971685]
	TIME [epoch: 5.71 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859979298440344		[learning rate: 0.00017271]
	Learning Rate: 0.000172706
	LOSS [training: 1.859979298440344 | validation: 3.746511826580927]
	TIME [epoch: 5.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.86366805928452		[learning rate: 0.0001721]
	Learning Rate: 0.000172095
	LOSS [training: 1.86366805928452 | validation: 3.76947665096626]
	TIME [epoch: 5.71 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8706274637764109		[learning rate: 0.00017149]
	Learning Rate: 0.000171487
	LOSS [training: 1.8706274637764109 | validation: 3.7501448768367753]
	TIME [epoch: 5.72 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8684550669359892		[learning rate: 0.00017088]
	Learning Rate: 0.00017088
	LOSS [training: 1.8684550669359892 | validation: 3.7523030550605574]
	TIME [epoch: 5.74 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8668449185231961		[learning rate: 0.00017028]
	Learning Rate: 0.000170276
	LOSS [training: 1.8668449185231961 | validation: 3.753783114198252]
	TIME [epoch: 5.71 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8595063025169782		[learning rate: 0.00016967]
	Learning Rate: 0.000169674
	LOSS [training: 1.8595063025169782 | validation: 3.7545970593412394]
	TIME [epoch: 5.69 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8621353679867407		[learning rate: 0.00016907]
	Learning Rate: 0.000169074
	LOSS [training: 1.8621353679867407 | validation: 3.734909895940344]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1202.pth
	Model improved!!!
EPOCH 1203/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8588882343187243		[learning rate: 0.00016848]
	Learning Rate: 0.000168476
	LOSS [training: 1.8588882343187243 | validation: 3.754974805152009]
	TIME [epoch: 5.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8568747490831745		[learning rate: 0.00016788]
	Learning Rate: 0.00016788
	LOSS [training: 1.8568747490831745 | validation: 3.74104033264105]
	TIME [epoch: 5.72 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654319438965516		[learning rate: 0.00016729]
	Learning Rate: 0.000167287
	LOSS [training: 1.8654319438965516 | validation: 3.751105403707154]
	TIME [epoch: 5.74 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8610139183176917		[learning rate: 0.0001667]
	Learning Rate: 0.000166695
	LOSS [training: 1.8610139183176917 | validation: 3.7443920161776423]
	TIME [epoch: 5.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8608581814091443		[learning rate: 0.00016611]
	Learning Rate: 0.000166106
	LOSS [training: 1.8608581814091443 | validation: 3.7563678055004743]
	TIME [epoch: 5.71 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.86452186566923		[learning rate: 0.00016552]
	Learning Rate: 0.000165518
	LOSS [training: 1.86452186566923 | validation: 3.745208307084604]
	TIME [epoch: 5.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.864409035480411		[learning rate: 0.00016493]
	Learning Rate: 0.000164933
	LOSS [training: 1.864409035480411 | validation: 3.750014904295865]
	TIME [epoch: 5.71 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8640824804807254		[learning rate: 0.00016435]
	Learning Rate: 0.00016435
	LOSS [training: 1.8640824804807254 | validation: 3.7513464266604206]
	TIME [epoch: 5.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.86504488789638		[learning rate: 0.00016377]
	Learning Rate: 0.000163769
	LOSS [training: 1.86504488789638 | validation: 3.741804469464376]
	TIME [epoch: 5.73 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8586404792951767		[learning rate: 0.00016319]
	Learning Rate: 0.00016319
	LOSS [training: 1.8586404792951767 | validation: 3.747178484959445]
	TIME [epoch: 5.69 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.86100287565398		[learning rate: 0.00016261]
	Learning Rate: 0.000162613
	LOSS [training: 1.86100287565398 | validation: 3.7460266036473615]
	TIME [epoch: 5.71 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8662922335961043		[learning rate: 0.00016204]
	Learning Rate: 0.000162037
	LOSS [training: 1.8662922335961043 | validation: 3.7548377119498304]
	TIME [epoch: 5.69 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8699883501545451		[learning rate: 0.00016146]
	Learning Rate: 0.000161464
	LOSS [training: 1.8699883501545451 | validation: 3.7545175912286806]
	TIME [epoch: 5.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8720942466265256		[learning rate: 0.00016089]
	Learning Rate: 0.000160893
	LOSS [training: 1.8720942466265256 | validation: 3.7488436614588894]
	TIME [epoch: 5.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859497601410468		[learning rate: 0.00016032]
	Learning Rate: 0.000160325
	LOSS [training: 1.859497601410468 | validation: 3.744175284604419]
	TIME [epoch: 5.73 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8607518375287067		[learning rate: 0.00015976]
	Learning Rate: 0.000159758
	LOSS [training: 1.8607518375287067 | validation: 3.7512660313959243]
	TIME [epoch: 5.69 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861511793885884		[learning rate: 0.00015919]
	Learning Rate: 0.000159193
	LOSS [training: 1.861511793885884 | validation: 3.750268722792072]
	TIME [epoch: 5.71 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8619042820068135		[learning rate: 0.00015863]
	Learning Rate: 0.00015863
	LOSS [training: 1.8619042820068135 | validation: 3.755155523248608]
	TIME [epoch: 5.69 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.870030322130941		[learning rate: 0.00015807]
	Learning Rate: 0.000158069
	LOSS [training: 1.870030322130941 | validation: 3.7649233725217117]
	TIME [epoch: 5.71 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8808020347795869		[learning rate: 0.00015751]
	Learning Rate: 0.00015751
	LOSS [training: 1.8808020347795869 | validation: 3.7658412790116236]
	TIME [epoch: 5.72 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8669268353988724		[learning rate: 0.00015695]
	Learning Rate: 0.000156953
	LOSS [training: 1.8669268353988724 | validation: 3.7464705371695404]
	TIME [epoch: 5.73 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8591181677117004		[learning rate: 0.0001564]
	Learning Rate: 0.000156398
	LOSS [training: 1.8591181677117004 | validation: 3.744307276901428]
	TIME [epoch: 5.7 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8684076241390604		[learning rate: 0.00015584]
	Learning Rate: 0.000155845
	LOSS [training: 1.8684076241390604 | validation: 3.7574671899114342]
	TIME [epoch: 5.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8743983125868582		[learning rate: 0.00015529]
	Learning Rate: 0.000155294
	LOSS [training: 1.8743983125868582 | validation: 3.741165585062831]
	TIME [epoch: 5.71 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8587474282533567		[learning rate: 0.00015474]
	Learning Rate: 0.000154745
	LOSS [training: 1.8587474282533567 | validation: 3.7453828468751835]
	TIME [epoch: 5.71 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85924355640685		[learning rate: 0.0001542]
	Learning Rate: 0.000154197
	LOSS [training: 1.85924355640685 | validation: 3.7379358561067377]
	TIME [epoch: 5.72 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8621618689751158		[learning rate: 0.00015365]
	Learning Rate: 0.000153652
	LOSS [training: 1.8621618689751158 | validation: 3.7471614435443747]
	TIME [epoch: 5.72 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8624254310276043		[learning rate: 0.00015311]
	Learning Rate: 0.000153109
	LOSS [training: 1.8624254310276043 | validation: 3.7440243482344897]
	TIME [epoch: 5.72 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8649739208190304		[learning rate: 0.00015257]
	Learning Rate: 0.000152567
	LOSS [training: 1.8649739208190304 | validation: 3.7559206035683737]
	TIME [epoch: 5.71 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8668006134598032		[learning rate: 0.00015203]
	Learning Rate: 0.000152028
	LOSS [training: 1.8668006134598032 | validation: 3.739741976495752]
	TIME [epoch: 5.71 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8655679436852863		[learning rate: 0.00015149]
	Learning Rate: 0.00015149
	LOSS [training: 1.8655679436852863 | validation: 3.7398084940912533]
	TIME [epoch: 5.71 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8630164241465703		[learning rate: 0.00015095]
	Learning Rate: 0.000150955
	LOSS [training: 1.8630164241465703 | validation: 3.7516829872314656]
	TIME [epoch: 5.73 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8703907309338965		[learning rate: 0.00015042]
	Learning Rate: 0.000150421
	LOSS [training: 1.8703907309338965 | validation: 3.7521042325266127]
	TIME [epoch: 5.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8700591462849134		[learning rate: 0.00014989]
	Learning Rate: 0.000149889
	LOSS [training: 1.8700591462849134 | validation: 3.7447437008264512]
	TIME [epoch: 5.71 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8619487508365753		[learning rate: 0.00014936]
	Learning Rate: 0.000149359
	LOSS [training: 1.8619487508365753 | validation: 3.7506242455434573]
	TIME [epoch: 5.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8687252708924906		[learning rate: 0.00014883]
	Learning Rate: 0.000148831
	LOSS [training: 1.8687252708924906 | validation: 3.7397293376285172]
	TIME [epoch: 5.71 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8617511642936417		[learning rate: 0.0001483]
	Learning Rate: 0.000148304
	LOSS [training: 1.8617511642936417 | validation: 3.739182356016079]
	TIME [epoch: 5.71 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8595266116383258		[learning rate: 0.00014778]
	Learning Rate: 0.00014778
	LOSS [training: 1.8595266116383258 | validation: 3.7381486737368674]
	TIME [epoch: 5.72 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.864725560317944		[learning rate: 0.00014726]
	Learning Rate: 0.000147257
	LOSS [training: 1.864725560317944 | validation: 3.745886556629587]
	TIME [epoch: 5.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8630717312231349		[learning rate: 0.00014674]
	Learning Rate: 0.000146737
	LOSS [training: 1.8630717312231349 | validation: 3.746332622626345]
	TIME [epoch: 5.72 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8636602792862256		[learning rate: 0.00014622]
	Learning Rate: 0.000146218
	LOSS [training: 1.8636602792862256 | validation: 3.7547161782290766]
	TIME [epoch: 5.69 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.869800782084572		[learning rate: 0.0001457]
	Learning Rate: 0.000145701
	LOSS [training: 1.869800782084572 | validation: 3.7472724979732757]
	TIME [epoch: 5.69 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8592255040709325		[learning rate: 0.00014519]
	Learning Rate: 0.000145185
	LOSS [training: 1.8592255040709325 | validation: 3.735228988959666]
	TIME [epoch: 5.71 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8666396614027574		[learning rate: 0.00014467]
	Learning Rate: 0.000144672
	LOSS [training: 1.8666396614027574 | validation: 3.7454896628252916]
	TIME [epoch: 5.72 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8593799142231673		[learning rate: 0.00014416]
	Learning Rate: 0.00014416
	LOSS [training: 1.8593799142231673 | validation: 3.7424761940546882]
	TIME [epoch: 5.75 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8643643770409954		[learning rate: 0.00014365]
	Learning Rate: 0.000143651
	LOSS [training: 1.8643643770409954 | validation: 3.7416174353640166]
	TIME [epoch: 5.72 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865844183903385		[learning rate: 0.00014314]
	Learning Rate: 0.000143143
	LOSS [training: 1.865844183903385 | validation: 3.7303357643948822]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1249.pth
	Model improved!!!
EPOCH 1250/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862061818899981		[learning rate: 0.00014264]
	Learning Rate: 0.000142637
	LOSS [training: 1.862061818899981 | validation: 3.7374495894962343]
	TIME [epoch: 5.69 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8588972033638178		[learning rate: 0.00014213]
	Learning Rate: 0.000142132
	LOSS [training: 1.8588972033638178 | validation: 3.736384706348805]
	TIME [epoch: 5.71 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8584623078333002		[learning rate: 0.00014163]
	Learning Rate: 0.00014163
	LOSS [training: 1.8584623078333002 | validation: 3.736250845739034]
	TIME [epoch: 5.72 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8609437707635463		[learning rate: 0.00014113]
	Learning Rate: 0.000141129
	LOSS [training: 1.8609437707635463 | validation: 3.7461602673806844]
	TIME [epoch: 5.74 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8633209412908		[learning rate: 0.00014063]
	Learning Rate: 0.00014063
	LOSS [training: 1.8633209412908 | validation: 3.735980618900277]
	TIME [epoch: 5.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862075573640733		[learning rate: 0.00014013]
	Learning Rate: 0.000140132
	LOSS [training: 1.862075573640733 | validation: 3.744985748721888]
	TIME [epoch: 5.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.866865088132478		[learning rate: 0.00013964]
	Learning Rate: 0.000139637
	LOSS [training: 1.866865088132478 | validation: 3.7393965914319107]
	TIME [epoch: 5.69 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8620660522407655		[learning rate: 0.00013914]
	Learning Rate: 0.000139143
	LOSS [training: 1.8620660522407655 | validation: 3.7442666798752318]
	TIME [epoch: 5.69 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.864209183377608		[learning rate: 0.00013865]
	Learning Rate: 0.000138651
	LOSS [training: 1.864209183377608 | validation: 3.729141238958145]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1258.pth
	Model improved!!!
EPOCH 1259/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8586945305771112		[learning rate: 0.00013816]
	Learning Rate: 0.000138161
	LOSS [training: 1.8586945305771112 | validation: 3.7383848054368833]
	TIME [epoch: 5.73 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8616345637249192		[learning rate: 0.00013767]
	Learning Rate: 0.000137672
	LOSS [training: 1.8616345637249192 | validation: 3.7343830485710696]
	TIME [epoch: 5.69 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654852230035706		[learning rate: 0.00013719]
	Learning Rate: 0.000137185
	LOSS [training: 1.8654852230035706 | validation: 3.7411010843716612]
	TIME [epoch: 5.71 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8667422043198676		[learning rate: 0.0001367]
	Learning Rate: 0.0001367
	LOSS [training: 1.8667422043198676 | validation: 3.7416088541800026]
	TIME [epoch: 5.69 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8639767716881788		[learning rate: 0.00013622]
	Learning Rate: 0.000136217
	LOSS [training: 1.8639767716881788 | validation: 3.734215326001716]
	TIME [epoch: 5.69 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8657302241596843		[learning rate: 0.00013574]
	Learning Rate: 0.000135735
	LOSS [training: 1.8657302241596843 | validation: 3.738497869846548]
	TIME [epoch: 5.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8652843322648054		[learning rate: 0.00013526]
	Learning Rate: 0.000135255
	LOSS [training: 1.8652843322648054 | validation: 3.744091171659024]
	TIME [epoch: 5.72 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.869749443524432		[learning rate: 0.00013478]
	Learning Rate: 0.000134777
	LOSS [training: 1.869749443524432 | validation: 3.7490500012033183]
	TIME [epoch: 5.7 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8718645305308865		[learning rate: 0.0001343]
	Learning Rate: 0.0001343
	LOSS [training: 1.8718645305308865 | validation: 3.7507665955148246]
	TIME [epoch: 5.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8631593852104407		[learning rate: 0.00013383]
	Learning Rate: 0.000133825
	LOSS [training: 1.8631593852104407 | validation: 3.743694015234687]
	TIME [epoch: 5.7 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8635496518733512		[learning rate: 0.00013335]
	Learning Rate: 0.000133352
	LOSS [training: 1.8635496518733512 | validation: 3.73283217076748]
	TIME [epoch: 5.69 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8616470358853379		[learning rate: 0.00013288]
	Learning Rate: 0.000132881
	LOSS [training: 1.8616470358853379 | validation: 3.7335883406105053]
	TIME [epoch: 5.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8583581823557689		[learning rate: 0.00013241]
	Learning Rate: 0.000132411
	LOSS [training: 1.8583581823557689 | validation: 3.736337042177569]
	TIME [epoch: 5.73 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8735918252633241		[learning rate: 0.00013194]
	Learning Rate: 0.000131942
	LOSS [training: 1.8735918252633241 | validation: 3.7421553227370943]
	TIME [epoch: 5.71 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8720738729845998		[learning rate: 0.00013148]
	Learning Rate: 0.000131476
	LOSS [training: 1.8720738729845998 | validation: 3.7320549357669517]
	TIME [epoch: 5.71 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8618694220076901		[learning rate: 0.00013101]
	Learning Rate: 0.000131011
	LOSS [training: 1.8618694220076901 | validation: 3.7444626525059546]
	TIME [epoch: 5.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8642875416536884		[learning rate: 0.00013055]
	Learning Rate: 0.000130548
	LOSS [training: 1.8642875416536884 | validation: 3.732073050495691]
	TIME [epoch: 5.69 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8597971329919194		[learning rate: 0.00013009]
	Learning Rate: 0.000130086
	LOSS [training: 1.8597971329919194 | validation: 3.7336355525655756]
	TIME [epoch: 5.73 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8584838972513134		[learning rate: 0.00012963]
	Learning Rate: 0.000129626
	LOSS [training: 1.8584838972513134 | validation: 3.73988509838264]
	TIME [epoch: 5.72 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8661982884176422		[learning rate: 0.00012917]
	Learning Rate: 0.000129168
	LOSS [training: 1.8661982884176422 | validation: 3.740167318609959]
	TIME [epoch: 5.7 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8595669254872709		[learning rate: 0.00012871]
	Learning Rate: 0.000128711
	LOSS [training: 1.8595669254872709 | validation: 3.7455396074230793]
	TIME [epoch: 5.69 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8602966101509628		[learning rate: 0.00012826]
	Learning Rate: 0.000128256
	LOSS [training: 1.8602966101509628 | validation: 3.740614308880174]
	TIME [epoch: 5.7 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8630201454477797		[learning rate: 0.0001278]
	Learning Rate: 0.000127802
	LOSS [training: 1.8630201454477797 | validation: 3.7440328689717184]
	TIME [epoch: 5.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857797369382633		[learning rate: 0.00012735]
	Learning Rate: 0.00012735
	LOSS [training: 1.857797369382633 | validation: 3.7454594375090404]
	TIME [epoch: 5.71 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8630408471871607		[learning rate: 0.0001269]
	Learning Rate: 0.0001269
	LOSS [training: 1.8630408471871607 | validation: 3.7375278131547676]
	TIME [epoch: 5.72 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8600983406464664		[learning rate: 0.00012645]
	Learning Rate: 0.000126451
	LOSS [training: 1.8600983406464664 | validation: 3.7398216818770362]
	TIME [epoch: 5.7 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8609918362059703		[learning rate: 0.000126]
	Learning Rate: 0.000126004
	LOSS [training: 1.8609918362059703 | validation: 3.75018943576792]
	TIME [epoch: 5.7 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865439187564959		[learning rate: 0.00012556]
	Learning Rate: 0.000125559
	LOSS [training: 1.865439187564959 | validation: 3.7433501591174854]
	TIME [epoch: 5.69 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8669506255822736		[learning rate: 0.00012511]
	Learning Rate: 0.000125115
	LOSS [training: 1.8669506255822736 | validation: 3.7489357981162175]
	TIME [epoch: 5.69 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8705276185886728		[learning rate: 0.00012467]
	Learning Rate: 0.000124672
	LOSS [training: 1.8705276185886728 | validation: 3.7342355238044425]
	TIME [epoch: 5.71 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8564664813848397		[learning rate: 0.00012423]
	Learning Rate: 0.000124231
	LOSS [training: 1.8564664813848397 | validation: 3.7385977404674815]
	TIME [epoch: 5.72 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8603416849760517		[learning rate: 0.00012379]
	Learning Rate: 0.000123792
	LOSS [training: 1.8603416849760517 | validation: 3.7388116539833276]
	TIME [epoch: 5.7 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8698470051487215		[learning rate: 0.00012335]
	Learning Rate: 0.000123354
	LOSS [training: 1.8698470051487215 | validation: 3.7438560892935526]
	TIME [epoch: 5.7 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8636702690664055		[learning rate: 0.00012292]
	Learning Rate: 0.000122918
	LOSS [training: 1.8636702690664055 | validation: 3.743375119920666]
	TIME [epoch: 5.7 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8567878002912797		[learning rate: 0.00012248]
	Learning Rate: 0.000122483
	LOSS [training: 1.8567878002912797 | validation: 3.7395356384512755]
	TIME [epoch: 5.71 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8619389698005113		[learning rate: 0.00012205]
	Learning Rate: 0.00012205
	LOSS [training: 1.8619389698005113 | validation: 3.74588614892789]
	TIME [epoch: 5.71 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8591213434193448		[learning rate: 0.00012162]
	Learning Rate: 0.000121619
	LOSS [training: 1.8591213434193448 | validation: 3.7440801146264175]
	TIME [epoch: 5.74 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8631184905761922		[learning rate: 0.00012119]
	Learning Rate: 0.000121189
	LOSS [training: 1.8631184905761922 | validation: 3.738485169902806]
	TIME [epoch: 5.7 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8578422608947214		[learning rate: 0.00012076]
	Learning Rate: 0.00012076
	LOSS [training: 1.8578422608947214 | validation: 3.746493928835323]
	TIME [epoch: 5.7 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8656979851004905		[learning rate: 0.00012033]
	Learning Rate: 0.000120333
	LOSS [training: 1.8656979851004905 | validation: 3.737792387620049]
	TIME [epoch: 5.71 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8564384952311608		[learning rate: 0.00011991]
	Learning Rate: 0.000119907
	LOSS [training: 1.8564384952311608 | validation: 3.7359698914428265]
	TIME [epoch: 5.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8637579132913533		[learning rate: 0.00011948]
	Learning Rate: 0.000119483
	LOSS [training: 1.8637579132913533 | validation: 3.764554506698179]
	TIME [epoch: 5.73 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8744295285146355		[learning rate: 0.00011906]
	Learning Rate: 0.000119061
	LOSS [training: 1.8744295285146355 | validation: 3.751244198282443]
	TIME [epoch: 5.74 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8601070446676296		[learning rate: 0.00011864]
	Learning Rate: 0.00011864
	LOSS [training: 1.8601070446676296 | validation: 3.7324438847707713]
	TIME [epoch: 5.71 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8584815405633432		[learning rate: 0.00011822]
	Learning Rate: 0.00011822
	LOSS [training: 1.8584815405633432 | validation: 3.7400825042096177]
	TIME [epoch: 5.71 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857502606449038		[learning rate: 0.0001178]
	Learning Rate: 0.000117802
	LOSS [training: 1.857502606449038 | validation: 3.735261265923616]
	TIME [epoch: 5.69 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581074432070195		[learning rate: 0.00011739]
	Learning Rate: 0.000117386
	LOSS [training: 1.8581074432070195 | validation: 3.744472257419838]
	TIME [epoch: 5.71 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8577420453543247		[learning rate: 0.00011697]
	Learning Rate: 0.000116971
	LOSS [training: 1.8577420453543247 | validation: 3.7424229584683815]
	TIME [epoch: 5.72 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8590397768055529		[learning rate: 0.00011656]
	Learning Rate: 0.000116557
	LOSS [training: 1.8590397768055529 | validation: 3.7385002789621264]
	TIME [epoch: 5.73 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8643555024956167		[learning rate: 0.00011614]
	Learning Rate: 0.000116145
	LOSS [training: 1.8643555024956167 | validation: 3.739402151976229]
	TIME [epoch: 5.71 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8604426176488578		[learning rate: 0.00011573]
	Learning Rate: 0.000115734
	LOSS [training: 1.8604426176488578 | validation: 3.729071991946169]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1309.pth
	Model improved!!!
EPOCH 1310/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8640008581919503		[learning rate: 0.00011532]
	Learning Rate: 0.000115325
	LOSS [training: 1.8640008581919503 | validation: 3.7446721228531157]
	TIME [epoch: 5.7 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8650484160977676		[learning rate: 0.00011492]
	Learning Rate: 0.000114917
	LOSS [training: 1.8650484160977676 | validation: 3.7420536666717688]
	TIME [epoch: 5.7 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.858147210059037		[learning rate: 0.00011451]
	Learning Rate: 0.000114511
	LOSS [training: 1.858147210059037 | validation: 3.7385009173256236]
	TIME [epoch: 5.71 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8626598829789982		[learning rate: 0.00011411]
	Learning Rate: 0.000114106
	LOSS [training: 1.8626598829789982 | validation: 3.739343480591001]
	TIME [epoch: 5.72 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8578545445361212		[learning rate: 0.0001137]
	Learning Rate: 0.000113702
	LOSS [training: 1.8578545445361212 | validation: 3.7373629055679713]
	TIME [epoch: 5.7 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861715887455181		[learning rate: 0.0001133]
	Learning Rate: 0.0001133
	LOSS [training: 1.861715887455181 | validation: 3.7426537721933846]
	TIME [epoch: 5.7 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862856219464902		[learning rate: 0.0001129]
	Learning Rate: 0.0001129
	LOSS [training: 1.862856219464902 | validation: 3.7428059448411988]
	TIME [epoch: 5.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581109347696692		[learning rate: 0.0001125]
	Learning Rate: 0.0001125
	LOSS [training: 1.8581109347696692 | validation: 3.7462189471040053]
	TIME [epoch: 5.7 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581877498213726		[learning rate: 0.0001121]
	Learning Rate: 0.000112103
	LOSS [training: 1.8581877498213726 | validation: 3.748395189245846]
	TIME [epoch: 5.71 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8594395403255988		[learning rate: 0.00011171]
	Learning Rate: 0.000111706
	LOSS [training: 1.8594395403255988 | validation: 3.739485475142439]
	TIME [epoch: 5.73 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8545165623199162		[learning rate: 0.00011131]
	Learning Rate: 0.000111311
	LOSS [training: 1.8545165623199162 | validation: 3.7411693316499153]
	TIME [epoch: 5.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857478747913931		[learning rate: 0.00011092]
	Learning Rate: 0.000110918
	LOSS [training: 1.857478747913931 | validation: 3.742632061686295]
	TIME [epoch: 5.7 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8554170351821069		[learning rate: 0.00011053]
	Learning Rate: 0.000110525
	LOSS [training: 1.8554170351821069 | validation: 3.7461851122048837]
	TIME [epoch: 5.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8604033406827374		[learning rate: 0.00011013]
	Learning Rate: 0.000110134
	LOSS [training: 1.8604033406827374 | validation: 3.7405070391886426]
	TIME [epoch: 5.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8650757887701865		[learning rate: 0.00010975]
	Learning Rate: 0.000109745
	LOSS [training: 1.8650757887701865 | validation: 3.745289378782502]
	TIME [epoch: 5.71 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865984571936732		[learning rate: 0.00010936]
	Learning Rate: 0.000109357
	LOSS [training: 1.865984571936732 | validation: 3.740435071214087]
	TIME [epoch: 5.72 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853700912633528		[learning rate: 0.00010897]
	Learning Rate: 0.00010897
	LOSS [training: 1.853700912633528 | validation: 3.739263260628568]
	TIME [epoch: 5.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573906518944476		[learning rate: 0.00010858]
	Learning Rate: 0.000108585
	LOSS [training: 1.8573906518944476 | validation: 3.7424605906562873]
	TIME [epoch: 5.7 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8598469718211172		[learning rate: 0.0001082]
	Learning Rate: 0.000108201
	LOSS [training: 1.8598469718211172 | validation: 3.739326314523546]
	TIME [epoch: 5.7 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8583279363704386		[learning rate: 0.00010782]
	Learning Rate: 0.000107818
	LOSS [training: 1.8583279363704386 | validation: 3.742580012381815]
	TIME [epoch: 5.7 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8588072653622167		[learning rate: 0.00010744]
	Learning Rate: 0.000107437
	LOSS [training: 1.8588072653622167 | validation: 3.7456149716882727]
	TIME [epoch: 5.71 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8552302084652736		[learning rate: 0.00010706]
	Learning Rate: 0.000107057
	LOSS [training: 1.8552302084652736 | validation: 3.7334600337796364]
	TIME [epoch: 5.72 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8572594515265641		[learning rate: 0.00010668]
	Learning Rate: 0.000106679
	LOSS [training: 1.8572594515265641 | validation: 3.737472607217384]
	TIME [epoch: 5.7 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8586795536290246		[learning rate: 0.0001063]
	Learning Rate: 0.000106301
	LOSS [training: 1.8586795536290246 | validation: 3.7530088515207773]
	TIME [epoch: 5.7 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8635384983373702		[learning rate: 0.00010593]
	Learning Rate: 0.000105925
	LOSS [training: 1.8635384983373702 | validation: 3.7514896753279117]
	TIME [epoch: 5.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8689843118005052		[learning rate: 0.00010555]
	Learning Rate: 0.000105551
	LOSS [training: 1.8689843118005052 | validation: 3.7460965454609805]
	TIME [epoch: 5.7 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.870222679612386		[learning rate: 0.00010518]
	Learning Rate: 0.000105178
	LOSS [training: 1.870222679612386 | validation: 3.7587242355028874]
	TIME [epoch: 5.71 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654826483412648		[learning rate: 0.00010481]
	Learning Rate: 0.000104806
	LOSS [training: 1.8654826483412648 | validation: 3.7491566162966454]
	TIME [epoch: 5.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8597085113272351		[learning rate: 0.00010444]
	Learning Rate: 0.000104435
	LOSS [training: 1.8597085113272351 | validation: 3.742427931760767]
	TIME [epoch: 5.7 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8558601506914993		[learning rate: 0.00010407]
	Learning Rate: 0.000104066
	LOSS [training: 1.8558601506914993 | validation: 3.7446490066247193]
	TIME [epoch: 5.7 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8618195904250907		[learning rate: 0.0001037]
	Learning Rate: 0.000103698
	LOSS [training: 1.8618195904250907 | validation: 3.7432377099613485]
	TIME [epoch: 5.7 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8579631646991406		[learning rate: 0.00010333]
	Learning Rate: 0.000103331
	LOSS [training: 1.8579631646991406 | validation: 3.736845685626063]
	TIME [epoch: 5.7 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8654935584570016		[learning rate: 0.00010297]
	Learning Rate: 0.000102966
	LOSS [training: 1.8654935584570016 | validation: 3.7480336546593622]
	TIME [epoch: 5.71 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8708621263278955		[learning rate: 0.0001026]
	Learning Rate: 0.000102602
	LOSS [training: 1.8708621263278955 | validation: 3.739590634782271]
	TIME [epoch: 5.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.867006336892548		[learning rate: 0.00010224]
	Learning Rate: 0.000102239
	LOSS [training: 1.867006336892548 | validation: 3.7436882255155997]
	TIME [epoch: 5.7 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8706902590519259		[learning rate: 0.00010188]
	Learning Rate: 0.000101877
	LOSS [training: 1.8706902590519259 | validation: 3.7435351436670024]
	TIME [epoch: 5.7 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.866977178074402		[learning rate: 0.00010152]
	Learning Rate: 0.000101517
	LOSS [training: 1.866977178074402 | validation: 3.7383834907815574]
	TIME [epoch: 5.7 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862555225419745		[learning rate: 0.00010116]
	Learning Rate: 0.000101158
	LOSS [training: 1.862555225419745 | validation: 3.7421162139134676]
	TIME [epoch: 5.7 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8643600178044262		[learning rate: 0.0001008]
	Learning Rate: 0.0001008
	LOSS [training: 1.8643600178044262 | validation: 3.7441248240872698]
	TIME [epoch: 5.71 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.864095696213693		[learning rate: 0.00010044]
	Learning Rate: 0.000100444
	LOSS [training: 1.864095696213693 | validation: 3.745537848063359]
	TIME [epoch: 5.72 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8574949623168686		[learning rate: 0.00010009]
	Learning Rate: 0.000100089
	LOSS [training: 1.8574949623168686 | validation: 3.7364673265947728]
	TIME [epoch: 5.7 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857476359756133		[learning rate: 9.9735e-05]
	Learning Rate: 9.97347e-05
	LOSS [training: 1.857476359756133 | validation: 3.7320036354509627]
	TIME [epoch: 5.7 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862944727031321		[learning rate: 9.9382e-05]
	Learning Rate: 9.9382e-05
	LOSS [training: 1.862944727031321 | validation: 3.744399246746178]
	TIME [epoch: 5.7 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8599829559198047		[learning rate: 9.9031e-05]
	Learning Rate: 9.90306e-05
	LOSS [training: 1.8599829559198047 | validation: 3.735156982226147]
	TIME [epoch: 5.7 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855266312402384		[learning rate: 9.868e-05]
	Learning Rate: 9.86804e-05
	LOSS [training: 1.855266312402384 | validation: 3.743905604302928]
	TIME [epoch: 5.71 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581611549720465		[learning rate: 9.8331e-05]
	Learning Rate: 9.83314e-05
	LOSS [training: 1.8581611549720465 | validation: 3.737050230715736]
	TIME [epoch: 5.73 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.858695395880968		[learning rate: 9.7984e-05]
	Learning Rate: 9.79837e-05
	LOSS [training: 1.858695395880968 | validation: 3.7442225910521096]
	TIME [epoch: 5.71 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85735577555473		[learning rate: 9.7637e-05]
	Learning Rate: 9.76372e-05
	LOSS [training: 1.85735577555473 | validation: 3.739230567485317]
	TIME [epoch: 5.7 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8565498213327607		[learning rate: 9.7292e-05]
	Learning Rate: 9.7292e-05
	LOSS [training: 1.8565498213327607 | validation: 3.7397143445586756]
	TIME [epoch: 5.7 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8626159016040251		[learning rate: 9.6948e-05]
	Learning Rate: 9.69479e-05
	LOSS [training: 1.8626159016040251 | validation: 3.7440176432864183]
	TIME [epoch: 5.7 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8656315054838415		[learning rate: 9.6605e-05]
	Learning Rate: 9.66051e-05
	LOSS [training: 1.8656315054838415 | validation: 3.740402502824046]
	TIME [epoch: 5.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862563557934425		[learning rate: 9.6263e-05]
	Learning Rate: 9.62635e-05
	LOSS [training: 1.862563557934425 | validation: 3.753036997777948]
	TIME [epoch: 5.73 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865848565226637		[learning rate: 9.5923e-05]
	Learning Rate: 9.59231e-05
	LOSS [training: 1.865848565226637 | validation: 3.744865520828488]
	TIME [epoch: 5.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8613877315114213		[learning rate: 9.5584e-05]
	Learning Rate: 9.55839e-05
	LOSS [training: 1.8613877315114213 | validation: 3.731786720295809]
	TIME [epoch: 5.71 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859467143898462		[learning rate: 9.5246e-05]
	Learning Rate: 9.52459e-05
	LOSS [training: 1.859467143898462 | validation: 3.7341283848620184]
	TIME [epoch: 5.7 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8580121207828266		[learning rate: 9.4909e-05]
	Learning Rate: 9.49091e-05
	LOSS [training: 1.8580121207828266 | validation: 3.743600296057141]
	TIME [epoch: 5.7 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8588637029252486		[learning rate: 9.4573e-05]
	Learning Rate: 9.45735e-05
	LOSS [training: 1.8588637029252486 | validation: 3.7433201828925178]
	TIME [epoch: 5.71 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551437243997888		[learning rate: 9.4239e-05]
	Learning Rate: 9.4239e-05
	LOSS [training: 1.8551437243997888 | validation: 3.73848480591044]
	TIME [epoch: 5.74 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854075187884781		[learning rate: 9.3906e-05]
	Learning Rate: 9.39058e-05
	LOSS [training: 1.854075187884781 | validation: 3.742023097944599]
	TIME [epoch: 5.71 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857774528225739		[learning rate: 9.3574e-05]
	Learning Rate: 9.35737e-05
	LOSS [training: 1.857774528225739 | validation: 3.7510096623691043]
	TIME [epoch: 5.71 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8660741570230392		[learning rate: 9.3243e-05]
	Learning Rate: 9.32428e-05
	LOSS [training: 1.8660741570230392 | validation: 3.746858795500399]
	TIME [epoch: 5.7 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8583129123657978		[learning rate: 9.2913e-05]
	Learning Rate: 9.29131e-05
	LOSS [training: 1.8583129123657978 | validation: 3.7412007218905616]
	TIME [epoch: 5.7 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8540163088631934		[learning rate: 9.2585e-05]
	Learning Rate: 9.25845e-05
	LOSS [training: 1.8540163088631934 | validation: 3.7373262597146186]
	TIME [epoch: 5.71 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8547013219854918		[learning rate: 9.2257e-05]
	Learning Rate: 9.22572e-05
	LOSS [training: 1.8547013219854918 | validation: 3.7400144096704357]
	TIME [epoch: 5.74 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861540726593353		[learning rate: 9.1931e-05]
	Learning Rate: 9.19309e-05
	LOSS [training: 1.861540726593353 | validation: 3.741191967774229]
	TIME [epoch: 5.71 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8611438013520747		[learning rate: 9.1606e-05]
	Learning Rate: 9.16058e-05
	LOSS [training: 1.8611438013520747 | validation: 3.7283858554386113]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1375.pth
	Model improved!!!
EPOCH 1376/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8582926747545339		[learning rate: 9.1282e-05]
	Learning Rate: 9.12819e-05
	LOSS [training: 1.8582926747545339 | validation: 3.7385250834546007]
	TIME [epoch: 5.7 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8552342178144563		[learning rate: 9.0959e-05]
	Learning Rate: 9.09591e-05
	LOSS [training: 1.8552342178144563 | validation: 3.7344362408851546]
	TIME [epoch: 5.7 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553910453380489		[learning rate: 9.0637e-05]
	Learning Rate: 9.06375e-05
	LOSS [training: 1.8553910453380489 | validation: 3.7368072615625842]
	TIME [epoch: 5.71 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.858565210423914		[learning rate: 9.0317e-05]
	Learning Rate: 9.03169e-05
	LOSS [training: 1.858565210423914 | validation: 3.7496257300101155]
	TIME [epoch: 5.73 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.866215375872867		[learning rate: 8.9998e-05]
	Learning Rate: 8.99976e-05
	LOSS [training: 1.866215375872867 | validation: 3.748999581938355]
	TIME [epoch: 5.7 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8637090740625788		[learning rate: 8.9679e-05]
	Learning Rate: 8.96793e-05
	LOSS [training: 1.8637090740625788 | validation: 3.7420851390118095]
	TIME [epoch: 5.7 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8603335335051856		[learning rate: 8.9362e-05]
	Learning Rate: 8.93622e-05
	LOSS [training: 1.8603335335051856 | validation: 3.734232405622149]
	TIME [epoch: 5.7 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85787098824377		[learning rate: 8.9046e-05]
	Learning Rate: 8.90462e-05
	LOSS [training: 1.85787098824377 | validation: 3.736361137282239]
	TIME [epoch: 5.7 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.861704799707816		[learning rate: 8.8731e-05]
	Learning Rate: 8.87313e-05
	LOSS [training: 1.861704799707816 | validation: 3.735453069690965]
	TIME [epoch: 5.71 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8561426086020885		[learning rate: 8.8418e-05]
	Learning Rate: 8.84175e-05
	LOSS [training: 1.8561426086020885 | validation: 3.7365739997359246]
	TIME [epoch: 5.72 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553477468026927		[learning rate: 8.8105e-05]
	Learning Rate: 8.81049e-05
	LOSS [training: 1.8553477468026927 | validation: 3.753647826666374]
	TIME [epoch: 5.7 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553986462507446		[learning rate: 8.7793e-05]
	Learning Rate: 8.77934e-05
	LOSS [training: 1.8553986462507446 | validation: 3.732544721704925]
	TIME [epoch: 5.7 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8568071893863602		[learning rate: 8.7483e-05]
	Learning Rate: 8.74829e-05
	LOSS [training: 1.8568071893863602 | validation: 3.7344043373971902]
	TIME [epoch: 5.7 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8572988222541316		[learning rate: 8.7174e-05]
	Learning Rate: 8.71735e-05
	LOSS [training: 1.8572988222541316 | validation: 3.726449426168688]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1389.pth
	Model improved!!!
EPOCH 1390/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856406751547297		[learning rate: 8.6865e-05]
	Learning Rate: 8.68653e-05
	LOSS [training: 1.856406751547297 | validation: 3.743527011993996]
	TIME [epoch: 5.71 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8593703130177333		[learning rate: 8.6558e-05]
	Learning Rate: 8.65581e-05
	LOSS [training: 1.8593703130177333 | validation: 3.7417021112987174]
	TIME [epoch: 5.72 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862279904067544		[learning rate: 8.6252e-05]
	Learning Rate: 8.6252e-05
	LOSS [training: 1.862279904067544 | validation: 3.7414584994965527]
	TIME [epoch: 5.7 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8570254904614094		[learning rate: 8.5947e-05]
	Learning Rate: 8.5947e-05
	LOSS [training: 1.8570254904614094 | validation: 3.732211446002985]
	TIME [epoch: 5.7 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8570456676739469		[learning rate: 8.5643e-05]
	Learning Rate: 8.56431e-05
	LOSS [training: 1.8570456676739469 | validation: 3.7367238906273594]
	TIME [epoch: 5.7 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862089586905699		[learning rate: 8.534e-05]
	Learning Rate: 8.53402e-05
	LOSS [training: 1.862089586905699 | validation: 3.74298647107156]
	TIME [epoch: 5.7 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8621673193295214		[learning rate: 8.5038e-05]
	Learning Rate: 8.50385e-05
	LOSS [training: 1.8621673193295214 | validation: 3.7276791522342445]
	TIME [epoch: 5.71 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8539361045015736		[learning rate: 8.4738e-05]
	Learning Rate: 8.47378e-05
	LOSS [training: 1.8539361045015736 | validation: 3.7419220029417035]
	TIME [epoch: 5.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551933943699508		[learning rate: 8.4438e-05]
	Learning Rate: 8.44381e-05
	LOSS [training: 1.8551933943699508 | validation: 3.7390626888853538]
	TIME [epoch: 5.7 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857136193356495		[learning rate: 8.414e-05]
	Learning Rate: 8.41395e-05
	LOSS [training: 1.857136193356495 | validation: 3.734646487852531]
	TIME [epoch: 5.7 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553847599332838		[learning rate: 8.3842e-05]
	Learning Rate: 8.3842e-05
	LOSS [training: 1.8553847599332838 | validation: 3.7318440401000177]
	TIME [epoch: 5.7 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8554049932779442		[learning rate: 8.3546e-05]
	Learning Rate: 8.35455e-05
	LOSS [training: 1.8554049932779442 | validation: 3.727633880535045]
	TIME [epoch: 5.7 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8590752041461223		[learning rate: 8.325e-05]
	Learning Rate: 8.32501e-05
	LOSS [training: 1.8590752041461223 | validation: 3.736728947993921]
	TIME [epoch: 5.71 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8626334973597163		[learning rate: 8.2956e-05]
	Learning Rate: 8.29557e-05
	LOSS [training: 1.8626334973597163 | validation: 3.737803269506636]
	TIME [epoch: 5.72 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573987435861472		[learning rate: 8.2662e-05]
	Learning Rate: 8.26624e-05
	LOSS [training: 1.8573987435861472 | validation: 3.737979537439733]
	TIME [epoch: 5.7 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8583824951452086		[learning rate: 8.237e-05]
	Learning Rate: 8.237e-05
	LOSS [training: 1.8583824951452086 | validation: 3.7302468911417646]
	TIME [epoch: 5.7 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856494644277149		[learning rate: 8.2079e-05]
	Learning Rate: 8.20788e-05
	LOSS [training: 1.856494644277149 | validation: 3.7379836643565203]
	TIME [epoch: 5.7 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573917956571053		[learning rate: 8.1789e-05]
	Learning Rate: 8.17885e-05
	LOSS [training: 1.8573917956571053 | validation: 3.7312723032235557]
	TIME [epoch: 5.7 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538832775242111		[learning rate: 8.1499e-05]
	Learning Rate: 8.14993e-05
	LOSS [training: 1.8538832775242111 | validation: 3.7344235335098794]
	TIME [epoch: 5.71 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8608628521782948		[learning rate: 8.1211e-05]
	Learning Rate: 8.12111e-05
	LOSS [training: 1.8608628521782948 | validation: 3.739921991078968]
	TIME [epoch: 5.72 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853931955645289		[learning rate: 8.0924e-05]
	Learning Rate: 8.0924e-05
	LOSS [training: 1.853931955645289 | validation: 3.735245626538358]
	TIME [epoch: 5.7 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856905735290747		[learning rate: 8.0638e-05]
	Learning Rate: 8.06378e-05
	LOSS [training: 1.856905735290747 | validation: 3.725840156751783]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1411.pth
	Model improved!!!
EPOCH 1412/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855735882974197		[learning rate: 8.0353e-05]
	Learning Rate: 8.03526e-05
	LOSS [training: 1.855735882974197 | validation: 3.7376460329124894]
	TIME [epoch: 5.7 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8541326628503783		[learning rate: 8.0069e-05]
	Learning Rate: 8.00685e-05
	LOSS [training: 1.8541326628503783 | validation: 3.7387149363852212]
	TIME [epoch: 5.7 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556386591876919		[learning rate: 7.9785e-05]
	Learning Rate: 7.97854e-05
	LOSS [training: 1.8556386591876919 | validation: 3.735615725758233]
	TIME [epoch: 5.72 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8588104686382825		[learning rate: 7.9503e-05]
	Learning Rate: 7.95032e-05
	LOSS [training: 1.8588104686382825 | validation: 3.7412830146689213]
	TIME [epoch: 5.72 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8583192592305218		[learning rate: 7.9222e-05]
	Learning Rate: 7.92221e-05
	LOSS [training: 1.8583192592305218 | validation: 3.745380348070883]
	TIME [epoch: 5.7 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855139351051712		[learning rate: 7.8942e-05]
	Learning Rate: 7.8942e-05
	LOSS [training: 1.855139351051712 | validation: 3.743182374882295]
	TIME [epoch: 5.7 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857632243488501		[learning rate: 7.8663e-05]
	Learning Rate: 7.86628e-05
	LOSS [training: 1.857632243488501 | validation: 3.739508595863441]
	TIME [epoch: 5.7 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8659800100633028		[learning rate: 7.8385e-05]
	Learning Rate: 7.83846e-05
	LOSS [training: 1.8659800100633028 | validation: 3.7580598951974653]
	TIME [epoch: 5.7 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.865694096339208		[learning rate: 7.8107e-05]
	Learning Rate: 7.81074e-05
	LOSS [training: 1.865694096339208 | validation: 3.7427636427598383]
	TIME [epoch: 5.71 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.864089162067609		[learning rate: 7.7831e-05]
	Learning Rate: 7.78312e-05
	LOSS [training: 1.864089162067609 | validation: 3.7435869391931647]
	TIME [epoch: 5.73 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8582600192481427		[learning rate: 7.7556e-05]
	Learning Rate: 7.7556e-05
	LOSS [training: 1.8582600192481427 | validation: 3.7407946643242433]
	TIME [epoch: 5.7 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8535496955105102		[learning rate: 7.7282e-05]
	Learning Rate: 7.72818e-05
	LOSS [training: 1.8535496955105102 | validation: 3.7324668594886266]
	TIME [epoch: 5.7 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536862051378322		[learning rate: 7.7008e-05]
	Learning Rate: 7.70085e-05
	LOSS [training: 1.8536862051378322 | validation: 3.7351476702493245]
	TIME [epoch: 5.7 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8576308202426146		[learning rate: 7.6736e-05]
	Learning Rate: 7.67362e-05
	LOSS [training: 1.8576308202426146 | validation: 3.7305955172399483]
	TIME [epoch: 5.7 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8544431270167796		[learning rate: 7.6465e-05]
	Learning Rate: 7.64648e-05
	LOSS [training: 1.8544431270167796 | validation: 3.7315933645273867]
	TIME [epoch: 5.71 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549481551204878		[learning rate: 7.6194e-05]
	Learning Rate: 7.61944e-05
	LOSS [training: 1.8549481551204878 | validation: 3.7388537959383017]
	TIME [epoch: 5.72 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8640774543924072		[learning rate: 7.5925e-05]
	Learning Rate: 7.5925e-05
	LOSS [training: 1.8640774543924072 | validation: 3.731570910954922]
	TIME [epoch: 5.7 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8619407848679967		[learning rate: 7.5656e-05]
	Learning Rate: 7.56565e-05
	LOSS [training: 1.8619407848679967 | validation: 3.73154170428469]
	TIME [epoch: 5.7 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85547634764297		[learning rate: 7.5389e-05]
	Learning Rate: 7.5389e-05
	LOSS [training: 1.85547634764297 | validation: 3.739308925249854]
	TIME [epoch: 5.69 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8583003564600373		[learning rate: 7.5122e-05]
	Learning Rate: 7.51224e-05
	LOSS [training: 1.8583003564600373 | validation: 3.722893609820017]
	TIME [epoch: 5.7 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1431.pth
	Model improved!!!
EPOCH 1432/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859977896914061		[learning rate: 7.4857e-05]
	Learning Rate: 7.48567e-05
	LOSS [training: 1.859977896914061 | validation: 3.7387498033294824]
	TIME [epoch: 5.71 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551001791724568		[learning rate: 7.4592e-05]
	Learning Rate: 7.4592e-05
	LOSS [training: 1.8551001791724568 | validation: 3.7351380699895245]
	TIME [epoch: 5.73 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542769615575003		[learning rate: 7.4328e-05]
	Learning Rate: 7.43283e-05
	LOSS [training: 1.8542769615575003 | validation: 3.7343346529591543]
	TIME [epoch: 5.71 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8558080345804504		[learning rate: 7.4065e-05]
	Learning Rate: 7.40654e-05
	LOSS [training: 1.8558080345804504 | validation: 3.739471120901113]
	TIME [epoch: 5.71 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8558285243292283		[learning rate: 7.3803e-05]
	Learning Rate: 7.38035e-05
	LOSS [training: 1.8558285243292283 | validation: 3.735385810184821]
	TIME [epoch: 5.71 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8562672192829497		[learning rate: 7.3543e-05]
	Learning Rate: 7.35425e-05
	LOSS [training: 1.8562672192829497 | validation: 3.739493239579158]
	TIME [epoch: 5.7 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8581027613403631		[learning rate: 7.3282e-05]
	Learning Rate: 7.32825e-05
	LOSS [training: 1.8581027613403631 | validation: 3.733293699764123]
	TIME [epoch: 5.72 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8590606161935594		[learning rate: 7.3023e-05]
	Learning Rate: 7.30233e-05
	LOSS [training: 1.8590606161935594 | validation: 3.746472604600176]
	TIME [epoch: 5.72 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8651221476149205		[learning rate: 7.2765e-05]
	Learning Rate: 7.27651e-05
	LOSS [training: 1.8651221476149205 | validation: 3.740410614876662]
	TIME [epoch: 5.71 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8558540174799254		[learning rate: 7.2508e-05]
	Learning Rate: 7.25078e-05
	LOSS [training: 1.8558540174799254 | validation: 3.7373168061885496]
	TIME [epoch: 5.71 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857009399378331		[learning rate: 7.2251e-05]
	Learning Rate: 7.22514e-05
	LOSS [training: 1.857009399378331 | validation: 3.735192134389804]
	TIME [epoch: 5.71 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854473931264168		[learning rate: 7.1996e-05]
	Learning Rate: 7.19959e-05
	LOSS [training: 1.854473931264168 | validation: 3.743856896076404]
	TIME [epoch: 5.69 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8554820906235736		[learning rate: 7.1741e-05]
	Learning Rate: 7.17413e-05
	LOSS [training: 1.8554820906235736 | validation: 3.735328484738382]
	TIME [epoch: 5.72 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553573463673905		[learning rate: 7.1488e-05]
	Learning Rate: 7.14876e-05
	LOSS [training: 1.8553573463673905 | validation: 3.739551349630295]
	TIME [epoch: 5.72 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8521131980149477		[learning rate: 7.1235e-05]
	Learning Rate: 7.12348e-05
	LOSS [training: 1.8521131980149477 | validation: 3.744273672991393]
	TIME [epoch: 5.71 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549518010746255		[learning rate: 7.0983e-05]
	Learning Rate: 7.09829e-05
	LOSS [training: 1.8549518010746255 | validation: 3.7264912084048922]
	TIME [epoch: 5.71 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859892034230687		[learning rate: 7.0732e-05]
	Learning Rate: 7.07319e-05
	LOSS [training: 1.859892034230687 | validation: 3.729313689936024]
	TIME [epoch: 5.71 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8545354432049996		[learning rate: 7.0482e-05]
	Learning Rate: 7.04818e-05
	LOSS [training: 1.8545354432049996 | validation: 3.732540228916384]
	TIME [epoch: 5.71 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8568970715025177		[learning rate: 7.0233e-05]
	Learning Rate: 7.02326e-05
	LOSS [training: 1.8568970715025177 | validation: 3.733309891441802]
	TIME [epoch: 5.72 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8560517024641812		[learning rate: 6.9984e-05]
	Learning Rate: 6.99842e-05
	LOSS [training: 1.8560517024641812 | validation: 3.730094935008901]
	TIME [epoch: 5.73 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855655215125401		[learning rate: 6.9737e-05]
	Learning Rate: 6.97367e-05
	LOSS [training: 1.855655215125401 | validation: 3.7403623009325244]
	TIME [epoch: 5.71 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8531393786381418		[learning rate: 6.949e-05]
	Learning Rate: 6.94901e-05
	LOSS [training: 1.8531393786381418 | validation: 3.7364292094513867]
	TIME [epoch: 5.71 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573274252564542		[learning rate: 6.9244e-05]
	Learning Rate: 6.92444e-05
	LOSS [training: 1.8573274252564542 | validation: 3.728463326559141]
	TIME [epoch: 5.71 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8575610446797872		[learning rate: 6.9e-05]
	Learning Rate: 6.89995e-05
	LOSS [training: 1.8575610446797872 | validation: 3.7393522333744573]
	TIME [epoch: 5.71 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529138600641812		[learning rate: 6.8756e-05]
	Learning Rate: 6.87555e-05
	LOSS [training: 1.8529138600641812 | validation: 3.7332720326265374]
	TIME [epoch: 5.72 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511548107863796		[learning rate: 6.8512e-05]
	Learning Rate: 6.85124e-05
	LOSS [training: 1.8511548107863796 | validation: 3.7273448559333047]
	TIME [epoch: 5.74 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542973026326517		[learning rate: 6.827e-05]
	Learning Rate: 6.82702e-05
	LOSS [training: 1.8542973026326517 | validation: 3.7336335555201616]
	TIME [epoch: 5.71 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8567411330963735		[learning rate: 6.8029e-05]
	Learning Rate: 6.80287e-05
	LOSS [training: 1.8567411330963735 | validation: 3.7302511018834994]
	TIME [epoch: 5.71 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8570115437530457		[learning rate: 6.7788e-05]
	Learning Rate: 6.77882e-05
	LOSS [training: 1.8570115437530457 | validation: 3.727752442668286]
	TIME [epoch: 5.69 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556398382153154		[learning rate: 6.7548e-05]
	Learning Rate: 6.75485e-05
	LOSS [training: 1.8556398382153154 | validation: 3.7321428231322287]
	TIME [epoch: 5.71 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8516636965353785		[learning rate: 6.731e-05]
	Learning Rate: 6.73096e-05
	LOSS [training: 1.8516636965353785 | validation: 3.726664848416543]
	TIME [epoch: 5.72 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8555447184323954		[learning rate: 6.7072e-05]
	Learning Rate: 6.70716e-05
	LOSS [training: 1.8555447184323954 | validation: 3.7358060382110203]
	TIME [epoch: 5.73 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8574363915911105		[learning rate: 6.6834e-05]
	Learning Rate: 6.68344e-05
	LOSS [training: 1.8574363915911105 | validation: 3.7339490551833117]
	TIME [epoch: 5.71 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8537141546448517		[learning rate: 6.6598e-05]
	Learning Rate: 6.65981e-05
	LOSS [training: 1.8537141546448517 | validation: 3.7368023552818785]
	TIME [epoch: 5.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8552670522642338		[learning rate: 6.6363e-05]
	Learning Rate: 6.63626e-05
	LOSS [training: 1.8552670522642338 | validation: 3.73386010450954]
	TIME [epoch: 5.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8578322864457857		[learning rate: 6.6128e-05]
	Learning Rate: 6.61279e-05
	LOSS [training: 1.8578322864457857 | validation: 3.735556864009104]
	TIME [epoch: 5.71 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.858250580959702		[learning rate: 6.5894e-05]
	Learning Rate: 6.58941e-05
	LOSS [training: 1.858250580959702 | validation: 3.7357356228458922]
	TIME [epoch: 5.72 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8598312982655312		[learning rate: 6.5661e-05]
	Learning Rate: 6.5661e-05
	LOSS [training: 1.8598312982655312 | validation: 3.7263569712045137]
	TIME [epoch: 5.73 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549785258559128		[learning rate: 6.5429e-05]
	Learning Rate: 6.54289e-05
	LOSS [training: 1.8549785258559128 | validation: 3.729127873783929]
	TIME [epoch: 5.71 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8531708167165453		[learning rate: 6.5197e-05]
	Learning Rate: 6.51975e-05
	LOSS [training: 1.8531708167165453 | validation: 3.7323712325942235]
	TIME [epoch: 5.71 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8564557313424448		[learning rate: 6.4967e-05]
	Learning Rate: 6.49669e-05
	LOSS [training: 1.8564557313424448 | validation: 3.733841315936953]
	TIME [epoch: 5.71 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8535584206787183		[learning rate: 6.4737e-05]
	Learning Rate: 6.47372e-05
	LOSS [training: 1.8535584206787183 | validation: 3.730281129978596]
	TIME [epoch: 5.69 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8539032479817483		[learning rate: 6.4508e-05]
	Learning Rate: 6.45083e-05
	LOSS [training: 1.8539032479817483 | validation: 3.735042585942365]
	TIME [epoch: 5.72 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8569199178790379		[learning rate: 6.428e-05]
	Learning Rate: 6.42802e-05
	LOSS [training: 1.8569199178790379 | validation: 3.7403560138706298]
	TIME [epoch: 5.73 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8550009389650235		[learning rate: 6.4053e-05]
	Learning Rate: 6.40529e-05
	LOSS [training: 1.8550009389650235 | validation: 3.730013179049088]
	TIME [epoch: 5.71 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8588166694433506		[learning rate: 6.3826e-05]
	Learning Rate: 6.38264e-05
	LOSS [training: 1.8588166694433506 | validation: 3.737368166179517]
	TIME [epoch: 5.7 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8570719030868679		[learning rate: 6.3601e-05]
	Learning Rate: 6.36007e-05
	LOSS [training: 1.8570719030868679 | validation: 3.7357291893831035]
	TIME [epoch: 5.71 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556436636098617		[learning rate: 6.3376e-05]
	Learning Rate: 6.33758e-05
	LOSS [training: 1.8556436636098617 | validation: 3.729102536003087]
	TIME [epoch: 5.68 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8567897514800524		[learning rate: 6.3152e-05]
	Learning Rate: 6.31517e-05
	LOSS [training: 1.8567897514800524 | validation: 3.731368956865232]
	TIME [epoch: 5.7 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853383640367508		[learning rate: 6.2928e-05]
	Learning Rate: 6.29283e-05
	LOSS [training: 1.853383640367508 | validation: 3.7336683518601443]
	TIME [epoch: 5.73 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8555332034368321		[learning rate: 6.2706e-05]
	Learning Rate: 6.27058e-05
	LOSS [training: 1.8555332034368321 | validation: 3.7297363999380067]
	TIME [epoch: 5.71 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543085675710578		[learning rate: 6.2484e-05]
	Learning Rate: 6.24841e-05
	LOSS [training: 1.8543085675710578 | validation: 3.728388458327046]
	TIME [epoch: 5.71 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8559407257981202		[learning rate: 6.2263e-05]
	Learning Rate: 6.22631e-05
	LOSS [training: 1.8559407257981202 | validation: 3.7428741740496596]
	TIME [epoch: 5.71 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8589988900169885		[learning rate: 6.2043e-05]
	Learning Rate: 6.20429e-05
	LOSS [training: 1.8589988900169885 | validation: 3.741049829367865]
	TIME [epoch: 5.69 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8576215551049429		[learning rate: 6.1824e-05]
	Learning Rate: 6.18235e-05
	LOSS [training: 1.8576215551049429 | validation: 3.743570374436798]
	TIME [epoch: 5.7 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859790237887451		[learning rate: 6.1605e-05]
	Learning Rate: 6.16049e-05
	LOSS [training: 1.859790237887451 | validation: 3.733522754018368]
	TIME [epoch: 5.73 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.862344658865126		[learning rate: 6.1387e-05]
	Learning Rate: 6.13871e-05
	LOSS [training: 1.862344658865126 | validation: 3.7347603918489645]
	TIME [epoch: 5.69 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8600684812251882		[learning rate: 6.117e-05]
	Learning Rate: 6.117e-05
	LOSS [training: 1.8600684812251882 | validation: 3.73174164503753]
	TIME [epoch: 5.71 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8575165286697313		[learning rate: 6.0954e-05]
	Learning Rate: 6.09537e-05
	LOSS [training: 1.8575165286697313 | validation: 3.736347844995738]
	TIME [epoch: 5.71 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8577891653331864		[learning rate: 6.0738e-05]
	Learning Rate: 6.07382e-05
	LOSS [training: 1.8577891653331864 | validation: 3.730632141941273]
	TIME [epoch: 5.7 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8528921358557482		[learning rate: 6.0523e-05]
	Learning Rate: 6.05234e-05
	LOSS [training: 1.8528921358557482 | validation: 3.739350872081798]
	TIME [epoch: 5.7 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8555701880311848		[learning rate: 6.0309e-05]
	Learning Rate: 6.03094e-05
	LOSS [training: 1.8555701880311848 | validation: 3.7323220890451103]
	TIME [epoch: 5.73 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8560334402537846		[learning rate: 6.0096e-05]
	Learning Rate: 6.00961e-05
	LOSS [training: 1.8560334402537846 | validation: 3.7443970961382673]
	TIME [epoch: 5.69 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855924083806631		[learning rate: 5.9884e-05]
	Learning Rate: 5.98836e-05
	LOSS [training: 1.855924083806631 | validation: 3.7321693119851296]
	TIME [epoch: 5.69 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538416824879862		[learning rate: 5.9672e-05]
	Learning Rate: 5.96718e-05
	LOSS [training: 1.8538416824879862 | validation: 3.73612350451392]
	TIME [epoch: 5.69 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.858900487161883		[learning rate: 5.9461e-05]
	Learning Rate: 5.94608e-05
	LOSS [training: 1.858900487161883 | validation: 3.735225512647198]
	TIME [epoch: 5.7 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8624675701439743		[learning rate: 5.9251e-05]
	Learning Rate: 5.92505e-05
	LOSS [training: 1.8624675701439743 | validation: 3.744504937104913]
	TIME [epoch: 5.7 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573561930540021		[learning rate: 5.9041e-05]
	Learning Rate: 5.9041e-05
	LOSS [training: 1.8573561930540021 | validation: 3.733585068269486]
	TIME [epoch: 5.72 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508336002771033		[learning rate: 5.8832e-05]
	Learning Rate: 5.88323e-05
	LOSS [training: 1.8508336002771033 | validation: 3.726462143782135]
	TIME [epoch: 5.69 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851109073544584		[learning rate: 5.8624e-05]
	Learning Rate: 5.86242e-05
	LOSS [training: 1.851109073544584 | validation: 3.7336067640036856]
	TIME [epoch: 5.7 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524522794953817		[learning rate: 5.8417e-05]
	Learning Rate: 5.84169e-05
	LOSS [training: 1.8524522794953817 | validation: 3.7330565988880196]
	TIME [epoch: 5.69 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8601592470632113		[learning rate: 5.821e-05]
	Learning Rate: 5.82103e-05
	LOSS [training: 1.8601592470632113 | validation: 3.7414962671641265]
	TIME [epoch: 5.7 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8559708206307493		[learning rate: 5.8004e-05]
	Learning Rate: 5.80045e-05
	LOSS [training: 1.8559708206307493 | validation: 3.7337439102096726]
	TIME [epoch: 5.7 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527853423012093		[learning rate: 5.7799e-05]
	Learning Rate: 5.77994e-05
	LOSS [training: 1.8527853423012093 | validation: 3.728238742269385]
	TIME [epoch: 5.73 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852521240971797		[learning rate: 5.7595e-05]
	Learning Rate: 5.7595e-05
	LOSS [training: 1.852521240971797 | validation: 3.7366811815874628]
	TIME [epoch: 5.7 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8545477959850551		[learning rate: 5.7391e-05]
	Learning Rate: 5.73913e-05
	LOSS [training: 1.8545477959850551 | validation: 3.7372310154390074]
	TIME [epoch: 5.7 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8575654235512395		[learning rate: 5.7188e-05]
	Learning Rate: 5.71884e-05
	LOSS [training: 1.8575654235512395 | validation: 3.742543500910994]
	TIME [epoch: 5.7 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8559907338904238		[learning rate: 5.6986e-05]
	Learning Rate: 5.69861e-05
	LOSS [training: 1.8559907338904238 | validation: 3.742984518196601]
	TIME [epoch: 5.69 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8545311418237338		[learning rate: 5.6785e-05]
	Learning Rate: 5.67846e-05
	LOSS [training: 1.8545311418237338 | validation: 3.7356382221122524]
	TIME [epoch: 5.7 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526080574055144		[learning rate: 5.6584e-05]
	Learning Rate: 5.65838e-05
	LOSS [training: 1.8526080574055144 | validation: 3.7241069801322864]
	TIME [epoch: 5.72 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527954794838566		[learning rate: 5.6384e-05]
	Learning Rate: 5.63837e-05
	LOSS [training: 1.8527954794838566 | validation: 3.7255619937380335]
	TIME [epoch: 5.69 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854715029632723		[learning rate: 5.6184e-05]
	Learning Rate: 5.61844e-05
	LOSS [training: 1.854715029632723 | validation: 3.7280495416832333]
	TIME [epoch: 5.7 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.859702397182033		[learning rate: 5.5986e-05]
	Learning Rate: 5.59857e-05
	LOSS [training: 1.859702397182033 | validation: 3.7237259317287386]
	TIME [epoch: 5.69 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855198124394016		[learning rate: 5.5788e-05]
	Learning Rate: 5.57877e-05
	LOSS [training: 1.855198124394016 | validation: 3.7342421437986633]
	TIME [epoch: 5.71 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508722397790542		[learning rate: 5.559e-05]
	Learning Rate: 5.55904e-05
	LOSS [training: 1.8508722397790542 | validation: 3.7304276892624433]
	TIME [epoch: 5.7 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8596179115670164		[learning rate: 5.5394e-05]
	Learning Rate: 5.53939e-05
	LOSS [training: 1.8596179115670164 | validation: 3.7347065852366286]
	TIME [epoch: 5.73 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856952380299016		[learning rate: 5.5198e-05]
	Learning Rate: 5.5198e-05
	LOSS [training: 1.856952380299016 | validation: 3.731300416047476]
	TIME [epoch: 5.69 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8563669724176697		[learning rate: 5.5003e-05]
	Learning Rate: 5.50028e-05
	LOSS [training: 1.8563669724176697 | validation: 3.7328138798028063]
	TIME [epoch: 5.69 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8592646184777157		[learning rate: 5.4808e-05]
	Learning Rate: 5.48083e-05
	LOSS [training: 1.8592646184777157 | validation: 3.7326811668124864]
	TIME [epoch: 5.69 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8564057943784529		[learning rate: 5.4614e-05]
	Learning Rate: 5.46145e-05
	LOSS [training: 1.8564057943784529 | validation: 3.732514851573518]
	TIME [epoch: 5.7 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8591900425449652		[learning rate: 5.4421e-05]
	Learning Rate: 5.44213e-05
	LOSS [training: 1.8591900425449652 | validation: 3.7283997164492635]
	TIME [epoch: 5.72 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857742522471423		[learning rate: 5.4229e-05]
	Learning Rate: 5.42289e-05
	LOSS [training: 1.857742522471423 | validation: 3.730344181071907]
	TIME [epoch: 5.72 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8535652429257596		[learning rate: 5.4037e-05]
	Learning Rate: 5.40371e-05
	LOSS [training: 1.8535652429257596 | validation: 3.734621618914525]
	TIME [epoch: 5.69 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538607533642337		[learning rate: 5.3846e-05]
	Learning Rate: 5.38461e-05
	LOSS [training: 1.8538607533642337 | validation: 3.7352833112095216]
	TIME [epoch: 5.69 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536575260874408		[learning rate: 5.3656e-05]
	Learning Rate: 5.36556e-05
	LOSS [training: 1.8536575260874408 | validation: 3.73725749263014]
	TIME [epoch: 5.69 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8531149294603244		[learning rate: 5.3466e-05]
	Learning Rate: 5.34659e-05
	LOSS [training: 1.8531149294603244 | validation: 3.729957926969714]
	TIME [epoch: 5.7 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8563384970083412		[learning rate: 5.3277e-05]
	Learning Rate: 5.32769e-05
	LOSS [training: 1.8563384970083412 | validation: 3.7369110088383013]
	TIME [epoch: 5.72 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8562858782702472		[learning rate: 5.3088e-05]
	Learning Rate: 5.30884e-05
	LOSS [training: 1.8562858782702472 | validation: 3.736352921786631]
	TIME [epoch: 5.73 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8540720409145588		[learning rate: 5.2901e-05]
	Learning Rate: 5.29007e-05
	LOSS [training: 1.8540720409145588 | validation: 3.735030352489565]
	TIME [epoch: 5.71 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853394098176877		[learning rate: 5.2714e-05]
	Learning Rate: 5.27137e-05
	LOSS [training: 1.853394098176877 | validation: 3.73217317869892]
	TIME [epoch: 5.71 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852589298762872		[learning rate: 5.2527e-05]
	Learning Rate: 5.25272e-05
	LOSS [training: 1.852589298762872 | validation: 3.7277692307139882]
	TIME [epoch: 5.71 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542139560427406		[learning rate: 5.2342e-05]
	Learning Rate: 5.23415e-05
	LOSS [training: 1.8542139560427406 | validation: 3.727911919778991]
	TIME [epoch: 5.71 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8537776931132126		[learning rate: 5.2156e-05]
	Learning Rate: 5.21564e-05
	LOSS [training: 1.8537776931132126 | validation: 3.734490068599529]
	TIME [epoch: 5.72 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852738070197406		[learning rate: 5.1972e-05]
	Learning Rate: 5.1972e-05
	LOSS [training: 1.852738070197406 | validation: 3.736851175904444]
	TIME [epoch: 5.74 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524384478427032		[learning rate: 5.1788e-05]
	Learning Rate: 5.17882e-05
	LOSS [training: 1.8524384478427032 | validation: 3.7335199469765405]
	TIME [epoch: 5.71 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8515992493797921		[learning rate: 5.1605e-05]
	Learning Rate: 5.16051e-05
	LOSS [training: 1.8515992493797921 | validation: 3.7329671634993553]
	TIME [epoch: 5.71 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8552392937284192		[learning rate: 5.1423e-05]
	Learning Rate: 5.14226e-05
	LOSS [training: 1.8552392937284192 | validation: 3.728170484638614]
	TIME [epoch: 5.71 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514554990620056		[learning rate: 5.1241e-05]
	Learning Rate: 5.12407e-05
	LOSS [training: 1.8514554990620056 | validation: 3.7297105998872953]
	TIME [epoch: 5.71 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538549317898203		[learning rate: 5.106e-05]
	Learning Rate: 5.10596e-05
	LOSS [training: 1.8538549317898203 | validation: 3.7351501714036806]
	TIME [epoch: 5.72 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8570163927427534		[learning rate: 5.0879e-05]
	Learning Rate: 5.0879e-05
	LOSS [training: 1.8570163927427534 | validation: 3.731858499761547]
	TIME [epoch: 5.72 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85624252569082		[learning rate: 5.0699e-05]
	Learning Rate: 5.06991e-05
	LOSS [training: 1.85624252569082 | validation: 3.7303862147914537]
	TIME [epoch: 5.71 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857168367631771		[learning rate: 5.052e-05]
	Learning Rate: 5.05198e-05
	LOSS [training: 1.857168367631771 | validation: 3.7330319241000502]
	TIME [epoch: 5.69 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8530955470893278		[learning rate: 5.0341e-05]
	Learning Rate: 5.03412e-05
	LOSS [training: 1.8530955470893278 | validation: 3.726819263754123]
	TIME [epoch: 5.71 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526404220644364		[learning rate: 5.0163e-05]
	Learning Rate: 5.01631e-05
	LOSS [training: 1.8526404220644364 | validation: 3.730383393294752]
	TIME [epoch: 5.71 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549779338355243		[learning rate: 4.9986e-05]
	Learning Rate: 4.99857e-05
	LOSS [training: 1.8549779338355243 | validation: 3.7294667087429243]
	TIME [epoch: 5.72 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534846368979467		[learning rate: 4.9809e-05]
	Learning Rate: 4.9809e-05
	LOSS [training: 1.8534846368979467 | validation: 3.7299113257847933]
	TIME [epoch: 5.74 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8544548188471306		[learning rate: 4.9633e-05]
	Learning Rate: 4.96329e-05
	LOSS [training: 1.8544548188471306 | validation: 3.7234476105102368]
	TIME [epoch: 5.71 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8535389828593085		[learning rate: 4.9457e-05]
	Learning Rate: 4.94573e-05
	LOSS [training: 1.8535389828593085 | validation: 3.7341767868354268]
	TIME [epoch: 5.69 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529236700870115		[learning rate: 4.9282e-05]
	Learning Rate: 4.92825e-05
	LOSS [training: 1.8529236700870115 | validation: 3.7307815326600657]
	TIME [epoch: 5.71 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8515841131878847		[learning rate: 4.9108e-05]
	Learning Rate: 4.91082e-05
	LOSS [training: 1.8515841131878847 | validation: 3.7307391255028266]
	TIME [epoch: 5.71 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8531167897539695		[learning rate: 4.8935e-05]
	Learning Rate: 4.89345e-05
	LOSS [training: 1.8531167897539695 | validation: 3.7317789375491133]
	TIME [epoch: 5.72 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8565662808958128		[learning rate: 4.8762e-05]
	Learning Rate: 4.87615e-05
	LOSS [training: 1.8565662808958128 | validation: 3.7330599044795116]
	TIME [epoch: 5.74 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556118871860563		[learning rate: 4.8589e-05]
	Learning Rate: 4.85891e-05
	LOSS [training: 1.8556118871860563 | validation: 3.7316479407548355]
	TIME [epoch: 5.71 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8558539672178977		[learning rate: 4.8417e-05]
	Learning Rate: 4.84172e-05
	LOSS [training: 1.8558539672178977 | validation: 3.7298445593132463]
	TIME [epoch: 5.69 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551777533265588		[learning rate: 4.8246e-05]
	Learning Rate: 4.8246e-05
	LOSS [training: 1.8551777533265588 | validation: 3.733573161186496]
	TIME [epoch: 5.71 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850051969603866		[learning rate: 4.8075e-05]
	Learning Rate: 4.80754e-05
	LOSS [training: 1.850051969603866 | validation: 3.723838652905036]
	TIME [epoch: 5.69 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8559848765666347		[learning rate: 4.7905e-05]
	Learning Rate: 4.79054e-05
	LOSS [training: 1.8559848765666347 | validation: 3.7290847711440205]
	TIME [epoch: 5.72 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8509054175528394		[learning rate: 4.7736e-05]
	Learning Rate: 4.7736e-05
	LOSS [training: 1.8509054175528394 | validation: 3.7274732791020915]
	TIME [epoch: 5.72 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8572127493720487		[learning rate: 4.7567e-05]
	Learning Rate: 4.75672e-05
	LOSS [training: 1.8572127493720487 | validation: 3.717576283739418]
	TIME [epoch: 5.71 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1560.pth
	Model improved!!!
EPOCH 1561/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523558007240206		[learning rate: 4.7399e-05]
	Learning Rate: 4.7399e-05
	LOSS [training: 1.8523558007240206 | validation: 3.7284685751404165]
	TIME [epoch: 5.69 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8541539109203597		[learning rate: 4.7231e-05]
	Learning Rate: 4.72314e-05
	LOSS [training: 1.8541539109203597 | validation: 3.7271959172183893]
	TIME [epoch: 5.69 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8539010873477637		[learning rate: 4.7064e-05]
	Learning Rate: 4.70644e-05
	LOSS [training: 1.8539010873477637 | validation: 3.728682374579746]
	TIME [epoch: 5.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853412204125183		[learning rate: 4.6898e-05]
	Learning Rate: 4.6898e-05
	LOSS [training: 1.853412204125183 | validation: 3.737405098579016]
	TIME [epoch: 5.72 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573941388427575		[learning rate: 4.6732e-05]
	Learning Rate: 4.67321e-05
	LOSS [training: 1.8573941388427575 | validation: 3.732549123903649]
	TIME [epoch: 5.72 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8554383532088121		[learning rate: 4.6567e-05]
	Learning Rate: 4.65669e-05
	LOSS [training: 1.8554383532088121 | validation: 3.7295663119180906]
	TIME [epoch: 5.7 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857113950027654		[learning rate: 4.6402e-05]
	Learning Rate: 4.64022e-05
	LOSS [training: 1.857113950027654 | validation: 3.7374207604385137]
	TIME [epoch: 5.69 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8554808581311615		[learning rate: 4.6238e-05]
	Learning Rate: 4.62381e-05
	LOSS [training: 1.8554808581311615 | validation: 3.731945482207759]
	TIME [epoch: 5.71 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8566798776406686		[learning rate: 4.6075e-05]
	Learning Rate: 4.60746e-05
	LOSS [training: 1.8566798776406686 | validation: 3.731552735291665]
	TIME [epoch: 5.69 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856719289185547		[learning rate: 4.5912e-05]
	Learning Rate: 4.59117e-05
	LOSS [training: 1.856719289185547 | validation: 3.740026863946015]
	TIME [epoch: 5.72 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543078284077712		[learning rate: 4.5749e-05]
	Learning Rate: 4.57493e-05
	LOSS [training: 1.8543078284077712 | validation: 3.72902403757409]
	TIME [epoch: 5.73 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543636837732085		[learning rate: 4.5588e-05]
	Learning Rate: 4.55875e-05
	LOSS [training: 1.8543636837732085 | validation: 3.726286167091498]
	TIME [epoch: 5.71 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8560831317828295		[learning rate: 4.5426e-05]
	Learning Rate: 4.54264e-05
	LOSS [training: 1.8560831317828295 | validation: 3.722644610270565]
	TIME [epoch: 5.69 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8555345406810044		[learning rate: 4.5266e-05]
	Learning Rate: 4.52657e-05
	LOSS [training: 1.8555345406810044 | validation: 3.7342974956282653]
	TIME [epoch: 5.71 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527430891224208		[learning rate: 4.5106e-05]
	Learning Rate: 4.51056e-05
	LOSS [training: 1.8527430891224208 | validation: 3.731054815995069]
	TIME [epoch: 5.69 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8515387493624305		[learning rate: 4.4946e-05]
	Learning Rate: 4.49461e-05
	LOSS [training: 1.8515387493624305 | validation: 3.73346938845217]
	TIME [epoch: 5.71 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854211354428609		[learning rate: 4.4787e-05]
	Learning Rate: 4.47872e-05
	LOSS [training: 1.854211354428609 | validation: 3.727079696639062]
	TIME [epoch: 5.73 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8558601445023182		[learning rate: 4.4629e-05]
	Learning Rate: 4.46288e-05
	LOSS [training: 1.8558601445023182 | validation: 3.726766202478424]
	TIME [epoch: 5.69 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543251113910206		[learning rate: 4.4471e-05]
	Learning Rate: 4.4471e-05
	LOSS [training: 1.8543251113910206 | validation: 3.7349010741875985]
	TIME [epoch: 5.69 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511240505123534		[learning rate: 4.4314e-05]
	Learning Rate: 4.43138e-05
	LOSS [training: 1.8511240505123534 | validation: 3.7302262045932295]
	TIME [epoch: 5.71 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8515300955580531		[learning rate: 4.4157e-05]
	Learning Rate: 4.41571e-05
	LOSS [training: 1.8515300955580531 | validation: 3.735276996341407]
	TIME [epoch: 5.71 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8519930598005143		[learning rate: 4.4001e-05]
	Learning Rate: 4.40009e-05
	LOSS [training: 1.8519930598005143 | validation: 3.7338631284904045]
	TIME [epoch: 5.72 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855095012300115		[learning rate: 4.3845e-05]
	Learning Rate: 4.38453e-05
	LOSS [training: 1.855095012300115 | validation: 3.7356666510184526]
	TIME [epoch: 5.74 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524235418548		[learning rate: 4.369e-05]
	Learning Rate: 4.36903e-05
	LOSS [training: 1.8524235418548 | validation: 3.738299431189615]
	TIME [epoch: 5.69 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525531375096156		[learning rate: 4.3536e-05]
	Learning Rate: 4.35358e-05
	LOSS [training: 1.8525531375096156 | validation: 3.7365365083281636]
	TIME [epoch: 5.69 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8522540192732166		[learning rate: 4.3382e-05]
	Learning Rate: 4.33818e-05
	LOSS [training: 1.8522540192732166 | validation: 3.7287648970905525]
	TIME [epoch: 5.69 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852758797336208		[learning rate: 4.3228e-05]
	Learning Rate: 4.32284e-05
	LOSS [training: 1.852758797336208 | validation: 3.734477405539654]
	TIME [epoch: 5.69 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8546378142863782		[learning rate: 4.3076e-05]
	Learning Rate: 4.30756e-05
	LOSS [training: 1.8546378142863782 | validation: 3.730490153083343]
	TIME [epoch: 5.7 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853303131588154		[learning rate: 4.2923e-05]
	Learning Rate: 4.29232e-05
	LOSS [training: 1.853303131588154 | validation: 3.7310030281838658]
	TIME [epoch: 5.72 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852289555762371		[learning rate: 4.2771e-05]
	Learning Rate: 4.27714e-05
	LOSS [training: 1.852289555762371 | validation: 3.7270571480615082]
	TIME [epoch: 5.69 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549369175182182		[learning rate: 4.262e-05]
	Learning Rate: 4.26202e-05
	LOSS [training: 1.8549369175182182 | validation: 3.732041928958212]
	TIME [epoch: 5.69 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8573072121294298		[learning rate: 4.2469e-05]
	Learning Rate: 4.24695e-05
	LOSS [training: 1.8573072121294298 | validation: 3.730919900268531]
	TIME [epoch: 5.69 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854024031108388		[learning rate: 4.2319e-05]
	Learning Rate: 4.23193e-05
	LOSS [training: 1.854024031108388 | validation: 3.732737506270355]
	TIME [epoch: 5.69 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8559670364753016		[learning rate: 4.217e-05]
	Learning Rate: 4.21697e-05
	LOSS [training: 1.8559670364753016 | validation: 3.7289067017374338]
	TIME [epoch: 5.7 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551822700036542		[learning rate: 4.2021e-05]
	Learning Rate: 4.20205e-05
	LOSS [training: 1.8551822700036542 | validation: 3.7240393285100346]
	TIME [epoch: 5.72 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851977373346005		[learning rate: 4.1872e-05]
	Learning Rate: 4.18719e-05
	LOSS [training: 1.851977373346005 | validation: 3.7404904540729182]
	TIME [epoch: 5.69 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556930748839153		[learning rate: 4.1724e-05]
	Learning Rate: 4.17239e-05
	LOSS [training: 1.8556930748839153 | validation: 3.734550411830503]
	TIME [epoch: 5.7 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854209677919227		[learning rate: 4.1576e-05]
	Learning Rate: 4.15763e-05
	LOSS [training: 1.854209677919227 | validation: 3.738205762874926]
	TIME [epoch: 5.69 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542868098401017		[learning rate: 4.1429e-05]
	Learning Rate: 4.14293e-05
	LOSS [training: 1.8542868098401017 | validation: 3.73675802162933]
	TIME [epoch: 5.69 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8499088091312137		[learning rate: 4.1283e-05]
	Learning Rate: 4.12828e-05
	LOSS [training: 1.8499088091312137 | validation: 3.7357187210341]
	TIME [epoch: 5.7 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8541304648704604		[learning rate: 4.1137e-05]
	Learning Rate: 4.11368e-05
	LOSS [training: 1.8541304648704604 | validation: 3.7229938184873363]
	TIME [epoch: 5.72 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848601996217073		[learning rate: 4.0991e-05]
	Learning Rate: 4.09914e-05
	LOSS [training: 1.848601996217073 | validation: 3.720162316066485]
	TIME [epoch: 5.69 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527203303600803		[learning rate: 4.0846e-05]
	Learning Rate: 4.08464e-05
	LOSS [training: 1.8527203303600803 | validation: 3.7301657398880743]
	TIME [epoch: 5.7 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8521797514900797		[learning rate: 4.0702e-05]
	Learning Rate: 4.0702e-05
	LOSS [training: 1.8521797514900797 | validation: 3.727872035906188]
	TIME [epoch: 5.69 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855435577979793		[learning rate: 4.0558e-05]
	Learning Rate: 4.0558e-05
	LOSS [training: 1.855435577979793 | validation: 3.7257993441305577]
	TIME [epoch: 5.7 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534734271174966		[learning rate: 4.0415e-05]
	Learning Rate: 4.04146e-05
	LOSS [training: 1.8534734271174966 | validation: 3.7407980487234]
	TIME [epoch: 5.71 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854536702455404		[learning rate: 4.0272e-05]
	Learning Rate: 4.02717e-05
	LOSS [training: 1.854536702455404 | validation: 3.7290972381961387]
	TIME [epoch: 5.73 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855402580584697		[learning rate: 4.0129e-05]
	Learning Rate: 4.01293e-05
	LOSS [training: 1.855402580584697 | validation: 3.7287572002992397]
	TIME [epoch: 5.71 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8546896492851437		[learning rate: 3.9987e-05]
	Learning Rate: 3.99874e-05
	LOSS [training: 1.8546896492851437 | validation: 3.740313524632373]
	TIME [epoch: 5.7 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853671691790844		[learning rate: 3.9846e-05]
	Learning Rate: 3.9846e-05
	LOSS [training: 1.853671691790844 | validation: 3.721781099080846]
	TIME [epoch: 5.71 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8533052656441167		[learning rate: 3.9705e-05]
	Learning Rate: 3.97051e-05
	LOSS [training: 1.8533052656441167 | validation: 3.7259907305927675]
	TIME [epoch: 5.7 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524660118953098		[learning rate: 3.9565e-05]
	Learning Rate: 3.95647e-05
	LOSS [training: 1.8524660118953098 | validation: 3.7270856960404455]
	TIME [epoch: 5.69 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526413500616847		[learning rate: 3.9425e-05]
	Learning Rate: 3.94248e-05
	LOSS [training: 1.8526413500616847 | validation: 3.729231753357567]
	TIME [epoch: 5.73 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857766680641629		[learning rate: 3.9285e-05]
	Learning Rate: 3.92854e-05
	LOSS [training: 1.857766680641629 | validation: 3.7277400810370813]
	TIME [epoch: 5.71 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852749201716568		[learning rate: 3.9146e-05]
	Learning Rate: 3.91464e-05
	LOSS [training: 1.852749201716568 | validation: 3.722270728099519]
	TIME [epoch: 5.69 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553176235692281		[learning rate: 3.9008e-05]
	Learning Rate: 3.9008e-05
	LOSS [training: 1.8553176235692281 | validation: 3.726050312955438]
	TIME [epoch: 5.71 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8585413432048816		[learning rate: 3.887e-05]
	Learning Rate: 3.88701e-05
	LOSS [training: 1.8585413432048816 | validation: 3.7330560319236414]
	TIME [epoch: 5.69 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852536556447434		[learning rate: 3.8733e-05]
	Learning Rate: 3.87326e-05
	LOSS [training: 1.852536556447434 | validation: 3.727627764587321]
	TIME [epoch: 5.69 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853360533578		[learning rate: 3.8596e-05]
	Learning Rate: 3.85957e-05
	LOSS [training: 1.853360533578 | validation: 3.7253828000994447]
	TIME [epoch: 5.74 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851251242799106		[learning rate: 3.8459e-05]
	Learning Rate: 3.84592e-05
	LOSS [training: 1.851251242799106 | validation: 3.725159902441784]
	TIME [epoch: 5.7 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508436642214166		[learning rate: 3.8323e-05]
	Learning Rate: 3.83232e-05
	LOSS [training: 1.8508436642214166 | validation: 3.7281082085056667]
	TIME [epoch: 5.71 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505975693092263		[learning rate: 3.8188e-05]
	Learning Rate: 3.81877e-05
	LOSS [training: 1.8505975693092263 | validation: 3.7344890335375713]
	TIME [epoch: 5.69 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8537378802789855		[learning rate: 3.8053e-05]
	Learning Rate: 3.80526e-05
	LOSS [training: 1.8537378802789855 | validation: 3.7345146729753744]
	TIME [epoch: 5.71 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8503991572418932		[learning rate: 3.7918e-05]
	Learning Rate: 3.79181e-05
	LOSS [training: 1.8503991572418932 | validation: 3.733428752887194]
	TIME [epoch: 5.7 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511556836546637		[learning rate: 3.7784e-05]
	Learning Rate: 3.7784e-05
	LOSS [training: 1.8511556836546637 | validation: 3.732452392528271]
	TIME [epoch: 5.73 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524577803135946		[learning rate: 3.765e-05]
	Learning Rate: 3.76504e-05
	LOSS [training: 1.8524577803135946 | validation: 3.731589275263374]
	TIME [epoch: 5.69 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523492889623379		[learning rate: 3.7517e-05]
	Learning Rate: 3.75172e-05
	LOSS [training: 1.8523492889623379 | validation: 3.7283007041410565]
	TIME [epoch: 5.69 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8559045048445437		[learning rate: 3.7385e-05]
	Learning Rate: 3.73846e-05
	LOSS [training: 1.8559045048445437 | validation: 3.7316469425505407]
	TIME [epoch: 5.7 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.856636981095143		[learning rate: 3.7252e-05]
	Learning Rate: 3.72524e-05
	LOSS [training: 1.856636981095143 | validation: 3.733700813731058]
	TIME [epoch: 5.69 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508455593627398		[learning rate: 3.7121e-05]
	Learning Rate: 3.71206e-05
	LOSS [training: 1.8508455593627398 | validation: 3.7324123953904103]
	TIME [epoch: 5.69 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854879484958221		[learning rate: 3.6989e-05]
	Learning Rate: 3.69894e-05
	LOSS [training: 1.854879484958221 | validation: 3.733568715470536]
	TIME [epoch: 5.73 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850843973876313		[learning rate: 3.6859e-05]
	Learning Rate: 3.68586e-05
	LOSS [training: 1.850843973876313 | validation: 3.735537081889281]
	TIME [epoch: 5.7 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525787772564142		[learning rate: 3.6728e-05]
	Learning Rate: 3.67282e-05
	LOSS [training: 1.8525787772564142 | validation: 3.7279581791364507]
	TIME [epoch: 5.71 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8517238291927158		[learning rate: 3.6598e-05]
	Learning Rate: 3.65984e-05
	LOSS [training: 1.8517238291927158 | validation: 3.7292154288137915]
	TIME [epoch: 5.69 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525709098381542		[learning rate: 3.6469e-05]
	Learning Rate: 3.64689e-05
	LOSS [training: 1.8525709098381542 | validation: 3.725816528389231]
	TIME [epoch: 5.71 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852283345374457		[learning rate: 3.634e-05]
	Learning Rate: 3.634e-05
	LOSS [training: 1.852283345374457 | validation: 3.735320816695603]
	TIME [epoch: 5.69 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855439194072568		[learning rate: 3.6211e-05]
	Learning Rate: 3.62115e-05
	LOSS [training: 1.855439194072568 | validation: 3.736842624697567]
	TIME [epoch: 5.74 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850417641441323		[learning rate: 3.6083e-05]
	Learning Rate: 3.60834e-05
	LOSS [training: 1.850417641441323 | validation: 3.7312524433009977]
	TIME [epoch: 5.7 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8522394471465813		[learning rate: 3.5956e-05]
	Learning Rate: 3.59558e-05
	LOSS [training: 1.8522394471465813 | validation: 3.7359915002501465]
	TIME [epoch: 5.71 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8550399197494232		[learning rate: 3.5829e-05]
	Learning Rate: 3.58287e-05
	LOSS [training: 1.8550399197494232 | validation: 3.734280501415448]
	TIME [epoch: 5.7 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8495356696720304		[learning rate: 3.5702e-05]
	Learning Rate: 3.5702e-05
	LOSS [training: 1.8495356696720304 | validation: 3.7334730212276868]
	TIME [epoch: 5.71 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851858687822521		[learning rate: 3.5576e-05]
	Learning Rate: 3.55757e-05
	LOSS [training: 1.851858687822521 | validation: 3.729438761868363]
	TIME [epoch: 5.71 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8540644297725954		[learning rate: 3.545e-05]
	Learning Rate: 3.54499e-05
	LOSS [training: 1.8540644297725954 | validation: 3.7343537319264684]
	TIME [epoch: 5.74 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854819759703732		[learning rate: 3.5325e-05]
	Learning Rate: 3.53246e-05
	LOSS [training: 1.854819759703732 | validation: 3.7234955665649876]
	TIME [epoch: 5.71 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8552635232259		[learning rate: 3.52e-05]
	Learning Rate: 3.51997e-05
	LOSS [training: 1.8552635232259 | validation: 3.730267594905861]
	TIME [epoch: 5.71 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8537414576508437		[learning rate: 3.5075e-05]
	Learning Rate: 3.50752e-05
	LOSS [training: 1.8537414576508437 | validation: 3.7313453734866524]
	TIME [epoch: 5.71 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850628895989537		[learning rate: 3.4951e-05]
	Learning Rate: 3.49512e-05
	LOSS [training: 1.850628895989537 | validation: 3.729082303037489]
	TIME [epoch: 5.71 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527705910279275		[learning rate: 3.4828e-05]
	Learning Rate: 3.48276e-05
	LOSS [training: 1.8527705910279275 | validation: 3.7326258681400835]
	TIME [epoch: 5.71 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853997700953816		[learning rate: 3.4704e-05]
	Learning Rate: 3.47044e-05
	LOSS [training: 1.853997700953816 | validation: 3.7281360032484985]
	TIME [epoch: 5.74 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85499753003166		[learning rate: 3.4582e-05]
	Learning Rate: 3.45817e-05
	LOSS [training: 1.85499753003166 | validation: 3.731437483300977]
	TIME [epoch: 5.71 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8498552764622533		[learning rate: 3.4459e-05]
	Learning Rate: 3.44594e-05
	LOSS [training: 1.8498552764622533 | validation: 3.7341431395122537]
	TIME [epoch: 5.71 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487442691218954		[learning rate: 3.4338e-05]
	Learning Rate: 3.43375e-05
	LOSS [training: 1.8487442691218954 | validation: 3.7246896079957956]
	TIME [epoch: 5.71 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543805754386615		[learning rate: 3.4216e-05]
	Learning Rate: 3.42161e-05
	LOSS [training: 1.8543805754386615 | validation: 3.7284023432113838]
	TIME [epoch: 5.71 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8560277072990212		[learning rate: 3.4095e-05]
	Learning Rate: 3.40951e-05
	LOSS [training: 1.8560277072990212 | validation: 3.7278202209948232]
	TIME [epoch: 5.71 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553239139923172		[learning rate: 3.3975e-05]
	Learning Rate: 3.39746e-05
	LOSS [training: 1.8553239139923172 | validation: 3.725704425038748]
	TIME [epoch: 5.75 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.857940038176176		[learning rate: 3.3854e-05]
	Learning Rate: 3.38544e-05
	LOSS [training: 1.857940038176176 | validation: 3.732713655274795]
	TIME [epoch: 5.71 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8507968088976918		[learning rate: 3.3735e-05]
	Learning Rate: 3.37347e-05
	LOSS [training: 1.8507968088976918 | validation: 3.72912514503954]
	TIME [epoch: 5.71 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556379572544999		[learning rate: 3.3615e-05]
	Learning Rate: 3.36154e-05
	LOSS [training: 1.8556379572544999 | validation: 3.731082258378145]
	TIME [epoch: 5.71 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543766138567488		[learning rate: 3.3497e-05]
	Learning Rate: 3.34965e-05
	LOSS [training: 1.8543766138567488 | validation: 3.7247703676156823]
	TIME [epoch: 5.71 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532747687038023		[learning rate: 3.3378e-05]
	Learning Rate: 3.33781e-05
	LOSS [training: 1.8532747687038023 | validation: 3.729620220541418]
	TIME [epoch: 5.71 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8512106617214914		[learning rate: 3.326e-05]
	Learning Rate: 3.32601e-05
	LOSS [training: 1.8512106617214914 | validation: 3.729796204036936]
	TIME [epoch: 5.74 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518813822072946		[learning rate: 3.3142e-05]
	Learning Rate: 3.31425e-05
	LOSS [training: 1.8518813822072946 | validation: 3.73089775091575]
	TIME [epoch: 5.69 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854951083149847		[learning rate: 3.3025e-05]
	Learning Rate: 3.30253e-05
	LOSS [training: 1.854951083149847 | validation: 3.72517149464521]
	TIME [epoch: 5.69 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8510243257501877		[learning rate: 3.2908e-05]
	Learning Rate: 3.29085e-05
	LOSS [training: 1.8510243257501877 | validation: 3.7311732244198494]
	TIME [epoch: 5.69 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8516638677237105		[learning rate: 3.2792e-05]
	Learning Rate: 3.27921e-05
	LOSS [training: 1.8516638677237105 | validation: 3.7282756091921065]
	TIME [epoch: 5.69 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505473488628383		[learning rate: 3.2676e-05]
	Learning Rate: 3.26761e-05
	LOSS [training: 1.8505473488628383 | validation: 3.7303720134178673]
	TIME [epoch: 5.71 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8533869147476705		[learning rate: 3.2561e-05]
	Learning Rate: 3.25606e-05
	LOSS [training: 1.8533869147476705 | validation: 3.7325104513371317]
	TIME [epoch: 5.74 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8516979965394844		[learning rate: 3.2445e-05]
	Learning Rate: 3.24455e-05
	LOSS [training: 1.8516979965394844 | validation: 3.728971983302921]
	TIME [epoch: 5.71 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853606684059704		[learning rate: 3.2331e-05]
	Learning Rate: 3.23307e-05
	LOSS [training: 1.853606684059704 | validation: 3.73282285073174]
	TIME [epoch: 5.71 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529533199815218		[learning rate: 3.2216e-05]
	Learning Rate: 3.22164e-05
	LOSS [training: 1.8529533199815218 | validation: 3.727227488291045]
	TIME [epoch: 5.71 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8552507391953936		[learning rate: 3.2102e-05]
	Learning Rate: 3.21025e-05
	LOSS [training: 1.8552507391953936 | validation: 3.7344802311171676]
	TIME [epoch: 5.71 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850577318641163		[learning rate: 3.1989e-05]
	Learning Rate: 3.1989e-05
	LOSS [training: 1.850577318641163 | validation: 3.727612851025223]
	TIME [epoch: 5.71 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529547554807972		[learning rate: 3.1876e-05]
	Learning Rate: 3.18758e-05
	LOSS [training: 1.8529547554807972 | validation: 3.73199664519716]
	TIME [epoch: 5.75 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8515713237348013		[learning rate: 3.1763e-05]
	Learning Rate: 3.17631e-05
	LOSS [training: 1.8515713237348013 | validation: 3.72647879389884]
	TIME [epoch: 5.71 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8533449073450465		[learning rate: 3.1651e-05]
	Learning Rate: 3.16508e-05
	LOSS [training: 1.8533449073450465 | validation: 3.731218075885008]
	TIME [epoch: 5.71 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551622431857853		[learning rate: 3.1539e-05]
	Learning Rate: 3.15389e-05
	LOSS [training: 1.8551622431857853 | validation: 3.733334462094108]
	TIME [epoch: 5.71 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525481494020006		[learning rate: 3.1427e-05]
	Learning Rate: 3.14274e-05
	LOSS [training: 1.8525481494020006 | validation: 3.7271864588543293]
	TIME [epoch: 5.69 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8510972976647277		[learning rate: 3.1316e-05]
	Learning Rate: 3.13162e-05
	LOSS [training: 1.8510972976647277 | validation: 3.7330399393978713]
	TIME [epoch: 5.69 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8547037242559945		[learning rate: 3.1205e-05]
	Learning Rate: 3.12055e-05
	LOSS [training: 1.8547037242559945 | validation: 3.7294147621609084]
	TIME [epoch: 5.72 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8504374917335025		[learning rate: 3.1095e-05]
	Learning Rate: 3.10951e-05
	LOSS [training: 1.8504374917335025 | validation: 3.7213224789177106]
	TIME [epoch: 5.71 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8548024990419048		[learning rate: 3.0985e-05]
	Learning Rate: 3.09852e-05
	LOSS [training: 1.8548024990419048 | validation: 3.7301820137180632]
	TIME [epoch: 5.71 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549220712016041		[learning rate: 3.0876e-05]
	Learning Rate: 3.08756e-05
	LOSS [training: 1.8549220712016041 | validation: 3.728672617557345]
	TIME [epoch: 5.71 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8566078971661142		[learning rate: 3.0766e-05]
	Learning Rate: 3.07664e-05
	LOSS [training: 1.8566078971661142 | validation: 3.7331694027558435]
	TIME [epoch: 5.69 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8551387916916706		[learning rate: 3.0658e-05]
	Learning Rate: 3.06576e-05
	LOSS [training: 1.8551387916916706 | validation: 3.7282094201781253]
	TIME [epoch: 5.69 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8512329088017179		[learning rate: 3.0549e-05]
	Learning Rate: 3.05492e-05
	LOSS [training: 1.8512329088017179 | validation: 3.7381881482831587]
	TIME [epoch: 5.72 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532263233085942		[learning rate: 3.0441e-05]
	Learning Rate: 3.04412e-05
	LOSS [training: 1.8532263233085942 | validation: 3.7332760566495518]
	TIME [epoch: 5.69 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542782995062725		[learning rate: 3.0334e-05]
	Learning Rate: 3.03336e-05
	LOSS [training: 1.8542782995062725 | validation: 3.7313935941152785]
	TIME [epoch: 5.69 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8522938215792928		[learning rate: 3.0226e-05]
	Learning Rate: 3.02263e-05
	LOSS [training: 1.8522938215792928 | validation: 3.730300055975499]
	TIME [epoch: 5.69 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853141251714463		[learning rate: 3.0119e-05]
	Learning Rate: 3.01194e-05
	LOSS [training: 1.853141251714463 | validation: 3.7313482931884288]
	TIME [epoch: 5.69 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851021206444341		[learning rate: 3.0013e-05]
	Learning Rate: 3.00129e-05
	LOSS [training: 1.851021206444341 | validation: 3.7329579364031815]
	TIME [epoch: 5.69 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8485714493755565		[learning rate: 2.9907e-05]
	Learning Rate: 2.99068e-05
	LOSS [training: 1.8485714493755565 | validation: 3.720615025272487]
	TIME [epoch: 5.72 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538559522669198		[learning rate: 2.9801e-05]
	Learning Rate: 2.9801e-05
	LOSS [training: 1.8538559522669198 | validation: 3.7306883044463777]
	TIME [epoch: 5.69 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534117683967772		[learning rate: 2.9696e-05]
	Learning Rate: 2.96956e-05
	LOSS [training: 1.8534117683967772 | validation: 3.727904909874104]
	TIME [epoch: 5.69 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536479204008498		[learning rate: 2.9591e-05]
	Learning Rate: 2.95906e-05
	LOSS [training: 1.8536479204008498 | validation: 3.729584786880814]
	TIME [epoch: 5.69 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523360334692494		[learning rate: 2.9486e-05]
	Learning Rate: 2.9486e-05
	LOSS [training: 1.8523360334692494 | validation: 3.733063832174092]
	TIME [epoch: 5.69 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518693062848517		[learning rate: 2.9382e-05]
	Learning Rate: 2.93817e-05
	LOSS [training: 1.8518693062848517 | validation: 3.7221218014208723]
	TIME [epoch: 5.69 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524631087467922		[learning rate: 2.9278e-05]
	Learning Rate: 2.92778e-05
	LOSS [training: 1.8524631087467922 | validation: 3.731290306466929]
	TIME [epoch: 5.72 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8509970439184231		[learning rate: 2.9174e-05]
	Learning Rate: 2.91743e-05
	LOSS [training: 1.8509970439184231 | validation: 3.7319242037189655]
	TIME [epoch: 5.69 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532246129572911		[learning rate: 2.9071e-05]
	Learning Rate: 2.90711e-05
	LOSS [training: 1.8532246129572911 | validation: 3.7355199089427003]
	TIME [epoch: 5.69 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8520723983187706		[learning rate: 2.8968e-05]
	Learning Rate: 2.89683e-05
	LOSS [training: 1.8520723983187706 | validation: 3.7357489966421586]
	TIME [epoch: 5.71 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8528832093946102		[learning rate: 2.8866e-05]
	Learning Rate: 2.88659e-05
	LOSS [training: 1.8528832093946102 | validation: 3.732314076624106]
	TIME [epoch: 5.71 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526702013971255		[learning rate: 2.8764e-05]
	Learning Rate: 2.87638e-05
	LOSS [training: 1.8526702013971255 | validation: 3.7385207032240886]
	TIME [epoch: 5.71 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8540998242006512		[learning rate: 2.8662e-05]
	Learning Rate: 2.86621e-05
	LOSS [training: 1.8540998242006512 | validation: 3.73718409297237]
	TIME [epoch: 5.75 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8537722508784809		[learning rate: 2.8561e-05]
	Learning Rate: 2.85607e-05
	LOSS [training: 1.8537722508784809 | validation: 3.7277824844547123]
	TIME [epoch: 5.71 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8516242164504115		[learning rate: 2.846e-05]
	Learning Rate: 2.84597e-05
	LOSS [training: 1.8516242164504115 | validation: 3.722143501397544]
	TIME [epoch: 5.69 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8502321026456945		[learning rate: 2.8359e-05]
	Learning Rate: 2.83591e-05
	LOSS [training: 1.8502321026456945 | validation: 3.7360016593889576]
	TIME [epoch: 5.71 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849573534839064		[learning rate: 2.8259e-05]
	Learning Rate: 2.82588e-05
	LOSS [training: 1.849573534839064 | validation: 3.7383199181739735]
	TIME [epoch: 5.71 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513247775291317		[learning rate: 2.8159e-05]
	Learning Rate: 2.81589e-05
	LOSS [training: 1.8513247775291317 | validation: 3.7322676687045067]
	TIME [epoch: 5.71 sec]
EPOCH 1709/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8535932784614582		[learning rate: 2.8059e-05]
	Learning Rate: 2.80593e-05
	LOSS [training: 1.8535932784614582 | validation: 3.732293560215486]
	TIME [epoch: 5.74 sec]
EPOCH 1710/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8502910205510656		[learning rate: 2.796e-05]
	Learning Rate: 2.79601e-05
	LOSS [training: 1.8502910205510656 | validation: 3.7391174944260293]
	TIME [epoch: 5.71 sec]
EPOCH 1711/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8550431888673742		[learning rate: 2.7861e-05]
	Learning Rate: 2.78612e-05
	LOSS [training: 1.8550431888673742 | validation: 3.72808621819377]
	TIME [epoch: 5.71 sec]
EPOCH 1712/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553157042881288		[learning rate: 2.7763e-05]
	Learning Rate: 2.77627e-05
	LOSS [training: 1.8553157042881288 | validation: 3.737740754983331]
	TIME [epoch: 5.71 sec]
EPOCH 1713/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8512457635821093		[learning rate: 2.7665e-05]
	Learning Rate: 2.76645e-05
	LOSS [training: 1.8512457635821093 | validation: 3.739083378771393]
	TIME [epoch: 5.7 sec]
EPOCH 1714/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8575029564874936		[learning rate: 2.7567e-05]
	Learning Rate: 2.75667e-05
	LOSS [training: 1.8575029564874936 | validation: 3.734300158165415]
	TIME [epoch: 5.71 sec]
EPOCH 1715/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526399406558522		[learning rate: 2.7469e-05]
	Learning Rate: 2.74692e-05
	LOSS [training: 1.8526399406558522 | validation: 3.7313688801840916]
	TIME [epoch: 5.73 sec]
EPOCH 1716/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855926145758982		[learning rate: 2.7372e-05]
	Learning Rate: 2.73721e-05
	LOSS [training: 1.855926145758982 | validation: 3.729286460059795]
	TIME [epoch: 5.69 sec]
EPOCH 1717/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852481179062163		[learning rate: 2.7275e-05]
	Learning Rate: 2.72753e-05
	LOSS [training: 1.852481179062163 | validation: 3.7286193170836897]
	TIME [epoch: 5.69 sec]
EPOCH 1718/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853596979901722		[learning rate: 2.7179e-05]
	Learning Rate: 2.71788e-05
	LOSS [training: 1.853596979901722 | validation: 3.7284838600203023]
	TIME [epoch: 5.71 sec]
EPOCH 1719/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8544849053250199		[learning rate: 2.7083e-05]
	Learning Rate: 2.70827e-05
	LOSS [training: 1.8544849053250199 | validation: 3.7259465212426774]
	TIME [epoch: 5.69 sec]
EPOCH 1720/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8520273993857175		[learning rate: 2.6987e-05]
	Learning Rate: 2.6987e-05
	LOSS [training: 1.8520273993857175 | validation: 3.730111317877521]
	TIME [epoch: 5.69 sec]
EPOCH 1721/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852321245441985		[learning rate: 2.6892e-05]
	Learning Rate: 2.68915e-05
	LOSS [training: 1.852321245441985 | validation: 3.7234485116167217]
	TIME [epoch: 5.72 sec]
EPOCH 1722/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487246306677387		[learning rate: 2.6796e-05]
	Learning Rate: 2.67964e-05
	LOSS [training: 1.8487246306677387 | validation: 3.7251368489746692]
	TIME [epoch: 5.69 sec]
EPOCH 1723/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852397830630768		[learning rate: 2.6702e-05]
	Learning Rate: 2.67017e-05
	LOSS [training: 1.852397830630768 | validation: 3.7326279561111946]
	TIME [epoch: 5.7 sec]
EPOCH 1724/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529806692794697		[learning rate: 2.6607e-05]
	Learning Rate: 2.66073e-05
	LOSS [training: 1.8529806692794697 | validation: 3.7374546171161787]
	TIME [epoch: 5.71 sec]
EPOCH 1725/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8516526256536108		[learning rate: 2.6513e-05]
	Learning Rate: 2.65132e-05
	LOSS [training: 1.8516526256536108 | validation: 3.7264317991041005]
	TIME [epoch: 5.69 sec]
EPOCH 1726/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8506487835883592		[learning rate: 2.6419e-05]
	Learning Rate: 2.64194e-05
	LOSS [training: 1.8506487835883592 | validation: 3.734052055307572]
	TIME [epoch: 5.69 sec]
EPOCH 1727/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514171295257926		[learning rate: 2.6326e-05]
	Learning Rate: 2.6326e-05
	LOSS [training: 1.8514171295257926 | validation: 3.728781563353272]
	TIME [epoch: 5.73 sec]
EPOCH 1728/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505647336272568		[learning rate: 2.6233e-05]
	Learning Rate: 2.62329e-05
	LOSS [training: 1.8505647336272568 | validation: 3.729961703608081]
	TIME [epoch: 5.71 sec]
EPOCH 1729/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8512873186564118		[learning rate: 2.614e-05]
	Learning Rate: 2.61401e-05
	LOSS [training: 1.8512873186564118 | validation: 3.7359529175502493]
	TIME [epoch: 5.69 sec]
EPOCH 1730/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854654685282092		[learning rate: 2.6048e-05]
	Learning Rate: 2.60477e-05
	LOSS [training: 1.854654685282092 | validation: 3.7277991935038375]
	TIME [epoch: 5.71 sec]
EPOCH 1731/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524054110833754		[learning rate: 2.5956e-05]
	Learning Rate: 2.59556e-05
	LOSS [training: 1.8524054110833754 | validation: 3.7305079710488247]
	TIME [epoch: 5.7 sec]
EPOCH 1732/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854978721272575		[learning rate: 2.5864e-05]
	Learning Rate: 2.58638e-05
	LOSS [training: 1.854978721272575 | validation: 3.737298742781285]
	TIME [epoch: 5.71 sec]
EPOCH 1733/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532309263318156		[learning rate: 2.5772e-05]
	Learning Rate: 2.57723e-05
	LOSS [training: 1.8532309263318156 | validation: 3.735667119665286]
	TIME [epoch: 5.72 sec]
EPOCH 1734/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518331139070097		[learning rate: 2.5681e-05]
	Learning Rate: 2.56812e-05
	LOSS [training: 1.8518331139070097 | validation: 3.73136126270868]
	TIME [epoch: 5.71 sec]
EPOCH 1735/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527039337447353		[learning rate: 2.559e-05]
	Learning Rate: 2.55904e-05
	LOSS [training: 1.8527039337447353 | validation: 3.727967591478972]
	TIME [epoch: 5.69 sec]
EPOCH 1736/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8496201847343572		[learning rate: 2.55e-05]
	Learning Rate: 2.54999e-05
	LOSS [training: 1.8496201847343572 | validation: 3.7319989378309577]
	TIME [epoch: 5.69 sec]
EPOCH 1737/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8504572537424127		[learning rate: 2.541e-05]
	Learning Rate: 2.54097e-05
	LOSS [training: 1.8504572537424127 | validation: 3.7355974031405608]
	TIME [epoch: 5.69 sec]
EPOCH 1738/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8548225209923737		[learning rate: 2.532e-05]
	Learning Rate: 2.53199e-05
	LOSS [training: 1.8548225209923737 | validation: 3.728103066356133]
	TIME [epoch: 5.69 sec]
EPOCH 1739/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8490660787515703		[learning rate: 2.523e-05]
	Learning Rate: 2.52303e-05
	LOSS [training: 1.8490660787515703 | validation: 3.730867483360159]
	TIME [epoch: 5.74 sec]
EPOCH 1740/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8521851146168666		[learning rate: 2.5141e-05]
	Learning Rate: 2.51411e-05
	LOSS [training: 1.8521851146168666 | validation: 3.7280207046546003]
	TIME [epoch: 5.69 sec]
EPOCH 1741/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536063277137802		[learning rate: 2.5052e-05]
	Learning Rate: 2.50522e-05
	LOSS [training: 1.8536063277137802 | validation: 3.7316915657964467]
	TIME [epoch: 5.69 sec]
EPOCH 1742/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8495096703701293		[learning rate: 2.4964e-05]
	Learning Rate: 2.49636e-05
	LOSS [training: 1.8495096703701293 | validation: 3.7318165000466332]
	TIME [epoch: 5.69 sec]
EPOCH 1743/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853920275116662		[learning rate: 2.4875e-05]
	Learning Rate: 2.48754e-05
	LOSS [training: 1.853920275116662 | validation: 3.730758635522848]
	TIME [epoch: 5.69 sec]
EPOCH 1744/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854673943097552		[learning rate: 2.4787e-05]
	Learning Rate: 2.47874e-05
	LOSS [training: 1.854673943097552 | validation: 3.7303948623046757]
	TIME [epoch: 5.69 sec]
EPOCH 1745/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8491487823347752		[learning rate: 2.47e-05]
	Learning Rate: 2.46997e-05
	LOSS [training: 1.8491487823347752 | validation: 3.7272228085425207]
	TIME [epoch: 5.72 sec]
EPOCH 1746/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8517651348952793		[learning rate: 2.4612e-05]
	Learning Rate: 2.46124e-05
	LOSS [training: 1.8517651348952793 | validation: 3.738392711197188]
	TIME [epoch: 5.69 sec]
EPOCH 1747/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8547212497722598		[learning rate: 2.4525e-05]
	Learning Rate: 2.45254e-05
	LOSS [training: 1.8547212497722598 | validation: 3.7368152849760143]
	TIME [epoch: 5.69 sec]
EPOCH 1748/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850797128335421		[learning rate: 2.4439e-05]
	Learning Rate: 2.44386e-05
	LOSS [training: 1.850797128335421 | validation: 3.7301718178998806]
	TIME [epoch: 5.71 sec]
EPOCH 1749/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851026797948625		[learning rate: 2.4352e-05]
	Learning Rate: 2.43522e-05
	LOSS [training: 1.851026797948625 | validation: 3.7285160117096026]
	TIME [epoch: 5.71 sec]
EPOCH 1750/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852636223981296		[learning rate: 2.4266e-05]
	Learning Rate: 2.42661e-05
	LOSS [training: 1.852636223981296 | validation: 3.7338051346392174]
	TIME [epoch: 5.69 sec]
EPOCH 1751/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852277458953406		[learning rate: 2.418e-05]
	Learning Rate: 2.41803e-05
	LOSS [training: 1.852277458953406 | validation: 3.7363582889110716]
	TIME [epoch: 5.72 sec]
EPOCH 1752/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8539657642350826		[learning rate: 2.4095e-05]
	Learning Rate: 2.40948e-05
	LOSS [training: 1.8539657642350826 | validation: 3.7322960791323014]
	TIME [epoch: 5.69 sec]
EPOCH 1753/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8560233465759315		[learning rate: 2.401e-05]
	Learning Rate: 2.40096e-05
	LOSS [training: 1.8560233465759315 | validation: 3.7402824280512124]
	TIME [epoch: 5.69 sec]
EPOCH 1754/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8553491796808066		[learning rate: 2.3925e-05]
	Learning Rate: 2.39247e-05
	LOSS [training: 1.8553491796808066 | validation: 3.733481536947537]
	TIME [epoch: 5.69 sec]
EPOCH 1755/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8517509735541708		[learning rate: 2.384e-05]
	Learning Rate: 2.38401e-05
	LOSS [training: 1.8517509735541708 | validation: 3.7286744523224717]
	TIME [epoch: 5.69 sec]
EPOCH 1756/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8502604746574876		[learning rate: 2.3756e-05]
	Learning Rate: 2.37558e-05
	LOSS [training: 1.8502604746574876 | validation: 3.723194393230947]
	TIME [epoch: 5.69 sec]
EPOCH 1757/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.84916000614793		[learning rate: 2.3672e-05]
	Learning Rate: 2.36718e-05
	LOSS [training: 1.84916000614793 | validation: 3.736153749532623]
	TIME [epoch: 5.73 sec]
EPOCH 1758/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849774869757364		[learning rate: 2.3588e-05]
	Learning Rate: 2.35881e-05
	LOSS [training: 1.849774869757364 | validation: 3.72580520250717]
	TIME [epoch: 5.71 sec]
EPOCH 1759/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542427306031894		[learning rate: 2.3505e-05]
	Learning Rate: 2.35047e-05
	LOSS [training: 1.8542427306031894 | validation: 3.729369545234846]
	TIME [epoch: 5.69 sec]
EPOCH 1760/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8530344512443702		[learning rate: 2.3422e-05]
	Learning Rate: 2.34215e-05
	LOSS [training: 1.8530344512443702 | validation: 3.730070455314158]
	TIME [epoch: 5.69 sec]
EPOCH 1761/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8533433194601867		[learning rate: 2.3339e-05]
	Learning Rate: 2.33387e-05
	LOSS [training: 1.8533433194601867 | validation: 3.723242092786437]
	TIME [epoch: 5.7 sec]
EPOCH 1762/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848546373701226		[learning rate: 2.3256e-05]
	Learning Rate: 2.32562e-05
	LOSS [training: 1.848546373701226 | validation: 3.732852587450568]
	TIME [epoch: 5.7 sec]
EPOCH 1763/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8507811853911824		[learning rate: 2.3174e-05]
	Learning Rate: 2.31739e-05
	LOSS [training: 1.8507811853911824 | validation: 3.7293099994245167]
	TIME [epoch: 5.72 sec]
EPOCH 1764/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525804570262467		[learning rate: 2.3092e-05]
	Learning Rate: 2.3092e-05
	LOSS [training: 1.8525804570262467 | validation: 3.731144813056153]
	TIME [epoch: 5.69 sec]
EPOCH 1765/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538686278519076		[learning rate: 2.301e-05]
	Learning Rate: 2.30103e-05
	LOSS [training: 1.8538686278519076 | validation: 3.7297890372278073]
	TIME [epoch: 5.69 sec]
EPOCH 1766/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855924179835214		[learning rate: 2.2929e-05]
	Learning Rate: 2.2929e-05
	LOSS [training: 1.855924179835214 | validation: 3.730818428367272]
	TIME [epoch: 5.69 sec]
EPOCH 1767/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8521027754004504		[learning rate: 2.2848e-05]
	Learning Rate: 2.28479e-05
	LOSS [training: 1.8521027754004504 | validation: 3.730460428929506]
	TIME [epoch: 5.71 sec]
EPOCH 1768/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852739020584273		[learning rate: 2.2767e-05]
	Learning Rate: 2.27671e-05
	LOSS [training: 1.852739020584273 | validation: 3.7305613799452324]
	TIME [epoch: 5.69 sec]
EPOCH 1769/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525505527275645		[learning rate: 2.2687e-05]
	Learning Rate: 2.26866e-05
	LOSS [training: 1.8525505527275645 | validation: 3.735665878729985]
	TIME [epoch: 5.74 sec]
EPOCH 1770/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518911945308802		[learning rate: 2.2606e-05]
	Learning Rate: 2.26064e-05
	LOSS [training: 1.8518911945308802 | validation: 3.7350121613691183]
	TIME [epoch: 5.69 sec]
EPOCH 1771/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8510633254468927		[learning rate: 2.2526e-05]
	Learning Rate: 2.25264e-05
	LOSS [training: 1.8510633254468927 | validation: 3.7238623926210916]
	TIME [epoch: 5.69 sec]
EPOCH 1772/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8503191192651267		[learning rate: 2.2447e-05]
	Learning Rate: 2.24468e-05
	LOSS [training: 1.8503191192651267 | validation: 3.7167645954719606]
	TIME [epoch: 5.69 sec]
	Saving model to: out/transition_rate_study_model_training_kl2/tr_study202/model_tr_study202_r1_20240310_015135/states/model_tr_study202_1772.pth
	Model improved!!!
EPOCH 1773/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851416945160446		[learning rate: 2.2367e-05]
	Learning Rate: 2.23674e-05
	LOSS [training: 1.851416945160446 | validation: 3.724055922226522]
	TIME [epoch: 5.69 sec]
EPOCH 1774/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536575355754805		[learning rate: 2.2288e-05]
	Learning Rate: 2.22883e-05
	LOSS [training: 1.8536575355754805 | validation: 3.7359992372056365]
	TIME [epoch: 5.69 sec]
EPOCH 1775/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85308378550552		[learning rate: 2.2209e-05]
	Learning Rate: 2.22095e-05
	LOSS [training: 1.85308378550552 | validation: 3.7253268553600085]
	TIME [epoch: 5.72 sec]
EPOCH 1776/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534362644783344		[learning rate: 2.2131e-05]
	Learning Rate: 2.21309e-05
	LOSS [training: 1.8534362644783344 | validation: 3.731463797397223]
	TIME [epoch: 5.69 sec]
EPOCH 1777/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523958016040554		[learning rate: 2.2053e-05]
	Learning Rate: 2.20527e-05
	LOSS [training: 1.8523958016040554 | validation: 3.7268912953786186]
	TIME [epoch: 5.69 sec]
EPOCH 1778/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849528205434759		[learning rate: 2.1975e-05]
	Learning Rate: 2.19747e-05
	LOSS [training: 1.849528205434759 | validation: 3.727241255819261]
	TIME [epoch: 5.69 sec]
EPOCH 1779/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8503914169531543		[learning rate: 2.1897e-05]
	Learning Rate: 2.1897e-05
	LOSS [training: 1.8503914169531543 | validation: 3.7301740480695003]
	TIME [epoch: 5.69 sec]
EPOCH 1780/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8530288834620823		[learning rate: 2.182e-05]
	Learning Rate: 2.18196e-05
	LOSS [training: 1.8530288834620823 | validation: 3.733673888684798]
	TIME [epoch: 5.69 sec]
EPOCH 1781/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852849779420135		[learning rate: 2.1742e-05]
	Learning Rate: 2.17424e-05
	LOSS [training: 1.852849779420135 | validation: 3.730618056008822]
	TIME [epoch: 5.74 sec]
EPOCH 1782/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8556149201626715		[learning rate: 2.1666e-05]
	Learning Rate: 2.16655e-05
	LOSS [training: 1.8556149201626715 | validation: 3.73131259273364]
	TIME [epoch: 5.71 sec]
EPOCH 1783/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8494551830456716		[learning rate: 2.1589e-05]
	Learning Rate: 2.15889e-05
	LOSS [training: 1.8494551830456716 | validation: 3.7238707595857363]
	TIME [epoch: 5.69 sec]
EPOCH 1784/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8458293792255875		[learning rate: 2.1513e-05]
	Learning Rate: 2.15126e-05
	LOSS [training: 1.8458293792255875 | validation: 3.7256164585533798]
	TIME [epoch: 5.69 sec]
EPOCH 1785/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8543510457802133		[learning rate: 2.1437e-05]
	Learning Rate: 2.14365e-05
	LOSS [training: 1.8543510457802133 | validation: 3.7284339286828434]
	TIME [epoch: 5.7 sec]
EPOCH 1786/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529191263131977		[learning rate: 2.1361e-05]
	Learning Rate: 2.13607e-05
	LOSS [training: 1.8529191263131977 | validation: 3.7306196469361397]
	TIME [epoch: 5.69 sec]
EPOCH 1787/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529499711972766		[learning rate: 2.1285e-05]
	Learning Rate: 2.12852e-05
	LOSS [training: 1.8529499711972766 | validation: 3.7287349663172584]
	TIME [epoch: 5.72 sec]
EPOCH 1788/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513038721207875		[learning rate: 2.121e-05]
	Learning Rate: 2.12099e-05
	LOSS [training: 1.8513038721207875 | validation: 3.73217148997418]
	TIME [epoch: 5.69 sec]
EPOCH 1789/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853584696155889		[learning rate: 2.1135e-05]
	Learning Rate: 2.11349e-05
	LOSS [training: 1.853584696155889 | validation: 3.726585971184491]
	TIME [epoch: 5.69 sec]
EPOCH 1790/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8490814407959917		[learning rate: 2.106e-05]
	Learning Rate: 2.10602e-05
	LOSS [training: 1.8490814407959917 | validation: 3.7245806460667383]
	TIME [epoch: 5.69 sec]
EPOCH 1791/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851030002523256		[learning rate: 2.0986e-05]
	Learning Rate: 2.09857e-05
	LOSS [training: 1.851030002523256 | validation: 3.7283104606783026]
	TIME [epoch: 5.69 sec]
EPOCH 1792/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8490214910580212		[learning rate: 2.0911e-05]
	Learning Rate: 2.09115e-05
	LOSS [training: 1.8490214910580212 | validation: 3.730094397925358]
	TIME [epoch: 5.69 sec]
EPOCH 1793/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505367564934347		[learning rate: 2.0838e-05]
	Learning Rate: 2.08375e-05
	LOSS [training: 1.8505367564934347 | validation: 3.7327707137613446]
	TIME [epoch: 5.73 sec]
EPOCH 1794/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8535043381405034		[learning rate: 2.0764e-05]
	Learning Rate: 2.07638e-05
	LOSS [training: 1.8535043381405034 | validation: 3.734978145035849]
	TIME [epoch: 5.71 sec]
EPOCH 1795/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527978958351654		[learning rate: 2.069e-05]
	Learning Rate: 2.06904e-05
	LOSS [training: 1.8527978958351654 | validation: 3.7323892120079494]
	TIME [epoch: 5.7 sec]
EPOCH 1796/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8503889998223657		[learning rate: 2.0617e-05]
	Learning Rate: 2.06173e-05
	LOSS [training: 1.8503889998223657 | validation: 3.7275625894542803]
	TIME [epoch: 5.69 sec]
EPOCH 1797/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8506366522277937		[learning rate: 2.0544e-05]
	Learning Rate: 2.05444e-05
	LOSS [training: 1.8506366522277937 | validation: 3.7286788067319354]
	TIME [epoch: 5.7 sec]
EPOCH 1798/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848868677915794		[learning rate: 2.0472e-05]
	Learning Rate: 2.04717e-05
	LOSS [training: 1.848868677915794 | validation: 3.733377428663486]
	TIME [epoch: 5.69 sec]
EPOCH 1799/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8495531608323055		[learning rate: 2.0399e-05]
	Learning Rate: 2.03993e-05
	LOSS [training: 1.8495531608323055 | validation: 3.7301386311366667]
	TIME [epoch: 5.73 sec]
EPOCH 1800/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524148294956602		[learning rate: 2.0327e-05]
	Learning Rate: 2.03272e-05
	LOSS [training: 1.8524148294956602 | validation: 3.728227334188562]
	TIME [epoch: 5.7 sec]
EPOCH 1801/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8545991402572213		[learning rate: 2.0255e-05]
	Learning Rate: 2.02553e-05
	LOSS [training: 1.8545991402572213 | validation: 3.7307467979275817]
	TIME [epoch: 5.7 sec]
EPOCH 1802/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8502537185464225		[learning rate: 2.0184e-05]
	Learning Rate: 2.01837e-05
	LOSS [training: 1.8502537185464225 | validation: 3.7297738285325908]
	TIME [epoch: 5.69 sec]
EPOCH 1803/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852540120216077		[learning rate: 2.0112e-05]
	Learning Rate: 2.01123e-05
	LOSS [training: 1.852540120216077 | validation: 3.730442483045728]
	TIME [epoch: 5.7 sec]
EPOCH 1804/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8495336362046337		[learning rate: 2.0041e-05]
	Learning Rate: 2.00412e-05
	LOSS [training: 1.8495336362046337 | validation: 3.728892281884191]
	TIME [epoch: 5.7 sec]
EPOCH 1805/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8510755706292494		[learning rate: 1.997e-05]
	Learning Rate: 1.99703e-05
	LOSS [training: 1.8510755706292494 | validation: 3.7270281584706586]
	TIME [epoch: 5.74 sec]
EPOCH 1806/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8510295700979558		[learning rate: 1.99e-05]
	Learning Rate: 1.98997e-05
	LOSS [training: 1.8510295700979558 | validation: 3.729817963654123]
	TIME [epoch: 5.71 sec]
EPOCH 1807/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849415509751016		[learning rate: 1.9829e-05]
	Learning Rate: 1.98293e-05
	LOSS [training: 1.849415509751016 | validation: 3.7279997127048943]
	TIME [epoch: 5.69 sec]
EPOCH 1808/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8485851321034066		[learning rate: 1.9759e-05]
	Learning Rate: 1.97592e-05
	LOSS [training: 1.8485851321034066 | validation: 3.7236190271056446]
	TIME [epoch: 5.69 sec]
EPOCH 1809/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853285547500315		[learning rate: 1.9689e-05]
	Learning Rate: 1.96893e-05
	LOSS [training: 1.853285547500315 | validation: 3.7274230311591463]
	TIME [epoch: 5.69 sec]
EPOCH 1810/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8507157837424715		[learning rate: 1.962e-05]
	Learning Rate: 1.96197e-05
	LOSS [training: 1.8507157837424715 | validation: 3.73274804064122]
	TIME [epoch: 5.69 sec]
EPOCH 1811/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850161167660604		[learning rate: 1.955e-05]
	Learning Rate: 1.95503e-05
	LOSS [training: 1.850161167660604 | validation: 3.726678047112186]
	TIME [epoch: 5.73 sec]
EPOCH 1812/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8517751817825565		[learning rate: 1.9481e-05]
	Learning Rate: 1.94812e-05
	LOSS [training: 1.8517751817825565 | validation: 3.722650461562005]
	TIME [epoch: 5.71 sec]
EPOCH 1813/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8504246905789286		[learning rate: 1.9412e-05]
	Learning Rate: 1.94123e-05
	LOSS [training: 1.8504246905789286 | validation: 3.7290097989172315]
	TIME [epoch: 5.71 sec]
EPOCH 1814/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8522940792797258		[learning rate: 1.9344e-05]
	Learning Rate: 1.93437e-05
	LOSS [training: 1.8522940792797258 | validation: 3.727557408508684]
	TIME [epoch: 5.71 sec]
EPOCH 1815/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8491694981717821		[learning rate: 1.9275e-05]
	Learning Rate: 1.92753e-05
	LOSS [training: 1.8491694981717821 | validation: 3.7389925215945556]
	TIME [epoch: 5.71 sec]
EPOCH 1816/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852550214381088		[learning rate: 1.9207e-05]
	Learning Rate: 1.92071e-05
	LOSS [training: 1.852550214381088 | validation: 3.7290964502034933]
	TIME [epoch: 5.71 sec]
EPOCH 1817/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849295129532245		[learning rate: 1.9139e-05]
	Learning Rate: 1.91392e-05
	LOSS [training: 1.849295129532245 | validation: 3.731070660202096]
	TIME [epoch: 5.74 sec]
EPOCH 1818/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8509339234160995		[learning rate: 1.9071e-05]
	Learning Rate: 1.90715e-05
	LOSS [training: 1.8509339234160995 | validation: 3.727725066583356]
	TIME [epoch: 5.71 sec]
EPOCH 1819/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8512264988416405		[learning rate: 1.9004e-05]
	Learning Rate: 1.90041e-05
	LOSS [training: 1.8512264988416405 | validation: 3.73174536451837]
	TIME [epoch: 5.71 sec]
EPOCH 1820/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508276594970363		[learning rate: 1.8937e-05]
	Learning Rate: 1.89369e-05
	LOSS [training: 1.8508276594970363 | validation: 3.7358343490666046]
	TIME [epoch: 5.71 sec]
EPOCH 1821/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8468428915615176		[learning rate: 1.887e-05]
	Learning Rate: 1.88699e-05
	LOSS [training: 1.8468428915615176 | validation: 3.7335732821294947]
	TIME [epoch: 5.71 sec]
EPOCH 1822/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508654155853812		[learning rate: 1.8803e-05]
	Learning Rate: 1.88032e-05
	LOSS [training: 1.8508654155853812 | validation: 3.732415452873904]
	TIME [epoch: 5.71 sec]
EPOCH 1823/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849365103883224		[learning rate: 1.8737e-05]
	Learning Rate: 1.87367e-05
	LOSS [training: 1.849365103883224 | validation: 3.7254511459261175]
	TIME [epoch: 5.73 sec]
EPOCH 1824/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514461081291684		[learning rate: 1.867e-05]
	Learning Rate: 1.86704e-05
	LOSS [training: 1.8514461081291684 | validation: 3.7249340469051413]
	TIME [epoch: 5.71 sec]
EPOCH 1825/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8489091976918228		[learning rate: 1.8604e-05]
	Learning Rate: 1.86044e-05
	LOSS [training: 1.8489091976918228 | validation: 3.733267663917209]
	TIME [epoch: 5.71 sec]
EPOCH 1826/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487268337053433		[learning rate: 1.8539e-05]
	Learning Rate: 1.85386e-05
	LOSS [training: 1.8487268337053433 | validation: 3.727904564718818]
	TIME [epoch: 5.71 sec]
EPOCH 1827/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8488256920365418		[learning rate: 1.8473e-05]
	Learning Rate: 1.84731e-05
	LOSS [training: 1.8488256920365418 | validation: 3.737988018628381]
	TIME [epoch: 5.69 sec]
EPOCH 1828/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852276889162133		[learning rate: 1.8408e-05]
	Learning Rate: 1.84077e-05
	LOSS [training: 1.852276889162133 | validation: 3.727221621502059]
	TIME [epoch: 5.69 sec]
EPOCH 1829/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513888204232567		[learning rate: 1.8343e-05]
	Learning Rate: 1.83426e-05
	LOSS [training: 1.8513888204232567 | validation: 3.728263268776178]
	TIME [epoch: 5.74 sec]
EPOCH 1830/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536504303570034		[learning rate: 1.8278e-05]
	Learning Rate: 1.82778e-05
	LOSS [training: 1.8536504303570034 | validation: 3.7316194633116986]
	TIME [epoch: 5.69 sec]
EPOCH 1831/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8566008669226202		[learning rate: 1.8213e-05]
	Learning Rate: 1.82131e-05
	LOSS [training: 1.8566008669226202 | validation: 3.7296172906760514]
	TIME [epoch: 5.7 sec]
EPOCH 1832/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508928927302188		[learning rate: 1.8149e-05]
	Learning Rate: 1.81487e-05
	LOSS [training: 1.8508928927302188 | validation: 3.7314226251666156]
	TIME [epoch: 5.69 sec]
EPOCH 1833/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854787596624855		[learning rate: 1.8085e-05]
	Learning Rate: 1.80846e-05
	LOSS [training: 1.854787596624855 | validation: 3.727792087256345]
	TIME [epoch: 5.69 sec]
EPOCH 1834/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529356136203052		[learning rate: 1.8021e-05]
	Learning Rate: 1.80206e-05
	LOSS [training: 1.8529356136203052 | validation: 3.7359261893473636]
	TIME [epoch: 5.71 sec]
EPOCH 1835/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848778303442735		[learning rate: 1.7957e-05]
	Learning Rate: 1.79569e-05
	LOSS [training: 1.848778303442735 | validation: 3.732125091415639]
	TIME [epoch: 5.73 sec]
EPOCH 1836/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532051414547355		[learning rate: 1.7893e-05]
	Learning Rate: 1.78934e-05
	LOSS [training: 1.8532051414547355 | validation: 3.7249161411344076]
	TIME [epoch: 5.7 sec]
EPOCH 1837/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8547464384510453		[learning rate: 1.783e-05]
	Learning Rate: 1.78301e-05
	LOSS [training: 1.8547464384510453 | validation: 3.730289275541727]
	TIME [epoch: 5.69 sec]
EPOCH 1838/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8517590127145294		[learning rate: 1.7767e-05]
	Learning Rate: 1.77671e-05
	LOSS [training: 1.8517590127145294 | validation: 3.7321877354228423]
	TIME [epoch: 5.69 sec]
EPOCH 1839/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853486876062619		[learning rate: 1.7704e-05]
	Learning Rate: 1.77042e-05
	LOSS [training: 1.853486876062619 | validation: 3.72291710922794]
	TIME [epoch: 5.7 sec]
EPOCH 1840/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8538093124706099		[learning rate: 1.7642e-05]
	Learning Rate: 1.76416e-05
	LOSS [training: 1.8538093124706099 | validation: 3.7308288380498627]
	TIME [epoch: 5.71 sec]
EPOCH 1841/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8546979035590114		[learning rate: 1.7579e-05]
	Learning Rate: 1.75792e-05
	LOSS [training: 1.8546979035590114 | validation: 3.7340082380653006]
	TIME [epoch: 5.74 sec]
EPOCH 1842/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527747269113384		[learning rate: 1.7517e-05]
	Learning Rate: 1.75171e-05
	LOSS [training: 1.8527747269113384 | validation: 3.7262840750126087]
	TIME [epoch: 5.69 sec]
EPOCH 1843/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8499829469839901		[learning rate: 1.7455e-05]
	Learning Rate: 1.74551e-05
	LOSS [training: 1.8499829469839901 | validation: 3.730776748332686]
	TIME [epoch: 5.7 sec]
EPOCH 1844/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852569024716744		[learning rate: 1.7393e-05]
	Learning Rate: 1.73934e-05
	LOSS [training: 1.852569024716744 | validation: 3.7247155988787473]
	TIME [epoch: 5.71 sec]
EPOCH 1845/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8494305161409623		[learning rate: 1.7332e-05]
	Learning Rate: 1.73319e-05
	LOSS [training: 1.8494305161409623 | validation: 3.731202709219465]
	TIME [epoch: 5.7 sec]
EPOCH 1846/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518461366873495		[learning rate: 1.7271e-05]
	Learning Rate: 1.72706e-05
	LOSS [training: 1.8518461366873495 | validation: 3.7321609240804037]
	TIME [epoch: 5.69 sec]
EPOCH 1847/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851335254981418		[learning rate: 1.721e-05]
	Learning Rate: 1.72095e-05
	LOSS [training: 1.851335254981418 | validation: 3.7317220864275558]
	TIME [epoch: 5.74 sec]
EPOCH 1848/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8547175294680964		[learning rate: 1.7149e-05]
	Learning Rate: 1.71487e-05
	LOSS [training: 1.8547175294680964 | validation: 3.7245244679441867]
	TIME [epoch: 5.71 sec]
EPOCH 1849/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505798688754744		[learning rate: 1.7088e-05]
	Learning Rate: 1.7088e-05
	LOSS [training: 1.8505798688754744 | validation: 3.724661720034815]
	TIME [epoch: 5.7 sec]
EPOCH 1850/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526856334449189		[learning rate: 1.7028e-05]
	Learning Rate: 1.70276e-05
	LOSS [training: 1.8526856334449189 | validation: 3.726033410798418]
	TIME [epoch: 5.71 sec]
EPOCH 1851/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851031239368971		[learning rate: 1.6967e-05]
	Learning Rate: 1.69674e-05
	LOSS [training: 1.851031239368971 | validation: 3.728996277958205]
	TIME [epoch: 5.7 sec]
EPOCH 1852/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534249747105311		[learning rate: 1.6907e-05]
	Learning Rate: 1.69074e-05
	LOSS [training: 1.8534249747105311 | validation: 3.739642815883253]
	TIME [epoch: 5.69 sec]
EPOCH 1853/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542666167264923		[learning rate: 1.6848e-05]
	Learning Rate: 1.68476e-05
	LOSS [training: 1.8542666167264923 | validation: 3.724976431948132]
	TIME [epoch: 5.74 sec]
EPOCH 1854/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8524012920636048		[learning rate: 1.6788e-05]
	Learning Rate: 1.6788e-05
	LOSS [training: 1.8524012920636048 | validation: 3.725263234850958]
	TIME [epoch: 5.71 sec]
EPOCH 1855/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8536094729986978		[learning rate: 1.6729e-05]
	Learning Rate: 1.67287e-05
	LOSS [training: 1.8536094729986978 | validation: 3.7321064435269236]
	TIME [epoch: 5.7 sec]
EPOCH 1856/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8541198155909602		[learning rate: 1.667e-05]
	Learning Rate: 1.66695e-05
	LOSS [training: 1.8541198155909602 | validation: 3.7272859671171874]
	TIME [epoch: 5.7 sec]
EPOCH 1857/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851512516034961		[learning rate: 1.6611e-05]
	Learning Rate: 1.66106e-05
	LOSS [training: 1.851512516034961 | validation: 3.734904262763962]
	TIME [epoch: 5.7 sec]
EPOCH 1858/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8493628615082365		[learning rate: 1.6552e-05]
	Learning Rate: 1.65518e-05
	LOSS [training: 1.8493628615082365 | validation: 3.7275442049171112]
	TIME [epoch: 5.71 sec]
EPOCH 1859/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532719487565712		[learning rate: 1.6493e-05]
	Learning Rate: 1.64933e-05
	LOSS [training: 1.8532719487565712 | validation: 3.7253064565995433]
	TIME [epoch: 5.73 sec]
EPOCH 1860/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8509096421476188		[learning rate: 1.6435e-05]
	Learning Rate: 1.6435e-05
	LOSS [training: 1.8509096421476188 | validation: 3.719851954567835]
	TIME [epoch: 5.71 sec]
EPOCH 1861/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8531289416803483		[learning rate: 1.6377e-05]
	Learning Rate: 1.63769e-05
	LOSS [training: 1.8531289416803483 | validation: 3.729503374059115]
	TIME [epoch: 5.69 sec]
EPOCH 1862/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513575100353012		[learning rate: 1.6319e-05]
	Learning Rate: 1.6319e-05
	LOSS [training: 1.8513575100353012 | validation: 3.721916575214066]
	TIME [epoch: 5.7 sec]
EPOCH 1863/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8530087805758528		[learning rate: 1.6261e-05]
	Learning Rate: 1.62612e-05
	LOSS [training: 1.8530087805758528 | validation: 3.729706164248845]
	TIME [epoch: 5.71 sec]
EPOCH 1864/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852665312224931		[learning rate: 1.6204e-05]
	Learning Rate: 1.62038e-05
	LOSS [training: 1.852665312224931 | validation: 3.733291656296485]
	TIME [epoch: 5.71 sec]
EPOCH 1865/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8488958735021368		[learning rate: 1.6146e-05]
	Learning Rate: 1.61464e-05
	LOSS [training: 1.8488958735021368 | validation: 3.7234053704644157]
	TIME [epoch: 5.72 sec]
EPOCH 1866/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8520221542964013		[learning rate: 1.6089e-05]
	Learning Rate: 1.60894e-05
	LOSS [training: 1.8520221542964013 | validation: 3.729314879779743]
	TIME [epoch: 5.71 sec]
EPOCH 1867/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850497538954154		[learning rate: 1.6032e-05]
	Learning Rate: 1.60325e-05
	LOSS [training: 1.850497538954154 | validation: 3.727228196905364]
	TIME [epoch: 5.7 sec]
EPOCH 1868/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523757614368686		[learning rate: 1.5976e-05]
	Learning Rate: 1.59758e-05
	LOSS [training: 1.8523757614368686 | validation: 3.7245223513130457]
	TIME [epoch: 5.69 sec]
EPOCH 1869/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8478220993078351		[learning rate: 1.5919e-05]
	Learning Rate: 1.59193e-05
	LOSS [training: 1.8478220993078351 | validation: 3.73471138339723]
	TIME [epoch: 5.69 sec]
EPOCH 1870/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8488143493017024		[learning rate: 1.5863e-05]
	Learning Rate: 1.5863e-05
	LOSS [training: 1.8488143493017024 | validation: 3.73174257059183]
	TIME [epoch: 5.69 sec]
EPOCH 1871/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851057031469483		[learning rate: 1.5807e-05]
	Learning Rate: 1.58069e-05
	LOSS [training: 1.851057031469483 | validation: 3.7326917787581033]
	TIME [epoch: 5.74 sec]
EPOCH 1872/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8519662392344545		[learning rate: 1.5751e-05]
	Learning Rate: 1.5751e-05
	LOSS [training: 1.8519662392344545 | validation: 3.7323839764569238]
	TIME [epoch: 5.69 sec]
EPOCH 1873/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8506915816463758		[learning rate: 1.5695e-05]
	Learning Rate: 1.56953e-05
	LOSS [training: 1.8506915816463758 | validation: 3.7341543801686736]
	TIME [epoch: 5.7 sec]
EPOCH 1874/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848318100499879		[learning rate: 1.564e-05]
	Learning Rate: 1.56398e-05
	LOSS [training: 1.848318100499879 | validation: 3.727544103645656]
	TIME [epoch: 5.69 sec]
EPOCH 1875/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526345153857067		[learning rate: 1.5584e-05]
	Learning Rate: 1.55845e-05
	LOSS [training: 1.8526345153857067 | validation: 3.72593146334808]
	TIME [epoch: 5.7 sec]
EPOCH 1876/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8539085095717964		[learning rate: 1.5529e-05]
	Learning Rate: 1.55294e-05
	LOSS [training: 1.8539085095717964 | validation: 3.725929707748134]
	TIME [epoch: 5.71 sec]
EPOCH 1877/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8494803795318453		[learning rate: 1.5474e-05]
	Learning Rate: 1.54745e-05
	LOSS [training: 1.8494803795318453 | validation: 3.7323641503359863]
	TIME [epoch: 5.74 sec]
EPOCH 1878/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8491456013279342		[learning rate: 1.542e-05]
	Learning Rate: 1.54197e-05
	LOSS [training: 1.8491456013279342 | validation: 3.728159146268065]
	TIME [epoch: 5.71 sec]
EPOCH 1879/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501071525747481		[learning rate: 1.5365e-05]
	Learning Rate: 1.53652e-05
	LOSS [training: 1.8501071525747481 | validation: 3.7288736312695914]
	TIME [epoch: 5.7 sec]
EPOCH 1880/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8504734875964364		[learning rate: 1.5311e-05]
	Learning Rate: 1.53109e-05
	LOSS [training: 1.8504734875964364 | validation: 3.735342969144384]
	TIME [epoch: 5.7 sec]
EPOCH 1881/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8485127555398995		[learning rate: 1.5257e-05]
	Learning Rate: 1.52567e-05
	LOSS [training: 1.8485127555398995 | validation: 3.7381423790449184]
	TIME [epoch: 5.7 sec]
EPOCH 1882/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8506699038665588		[learning rate: 1.5203e-05]
	Learning Rate: 1.52028e-05
	LOSS [training: 1.8506699038665588 | validation: 3.7280655833819574]
	TIME [epoch: 5.7 sec]
EPOCH 1883/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.85358425821344		[learning rate: 1.5149e-05]
	Learning Rate: 1.5149e-05
	LOSS [training: 1.85358425821344 | validation: 3.729391602623679]
	TIME [epoch: 5.74 sec]
EPOCH 1884/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513501772100867		[learning rate: 1.5095e-05]
	Learning Rate: 1.50955e-05
	LOSS [training: 1.8513501772100867 | validation: 3.7294114659813102]
	TIME [epoch: 5.7 sec]
EPOCH 1885/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487995348352235		[learning rate: 1.5042e-05]
	Learning Rate: 1.50421e-05
	LOSS [training: 1.8487995348352235 | validation: 3.7255590162084986]
	TIME [epoch: 5.7 sec]
EPOCH 1886/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849082004963102		[learning rate: 1.4989e-05]
	Learning Rate: 1.49889e-05
	LOSS [training: 1.849082004963102 | validation: 3.7340845727011787]
	TIME [epoch: 5.7 sec]
EPOCH 1887/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8499566485194496		[learning rate: 1.4936e-05]
	Learning Rate: 1.49359e-05
	LOSS [training: 1.8499566485194496 | validation: 3.725342768499887]
	TIME [epoch: 5.7 sec]
EPOCH 1888/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511039945828793		[learning rate: 1.4883e-05]
	Learning Rate: 1.48831e-05
	LOSS [training: 1.8511039945828793 | validation: 3.733477634085828]
	TIME [epoch: 5.69 sec]
EPOCH 1889/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8495566845184241		[learning rate: 1.483e-05]
	Learning Rate: 1.48304e-05
	LOSS [training: 1.8495566845184241 | validation: 3.7334713305843774]
	TIME [epoch: 5.74 sec]
EPOCH 1890/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851842680405301		[learning rate: 1.4778e-05]
	Learning Rate: 1.4778e-05
	LOSS [training: 1.851842680405301 | validation: 3.728871648258462]
	TIME [epoch: 5.69 sec]
EPOCH 1891/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511126360540333		[learning rate: 1.4726e-05]
	Learning Rate: 1.47257e-05
	LOSS [training: 1.8511126360540333 | validation: 3.7337927181010513]
	TIME [epoch: 5.7 sec]
EPOCH 1892/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851513931186485		[learning rate: 1.4674e-05]
	Learning Rate: 1.46737e-05
	LOSS [training: 1.851513931186485 | validation: 3.7260393762648447]
	TIME [epoch: 5.7 sec]
EPOCH 1893/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8498789335979446		[learning rate: 1.4622e-05]
	Learning Rate: 1.46218e-05
	LOSS [training: 1.8498789335979446 | validation: 3.7244925800866633]
	TIME [epoch: 5.69 sec]
EPOCH 1894/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852695263605243		[learning rate: 1.457e-05]
	Learning Rate: 1.45701e-05
	LOSS [training: 1.852695263605243 | validation: 3.7268524401797105]
	TIME [epoch: 5.7 sec]
EPOCH 1895/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8506517994597453		[learning rate: 1.4519e-05]
	Learning Rate: 1.45185e-05
	LOSS [training: 1.8506517994597453 | validation: 3.7308049694766927]
	TIME [epoch: 5.73 sec]
EPOCH 1896/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8550915204131122		[learning rate: 1.4467e-05]
	Learning Rate: 1.44672e-05
	LOSS [training: 1.8550915204131122 | validation: 3.7378310529205643]
	TIME [epoch: 5.71 sec]
EPOCH 1897/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513740706468869		[learning rate: 1.4416e-05]
	Learning Rate: 1.44161e-05
	LOSS [training: 1.8513740706468869 | validation: 3.726913066919454]
	TIME [epoch: 5.71 sec]
EPOCH 1898/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513789251096195		[learning rate: 1.4365e-05]
	Learning Rate: 1.43651e-05
	LOSS [training: 1.8513789251096195 | validation: 3.733625542301004]
	TIME [epoch: 5.71 sec]
EPOCH 1899/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505862205495411		[learning rate: 1.4314e-05]
	Learning Rate: 1.43143e-05
	LOSS [training: 1.8505862205495411 | validation: 3.7264076705859885]
	TIME [epoch: 5.7 sec]
EPOCH 1900/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508797051478194		[learning rate: 1.4264e-05]
	Learning Rate: 1.42637e-05
	LOSS [training: 1.8508797051478194 | validation: 3.7257771708747076]
	TIME [epoch: 5.7 sec]
EPOCH 1901/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514482274567874		[learning rate: 1.4213e-05]
	Learning Rate: 1.42132e-05
	LOSS [training: 1.8514482274567874 | validation: 3.7289070751066182]
	TIME [epoch: 5.74 sec]
EPOCH 1902/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8456366988055328		[learning rate: 1.4163e-05]
	Learning Rate: 1.4163e-05
	LOSS [training: 1.8456366988055328 | validation: 3.7284314509557523]
	TIME [epoch: 5.71 sec]
EPOCH 1903/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8498603701854563		[learning rate: 1.4113e-05]
	Learning Rate: 1.41129e-05
	LOSS [training: 1.8498603701854563 | validation: 3.7298864281013597]
	TIME [epoch: 5.69 sec]
EPOCH 1904/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853065503873519		[learning rate: 1.4063e-05]
	Learning Rate: 1.4063e-05
	LOSS [training: 1.853065503873519 | validation: 3.7217421178026497]
	TIME [epoch: 5.69 sec]
EPOCH 1905/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523172655444697		[learning rate: 1.4013e-05]
	Learning Rate: 1.40132e-05
	LOSS [training: 1.8523172655444697 | validation: 3.7383731995663063]
	TIME [epoch: 5.69 sec]
EPOCH 1906/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8477230462577832		[learning rate: 1.3964e-05]
	Learning Rate: 1.39637e-05
	LOSS [training: 1.8477230462577832 | validation: 3.7311467798847175]
	TIME [epoch: 5.68 sec]
EPOCH 1907/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850946532606173		[learning rate: 1.3914e-05]
	Learning Rate: 1.39143e-05
	LOSS [training: 1.850946532606173 | validation: 3.727568334405487]
	TIME [epoch: 5.72 sec]
EPOCH 1908/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8507999882256139		[learning rate: 1.3865e-05]
	Learning Rate: 1.38651e-05
	LOSS [training: 1.8507999882256139 | validation: 3.730053601884507]
	TIME [epoch: 5.71 sec]
EPOCH 1909/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8519051774970183		[learning rate: 1.3816e-05]
	Learning Rate: 1.38161e-05
	LOSS [training: 1.8519051774970183 | validation: 3.728721638940746]
	TIME [epoch: 5.69 sec]
EPOCH 1910/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514680920820175		[learning rate: 1.3767e-05]
	Learning Rate: 1.37672e-05
	LOSS [training: 1.8514680920820175 | validation: 3.7281057788043372]
	TIME [epoch: 5.69 sec]
EPOCH 1911/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525134434210042		[learning rate: 1.3719e-05]
	Learning Rate: 1.37185e-05
	LOSS [training: 1.8525134434210042 | validation: 3.7278222912276813]
	TIME [epoch: 5.7 sec]
EPOCH 1912/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8512814541227876		[learning rate: 1.367e-05]
	Learning Rate: 1.367e-05
	LOSS [training: 1.8512814541227876 | validation: 3.7250277682532067]
	TIME [epoch: 5.69 sec]
EPOCH 1913/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8499131204988903		[learning rate: 1.3622e-05]
	Learning Rate: 1.36217e-05
	LOSS [training: 1.8499131204988903 | validation: 3.727950008339458]
	TIME [epoch: 5.74 sec]
EPOCH 1914/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501447400116189		[learning rate: 1.3574e-05]
	Learning Rate: 1.35735e-05
	LOSS [training: 1.8501447400116189 | validation: 3.7361612057496507]
	TIME [epoch: 5.7 sec]
EPOCH 1915/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514586471350212		[learning rate: 1.3526e-05]
	Learning Rate: 1.35255e-05
	LOSS [training: 1.8514586471350212 | validation: 3.7247643016570806]
	TIME [epoch: 5.69 sec]
EPOCH 1916/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8496051531008157		[learning rate: 1.3478e-05]
	Learning Rate: 1.34777e-05
	LOSS [training: 1.8496051531008157 | validation: 3.732439493891228]
	TIME [epoch: 5.69 sec]
EPOCH 1917/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852605988660467		[learning rate: 1.343e-05]
	Learning Rate: 1.343e-05
	LOSS [training: 1.852605988660467 | validation: 3.7168395394139715]
	TIME [epoch: 5.7 sec]
EPOCH 1918/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854467796321181		[learning rate: 1.3383e-05]
	Learning Rate: 1.33825e-05
	LOSS [training: 1.854467796321181 | validation: 3.729003107547452]
	TIME [epoch: 5.69 sec]
EPOCH 1919/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8485781541864368		[learning rate: 1.3335e-05]
	Learning Rate: 1.33352e-05
	LOSS [training: 1.8485781541864368 | validation: 3.7257053039114134]
	TIME [epoch: 5.74 sec]
EPOCH 1920/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851215687858735		[learning rate: 1.3288e-05]
	Learning Rate: 1.32881e-05
	LOSS [training: 1.851215687858735 | validation: 3.726023043490817]
	TIME [epoch: 5.7 sec]
EPOCH 1921/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523550236008441		[learning rate: 1.3241e-05]
	Learning Rate: 1.32411e-05
	LOSS [training: 1.8523550236008441 | validation: 3.7269160658583504]
	TIME [epoch: 5.7 sec]
EPOCH 1922/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8529818474347346		[learning rate: 1.3194e-05]
	Learning Rate: 1.31942e-05
	LOSS [training: 1.8529818474347346 | validation: 3.7283310264068086]
	TIME [epoch: 5.69 sec]
EPOCH 1923/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850833682955806		[learning rate: 1.3148e-05]
	Learning Rate: 1.31476e-05
	LOSS [training: 1.850833682955806 | validation: 3.7291186885400904]
	TIME [epoch: 5.69 sec]
EPOCH 1924/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851040249560902		[learning rate: 1.3101e-05]
	Learning Rate: 1.31011e-05
	LOSS [training: 1.851040249560902 | validation: 3.7292908711399075]
	TIME [epoch: 5.7 sec]
EPOCH 1925/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8526588448584804		[learning rate: 1.3055e-05]
	Learning Rate: 1.30548e-05
	LOSS [training: 1.8526588448584804 | validation: 3.7294687330053105]
	TIME [epoch: 5.74 sec]
EPOCH 1926/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849949477864913		[learning rate: 1.3009e-05]
	Learning Rate: 1.30086e-05
	LOSS [training: 1.849949477864913 | validation: 3.7325809162274464]
	TIME [epoch: 5.7 sec]
EPOCH 1927/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8488299004202082		[learning rate: 1.2963e-05]
	Learning Rate: 1.29626e-05
	LOSS [training: 1.8488299004202082 | validation: 3.7306515735865458]
	TIME [epoch: 5.71 sec]
EPOCH 1928/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8490961652619051		[learning rate: 1.2917e-05]
	Learning Rate: 1.29168e-05
	LOSS [training: 1.8490961652619051 | validation: 3.725056687004949]
	TIME [epoch: 5.69 sec]
EPOCH 1929/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505212885772426		[learning rate: 1.2871e-05]
	Learning Rate: 1.28711e-05
	LOSS [training: 1.8505212885772426 | validation: 3.7294459711924084]
	TIME [epoch: 5.7 sec]
EPOCH 1930/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8475513683580145		[learning rate: 1.2826e-05]
	Learning Rate: 1.28256e-05
	LOSS [training: 1.8475513683580145 | validation: 3.7213678483223744]
	TIME [epoch: 5.69 sec]
EPOCH 1931/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8482901952967428		[learning rate: 1.278e-05]
	Learning Rate: 1.27802e-05
	LOSS [training: 1.8482901952967428 | validation: 3.727037118198837]
	TIME [epoch: 5.73 sec]
EPOCH 1932/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513280583054212		[learning rate: 1.2735e-05]
	Learning Rate: 1.2735e-05
	LOSS [training: 1.8513280583054212 | validation: 3.73261480908513]
	TIME [epoch: 5.71 sec]
EPOCH 1933/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8499180949363072		[learning rate: 1.269e-05]
	Learning Rate: 1.269e-05
	LOSS [training: 1.8499180949363072 | validation: 3.7377560288973393]
	TIME [epoch: 5.7 sec]
EPOCH 1934/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8537952459641271		[learning rate: 1.2645e-05]
	Learning Rate: 1.26451e-05
	LOSS [training: 1.8537952459641271 | validation: 3.721463452920044]
	TIME [epoch: 5.69 sec]
EPOCH 1935/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8471679200520934		[learning rate: 1.26e-05]
	Learning Rate: 1.26004e-05
	LOSS [training: 1.8471679200520934 | validation: 3.7266954421980514]
	TIME [epoch: 5.69 sec]
EPOCH 1936/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8517305813168528		[learning rate: 1.2556e-05]
	Learning Rate: 1.25559e-05
	LOSS [training: 1.8517305813168528 | validation: 3.72451926502892]
	TIME [epoch: 5.69 sec]
EPOCH 1937/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8497168040692826		[learning rate: 1.2511e-05]
	Learning Rate: 1.25115e-05
	LOSS [training: 1.8497168040692826 | validation: 3.727584392302883]
	TIME [epoch: 5.72 sec]
EPOCH 1938/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8518747691909858		[learning rate: 1.2467e-05]
	Learning Rate: 1.24672e-05
	LOSS [training: 1.8518747691909858 | validation: 3.725308004911823]
	TIME [epoch: 5.69 sec]
EPOCH 1939/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8455089431063167		[learning rate: 1.2423e-05]
	Learning Rate: 1.24231e-05
	LOSS [training: 1.8455089431063167 | validation: 3.7216865706677016]
	TIME [epoch: 5.69 sec]
EPOCH 1940/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8508240025563638		[learning rate: 1.2379e-05]
	Learning Rate: 1.23792e-05
	LOSS [training: 1.8508240025563638 | validation: 3.7238184641917904]
	TIME [epoch: 5.7 sec]
EPOCH 1941/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8503593674713903		[learning rate: 1.2335e-05]
	Learning Rate: 1.23354e-05
	LOSS [training: 1.8503593674713903 | validation: 3.724874542000808]
	TIME [epoch: 5.69 sec]
EPOCH 1942/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.853371558384366		[learning rate: 1.2292e-05]
	Learning Rate: 1.22918e-05
	LOSS [training: 1.853371558384366 | validation: 3.7261024056647467]
	TIME [epoch: 5.71 sec]
EPOCH 1943/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8493534073635658		[learning rate: 1.2248e-05]
	Learning Rate: 1.22483e-05
	LOSS [training: 1.8493534073635658 | validation: 3.7237757852478]
	TIME [epoch: 5.75 sec]
EPOCH 1944/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8522632910127546		[learning rate: 1.2205e-05]
	Learning Rate: 1.2205e-05
	LOSS [training: 1.8522632910127546 | validation: 3.7244220878969725]
	TIME [epoch: 5.7 sec]
EPOCH 1945/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8527256989685132		[learning rate: 1.2162e-05]
	Learning Rate: 1.21619e-05
	LOSS [training: 1.8527256989685132 | validation: 3.725970285808015]
	TIME [epoch: 5.7 sec]
EPOCH 1946/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511010887041315		[learning rate: 1.2119e-05]
	Learning Rate: 1.21189e-05
	LOSS [training: 1.8511010887041315 | validation: 3.7270925289423484]
	TIME [epoch: 5.7 sec]
EPOCH 1947/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8493434713925616		[learning rate: 1.2076e-05]
	Learning Rate: 1.2076e-05
	LOSS [training: 1.8493434713925616 | validation: 3.7198354738022843]
	TIME [epoch: 5.7 sec]
EPOCH 1948/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8479506402508687		[learning rate: 1.2033e-05]
	Learning Rate: 1.20333e-05
	LOSS [training: 1.8479506402508687 | validation: 3.7260234785944273]
	TIME [epoch: 5.69 sec]
EPOCH 1949/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850727259216741		[learning rate: 1.1991e-05]
	Learning Rate: 1.19907e-05
	LOSS [training: 1.850727259216741 | validation: 3.7285277206367913]
	TIME [epoch: 5.73 sec]
EPOCH 1950/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.854587791194784		[learning rate: 1.1948e-05]
	Learning Rate: 1.19483e-05
	LOSS [training: 1.854587791194784 | validation: 3.735031527532147]
	TIME [epoch: 5.71 sec]
EPOCH 1951/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513658773892487		[learning rate: 1.1906e-05]
	Learning Rate: 1.19061e-05
	LOSS [training: 1.8513658773892487 | validation: 3.7375503904315877]
	TIME [epoch: 5.71 sec]
EPOCH 1952/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8520620750487518		[learning rate: 1.1864e-05]
	Learning Rate: 1.1864e-05
	LOSS [training: 1.8520620750487518 | validation: 3.7261207742162883]
	TIME [epoch: 5.71 sec]
EPOCH 1953/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534528600866864		[learning rate: 1.1822e-05]
	Learning Rate: 1.1822e-05
	LOSS [training: 1.8534528600866864 | validation: 3.7295022547547934]
	TIME [epoch: 5.69 sec]
EPOCH 1954/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501566846672062		[learning rate: 1.178e-05]
	Learning Rate: 1.17802e-05
	LOSS [training: 1.8501566846672062 | validation: 3.7336914466891957]
	TIME [epoch: 5.69 sec]
EPOCH 1955/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501038924370645		[learning rate: 1.1739e-05]
	Learning Rate: 1.17386e-05
	LOSS [training: 1.8501038924370645 | validation: 3.7245479092372693]
	TIME [epoch: 5.74 sec]
EPOCH 1956/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8514515019286755		[learning rate: 1.1697e-05]
	Learning Rate: 1.16971e-05
	LOSS [training: 1.8514515019286755 | validation: 3.7344220548323985]
	TIME [epoch: 5.7 sec]
EPOCH 1957/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852363933803111		[learning rate: 1.1656e-05]
	Learning Rate: 1.16557e-05
	LOSS [training: 1.852363933803111 | validation: 3.735502528462887]
	TIME [epoch: 5.69 sec]
EPOCH 1958/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8542004594401382		[learning rate: 1.1614e-05]
	Learning Rate: 1.16145e-05
	LOSS [training: 1.8542004594401382 | validation: 3.726973704769289]
	TIME [epoch: 5.69 sec]
EPOCH 1959/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851840750997673		[learning rate: 1.1573e-05]
	Learning Rate: 1.15734e-05
	LOSS [training: 1.851840750997673 | validation: 3.722047428916269]
	TIME [epoch: 5.7 sec]
EPOCH 1960/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8510643005763114		[learning rate: 1.1532e-05]
	Learning Rate: 1.15325e-05
	LOSS [training: 1.8510643005763114 | validation: 3.7384531082219]
	TIME [epoch: 5.69 sec]
EPOCH 1961/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.855096068093282		[learning rate: 1.1492e-05]
	Learning Rate: 1.14917e-05
	LOSS [training: 1.855096068093282 | validation: 3.7298164782259624]
	TIME [epoch: 5.73 sec]
EPOCH 1962/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8533558560528307		[learning rate: 1.1451e-05]
	Learning Rate: 1.14511e-05
	LOSS [training: 1.8533558560528307 | validation: 3.7188184591765037]
	TIME [epoch: 5.7 sec]
EPOCH 1963/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8509073699023488		[learning rate: 1.1411e-05]
	Learning Rate: 1.14106e-05
	LOSS [training: 1.8509073699023488 | validation: 3.7267050871086473]
	TIME [epoch: 5.7 sec]
EPOCH 1964/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851330919626442		[learning rate: 1.137e-05]
	Learning Rate: 1.13702e-05
	LOSS [training: 1.851330919626442 | validation: 3.726147994543715]
	TIME [epoch: 5.69 sec]
EPOCH 1965/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501100033461806		[learning rate: 1.133e-05]
	Learning Rate: 1.133e-05
	LOSS [training: 1.8501100033461806 | validation: 3.7260143004432567]
	TIME [epoch: 5.7 sec]
EPOCH 1966/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8490673976995908		[learning rate: 1.129e-05]
	Learning Rate: 1.129e-05
	LOSS [training: 1.8490673976995908 | validation: 3.7291681808635566]
	TIME [epoch: 5.69 sec]
EPOCH 1967/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8509980041282343		[learning rate: 1.125e-05]
	Learning Rate: 1.125e-05
	LOSS [training: 1.8509980041282343 | validation: 3.7298865366806546]
	TIME [epoch: 5.73 sec]
EPOCH 1968/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8505858813587714		[learning rate: 1.121e-05]
	Learning Rate: 1.12103e-05
	LOSS [training: 1.8505858813587714 | validation: 3.7296335434644945]
	TIME [epoch: 5.7 sec]
EPOCH 1969/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8493808019699598		[learning rate: 1.1171e-05]
	Learning Rate: 1.11706e-05
	LOSS [training: 1.8493808019699598 | validation: 3.722873072620006]
	TIME [epoch: 5.71 sec]
EPOCH 1970/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848593483242885		[learning rate: 1.1131e-05]
	Learning Rate: 1.11311e-05
	LOSS [training: 1.848593483242885 | validation: 3.7357702491486497]
	TIME [epoch: 5.69 sec]
EPOCH 1971/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8525952469834972		[learning rate: 1.1092e-05]
	Learning Rate: 1.10918e-05
	LOSS [training: 1.8525952469834972 | validation: 3.7302499930154966]
	TIME [epoch: 5.71 sec]
EPOCH 1972/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487216046841368		[learning rate: 1.1053e-05]
	Learning Rate: 1.10525e-05
	LOSS [training: 1.8487216046841368 | validation: 3.7344820635835014]
	TIME [epoch: 5.69 sec]
EPOCH 1973/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8487377887814735		[learning rate: 1.1013e-05]
	Learning Rate: 1.10134e-05
	LOSS [training: 1.8487377887814735 | validation: 3.72879036768972]
	TIME [epoch: 5.74 sec]
EPOCH 1974/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8511897456414945		[learning rate: 1.0975e-05]
	Learning Rate: 1.09745e-05
	LOSS [training: 1.8511897456414945 | validation: 3.730664780575994]
	TIME [epoch: 5.7 sec]
EPOCH 1975/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8462024199362859		[learning rate: 1.0936e-05]
	Learning Rate: 1.09357e-05
	LOSS [training: 1.8462024199362859 | validation: 3.7287683796663935]
	TIME [epoch: 5.69 sec]
EPOCH 1976/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8471393251851294		[learning rate: 1.0897e-05]
	Learning Rate: 1.0897e-05
	LOSS [training: 1.8471393251851294 | validation: 3.724611719652568]
	TIME [epoch: 5.69 sec]
EPOCH 1977/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.848192263836637		[learning rate: 1.0858e-05]
	Learning Rate: 1.08585e-05
	LOSS [training: 1.848192263836637 | validation: 3.7275210626411144]
	TIME [epoch: 5.69 sec]
EPOCH 1978/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8519088050355754		[learning rate: 1.082e-05]
	Learning Rate: 1.08201e-05
	LOSS [training: 1.8519088050355754 | validation: 3.7339990111700754]
	TIME [epoch: 5.69 sec]
EPOCH 1979/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.852168193274336		[learning rate: 1.0782e-05]
	Learning Rate: 1.07818e-05
	LOSS [training: 1.852168193274336 | validation: 3.725881002699697]
	TIME [epoch: 5.73 sec]
EPOCH 1980/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850730433971917		[learning rate: 1.0744e-05]
	Learning Rate: 1.07437e-05
	LOSS [training: 1.850730433971917 | validation: 3.734348301075385]
	TIME [epoch: 5.72 sec]
EPOCH 1981/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849377855125113		[learning rate: 1.0706e-05]
	Learning Rate: 1.07057e-05
	LOSS [training: 1.849377855125113 | validation: 3.728229821076684]
	TIME [epoch: 5.69 sec]
EPOCH 1982/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8519320281371443		[learning rate: 1.0668e-05]
	Learning Rate: 1.06679e-05
	LOSS [training: 1.8519320281371443 | validation: 3.727587051416989]
	TIME [epoch: 5.69 sec]
EPOCH 1983/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8532810967001179		[learning rate: 1.063e-05]
	Learning Rate: 1.06301e-05
	LOSS [training: 1.8532810967001179 | validation: 3.7303573810202852]
	TIME [epoch: 5.69 sec]
EPOCH 1984/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.849674421240838		[learning rate: 1.0593e-05]
	Learning Rate: 1.05925e-05
	LOSS [training: 1.849674421240838 | validation: 3.7264557263516265]
	TIME [epoch: 5.7 sec]
EPOCH 1985/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8516385121529821		[learning rate: 1.0555e-05]
	Learning Rate: 1.05551e-05
	LOSS [training: 1.8516385121529821 | validation: 3.7293073299172494]
	TIME [epoch: 5.73 sec]
EPOCH 1986/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501968976746503		[learning rate: 1.0518e-05]
	Learning Rate: 1.05178e-05
	LOSS [training: 1.8501968976746503 | validation: 3.724482074656941]
	TIME [epoch: 5.7 sec]
EPOCH 1987/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8493280803299381		[learning rate: 1.0481e-05]
	Learning Rate: 1.04806e-05
	LOSS [training: 1.8493280803299381 | validation: 3.723100921568991]
	TIME [epoch: 5.7 sec]
EPOCH 1988/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8513549013646877		[learning rate: 1.0444e-05]
	Learning Rate: 1.04435e-05
	LOSS [training: 1.8513549013646877 | validation: 3.727812760306467]
	TIME [epoch: 5.69 sec]
EPOCH 1989/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.850326356115589		[learning rate: 1.0407e-05]
	Learning Rate: 1.04066e-05
	LOSS [training: 1.850326356115589 | validation: 3.7268154487226073]
	TIME [epoch: 5.69 sec]
EPOCH 1990/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8501305524634697		[learning rate: 1.037e-05]
	Learning Rate: 1.03698e-05
	LOSS [training: 1.8501305524634697 | validation: 3.727150364902185]
	TIME [epoch: 5.69 sec]
EPOCH 1991/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.847281967225397		[learning rate: 1.0333e-05]
	Learning Rate: 1.03331e-05
	LOSS [training: 1.847281967225397 | validation: 3.726716808018157]
	TIME [epoch: 5.75 sec]
EPOCH 1992/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8549542367337706		[learning rate: 1.0297e-05]
	Learning Rate: 1.02966e-05
	LOSS [training: 1.8549542367337706 | validation: 3.7293568348451562]
	TIME [epoch: 5.7 sec]
EPOCH 1993/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8484756110497018		[learning rate: 1.026e-05]
	Learning Rate: 1.02602e-05
	LOSS [training: 1.8484756110497018 | validation: 3.719739264711728]
	TIME [epoch: 5.7 sec]
EPOCH 1994/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8534566945885933		[learning rate: 1.0224e-05]
	Learning Rate: 1.02239e-05
	LOSS [training: 1.8534566945885933 | validation: 3.7313445677213726]
	TIME [epoch: 5.7 sec]
EPOCH 1995/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8483774864771116		[learning rate: 1.0188e-05]
	Learning Rate: 1.01877e-05
	LOSS [training: 1.8483774864771116 | validation: 3.7329120293016196]
	TIME [epoch: 5.71 sec]
EPOCH 1996/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8544246806604232		[learning rate: 1.0152e-05]
	Learning Rate: 1.01517e-05
	LOSS [training: 1.8544246806604232 | validation: 3.730967750268485]
	TIME [epoch: 5.69 sec]
EPOCH 1997/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8490828962864994		[learning rate: 1.0116e-05]
	Learning Rate: 1.01158e-05
	LOSS [training: 1.8490828962864994 | validation: 3.7308545157280255]
	TIME [epoch: 5.74 sec]
EPOCH 1998/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8523086064385523		[learning rate: 1.008e-05]
	Learning Rate: 1.008e-05
	LOSS [training: 1.8523086064385523 | validation: 3.727816039263334]
	TIME [epoch: 5.7 sec]
EPOCH 1999/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.851666552313215		[learning rate: 1.0044e-05]
	Learning Rate: 1.00444e-05
	LOSS [training: 1.851666552313215 | validation: 3.725351114184189]
	TIME [epoch: 5.69 sec]
EPOCH 2000/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.8492066668446427		[learning rate: 1.0009e-05]
	Learning Rate: 1.00089e-05
	LOSS [training: 1.8492066668446427 | validation: 3.7290509084797483]
	TIME [epoch: 5.7 sec]
Finished training in 11634.792 seconds.
